{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:57.161999Z",
     "start_time": "2024-04-17T01:01:57.158242Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:57.211132Z",
     "start_time": "2024-04-17T01:01:57.205476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Version Info\n",
    "print(\"Tensforflow Version : \" ,tf.__version__)\n",
    "print(\"Transformers Version : \" ,transformers.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensforflow Version :  2.12.0\n",
      "Transformers Version :  4.35.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:57.365458Z",
     "start_time": "2024-04-17T01:01:57.315754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make a data folder\n",
    "!mkdir -p data\n",
    "\n",
    "# download the data\n",
    "out_path = tf.keras.utils.get_file(origin=\"https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\",extract=True,cache_dir=\"data\")\n",
    "print(\"\\n\",out_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " data\\datasets\\sentiment+labelled+sentences.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file data already exists.\n",
      "Error occurred while processing: data.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.652096Z",
     "start_time": "2024-04-17T01:01:57.365458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Define the data directory path\n",
    "data_dir = os.path.join(parent_dir, 'data', 'text_entailment_dataset')\n",
    "\n",
    "# Define file paths\n",
    "train_csv_path = os.path.join(data_dir, 'train.csv')\n",
    "train_data_csv_path = os.path.join(data_dir, 'train_data.csv')\n",
    "validation_data_csv_path = os.path.join(data_dir, 'validation_data.csv')\n",
    "test_data_csv_path = os.path.join(data_dir, 'test_data.csv')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training set into train and test subsets\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Write the training and validation DataFrames to separate CSV files\n",
    "train_df.to_csv(train_data_csv_path, index=False)\n",
    "validation_df.to_csv(validation_data_csv_path, index=False)\n",
    "test_df.to_csv(test_data_csv_path, index=False)\n",
    "\n",
    "# Read datasets from CSV files\n",
    "train_dataset = pd.read_csv(train_data_csv_path)\n",
    "validation_dataset = pd.read_csv(validation_data_csv_path)\n",
    "test_dataset = pd.read_csv(test_data_csv_path)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.656719Z",
     "start_time": "2024-04-17T01:01:59.652096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "(351590, 3)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.665891Z",
     "start_time": "2024-04-17T01:01:59.656719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "validation_dataset.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  A young couple looking at different types and ...   \n1  A man in a purple had climbs a rocky wall with...   \n2  A group of friends playing cards and trying to...   \n3             People looking at fish at an aquarium.   \n4  An older lady blowing out a 9 and a 0 number c...   \n\n                             hypothesis  label  \n0                   a couple is looking      0  \n1         A man is going to the temple.      2  \n2   The people are playing a card game.      0  \n3               People looking at fish.      0  \n4  A woman drinks coffee while driving.      2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A young couple looking at different types and ...</td>\n      <td>a couple is looking</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A man in a purple had climbs a rocky wall with...</td>\n      <td>A man is going to the temple.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A group of friends playing cards and trying to...</td>\n      <td>The people are playing a card game.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>People looking at fish at an aquarium.</td>\n      <td>People looking at fish.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>An older lady blowing out a 9 and a 0 number c...</td>\n      <td>A woman drinks coffee while driving.</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.674083Z",
     "start_time": "2024-04-17T01:01:59.665891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "validation_dataset.shape\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54937, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.682881Z",
     "start_time": "2024-04-17T01:01:59.674083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             premise  \\\n",
       "0  This church choir sings to the masses as they ...   \n",
       "1  This church choir sings to the masses as they ...   \n",
       "2  This church choir sings to the masses as they ...   \n",
       "3  A woman with a green headscarf, blue shirt and...   \n",
       "4  A woman with a green headscarf, blue shirt and...   \n",
       "\n",
       "                              hypothesis  \n",
       "0  The church has cracks in the ceiling.  \n",
       "1        The church is filled with song.  \n",
       "2    A choir singing at a baseball game.  \n",
       "3                    The woman is young.  \n",
       "4               The woman is very happy.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church has cracks in the ceiling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church is filled with song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>A choir singing at a baseball game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is young.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is very happy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.690678Z",
     "start_time": "2024-04-17T01:01:59.682881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9824, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:01:59.697114Z",
     "start_time": "2024-04-17T01:01:59.690678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define a max length constant\n",
    "MAX_LENGTH = 64"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:00.651511Z",
     "start_time": "2024-04-17T01:01:59.697114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:00.656022Z",
     "start_time": "2024-04-17T01:02:00.651511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer)\n",
    "print(bert)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:10.986179Z",
     "start_time": "2024-04-17T01:02:00.656022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset[[\"premise\"]] = train_dataset[[\"premise\"]].astype(str)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.change_lower)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.clean_data)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "train_dataset[[\"hypothesis\"]] = train_dataset[[\"hypothesis\"]].astype(str)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"premise\"]] = validation_dataset[[\"premise\"]].astype(str)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.change_lower)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.clean_data)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"hypothesis\"]] = validation_dataset[[\"hypothesis\"]].astype(str)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"premise\"]] = test_dataset[[\"premise\"]].astype(str)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.change_lower)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.clean_data)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"hypothesis\"]] = test_dataset[[\"hypothesis\"]].astype(str)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.remover)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:11.043313Z",
     "start_time": "2024-04-17T01:02:10.986179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = train_dataset['premise'] + train_dataset['hypothesis']\n",
    "X_Val = validation_dataset['premise'] + validation_dataset['hypothesis']\n",
    "X_test = test_dataset['premise'] + test_dataset['hypothesis']"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:11.115653Z",
     "start_time": "2024-04-17T01:02:11.043313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:11.861582Z",
     "start_time": "2024-04-17T01:02:11.115653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define a batch size for our experiments\n",
    "BATCH_SIZE = 64\n",
    "# define a percentage of the data to use for training\n",
    "\n",
    "train_dataset = df = pd.read_csv('data/text_entailment_dataset/train_data.csv')\n",
    "validation_dataset = df = pd.read_csv('data/text_entailment_dataset/validation_data.csv')\n",
    "test_dataset = df = pd.read_csv('data/text_entailment_dataset/test_data.csv')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:55.797601Z",
     "start_time": "2024-04-17T01:02:11.861582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_sentences = [train_dataset.loc[s][0] + train_dataset.loc[s][1] for s in range(len(train_dataset))]\n",
    "train_labels = [train_dataset.loc[l][2] for l in range(len(train_dataset))]\n",
    "\n",
    "validation_sentences = [validation_dataset.loc[s][0] + validation_dataset.loc[s][1] for s in range(len(validation_dataset))]\n",
    "validation_labels = [validation_dataset.loc[l][2] for l in range(len(validation_dataset))]\n",
    "\n",
    "test_sentences = [test_dataset.loc[s][0] + test_dataset.loc[s][1] for s in range(len(test_dataset))]\n",
    "\n",
    "print(\"LENGTHS // Train sentences: \" + str(len(train_sentences)) + \". Train labels: \" + str(len(train_labels)))\n",
    "print(\"LENGTHS // Test sentences: \" + str(len(validation_sentences)) + \". Test labels: \" + str(len(validation_labels)))\n",
    "print(\"LENGTHS // Test sentences: \" + str(len(test_sentences)))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_15616\\67804960.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  train_sentences = [train_dataset.loc[s][0] + train_dataset.loc[s][1] for s in range(len(train_dataset))]\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_15616\\67804960.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  train_labels = [train_dataset.loc[l][2] for l in range(len(train_dataset))]\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_15616\\67804960.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  validation_sentences = [validation_dataset.loc[s][0] + validation_dataset.loc[s][1] for s in range(len(validation_dataset))]\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_15616\\67804960.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  validation_labels = [validation_dataset.loc[l][2] for l in range(len(validation_dataset))]\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_15616\\67804960.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  test_sentences = [test_dataset.loc[s][0] + test_dataset.loc[s][1] for s in range(len(test_dataset))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTHS // Train sentences: 494424. Train labels: 494424\n",
      "LENGTHS // Test sentences: 54937. Test labels: 54937\n",
      "LENGTHS // Test sentences: 9824\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:02:55.835128Z",
     "start_time": "2024-04-17T01:02:55.797601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels = train_dataset[\"label\"]\n",
    "validation_labels = validation_dataset[\"label\"]\n",
    "\n",
    "# Convert to one-hot encoded format\n",
    "num_classes = len(set(train_labels))  # Calculate the number of classes\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "print(\"train label shape:\", train_labels.shape)\n",
    "print(\"val label shape:\", validation_labels.shape)\n",
    "\n",
    "steps_per_epoch = len(train_labels) // BATCH_SIZE\n",
    "validation_steps = len(validation_labels) // BATCH_SIZE\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label shape: (494424, 3)\n",
      "val label shape: (54937, 3)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.325924Z",
     "start_time": "2024-04-17T01:02:55.835830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Implementing KMeans clustering for better labels\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(train_sentences)\n",
    "\n",
    "# Choosing 3 clusters\n",
    "k = 3\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(X)\n",
    "\n",
    "# Get cluster assignments for the training data\n",
    "train_cluster_labels = kmeans.labels_\n",
    "train_labels = to_categorical(train_cluster_labels, num_classes=num_classes)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.331680Z",
     "start_time": "2024-04-17T01:03:03.325924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data generator for the model\n",
    "def data_generator(sentences: np.array,labels: np.array,batch_size: int) -> (dict,tf.Tensor):\n",
    "    i = 0\n",
    "    start_idx = -1 * batch_size\n",
    "    end_idx = 0\n",
    "\n",
    "    while True:\n",
    "        start_idx += batch_size\n",
    "        end_idx += batch_size\n",
    "        # TODO: append batch_size number of sentences and labels to batch_x and batch_y\n",
    "        # Make sure that you don't re-use sentences and labels that you've already put into batches!\n",
    "\n",
    "        if end_idx > len(sentences):\n",
    "            end_idx = batch_size\n",
    "            start_idx = 0\n",
    "\n",
    "        batch_y = labels[start_idx:end_idx]\n",
    "\n",
    "        # TODO: tokenize the batch_x, padding to MAX_LENGTH, and truncating to MAX_LENGTH\n",
    "        batch_x = tokenizer(sentences[start_idx:end_idx], return_tensors=\"tf\", max_length=MAX_LENGTH, truncation=\"longest_first\", padding=\"max_length\")\n",
    "\n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size, MAX_LENGTH)\n",
    "        # print(batch_x['input_ids'].shape)\n",
    "\n",
    "        # convert our ys into the appropriate tensor\n",
    "        batch_y = tf.convert_to_tensor(batch_y)\n",
    "\n",
    "        # debugging prints (make sure that these are commented out when you actually train your model)\n",
    "        # should be (batch_size,)\n",
    "        # print(batch_y.shape)\n",
    "        yield dict(batch_x), batch_y\n",
    "\n",
    "train_data = data_generator(train_sentences,train_labels,BATCH_SIZE)\n",
    "val_data = data_generator(validation_sentences,validation_labels,BATCH_SIZE)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.345698Z",
     "start_time": "2024-04-17T01:03:03.332261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Take a look at the contents of tmp_batch_x and tmp_batch_y and report the shapes of the `input_ids`\n",
    "# and the y label tensor.\n",
    "# make sure that the shapes are what you expect them to be\n",
    "# (take a look at the comments in the data_generator code)\n",
    "\n",
    "tmp_batch_x,tmp_batch_y = next(train_data)\n",
    "val_batch_x, val_batch_y = next(val_data)\n",
    "\n",
    "print(tmp_batch_x[\"input_ids\"].shape)\n",
    "print(tmp_batch_y.shape)\n",
    "#print(tmp_batch_y)\n",
    "\n",
    "print(val_batch_x[\"input_ids\"].shape)\n",
    "print(val_batch_y.shape)\n",
    "#print(val_batch_y)\n",
    "print(train_cluster_labels)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 3)\n",
      "(64, 64)\n",
      "(64, 3)\n",
      "[2 2 2 ... 2 1 1]\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.353357Z",
     "start_time": "2024-04-17T01:03:03.345698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import History\n",
    "from transformers import TFDistilBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Define custom metrics functions\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "    return f1\n"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.364208Z",
     "start_time": "2024-04-17T01:03:03.353357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.370357Z",
     "start_time": "2024-04-17T01:03:03.365598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Get the number of GPUs needed\n",
    "print(\"Total GPUs: \", strategy.num_replicas_in_sync)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.478142Z",
     "start_time": "2024-04-17T01:03:03.370357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Prints the GPUs on your machine (if it's nvidia)\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 16 21:03:03 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.78                 Driver Version: 551.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8             13W /  115W |      92MiB /   6144MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4880    C+G   ...0vhbc3ng4wbp0\\DellMobileConnect.exe      N/A      |\n",
      "|    0   N/A  N/A      5728    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A      7928    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9220    C+G   ...8.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     11348    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18980    C+G   ...\\Local\\slack\\app-4.37.101\\slack.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T01:03:03.483155Z",
     "start_time": "2024-04-17T01:03:03.478142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#This will be our steps per epoch\n",
    "print(len(train_sentences)//BATCH_SIZE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7725\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-17T01:03:03.483155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with strategy.scope() as scope:\n",
    "\n",
    "    bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased',output_attentions = False,return_dict=False)\n",
    "    # we do not need attention outputs\n",
    "    # we want to return tuples since they are easier to access\n",
    "\n",
    "    bert_model.trainable = False\n",
    "    # setting trainable to false ensures\n",
    "    # we do not update its weights\n",
    "\n",
    "    # Define the learning rate schedule parameters\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_rate = 0.95\n",
    "    decay_steps = 1000\n",
    "\n",
    "    # Create an exponential decay learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True  # Optional: Whether to apply decay in a staircase manner\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model_ = tf.keras.Sequential([\n",
    "        bert_model,\n",
    "        tf.keras.layers.Lambda(lambda x: x[0][:,0,:]), # https://keras.io/api/layers/core_layers/lambda/\n",
    "        tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(32,activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10,activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(3,activation=\"softmax\") # we have 3 classes\n",
    "    ])\n",
    "\n",
    "    # Define a checkpoint callback to save the best model\n",
    "    checkpoint = ModelCheckpoint('distilbert_trained_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "    model_.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "    # Use the first batch of training data we got to instantiate -- needed for loading weights\n",
    "    model_(tmp_batch_x) \n",
    "    \n",
    "    #Load model if needed\n",
    "    #model_.load_weights('transformer_weights.h5')\n",
    "    # Define a callback to collect metrics history\n",
    "    history = History()\n",
    "\n",
    "    model_.fit(\n",
    "        train_data,\n",
    "        epochs=2,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=validation_steps,\n",
    "        validation_batch_size=BATCH_SIZE,\n",
    "        callbacks=[history, checkpoint]\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 174/7725 [..............................] - ETA: 2:13:06 - loss: 0.5664 - accuracy: 0.7651 - precision: 0.7971 - recall: 0.6592 - f1: 0.6972"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#save our weights -- CANNOT SAVE MODEL BECAUSE OF LAMBDA LAYER\n",
    "model_.save_weights('transformer_weights.h5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Access the metrics history\n",
    "print(history.history.keys())  # To see what metrics are available"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Access the metrics history\n",
    "epochs = list(range(1, len(history.history['accuracy']) + 1))  # Assuming all metrics have the same length\n",
    "precision = np.array(history.history['precision'])\n",
    "recall = np.array(history.history['recall'])\n",
    "f1 = np.array(history.history['f1'])\n",
    "accuracy = np.array(history.history['accuracy'])\n",
    "loss = np.array(history.history['loss'])\n",
    "val_accuracy = np.array(history.history['val_accuracy'])\n",
    "val_loss = np.array(history.history['val_loss'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot all four metrics on one graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "\n",
    "plt.title('Metrics across Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.savefig(\"Transformer_BERT_training_val_loss_acc(1).pdf\")  # Save the plot before showing\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot all four metrics on one graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, precision, label='Precision')\n",
    "plt.plot(epochs, recall, label='Recall')\n",
    "plt.plot(epochs, f1, label='F1 Score')\n",
    "plt.plot(epochs, accuracy, label='Accuracy')\n",
    "\n",
    "plt.title('Metrics across Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(epochs)\n",
    "plt.legend()\n",
    "plt.savefig(\"Transformer_BERT_training_PRFA(1).pdf\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
