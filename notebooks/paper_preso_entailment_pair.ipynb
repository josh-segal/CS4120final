{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# !pip install datasets\n",
    "# !pip install evaluate\n",
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "# !pip install bs4\n",
    "# !pip install lxml"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5eWlFMrgojZ",
    "outputId": "3c4706a2-652c-4d64-9a0a-a94f8ed55712",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.361549Z",
     "start_time": "2024-04-10T00:38:15.649587Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67eu0djHdb4N",
    "outputId": "fd234abb-2c3f-45c2-bfc0-14b5970b2495",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.369154Z",
     "start_time": "2024-04-10T00:38:28.363098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuasegal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuasegal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# dataset_path = 'dataset'\n",
    "# papers_path = 'papers'\n",
    "# presentations_path = 'presentations'\n",
    "#\n",
    "# utils.move_xml_files(dataset_path, papers_path, presentations_path)\n",
    "\n",
    "\n",
    "# source_folder = \"data/paper_slides_data/raw_data/dataset\"\n",
    "# papers_folder = \"data/paper_slides_data/raw_data/papers\"\n",
    "# presentations_folder = \"data/paper_slides_data/raw_data/presentations\"\n",
    "#\n",
    "# utils.organize_xml_folders(source_folder, papers_folder, presentations_folder)"
   ],
   "metadata": {
    "id": "S2bstsijdb4O",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.377348Z",
     "start_time": "2024-04-10T00:38:28.370316Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/paper_slides_data/raw_data/dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m papers_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/paper_slides_data/raw_data/papers\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     10\u001B[0m presentations_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/paper_slides_data/raw_data/presentations\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m utils\u001B[38;5;241m.\u001B[39morganize_xml_folders(source_folder, papers_folder, presentations_folder)\n",
      "File \u001B[0;32m~/Coding/Jupyter/NLP/final/notebooks/../utils.py:198\u001B[0m, in \u001B[0;36morganize_xml_folders\u001B[0;34m(source_folder, papers_folder, presentations_folder)\u001B[0m\n\u001B[1;32m    195\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(presentations_folder, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# Iterate through each folder in the source folder\u001B[39;00m\n\u001B[0;32m--> 198\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, folder_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28msorted\u001B[39m(os\u001B[38;5;241m.\u001B[39mlistdir(source_folder))):\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;66;03m# Create the full path to the current folder\u001B[39;00m\n\u001B[1;32m    200\u001B[0m     folder_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(source_folder, folder_name)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m# Check if it's a directory\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/paper_slides_data/raw_data/dataset'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the paths\n",
    "presentation_dir = \"../data/paper_slides_data/sample_data/presentations\"\n",
    "paper_dir = \"../data/paper_slides_data/sample_data/papers\"\n",
    "\n",
    "sample_xml_pres_filename = \"slide.clean_tika.xml\"\n",
    "sample_xml_paper_filename = \"Paper_BRM.tei.xml\"\n",
    "\n",
    "# Join the paths\n",
    "sample_xml_pres_path = os.path.join(presentation_dir, sample_xml_pres_filename)\n",
    "sample_xml_paper_path = os.path.join(paper_dir, sample_xml_paper_filename)\n",
    "\n",
    "# Read files\n",
    "sample_xml_pres = utils.read_file(sample_xml_pres_path)\n",
    "sample_xml_paper = utils.read_file(sample_xml_paper_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvKeerpSdb4P",
    "outputId": "c5566859-b849-49c0-a03f-53012d93c89c",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.387989Z",
     "start_time": "2024-04-10T00:38:28.378404Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "sample_pres_text = utils.parse_presentation_xml(sample_xml_pres)\n",
    "print(len(sample_pres_text))\n",
    "print(sample_pres_text[:3])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "0ANc_UhWdb4P",
    "outputId": "8c40584a-da10-4fdc-d5f5-0e7a2d55c4cc",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.397030Z",
     "start_time": "2024-04-10T00:38:28.389096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "['Noam Nisan, Michael Schapira, Gregory Valiant, and Aviv Zohar', 'Motivation Equilibrium is the basic object of study in game theory. Question: How is an equilibrium reached? In a truly satisfactory answer each players rule of behavior is simple and locally rational repeated best-response repeated better-response regret-minimization', 'Motivation Repeated best-response is often employed in practice e.g., Internet routing We ask: When is such locallyrational behavior really rational?']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "sample_paper_text = utils.parse_paper_xml(sample_xml_paper)\n",
    "print(len(sample_paper_text))\n",
    "print(sample_paper_text[:3])"
   ],
   "metadata": {
    "id": "A2NuAWphdb4P",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.413436Z",
     "start_time": "2024-04-10T00:38:28.398220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "['The basic object of study in game theory and in economics is the equilibrium: a \"stable\" state from which none of the players wish to deviate.', 'Equilibrium is a static concept that often abstracts away the question of how it is reached.', 'Once we start looking at dynamics, or at algorithms for finding equilibria, we cannot escape questions of the form \"How is an equilibrium reached?\".']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "sample_paper_title = utils.parse_title(sample_xml_paper)\n",
    "print(sample_paper_title)\n",
    "sample_pres_title = utils.parse_title(sample_xml_pres)\n",
    "print(sample_pres_title)"
   ],
   "metadata": {
    "id": "q1RE2aBldb4P",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.418156Z",
     "start_time": "2024-04-10T00:38:28.414456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-Response Mechanisms\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "sample_pres_preprocessed = utils.preprocess_text(sample_pres_text)"
   ],
   "metadata": {
    "id": "7S4Gkn9Sdb4P",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.433630Z",
     "start_time": "2024-04-10T00:38:28.418668Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "sample_paper_preprocessed = utils.preprocess_text(sample_paper_text)"
   ],
   "metadata": {
    "id": "IfPoEVgfdb4P",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.485274Z",
     "start_time": "2024-04-10T00:38:28.434640Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "print(sample_pres_preprocessed[:3])\n",
    "print(sample_paper_preprocessed[:3])"
   ],
   "metadata": {
    "id": "qnzkDNmEdb4P",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.491659Z",
     "start_time": "2024-04-10T00:38:28.487341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['noam nisan  michael schapira  gregori valiant  aviv zohar'], ['motiv equilibrium basic object studi game theori ', 'question  equilibrium reach ', 'truli satisfactori answer player rule behavior simpl local ration repeat bestrespons repeat betterrespons regretminim'], ['motiv repeat bestrespons often employ practic eg  internet rout ask  locallyr behavior realli ration ']]\n",
      "[['basic object studi game theori econom equilibrium   stabl  state none player wish deviat '], ['equilibrium static concept often abstract away question reach '], ['start look dynam  algorithm find equilibria  escap question form  equilibrium reach ', ' ']]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_paper_preprocessed))\n",
    "print(len(sample_pres_preprocessed))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m papers_folder_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mgetcwd(), papers_folder)\n\u001B[1;32m     27\u001B[0m presentations_folder_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mgetcwd(), presentations_folder)\n\u001B[0;32m---> 29\u001B[0m combined_data \u001B[38;5;241m=\u001B[39m combine_data(papers_folder_path, presentations_folder_path)\n",
      "Cell \u001B[0;32mIn[22], line 14\u001B[0m, in \u001B[0;36mcombine_data\u001B[0;34m(papers_folder, presentations_folder)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcombine_data\u001B[39m(papers_folder, presentations_folder):\n\u001B[1;32m     13\u001B[0m     papers_data \u001B[38;5;241m=\u001B[39m process_folder(papers_folder, utils\u001B[38;5;241m.\u001B[39mparse_paper_xml, utils\u001B[38;5;241m.\u001B[39mpreprocess_text)\n\u001B[0;32m---> 14\u001B[0m     presentations_data \u001B[38;5;241m=\u001B[39m process_folder(presentations_folder, utils\u001B[38;5;241m.\u001B[39mparse_presentation_xml, utils\u001B[38;5;241m.\u001B[39mpreprocess_text)\n\u001B[1;32m     16\u001B[0m     combined_data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpapers\u001B[39m\u001B[38;5;124m\"\u001B[39m: papers_data,\n\u001B[1;32m     17\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresentations\u001B[39m\u001B[38;5;124m\"\u001B[39m: presentations_data}\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m combined_data\n",
      "Cell \u001B[0;32mIn[22], line 8\u001B[0m, in \u001B[0;36mprocess_folder\u001B[0;34m(folder_path, parse_func, preprocess_func)\u001B[0m\n\u001B[1;32m      6\u001B[0m         file_content \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m      7\u001B[0m         parsed_data \u001B[38;5;241m=\u001B[39m parse_func(file_content)\n\u001B[0;32m----> 8\u001B[0m         preprocessed_data \u001B[38;5;241m=\u001B[39m preprocess_func(parsed_data)\n\u001B[1;32m      9\u001B[0m         data_list\u001B[38;5;241m.\u001B[39mappend(preprocessed_data)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data_list\n",
      "File \u001B[0;32m~/Coding/Jupyter/NLP/final/notebooks/../utils.py:119\u001B[0m, in \u001B[0;36mpreprocess_text\u001B[0;34m(text_data)\u001B[0m\n\u001B[1;32m    117\u001B[0m     words \u001B[38;5;241m=\u001B[39m word_tokenize(sentence)\n\u001B[1;32m    118\u001B[0m     words \u001B[38;5;241m=\u001B[39m [clean_word(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words \u001B[38;5;28;01mif\u001B[39;00m clean_word(word) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[0;32m--> 119\u001B[0m     words \u001B[38;5;241m=\u001B[39m [stemmer\u001B[38;5;241m.\u001B[39mstem(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words]\n\u001B[1;32m    120\u001B[0m     preprocessed_sentences\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(words))\n\u001B[1;32m    121\u001B[0m preprocessed_text\u001B[38;5;241m.\u001B[39mappend(preprocessed_sentences)\n",
      "File \u001B[0;32m~/Coding/Jupyter/NLP/final/notebooks/../utils.py:119\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    117\u001B[0m     words \u001B[38;5;241m=\u001B[39m word_tokenize(sentence)\n\u001B[1;32m    118\u001B[0m     words \u001B[38;5;241m=\u001B[39m [clean_word(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words \u001B[38;5;28;01mif\u001B[39;00m clean_word(word) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[0;32m--> 119\u001B[0m     words \u001B[38;5;241m=\u001B[39m [stemmer\u001B[38;5;241m.\u001B[39mstem(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words]\n\u001B[1;32m    120\u001B[0m     preprocessed_sentences\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(words))\n\u001B[1;32m    121\u001B[0m preprocessed_text\u001B[38;5;241m.\u001B[39mappend(preprocessed_sentences)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/stem/porter.py:670\u001B[0m, in \u001B[0;36mPorterStemmer.stem\u001B[0;34m(self, word, to_lowercase)\u001B[0m\n\u001B[1;32m    667\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stem\n\u001B[1;32m    669\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step1a(stem)\n\u001B[0;32m--> 670\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step1b(stem)\n\u001B[1;32m    671\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step1c(stem)\n\u001B[1;32m    672\u001B[0m stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step2(stem)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/stem/porter.py:365\u001B[0m, in \u001B[0;36mPorterStemmer._step1b\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m rule_2_or_3_succeeded:\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m word\n\u001B[0;32m--> 365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_rule_list(\n\u001B[1;32m    366\u001B[0m     intermediate_stem,\n\u001B[1;32m    367\u001B[0m     [\n\u001B[1;32m    368\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mat\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mate\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),  \u001B[38;5;66;03m# AT -> ATE\u001B[39;00m\n\u001B[1;32m    369\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mble\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),  \u001B[38;5;66;03m# BL -> BLE\u001B[39;00m\n\u001B[1;32m    370\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miz\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mize\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),  \u001B[38;5;66;03m# IZ -> IZE\u001B[39;00m\n\u001B[1;32m    371\u001B[0m         \u001B[38;5;66;03m# (*d and not (*L or *S or *Z))\u001B[39;00m\n\u001B[1;32m    372\u001B[0m         \u001B[38;5;66;03m# -> single letter\u001B[39;00m\n\u001B[1;32m    373\u001B[0m         (\n\u001B[1;32m    374\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*d\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    375\u001B[0m             intermediate_stem[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m    376\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m stem: intermediate_stem[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ml\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mz\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    377\u001B[0m         ),\n\u001B[1;32m    378\u001B[0m         \u001B[38;5;66;03m# (m=1 and *o) -> E\u001B[39;00m\n\u001B[1;32m    379\u001B[0m         (\n\u001B[1;32m    380\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    382\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m stem: (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_measure(stem) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ends_cvc(stem)),\n\u001B[1;32m    383\u001B[0m         ),\n\u001B[1;32m    384\u001B[0m     ],\n\u001B[1;32m    385\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/stem/porter.py:266\u001B[0m, in \u001B[0;36mPorterStemmer._apply_rule_list\u001B[0;34m(self, word, rules)\u001B[0m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m         \u001B[38;5;66;03m# Don't try any further rules\u001B[39;00m\n\u001B[1;32m    265\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m word\n\u001B[0;32m--> 266\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m word\u001B[38;5;241m.\u001B[39mendswith(suffix):\n\u001B[1;32m    267\u001B[0m     stem \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replace_suffix(word, suffix, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m condition \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m condition(stem):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def process_folder(folder_path, parse_func, preprocess_func):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            parsed_data = parse_func(file_content)\n",
    "            preprocessed_data = preprocess_func(parsed_data)\n",
    "            data_list.append(preprocessed_data)\n",
    "    return data_list\n",
    "\n",
    "def combine_data(papers_folder, presentations_folder):\n",
    "    papers_data = process_folder(papers_folder, utils.parse_paper_xml, utils.preprocess_text)\n",
    "    presentations_data = process_folder(presentations_folder, utils.parse_presentation_xml, utils.preprocess_text)\n",
    "\n",
    "    combined_data = {\"papers\": papers_data,\n",
    "                     \"presentations\": presentations_data}\n",
    "    return combined_data\n",
    "\n",
    "# Example usage:\n",
    "# Define the folders relative to the current directory\n",
    "papers_folder = \"../data/paper_slides_data/raw_data/papers\"\n",
    "presentations_folder = \"../data/paper_slides_data/raw_data/presentations\"\n",
    "\n",
    "# Join the paths\n",
    "papers_folder_path = os.path.join(os.getcwd(), papers_folder)\n",
    "presentations_folder_path = os.path.join(os.getcwd(), presentations_folder)\n",
    "\n",
    "combined_data = combine_data(papers_folder_path, presentations_folder_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Zip presentations and papers together\n",
    "zipped_data = zip(combined_data[\"presentations\"][:3], combined_data[\"papers\"][:3])\n",
    "\n",
    "# Print the zipped data\n",
    "for i, (presentation, paper) in enumerate(zipped_data, start=1):\n",
    "    print(\"preso sentences\", len(presentation))\n",
    "    print(f\"Pair {i}: \\n Presentation - {presentation[:10]}\")\n",
    "    print(\"\")\n",
    "    print(\"paper sentences\", len(paper))\n",
    "    print(f\"Paper - {paper[:10]}\")\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def find_most_similar_sentence(query_sentence, sentences):\n",
    "    # Combine query sentence with the list of sentences\n",
    "    all_sentences = [query_sentence] + sentences\n",
    "\n",
    "    # Initialize TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Compute TF-IDF vectors for all sentences\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "\n",
    "    # Calculate cosine similarity between query sentence and all sentences\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "\n",
    "    # Find the index of the most similar sentence\n",
    "    most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "    # Return the most similar sentence and its similarity score\n",
    "    most_similar_sentence = sentences[most_similar_index]\n",
    "    similarity_score = similarity_scores[most_similar_index]\n",
    "\n",
    "    return most_similar_sentence, similarity_score\n",
    "\n",
    "presentation_paper_pairs = []\n",
    "for presentation, paper in zip(combined_data[\"presentations\"], combined_data[\"papers\"]):\n",
    "    presentation_flat = [sentence for sublist in presentation for sentence in sublist]\n",
    "    paper_flat = [sentence for sublist in paper for sentence in sublist]\n",
    "    presentation_sentence_pairs = []\n",
    "    for sentence in presentation_flat:\n",
    "        most_similar_sentence, similarity_score = find_most_similar_sentence(sentence, paper_flat)\n",
    "        presentation_sentence_pairs.append([sentence, most_similar_sentence, similarity_score])\n",
    "        sorted_presentation_sentence_pairs = sorted(presentation_sentence_pairs, key=lambda x: x[2], reverse=True)\n",
    "    presentation_paper_pairs.append(sorted_presentation_sentence_pairs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(presentation_paper_pairs))\n",
    "print(presentation_paper_pairs[:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "average_similarity = []\n",
    "for sentence_pairs in presentation_paper_pairs:\n",
    "    similarity_score_sum = sum(pair[2] for pair in sentence_pairs)\n",
    "    similarity_score_average = similarity_score_sum / len(sentence_pairs)\n",
    "    average_similarity.append(similarity_score_average)\n",
    "\n",
    "print(average_similarity[:5])\n",
    "\n",
    "print(sum(average for average in average_similarity) / len(average_similarity))\n"
   ],
   "metadata": {
    "id": "j8AdV2bqdb4Q",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.710189Z",
     "start_time": "2024-04-10T00:38:28.611339Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "presentation_sentences_list = []\n",
    "paper_sentences_list = []\n",
    "\n",
    "for pair in presentation_paper_pairs:\n",
    "    presentation_sentences = [sentences[0] for sentences in pair]\n",
    "    paper_sentences = [sublist[1] for sublist in pair]\n",
    "    presentation_sentences_list.append(presentation_sentences)\n",
    "    paper_sentences_list.append(paper_sentences)\n",
    "\n",
    "# Print the separated lists for each pair\n",
    "for i in range(3):\n",
    "    print(\"Presentation sentences for pair\", i+1, \":\", presentation_sentences_list[i])\n",
    "    print()\n",
    "    print(\"Paper sentences for pair\", i+1, \":\", paper_sentences_list[i])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(presentation_sentences_list[5]))\n",
    "print(len(presentation_sentences_list[5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model(\"best_model.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(presentation_sentences_list[0])\n",
    "print(paper_sentences_list[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: tokenize and vectorize input data\n",
    "import pickle\n",
    "\n",
    "models_dir = '../models/'\n",
    "\n",
    "# Load the tokenizer object\n",
    "with open(os.path.join(models_dir, 'tokenizer.pkl'), 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize inference sentences using the loaded tokenizer\n",
    "inference_premise_sequences = tokenizer.texts_to_sequences(presentation_sentences_list[0])\n",
    "inference_hypothesis_sequences = tokenizer.texts_to_sequences(paper_sentences_list[0])\n",
    "\n",
    "# Pad the sequences to the same maximum sequence length\n",
    "inference_premise_sequences = pad_sequences(inference_premise_sequences, maxlen=45, padding='post')\n",
    "inference_hypothesis_sequences = pad_sequences(inference_hypothesis_sequences, maxlen=45, padding='post')\n",
    "\n",
    "# Print the size of inference sequences\n",
    "print(\"Size of Premise Inference Sequences:\", len(inference_premise_sequences))\n",
    "print(\"Size of Hypothesis Inference Sequences:\", len(inference_hypothesis_sequences))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probabilities = model.predict([inference_premise_sequences, inference_hypothesis_sequences])\n",
    "print(probabilities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(probabilities)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "predicted_classes = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Step 3: Class Labels\n",
    "class_labels = [\"Entailment\", \"Neutral\", \"Contradictory\"]  # Replace with your actual class labels\n",
    "predicted_labels = [class_labels[idx] for idx in predicted_classes]\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ],
   "metadata": {
    "id": "f6Ik2bemdb4Q",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.715504Z",
     "start_time": "2024-04-10T00:38:28.711288Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = {label: predicted_labels.count(label) for label in set(predicted_labels)}\n",
    "\n",
    "# Calculate the total count of all labels\n",
    "total_count = sum(label_counts.values())\n",
    "\n",
    "# Calculate the proportion of each label\n",
    "label_proportions = {label: count / total_count for label, count in label_counts.items()}\n",
    "\n",
    "# Define weights for each label\n",
    "label_weights = {\n",
    "    'Contradictory': 0,\n",
    "    'Neutral': 0.5,\n",
    "    'Entailment': 1\n",
    "}\n",
    "\n",
    "# Calculate the weighted sum of counts for all labels\n",
    "weighted_sum = sum(label_counts[label] * label_weights[label] for label in label_counts)\n",
    "\n",
    "# Normalize the weighted sum to range from 0 to 1\n",
    "normalized_weighted_sum = weighted_sum / (total_count * max(label_weights.values()))\n",
    "\n",
    "# Print the normalized weighted sum\n",
    "print(\"Normalized Weighted Sum of Label Counts:\", normalized_weighted_sum)\n",
    "\n",
    "# Print the label proportions\n",
    "print(\"Label Proportions:\", label_proportions)\n"
   ],
   "metadata": {
    "id": "BVLIYEZ7db4Q",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.822426Z",
     "start_time": "2024-04-10T00:38:28.734952Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the thresholds\n",
    "thresholds = {\n",
    "    'BAD': 0.33,\n",
    "    'GOOD': 0.66,\n",
    "    'GREAT': 1.0\n",
    "}\n",
    "\n",
    "# Determine the category based on the normalized weighted sum\n",
    "category = None\n",
    "for label, threshold in thresholds.items():\n",
    "    if normalized_weighted_sum <= threshold:\n",
    "        category = label\n",
    "        break\n",
    "\n",
    "# Print the category\n",
    "print(\"This presentation was a\", category, \"representation of this paper.\")\n"
   ],
   "metadata": {
    "id": "-p0GWz-uBhbU",
    "ExecuteTime": {
     "end_time": "2024-04-10T00:38:28.826097Z",
     "start_time": "2024-04-10T00:38:28.823430Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
