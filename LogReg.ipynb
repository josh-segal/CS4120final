{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuasegal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-04-15 12:45:09.641080: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-15 12:45:09.641965: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-15 12:45:09.642520: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-15 12:45:09.723538: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-15 12:45:09.724488: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-15 12:45:09.725133: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, cross_validate\n",
    "import utils as utils\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('text_entailment_dataset/train.csv')\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=0.1, random_state=42)  # Shuffle with fixed seed for reproducibility\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, validation_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Write the training and validation DataFrames to separate CSV files\n",
    "train_df.to_csv('text_entailment_dataset/train_data.csv', index=False)\n",
    "validation_df.to_csv('text_entailment_dataset/validation_data.csv', index=False)\n",
    "\n",
    "\n",
    "train_dataset = df = pd.read_csv('text_entailment_dataset/train_data.csv')\n",
    "validation_dataset = df = pd.read_csv('text_entailment_dataset/validation_data.csv')\n",
    "test_dataset = df = pd.read_csv('text_entailment_dataset/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                             premise  \\\n0  This church choir sings to the masses as they ...   \n1  This church choir sings to the masses as they ...   \n2  This church choir sings to the masses as they ...   \n3  A woman with a green headscarf, blue shirt and...   \n4  A woman with a green headscarf, blue shirt and...   \n\n                              hypothesis  \n0  The church has cracks in the ceiling.  \n1        The church is filled with song.  \n2    A choir singing at a baseball game.  \n3                    The woman is young.  \n4               The woman is very happy.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>The church has cracks in the ceiling.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>The church is filled with song.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This church choir sings to the masses as they ...</td>\n      <td>A choir singing at a baseball game.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is young.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A woman with a green headscarf, blue shirt and...</td>\n      <td>The woman is very happy.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(9824, 2)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[[\"premise\"]] = train_dataset[[\"premise\"]].astype(str)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.change_lower)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.clean_data)\n",
    "train_dataset[\"premise\"] = train_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "train_dataset[[\"hypothesis\"]] = train_dataset[[\"hypothesis\"]].astype(str)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "train_dataset[\"hypothesis\"] = train_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"premise\"]] = validation_dataset[[\"premise\"]].astype(str)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.change_lower)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.clean_data)\n",
    "validation_dataset[\"premise\"] = validation_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "validation_dataset[[\"hypothesis\"]] = validation_dataset[[\"hypothesis\"]].astype(str)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "validation_dataset[\"hypothesis\"] = validation_dataset[\"hypothesis\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"premise\"]] = test_dataset[[\"premise\"]].astype(str)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.change_lower)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.clean_data)\n",
    "test_dataset[\"premise\"] = test_dataset[\"premise\"].apply(utils.remover)\n",
    "\n",
    "test_dataset[[\"hypothesis\"]] = test_dataset[[\"hypothesis\"]].astype(str)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.change_lower)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.clean_data)\n",
    "test_dataset[\"hypothesis\"] = test_dataset[\"hypothesis\"].apply(utils.remover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = [train_dataset[\"premise\"][i] + \" \" + train_dataset[\"hypothesis\"][i] for i in range(len(train_dataset.index))]\n",
    "validation_pairs = [validation_dataset[\"premise\"][i] + \" \" + validation_dataset[\"hypothesis\"][i] for i in range(len(validation_dataset.index))]\n",
    "test_pairs = [test_dataset[\"premise\"][i] + \" \" + test_dataset[\"hypothesis\"][i] for i in range(len(test_dataset.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man bright orange shirt scales slate colored rock wall face man wears blue shirt\n"
     ]
    }
   ],
   "source": [
    "print(train_pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARIZED = False\n",
    "\n",
    "vectorizer = CountVectorizer(binary=BINARIZED)\n",
    "\n",
    "train_vectorized = vectorizer.fit_transform(train_pairs)\n",
    "\n",
    "validation_vectorized = vectorizer.transform(validation_pairs)\n",
    "\n",
    "test_vectorized = vectorizer.transform(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 15825\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vocabulary:\", len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_vectorized = np.array([row for row in train_vectorized] + [row for row in validation_vectorized])\n",
    "\n",
    "labels = np.array(train_dataset[\"label\"].to_list() + validation_dataset[\"label\"].to_list())\n",
    "\n",
    "split_groups = [0 for i in range(train_vectorized.shape[0])] + [1 for i in range(validation_vectorized.shape[0])]\n",
    "\n",
    "split = PredefinedSplit(split_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTypeError: float() argument must be a string or a real number, not 'csr_matrix'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1801, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m classifier \u001B[38;5;241m=\u001B[39m LogisticRegressionCV(max_iter\u001B[38;5;241m=\u001B[39mMAX_ITER, cv\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m      4\u001B[0m scoring_metrics \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecision_micro\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecall_micro\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1_micro\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mneg_log_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m results \u001B[38;5;241m=\u001B[39m cross_validate(classifier, train_validation_vectorized, labels, scoring\u001B[38;5;241m=\u001B[39mscoring_metrics, cv\u001B[38;5;241m=\u001B[39msplit)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    209\u001B[0m         )\n\u001B[1;32m    210\u001B[0m     ):\n\u001B[0;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    221\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:328\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    308\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m    309\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    310\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    311\u001B[0m         clone(estimator),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[1;32m    326\u001B[0m )\n\u001B[0;32m--> 328\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(scoring):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:414\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[0;34m(results, error_score)\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[1;32m    408\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    409\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    412\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    413\u001B[0m     )\n\u001B[0;32m--> 414\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    417\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: \nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTypeError: float() argument must be a string or a real number, not 'csr_matrix'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1801, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 917, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/joshuasegal/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 380, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence.\n"
     ]
    }
   ],
   "source": [
    "MAX_ITER = 10\n",
    "classifier = LogisticRegressionCV(max_iter=MAX_ITER, cv=split)\n",
    "\n",
    "scoring_metrics = (\"precision_micro\", \"recall_micro\", \"f1_micro\", \"accuracy\", \"neg_log_loss\")\n",
    "\n",
    "results = cross_validate(classifier, train_validation_vectorized, labels, scoring=scoring_metrics, cv=split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
