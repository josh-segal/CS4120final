<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generic Soft Pattern Models for Definitional Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Cui</surname></persName>
							<email>cuihang@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore Tat-Seng Chua</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School of Computing</orgName>
								<orgName type="institution">National University of Singapore Tat-Seng Chua</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generic Soft Pattern Models for Definitional Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I27 [Artificial Intelligence]: Natural Language Processing</term>
					<term>H33 [Information Storage and Retrieval]: Information Search and Retrieval -Retrieval Models</term>
					<term>General Terms Algorithms, Measurement, Experimentation Keywords Definitional Question Answering, Soft Pattern, Probabilistic Models</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper explores probabilistic lexico-syntactic pattern matching, also known as soft pattern matching. While previous methods in soft pattern matching are ad hoc in computing the degree of match, we propose two formal matching models: one based on bigrams and the other on the Profile Hidden Markov Model (PHMM). Both models provide a theoretically sound method to model pattern matching as a probabilistic process that generates token sequences. We demonstrate the effectiveness of these models on definition sentence retrieval for definitional question answering. We show that both models significantly outperform state-of-the-art manually constructed patterns. A critical difference between the two models is that the PHMM technique handles language variations more effectively but requires more training data to converge. We believe that both models can be extended to other areas where lexico-syntactic pattern matching can be applied.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Natural language texts often exhibit patterns, and thus lexicosyntactic patterns are pervasive in natural language retrieval and extraction tasks, such as information extraction (e.g., <ref type="bibr" target="#b10">[11]</ref>). An example of such a pattern is "&lt;PersonIN&gt; , NNP, BE$ named to POST$" 1 , which may be used to extract the person name Bob Lloyd from the sentence "…&lt;PersonIN&gt; Bob Lloyd &lt;/PersonIN&gt;, president and chief operating officer, was named to the &lt;POST&gt; chief executive &lt;/POST&gt;". Besides information extraction, such patterns have been applied to areas including:</p><p>1. Question answering (QA): Pattern matching is utilized to improve precision in both factoid QA <ref type="bibr" target="#b11">[12]</ref> and definitional QA <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1]</ref>. The former learns surface text patterns to extract exact answers for simple questions about facts while the latter utilizes more complicated definition patterns to identify definition sentences to define a topic.</p><p>2. Retrieval of subjective expressions: Riloff and Wiebe <ref type="bibr" target="#b12">[13]</ref> applied an IE system to learn patterns of subjective expressions so that opinions can be identified from news articles.</p><p>Lexico-syntactic patterns, which are either manually constructed or machine learned, are often represented and matched as regular expressions. They perform slot by slot matching against test sentences, which we call hard matching. While these patterns are highly precise, they often fare poorly in recall because of language variations. For instance, the sample pattern given earlier cannot match the sentence: which can be reduced to:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;PersonIN&gt; , NUM$ NN ADJ, NNP, NNP, BE$ named to &lt;POST&gt;</head><p>The pattern fails to match the sentence due to additional tokens (underscored in the above) that are not found in training samples. Such mismatches are common in natural language texts because authors can use diverse expressions to convey the same meaning. We conjecture that current pattern matching applications may be hindered due to the rigidity of hard matching. One promising technique to circumvent this is soft pattern matching. Previously examined by Cui et al. <ref type="bibr" target="#b1">[2]</ref>, soft patterns (SP) have shown to significantly outperform hard patterns in extracting definition sentences as they model language variations probabilistically.</p><p>While that work has demonstrated the performance of soft patterns, it has not been anchored in a theoretically sound framework.</p><p>In this paper, we build upon the earlier work in soft pattern matching. Different from previous empirical work, we show how soft pattern matching is achieved within the framework of two standard probabilistic models. We take both patterns and test instances as sequences of lexical and syntactic tokens. Here, pattern matching can be considered probabilistic generation of test sequences based on training sequences. The first model is derived from a bigram language model with linear interpolation 1 NNP is a POS tag that represents a noun phrase. BE$ stands for all "be" <ref type="bibr" target="#b6">[7]</ref> of unigram and bigram probabilities. The second model is based on the Profile Hidden Markov Model (PHMM). Parameters in both models are estimated using Expectation Maximization (EM). While the language model and the PHMM have been studied in other areas, their use in modeling lexico-syntactic pattern matching is a novel contribution of our work.</p><p>We choose the task of definition sentence retrieval within a typical definitional QA system to demonstrate the models' effectiveness.</p><p>The reason is two-fold: (1) Definition sentences are diversified in exhibited patterns <ref type="bibr" target="#b1">[2]</ref>, and thus require flexible pattern matching to achieve high recall; and (2) definitional QA remains one of the least explored areas in QA research. To answer definition questions, such as "Who is Aaron Copland" or "What are prions", a system is expected to generate a summary of all pieces of salient information (or nuggets) about the given target. To generate definitions for a target, a typical definitional QA system first retrieves linguistic constructs (e.g., sentences, appositives or relative clauses) that might contain salient information about the target, and then summarizes these units into readable definitions. To identify such constructs, current systems use definition patterns to recognize definitions. Although a number of other techniques influence the retrieval of such definition units, component evaluations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> suggest that definition patterns are the most important feature.</p><p>In addition to our theoretical work, we also assess the performance of the formal soft matching models by empirical evaluation. We conduct a series of extrinsic experiments using the two soft pattern models on TREC definitional QA task test data. Our experiments show that the performance of both models significantly outperform state-of-the-art manually constructed hard matching patterns by 11.67% and 9.18% in automatic ROUGE score, and by 9.83% and 7.30% in the manual F 3 measure used by TREC, respectively. Moreover, the two models achieve better performance compared to the original soft matching method proposed in Cui et al. <ref type="bibr" target="#b1">[2]</ref>. The evaluation results also reveal that the PHMM model is more tolerant to variations in model length but requires more training data for it to converge.</p><p>In Section 2, we present a typical definitional QA system with which our experiments are performed, and we give the necessary background for understanding the proposed models. In Section 3, we discuss the two proposed generic soft matching models and our adaptation in detail. We present the evaluation results in Section 4 and conclude the paper with future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SYSTEM ARCHITECTURE AND BACKGROUND</head><p>The recent development of definitional QA has been boosted by the Text REtrieval Conference (TREC). TREC-12 and TREC-13 conducted systematic evaluations for definition questions. We use a standard definitional QA system that structurally conforms to those top performing TREC systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Its architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Retrieval:</head><p>Given a definition question such as "Who is X" or "What is X", the search target, i.e., "X" in the question, is fed as query into a standard document retrieval system. The retrieved documents are split into sentences. An instance is further split into left and right sequences based on the position of &lt;SCH_TERM&gt;. We model the left and right sequences separately using soft pattern models. In the above example, "DT$ NN" is the left sequence and "BE$ owned by" is the right sequence.</p><p>Definition Sentence Retrieval: The definition sentence retrieval module identifies and extracts definition sentences from the relevant document set. Systems fielded at TREC rank definition sentences using two sets of features: definition patterns and bagof-words pertinent to the target. Definition pattern matching is the most important feature used for identifying definitions. Xu et al.</p><p>[19] and Hildebrandt et al.</p><p>[6] employed various manually constructed definition patterns at both lexical and syntactic levels to identify specific linguistic constructs and assign different weights to the types of patterns. Other TREC systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> also employ various manual definition patterns but they treat all patterns equally. All these systems use hard pattern matching. In contrast, soft matching patterns compute a probabilistic match for a test instance by combining individual slot and sequence probabilities. Cui et al. <ref type="bibr" target="#b1">[2]</ref> experimentally demonstrated that soft pattern matching outperforms hard pattern matching in the definition sentence retrieval task. In this paper, we also adopt the soft pattern matching approach, but we augment the previous model with a rigorous mathematical foundation for soft pattern matching.</p><p>In addition to pattern matching, definitional QA systems also use the bag-of-words approach to rank extracted constructs. BlairGoldensohn et al. <ref type="bibr" target="#b0">[1]</ref> and Xu et al. <ref type="bibr" target="#b18">[19]</ref> constructed a profile for the target by selecting centroid words, i.e., words that frequently co-occur with the target. The profile is then used to rank definition sentences by their cosine similarity with the profile. Many systems also reinforce such profiles by exploiting word statistics from external resources, such as biographical web sites and encyclopedias. We follow the literature in constructing our system, by adopting the centroid method reinforced by existing definitions from biography.com and wikipedia.com. To retrieve definition sentences, we linearly combine the scores of pattern matching and bag-of-words ranking in the experiments <ref type="bibr" target="#b2">[3]</ref>.</p><p>Redundancy Removal: The redundancy removal module takes a list of ranked definition sentences as input. It produces the final answer by removing redundant sentences. A sentence is removed if its cosine similarity with any sentence already selected for the answer exceeds a predefined threshold <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Soft Pattern Matching</head><p>After preprocessing, each definition sentence is converted into a pattern instance that consists of left and right token sequences relative to the search target. At the training phase, token sequences are aligned and represented as a single vector P: &lt;S 1 ,S 2 , …, S l &gt; that combines information over all of the training sequences, where S i represents the i th slot (or position) left or right of the search target and contains tokens appearing in that position. Unlike hard matching patterns that generalize training instances into pattern rules, soft matching patterns model each token's distribution statistics in each slot over all training instances. Given a test sequence T with length l:&lt;t 1 , t 2 …… t l &gt; where t i is the token corresponding to the i th slot, soft pattern models are used to model the generative probability of sequence T, given training sequences represented in vector P. In the next two sub-sections, we briefly review the background to our new soft pattern models before proposing the models in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Language Modeling</head><p>Language modeling has been extensively studied in speech recognition, part-of-speech tagging and syntactic parsing <ref type="bibr" target="#b13">[14]</ref>. Ngram language modeling is one important approach which models local sequential dependencies between adjacent tokens. Trigrams (n=3) are a common choice when large training corpora are available. We use a bigram (n=2) model in this paper, as we have a limited amount of training data. We also remedy problems with sparse data by smoothing n-gram probabilities. The first soft matching pattern model we introduce is based on n-gram language models, in which we incorporate linear interpolation <ref type="bibr" target="#b6">[7]</ref> of unigrams and bigrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Hidden Markov Models in Information Extraction and Biological Sequence Modeling</head><p>Hidden Markov Models (HMMs) have been widely applied to speech recognition and various natural language processing applications <ref type="bibr" target="#b9">[10]</ref>, including information extraction (IE) and biological sequence modeling, which are most relevant to definition pattern matching.</p><p>IE relies heavily on patterns at the lexical, syntactic and semantic levels. Two types of extraction patterns are exploited in current IE systems: machine-induced hard matching rules <ref type="bibr" target="#b10">[11]</ref> and probabilistic models such as HMMs. Skounakis et al. <ref type="bibr" target="#b14">[15]</ref> applied hierarchical HMMs to the task of extracting binary relations in biomedical texts. They constructed two types of HMMs to represent words and phrases, which are two levels of emission units. Although these variations of HMMs also model pattern matching as token sequence generation, the topology they employ are more task specific and not general enough to be extended to other applications such as definition pattern matching.</p><p>HMMs recently have also been applied in computational biology to model protein families. Krogh et al. <ref type="bibr" target="#b7">[8]</ref> utilized an HMM with a generic topology, called the Profile HMM (PHMM) (see <ref type="figure" target="#fig_2">Figure  2</ref>), to model multiple sequence alignment of protein families. PHMMs can be considered a probabilistic implementation of edit distance. It has different states for match, insertion and deletion operations. As it demonstrates flexible matching of lexicosyntactic patterns, we can easily adapt PHMMs for soft pattern matching by changing "amino acids" to "words and syntactic tags". The PHMM approach is our second model proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GENERIC SOFT PATTERN MATCHING MODELS</head><p>In this section, we describe the derivation of our two soft pattern models and parameter estimation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bigram Soft Pattern Model</head><p>We first adopt a bigram model to model pattern instances. While the original bigram model is simply a product of probabilities of all bigrams in a sequence, we apply linear interpolation of unigrams and bigrams to represent probability of bigrams. The reason is two-fold: (1) to smooth probability distribution to generate more accurate statistics for unseen data, and (2) to incorporate conditional probability of individual tokens appearing in specific slots. In particular, we model a sequence of pattern tokens as:</p><formula xml:id="formula_0">∏ − + = ∏ − + = = − = − L i i i i i L i i i i L S t P t t P S t P t P t t P t P t t P 2 1 1 1 2 1 1 1 )) | ( ) 1 ( ) | ( ( ) | ( )) , ( ) 1 ( ) , | ( ( ) | ( ) ( λ λ µ λ µ λ µ K (1)</formula><p>where µ stands for the bigram model and P(t i |S i ) stands for the conditional probability of token t i appearing in slot S i . λ is the mixture weight combining the unigram and bigram probabilities. Note that we use the conditional probability of a unigram being in a slot to represent unigram probability. This is because the position of a token is important in modeling: for instance, a comma always appears in the first slot right of the target in an appositive expression. Incorporating individual slots' probabilities enables the bigram model to allow partial matching, which is a characteristic of soft pattern matching. In other words, even if some slots cannot be matched, the bigram model can still yield a high match score by combining those matched slots' unigram probabilities.</p><p>As test instances are often different in length, we normalize the log-likelihood of Equation <ref type="formula" target="#formula_4">(1)</ref> by the length l of the test instance:</p><formula xml:id="formula_1">)) ) | ( ) 1 ( ) | ( log( ) | ( (log 1 2 1 1 1 ∑ = − − + + = l i i i i i norm S t P t t P S t P l P λ λ (2)</formula><p>where l denotes the number of tokens in the test instance.</p><p>Next, we estimate unigram and bigram probabilities by their maximum likelihood (ML) estimates:</p><formula xml:id="formula_2">∑ = k i k i i ML i i S t S t S t P | ) ( | | ) ( | ) | ( (3) | ) ( | | ) ( ) ( | ) | ( 1 1 1 i i i i i i ML i i S t S t S t t t P − − − = (4)</formula><p>where t i (S i ) denotes that token t i appears in slot S i and |t| denotes the frequency of the token t. In language modeling, ML estimates often suffer from the sparse data problem. It is even worse in our scenario because we count tokens with respect to slot positions, which makes the training data even sparser. As such, we need to employ some smoothing technique to counter the problem. For simplicity, we use Laplace smoothing on unigram probabilities (recall that bigram probabilities have already been smoothed by interpolation):</p><formula xml:id="formula_3">∑ × + + = k j k i i i i t N S t S t S t P | ) ( | | ) ( | | ) ( | ) | ( δ δ (5)</formula><p>where |N(t)| gives the total number of unique tokens in our training data and δ is a constant, which is 2 in our experiments.</p><p>Note that we count frequencies of words and general syntactic tags separately. General tags typically have a much higher frequency compared to individual words, and would thus skew the distribution if combined with words. Thus we need to separate the two types and estimate each token's unigram probability against its own set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating the mixture weight λ: We use the Expectation Maximization (EM) algorithm [4] to find optimal settings of λ.</head><p>Specifically, we estimate λ by maximizing the likelihood of all training instances, given the bigram model: </p><formula xml:id="formula_4">)) | ( ) 1 ( ) | ( ( log 1 1 max arg ) | ( max arg ) ( ) ( )<label>( 1 )</label></formula><p>P(t 1 |S 1 ) is ignored because it does not affect λ. λ can be estimated using the EM iterative procedure:</p><p>1. Initialize λ to a random estimate between 0 and 1, say 0.5. 2. Update λ using:</p><formula xml:id="formula_6">∑ ∑ − + − × = = = − − ) ( 1 2 ) ( ) ( )<label>( 1 ) ( ) ( 1 ) (</label></formula><formula xml:id="formula_7">) | ( ) 1 ( ) | ( ) | ( 1 1 | | 1 ' INS f j j l i j i j i j i j i j i j i j S t P t t P t t P l INS λ λ λ λ (7)</formula><p>where INS denotes all training instances and |INS| is the number of training instances which is used as a normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Repeat</head><p>Step 2 until λ converges.</p><p>We set λ to 0.3 according to the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PHMM Soft Pattern Model</head><p>Although the bigram model allows partial matching, it lacks the ability to deal with gaps in test instances. For instance, given training instances such as "&lt;SCH_TERM&gt; which is known for …", the trained bigram model cannot give reasonable match scores to test instances such as "&lt;SCH_TERM&gt; which is best known for …" or "&lt;SCH_TERM&gt; , whose xxx is known for …" even though they are simple variants of the training instances in which insertions or deletions occur. The gaps can be captured by PHMMs, which allow insertion and deletion operations in the matching process. <ref type="figure" target="#fig_2">Figure 2</ref> shows the topology of a PHMM.  The PHMM contains a sequence of match states, which are denoted by M i (i=1..L). These match states correspond to slots in pattern instances and determine model length L. Each match state can emit a token t from all tokens in the training instances with the emission probability P(t|M i ). For each match state, there is a deletion state, denoted by D i , which does not emit a token and is used to skip the corresponding match state. Insertion states emit a token t with the emission probability P(t|I i ). Insertion states insert tokens after match or deletion states, as with the word "best" in the earlier example. While transitions from match states and deletion states always move forward in the model, insertion states allow self-loops, corresponding to multiple insertions. A token sequence representing a pattern instance can be generated by moving through this model with state transition probabilities P(S i |S j ). The deletion and insertion states allow the PHMM to model missing or unobserved words in training. Specifically, the probability of a sequence of tokens t 1 … t N that are generated by moving through the states S 0 … S L+1 (the Start and End states are S 0 and S L+1 ) is as follows:</p><formula xml:id="formula_8">∏ = = − + + L i i i i i n L L L N S S T S t P S S T S S t t ob 1 1 ) ( 1 1 0 1 ) | ( ) | ( ) | ( ) , | ( Pr µ K K (8)</formula><p>where µ stands for the model. P(t n(i) |S i ) is set to 1 when S i is a deletion state. To recognize a definition pattern, we choose the most probable state path in the above equation to approximate the probability of the sequence being given all possible state paths. The rationale is that most often, the most probable state path gives a much higher probability than any other paths. Equation (8) can be efficiently calculated by the forward-backward algorithm <ref type="bibr" target="#b9">[10]</ref>. We employ the Viterbi algorithm <ref type="bibr" target="#b9">[10]</ref> to find the most probable state path. In <ref type="figure" target="#fig_3">Figure 3</ref>, we show an example to illustrate how the PHMM finds the optimal path to account for the "gaps" between training instances and the test instance. Although the training data does not contain any instance that has "known" in Slot 1 and "NNP" in Slot 4, the PHMM automatically selects the path that goes through a deletion state to skip Slot 1 and uses an insertion state to emit "NNP". Thus, the tokens are re-aligned with their most probable occurring slots such that the unseen test instance can still obtain a reasonable generative probability. Estimation of the model: During training, we need to estimate transition and emission probabilities for the PHMM. The training process, also called the estimation process, can be accomplished by employing the standard Baum-Welch algorithm <ref type="bibr" target="#b9">[10]</ref>. Corresponding to our adaptation to the calculation of sequence probability, we use the Viterbi algorithm to determine the path with the highest probability during the re-estimation process, unlike the standard Baum-Welch algorithm which considers all possible paths which are weighted by their probabilities.  Initialization of the model: Although probabilities in a PHMM can be estimated automatically using an iterative EM algorithm starting with random or uniform probabilities, the re-estimation process can only guarantee that the model reaches local maxima. In addition, in capturing definition patterns, definition expressions are diverse and sparse in terms of both lexical tokens and POS tags. If we start with random or uniform setting of the model, we would likely end up with an unsatisfactory model that gives close estimates of different possibilities. To make training manageable given our small training set, we assume that the most probable state path for a sequence should go through as many match states as possible. The reason is that although insertion and deletion states add flexibility, they may hurt generalization of underlying definition patterns if the model gives high probabilities of going through them. Specifically, we set the emission probabilities for each match and insertion state using the smoothed maximum likelihood estimate of the emission probabilities (Equation 5). We adjust the value of δ such that the probability of emitting a token from match states is higher than that of insertion states. We set the initial state transition probabilities to the inverse proportion of the number of transition links from a state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Left and Right Sequences</head><p>The pattern matching score for a test instance against definition patterns is obtained by combining the soft pattern matching scores for both the left and right sequences in the context of the search target. We use the linear combination of the scores:</p><formula xml:id="formula_9">) | _ ( ) 1 ( ) | _ ( ) | ( R L SP seq right P SP seq left P C ins P α α − + = (9)</formula><p>where ins represents a test instance and C denotes the context model. P(left_seq|SP L ) and P(right_seq|SP R ) give the probabilistic pattern matching scores of the left and right sequences of the instance, given the corresponding soft pattern (SP) matching models. α is the mixture weight. We adopt an EM algorithm similar to that in Section 3.1 to estimate the value of α. We set α to 0.3 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions on the Two Generic Soft Matching Models</head><p>We have presented two generic soft pattern models: Bigram SP and PHMM SP. These two models differ in their complexity. The bigram model can be considered a simplified first-order Markov model with one state for each token. It directly captures sequence information using bigram probabilities. In contrast, PHMM has a more complicated topology that aggregates token sequence probability into state transition probabilities. Theoretically, the PHMM needs more training data to converge to an accurate model as it has more parameters to estimate. Appropriate initialization (as shown in Section 3.2) for the PHMM also aids convergence to the global maxima. An advantage is that PHMMs are less sensitive to model settings, e.g., model length, because it makes its transitions between hidden states which correspond to aggregations of tokens rather than directly between surface tokens. We present experiments in Section 4 to validate these conjectures.</p><p>Despite their differences, the two models are inherently connected because both the models deem definition patterns as sequences of tokens. They model the same structural information for patterns: First, both models capture the importance of a token's position in the context of a search target: the bigram model uses unigram probabilities while the PHMM model uses emission probabilities to represent a token's independent probability of appearing in each position. Second, both models account for the sequential order of tokens. This sequential information is captured by bigram probability in the bigram model and state transition probability in the PHMM model. Thus, the soft matching method proposed in Cui et al.</p><p>[2] may be considered a special case of our Bigram SP model.</p><p>Our assumption is that all definition pattern instances embedded in definition sentences are generated by a single model. Although it may be advantageous if we could train separate probabilistic models for different types of definition patterns, limited training prevents this, and such models impair the objective of establishing a uniform matching model for definition patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATIONS</head><p>We have three goals in our evaluations: (1) to compare the performance of our soft matching models against other state-ofthe-art pattern matching techniques in the context of a standard definitional QA system, and to study the two models' sensitivity to: (2) model length, and (3) amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data Set</head><p>We employ the data set from the TREC-13 Question Answering Task. It includes the AQUAINT corpus of over one million news articles and 64 definition question 1 and answer pairs. We use the data set from the TREC-12 definitional QA task as training data, which shares the same corpus with TREC-13 and includes an additional 50 definition question and answer pairs. Based on the answer nuggets (ground truth, manually labeled data) provided by TREC for these 50 questions, we manually label all sentences that cover the nuggets from the corpus as definition sentences. In total, we obtain 761 labeled definition sentences as training data for estimating the soft matching models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We adopt the evaluation metrics used in the TREC definitional question answering task <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. TREC provides a list of essential and acceptable nuggets for answering each question. We use these nuggets to assess the various QA systems in our evaluation in both manual and automatic assessments. In the manual assessment used in official TREC evaluations, an assessor examines how many essential and acceptable nuggets are covered in the returned answer. Each definition is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on answer length. These scores are combined using the F 3 measure with recall being weighted three times as important as precision <ref type="bibr" target="#b16">[17]</ref>.</p><p>In addition to manual assessment, we perform automated evaluation using ROUGE <ref type="bibr" target="#b8">[9]</ref>. Automatic scores can be a good supplement to manual evaluations for two reasons. First, as Xu et al. <ref type="bibr" target="#b18">[19]</ref> suggested, ROUGE gives automatic scores that are highly correlated with manual counting of nuggets. Second, the manual checking of nuggets is subject to inconsistent scoring across runs <ref type="bibr" target="#b15">[16]</ref>. ROUGE is a metric originally designed for summarization evaluation and has previously been adapted for definitional QA evaluation <ref type="bibr" target="#b18">[19]</ref>. We use the metric ROUGE-3, which counts the trigrams shared between the official answer and the system answer.</p><p>To perform automatic scoring, for each search target, we construct five groups of sentences as the gold standard. According to TREC-13 guidelines, gold standard sentences are selected based on answers to the factoid/list questions about the target and "other" information about the target, which includes essential and acceptable nuggets. We give details of how to construct the gold standard in the Appendix. We use two ROUGE metrics: ROUGE-3-ALL (R3A) for evaluations against all sentences in the gold standard and ROUGE-3-ESSENTIAL (R3E) for evaluations against those sentences that contain only factoid/list answers and essential nuggets in the gold standard list. The final ROUGE scores are the average scores obtained by running the evaluation tool over the five groups of gold standard lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison Systems</head><p>In our experiments, the base definition generation system used is the system discussed in Section 2 and illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In evaluations, we only vary the definition pattern matching module while holding constant all other components and their parameters. For comparison, we apply a set of manually constructed hard matching definition patterns which has demonstrated state-of-theart performance as the baseline system. The patterns combine those used in Cui et al. <ref type="bibr" target="#b1">[2]</ref> and Hildebrandt et al. <ref type="bibr" target="#b5">[6]</ref>, which comprise the most complete published list of patterns to our knowledge. In particular, we use the following comparison systems:</p><p>(1) HP-Filter: This system employs the method used in Xu et al.</p><p>[19] and Hildebrandt et al. <ref type="bibr" target="#b5">[6]</ref> where bag-of-words is used to rank those constructs matched by any manual definition pattern.</p><p>(2) Original SP: We also use the soft pattern matching method proposed in Cui et al. <ref type="bibr" target="#b1">[2]</ref>, and adopt the same parameter settings.</p><p>In our evaluations, we set answer length N to 14 sentences for all systems to approximate the desirable answer length used in successful TREC systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Evaluation</head><p>In the first evaluation, we assess the performance obtained by the two soft matching models against that of the comparison systems. We set model length L to optimal values based on experiments which we will present in the next subsection. We list the evaluation results in <ref type="table" target="#tab_3">Table 1</ref>. We take HP-Filter as the baseline system for comparison with the soft pattern matching models. From <ref type="table" target="#tab_3">Table 1</ref>, we arrive at the following:</p><p>1. We reaffirm the conclusion drawn by Cui et al. <ref type="bibr" target="#b1">[2]</ref> that soft matching patterns outperform manually constructed hard matching patterns in both manual and automatic evaluations. With the manual F 3 measure, all three soft pattern models perform significantly better than the baseline (p ≤ 0.01). The significance measures change slightly when ROUGE scores are used. With R3E, only the bigram and PHMM models achieve significant improvement over the baseline (p=0.03 and p=0.08). The original and PHMM models achieve similar performance in R3A scores while the bigram model achieves some improvement. We conjecture that the differences among the significance tests are due to the long standard answers. Recall that we have compiled a list of sentences as the gold standard for ROUGE evaluation. While the human assessor is able to figure out the real answer nuggets embedded in the system-returned answers, the ROUGE evaluation tool is likely to overestimate recall due to the long standard answers. 3. We observe that the manual F 3 scores are highly correlated with the automatic metrics R3A and R3E despite that the ROUGE scores might have minor disturbance due to the long gold standard answers. We calculate statistical correlation between F 3 and ROUGE scores. Correlation measures vary from -1 (perfectly anticorrelated) to 1 (perfectly correlated). All the correlation measures in our evaluations are between 0.6 and 0.7, which indicate strong correlations between the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Sensitivity to Model Length</head><p>As the parameters of the models are estimated automatically, we vary the only arbitrarily set factor -model length -for the two models. We list the ROUGE scores for both models when varying their model length (number of slots) from 2 to 6 in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>In <ref type="table" target="#tab_4">Table 2</ref>, we see that Bigram SP obtains the best performance with the model length of 3 while PHMM SP achieves the highest performance with the model length of 4. Both models slacken in their performance when more slots are used.</p><p>We also compare percentage change in performance against the highest score for each scoring metric. The performance of the bigram model fluctuates more over different model lengths compared to PHMM SP. This is evidence that PHMM SP may be more stable amid changes in model typology.</p><p>Another observation is that with model lengths of 5 and 6, PHMM SP performs better than the bigram model. We hypothesize that the PHMM model may be more capable of dealing with longer contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Sensitivity to Amount of Training Data</head><p>In this evaluation, we experiment with different amounts of training data. We divide the training data into two or three equal portions. We train the bigram and PHMM models by using different amounts of training data while testing on the same test data set as before. We perform multiple runs with different portions of training data, and average the scores obtained by the system. <ref type="table" target="#tab_6">Table 3</ref> lists the results.  <ref type="table" target="#tab_6">Table 3</ref> shows that PHMM SP achieves higher improvement than Bigram SP when more training data is used. On the other hand, comparing the performance difference between the PHMM and bigram models with different amounts of training data reveals that with more training data, the performance difference between the two models narrows. For instance, the difference decreases from 7.22% to 2.28% and from 5.61% to 3.09% in R3E and R3A scores respectively when we change from using one third to using the full amount of training data. This observation supports our conjecture that PHMM requires a larger amount of training data for parameter estimation. Although <ref type="table" target="#tab_3">Table 1</ref> shows the bigram model performing better, we believe that with enough training data, PHMM SP may outperform the bigram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>We have proposed two generic soft pattern models: one based on a bigram language model and the other on the PHMM. Both provide formal probabilistic methods to model lexico-syntactic patterns represented by token sequences. In particular, we have shown that PHMM overcomes the problem of gaps caused by language variations in pattern matching. Our experiments show both models obtaining significantly better performance than carefully constructed hard matching patterns in a definitional QA system. Although the bigram model shows slightly better performance between the two new models in our evaluations, we believe that the PHMM model can perform better with more training data. Moreover, as the PHMM model has shown to be more tolerant to language variations, it is likely to be suitable in applications with diverse training and test instances.</p><p>Providing formal models for modeling contextual lexico-syntactic patterns is the main contribution of this work. Our two soft matching models are generic and can be extended to related areas that require modeling of contextual patterns, such as information extraction (IE). The pattern matching problem in IE tasks are formally the same as definition sentence retrieval. When conducted on free texts, an IE system can also suffer from various unseen instances not being matched by trained patterns. <ref type="bibr">Xiao et al. [18]</ref> have demonstrated that soft pattern matching greatly improves recall in an IE system. Although some HMM topologies have been employed for IE tasks, our models are more generic and require less configuration and parameter tuning with changing domains. The models can help IE systems overcome difficulties caused by language variations in pattern matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Architecture of the definitional QA system. Preprocessing of Sentences: We then process the sentences into pattern instances, on which soft pattern generation and matching are performed. First, words specific to the search targets are replaced with their general syntactic (POS or chunk) tags. Remaining words are stemmed. We refer to these remaining lexical words and substituted general syntactic tags as tokens. Definition patterns are expressed by sequences of such tokens. Second, we crop the windows surrounding the search target as instances according to model length L. The following box illustrates the process: T he chan nel Iqra is ow ned by th e A rab R adio and T elevision co m pany an d is the b rainchild o f the Sau di m illionaire, Saleh K am el. PO S tagging, chunking and su b stitu tio n D T $ N N &lt; SC H _T E R M &gt; B E $ ow ned by D T $ N P an d B E $ D T $ brain child of N P . Cropping (L= 3) D T $ N N &lt;SC H _ T E R M &gt; B E $ ow ned b y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . Illustration of the Profile HMM (L=4). Matching states are represented by M i in squares, insertion states by I i in diamonds, and deletion states by D i in circles. Start and End states occupy the positions of 0 and L+1.</head><label>2</label><figDesc>Figure 2. Illustration of the Profile HMM (L=4). Matching states are represented by M i in squares, insertion states by I i in diamonds, and deletion states by D i in circles. Start and End states occupy the positions of 0 and L+1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 . Generating a test instance with gaps using the PHMM. The optimal path is in bold, and words or tags emitted are shown in callouts.</head><label>3</label><figDesc>Figure 3. Generating a test instance with gaps using the PHMM. The optimal path is in bold, and words or tags emitted are shown in callouts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>F 3 performance comparison (percentage improvement 
shown in brackets; ** and * represent different significance 
levels by t-test: p ≤ 0.01 and 0.05, respectively) 

Configurations 
HP-Filter 
(Baseline) 
Original SP 
Bigram SP 
PHMM SP 

R3A 
0.2106 
0.2233 
(+6.00%) 

0.2303 
(+9.37%) 

0.2234 
(+6.08%) 

R3E 
0.2286 
0.2378 
(+4.00%) 

0.2553 
(+11.67%)* 

0.2496 
(+9.18%) 

NR 
0.5027 
0.5376 
0.5519 
0.5420 

NP 
0.3159 
0.3238 
0.3403 
0.3264 

F3 
0.4633 
0.4937 
(+6.56%)** 

0.5088 
(+9.83%)** 

0.4971 
(+7.30%)** 

Correlation 
F3, R3A 
0.63 
0.63 
0.66 
0.61 

Correlation 
F3, R3E 
0.63 
0.69 
0.67 
0.60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>2 .</head><label>2</label><figDesc></figDesc><table>We note that both Bigram SP and PHMM SP outperform 
Original SP in all scores. Bigram SP outperforms Original SP by 
7.36% (p=0.09) and 3.05% (p=0.1) in R3E and F 3 scores, 
respectively. PHMM SP achieves 5.00% improvement over 
Original SP in R3E scores. These results show that the 
preliminary soft matching method is not optimized in parameter 
setting. Finding best parameters is often tedious and difficult for 
such ad hoc systems. In contrast, Bigram SP and PHMM SP 
provide a sound framework for parameter estimation. This should 

facilitate the migration of the two generic soft matching models to 
other applications. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Performance with different model lengths. The 
percentage values in parentheses are difference measures 
compared to the maximum. Note that PHMM SP's minimum 
length for training is 3. 

Model 
Length (# 
Slots) 

2 
3 
4 
5 
6 

PHMM SP 
R3A 
N/A 
0.2139 
(-4.25%) 
0.2234 
0.2190 
(-1.97%) 

0.2125 
(-4.88%) 

PHMM SP 
R3E 
N/A 
0.2369 
(-5.09%) 
0.2496 
0.2422 
(-2.97%) 

0.2367 
(-5.17%) 

BIGRAM 
SP R3A 

0.2128 
(-7.60%) 
0.2303 
0.2165 
(-6.00%) 

0.2152 
(-6.56%) 

0.2086 
(-9.42%) 

BIGRAM 
SP R3E 

0.2340 
(-8.34%) 
0.2553 
0.2363 
(-7.44%) 

0.2354 
(-7.80%) 

0.2346 
(-8.11%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 . Performance comparison across varying amounts of training data.</head><label>3</label><figDesc></figDesc><table>Training Data size (fraction of whole training corpus) 

1/3 
1/2 
1 

PHMM R3A 
0.2110 
0.2179 (+3.24%) 
0.2234 (+5.85%) 

PHMM R3E 
0.2311 
0.2402 (+3.93%) 
0.2496 (+8.00%) 

Bigram R3A 
0.2229 
0.2269 (+1.76%) 
0.2303 (+3.32%) 

Bigram R3E 
0.2478 
0.2510 (+1.29%) 
0.2553 (+3.03%) 

</table></figure>

			<note place="foot" n="1"> The test data for TREC-13 includes 65 definition questions. NIST drops one in the official evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>The authors are grateful to Wee Sun Lee for enlightening us on Profile HMM and for his valuable suggestions on the draft of the paper. We also thank Alexia Leong for proofreading the paper. Thanks also go to anonymous reviewers whose comments have helped improve the final version of this paper. The first author is supported by the Singapore Millennium Foundation Scholarship in his PhD studies <ref type="bibr">(ref no. 2003-SMS-0230</ref>).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">APPENDIX</head><p>According to TREC-13 QA guidelines <ref type="bibr" target="#b16">(Voorhees 2004</ref>), definitional QA systems are required to present "other" information about the search target that is not covered by the factoid or list questions related to the target. As our purpose is to evaluate a definitional QA system, we perform the following alterations to make the evaluation complete: We use a list of answer patterns for the factoid and list questions about a target to search for sentences that contain the answers. We treat these factoid/list answers as essential nuggets and add the answer sentences to the gold standard list. This is based on the guideline that factoid/list questions are about the most essential information about the target. We choose sentences because answers for factoid/list questions are only phrases and other nuggets about the target are often ungrammatical text fragments. The original form of answers and nuggets cannot be matched by ROUGE in most cases. As the same answer may be embedded in different sentences, we search for up to five sentences for each factoid/list answer and for each nugget in the definition part. Accordingly, we create five groups of gold standard lists for each target. Besides factoid and list answers, we also add sentences containing essential and acceptable nuggets to "other" questions to the gold standard list.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Hybrid Approach for QA Track Definitional Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlaikjer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Soft Patterns for Generating Definitions from Online News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW &apos;04</title>
		<meeting>of WWW &apos;04<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">A Comparative Study on Sentence Retrieval for Definitional Question Answering, SIGIR Workshop on Information Retrieval for Question Answering (IR4QA)</title>
		<meeting><address><addrLine>Sheffield, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Answer Mining by Combining Extraction Techniques with Abductive Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bensley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC 2003</title>
		<meeting>of TREC 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Answering Definition Questions with Multiple Knowledge Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT/NAACL</title>
		<meeting>of HLT/NAACL<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpolated estimation of markov source parameters from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop Pattern Recognition in Practice</title>
		<meeting>of the Workshop Pattern Recognition in Practice<address><addrLine>Amsterdam, Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hidden Markov Models in Computational Biology -Applications to Protein Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sjolander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="1501" to="1531" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL &apos;03</title>
		<meeting>of HLT-NAACL &apos;03<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extraction patterns for information extraction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI-99 Workshop on Machine Learning for Information Extraction</title>
		<meeting>of AAAI-99 Workshop on Machine Learning for Information Extraction</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL &apos;02</title>
		<meeting>of ACL &apos;02<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Extraction Patterns for Subjective Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP &apos;03</title>
		<meeting>of EMNLP &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two decades of statistical language modeling: Where do we go from here</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="2000-08" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical hidden markov models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skounakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI &apos;03</title>
		<meeting>of IJCAI &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2003 question answering track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC 2003</title>
		<meeting>of TREC 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2004 question answering track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TREC</title>
		<meeting>of TREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascading Use of Soft and Hard Matching Pattern Rules for Weakly Supervised Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING &apos;04</title>
		<meeting>of COLING &apos;04<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="542" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of an extraction-based approach to answering definitional questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Licuanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR &apos;04</title>
		<meeting>of SIGIR &apos;04<address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="418" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maslennikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">QUALIFIER in TREC 12 QA Main Task, Proc. of TREC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
