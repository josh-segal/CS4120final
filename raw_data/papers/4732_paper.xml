<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Should one compute the Temporal Difference fix point or minimize the Bellman Residual ? The unified oblique projection view</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Scherrer</surname></persName>
							<email>scherrer@loria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LORIA -INRIA Lorraine -Campus Scientifique</orgName>
								<address>
									<postBox>BP 239</postBox>
									<postCode>54506</postCode>
									<settlement>Vandoeuvre-l` es-Nancy</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Should one compute the Temporal Difference fix point or minimize the Bellman Residual ? The unified oblique projection view</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method out-performs the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu &amp; Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We consider linear approximations of the value function of the policy in the framework of Markov Decision Processes (MDP). We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which <ref type="bibr" target="#b0">Antos et al. (2008)</ref>; <ref type="bibr" target="#b3">Farahmand et al. (2008)</ref>; <ref type="bibr" target="#b11">Sutton et al. (2009)</ref> have recently presented as the minimization of the mean-square projected Bellman Equation, and the minimization of the meansquare Bellman Residual (BR). In this article, we present some new analytical and empirical data, that shed some light on both approaches. The paper is organized as follows. Section 1 describes the MDP linear approximation framework and the two projection methods. Section 2 presents small MDP examples, where each method outperforms the other. Section 3 highlights a simple relation between the quantities TD and BR optimize, and show that while BR enjoys a performance guarantee, TD does not in general. Section 4 contains the main contribution of this paper: we describe a unified view in terms of oblique projections of the Bellman equation, which simplifies and extends the characterization of <ref type="bibr" target="#b10">Schoknecht (2002)</ref> and the recent analysis of <ref type="bibr" target="#b16">Yu &amp; Bertsekas (2008)</ref>. Eventually, Section 5 presents some simulations, that address the following practical questions: which of the method gives the best approximation? and how useful is our analysis for selecting it a priori?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Framework and Notations</head><p>The model We consider an MDP with a fixed policy, that is an uncontrolled discrete-time dynamic system with instantaneous rewards. We assume that there is a state space X of finite size N . When at state i ∈ {1, .., N }, there is a transition probability p ij of getting to the next state j. Let i k the state of the system at time k. At each time step, the system is given a reward γ k r(i k ) where r is the instantaneous reward function, and 0 &lt; γ &lt; 1 is a discount factor. The value at state i is defined as the total expected return: v(i) := lim N →∞ E N −1 k=0 γ k r(i k ) i 0 = i . We write P the N × N stochastic matrix whose elements are p ij . v can be seen as a vector of R N . v is known to be the unique fixed point of the Bellman operator: T v := r + γP v, that is v solves the Bellman Equation v = T v and is equal to L −1 r where L = I − γP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>inria-00537403, version 1 -19 Nov 2010</head><p>Author manuscript, published in "N/P" TD or BR? The unified oblique projection view Approximation Scheme When the size N of the state space is large, one usually comes down to solving the Bellman Equation approximately. One possibility is to look for an approximate solutionˆvsolutionˆ solutionˆv in some specific small space. The simplest and best understood choice is a linear parameterization: ∀i, ˆ v(i) = m j=1 w j φ j (i) where m ≪ N , the φ j are some feature functions that should capture the general shape of v, and w j are the weights that characterize the approximate valuê v. For all i and j, write φ j the N -dimensional vector corresponding to the j th feature function and φ(i) the mdimensional vector giving the features of state i. For any vector of matrix X, denote X ′ its transpose. The</p><formula xml:id="formula_0">following N × m feature matrix Φ = (φ 1 . . . φ m ) = (φ(i 1 ) . . . φ(i N ))</formula><p>′ leads to write the parameterization of v in a condensed matrix form: ˆ v = Φw, where w = (w 1 , ..., w m ) is the m-dimensional weight vector. We will now on denote span (Φ) this subspace of R N and assume that the vectors φ 1 , ..., φ m form a linearly independent set.</p><p>Some approximationˆvapproximationˆ approximationˆv of v can be obtained by minimizingˆvmizingˆ mizingˆv → ˆ v − v for some norm · , that is equivalently by projecting v onto span (Φ) orthogonally with respect to · . In a very general way, any symmetric positive definite matrix Q of R N induces a quadratic norm · Q on R N as follows:</p><formula xml:id="formula_1">v Q = √ v ′ Qv.</formula><p>It is well known that the orthogonal projection with respect to such a norm, which we will denote Π ·Q , has the following closed form: Π ·Q = Φπ ·Q where π ·Q = (Φ ′ QΦ) −1 Φ ′ Q is the linear application from R N to R m that returns the coordinates of the projection of a point in the basis (φ 1 , . . . , φ m ). With these notations, the following relations π ·Q Φ = I and π ·Q Π ·Q = π ·Q hold.</p><p>In an MDP approximation context, where one is modeling a stochastic system, one usually considers a specific kind of norm/projection. Let ξ = (ξ i ) be some distribution on X such that ξ &gt; 0 (it assigns a positive probability to all states). Let Ξ be the diagonal matrix with the elements of ξ on the diagonal. Consider the orthogonal projection of R N onto the feature space span (Φ) with respect to the ξ-weighted quadratic</p><formula xml:id="formula_2">norm v ξ = N j=1 ξ i v i 2 = √ v ′ Ξv.</formula><p>For clarity of exposition, we will denote this specific projection Π := Π ·Ξ = Φπ where π :</p><formula xml:id="formula_3">= π ·Ξ = (Φ ′ ΞΦ) −1 Φ ′ Ξ.</formula><p>Ideally, one would like to compute the "best" approximation ˆ v best = Φw best with w best = πv = πL −1 r.</p><p>This can be done with algorithms like TD(1) / LSTD(1) <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996;</ref><ref type="bibr" target="#b2">Boyan, 2002</ref>), but they require simulating infinitely long trajectories and usually suffer from a high variance. The projections methods, which we focus on in this paper, are alternatives that only consider one-step samples.</p><p>TD <ref type="formula">(0)</ref>   <ref type="bibr" target="#b11">Sutton et al. (2009)</ref>, when the inverse exists, the above computation is equivalent to minimizing forˆvforˆ forˆv ∈ span (Φ) the TD error</p><formula xml:id="formula_4">E T D (ˆ v) := ˆ v − ΠTˆvΠTˆ ΠTˆv ξ down to 0 3 .</formula><p>BR minimization method The principle of the Bellman Residual (BR) method is to look forˆvforˆ forˆv ∈ span (Φ) so that it minimizes the norm of the Bellman Residual, that is the quantity</p><formula xml:id="formula_5">E BR (ˆ v) := ˆ v − T ˆ v ξ . Sincê</formula><p>v is of the form Φw, it can be seen that E BR (ˆ v) = Φw − γP Φw − r ξ = Ψw − r ξ using the notation Ψ = LΦ. Using standard linear least squares arguments, one can see that the minimum BR is obtained forˆvforˆ forˆv BR = Φw BR with</p><formula xml:id="formula_6">w BR = (Ψ ′ ΞΨ) −1 Ψ ′ Ξr.<label>(2)</label></formula><p>Note that in this case, the above inverse always exists <ref type="bibr" target="#b10">(Schoknecht, 2002</ref>). , and the weight of the best approximation is w best = πv = 1 5 r 1 + 2+γ 5(1−γ) r 2 . This example has been proposed by <ref type="bibr" target="#b1">Bertsekas &amp; Tsitsiklis (1996)</ref> in order to show that fitted Value Iteration can diverge if the samples are not generated by the stationary distribution of the policy. In <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996)</ref>, the authors only consider the case r 1 = r 2 = 0  <ref type="figure">Figure 1</ref>. Error ratio (in log scale) between the TD/BR projection methods and the best approximation for Example 1, with respect to the discount factor γ and the parameter θ of the reward (Left). It turns out that these surfaces do not depend on θ so we also draw the graph with respect to γ only (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Two simple examples</head><p>so that this diverging result was true even though the exact value function v(0) = v(1) = 0 did belong to the feature space. In the case r 1 = r 2 = 0, the TD and BR methods do calculate the exact solution (we will see later that this is indeed a general fact when the exact value function belongs to the feature space). We thus extend this model by taking (r 1 , r 2 ) 񮽙 = (0, 0). As a scaling of the reward is translated exactly in the approximation, we consider the general form (r 1 , r 2 ) = (cos θ, sin θ).</p><p>Consider the TD solution: one has</p><formula xml:id="formula_7">Φ ′ Ξ = 1 2 1 , (I − γP )Φ = (1 − 2γ 1 − γ), thus (Φ ′ ΞΨ) = 5</formula><p>2 − 3γ and Φ ′ Ξr = r1 2 + r 2 . Eventually the weight of the TD approximation is w T D = r1+2r2 5−6γ . One notices here that the value γ = 5/6 is singular. Now, consider the BR solution. One can see that (</p><formula xml:id="formula_8">Ψ ′ ΞΨ) −1 = (1−2γ) 2 +(2−2γ) 2 2</formula><p>and</p><formula xml:id="formula_9">Ψ ′ Ξr = (1−2γ)r1+(2−2γ)r2 2</formula><p>. Thus, the weight of the BR approximation is</p><formula xml:id="formula_10">w BR = (1−2γ)r1+(2−2γ)r2 (1−2γ) 2 +(2−2γ) 2 .</formula><p>For all these approximations, one can compute the squared error e with respect to the optimal solution v:</p><formula xml:id="formula_11">For any weight w ∈ {w best , w T D , w BR }, e(w) = v − Φw 2 ξ = 1 2 (v(1) − w) 2 + 1 2 (v(2) − 2w) 2 . In Fig- ure 1, we plot the squared error ratios e(wT D )</formula><p>e(w best ) and e(wBR) e(w best ) on a log scale (they are by definition greater than 1) with respect to θ and γ. It turns out that these ratios do not depend on θ (instead of showing this through painful arithmetic manipulations, we will come back to this point and prove it later on). This Figure also displays the graph with respect to γ only. We can observe that for any choice of reward function and discount factor, the BR method returns a better value than the TD method. Also, when γ is in the neighborhood of 5 6 , the TD error ratio tends to ∞ while BR's stays bounded. This Example shows that there exists MDPs where the BR is consistenly better than the TD method, which can give an unbounded error. One should however not conclude too quickly that BR is always better than TD. The literature contains several arguments in favor of TD, one of which is considered in the following Example.</p><p>Example 2 Sutton et al. <ref type="formula" target="#formula_6">(2009)</ref> recently described a 3-state MDP example where the TD method computes the best projection while BR does not. The idea behind this 3-state example can be described in a quite general way 4 : Suppose we have a k + l-state MDP, of which the Bellman Equation has a block triangular structure:</p><formula xml:id="formula_12">v 1 = γP 1 v 1 + r 1 / v 2 = γP 21 v 1 + P 22 v 2 + r 2</formula><p>where v 1 ∈ R k and v 2 ∈ R l (the concatenation of the vectors v 1 and v 2 form the value function). Suppose also that the approximation subspace span (Φ) is R k × S 2 where S 2 is a subspace of R l . For the first component v 1 , the approximation space is the entire space R k . With TD, we obtain the exact value for the k first components of the value, while with Bellman residual minimization, we do not: satisfying the first equation exactly is traded for decreasing the error in satisfying the second one (which also involves v 1 ). In an optimal control context, the example above can have quite dramatic implications, as v 1 can be related to the costs at some future states accessible from those states associated with v 2 , and the future costs are all that matters when making decisions.</p><p>Overall, the two methods generate different types of biases, and distribute error in different manners. In order to gain some more insight, we now turn on to some analytical facts about them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Relation and Stability Issues</head><p>Though several works have compared and considered both methods <ref type="bibr" target="#b10">(Schoknecht, 2002;</ref><ref type="bibr" target="#b6">Lagoudakis &amp; Parr, 2003;</ref><ref type="bibr" target="#b7">Munos, 2003;</ref><ref type="bibr" target="#b16">Yu &amp; Bertsekas, 2008)</ref>, the following simple fact has, to our knowledge, never been emphasized per se:</p><formula xml:id="formula_13">Proposition 1</formula><p>The BR is an upper bound of the TD error, and more precisely:</p><formula xml:id="formula_14">∀ˆv∀ˆv ∈ span (Φ) , E BR (ˆ v) 2 = E T D (ˆ v) 2 + T ˆ v − ΠTˆvΠTˆ ΠTˆv 2 ξ .</formula><p>Proof This simply follows from Pythagore, as ΠTˆvΠTˆ ΠTˆv − T ˆ v is orthogonal to span (Φ) andˆvandˆ andˆv − ΠTˆvΠTˆ ΠTˆv belongs to span (Φ).</p><p>This implies that if one can make the BR small, then the TD Error will also be small. In the limit case where one can make the BR equal to 0, then the TD Error is also 0.</p><p>One of the motivation for minimizing the BR is historically related to a well-known result of <ref type="bibr" target="#b15">Williams &amp; Baird (1993)</ref></p><formula xml:id="formula_15">: ∀ˆv∀ˆv, v − ˆ v ∞ ≤ 1 1−γ T ˆ v − ˆ v ∞ .</formula><p>Since one considers the weighted quadratic norm in practice 5 , the related result 6 that really makes sense here is:</p><formula xml:id="formula_16">∀ˆv∀ˆv, v − ˆ v ξ ≤ √ C(ξ) 1−γ T ˆ v − ˆ v ξ where C(ξ) := max i,j</formula><p>pij ξi is a "concentration coefficient", that can be seen as some measure of the stochasticity of the MDP 7 . This result shows that it is sound to minimize the BR, since it controls (through a constant) the approximation error v − ˆ v BR ξ .</p><p>On the TD side, there does not exist any similar result. Actually, the fact that one can build examples (like Example 1) where the TD projection is numerically unstable implies that one cannot prove such a result. Proposition 1 allows to understand better the TD method: by minimizing the TD Error, one only minimizes one part of the BR, or equivalently this means that one does not care about the term T v − ΠT v 2 ξ , which may be interpreted as a measure of adequacy of the projection Π with the Bellman operator T . In Example 1, the approximation error of the TD projection goes to infinity because this adequacy term diverges. In , the authors use an algorithm based on the TD Error and make an assumption on this adequacy term (there called the inherent Bellman error of the approximation space), so that their algorithm can be proved convergent.</p><p>A complementary view on the potential instability of TD, has been referred to as a norm incompatibility issue <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996;</ref><ref type="bibr" target="#b5">Guestrin et al., 2001</ref>), and can be revisited through the notion of concentration coefficient. Stochastic matrices P statisfy P ∞ = 1, which makes the Bellman operator T γ-contracting, and thus its fixed point is well-defined. The orthogonal projection with respect to · ξ is such that Π ξ = 1. Thus P and Π are of norm 1, but for different norms. Unfortunately, a general (tight) bound for linear projections is Π ∞ ≤ 5 Mainly because it is computationnally easier than doing a max-norm minimization, see however ( <ref type="bibr" target="#b5">Guestrin et al., 2001</ref>) for an attempt of doing max-norm projection. <ref type="bibr">6</ref> The proof is a consequence of Jensen's inequality and the arguments are very close to the ones in <ref type="bibr" target="#b7">(Munos, 2003)</ref>.</p><p>7 If ξ is the uniform law, then there always exists such a C(ξ) ∈ (1, N ) where one recalls that N is the size of the state space; in such a case, C(ξ) is minimal if all next-states are chosen with the uniform law, and maximal as soon as there exists a deterministic transition. See <ref type="bibr" target="#b7">(Munos, 2003)</ref> for more discussion on this coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1+</head><p>√ N 2 <ref type="bibr" target="#b13">(Thompson, 1996)</ref> and it can be shown 8 that P ξ ≤ C(ξ) (which can thus also be of the order of √ N ). Consequently, ΠP ∞ and ΠP ξ may be greater than 1, and thus the fixed point of the projected Bellman equation may not be well-defined. A known exception where the composition ΠP has norm 1, is when one can prove that P ξ = 1 (as for instance when ξ is the stationary distribution of P ) and in this case we know from <ref type="bibr" target="#b1">Bertsekas &amp; Tsitsiklis (1996)</ref>; <ref type="bibr" target="#b14">Tsitsiklis &amp; Van Roy (1997</ref></p><formula xml:id="formula_17">) that v − ˆ v T D ξ ≤ 1 1 − γ 2 v − ˆ v best ξ .<label>(3)</label></formula><p>Another notable such exception is when Π max = 1, as in the so-called "averager" approximation <ref type="bibr" target="#b4">(Gordon, 1995)</ref>. However, in general, the stability of TD is difficult to guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The unified oblique projection view</head><p>In the TD approach, we consider finding the fixed point of the composition of an orthogonal projection Π and the Bellman operator T . Suppose now we consider using a (non necessarily orthogonal) projection Π onto span (φ), that is any linear operator that satisfies Π 2 = Π and whose range is span (Φ). In their most general form, such operators are called oblique projections and can be written Π X = Φπ X with π X = (X ′ Φ) −1 X ′ . The parameter X specifies the projection direction: precisely, Π X is the projection onto span (Φ) orthogonally to span (X). As for the orthogonal projections, the following relations π X Φ = I and π X Π X = π X hold. Recall that L = I − γP . We are ready to state the main result of this paper:</p><formula xml:id="formula_18">Proposition 2 Write X T D = ΞΦ and X BR = ΞLΦ.<label>(1)</label></formula><p>The TD fix point computation and the BR minimization are solutions (respectively with X = X T D and X = X BR ) of the projected equationˆvequationˆ equationˆv X = Π X T ˆ v X . (2) When it exists, the solution of this projected equation is the projection of v onto span (Φ) orthogonally to span (L ′ X), i.e. formallyˆvformallyˆ formallyˆv X = Π L ′ X v.</p><p>Proof We begin by showing part (2). WritingˆvWritingˆ Writingˆv X = Φw X , the fixed point equation is: Φw X = Π X (r + γP Φw x ). Multiplying on both sides by π X , one obtains: w X = π X (r + γP Φw x ) and therefore w X = (I − γπ X P Φ) −1 π X r. Using the definition of π X , one <ref type="bibr">8</ref> One can prove that for all x, P x 2 ξ ≤ x 2 ξP ≤ C(ξ)x 2 ξ . The argument for the first inequality involves Jensen's inequality and is again close to what is done in <ref type="bibr" target="#b7">(Munos, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>inria-00537403, version 1 -19 Nov 2010</head><p>obtains:</p><formula xml:id="formula_19">w X = (I − γ(X ′ Φ) −1 X ′ P Φ) −1 (X ′ Φ) −1 X ′ r = (X ′ Φ)(I − γ(X ′ Φ) −1 X ′ P Φ) −1 X ′ r = (X ′ (I − γP )Φ) −1 X ′ r (4) = (X ′ LΦ) −1 X ′ Lv = π L ′ X v</formula><p>where we enventually used r = Lv.</p><p>The proof of part (1) now follows. The fact that TD is a special case with X = ΞΦ is trivial by construction since then Π X is the orthogonal projection with respect to · ξ . When X = ΞLΦ, one simply needs to observe from Equations 2 and 4 and the definition of Ψ = LΦ that w X = w BR .</p><p>Beyond its nice and simple geometric flavour, a direct consequence of Proposition 2 is that it allows to derive tight error bounds for TD, BR, and any other method for general X. For any square matrix M , write σ(M ) its spectral radius.</p><p>Proposition 3 For any choice of X, the approximation error satisfies:</p><formula xml:id="formula_20">v − ˆ v X ξ ≤ Π L ′ X ξ v − ˆ v best ξ (5) = σ(ABCB ′ )v − ˆ v best ξ</formula><p>where A = Φ ′ ΞΦ, B = (X ′ LΦ) −1 and C = XLΞ −1 L ′ X are matrices of size m × m.</p><p>Thus, for any X, the amplification of the smallest error v − ˆ v best ξ depends on the norm of the associated oblique projection, which can be estimated as the spectral radius of the product of small matrices. A simple corollary of this Proposition is the following: if the real value v belongs to the feature space span (Φ) (in such a case v = ˆ v best ) then all oblique projection methods find it (ˆ v X = v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Proposition 3 Proposition 2 implies that</head><formula xml:id="formula_21">v − ˆ v X = (I − Π L ′ X )v = (I − Π L ′ X )(I − Π ΞΦ )v.</formula><p>where we used the fact that Π L ′ X Π ΞΦ = Π ΞΦ since Π L ′ X and Π ΞΦ are projections onto span (Φ). Taking the norm,</p><formula xml:id="formula_22">one obtains v − ˆ v X ξ ≤ I − Π L ′ X ξ v − Π ΞΦ v ξ = Π L ′ X ξ v − ˆ</formula><p>v best ξ where we used the definition ofˆv ofˆ ofˆv best , and the fact that I − Π L ′ X ξ = Π L ′ X ξ since Π L ′ X is a (non-trivial) projection (see e.g. <ref type="bibr" target="#b12">(Szyld, 2006)</ref>). Thus Equation 5 holds.</p><p>In order to evaluate the norm in terms of small size matrices, one will use the following Lemma on the pro-</p><formula xml:id="formula_23">jection matrix Π L ′ X = Φπ L ′ X : Lemma 1 (Yu &amp; Bertsekas (2008)) Let Y be an N × m matrix, and Z a m × N matrix, then Y Z 2 ξ = σ (Y ′ ΞY )(ZΞ −1 Z ′ ) .</formula><p>Thus,</p><formula xml:id="formula_24">Π L ′ X 2 ξ = Φπ L ′ X 2 ξ = σ[(Φ ′ ΞΦ)(π L ′ X Ξ −1 (π L ′ X ) ′ )] = σ[Φ ′ ΞΦ(X ′ LΦ) −1 X ′ LΞ −1 L ′ X(Φ ′ L ′ X) −1 ] = σ[ABCB ′ ].</formula><p>Proposition 2 is closely related to the work of <ref type="bibr" target="#b10">(Schoknecht, 2002)</ref>, in which the author derived the following characterization of the TD and BR solutions: This "orthogonal projection" characterization and our "oblique projection" characterization are in fact equivalent. On the one hand for BR, it is immediate to notice that Π ·Q BR = Π L ′ XBR . On the other hand for TD, writing</p><formula xml:id="formula_25">Y = L ′ X T D , one simply needs to notice that Π L ′ XT D = Π Y = Φ(Y ′ Φ) −1 Y ′ = Φ(Y ′ Φ) −1 (Φ ′ Y ) −1 (Φ ′ Y )Y ′ = Φ(Φ ′ Y Y ′ Φ) −1 Φ ′ Y Y ′ = Π ·Q T D .</formula><p>The work of <ref type="bibr" target="#b10">Schoknecht (2002)</ref> suggests that TD and BR are optimal for different criteria, since both look for somê v ∈ span (Φ) that minimizesˆv minimizesˆminimizesˆv − v for some (semi)norm · . Curiously, our result suggests that neither is optimal, since neither uses the best projection direction X * := L ′−1 ΞΦ for whichˆv whichˆ whichˆv X * = Π L ′ X * v = Π ΞΦ v = ˆ v best and this supports the empirical evidence that there is no clear "winner" between TD and BR.</p><p>Our main results, stated in Propositions 2 and 3, constitutes a revisit of the work of <ref type="bibr" target="#b16">Yu &amp; Bertsekas (2008)</ref>, where the authors similarly derived error bounds for TD and BR. Our approach mimicks theirs: 1) we derive a linear relation between the projectionˆvtionˆ tionˆv, the real value v and the best projectionˆvprojectionˆ projectionˆv best , then 2) analyze the norm of the matrices involved in this relation in terms of spectral radius of small matrices (through Lemma 1, which is taken from <ref type="bibr" target="#b16">(Yu &amp; Bertsekas, 2008)</ref>). From a purely quantitative point of view, our bounds are identical to the ones derived there. Two immediate consequences of this quantitative equivalence are that, as in <ref type="bibr" target="#b16">(Yu &amp; Bertsekas, 2008)</ref>, (1) our bound is tight in the sense that there exists a worst choice for the reward for which it holds with equality, and (2) it is always better than that of Equation 3 from <ref type="bibr" target="#b1">Bertsekas &amp; Tsitsiklis (1996)</ref>; <ref type="bibr" target="#b14">Tsitsiklis &amp; Van Roy (1997)</ref>. However, our work is qualitatively different: by highlighting the oblique projection relation betweenˆvbetweenˆ betweenˆv and v, not only do we provide a clear geometric intuition for both methods, but we also greatly simplify the form of the results and their proofs (see <ref type="bibr" target="#b16">(Yu &amp; Bertsekas, 2008</ref>) for details).</p><p>Last but not least, there is globally a significant difference between our work and the two works we have just mentionned. The analysis we propose is unified for TD and BR (and even extends to potential new methods through other choices of the parameter X), while the results in <ref type="bibr" target="#b10">(Schoknecht, 2002)</ref> and <ref type="bibr" target="#b16">(Yu &amp; Bertsekas, 2008)</ref> are proved independently for each method. We hope that our unified approach will help understanding better the pros and cons of TD, BR, and related alternative approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">An Empirical Comparison</head><p>In order to further compare the TD and the BR projections, we have made some empirical comparison, which we describe now. We consider spaces of dimensions n = 2, 3, .., 30. For each n, we consider projections of dimensions k = 1, 2, .., n. For each (n, k) couple, we generate 20 random projections (through random matrices 10 Φ of size (n, k) and random weight vectors ξ) and 20 random (uncontrolled) chain like MDP: from each state i, there is a probability p i (chosen randomly uniformly on (0, 1)) to get to state i + 1 and a probability 1 − p i to stay in i (the last state is absorbing); Using this raw data on 20 × 20 problems, we compute for each (n, k) couple some statistics, which we describe now. All the graphs that we display shows the dimension of the space N and of the projected space m on the x − y axes. The z axis correspond to the different statistics of interest. consistently greater than 1 2 , which means that the TD method is usually better than the BR method. <ref type="figure">Figure  3</ref> presents the ratio of time the bounds we have presented in Propostion 4 correctly guesses which method is the best (i.e. the expectation of the indicator func-</p><formula xml:id="formula_26">tion of [e T D &lt; e BR ] = [b T D &lt; b BR ])</formula><p>. Unless the feature space dimension is close to the state space dimension, the bounds do not appear very useful for such a decision. <ref type="figure" target="#fig_3">Figure 4</ref> displays the expectation of e T D /e BR . One can observe that, on average, this expectation is bigger than 1, that is the BR tends to be better, on average, than the TD error. This may look contradictory with our interpretation of <ref type="figure" target="#fig_4">Figure  2</ref>, but the explanation is the following: when the BR method is better than the TD method, it is by a larger gap than when it is the other way round. We believe this corresponds to the situation when the TD method in unstable. <ref type="figure">Figure 5</ref> allows to confirm this point: it shows the expectation of the relative approximation errors with respect to the best possible error, that is the expectation of e T D /e and e BR /e. One observes on all charts that this average relative quality of the TD fix point has lots of pikes (corresponding to numerical instabilities), while that of the BR method is smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We have presented the TD fix point and the BR minimization methods for approximating the value of some MDP fixed policy. We have described two original examples: in the former, the BR method is consistently better than the TD method, while the latter (which generalizes the spirit of the example of <ref type="bibr" target="#b11">Sutton et al. (2009)</ref>) is best treated by TD. Proposition 1 highlights the close relation between the objective criteria that correspond to both methods. It shows that minimizing the BR implies minimizing the TD error and some extra "adequacy" term, which happens to be crucial for numerical stability.</p><p>Our main contribution, stated in Proposition 2, provides a new viewpoint for comparing the two projection methods, and potential ideas for alternatives. Both TD and BR can be characterized as solving a projected fixed point equation and this is to our knowledge new for BR. Also, the solutions to both methods are some oblique projection of the value v and this is to our knowledge new for TD and BR. Eventually, this simple geometric characterization allows to derive some tight error bounds (Proposition 3). We have discussed the close relations of our results with those of <ref type="bibr" target="#b10">Schoknecht (2002)</ref> and <ref type="bibr" target="#b16">Yu &amp; Bertsekas (2008)</ref>, and argued that our work simplifies and extends them. Though apparently new to the Reinforcement Learning community, the very idea of oblique projections of fixed point equations has been studied in the Numerical Analysis community (see e.g. <ref type="bibr" target="#b9">Saad (2003)</ref>). In the future, we plan to study more carefully this literature, and particularly investigate whether it may further contribute to the MDP context.</p><p>Concerning the practical question of choosing among the two methods TD and BR, the situation can be summarized as follows: the BR method is sounder than the TD method, since the former has a performance guarantee while the latter will never have one in general. Extensive simulations (on random chainlike problems of size up to 30 states, and for many projection of all the possible space sizes) further suggest the following facts: (a) the TD solution is more often better than the BR solution; (b) however sometimes, TD failed dramatically; (c) overall, this makes BR better on average. Equivalently, one may say that TD is more risky than BR.</p><p>Even if TD is more risky, there remains several reasons inria-00537403, version <ref type="bibr">1 -19 Nov 2010</ref> why one may want to use it in practice, and which our study did not focus on. In large scale problems, one usually estimates the m × m linear systems through sampling. Sampling based methods for BR are more constraining since they generally require double sampling. Independently, the fact, highlighted by Propostion 1, that the BR is an upper bound of the TD error, suggests two things. First, we believe that the variance of the BR problem is higher than that of the TD problem; thus, given a fixed amount of samples, the TD solution might be less affected by the corresponding stochastic noise than the BR one. More generally, the BR problem may be harder to solve than the TD problem, and from a numerical viewpoint, the latter may provide better solutions. Eventually, we only discussed the TD(0) fix point method, that is the specific variant of TD(λ) <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996;</ref><ref type="bibr" target="#b2">Boyan, 2002</ref>) where λ = 0. Values of λ &gt; 0 solve some of the weaknesses of TD <ref type="formula">(0)</ref>: it can be show that the stability issues disappear for values of λ close to 1, and the optimal projectionˆvprojectionˆ projectionˆv best is obtained when λ = 1. Further analytical and empirical comparisons of TD(λ) with the algorithms we have considered here (and with some "BR(λ)" algorithm) constitute future research.</p><p>Eventually, a somewhat disappointing observation of our study is that the bounds of Proposition 3, which are the tightest possible bounds independent of the reward function, did not prove useful for deciding a priori which of the two methods one should trust better (recall the results showed in <ref type="figure">Figure 3</ref>). Extending them in a way that would take the reward into account, as well as trying to exploit our original unified vision of the bounds (Propositions 2 and 3) are some potential tracks for improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Appearing in Proceedings of the 27 th International Confer- ence on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Denote the rewards r 1 and r 2 . One thus have v(1) = r 1 + γr2 1−γ and v(2) = r2 1−γ . Consider the one-feature linear approximation with Φ = (1 2) ′ , with uniform distribution ξ = (.5 .5) ′ . Φ ′ ΞΦ =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 4 (</head><label>4</label><figDesc>Schoknecht (2002)) The TD fix point computation and the BR minimization are or- thogonal projections of the value v respectively induced by the seminorm · QT D 9 with Q T D = L ′ ΞΦΦ ′ ΞL and by the norm · QBR with Q BR = L ′ ΞL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. TD win ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Prediction of the best method through Prop. 3 γ = 0.9 E[TD_err/BR_err] 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 showsFigure 5 .</head><label>25</label><figDesc>Figure 2 shows the proportion of sampled problems where TD method returns a better approximation than BR (i.e. the expectation of the indicator function of e T D &lt; e BR ). It turns out that this ratio is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>fix point method The principle of the TD(0) method (TD for short) is to look for a</head><label></label><figDesc></figDesc><table>fixed 
point of ΠT , that is, one looks forˆvforˆ forˆv T D in the space 
span (Φ) satisfyingˆvsatisfyingˆ satisfyingˆv T D = ΠTˆvΠTˆ ΠTˆv T D . Assuming that 
the matrix inverse below exists 1 , it can be proved 2 
thatˆvthatˆ thatˆv T D = Φw T D with 

w T D = (Φ ′ ΞLΦ) −1 Φ ′ Ξr 
(1) 

As 
pointed 
out 
by 
Antos et al. 
(2008); 
Farahmand et al. (2008); </table></figure>

			<note place="foot" n="1"> This is not necessary the case, as the forthcoming Example 1 (Section 2) shows. 2 Section 4 will generalize this derivation. 3 This remark is also true if we replace · ξ by any equivalent norm · . This observation lead Sutton et al. (2009) to propose original off-policy gradient algorithms for computing the TD solution. inria-00537403, version 1 -19 Nov 2010</note>

			<note place="foot" n="4"> The rest of this section is strongly inspired by a personal communication with Yu. inria-00537403, version 1 -19 Nov 2010</note>

			<note place="foot" n="9"> This is a seminorm because the matrix QT D is only semidefinite (since ΦΦ ′ has rank smaller than m &lt; N ). The corresponding projection can still be well defined (i.e. each point has exactly one projection) provided that span (Φ) ∩ {x; xQ T D = 0} = {0}. inria-00537403, version 1 -19 Nov 2010</note>

			<note place="foot" n="10"> Each entry is a random uniform number between -1 and 1.</note>

			<note place="foot">inria-00537403, version 1 -19 Nov 2010</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowlegments</head><p>The author would like to thank Janey Yu for helpful discussions, and the anonymous reviewers for providing comments that helped to improve the presentation of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="89" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neurodynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Technical update: Least-squares temporal difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="233" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularized policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stable function approximation in dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Max-norm projections for factored mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Least-squares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Error bounds for approximate policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finite-time bounds for fitted value iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="815" to="857" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Iterative Methods for Sparse Linear Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Philadelpha, PA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition. SIAM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimality of reinforcement learning algorithms with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schoknecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1555" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast gradient-descent methods for temporaldifference learning with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The many proofs of an identity on the norm of oblique projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Szyld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Algorithms</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="309" to="323" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minkowski Geometry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tight performance bounds on greedy policies based on imperfect value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>College of Computer Science, Northeastern University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">New error bounds for approximations from projected linear equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-07" />
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Univ. of Helsinki</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report C-2008-43</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
