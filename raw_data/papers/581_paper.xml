<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Weight Learning for Markov Logic Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
							<email>lowd@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195-2350</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
							<email>pedrod@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195-2350</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Weight Learning for Markov Logic Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Markov logic networks (MLNs) combine Markov networks and first-order logic, and are a powerful and increasingly popular representation for statistical relational learning. The state-of-the-art method for discriminative learning of MLN weights is the voted perceptron algorithm , which is essentially gradient descent with an MPE approximation to the expected sufficient statistics (true clause counts). Unfortunately, these can vary widely between clauses, causing the learning problem to be highly ill-conditioned, and making gradient descent very slow. In this paper, we explore several alternatives, from per-weight learning rates to second-order methods. In particular, we focus on two approaches that avoid computing the partition function: diagonal Newton and scaled conjugate gradient. In experiments on standard SRL datasets, we obtain order-of-magnitude speedups, or more accurate models given comparable learning times.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical relational learning (SRL) focuses on domains where data points are not i.i.d. (independent and identically distributed). It combines ideas from statistical learning and inductive logic programming, and interest in it has grown rapidly in recent years <ref type="bibr" target="#b5">[6]</ref>. One of the most powerful representations for SRL is Markov logic, which generalizes both Markov random fields and first-order logic <ref type="bibr" target="#b15">[16]</ref>. Representing a problem as a Markov logic network (MLN) involves simply writing down a list of first-order formulas and learning weights for those formulas from data. The first step is the task of the knowledge engineer; the second is the focus of this paper.</p><p>Currently, the best-performing algorithm for learning MLN weights is Singla and Domingos' voted perceptron <ref type="bibr" target="#b18">[19]</ref>, based on Collins' earlier one <ref type="bibr" target="#b2">[3]</ref> for hidden Markov models. Voted perceptron uses gradient descent to approximately optimize the conditional likelihood of the query atoms given the evidence. Weight learning in Markov logic is a convex optimization problem, and thus gradient descent is guaranteed to find the global optimum. However, convergence to this optimum may be extremely slow. MLNs are exponential models, and their sufficient statistics are the numbers of times each clause is true in the data. Because this number can easily vary by orders of magnitude from one clause to another,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Markov Logic</head><p>A Markov logic network (MLN) consists of a set of first-order formulas and their weights, {(w i , f i )}. Intuitively, a formula represents a noisy relational rule, and its weight represents the relative strength or importance of that rule. Given a finite set of constants, we can instantiate an MLN as a Markov random field (MRF) in which each node is a grounding of a predicate (atom) and each feature is a grounding of one of the formulas (clauses). This leads to the following joint probability distribution for all atoms:</p><formula xml:id="formula_0">P (X = x) = 1 Z exp i w i n i (x)</formula><p>where n i is the number of times the ith formula is satisfied by the state of the world x and Z is a normalization constant, required to make the probabilities of all worlds to sum to one.</p><p>The formulas in an MLN are typically specified by an expert, or they can be obtained (or refined) by inductive logic programming or MLN structure learning <ref type="bibr" target="#b9">[10]</ref>. Many complex models, and in particular many non-i.i.d. ones, can be very compactly specified using MLNs.</p><p>Exact inference in MLNs is intractable. Instead, we can perform approximate inference using Markov chain Monte Carlo (MCMC), and in particular Gibbs sampling <ref type="bibr" target="#b6">[7]</ref>. However, when weights are large convergence can be very slow, and when they are infinite (corresponding to deterministic dependencies) ergodicity breaks down. This remains true even for more sophisticated alternatives like simulated tempering. A much more efficient alternative, which also preserves ergodicity in the presence of determinism, is the MC-SAT algorithm, recently introduced by Poon and Domingos <ref type="bibr" target="#b14">[15]</ref>. MC-SAT is a "slice sampling" MCMC algorithm that uses a modified satisfiability solver to sample from the slice. The solver is able to find isolated modes in the distribution very efficiently, and as a result the Markov chain mixes very rapidly. The slice sampling scheme ensures that detailed balance is (approximately) preserved. In this paper we use MC-SAT for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weight Learning for MLNs</head><p>Given a set of formulas and a database of atoms, we wish to find the formulas' maximum a posteriori (MAP) weights, i.e., the weights that maximize the product of their prior probability and the data likelihood. Since optimization is typically posed as function minimization, we will equivalently minimize the negative log-likelihood.</p><p>Richardson and Domingos <ref type="bibr" target="#b15">[16]</ref> originally proposed learning weights generatively using pseudo-likelihood <ref type="bibr" target="#b1">[2]</ref>. Pseudo-likelihood is the product of the conditional likelihood of each variable given the values of its neighbors in the data. While efficient for learning, it can give poor results when long chains of inference are required at query time. Singla and Domingos <ref type="bibr" target="#b18">[19]</ref> showed that pseudo-likelihood is consistently outperformed by discriminative training, which minimizes the negative conditional likelihood of the query predicates given the evidence ones. Thus, in this paper we focus on this type of learning. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voted Perceptron</head><p>Gradient descent algorithms use the gradient, g, scaled by a learning rate, η, to update the weight vector w in each step:</p><formula xml:id="formula_1">w t+1 = w t − ηg</formula><p>In an MLN, the derivative of the negative conditional log-likelihood (CLL) with respect to a weight is the difference of the expected number of true ground-ings of the corresponding clause and the actual number according to the data:</p><formula xml:id="formula_2">∂ ∂w i − log P (Y = y|X = x) = E w [n i ] − n i</formula><p>where y is the state of the non-evidence atoms in the data, and x is the state of the evidence. The basic idea of the voted perceptron (VP) algorithm <ref type="bibr" target="#b2">[3]</ref> is to approximate the intractable expectations E w [n i ] with the counts in the most probable explanation (MPE) state, which is the most probable state of non-evidence atoms given the evidence. To combat overfitting, instead of returning the final weights, VP returns the average of the weights from all iterations of gradient descent.</p><p>Collins originally proposed VP for training hidden Markov models discriminatively, and in this case the MPE state is unique and can be computed exactly in polynomial time using the Viterbi algorithm. In MLNs, MPE inference is intractable but can be reduced to solving a weighted maximum satisfiability problem, for which efficient algorithms exist such as MaxWalkSAT <ref type="bibr" target="#b8">[9]</ref>. Singla and Domingos <ref type="bibr" target="#b18">[19]</ref> use this approach and discuss how the resulting algorithm can be viewed as approximately optimizing log-likelihood. However, the use of voted perceptron in MLNs is potentially complicated by the fact that the MPE state may no longer be unique, and MaxWalkSAT is not guaranteed to find it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Divergence</head><p>The contrastive divergence (CD) algorithm is identical to VP, except that it approximates the expectations E w [n i ] from a small number of MCMC samples instead of using the MPE state. Using MCMC is presumably more accurate and stable, since it converges to the true expectations in the limit. While running an MCMC algorithm to convergence at each iteration of gradient descent is infeasibly slow, Hinton <ref type="bibr" target="#b7">[8]</ref> has shown that a few iterations of MCMC yield enough information to choose a good direction for gradient descent. Hinton named this method contrastive divergence, because it can be interpreted as optimizing a difference of Kullback-Leibler divergences. Contrastive divergence can also be seen as an efficient way to approximately optimize log-likelihood.</p><p>The MCMC algorithm typically used with contrastive divergence is Gibbs sampling, but for MLNs the much faster alternative of MC-SAT is available. Because successive samples in MC-SAT are much less correlated than successive sweeps in Gibbs sampling, they carry more information and are likely to yield a better descent direction. In particular, the different samples are likely to be from different modes, reducing the error and potential instability associated with choosing a single mode.</p><p>In our experiments, we found that five samples were sufficient, and additional samples were not worth the time: any increased accuracy that 10 or 100 samples might bring was offset by the increased time per iteration. We avoid the need for burn-in by starting at the last state sampled in the previous iteration of gradient descent. (This differs from Hinton's approach, which always starts at the true values in the training data.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Per-Weight Learning Rates</head><p>VP and CD are both simple gradient descent procedures, and as a result highly vulnerable to the problem of ill-conditioning. Ill-conditioning occurs when the condition number, the ratio between the largest and smallest absolute eigenvalues of the Hessian, is far from one. On ill-conditioned problems, gradient descent is very slow, because no single learning rate is appropriate for all weights. In MLNs, the Hessian is the negative covariance matrix of the clause counts. Because some clauses can have vastly greater numbers of true groundings than others, the variances of their counts can be correspondingly larger, and ill-conditioning becomes a serious issue.</p><p>One solution is to modify both algorithms to have a different learning rate for each weight. Since tuning every learning rate separately is impractical, we use a simple heuristic to assign a learning rate to each weight:</p><formula xml:id="formula_3">η i = η n i</formula><p>where η is the user-specified global learning rate and n i is the number of true groundings of the ith formula. (To avoid dividing by zero, if n i = 0 then η i = η.) When computing this number, we ignore the groundings that are satisfied by the evidence (e.g., A ⇒ B when A is false). This is because, being fixed, they cannot contribute to the variance. We refer to the modified versions of VP and CD as VP-PW and CD-PW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diagonal Newton</head><p>When the function being optimized is quadratic, Newton's method can move to the global minimum or maximum in a single step. It does so by multiplying the gradient, g, by the inverse Hessian, H −1 :</p><formula xml:id="formula_4">w t+1 = w t − H −1 g</formula><p>When there are many weights, using the full Hessian becomes infeasible. A common approximation is to use the diagonal Newton (DN) method, which uses the inverse of the diagonalized Hessian in place of the inverse Hessian. DN typically uses a smaller step size than the full Newton method. This is important when applying the algorithm to non-quadratic functions, such as MLN negative CLL, where the quadratic approximation is only good within a local region.</p><p>The Hessian of the negative CLL for an MLN is simply the covariance matrix:</p><formula xml:id="formula_5">∂ ∂w i ∂w j − log P (Y = y|X = x) = E w [n i n j ] − E w [n i ]E w [n j ]</formula><p>Like the gradient, this can be estimated using samples from MC-SAT. In each iteration, we take a step in the diagonalized Newton direction:</p><formula xml:id="formula_6">w i = w i − α E w [n i ] − n i E w [n 2 i ] − (E w [n i ]) 2</formula><p>The step size α could be computed in a number of ways, including keeping it fixed, but we achieved the best results using the following method. Given a search direction d and Hessian matrix H, we compute the step size as follows:</p><formula xml:id="formula_7">α = −d T g d T Hd + λd T d</formula><p>where d is the search direction. For a quadratic function and λ = 0, this step size would move to the minimum function value along d. Since our function is not quadratic, a non-zero λ term serves to limit the size of the step to a region in which our quadratic approximation is good. After each step, we adjust λ to increase or decrease the size of the so-called model trust region based on how well the approximation matched the function. Let ∆ actual be the actual change in the function value, and let ∆ pred be the predicted change in the function value from the previous gradient and Hessian and our last step, d t−1 :</p><formula xml:id="formula_8">∆ pred = d T t−1 g t−1 + 1/2 d T t−1 H t−1 d t−1 A standard method for adjusting λ is as follows [5]: if (∆ actual /∆ pred &gt; 0.75) then λ t+1 = λ t /2 if (∆ actual /∆ pred &lt; 0.25) then λ t+1 = 4λ t</formula><p>Since we cannot efficiently compute the actual change in negative CLL, we approximate it as the product of the step we just took and the gradient after taking it:</p><formula xml:id="formula_9">∆ actual = d T t−1 g t .</formula><p>Since the negative CLL is a convex function, this product is an upper bound on the actual change. When this value is positive our CLL may be worse than before, so the step is rejected and redone after adjusting λ.</p><p>In models with thousands of weights or more, storing the entire Hessian matrix becomes impractical. However, when the Hessian appears only inside a quadratic form, as above, the value of this form can be computed simply as:</p><formula xml:id="formula_10">d T Hd = E w [( i d i n i ) 2 ] − (E w [ i d i n i ]) 2</formula><p>The product of the Hessian by a vector can also be computed compactly <ref type="bibr" target="#b13">[14]</ref>. Note that α is computed using the full Hessian matrix, but the step direction is computed from the diagonalized approximation which is easier to invert.</p><p>Our per-weight learning rates can actually be seen as a crude approximation of the diagonal Newton method. The number of true groundings not satisfied by evidence is a heuristic approximation to the count variance, which the diagonal Newton method uses to rescale each dimension of the gradient. The diagonal Newton method, however, can adapt to changes in the second derivative at different points in the weight space. Its main limitation is that clauses can be far from uncorrelated. The next method addresses this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Scaled Conjugate Gradient</head><p>Gradient descent can be sped up by, instead of taking a small step of constant size at each iteration, performing a line search to find the optimum along the chosen descent direction. However, on ill-conditioned problems this is still inefficient, because line searches along successive directions tend to partly undo the effect of each other: each line search makes the gradient along its direction zero, but the next line search will generally make it non-zero again. In long narrow valleys, instead of moving quickly to the optimum, gradient descent zigzags.</p><p>A solution to this is to impose at each step the condition that the gradient along previous directions remain zero. The directions chosen in this way are called conjugate, and the method conjugate gradient <ref type="bibr" target="#b17">[18]</ref>. We used the PolakRibiere method for choosing conjugate gradients since it has generally been found to be the best-performing one. Conjugate gradient methods are some of the most efficient available, on a par with quasi-Newton ones. Unfortunately, applying them to MLNs is difficult, because line searches require computing the objective function, and therefore the partition function Z, which is highly intractable. (Computing Z is equivalent to computing all moments of the MLN, of which the gradient and Hessian are the first two.) Fortunately, we can use the Hessian instead of a line search to choose a step size. This method is known as scaled conjugate gradient (SCG), and was originally proposed by <ref type="bibr">Møller [12]</ref> for training neural networks. In our implementation, we choose a step size the same way as in diagonal Newton.</p><p>Conjugate gradient is usually more effective with a preconditioner, a linear transformation that attempts to reduce the condition number of the problem (e.g., <ref type="bibr" target="#b16">[17]</ref>). Good preconditioners approximate the inverse Hessian. We use the inverse diagonal Hessian as our preconditioner. We refer to SCG with the preconditioner as PSCG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our experiments used two standard relational datasets representing two important relational tasks: Cora for entity resolution, and WebKB for collective classification.</p><p>The Cora dataset consists of 1295 citations of 132 different computer science papers, drawn from the Cora Computer Science Research Paper Engine. This dataset was originally labeled by Andrew McCallum 2 . We used a cleaned version from Singla and Domingos <ref type="bibr" target="#b19">[20]</ref>, with five splits for cross-validation.</p><p>The task on Cora is to predict which citations refer to the same paper, given the words in their author, title, and venue fields. The labeled data also specifies which pairs of author, title, and venue fields refer to the same entities. In our experiments, we evaluated the ability of the model to deduplicate fields as well as citations. Since the number of possible equivalances is very large, we used the canopies found by Singla and Domingos <ref type="bibr" target="#b19">[20]</ref> to make this problem tractable.</p><p>The MLN we used for this is very similar to the "MLN(B+C+T)" model used by Singla and Domingos <ref type="bibr" target="#b19">[20]</ref>. Its formulas link words to citation identity, words to field identity, and field identity to citation identity. In this way, word co-occurrence affects the probability that two citations are the same both indirectly, through field similarities, and directly. These rules are repeated for each word appearing in the database so that individualized weights can be learned, representing the relative importance of each word in each context. This model also features transitive closure for all equivalence predicates.</p><p>We did two things differently from Singla and Domingos. First, we added rules that relate words to field identity but apply equally to all words. Because these rules are not specific to particular words, they can potentially improve generalization and reduce overfitting. Secondly, we learned weights for all rules. Singla and Domingos set the weights for all word-specific rules using a naive Bayes model, and only learned the other rules' weights using VP. Our learning problem is therefore much harder and more ill-conditioned, but our more powerful algorithms enabled us to achieve the best results to date on Cora.</p><p>In our version, the total number of weights is 6141. During learning, the number of ground clauses exceeded 3 million.</p><p>The WebKB dataset consists of labeled web pages from the computer science departments of four universities. We used the relational version of the dataset from Craven and Slattery <ref type="bibr" target="#b3">[4]</ref>, which features 4165 web pages and 10,935 web links, along with the words on the webpages, anchors of the links, and neighborhoods around each link.</p><p>Each web page is marked with some subset of the categories: person, student, faculty, professor, department, research project, and course. Our goal is to predict these categories from the web pages' words and link structures.</p><p>We used a very simple MLN for this model, consisting only of formulas linking words to page classes, and page classes to the classes of linked pages. The "wordclass" rules were of the following form:</p><p>Has(page,word) ⇒ Class(page,class) ¬Has(page,word) ⇒ Class(page,class)</p><p>We learned a separate weight for each of these rules for each (word, class) pair. Classes of linked pages were related by the formula:</p><formula xml:id="formula_11">Class(page1,class1) ∧ LinksTo(page1,page2) ⇒ Class(page2,class2)</formula><p>We learned a separate weight for this rule for each pair of classes. When instantiated for each word and class, the model contained 10,891 weights. While simple to write, this model represents a complex, non-i.i.d. probability distribution in which query predicates are linked in a large graph. During learning, the number of ground clauses exceeded 300,000.</p><p>We estimated the condition number for both Cora and WebKB at the point where all weights are zero. (Because our learning problem is not quadratic, the condition number depends on the current weights.) The size of these problems makes computing the condition number of the full Hessian matrix difficult, but we can easily compute the condition number of the diagonalized Hessian, which is simply the largest ratio of two clause variances. For Cora, this was over 600,000, while for WebKB it was approximately 7000. This indicates that both learning problems are ill-conditioned, but Cora is much worse than WebKB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>To score our models, we ran MC-SAT for 100 burn-in and 1000 sampling iterations on the test data. The marginal conditional probability of each query atom is the fraction of samples in which the atom was true with a small prior to prevent zero counts.</p><p>From these marginal probabilities, we estimate conditional log-likelihood (CLL) by averaging the log marginal probabilities of the true values of the query predicates. CLL is the metric all of the algorithms attempt to optimize. However, in cases such as entity resolution where the class distribution is highly skewed, CLL can be a poor metric. For this reason, we also look at AUC, the area under the precision-recall curve. The disadvantage of AUC is that it ignores calibration: AUC only considers whether true atoms are given higher probability than false atoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methodology</head><p>We ran our experiments using five-way cross-validation for Cora and four-way cross-validation for WebKB. For each train/test split, one of the training datasets was selected as a validation set and the remaining ones formed the tuning set. The tuning procedure consisted of training each algorithm for four hours on the tuning sets with various values of the learning rate. For each algorithm on each split, we chose the learning rates that worked best on the corresponding validation set for each evaluation metric.</p><p>We used the implementation of voted perceptron for MLNs in the Alchemy package <ref type="bibr" target="#b10">[11]</ref>, and implemented the other algorithms as extensions of Alchemy. For DN, SCG, and PSCG, we started with λ = 1 and let the algorithm adjust it automatically. For algorithms based on MC-SAT, we used 5 samples of MC-SAT for each iteration of the learning algorithm. The width of the Gaussian prior was set for each dataset based on preliminary experiments.</p><p>After tuning all algorithms, we reran them for 10 hours with their respective training sets, including the held-out validation data. For the gradient descent algorithms, we averaged the weights from all iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Our results for the Cora and WebKB datasets are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Error bars are omitted for clarity; at the final data point, all differences exceed twice the standard error. For AUC, we computed the standard deviation using the technique of Richardson and Domingos <ref type="bibr" target="#b15">[16]</ref>.</p><p>PSCG is the most accurate of all the algorithms compared, obtaining the best CLL and AUC on both Cora and WebKB. It converges relatively quickly as well: on WebKB, the PSCG learning curve dominates all others after 2 minutes; on Cora, it dominates after 15 minutes. DN is consistently close behind PSCG in CLL and AUC, briefly doing better when PSCG starts to overfit. In contrast, VP and CD consistently converge more slowly to worse AUC and CLL.</p><p>On Cora, the algorithms that adjust the search direction using true clause counts or count variance do much better than those that do not. This suggests that these techniques help greatly in cases of extreme ill-conditioning. Without a preconditioner, even SCG does poorly. This is because, like VP and CD, the first step it takes is in the direction of the gradient. On a very ill-conditioned dataset like Cora, the gradient is a very poor choice of search direction.</p><p>The AUC results we show for Cora are for all query predicates-SameAuthor, SameVenue, SameTitle, and SameBib. When computing the AUC for just the SameBib predicate, PSCG reaches a high of 0.992 but ends at 0.990 after overfitting slightly. DN and CD-PW do about the same, ending at AUCs of 0.992 and 0.991, respectively. All of these algorithms exceed the 0.988 AUC reported by Singla and Domingos <ref type="bibr" target="#b19">[20]</ref>, the best previously published result on this dataset, and they do so by more than twice the standard error.</p><p>On WebKB, the ill-conditioning is less of an issue. PSCG still does better than SCG, but not drastically better. VP-PW and CD-PW actually do worse than VP and CD. This is because the per-weight learning rates are much smaller for the relational rules than the word-specific rules. This makes the relational rules converge much more slowly than they should.</p><p>The performance of some of the algorithms sometimes degrades with additional learning time. For some of the algorithms, such as PSCG, DN, and VP-PW on Cora, this is simply a symptom of overfitting. More careful tuning or a better prior could help correct this. But for other algorithms, such as SCG and VP on Cora, the later models perform worse on training data as well. For SCG, this seems to be the result of noisy inference and very ill-conditioned problems, which cause even a slight error in the step direction to potentially have a significant effect. Our lower bound on the improvement in log-likelihood prevents this in theory, but in practice a noisy gradient may still cause us to take bad steps. PSCG suffers much less from this effect, since the preconditioning makes the learning problem better behaved. For VP and CD, the most likely cause is learning rates that are too high. Our tuning experiments selected the learning rates that worked best after four hours on a smaller set of data. The increased amount of data in the test scenario increased the magnitude of the gradients, making these learning rates less stable than they were in the tuning scenario. This extreme sensitivity to learning rate makes learning good models with VP and CD much more difficult. We also experimented with the stochastic meta-descent algorithm <ref type="bibr" target="#b20">[21]</ref>, which automatically adjusts learning rates in each dimension, but found it to be too unstable for these domains.</p><p>In sum, the MLN weight learning methods we have introduced in this paper greatly outperform the voted perceptron. Given similar learning time, they learn much more accurate models; and, judging from the curves in <ref type="figure" target="#fig_0">Figure 1</ref>, running VP until it reaches the same accuracy as the better algorithms would take an extremely long time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Weight learning for Markov logic networks can be extremely ill-conditioned, making simple gradient descent-style algorithms very slow to converge. In this paper we studied a number of more sophisticated alternatives, of which the best-performing one is preconditioned scaled conjugate gradient. This can be attributed to its effective use of second-order information. However, the simple heuristic of dividing the learning rate by the true clause counts for each weight can sometimes give very good results. Using one of these methods instead of gradient descent can yield a much better model in less time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CLL and AUC for Cora (above) and WebKB (below). Learning times are shown on a logarithmic scale.</figDesc></figure>

			<note place="foot" n="1"> For simplicity, we omit prior terms throughout; in our experiments, we use a zeromean Gaussian prior on all weights with all algorithms.</note>

			<note place="foot" n="2"> http://www.cs.umass.edu/∼mccallum/data/cora-refs.tar.gz</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving the convergence of back-propagation learning with second order methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1988 Connectionist Models Summer School</title>
		<meeting>1988 Connectionist Models Summer School</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CEMNLP-2002</title>
		<meeting>CEMNLP-2002</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relational learning with statistical predicate invention: Better models for hypertext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="97" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Practical Methods of Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Relational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Markov Chain Monte Carlo in Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general stochastic approach to solving problems with hard and soft constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Satisfiability Problem: Theory and Applications</title>
		<editor>D. Du, J. Gu, and P. M. Pardalos</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning the structure of Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML-2005</title>
		<meeting>ICML-2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Alchemy system for statistical relational AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<ptr target="http://alchemy.cs.washington.edu/" />
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Seattle, WA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, University of Washington</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A scaled conjugate gradient algorithm for fast supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Møller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="525" to="533" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the Hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sound and efficient inference with probabilistic and deterministic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI-2006</title>
		<meeting>AAAI-2006</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-2003</title>
		<meeting>ACL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shewchuck</surname></persName>
		</author>
		<idno>CMU-CS-94-125</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative training of Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI-2005</title>
		<meeting>AAAI-2005</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="868" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Entity resolution with Markov logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM-2006</title>
		<meeting>ICDM-2006</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerated training of conditional random fields with stochastic gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML-2006</title>
		<meeting>ICML-2006</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
