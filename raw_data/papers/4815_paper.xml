<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Effectiveness of Lloyd-type Methods for the k-Means Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafail</forename><surname>Ostrovsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Rabani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Swamy</surname></persName>
						</author>
						<title level="a" type="main">The Effectiveness of Lloyd-type Methods for the k-Means Problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate variants of Lloyd&apos;s heuristic for clustering high dimensional data in an attempt to explain its popularity (a half century after its introduction) among practitioners, and in order to suggest improvements in its application. We propose and justify a clusterability criterion for data sets. We present variants of Lloyd&apos;s heuristic that quickly lead to provably near-optimal clustering solutions when applied to well-clusterable instances. This is the first performance guarantee for a variant of Lloyd&apos;s heuristic. The provision of a guarantee on output quality does not come at the expense of speed: some of our algorithms are candidates for being faster in practice than currently used variants of Lloyd&apos;s method. In addition, our other algorithms are faster on well-clusterable instances than recently proposed approximation algorithms, while maintaining similar guarantees on clustering quality. Our main algo-rithmic contribution is a novel probabilistic seeding process for the starting configuration of a Lloyd-type iteration.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Overview. There is presently a wide and unsatisfactory gap between the practical and theoretical clustering literatures. For decades, practitioners have been using heuristics of great speed but uncertain merit; the latter should not be surprising since the problem is NP-hard in almost any formulation. However, in the last few years, algorithms researchers have made considerable innovations, and even obtained polynomialtime approximation schemes (PTAS's) for some of the most popular clustering formulations. Yet these contributions have not had a noticeable impact on practice. Practitioners instead continue to use a variety of heuristics (Lloyd, EM, agglomerative methods, etc.) that have no known performance guarantees.</p><p>There are two ways to approach this disjuncture. The most obvious is to continue developing new techniques until they are so good-down to the implementations-that they displace entrenched methods. The other is to look toward popular heuristics and ask whether there are reasons that justify their extensive use, but elude the standard theoretical criteria; and in addition, whether theoretical scrutiny suggests improvements in their application. This is the approach we take in this paper.</p><p>As in other prominent cases <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b41">42]</ref>, such an analysis typically involves some abandonment of the worst-case inputs criterion. (In fact, part of the challenge is to identify simple conditions on the input, that allow one to prove a performance guarantee of wide applicability.) Our starting point is the notion that (as faster, and also simpler, than the PTAS of <ref type="bibr">Kumar et al. [30]</ref> (applying the separation condition to both algorithms; the latter does not run faster under the condition).</p><p>The problem of minimizing the k-means cost is one of the earliest and most intensively studied formulations of the clustering problem, both because of its mathematical elegance and because it bears closely on statistical estimation of mixture models of k point sources under spherically symmetric Gaussian noise. We briefly survey the most relevant literature here. The k-means problem seems to have been first considered by Steinhaus in 1956 <ref type="bibr" target="#b48">[49]</ref>. A simple greedy iteration to minimize cost was suggested in 1957 by Lloyd <ref type="bibr" target="#b31">[32]</ref> (and less methodically in the same year by <ref type="bibr">Cox [9]</ref>; also apparently by psychologists between 1959-67 <ref type="bibr" target="#b49">[50]</ref>). This and similar iterative descent methods soon became the dominant approaches to the problem <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref> (see also <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref> and the references therein); they remain so today, and are still being improved <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b27">28]</ref>. Lloyd's method (in any variant) converges only to local optima however, and is sensitive to the choice of the initial centers <ref type="bibr" target="#b37">[38]</ref>. Consequently, a lot of research has been directed toward seeding methods that try to start off Lloyd's method with a good initial configuration <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref>. Very few theoretical guarantees are known about Lloyd's method or its variants. The convergence rate of Lloyd's method has recently been investigated in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2]</ref> and in particular, <ref type="bibr" target="#b1">[2]</ref> shows that Lloyd's method can require a superpolynomial number of iterations to converge.</p><p>The k-means problem is NP-hard even for k = 2 <ref type="bibr" target="#b12">[13]</ref>. Recently there has been substantial progress in developing approximation algorithms for this problem. Matoušek <ref type="bibr" target="#b33">[34]</ref> gave the first PTAS for this problem, with running time polynomial in n, for a fixed k and dimension. Subsequently a succession of algorithms have appeared <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> with varying runtime dependency on n, k and the dimension. The most recent of these is the algorithm of Kumar, Sabharwal and Sen <ref type="bibr" target="#b29">[30]</ref>, which presents a linear time PTAS for a fixed k. There are also various constant-factor approximation algorithms for the related k-median problem <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37]</ref>, which also yield approximation algorithms for k-means, and have running time polynomial in n, k and the dimension; recently Kanungo et al. <ref type="bibr" target="#b26">[27]</ref> adapted the k-median algorithm of <ref type="bibr" target="#b2">[3]</ref> to obtain a (9 + )-approximation algorithm for k-means.</p><p>However, none of these methods match the simplicity and speed of the popular Lloyd's method. Researchers concerned with the runtime of Lloyd's method bemoan the need for n nearest-neighbor computations in each descent step <ref type="bibr" target="#b27">[28]</ref> ! Interestingly, the last reference provides a data structure that provably speeds up the nearest-neighbor calculations of Lloyd descent steps, under the condition that the optimal clusters are well-separated. (This is unrelated to providing performance guarantees for the outcome.) Their data structure may be used in any Lloyd-variant, including ours, and is well suited to the conditions under which we prove performance of our method; however, ironically, it may not be worthwhile to precompute their data structure since our method requires so few descent steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We use the following notation throughout. For a point set S, we use ctr(S) to denote the center of mass of S. Let partition X 1 ∪ · · · ∪ X k = X be an optimal k-means clustering of the input X, and let c i = ctr(X i ) and c = ctr(X).</p><formula xml:id="formula_0">So ∆ 2 k (X) = k i=1 x∈X i x − c i 2 = k i=1 ∆ 2 1 (X i )</formula><p>. Let n i = |X i |, n = |X|, and</p><formula xml:id="formula_1">r 2 i = ∆ 2 1 (X i ) n i</formula><p>, that is, r 2 i is the "mean squared error" in cluster X i . Define D i = min j =i c j − c i . We assume throughout that X is -separated for k-means, that is, ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X), where 0 &lt; ≤ 0 with 0 being a suitably small constant. We use the following basic lemmas quite frequently.</p><formula xml:id="formula_2">Lemma 2.1 For every x, y∈X x − y 2 = ∆ 2 1 (X) + nx − c 2 . Hence {x,y}⊆X x − y 2 = n∆ 2 1 (X).</formula><p>Lemma 2.2 Consider any set S ⊆ R d and any partition S 1 ∪ S 2 of S with S 1 = ∅. Let s, s 1 , s 2 denote respectively ctr(S), ctr(S 1 ), ctr(S 2 ). Then, (i)</p><formula xml:id="formula_3">∆ 2 1 (S) = ∆ 2 1 (S 1 ) + ∆ 2 1 (S 2 ) + |S 1 ||S 2 | |S| s 1 − s 2 2 , and (ii) s 1 − s 2 ≤ ∆ 2 1 (S) |S| · |S 2 | |S 1 | .</formula><p>Proof : Let a = |S 1 | and b = |S 2 | = |S| − |S 1 |. We have</p><formula xml:id="formula_4">∆ 2 1 (S) = x∈S 1 x − s 2 + x∈S 2 x − c 2 = ∆ 2 1 (S 1 ) + as 1 − s 2 + ∆ 2 1 (S 2 ) + bs 2 − s 2 (by Lemma 2.1) = ∆ 2 1 (S 1 ) + ∆ 2 1 (S 2 ) + ab a+b · s 1 − s 2 2 .</formula><p>The second equality follows from Lemma 2.1 by noting that s is also the center of mass of the point set where a points are located at s 1 and b points are located at s 2 , and so the optimal 1-means cost of this point set is given by as 1 − s 2 + bs 2 − s 2 . This proves part (i). Part (ii) follows by substituting</p><formula xml:id="formula_5">s 1 − s = s 1 − s 2 · b/(a + b)</formula><p>in part (i) and dropping the ∆ 2 1 (S 1 ) and ∆ 2 1 (S 2 ) terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The 2-means problem</head><p>We first consider the 2-means case. We assume that the input X is -separated for 2-means. We present an algorithm that returns a solution of cost at most 1 + f () ∆ 2 2 (X) in linear time, for a suitably defined function f that satisfies lim →0 f () = 0. An appealing feature of our algorithm is its simplicity, both in description and analysis. In Section 4, where we consider the k-means case, we will build upon this algorithm to obtain both a linear time constant-factor (of the form 1 + f ()) approximation algorithm and a PTAS with running time exponential in k, but linear in n, d.</p><p>The chief algorithmic novelty in our 2-means algorithm is a non-uniform sampling process to pick two seed centers. Our sampling process is very simple: we pick the pair x, y ∈ X with probability proportional to x − y 2 . This biases the distribution towards pairs that contribute a large amount to ∆ 2 1 (X) (noting that n∆ 2 1 (X) = {x,y}⊆X x − y 2 ). We emphasize that, as improving the seeding is the only way to get Lloyd's method to find a high-quality clustering, the topic of picking the initial seed centers has received much attention in the experimental literature (see, e.g., <ref type="bibr" target="#b43">[44]</ref> and references therein). However, to the best of our knowledge, this simple and intuitive seeding method is new to the vast literature on the k-means problem. By putting more weight on pairs that contribute a lot to ∆ 2 1 (X), the sampling process aims to pick the initial centers from the cores of the two optimal clusters. We define the core of a cluster precisely later, but loosely speaking, it consists of points in the cluster that are significantly closer to this cluster-center than to any other center. Lemmas 3.1 and 3.2 make the benefits of this approach precise. Thus, in essence, we are able to leverage the separation condition to nearly isolate the optimal centers. Once we have the initial centers within the cores of the two optimal clusters, we show that a simple Lloyd-like step, which is also simple to analyze, yields a good performance guarantee: we consider a suitable ball around each center and move the center to the centroid of this ball to obtain the final centers. This "ball-k-means" step is adopted from Effros and Schulman <ref type="bibr" target="#b15">[16]</ref>, where it is shown that if the k-means cost of the current solution is small compared to ∆ 2 k−1 (X) (which holds for us since the initial centers lie in the cluster-cores) then a Lloyd step followed by a ball-k-means step yields a clustering of cost close to ∆ 2 k (X). In our case, we are able to eliminate the Lloyd step, and show that the ball-k-means step alone guarantees a good clustering.</p><p>1. Sampling. Randomly select a pair of points from the set X to serve as the initial centers, picking the pair x, y ∈ X with probability proportional to x − y 2 . LetˆcLetˆ Letˆc 1 , ˆ c 2 denote the two picked centers.</p><p>2. "Ball-k-means" step. For eachˆceachˆ eachˆc i , consider the ball of radiusˆcradiusˆradiusˆc 1 − ˆ c 2 /3 aroundˆcaroundˆ aroundˆc i and compute the centroid ¯ c i of the portion of X in this ball. Return ¯ c 1 , ¯ c 2 as the final centers.</p><p>Running time The entire algorithm runs in time O(nd).</p><p>Step 2 clearly takes only O(nd) time. We show that the sampling step can be implemented to run in O(nd) time. Consider the following two-step sampling procedure: (a) first pick centerˆccenterˆ centerˆc 1 by choosing a point x ∈ X with probability equal to</p><formula xml:id="formula_6">P y∈X x−y 2 P x,y∈X x−y 2 = ∆ 2 1 (X) + nx − c 2 /2n∆ 2 1 (X)</formula><p>(using Lemma 2.1); (b) pick the second center by choosing point y ∈ X with probability equal to</p><formula xml:id="formula_7">y − ˆ c 1 2 / ∆ 2 1 (X) + nc − ˆ c 1 2</formula><p>. This two-step sampling procedure is equivalent to the sampling process in step 1, that is, it picks pair x 1 , x 2 ∈ X with probability</p><formula xml:id="formula_8">x 1 −x 2 2 P {x,y}⊆X x−y 2 . Each step takes only O(nd) time since ∆ 2 1 (X) can be precomputed in O(nd) time.</formula><p>Analysis The analysis hinges on the important fact that under the separation condition, the radius r i of each optimal cluster is substantially smaller than the inter-cluster separation c 1 − c 2 (Lemma 3.1). This allows us to show in Lemma 3.2 that with high probability, each initial centerˆccenterˆ centerˆc i lies in the core (suitably defined) of a distinct optimal cluster, say X i , and hence c 1 − c 2 is much larger than the distancesˆcdistancesˆdistancesˆc i − c i for i = 1, 2. Assuming thatˆcthatˆ thatˆc 1 , ˆ c 2 lie in the cores of the clusters, we prove in Lemma 3.3 that the ball aroundˆc aroundˆ aroundˆc i contains only, and most of the mass of cluster X i , and therefore the centroid ¯ c i of this ball is very "close" to c i . This in turn implies that the cost of the clustering around ¯ c 1 , ¯ c 2 is small.</p><formula xml:id="formula_9">Lemma 3.1 max(r 2 1 , r 2 2 ) ≤ 2 1− 2 c 1 − c 2 2 = O( 2 )c 1 − c 2 2 .</formula><p>Proof : By part (i) of Lemma 2.2 we have ∆ 2 1 (X) = ∆ 2 2 (X) + n 1 n 2 n · c 1 − c 2 2 which is equivalent to</p><formula xml:id="formula_10">n n 1 n 2 · ∆ 2 2 (X) = c 1 − c 2 2 ∆ 2 2 (X) ∆ 2 1 (X)−∆ 2 2 (X)</formula><p>. This implies that r 2 1 · n n 2 + r 2 2 · n n 1</p><formula xml:id="formula_11">≤ 2 1− 2 c 1 − c 2 2 .</formula><p>Let ρ = 100 2 1− 2 . We require that ρ &lt; 1. We define the core of cluster X i as the set X cor</p><formula xml:id="formula_12">i = x ∈ X i : x − c i 2 ≤ r 2 i ρ . By Markov's inequality, |X cor i | ≥ (1 − ρ)n i for i = 1, 2. Lemma 3.2 Pr [{ˆc{ˆc 1 , ˆ c 2 } ∩ X cor 1 = ∅ and {ˆc{ˆc 1 , ˆ c 2 } ∩ X cor 2 = ∅] = 1 − O(ρ).</formula><p>Proof : To simplify our expressions, we assume that all the points are scaled by</p><formula xml:id="formula_13">1 c 1 −c 2 (so c 1 − c 2 = 1). By part (i) of Lemma 2.2, we have ∆ 2 1 (X) = ∆ 2 2 (X) + n 1 n 2 n · c 1 − c 2 2 which implies that ∆ 2 1 (X) ≤ n 1 n 2 n(1− 2 )</formula><p>. Let c i denote the center of mass of X cor i . Applying part (ii) of Lemma 2.2 (taking S = X i and </p><formula xml:id="formula_14">S 1 = X cor i ) we get that c i − c i 2 ≤ ρ 1−ρ · r 2 i .</formula><formula xml:id="formula_15">B = {x,y}⊆X x − y 2 = n∆ 2 1 (X) ≤ n 1 n 2 1− 2 .</formula><p>By the above bounds on c i − c i and Lemma 3.1, we get</p><formula xml:id="formula_16">c 1 − c 2 ≥ 1 − 2 ρ (1−ρ)(1− 2 )</formula><p>. So A = 1 − O(ρ) n 1 n 2 , and A/B = 1 − O(ρ) .</p><p>So we may assume that each initial centerˆccenterˆ centerˆc i lies in</p><formula xml:id="formula_17">X cor i . LetˆdLetˆ Letˆd = ˆ c 1 − ˆ c 2 and B i = {x ∈ X : x − ˆ c i ≤ ˆ d/3}.</formula><p>Recall that ¯ c i is the centroid of B i , and we return ¯ c 1 , ¯ c 2 as our final solution.</p><p>Lemma 3.3 For each i, we have X cor</p><formula xml:id="formula_18">i ⊆ B i ⊆ X i . Hence, ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i .</formula><p>Proof : By Lemma 3.1 and the definition of X cor i , we know thatˆcthatˆthatˆc i − c i ≤ θc 1 − c 2 for i = 1, 2 where θ =</p><formula xml:id="formula_19">√ ρ(1− 2 ) ≤ 1 10 . So 4 5 ≤ ˆ d c 1 −c 2 ≤ 6 5 . For any x ∈ B i we have x − c i ≤ ˆ d 3 + ˆ c i − c i ≤ c 1 −c 2 2 , so x ∈ X i . Also for any x ∈ X cor i , x − ˆ c i ≤ 2θc 1 − c 2 ≤ ˆ d 3 , so x ∈ B i . Now by part (ii) of Lemma 2.2, with S = X i and S 1 = B i , we obtain that ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i since |B i | ≥ |X cor i | for each i.</formula><p>Theorem 3.4</p><p>The above algorithm returns a clustering of cost at most</p><formula xml:id="formula_20">∆ 2 2 (X) 1−ρ with probability at least 1 − O(ρ) in time O(nd), where ρ = Θ( 2 ).</formula><p>Proof : The cost of the solution is at most</p><formula xml:id="formula_21">i,x∈X i x − ¯ c i 2 = i ∆ 2 1 (X i ) + n i ¯ c i − c i 2 ≤ ∆ 2 2 (X) 1−ρ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The k-means problem</head><p>We now consider the k-means setting. We assume that</p><formula xml:id="formula_22">∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X).</formula><p>We describe a linear time constant-factor approximation algorithm, and a PTAS that returns a (1 + ω)-optimal solution in time</p><formula xml:id="formula_23">O 2 O(k/w) nd .</formula><p>The algorithms consist of various ingredients, which we describe separately first for ease of understanding, before gluing them together to obtain the final algorithm.</p><p>Conceptually both algorithms proceed in two stages. The first stage is a seeding stage, which performs the bulk of the work and guarantees that at the end of this stage there are k seed centers positioned at nearly the right locations. By this we mean that if we consider distances at the scale of the inter-cluster separation, then at the end of this stage, each optimal center has a (distinct) initial center located in close proximity -this is precisely the leverage that we obtain from the k-means separation condition (as in the 2-means case). We shall employ three simple seeding procedures with varying time vs. quality guarantees that will exploit this condition and seed the k centers at locations very close to the optimal centers. In Section 4.1.1, we consider a natural generalization of the sampling procedure used for the 2-means case, and show that this picks the k initial centers from the cores of the optimal clusters. This sampling procedure runs in linear time but it succeeds with probability that is exponentially small in k. In Section 4.1.2, we present a very simple deterministic greedy deletion procedure, where we start off with all points in X as the centers and then greedily delete points (and move centers) until there are k centers left. The running time here is O(n 3 d). Our deletion procedure is similar to the reverse greedy algorithm proposed by Chrobak, Kenyon and Young <ref type="bibr" target="#b7">[8]</ref> for the k-median problem. Chrobak et al. show that their reverse greedy algorithm attains an approximation ratio of O(log n), which is tight up to a factor of log log n. In contrast, for the k-means</p><formula xml:id="formula_24">problem, if ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X)</formula><p>, we show that our greedy deletion procedure followed by a clean-up step (in the second stage) yields a 1 + f () -approximation algorithm.Finally, in Section 4.1.3 we combine the sampling and deletion procedures to obtain an O(nkd+k 3 d)-time initialization procedure. We sample O(k) centers, which ensures that every cluster has an initial center in a slightly expanded version of the core, and then run the deletion procedure on an instance of size O(k) derived from the sampled points to obtain the k seed centers.</p><p>Once the initial centers have been positioned sufficiently close to the optimal centers, we can proceed in two ways in the second-stage (Section 4.2). One option is to use a ball-k-means step, as in 2-means, which yields a clustering of cost 1 + f () ∆ 2 k (X) due to exactly the same reasons as in the 2-means case. Thus, combined with the initialization procedure of Section 4.1.3, this yields a constant-factor approximation algorithm with running time O(nkd + k 3 d). The entire algorithm is summarized in Section 4.3.</p><p>The other option, which yields a PTAS, is to use a sampling idea of Kumar et al. <ref type="bibr" target="#b29">[30]</ref>. For each initial center, we compute a list of candidate centers for the corresponding optimal cluster as follows: we sample a small set of points uniformly at random from a slightly expanded Voronoi region of the initial center, and consider the centroid of every subset of the sampled set of a certain size as a candidate. We exhaustively search for the k candidates (picking one candidate per initial center) that yield the least cost solution, and output these as our final centers. The fact that each optimal center c i has an initial center in close proximity allows us to argue that the entire optimal cluster X i is contained in the expanded Voronoi region of this initial center, and moreover that |X i | is a significant fraction of the total mass in this region. Given this property, as argued by Kumar et al. (Lemma 2.3 in <ref type="bibr" target="#b29">[30]</ref>), a random sample from the expanded Voronoi region also (essentially) yields a random sample from X i , which allows us to compute a good estimate of the centroid of X i , and hence of ∆ 2 1 (X i ). We obtain a (1 + ω)-optimal solution in time O 2 O(k/ω) nd with constant probability. Since we incur an exponential dependence on k anyway, we just use the simple sampling procedure of Section 4.1.1 in the first-stage to pick the k initial centers. Although the running time is exponential in k, it is significantly better than the running time of O 2 (k/ω) O <ref type="formula">(1)</ref> nd incurred by the algorithm of Kumar et al.; we also obtain a simpler PTAS. Both of these features can be traced to the separation condition, which enables us to nearly isolate the positions of the optimal centers in the first stage. Kumar et al. do not have any such facility, and therefore need to sequentially "guess" (i.e., exhaustively search) the various centroids, incurring a corresponding increase in the run time. This PTAS is described in Section 4.4.</p><p>The following lemma, which is a simple extension of Lemma 3.1 to the k-means case and is proved via an almost identical argument, will be used repeatedly. 4.1 Seeding procedures used in stage I</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Sampling</head><p>We pick k initial centers as follows: first pick two centersˆccentersˆ centersˆc 1 , ˆ c 2 as in the 2-means case, that is, choose x, y ∈ X with probability proportional to x − y 2 . Suppose we have already picked i centersˆccentersˆ centersˆc 1 , . . . , ˆ c i where 2 ≤ i &lt; k. Now pick a random point x ∈ X with probability proportional to min j∈{1,...,i} x − ˆ c j 2 and set that as centerˆccenterˆ centerˆc i+1 .</p><p>Running time The sampling procedure consists of k iterations, each of which takes O(nd) time. This is because after sampling a new pointˆcpointˆ pointˆc i+1 , we can update the quantity min j∈{1,...,i+1} x − ˆ c j for each point</p><formula xml:id="formula_25">x in O(d) time. So the overall running time is O(nkd).</formula><p>Analysis Let 2 ρ &lt; 1 be a parameter that we will set later. As in the 2-means case, we define the core of cluster X i as X cor</p><formula xml:id="formula_26">i = x ∈ X i : x − c i 2 ≤ r 2 i ρ .</formula><p>We show that under our separation assumption, the above sampling procedure will pick the k initial centers to lie in the cores of the clusters X 1 , . . . , X k with probability 1 − O(ρ) k . We also show in Lemma 4.5 that if more than k, but still O(k), points are sampled, then with constant probability, every cluster will contain a sampled point that lies in a somewhat larger core, that we call the outer core of the cluster. This analysis will be useful in Section 4.1.3. Proof :</p><p>The key observation is that for any pair of distinct clusters X i , X j , the 2-means separation condition holds, that is,</p><formula xml:id="formula_27">∆ 2 2 (X i ∪ X j ) = ∆ 2 1 (X i ) + ∆ 2 1 (X j ) ≤ 2 ∆ 2 1 (X i ∪ X j )</formula><p>. This is because</p><formula xml:id="formula_28">∆ 2 k−1 (X) ≤ =i,j ∆ 2 1 (X ) + ∆ 2 1 (X i ∪ X j ) = ∆ 2 k (X) + ∆ 2 1 (X i ∪ X j ) − ∆ 2 2 (X i ∪ X j ) . So ∆ 2 1 (X i ∪ X j ) − ∆ 2 2 (X i ∪ X j ) ≥ 1 2 − 1 ∆ 2 k (X) ≥ 1 2 − 1 ∆ 2 2 (X i ∪ X j )</formula><p>. So using Lemma 3.2 we obtain that</p><formula xml:id="formula_29">x∈X cor i ,y∈X cor j x − y 2 = 1 − O(ρ)</formula><p>{x,y}⊆X i ∪X j</p><p>x − y 2 . Summing over all pairs i, j yields the lemma. Now inductively suppose that the first i centers pickedˆcpickedˆ pickedˆc 1 , . . . , ˆ c i lie in the cores of clusters X j 1 , . . . , X j i . We show that conditioned on this event, centerˆccenterˆ centerˆc i+1 lies in the core of some cluster X where / ∈ {j 1 , . . . , j i } with probability 1 − O(ρ). Given a set S of points, we use d(x, S) to denote min y∈S x − y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.3 Prˆc</head><p>Prˆ</p><formula xml:id="formula_30">Prˆc i+1 ∈ / ∈{j 1 ,...,j i } X cor | ˆ c 1 , . . . , ˆ c i lie in the cores of X j 1 , . . . , X j i = 1 − O(ρ).</formula><p>Proof : For notational convenience, re-index the clusters so that {j 1 , . . . ,</p><formula xml:id="formula_31">j i } = {1, . . . , m}. LetˆCLetˆ LetˆC = {ˆc{ˆc 1 , . . . , ˆ c i }. For any cluster X j , let p j ∈ {1, . . . , i} be the index such that d(c j , ˆ C) = c j − ˆ c p j . Let A = k j=m+1 x∈X cor j d(x, ˆ C) 2 , and B = k j=1 x∈X j d(x, ˆ C) 2 .</formula><p>Observe that the probability of the event stated in the lemma is exactly A/B. Let α denote the maximum over all j ≥ m + 1 of the quantity</p><formula xml:id="formula_32">max x∈X cor j x − c j /d(c j , ˆ C). For any point x ∈ X cor j , j ≥ m + 1, we have d(x, ˆ C) ≥ (1 − α)d(c j , ˆ C).</formula><p>Note that by Lemma 4.</p><formula xml:id="formula_33">1, α ≤ / √ ρ(1− 2 ) 1−/ √ ρ(1− 2 ) ≤ 2 √ ρ(1− 2 )</formula><p>&lt; 1 for a small enough ρ. Therefore,</p><formula xml:id="formula_34">A = k j=m+1 x∈X cor j d(x, ˆ C) 2 ≥ k j=m+1 (1 − ρ)(1 − α) 2 n j d(c j , ˆ C) 2 ≥ (1 − ρ − 2α) k j=m+1 n j d(c j , ˆ C) 2 .</formula><p>On the other hand, for any point x ∈ X j , j = 1, . . . , k, we have</p><formula xml:id="formula_35">d(x, ˆ C) ≤ x − ˆ c p j . Also note that for j = 1, . . . , m, ˆ c p j lies in X cor j , so c j − ˆ c p j ≤ r j √ ρ . Therefore, B ≤ k j=1 x∈X j x − ˆ c p j 2 ≤ k j=1 ∆ 2 1 (X j ) + n j c j − ˆ c p j 2 ≤ 1 + 1 ρ ∆ 2 k (X) + k j=m+1 n j d(c j , ˆ C) 2 .</formula><p>Finally, for any j = m+1, . . . k, if we assign all the points in cluster X j to the pointˆcpointˆ pointˆc p j , then the increase in cost is exactly n j c j − ˆ c p j 2 and at least</p><formula xml:id="formula_36">∆ 2 k−1 (X) − ∆ 2 k (X). Therefore 1 2 − 1 ∆ 2 k (X) ≤ n j d(c j , ˆ C) 2</formula><p>for any j = m + 1, . . . , k, and</p><formula xml:id="formula_37">B ≤ 1+ 2 /ρ 1− 2 k j=m+1 n j d(c j , ˆ C) 2 .</formula><p>Comparing with A and plugging in the value of α, we get that</p><formula xml:id="formula_38">A = 1 − O(ρ + √ ρ ) B. If we set ρ = Ω( 2/3 ), we obtain A/B = 1 − O(ρ).</formula><p>Next, we analyze the case when more than k points are sampled. Let ρ 1 = ρ 3 . Define the outer core of X i to be X out</p><formula xml:id="formula_39">i = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 1 }. Note that X cor i ⊆ X out i . Let N = 2k 1−5ρ + 2 ln(2/δ) (1−5ρ) 2</formula><p>where 0 &lt; δ &lt; 1 is a desired error tolerance. We prove in Lemma 4.4 that at every sampling step, there is a constant probability that the sampled point lies in the core of some cluster whose outer core does not contain a previously sampled point. The crucial difference between this lemma and Lemma 4.3, is that Lemma 4.3 only shows that the "good" event happens conditioned on the fact that previous samples were also "good", whereas here we give an unconditional bound. Using this, Lemma 4.5 shows that if we sample N points from X, then with some constant probability, each outer core X out i will contain a sampled point. The proof is based on a straightforward martingale analysis. </p><formula xml:id="formula_40">α ≤ ρ 1 /ρ &lt; 1. Then for any point x ∈ X cor j , j ≥ m + 1, we have d(x, ˆ C) ≥ (1 − α)d(c j , ˆ C) and as in Lemma 4.3, A = k j=m+1 x∈X cor j d(x, ˆ C) 2 ≥ (1 − ρ − 2α) k j=m+1 n j d(c j , ˆ C) 2 .</formula><p>On the other hand, again arguing as in Lemma 4.3, we have</p><formula xml:id="formula_41">B = k j=1 x∈X j d(x, ˆ C) 2 ≤ 1+ 2 /ρ 1 1− 2 k j=m+1 n j d(c j , ˆ C) 2 . Therefore A/B ≥ 1 − ρ + 2 ρ 1 ρ + 2 ρ 1 + 2 . Since ρ 1 = ρ 3 , taking ρ = √ gives A/B ≥ 1 − 5ρ.</formula><p>Lemma 4.5 Suppose we sample N pointsˆxpointsˆ pointsˆx 1 , . . . , ˆ x N from X using the above sampling procedure. Then, Pr[∀j = 1, . . . , k, there exists somê</p><formula xml:id="formula_42">x i ∈ X out j ] ≥ 1 − δ.</formula><p>Proof : Let Y t be a random variable that denotes the number of clusters that do not contain a sampled point in their outer cores, after t points have been sampled. We want to bound Pr[Y N &gt; 0]. Consider the following random walk on the line with W t denoting the (random) position after t time steps: W 0 = k, and W t+1 = W t with probability 5ρ and W t −1 with probability 1−5ρ.</p><formula xml:id="formula_43">Notice that Pr[Y N &gt; 0] ≤ Pr[W N &gt; 0]</formula><p>, because as long as W t &gt; 0, any outcome that leads to a left move in the random walk can be mapped to an outcome (in the probability space corresponding to the sampling process) where the outer core of a new cluster is hit by the currently sampled point. So we bound</p><formula xml:id="formula_44">Pr[W N &gt; 0]. Define Z t = W t + t(1 − 5ρ). Then E Z t+1 |Z 1 , . . . , Z t ≤ Z t , so Z 0 , Z 1 , .</formula><p>. . forms a supermartingale. Clearly |Z t+1 − Z t | ≤ 1 for all t. So by Azuma's inequality (see, e.g., <ref type="bibr" target="#b38">[39]</ref></p><formula xml:id="formula_45">), Pr[Z N − Z 0 &gt; 2N ln(2/δ)] ≤ δ which implies that W N ≤ k + 2N ln(2/δ) − N (1 − 5ρ</formula><p>) with probability at least 1 − δ. Plugging the value of N shows that</p><formula xml:id="formula_46">N (1 − 5ρ) − 2N ln(2/δ) ≥ k.</formula><p>Corollary 4.6 (i) If we sample k pointsˆcpointsˆ pointsˆc 1 , . . . , ˆ c k , then with probability</p><formula xml:id="formula_47">1 − O(ρ) k , where ρ = Ω( 2/3 ), for each i there is a distinct centerˆccenterˆ centerˆc i ∈ X cor i , that is, ˆ c i − c i ≤ r i / √ ρ.</formula><p>(ii) If we sample N pointsˆxpointsˆ pointsˆx 1 , . . . , ˆ x N , where N = 2k 1−5ρ + 2 ln(2/ρ) (1−5ρ) 2 and ρ = √ , then with probability</p><formula xml:id="formula_48">1 − O(ρ), for each i there is a distinct pointˆxpointˆ pointˆx i ∈ X out i , that is, ˆ x i − c i ≤ r i / ρ 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Greedy deletion procedure</head><p>We maintain a set of centersˆCcentersˆ centersˆC that are currently used to cluster X. For any point x ∈ R d , let R(x) ⊆ X denote the points of X in the Voronoi region of x (given the set of centersˆCcentersˆ centersˆC). We refer to R(x) as the Voronoi set of x. Initializê C ← X. Repeat the following steps until | ˆ C| = k.</p><p>B1. Compute T = cost of clustering X around the centers inˆCinˆ inˆC = x∈ˆCx∈ˆ x∈ˆC y∈R(x) y − x 2 . Also for every x ∈ ˆ C, compute T x = cost of clustering X aroundˆCaroundˆ aroundˆC \ {x} = z∈ˆC\{x}z∈ˆ z∈ˆC\{x} y∈R −x (z) y − z 2 , where R −x (z) denotes the Voronoi set of z given the center setˆCsetˆ setˆC \ {x}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B2. Pick the center y ∈ ˆ</head><p>C for which T x − T is minimum and setˆCsetˆ setˆC ← ˆ C \ {y}.</p><p>B3. Recompute the Voronoi sets R(x) = R −y (x) ⊆ X for each (remaining) center x ∈ ˆ C. Now we "move" the centers to the centroids of their respective (new) Voronoi sets, that is, for every set R(x), we updatê C ← ˆ C \ {x} ∪ {ctr(R(x))}.</p><p>Running time There are n − k iterations of the B1-B3 loop. Each iteration takes O(n 2 d) time: computing T and the sets R(x) for each x takes O(n 2 d) time and we can then compute each T x in O(|R(x)|d) time (since while computing T , we can also compute for each point its second-nearest center inˆCinˆ inˆC). Therefore the overall running time is O(n 3 d).</p><p>Analysis Let ρ be a parameter such that</p><formula xml:id="formula_49">ρ ≤ 1 10 , / ρ(1 − 2 ) ≤ 1 14 . Recall that D i = min j =i c j − c i . Define d 2 i = ∆ 2 k (X)/n i .</formula><p>We will use a different notion of a cluster-core here, but the notion will still capture the fact that the core consists of points that are quite close to the cluster-center compared to the inter-cluster distance, and contains most of the mass of the cluster. Let B(x, r) = {y ∈ R d : x − y ≤ r} denote the ball of radius r centered at x. Define the kernel of X i to be the ball Z i = B(c i , d i / √ ρ) and the core of</p><formula xml:id="formula_50">X i as X cor i = X i ∩ Z i . Observe that r i ≤ d i , so by Markov's inequality |X cor i | ≥ (1 − ρ)n i . Also, since ∆ 2 k−1 (X) − ∆ 2 k (X) ≤ n i D 2 i we have that d 2 i ≤ D 2 i · 2 1− 2 . Therefore, X cor i = X ∩ Z i . We prove that,</formula><p>at the start of every iteration, for every i, there is a (distinct) center x ∈ ˆ C that lies in Z i .</p><p>Clearly (*) holds at the beginning, sincê C = X and X cor i = ∅ for every cluster X i . First we show (Lemma 4.7) that if x ∈ ˆ C is the only center that lies in a slightly enlarged version of the ball Z i for some i, then x is not deleted . Lemma 4.8 then makes the crucial observation that even after a center y is deleted, if the new Voronoi region R −y (x) of a center x ∈ ˆ C captures points from X cor i , then R −y (x) cannot "extend" too far into some other cluster X i , that is, for x ∈ R −y (x) ∩ X j where j = i, y − c i is not much larger than y − c j . It will then follow that invariant (*) is maintained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.7 Suppose (*) holds at the start of an iteration, and x ∈ ˆ</head><p>C is the only center in B c i , 4d i √ ρ for some cluster X i , then x ∈ ˆ C after step B2.</p><p>Proof : Since property (*) holds, we also know that x ∈ Z i and so X cor i ⊆ R(x) ∩ X i . If x is deleted in step B2 then all points in X cor i will be reassigned to a center at least 4d i √ ρ away from c i . So the cost-increase</p><formula xml:id="formula_52">T x − T is at least A = 5(1−ρ) ρ · n i d 2 i = 5(1−ρ) ρ · ∆ 2 k (X)</formula><p>. Now since | ˆ C| &gt; k, there is some j (j could be i) such that the Voronoi region of c j (with respect to the optimal center-set) contains at least two centers fromˆCfromˆ fromˆC. We will show that deleting one of these centers will be less expensive than deleting x. Let z ∈ ˆ C be the center closest to c for = 1, . . . , k. Note that z ∈ Z . Let y ∈ ˆ C, y = z j be another center in the Voronoi region of c j . Suppose we delete y. We can upper bound the cost-increase T y − T by the cost-increase due to the reassignment where we assign all points in R y ∩ X to z for = 1, . . . , k. For any  . Hence, the cost-increase of the reassignment is at most</p><formula xml:id="formula_53">x ∈ R(y) ∩ X we have x − z ≤ x − c + c − z ≤ x − c + d √ ρ . For = j,</formula><formula xml:id="formula_54">y − c ≤ x − c + x − y ≤ x − c + x − z ≤ 2x − c + c − z ≤ 2x − c + d √ ρ . Therefore, D ≤ 4x − c + 2d √ ρ which implies that √ 1− 2 − 2 √ ρ d ≤ 4x − c</formula><formula xml:id="formula_55">B = k =1 x ∈R(y)∩X x − z 2 ≤ x ∈R(y)∩X j 2 x − c j 2 + d 2 j ρ + =j x ∈R(y)∩X β 2 x − c 2 ≤ max(2, β 2 )∆ 2 k (X) + 2 ρ · n j d 2 j = max(2, β 2 ) + 2 ρ ∆ 2 k (X).</formula><p>Any ρ satisfying the bounds stated in Section 4.1.2 ensures that A &gt; B (since β &lt; 4 3 and ρ &lt; 3 7 ). Thus, x is not the cheapest center to delete, which completes the proof.</p><formula xml:id="formula_56">Lemma 4.8 Suppose center y ∈ ˆ C is deleted in step B2. Let x ∈ ˆ C \ {y} be such that R −y (x) ∩ X cor j = ∅ for some j. Then for any x ∈ R −y (x) ∩ X , = j we have x − c j ≤ x − c + max(d +6d j ,4d +3d j ) √ ρ .</formula><p>Proof : Suppose that y lies in the Voronoi region of center c i (wrt. optimal centers). LetˆCLetˆ LetˆC = ˆ C \ {y}. There must be a center z i ∈ ˆ C such that z i − c i ≤ 4d i √ ρ . If y / ∈ Z i , this follows from property (*) otherwise this follows from Lemma 4.7. For any = i, we know by property (*) that there is some center z ∈ ˆ C that lies in Z . Let x be a point in R −y (x) ∩ X cor j . Then,</p><formula xml:id="formula_57">x − c j ≤ x − x + x − c j ≤ x − z j + x − c j ≤ z j − c j + 2x − c j ≤ z j − c j + 2d j √ ρ .</formula><p>Now considering the point x , we have (since it could be that = i).</p><formula xml:id="formula_58">x − c j ≤ x − x + x − c j ≤ x − z + x − c j ≤ x − c + z − c + x − c j ≤ x − c + z − c + z j − c j + 2d j √ ρ .</formula><p>Lemma 4.9 Suppose that property (*) holds at the beginning of some iteration in the deletion phase. Then (*) also holds at the end of the iteration, i.e., after step B3.</p><p>Proof : Suppose that we delete center y ∈ ˆ C that lies in the Voronoi region of center c i (wrt. optimal centers) in step B2. LetˆCLetˆ LetˆC = ˆ C \ {y} and</p><formula xml:id="formula_59">R (x) = R −y (x) for any x ∈ ˆ C . Fix a cluster X j . Let S = {x ∈ ˆ C : R (x) ∩ X cor j = ∅} and Y = x∈S R (x)</formula><p>. We show that there is some set R (x), x ∈ ˆ C whose centroid ctr(R (x)) lies in the ball Z j , which will prove the lemma. By Lemma 4.8 and noting that d 2</p><formula xml:id="formula_60">≤ 2 1− 2 · D 2 for every , for any x ∈ Y ∩ X where = j, we have x − c j ≤ x − c + √ ρ(1− 2 ) · max(D + 6D j , 4D + 3D j ). Also D j , D ≤ c j − c ≤ x − c j + x − c . Substituting for D j , D we get that y − c j ≤ βy − c where β = 1+7/ √ ρ(1− 2 ) 1−7/ √ ρ(1− 2 )</formula><p>. Using this we obtain that</p><formula xml:id="formula_61">A = x ∈Y x − c j 2 ≤ β 2 k =1 x ∈Y ∩X x − c 2 ≤ β 2 ∆ 2 k (X). We also have A = x∈S x ∈R (x) y−c j 2 = x∈S ∆ 2 1 (R (x))+|R (x)||ctr(R (x))−c j 2 ≥ |Y | min x∈S ctr(R (x))−c j 2 . Since X cor j ⊆ Y we have |Y | ≥ (1 − ρ)n j , so we obtain that min x∈S ctr(R (x)) − c j ≤ β √ 1−ρ · d i . The bounds on ρ ensure that ρβ 2 1−ρ ≤ 1, so that min x∈S ctr(R (x)) − c j ≤ d j √ ρ .</formula><p>Corollary 4.10 After the deletion phase, for every i, there is a centerˆccenterˆ centerˆc i ∈ ˆ C withˆcwithˆwithˆc i −c i ≤</p><formula xml:id="formula_62">√ ρ(1− 2 )</formula><p>·D i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">A linear time seeding procedure</head><p>We now combine the sampling idea with the deletion procedure to obtain an initialization procedure that runs in time O(nkd + k 3 d) and succeeds with high probability. We first sample O(k) points from X using the sampling procedure. Then we run the deletion procedure on an O(k)-size instance consisting of the centroids of the Voronoi regions of the sampled, points, with each centroid having a weight equal to the mass of X in its corresponding Voronoi region. The sampling process will ensure that with high probability, every cluster X i contains a pointˆcpointˆ pointˆc i that is close to its center c i . This will allow us to argue that the ∆ 2 k (.) cost of the sampled instance is much smaller than its ∆ 2 k−1 (.) cost, and that the optimal centers for the sampled instance lie near the optimal centers for X. We can then use the analysis of the previous section to argue that after the deletion procedure the k centers are still quite close to the optimal centers for the sampled instance, and hence also close to the optimal centers for X.</p><formula xml:id="formula_63">Fix ρ 1 = √ . C1. Sampling. Sample N = 2k 1−5ρ 1 + 2 ln(2/ρ 1 )</formula><p>(1−5ρ 1 ) 2 points from X using the sampling procedure of Section 4.1.1. Let S denote the set of sampled points.</p><p>C2. Deletion phase. For each x ∈ S, let R(x) = {y ∈ X : y − x = min z∈ˆSz∈ˆ z∈ˆS y − z} be its Voronoi set (wrt. the sampled points). We now ignore X, and consider a weighted instancê S obtained as follows: setˆSsetˆ setˆS ← {ˆx{ˆx = ctr(R(x)) : x ∈ S}, and assign eachˆxeachˆ eachˆx a weight w(ˆ x) = |R(x)|. Run the deletion procedure of Section 4.1.2, on this new instance to obtain k centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running time</head><p>Step </p><formula xml:id="formula_64">Analysis Recall that ρ 1 = √ . Let ρ 2 = ρ 3 1 . Let X cor i = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 1 }. Let X out i = {x ∈ X i : x − c i 2 ≤ r 2 i ρ 2</formula><p>} denote the outer core of cluster X i . By part (ii) of Corollary 4.6 we know that with probability 1 − O(ρ 1 ), every cluster X i contains a sampled point in its outer core after step C1. So assume that this event happens. LetˆsLetˆ Letˆs 1 , . . . , ˆ s k denote the optimal k centers forˆSforˆ forˆS andˆcandˆ andˆc 1 , . . . , ˆ c k be the centers returned by the deletion phase. Lemma 4.11 shows that the k-means separation condition also holds forˆSforˆ forˆS, and the optimal centers forˆSforˆ forˆS are close to the optimal centers for X. This will imply that the centers returned by the deletion phase are close to the optimal centers for X.</p><formula xml:id="formula_65">Lemma 4.11 (i) ∆ 2 k ( ˆ S) = O( 2 )∆ 2 k−1 ( ˆ S).</formula><p>(ii) For every optimal center c i of X, there is a centerˆscenterˆ centerˆs i such thatˆs</p><formula xml:id="formula_66">thatˆthatˆs i − c i ≤ D i 25 + r i √ ρ 1 .</formula><p>Proof : For each sampled point x ∈ S recall that R(x) ⊆ X denotes its Voronoi set (wrt. S). For j = 1, . . . , k, let z j ∈ S be a sampled point in</p><formula xml:id="formula_67">X out j , so z j − c j ≤ r j √ ρ 2</formula><p>. Consider an optimal (k − 1)-clustering ofˆSofˆ ofˆS. We can obtain a (k − 1)-clustering of X from this by assigning all the points in R(x), where x ∈ S, to the center to which ctr(R(x)) ∈ ˆ S is assigned. The cost-increase in doing so is exactly</p><formula xml:id="formula_68">A = x∈S ∆ 2 1 (R(x)), so ∆ 2 k−1 (X) ≤ ∆ 2 k−1 ( ˆ S) + A. Since y − x 2 ≤ y − z j 2 ≤ 2 y − c j 2 + r 2 j ρ 2 for any y ∈ R(x) ∩ X j , we obtain that A ≤ 2 1 + 1 ρ 2 ∆ 2 k (X). To upper bound ∆ 2 k ( ˆ S)</formula><p>, consider the following k-clustering ofˆSofˆ ofˆS: for eachˆxeachˆ eachˆx = ctr(R(x)) ∈ ˆ S where x ∈ S ∩ X i , assignˆxassignˆ assignˆx to center c i . To bound the cost of this assignment, first note that for a point y ∈ R(x) ∩ X j where x ∈ X i and j = i, we have</p><formula xml:id="formula_69">y − c i ≤ y − x + x − c i ≤ y − x + x − c j ≤ 2y − x + y − c j ≤ 3y − c j + 2z j − c j . We also have z j − c j ≤ r j √ ρ 2 ≤ D j √ ρ 2 (1− 2 ) and D j ≤ y − c i + y − c j , which implies that y − c i ≤ βy − c j where β = 3+2/ √ ρ 2 (1− 2 ) 1−2/ √ ρ 2 (1− 2 )</formula><p>. Thus,</p><formula xml:id="formula_70">∆ 2 k ( ˆ S) ≤ k i=1 x∈S∩X i |R(x)||ctr(R(x)) − c i 2 ≤ k i=1 x∈S∩X i y∈R(x) y − c i 2 ≤ k i=1 x∈S∩X i y∈R(x)∩X i y − c i 2 + j =i,y∈R(x)∩X j β 2 y − c j 2 ≤ β 2 ∆ 2 k (X).</formula><p>Combining the two bounds we get,</p><formula xml:id="formula_71">∆ 2 k−1 (S) ≥ ∆ 2 k−1 (X) − A ≥ 1 2 − 2 − 2 ρ 2 ∆ 2 k (X) ≥ 1// 2 − 2 − 2/ρ 2 β 2 · ∆ 2 k ( ˆ S).</formula><p>Since</p><formula xml:id="formula_72">ρ 1 = √ and ρ 2 = ρ 3 1 , we get that ∆ 2 k ( ˆ S) = O( 2 )∆ 2 k−1 ( ˆ S). This proves part (i). Consider any center c i . Supposê s j −c i &gt; D i /25+ r i √ ρ 1</formula><p>for every pointˆspointˆ pointˆs j . Then the cost of clustering</p><formula xml:id="formula_73">X around the centersˆscentersˆ centersˆs 1 , . . . , ˆ s k is at least 1−ρ 1 625 · n i D 2 i = Ω( −2 )∆ 2 k (X).</formula><p>On the other hand, we also have that the cost of this clustering for X is at most ∆ 2 k ( ˆ S) + A = O( −3/2 )∆ 2 k (X), which contradicts with the earlier bound. Proof :</p><formula xml:id="formula_74">LetˆDLetˆ LetˆD i = min j =î s j − ˆ s i . Then (1 − 2θ) ≤ ˆ D i D i ≤ 1 + 2θ) where θ ≤ 1 25 + √ ρ 1 (1− 2 ) .</formula><p>Since ρ 1 = √ , for small enough, we have that θ &lt; 1 22 . Choosing ρ for the deletion phase suitably, by Corollary 4.10, we can ensure that the deletion phase returns a pointˆcpointˆ pointˆc i such thatˆc</p><formula xml:id="formula_75">thatˆthatˆc i − ˆ s i ≤ ˆ D i</formula><p>20 . Thus, using Lemma 4.11ˆc</p><formula xml:id="formula_75">thatˆthatˆc i − ˆ s i ≤ ˆ D i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Procedures used in stage II</head><p>Given k seed centersˆccentersˆ centersˆc 1 , . . . , ˆ c k located sufficiently close to the optimal centers after stage I, we use two procedures in stage II to obtain a near-optimal clustering: the ball-k-means step, which yields a 1 + f () -approximation algorithm, or the centroid estimation step, based on a sampling idea of <ref type="bibr">Kumar et al. [30]</ref>, which yields a PTAS with running time exponential in k. Definê</p><formula xml:id="formula_77">d i = min j =î c j − ˆ c i . Recall that D i = min j =i c j − c i .</formula><p>(A) Ball-k-means step. Let B i be the points of X in a ball of radiusˆdradiusˆ radiusˆd i /3 aroundˆcaroundˆ aroundˆc i , and ¯ c i be the centroid of B i . Return ¯ c 1 , . . . , ¯ c k as the final centers.</p><p>Lemma 4.13 (Ball-k-means) Suppose that for each i, there is a centerˆccenterˆ centerˆc i such thatˆc</p><formula xml:id="formula_78">thatˆthatˆc i − c i ≤ D i /10. Let ρ = 36 2 1− 2 and Y i = x ∈ X i : x − c i 2 ≤ r 2 i ρ . Then Y i ⊆ B i ⊆ X i , and ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i .</formula><p>The proof of the above lemma is essentially identical to that of Lemma 3.3, and hence is omitted.</p><p>(B) Centroid estimation. For each i, we will obtain a set of candidate centers for cluster X i as follows. Fix β = 1 1+144 2 . Define the expanded Voronoi region ofˆcofˆ ofˆc i as follows: for any x ∈ X, letˆcletˆ letˆc(x) denote the center</p><formula xml:id="formula_79">such that x − ˆ c(x) = min j x − ˆ c j . Let R i ⊆ X = {x ∈ X : x − ˆ c i ≤ x − ˆ c(x) + ˆ c i − ˆ c(x)/4}. Sample 4</formula><p>βω points independently and uniformly at random from R i , where ω is a given input parameter, to obtain a random subset S i ⊆ R i . Compute the centroid of every subset of S i of size 2 ω ; let T i be the set consisting of all these centroids. Select the candidates ¯ c 1 ∈ T 1 , . . . , ¯ c k ∈ T k that yield the least-cost solution, and return these as the final centers.</p><p>Lemma 4.14 (Centroid-estimation) Suppose that for each i, there is a centerˆccenterˆ centerˆc i such thatˆc</p><formula xml:id="formula_80">thatˆthatˆc i − c i ≤ D i /10. Then X i ⊆ R i</formula><p>, where R i is as defined in the centroid-estimation procedure, and |X i | ≥ β|R i |.</p><p>Proof : For any j = i, we have</p><formula xml:id="formula_81">4 5 · c i − c j ≤ ˆ c i − ˆ c j ≤ 6 5 · c i − c j . Hence, 4D i 5 ≤ ˆ d i ≤ 6D i 5 . Consider any x ∈ X i that lies in the Voronoi region ofˆcofˆ ofˆc j (sô c(x) = ˆ c j ). We have x − c i ≤ x − c j , therefore x − ˆ c i ≤ x − ˆ c j + D i +D j 10 ≤ x − ˆ c i + c i − c j /5 ≤ x − ˆ c j + ˆ c i − ˆ c j /4; so x ∈ R i . Suppose |X i | ≤ β|R i |. Let a j = |R i ∩X j | |R i | . So a i 1−a i ≤ β 1−β .</formula><p>Consider the clustering where we arbitrarily assign some a j 1−a i points of X i to center c j for each j = i. For any x ∈ X i and j = i, we have x − c j 2 ≤ 2(x − c i 2 + c i − c j 2 ). So the cost of reassigning points in X i is at most</p><formula xml:id="formula_82">2∆ 2 1 (X i ) + 2n i 1−a i · j =i a j c i − c j 2 ≤ 2∆ 2 1 (X i ) + 2β 1−β · j =i a j |R i ||c i − c j 2 . We also know that for any y ∈ R i ∩ X j , y − c i ≤ y − ˆ c(y) + ˆ c i − ˆ c(y) 4 + D i 10 ≤ y − ˆ c j + y − ˆ c i 2 + D i 10 =⇒ y − c i 2 ≤ y − c j + 1.5D i + D j 10 . Since D i , D j ≤ c i − c j , this in turn implies that y − c i ≤ 2y − c j + c i − c j /2, which implies that c i − c j ≤ 6 · y − c j . Therefore, we can bound a j |R i ||c i − c j 2 by 36 · y∈R i ∩X j y − c j 2 .</formula><p>Hence, the cost of this clustering is at most max 2, 1 + 72β</p><formula xml:id="formula_83">1−β ∆ 2 k (X) ≤ 1 + 1 2 2 ∆ 2 k (X).</formula><p>The cost of this clustering is also at least ∆ 2 k−1 (X). This is a contradiction to the assumption that</p><formula xml:id="formula_84">∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A linear time constant-factor approximation algorithm</head><p>This algorithm uses the initialization procedure of Section 4.1.3 followed by a ball-k-means step, and hence runs in time O(nkd + k 3 d).</p><p>D1. Execute the seeding procedure of Section 4.1.3 to obtain k initial centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .</p><p>D2. Run the ball-k-means step of Section 4.2 to obtain the final centers.</p><p>By Lemma 4.12, we know that with probability 1 − O( √ ), for each c i , there is a distinct centerˆccenterˆ centerˆc i such thatˆcthatˆthatˆc i − c i ≤ D i /10. Therefore, by Lemma 4.13, for each c i , we have ¯ c i − c i 2 ≤ ρ 1−ρ · r 2 i . Hence, by mimicking the proof of Theorem 3.4, we obtain the following theorem.</p><p>Theorem 4.15 Assuming that ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X) for a small enough , the above algorithm returns a solution of cost at most 1− 2 1−37 2 · ∆ 2 k (X) with probability 1 − O(</p><formula xml:id="formula_85">√ ) in time O(nkd + k 3 d).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A PTAS for any fixed k</head><p>The PTAS combines the sampling procedure of Section 4.1.1 (we could also use the seeding procedure of Section 4.1.3) with the centroid estimation step described in Section 4.2.</p><p>E1. Use the procedure in Section 4.1.1 to pick k initial centersˆccentersˆ centersˆc 1 , . . . , ˆ c k .</p><p>E2. Run the centroid estimation procedure of Section 4.2 to obtain the final centers.</p><p>The running time is dominated by the exhaustive search in the centroid estimation procedure, which takes time O 2 (4k/βω) nd . We show that the cost of the final solution is at most (1 + ω)∆ 2 k (X), with probability γ k for some constant γ. By repeating the procedure O(γ −k ) times, we can boost this to a constant.</p><formula xml:id="formula_86">Theorem 4.16 Assuming that ∆ 2 k (X) ≤ 2 ∆ 2 k−1 (X)</formula><p>for a small enough , there is a PTAS for the k-means problem that returns a (1 + ω)-optimal solution with constant probability in time</p><formula xml:id="formula_87">O(2 O(k(1+ 2 )/ω) nd).</formula><p>Proof : By appropriately setting ρ in the sampling procedure, we can ensure that with probability Θ(1) k , it returns centersˆccentersˆ centersˆc 1 , . . . , ˆ c k such that for each i,  <ref type="bibr" target="#b29">[30]</ref> shows that for every i, with constant probability, there is some candidate point c i ∈ T i such that</p><formula xml:id="formula_88">ˆ c i − c i ≤ D i /10 (part (i) of</formula><formula xml:id="formula_89">x∈X i x − c i 2 ≤ (1 + ω)∆ 2 1 (X i ).</formula><p>The cost of the best-candidate solution is at most the cost of the solution due to the points c 1 ∈ T 1 , . . . , c k ∈ T k , which is at most (1 + ω)∆ 2 k (X). The overall success probability for one call of the procedure is γ k for some constant γ &lt; 1, so by repeating the procedure O(γ −k ) times we can obtain constant success probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The separation condition</head><p>We now show that our separation condition implies, and is implied by, the condition that any two nearoptimal k-clusterings disagree on only a small fraction of the data. Let cost(x 1 , . . . , x k ) denote the cost of clustering X around the centers x 1 , . . . , x k ∈ R d . We use R(x) to denote the Voronoi region of point x (the centers will be clear from the context). Let S 1 S 2 = (S 1 \ S 2 ) ∪ (S 2 \ S 1 ) denote the symmetric difference of S 1 and S 2 .</p><p>Theorem 5.1 Suppose that X ⊆ R d is -separated for k-means for a small enough . The following hold:</p><formula xml:id="formula_90">(i) If there are centersˆccentersˆ centersˆc 1 , . . . , ˆ c k such that cost(ˆ c 1 , . . . , ˆ c k ) ≤ α∆ 2 k−1 (X), where 0 &lt; α ≤ 1−401 2 400 , then for eachˆceachˆ eachˆc i there is a distinct optimal center c σ(i) such that |R(ˆ c i ) X σ(i) | ≤ 161 2 |X σ(i) |;</formula><p>(ii) IfˆXIfˆ IfˆX is a point set obtained by perturbing each x ∈ X i by a distance of (at most)</p><formula xml:id="formula_91">∆ k−1 (X) √ n (in any direction) then ∆ 2 k ( ˆ X) = O( 2 )∆ 2 k−1 ( ˆ X).</formula><p>Proof :</p><formula xml:id="formula_92">Let ρ = α 2 + 1 −1 . Note that ρ ≥ 400 2 1− 2 . Define X cor i = x ∈ X i : x − c i ≤ r i</formula><p>√ ρ , and  . Also by Lemma 4.14,</p><formula xml:id="formula_93">let d 2 i = 2 ∆ 2 k−1 (X)/n i . Note that r 2 i ≤ d 2 i ≤ 2 1− 2 · D 2 i .</formula><formula xml:id="formula_94">cost(ˆ c 1 , . . . , ˆ c k ) &gt; 1 ρ − 1 n i d 2 i = α∆ 2 k−1 (X) giving</formula><formula xml:id="formula_95">we have |X i | ≥ β|R(ˆ c i )| where β = 1 1+144 2 . Therefore, we get that |R(ˆ c i ) X i | ≤ 2ρ 1 + 1 β − 1 |X i | ≤ 161 2 |X i | for ≤ 1</formula><p>2 . For a point x ∈ X, we usê x to denote its perturbed image inˆXinˆ inˆX. Note that for any y ∈ R d we havê</p><formula xml:id="formula_96">havê x − y 2 ≤ 2 x − y 2 + 2 ∆ 2 k−1 (X) n .</formula><p>Consider the k-clustering ofˆXofˆ ofˆX where we assign all the perturbed points of X i to c i . The cost of this clustering forˆXforˆ forˆX is at most 2∆ 2 k (X) + 2 2 ∆ 2 k−1 (X). Conversely, one can obtain a (k − 1)-clustering of X from an optimal (k − 1)-clustering ofˆXofˆ ofˆX by assigning each x ∈ X to the center to whichˆxwhichˆ whichˆx is assigned. Thus we get that ∆ 2 k−1 (X) ≤ 2∆ 2 k−1 ( ˆ X) + 2 2 ∆ 2 k−1 (X). Combining the two bounds, we get that ∆ 2 k ( ˆ X) ≤ γ∆ 2 k−1 ( ˆ X) where γ = 8 2 1−2 2 = O( 2 ).</p><p>Theorem 5.2 Let ≤ 1 3 . Suppose that for every k-clusteringˆXclusteringˆ clusteringˆX 1 , . . . , ˆ X k of X of cost at most α 2 ∆ 2 k (X), (i) there exists a bijection σ such that ∀i, | ˆ X i X σ(i) | ≤ |X σ(i) |; AND/OR (ii) there is a bijection σ such that</p><formula xml:id="formula_97">k i=1 | ˆ X i X σ(i) | ≤ k−1 |X|.</formula><p>Then, X is α-separated for k-means.</p><p>Proof : Let R 1 , . . . , R k−1 be an optimal (k − 1)-means solution. We will construct a refinement of R 1 , . . . , R k−1 and argue that this has large Hamming distance to X 1 , . . . , X k , and hence has cost at least α 2 ∆ 2 k (X). Since the cost of R 1 , . . . , R k−1 is at least the cost of any refinement of it, this will imply that ∆ 2 k−1 (X) ≥ α 2 ∆ 2 k (X). Let R k−1 be the largest cluster. We start with an arbitrary refinement R 1 , . . . , R k−2 , ˆ X k−1 , ˆ X k wherê</p><formula xml:id="formula_98">X k−1 ∪ ˆ X k = R k−1 , ˆ X k−1 , ˆ X k = ∅.</formula><p>If the cost of this k-clustering is at least α 2 ∆ 2 k (X) then we are done. So assume that this is not the case, and let σ be the claimed bijection. For part (i), we introduce a large disagreement by splittingˆXsplittingˆ splittingˆX k−1 ∩ X σ(k−1) andˆXandˆ andˆX k ∩ X σ(k) into two equal-sized halves, A k−1 ∪ B k−1 and A k ∪ B k respectively, and "mismatching" them. More precisely, we claim that the clustering R 1 , . . . , R k−2 , X k−1 = ( ˆ X k−1 \ A k−1 ) ∪ A k , X k = ( ˆ X k \ A k ) ∪ A k−1 has large Hamming distance. For any bijection σ , if σ (i) = σ(i) for i ≤ k − 2, then</p><formula xml:id="formula_99">|R i X σ (i) | ≥ |R i ∩ X σ(i) | ≥ (1 − )|R i |; otherwise, σ (k) ∈ {σ(k − 1), σ(k)}, so |X k X σ (k) | ≥ 1− 2 |X k | ≥ |X k | since X k \ X σ(k−1) ⊇ B k , X k \ X σ(k) ⊇ A k−1 . For part (ii), since |R k−1 | ≥ |X| k−1 , we have | ˆ X k−1 ∩ X σ(k−1) | + | ˆ X k ∩ X σ(k) | ≥ 1− k−1 |X|.</formula><p>After the above mismatch operation, for any bijection σ , the total disagreement is at least</p><formula xml:id="formula_100">|X k−1 X σ (k−1) | + |X k X σ (k) | ≥ 1 2 | ˆ X k−1 ∩ X σ(k−1) | + | ˆ X k ∩ X σ(k) | ≥ 1− 2(k−1) |X| ≥ k−1 |X|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and discussion</head><p>We initiate a mathematical analysis of Lloyd-style methods that attempts to explain the performance of these heuristics. We show that if the data satisfies a natural "clusterability" or "separation" condition, then various Lloyd-style methods perform well and return a near-optimal clustering. Our chief algorithmic contribution is a novel and efficient sampling procedure for seeding Lloyd's method with initial centers, such that if the data satisfies our separation condition then (even) a single Lloyd-type descent step suffices to yield a constant-factor approximation. It may have struck the reader that there is something too good about our performance guarantees. Since we need to use only one round of Lloyd's method, we cannot possibly be taking full advantage of the algorithm, in particular, its capacity to start out with a seeding that is unbalanced across clusters and correct it by shifting centers from one cluster to another. The extent to which Lloyd's method is successful at doing so is, in fact, unclear, and for this reason there is much literature exploring the merits of different seeding procedures. Nevertheless we feel that Lloyd's method is better than we have accounted for, and that our results fall short of explaining (or predicting) the performance of Lloyd-style methods; instead, they suggest that our separation condition is perhaps too stringent (and too restrictive as a measure of data-clusterability). If so, then the main open question that emerges from our work is to demonstrate a condition weaker than ours, for which the initial seeding is not necessarily close to an optimal solution, but yet Lloyd's algorithm can be shown to converge in a small number of rounds to a near-optimal solution.</p><p>An orthogonal research direction is to explore further implications of our separation condition (or similar ones) for the k-means and possibly other clustering problems. For instance, it might be possible to obtain stronger, or more general, algorithmic results. Nissim et al. <ref type="bibr" target="#b39">[40]</ref> have obtained a result in this vein: they exploit the robustness of our separation condition to design secure, privacy-preserving ways of computing a near-optimal k-means solution when the data satisfies our separation condition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 4 . 1</head><label>41</label><figDesc>For every i, we have r 2 i ≤ 2 1− 2 · min j =i c i − c j 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 4 . 2</head><label>42</label><figDesc>With probability 1 − O(ρ), the first two centersˆccentersˆ centersˆc 1 , ˆ c 2 lie in the cores of different clusters, that is, Pr[ i =j (x ∈ X cor i and y ∈ X cor j )] = 1 − O(ρ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 4. 4</head><label>4</label><figDesc>Suppose that we have sampled i points {ˆx{ˆx 1 , . . . , ˆ x i } from X. Let X 1 , . . . , X m be all the clusters whose outer cores contain some sampled pointˆxpointˆ pointˆx j . Then Pr[ˆ x i+1 ∈ k j=m+1 X cor j ] ≥ 1 − 5ρ. Proof : For i = 0, 1 this follows from Lemma 4.2. We mimic the proof of Lemma 4.3. LetˆCLetˆ LetˆC = {ˆx{ˆx 1 , . . . , ˆ x i }. We have X out j ∩ ˆ C = ∅ for j = 1, . . . , m and X out j ∩ ˆ C = ∅ for j = m + 1, . . . , k. Let α denote the maximum over all j ≥ m + 1 of the quantity (max x∈X cor j x − c j )/d(c j , ˆ C). Here we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>we also have D ≤ c j − y + y − c ≤ 2y − c since c j is closer to y than c , and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>.</head><label></label><figDesc>Combining this with the bound on x − z , we get that for = j, x − z ≤ βx − c where β = 1 + 4 √ ρ(1− 2 )−2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>If j = i, then we get that, x − c i ≤ x − c + d +6d i √ ρ . For any other j, we have that x − c j ≤ x − c + 4d +3d j √ ρ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>C1 takes O(nN d) = O(nkd) time. The run-time analysis of the deletion phase in Section 4.1.2, shows that step C2 takes O(N 3 d) = O(k 3 d) time. So the overall running time is O(nkd + k 3 d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 4 .</head><label>4</label><figDesc>12 For each center c i , there is a centerˆccenterˆ centerˆc i such thatˆcthatˆthatˆc i − c i ≤ D i 10 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>a contradiction. Re-indexing the clusters this way, we show that σ(i) = i yields the desired mapping. This is because R(ˆ c i ) contains each point x ∈ X i such that x−c i ≤ 2D i /5, and therefore |R(ˆ c i )∩X i | ≥ (1−ρ 1 )|X i | where ρ 1 = 25 2 4(1− 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>We claim that for everyˆceveryˆ everyˆc i , there must be a</head><label></label><figDesc></figDesc><table>distinct optimal center, call it c i , such thatˆcthatˆthatˆc i − c i ≤ 

2d 

i 

√ ρ ≤ D i 
10 . Suppose not. Then, in the clustering 
aroundˆcaroundˆ aroundˆc 1 , . . . , ˆ 
c k , all the points in X cor 

i 

are assigned to a center that is at least 

2d 

i 

√ ρ away from c i . Therefore, 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An efficient k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alsabti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Workshop on High Performance Data Mining</title>
		<meeting>1st Workshop on High Performance Data Mining</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How slow is the k-means method?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd SoCG</title>
		<meeting>22nd SoCG</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="144" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local search heuristics for k-median and facility location problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khandekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Munagala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SICOMP</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="544" to="562" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximate clustering via core-sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Badoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th STOC</title>
		<meeting>34th STOC</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Refining initial points for K-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ICML</title>
		<meeting>15th ICML</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved combinatorial algorithms for the facility location and k-median problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th FOCS</title>
		<meeting>40th FOCS</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A constant-factor approximation algorithm for the k-median problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">´</forename><forename type="middle">E</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. and Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="129" to="149" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The reverse greedy algorithm for the metric k-median problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrobak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="68" to="72" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Note on grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. American Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How fast is k-means?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th COLT</title>
		<meeting>16th COLT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">735</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation schemes for clustering problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fernandez De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th ACM STOC</title>
		<meeting>35th ACM STOC</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm (with discussion)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustering large graphs via the Singular Value Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="9" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deterministic clustering with data nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>TR04-050</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Electronic Tech Report ECCC</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic clustering with data nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Effros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISIT</title>
		<meeting>ISIT</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterative optimization and simplification of hierarchical clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="147" to="178" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency vs. interpretability of classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Forgey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">768</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vector quantization and signal compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantization. IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2325" to="2384" />
			<date type="published" when="1998-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On coresets for k-means and k-median clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 36th STOC</title>
		<meeting>36th STOC</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How fast is the k-means method? Algorithmica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Har-Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sadri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="185" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Experimental designs for selecting molecules from large chemical databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Higgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Wikel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Comp. Sci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="861" to="870" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Greedy facility location algorithms analyzed using dual-fitting with factor-revealing LP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="795" to="824" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="274" to="296" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A local search approximation algorithm for k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="89" to="112" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient k-means clustering algorithm: Analysis and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="881" to="892" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Finding groups in data. An introduction to cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple linear time (1+ε)-approximation algorithm for k-means clustering in any dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th FOCS</title>
		<meeting>45th FOCS</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="454" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantization design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun., COM</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM. Special issue on quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Berkeley Symp. on Math. Statistics and Probability</title>
		<meeting>5th Berkeley Symp. on Math. Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On approximate geometric k-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matousek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="61" to="84" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantizing for minimum distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, IT</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1960-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An experimental comparison of several clustering and initialization methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th UAI</title>
		<meeting>14th UAI</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="386" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal time bounds for approximate clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="35" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An examination of the effect of six types of error perturbation on fifteen clustering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Milligan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="325" to="342" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Randomized Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Cambridge U. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smooth sensitivity and sampling in private data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rashkhodnikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th STOC</title>
		<meeting>39th STOC</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Polynomial time approximation schemes for geometric clustering problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="156" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent semantic indexing: a probabilistic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accelerating exact k-means algorithms with geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM KDD</title>
		<meeting>5th ACM KDD</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="277" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An empirical comparison of four initialization methods for the k-means algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1027" to="1040" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Acceleration of k-means and related clustering problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ALENEX</title>
		<meeting>4th ALENEX</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clustering for edge-cost minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd ACM STOC</title>
		<meeting>32nd ACM STOC</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comparison of algorithms for dissimilarity-based compound selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snarey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Terrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Willet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Graphics and Modelling</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="372" to="385" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd ACM STOC</title>
		<meeting>33rd ACM STOC</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sur la division des corp materiels en parties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Acad. Polon. Sci., C1. III</title>
		<imprint>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="801" to="804" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Tryon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Bailey</surname></persName>
		</author>
		<title level="m">Cluster Analysis</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1970" />
			<biblScope unit="page" from="147" to="150" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
