<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Negative evidences and co-occurrences in image retrieval: the benefit of PCA and whitening</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Rennes</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Cybernetics</orgName>
								<orgName type="department" key="dep2">Faculty of EE</orgName>
								<orgName type="department" key="dep3">CTU in Prague</orgName>
								<orgName type="laboratory">CMP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Negative evidences and co-occurrences in image retrieval: the benefit of PCA and whitening</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The paper addresses large scale image retrieval with short vector representations. We study dimensionality reduction by Principal Component Analysis (PCA) and propose improvements to its different phases. We show and explicitly exploit relations between i) mean subtraction and the negative evidence, i.e., a visual word that is mutually missing in two descriptions being compared, and ii) the axis de-correlation and the co-occurrences phenomenon. Finally, we propose an effective way to alleviate the quantization artifacts through a joint dimensionality reduction of multiple vocabularies. The proposed techniques are simple, yet significantly and consistently improve over the state of the art on compact image representations. Complementary experiments in image classification show that the methods are generally applicable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper mainly addresses the problem of large-scale image search and object recognition, as considered by many papers in the literature <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. More precisely, the task consists of finding images in a large image database that most closely resemble a query image based on their visual similarity. A majority of the papers rely on the bag-of-words (BOW) representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> or its derivatives, e.g., <ref type="bibr" target="#b3">[4]</ref>. These approaches are limited to search in a few million images on a single machine due to computational or memory constraints. In this paper, we will mainly focus on more scalable approaches in the spirit of recent work on compact images representations <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, where the image description is a short vector, which is subsequently encoded in compact codes using binarization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> or product quantization techniques <ref type="bibr" target="#b9">[10]</ref>. The best performing methods in this context are those that produce the vector representing an image from local features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> such as the Fisher Vectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7]</ref> or its non probabilistic version, namely the VLAD descriptor <ref type="bibr" target="#b7">[8]</ref>. In contrast to global description techniques computed from the pixels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6]</ref> in a more direct manner, these representations inherit, to some extent, the invariance properties (change in viewpoints, cropping, etc) of the local descriptors from which they are computed.</p><p>Methods generating a short code image representation commonly exploit the PCA <ref type="bibr" target="#b13">[14]</ref> to perform the dimensionality reduction. It was observed <ref type="bibr" target="#b7">[8]</ref> that the performance of BOW is even improved by PCA reduction. In the paper, we study this phenomenon. The PCA can be seen as a two step process (1) centering the data, and (2) selecting a de-correlated (orthogonal) basis of a subspace minimizing the dimensionality reduction error. We show that each of the steps has a positive impact on the retrieval, and we provide interpretation of such a behavior. Based on the analysis, we propose simple yet effective techniques to further improve the quality of BOW and VLAD representations. First, we consider the role of negative evidence: given two BOW vectors, a visual word jointly missing in both vectors is information that should receive more importance in the similarity measurement. We show relation of the negative evidence to the centering of BOW vectors (mean subtraction). Secondly, both BOW and VLAD representations are further improved by exploiting the de-correlation of the descriptor entries. Two complementary approaches are proposed 1) whitening the vector space, thereby addressing the problem of co-occurrences; and 2) by considering multiple vocabularies with a joint dimensionality reduction. Multiple vocabularies have been considered by prior art, e.g., in the hierarchical k-means <ref type="bibr" target="#b1">[2]</ref> or in the rank aggregation technique of <ref type="bibr" target="#b14">[15]</ref>. In contrast to those, our method increases the search accuracy for a fixed size of the vector describing the image. When querying the indexing structure, the memory and computational complexities are the same as when considering a unique vocabulary.</p><p>Albeit simple, the proposed techniques consistently and significantly improve the state-of-the-art image search based on short vectors, as demonstrated by our results on four popular benchmarks. Finally, we will briefly show with experiments on the PASCAL VOC'07 benchmark that the better representation for retrieval also translates to better classification results: Our short vectors obtained from BOW and combined with a linear classifier significantly outperform a soft BOW combined with a Chi-square kernel.</p><p>The paper is organized as follows: After introducing the context in Section 2, Section 3 shows the role of co-missing visual words and Section 4 exploits whitening to address the issue arising with co-occurrence over-counting. Section 5 extends it to multiple vocabularies and compares with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image description Framework</head><p>Bag-of-words. As a baseline, we first consider the regular bag-of-words representation, as proposed by Sivic and Zisserman <ref type="bibr" target="#b0">[1]</ref>. This representation extracts a global description vector from an image using the following procedure.</p><p>4. The resulting vector is subsequently normalized. As proposed in <ref type="bibr" target="#b0">[1]</ref>, we adopt the L2 normalization.</p><p>VLAD. The vector of locally aggregated descriptors <ref type="bibr" target="#b7">[8]</ref> is a simplification of the Fisher vector <ref type="bibr" target="#b10">[11]</ref>. This representation departs from BOW only in the Step 3: instead of producing the histogram of occurrences, VLAD accumulates, in the output vector of size D = k × d, the difference between the descriptors and their respective centroids.</p><p>Power-law normalization. Both the VLAD and Fisher vector representations are improved <ref type="bibr" target="#b18">[19]</ref> by using the so-called power-law normalization <ref type="bibr" target="#b6">[7]</ref>. This simple method post-processes the output image vector</p><formula xml:id="formula_0">v = (v 1 , ...v D ) as v i := |v i | β × sign(v i )</formula><p>, with 0 ≤ β &lt; 1 a fixed constant. The updated vector v is L2-normalized in turn. The impact of this post-processing is argued <ref type="bibr" target="#b18">[19]</ref> to reduce the impact of multiple matches and visual bursts <ref type="bibr" target="#b19">[20]</ref>. This variant will be considered for β = 0.5 in the following, denoted by SSR (signed square rooting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient PCA</head><p>The BOW and VLAD vectors are high dimensional. For instance, typical values of D for BOW ranges from one thousand to one million components, while VLAD vectors are k × d-dimensional, d being the dimensionality of the local descriptor. This means D = 65, 536 for the typical parameters d = 128 and k = 512. It is therefore not efficient or even feasible to perform the PCA using the covariance matrix method. However, we only need the first D first eigenvectors and eigenvalues in Equation 5. By limiting the learning set Y to a reasonable number of vectors (we used the learning image sets introduced in Section 2), one can use the dual gram method (see, e.g., Paragraph 12.1.4 in <ref type="bibr" target="#b13">[14]</ref>) to learn the matrix P and eigenvalues λ 1 to λ D . This amounts to computing the n × n gram matrix Y Y instead of the D × D covariance matrix C, and to exploiting the analytical relationship between the eigen-decomposition of these two matrices. The eigenvalue decomposition is performed using the Arnoldi algorithm, which computes the D desired eigenvectors, i.e., those associated with the largest eigenvalues, using an iterative procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Datasets</head><p>The interest of the proposed techniques is evaluated on a number of popular datasets widely used in the literature.</p><p>Oxford5k <ref type="bibr" target="#b20">[21]</ref> and Paris6k <ref type="bibr" target="#b21">[22]</ref>: These datasets are collections of images from Flickr. The 55 queries correspond to 11 distinct buildings, given by bounding boxes in 55 images from the set. The task is to retrieve all corresponding buildings. The performance is measured by mean average precision (mAP), as defined in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Holidays <ref type="formula">(</ref> images serves as queries. Each query is compared to the other 1490 images in a leave-one-out fashion. To evaluate the performance on a large scale, a distractor dataset of 1 million images downloaded from Flickr is also provided. As for Oxford5k, the performance is measured by mAP.</p><p>University of Kentucky benchmark (UKB). This image set contains 10200 images, corresponding to 2550 distinct objects and scenes (4 images per group). Each image is compared to all the others. The usual performance score is the mean number of images ranked in the first 4 positions.</p><p>For the Oxford5k and Paris datasets, we have used to the detector and descriptor used in <ref type="bibr" target="#b22">[23]</ref>, while the descriptors available online have been used for the other datasets.</p><p>Dataset for learning stages: We use an independent dataset (no intersection with the test set) to learn the visual vocabularies and for the other learning stages involved in our technique. When evaluating on Holidays, Holidays+Flickr1M and UKB, the independent dataset consists of 10000 images from Flickr. Paris6k is used to learn the meta-data associated with the evaluation on Oxford5k. Note that the idf terms do not involve any learning stage and are applied on-the-fly, based on the indexed dataset statistics.</p><formula xml:id="formula_1">s(u, v) = 1 u . v k i u i v i .<label>(1)</label></formula><p>If u i = 0, the individual contribution of the visual word with index i is the same if v i is equal or greater than 0. The difference between these cases is only taken into account by the normalization factor ||v||. This under-estimates the importance of jointly zero components, which give some limited yet important evidence on visual similarity. We, therefore, propose a simple way to better take into account this case in BOW vectors. Instead of measuring the angle between points u and v from the origin, we consider an angle between those two points measured at different point m. A good choice for m is a fraction of the mean bag-of-words vector m = α · ¯ v. The novel cosine similarity is computed by Equation (1) on transformed vectors by</p><formula xml:id="formula_2">v := v − α.¯ v.<label>(2)</label></formula><p>The value α = 1 corresponds to the case where the mean of the vector (produced from a learning set) is subtracted. Applying such a transformation, the cosine similarity gives a positive contribution for a particular visual word if it is absent (more precisely, if it appeared less than on average) in both the compared images. As a result, the similarity between bag-of-words is improved, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows the impact of this correction as a function of α when measuring the performance on the Holidays and Oxford5K benchmarks. As one can see, the proposed update gives some significant improvement, in particular for smaller vocabulary sizes, and this for a negligible computing and memory cost, as explained below.</p><p>Integration within the inverted file system. The inverted file structure allows for efficient evaluation of the cosine distance for sparse vectors by evaluating only non-negative elements of the product in Equation (1). A naive subtraction of a non-sparse vector m from all sparse vectors in the database has a severe negative effect on both the efficiency of the retrieval and the memory footprint. It is however possible to compute the new similarity measure using the same inverted file structure as for evaluating (1). The cosine distance after the subtraction is expressed as</p><formula xml:id="formula_3">s(u, v) = 1 u − m . v − m k i (u i − m i )(v i − m i ).<label>(3)</label></formula><p>For each document v in a database, the normalization factor ||v − m|| is queryindependent and therefore pre-computed. Re-writing the similarity as</p><formula xml:id="formula_4">k i (u i − m i )(v i − m i ) = u v − v m − u m + m 2 ,<label>(4)</label></formula><p>where the dot product u v is efficiently computed using the original inverted file structure as in (1). The term v m is independent of the query. It is therefore computed and stored when adding a BOW vector to the index. The term u m only depends on the query (computed once per query) and m 2 is a constant. Therefore, although the rest of this paper mainly considers short vector representations, we must mention that our shifting approach is effective when considering a regular inverted file implementation <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, at negligible memory and computational costs.</p><p>Discussion. From <ref type="figure" target="#fig_1">Fig. 2</ref> it can be seen that the impact of the negative evidence is higher for small vocabularies and diminishes for large vocabularies. This has an intuitive explanation. For large (fine) vocabularies the co-missing visual words become more common event that carries less evidence, while the presence of the same visual words provides a strong evidence. For smaller vocabularies, the relative weights of the positive and negative evidence is changing. This can be observed in <ref type="figure" target="#fig_1">Fig. 2</ref> (especially the plot for Oxford 5K dataset), where the optimal value of α (the higher value the higher weight on the negative evidence) is shifting to the left with increasing size of the vocabulary.</p><p>4 Co-occurrence over-counting: the benefit of whitening</p><p>An efficient way to obtain a shorter image vector representation consists of applying principal component analysis (PCA) dimensionality reduction directly on the BOW (or VLAD) vector <ref type="bibr" target="#b7">[8]</ref>. This, first, performs the implicit centering of the data, therefore taking into account the co-missing visual words and thereby improving the similarity measurement. Second, by concentrating the vector energy of the first components, the similarity between reduced vectors provides a reasonable approximation of the similarity before the projection. We adopt this method to produce short vectors from BOW and VLAD representations. However, it is worth noticing that an important phenomenon is ignored by such a blind dimensionality reduction, namely the problem of co-occurrences. Chum et al. <ref type="bibr" target="#b23">[24]</ref> notice that co-occurrences lead to over-count some visual patterns when comparing two image vector representations. The detector may also introduce some artificial visual word co-occurrences, for instance when an image hal-00722622, version 2 -2 Aug 2012 region is described multiple times for different orientations <ref type="bibr" target="#b24">[25]</ref>, producing two different but strongly co-occurring descriptors.</p><p>Let consider the learning set of image global descriptors (BOW or VLAD), centered according to the mean, and represented by a matrix</p><formula xml:id="formula_5">Y = [Y 1 | . . . |Y n ].</formula><p>The D-dimensional covariance matrix is estimated as C = Y × Y . The visual word co-occurrences are captured in this matrix, generating strong responses out of the diagonal and favoring the emergence of an eigenvector associated with a large eigenvalue comprising those values. An efficient way to limit the impact of co-occurrences therefore consists in whitening the data, as done in independent component analysis <ref type="bibr" target="#b25">[26]</ref>, and as implicitly performed by the Mahalanobis distance.</p><p>In our case, this whitening operation is performed jointly with the dimensionality reduction from D to D components: A given image descriptor X (BOW or VLAD) is first PCA-projected and truncated, and subsequently whitened and re-normalized to a new vectorˆXvectorˆ vectorˆX that is our short vector image representation. It is therefore obtained as follows:</p><formula xml:id="formula_6">ˆ X = diag(λ − 1 2 1 , . . . , λ − 1 2 D )P X diag(λ − 1 2 1 , . . . , λ − 1 2 D )P X ,<label>(5)</label></formula><p>where the D × D matrix P is formed by the largest eigenvectors of the covariance matrix C, and where λ i is the eigenvalue associated with the i th largest eigenvector. Comparing two vectors obtained after this dimensionality reduction with the Euclidean distance is therefore similar to using a Mahalanobis distance, but differs from it in that the vectors are truncated and re-normalized. The comparison is efficiently performed by comparing the reduced vectors using the Cosine similarity. The re-normalization step turns out to be critical for a better comparison metric (up to 10% of mAP of difference on the Holidays dataset).</p><p>Impact on performance. For the sake of consistency, the vector dimensionality is reduced to D'=128 dimensions in all the experiments presented in this paper. <ref type="figure" target="#fig_2">Figure 3</ref> gives the impact of the dimensionality reduction, of the SSR componentwise normalization, and of our whitening technique, which is shown to provide a large improvement over the BOW baseline.</p><p>Remarks:</p><p>-The idf weighting terms can not be longer applied on-the-fly with the dimensionality reduction, and are therefore be learned on the independent dataset.</p><p>-As a side effect of the dimensionality reduction, two ambiguous visual words i and j generate a higher value than for other tuples in the covariance matrix, which favors the projection of these visual words to the same component in the projected vector. This phenomenon can be observed when reconstructing the BOW vector from its PCA projection: The component of the other visual word is "hallucinated". -For large values of D , the whitening stage negatively impacts the performance by magnifying the noise on the low-energy components. This issue is addressed by using a robust PCA/whitening method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Joint de-correlation of multiple vocabularies</head><p>It is well known that the quantization effect have significant impact on the retrieval quality. Different approaches were suggested to overcome the problem, ranging from hierarchical quantization <ref type="bibr" target="#b1">[2]</ref>, soft assignment <ref type="bibr" target="#b21">[22]</ref>, to Hamming embedding <ref type="bibr" target="#b3">[4]</ref>. We show that the quantization effects are alleviated by multiple quantization. However, straightforward concatenation of the BOW representations not only linearly increases the memory requirements, but improve only marginally the retrieval results, see <ref type="figure" target="#fig_3">Fig. 4</ref> or <ref type="bibr" target="#b14">[15]</ref>. The different BOW representations are strongly correlated. We show that the PCA removes the correlation, while preserving the additional information from the different quantizations.</p><p>Results outperforming the state of the art for short image representation are achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Related work on multiple vocabularies</head><p>Some prior art has proposed to use multiple vocabularies to improve the quality of the search, at the cost of reduced efficiency and increased memory usage. For instance, a common and simple strategy consists in simply considering the concatenating of the different BOW vectors as the image representation, as done by Nister et al. <ref type="bibr" target="#b1">[2]</ref>, who consider a hierarchical quantization method where the intermediate nodes correspond to smaller vocabularies. A late fusion technique based on rank aggregation was also proposed <ref type="bibr" target="#b14">[15]</ref>, but several inverted files have to be stored and queried in parallel. In addition, those techniques do not take into account the relationship between the vocabularies: their output is processed independently without considering the dependencies between the quantizers. A more popular alternative consists of using multiple <ref type="bibr" target="#b14">[15]</ref> or soft <ref type="bibr" target="#b21">[22]</ref> assignment. This also increases the query time when the search is performed using an inverted file 1 . Since we are more particularly interested by image search in larger databases, taking care of the memory size of the representation is critical in order to keep the indexing structure in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Joint reduction of multiple vocabularies</head><p>The key points of our multiple vocabulary method, which departs from those proposed in the literature, is that it performs the joint dimensionality reduction of the BOW vectors produced for the different vocabularies, and apply in turn the whitening technique proposed the previous section to correct the artifacts resulting from the use of multiple vocabularies. Indeed, the different vocabularies are redundant: two descriptors assigned to the same visual word for one vocabulary have higher probability to be assigned to the same visual word for another vocabulary, leading to co-occurrences as those mentioned in Subsection 4.</p><p>Another difference with <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b26">[27]</ref> is that we consider overlapping quantizers. This, jointly with the dimensionality reduction, better addresses the problem of quantization artifacts than these approaches, or of multiple or soft quantization techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>, as demonstrated later <ref type="table">(Table 2)</ref> by comparing multiple vocabulary with VLAD (hard assignment) with Fisher Kernel (soft assignment based on a Gaussian mixture model) with a single vocabulary.</p><p>We therefore propose the following reduction for multiple vocabularies:  <ref type="table">Table 1</ref>. Performance of vocabularies of identical and multiple sizes (Holidays). Complexity: number of vectors comparison per local descriptor when constructing the aggregated representation formed by all BOW vectors. Weighting: see text for details.</p><note type="other">Vocabularies weighting mAP (%) complexity 2 × 32k N/A 53.3 65,536 4 ×</note><p>1. The BOW or VLAD vectors are produced independently, using SSR componentwise normalization (see Section 2). The idf term is ignored, as it occurs that its influence is limited with multiple vocabularies. The SSR component-wise normalization is applied and the concatenated vector is normalized.</p><p>2. The different vectors are jointly reduced and whitened according to the guidelines of Subsection 4. <ref type="figure" target="#fig_3">Figure 4</ref> shows that multiple vocabularies, used jointly with our dimensionality reduction technique, provides an significant improvement for both BOW and VLAD representations, and this for a fixed output vector size D'. Larger vocabularies for BOW are not necessarily better than smaller ones: although k=32k provides better results on Holidays with a single vocabulary, we observe the opposite outcome for multiple vocabularies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Merging vocabularies of different sizes</head><p>The goal of this subsection is to address the trade-off between absolute search quality (for a given vector size), and the quantization cost, and to provide a comparison with similar methods of the literature. The following analysis is mainly intended for BOW, since VLAD typically uses vocabularies of much smaller sizes (e.g., k=256). Although the quantization cost does not depend on the dataset size, it delays the query (jointly with the extraction of the descriptor from the image), which might be critical for some applications. For reference, quantizing 2000 local descriptors of a query image, for 4 vocabularies comprising k = 8, 192 centroids each, takes 0.45s on 12 cores, using an efficient multithreaded implementation of exhaustive search (exact). The timings are in this case proportional to k.</p><p>To reduce the quantization cost, we consider vocabularies of different sizes, in the spirit of the hierarchical k-means method <ref type="bibr" target="#b1">[2]</ref> and of the pyramid match kernel <ref type="bibr" target="#b26">[27]</ref>. Vocabularies of different sizes have a different importance, and therefore their respective contribution should be adapted. We compare four different approaches to adjust the contribution of the vocabularies:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Vocabulary size(s) Holidays Oxford5k UKB GIST <ref type="bibr" target="#b12">[13]</ref> N/A 36. 2. Similar to <ref type="bibr" target="#b26">[27]</ref>, the weight of the vocabulary is proportional to its size (to the number of bins).</p><p>3. We consider weights proportional to the logarithm of the vocabulary size.</p><p>4. Similar to <ref type="bibr" target="#b1">[2]</ref>, the weights are determined by idf after all vocabularies are concatenated.</p><p>In the first three approaches (referred to as"1", "k" and "log k"), each descriptor for each vocabulary is first transformed by SSR and L2-normalized, then multiplied by the vocabulary weight. The descriptors of different vocabularies are concatenated and finally, the concatenated vectors are L2-normalized. In the fourth weighing scheme, the idf weighting is applied to the vectors after the concatenation of the vocabularies, as proposed in <ref type="bibr" target="#b1">[2]</ref>. <ref type="table">Table 1</ref> shows the results with when considering multiple vocabularies of fixed and different sizes, and compares the different weighting techniques. For a fixed quantization cost, in this experiment the best choice is to use vocabularies of different sizes and our log weighting technique. Note however that the improvement of this latter is only 1% compared with equal weights for all sizes. <ref type="table">Table 2</ref> compares our method to the state-of-the-art on short vector representations. The results obtained with the proposed short vector construction are consistently better than reference results. The improvement brought our method is higher when applied to BOW than to VLAD. Compared with BOW of 20k centroids reduced to 128D <ref type="bibr" target="#b18">[19]</ref>, our method, when applied to BOW with 4 vocabularies of size 8k, increases the mAP of +14.8% on Holidays, +21.9% on Oxford5k. The UKB score is 3.19/4 (BOW reference: 2.95). As a result, the BOW-based representation is competitive, when not better, than the best results reported with PCA-reduced VLAD and Fisher representations, where in <ref type="bibr" target="#b18">[19]</ref> these representations are shown to significantly outperform BOW. By applying our method on the VLAD representation, we still obtain an improvement of +5.7% over the state of the art. The improvement is not significant on UKB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the state-of-the-art</head><p>Method mAP on Holidays VLAD <ref type="bibr" target="#b9">[10]</ref> 46.0 Fisher vector <ref type="bibr" target="#b18">[19]</ref> 50.6 Ours/BOW -4×(k=8k) 49.8 Ours/VLAD -4×(k=256) 53.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Encoding our short vectors with compact codes</head><p>The better results obtained with shorter vectors lead to better results when further coding the vectors using a compressed-domain approximate nearest neighbors search technique <ref type="bibr" target="#b9">[10]</ref>, as shown by <ref type="table" target="#tab_3">Table 3</ref>. With codes of 16 bytes, we increase the mAP of the BOW baseline by +3.6% of mAP on Holidays. with VLAD (4×k=256) on Holidays, we outperform the state-of-the-art coded Fisher Vector <ref type="bibr" target="#b18">[19]</ref> by +2.5% of mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Large scale experiments</head><p>We have evaluated our approach on one million images, by merging the Holidays dataset with the Flickr1M distractor set, as done in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Our method is compared (from curves in <ref type="bibr" target="#b18">[19]</ref>) with the BOW representation (k=200k).</p><p>The results are presented in <ref type="figure" target="#fig_4">Figure 5</ref>. Our approach significantly outperforms the baseline, and this by using an image representation which is a 128 dimensional vector only, i.e., using significantly less memory than sparse BOW, which typically requires 4 bytes per encoded local descriptors. The efficiency is also much better than BOW. With an efficient implementation of exhaustive search, querying the whole 500 query images from Holidays takes 3.08 seconds using 12 cores of a 3 Ghz machine, which corresponds to 6 ms per query. This is about two orders of magnitude faster than the timings reported for BOW <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Improving BOW for classification</head><p>Although the primary goal of this paper is to consider image retrieval on a large scale with short vectors, we report some preliminary results showing the interest of our method in a context of classification with very short vectors and efficient linear classifiers. For this purpose, we improve the BOW baseline with our approach (SSR, whitening joint de-correlation of 4 vocabularies) and compare to methods combined with a linear classifier. On Pascal VOC'07 <ref type="bibr" target="#b27">[28]</ref>, by considering the same protocol as the one proposed in <ref type="bibr" target="#b28">[29]</ref>, our technique is significantly better than the corresponding BOW: we obtain mAP=46.9% with D'=256 dimensions, instead of 41.4% for BOW with 4k dimensions. The result is approximately the same of the one of spatial pyramid matching (SPM, <ref type="bibr" target="#b29">[30]</ref>) with a linear classifier, but with a vector which is 100 times shorter.  <ref type="bibr" target="#b18">[19]</ref>) and our method with a 128-D vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Different techniques to improve dimensionality reduction by PCA for large scale image retrieval were proposed. First, a solution is proposed for giving more importance to jointly non-occurring visual words, leading to improved image search quality with bag-of-features at a negligible cost in memory and computational complexity. This approach can be also integrated into an inverted file. Then, we considered the problem of co-occurring and correlated visual words, jointly with the dimensionality reduction and the use of multiple vocabularies. This method produces short vectors (128-dimensional, i.e., the size of a single SIFT local descriptor) yielding a high retrieval accuracy, as demonstrated by our results on popular image search benchmarks. Finally, it was shown on image classification that the methods are generally applicable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Empirical distribution (Holidays dataset) of the similarities for true (TP) and false positives (FP) before (regular BOW) and after the proposed correction (shifted BOW). Observe the better separation of true and false positives, which are centered on zero with the shifted BOW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. BOW for Holidays and Oxford5K: mAP performance as a function of α, for different vocabulary sizes k. The optimum values outperform the state-of-the-art for pure-BOW approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Impact of the different steps on search accuracy (Holidays) for BOW vectors, as a function of vocabulary size k. The proposed whitening step is denoted by Wh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The leftmost plot gives the BOW baseline when concatenating different numbers of BOW into a single vector (of increasing dimensionality), which provides only a small improvement, while linearly increasing the memory requirements. The middle and right plots show the search accuracy with the proposed joint reduction of the vocabularies to a fixed vector size of 128 components. Observe that the improvement brought by the use of several hash functions is comparatively better with our technique than for the concatenation of BOWs, thanks to the joint dimensionality reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Holidays+1M distractors: proportion of true positives returned in the first r ranks (recall@r), for BOW with 200k (from [19]) and our method with a 128-D vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 . Comparison against the state of the art on image representations with</head><label>3</label><figDesc></figDesc><table>short 
</table></figure>

			<note place="foot" n="3"> Exploiting evidences from co-missing words Being produced as a weighted histogram of occurrences of visual words, the regular BOW representation contains only non-negative values. Let consider the cosine measure for similarity s(u, v) between BOW vectors u and v, i.e.,</note>

			<note place="foot">hal-00722622, version 2 -2 Aug 2012</note>

			<note place="foot" n="1"> The memory requirement is similarly increased if this assignment is performed in a symmetrical manner on query and database sides. hal-00722622, version 2 -2 Aug 2012</note>

			<note place="foot" n="1">. The same unit weight is applied for all vocabularies. hal-00722622, version 2 -2 Aug 2012 Negative evidences and co-occurrences in image retrieval</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop Statistical Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Small codes and large databases for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large-scale image retrieval with compressed Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing. In: NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<editor>ECCV.</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image search using the contextual dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scale and affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the burstiness of visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient representation of local geometry for large scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of co-occurrence in sparse high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>BMVC.</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
