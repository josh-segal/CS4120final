<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
							<email>singh@cae</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering Department of Computer Sciences</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison University of Wisconsin -Madison Madison</orgName>
								<address>
									<postCode>53706, 53706</postCode>
									<settlement>Madison</settlement>
									<region>WI, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
							<email>nowak@engr.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering Department of Computer Sciences</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison University of Wisconsin -Madison Madison</orgName>
								<address>
									<postCode>53706, 53706</postCode>
									<settlement>Madison</settlement>
									<region>WI, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering Department of Computer Sciences</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison University of Wisconsin -Madison Madison</orgName>
								<address>
									<postCode>53706, 53706</postCode>
									<settlement>Madison</settlement>
									<region>WI, WI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conflicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a finite sample analysis that characterizes the value of un-labeled data and quantifies the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can significantly outperform supervised learning, in finite sample regimes and sometimes also in terms of error convergence rates.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Labeled data can be expensive, time-consuming and difficult to obtain in many applications. Semisupervised learning (SSL) aims to capitalize on the abundance of unlabeled data to improve learning performance. Empirical evidence suggests that in certain favorable situations unlabeled data can help, while in other situations it does not. As a result, there have been several recent attempts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> at developing a theoretical understanding of semi-supervised learning. It is wellaccepted that unlabeled data can help only if there exists a link between the marginal data distribution and the target function to be learnt. Two common types of links considered are the cluster assumption <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> which states that the target function is locally smooth over subsets of the feature space delineated by some property of the marginal density (but may not be globally smooth), and the manifold assumption <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> which assumes that the target function lies on a low-dimensional manifold. Knowledge of these sets, which can be gleaned from unlabeled data, simplify the learning task. However, recent attempts at characterizing the amount of improvement possible under these links only provide a partial and sometimes apparently conflicting (for example, <ref type="bibr" target="#b3">[4]</ref> vs. <ref type="bibr" target="#b5">[6]</ref>) explanations of whether or not, and to what extent semi-supervised learning helps. In this paper, we bridge the gap between these seemingly conflicting views and develop a minimax framework based on finite sample bounds to identify situations in which unlabeled data help to improve learning. Our results quantify both the amount of improvement possible using SSL as well as the the relative value of unlabeled data.</p><p>We focus on learning under a cluster assumption that is formalized in the next section, and establish that there exist nonparametric classes of distributions, denoted P XY , for which the decision sets (over which the target function is smooth) are discernable from unlabeled data. Moreover, we show that there exist clairvoyant supervised learners that, given perfect knowledge of the decision sets denoted by D, can significantly outperform any generic supervised learner f n in these Figure 1: (a) Two separated high density sets with different labels that (b) cannot be discerned if the sample size is too small, but (c) can be estimated if sample density is high enough.</p><p>classes. That is, if R denotes a risk of interest, n denotes the labeled data sample size, f D,n denotes the clairvoyant supervised learner, and E denotes expectation with respect to training data, then</p><formula xml:id="formula_0">sup PXY E[R( f D,n )] &lt; inf fn sup PXY E[R(f n )]</formula><p>. Based on this, we establish that there also exist semi-supervised learners, denoted f m,n , that use m unlabeled examples in addition to the n labeled examples in order to estimate the decision sets, which perform as well as f D,n , provided that m grows appropriately relative to n. Specifically, if the error bound for f D,n decays polynomially (exponentially) in n, then the number of unlabeled data m needs to grow polynomially (exponentially) with the number of labeled data n. We provide general results for a broad range of learning problems using finite sample error bounds. Then we examine a concrete instantiation of these general results in the regression setting by deriving minimax lower bounds on the performance of any supervised learner and compare that to upper bounds on the errors of f D,n and f m,n .</p><p>In their seminal papers, Castelli and Cover <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> suggested that in the classification setting the marginal distribution can be viewed as a mixture of class conditional distributions. If this mixture is identifiable, then the classification problem may reduce to a simple hypothesis testing problem for which the error converges exponentially fast in the number of labeled examples. The ideas in this paper are similar, except that we do not require identifiability of the mixture component densities, and show that it suffices to only approximately learn the decision sets over which the label is smooth.</p><p>More recent attempts at theoretically characterizing SSL have been relatively pessimistic. Rigollet <ref type="bibr" target="#b2">[3]</ref> establishes that for a fixed collection of distributions satisfying a cluster assumption, unlabeled data do not provide an improvement in convergence rate. A similar argument was made by Lafferty and Wasserman <ref type="bibr" target="#b3">[4]</ref>, based on the work of Bickel and Li <ref type="bibr" target="#b9">[10]</ref>, for the manifold case. However, in a recent paper, Niyogi <ref type="bibr" target="#b5">[6]</ref> gives a constructive example of a class of distributions supported on a manifold whose complexity increases with the number of labeled examples, and he shows that the error of any supervised learner is bounded from below by a constant, whereas there exists a semisupervised learner that can provide an error bound of O(n −1/2 ), assuming infinite unlabeled data. In this paper, we bridge the gap between these seemingly conflicting views. Our arguments can be understood by the simple example shown in <ref type="figure">Fig. 1</ref>, where the distribution is supported on two component sets separated by a margin γ and the target function is smooth over each component. Given a finite sample of data, these decision sets may or may not be discernable depending on the sampling density (see <ref type="figure">Fig. 1(b)</ref>, (c)). If γ is fixed (this is similar to fixing the class of cluster-based distributions in <ref type="bibr" target="#b2">[3]</ref> or the manifold in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>), then given enough labeled data a supervised learner can achieve optimal performance (since, eventually, it operates in regime (c) of <ref type="figure">Fig. 1</ref>). Thus, in this example, there is no improvement due to unlabeled data in terms of the rate of error convergence for a fixed collection of distributions. However, since the true separation between the component sets is unknown, given a finite sample of data, there always exists a distribution for which these sets are indiscernible (e.g., γ → 0). This perspective is similar in spirit to the argument in <ref type="bibr" target="#b5">[6]</ref>. We claim that meaningful characterizations of SSL performance and quantifications of the value of unlabeled data require finite sample error bounds, and that rates of convergence and asymptotic analysis may not capture the distinctions between SSL and supervised learning. Simply stated, if the component density sets are discernable from a finite sample size m of unlabeled data but not from a finite sample size n &lt; m of labeled data, then SSL can provide better performance than supervised learning. We also show that there are certain plausible situations in which SSL yields rates of convergence that cannot be achieved by any supervised learner.</p><p>γ positive</p><formula xml:id="formula_1">γ negative γ γ x 1 x 2 g (x )</formula><p>2 <ref type="formula" target="#formula_7">(2)</ref> g <ref type="formula">(x )</ref> 1 <ref type="formula">(1)</ref> g <ref type="formula">(x )</ref> 1 <ref type="formula" target="#formula_7">(2)</ref> g <ref type="formula">(x )</ref> 2 (1)</p><formula xml:id="formula_2">x 2 x 1 g (x )</formula><p>2 <ref type="formula" target="#formula_7">(2)</ref> g <ref type="formula">(x )</ref> 1 <ref type="formula">(1)</ref> g <ref type="formula">(x )</ref> 2 <ref type="formula">(1)</ref> g <ref type="formula">(x )</ref> 1 <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Characterization of model distributions under the cluster assumption</head><p>Based on the cluster assumption <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, we define the following collection of joint distributions P XY (γ) = P X × P Y |X indexed by a margin parameter γ. Let X, Y be bounded random variables with marginal distribution P X ∈ P X and conditional label distribution</p><formula xml:id="formula_3">P Y |X ∈ P Y |X , supported on the domain X = [0, 1] d . The marginal density p(x) = K k=1 a k p k (x)</formula><p>is the mixture of a finite, but unknown, number of component densities {p k } K k=1 , where K &lt; ∞. The unknown mixing proportions a k ≥ a &gt; 0 and K k=1 a k = 1. In addition, we place the following assumptions on the mixture component densities: 1. p k is supported on a unique compact, connected set C k ⊆ X with Lipschitz boundaries. Specifically, we assume the following form for the component support sets: (See <ref type="figure" target="#fig_1">Fig. 2</ref> for d=2 illustration.)</p><formula xml:id="formula_4">C k = {x ≡ (x 1 , . . . , x d ) ∈ X : g (1) k (x 1 , . . . , x d−1 ) ≤ x d ≤ g (2) k (x 1 , . . . , x d−1 )}, where g (1) k (·), g (2)</formula><p>k (·) are d − 1 dimensional Lipschitz functions with Lipschitz constant L. 1 2. p k is bounded from above and below, 0 &lt; b ≤ p k ≤ B. 3. p k is Hölder-α smooth on C k with Hölder constant K 1 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Let the conditional label density on C k be denoted by p k (Y |X = x). Thus, a labeled training point (X, Y ) is obtained as follows. With probability a k , X is drawn from p k and Y is drawn from p k (Y |X = x). In the supervised setting, we assume access to n labeled data</p><formula xml:id="formula_5">L = {X i , Y i } n i=1</formula><p>drawn i.i.d according to P XY ∈ P XY (γ), and in the semi-supervised setting, we assume access to m additional unlabeled data U = {X i } m i=1 drawn i.i.d according to P X ∈ P X . Let D denote the collection of all non-empty sets obtained as intersections of</p><formula xml:id="formula_6">{C k } K k=1 or their complements {C c k } K k=1</formula><p>, excluding the set ∩ K k=1 C c k that does not lie in the support of the marginal density. Observe that |D| ≤ 2 K , and in practical situations the cardinality of D is much smaller as only a few of the sets are non-empty. The cluster assumption is that the target function will be smooth on each set D ∈ D, hence the sets in D are called decision sets. At this point, we do not consider a specific target function.</p><p>The collection P XY is indexed by a margin parameter γ, which denotes the minimum width of a decision set or separation between the component support sets C k . The margin γ is assigned a positive sign if there is no overlap between components, otherwise it is assigned a negative sign as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Formally, for j, k ∈ {1, . . . , K}, let</p><formula xml:id="formula_7">d jk := min p,q∈{1,2} g (p) j − g (q) k ∞ j 񮽙 = k, d kk := g (1) k − g<label>(2)</label></formula><p>k ∞ .</p><p>Then the margin is defined as</p><formula xml:id="formula_8">γ = σ · min j,k∈{1,...,K} d jk , where σ = 1 if C j ∩ C k = ∅ ∀j 񮽙 = k −1 otherwise .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Decision Sets</head><p>Ideally, we would like to break a given learning task into separate subproblems on each D ∈ D since the target function is smooth on each decision set. Note that the marginal density p is also smooth within each decision set, but exhibits jumps at the boundaries since the component densities are bounded away from zero. Hence, the collection D can be learnt from unlabeled data as follows: 1) Marginal density estimation -The procedure is based on the sup-norm kernel density estimator proposed in <ref type="bibr" target="#b13">[14]</ref>. </p><formula xml:id="formula_9">(x) is p(x) = 1 mh d m m i=1 G(H −1 m (X i − [x])).</formula><p>2) Decision set estimation -Two points x 1 , x 2 ∈ X are said to be connected, denoted by x 1 ↔ x 2 , if there exists a sequence of points</p><formula xml:id="formula_10">x 1 = z 1 , z 2 , . . . , z l−1 , z l = x 2 such that z 2 , . . . , z l−1 ∈ U, z j −z j+1 ≤ 2 √ dh m</formula><p>, and for all points that satisfy</p><formula xml:id="formula_11">z i −z j ≤ h m log m, | p(z i )− p(z j )| ≤ δ m := (log m) −1/3</formula><p>. That is, there exists a sequence of 2 √ dh m -dense unlabeled data points between x 1 and x 2 such that the marginal density varies smoothly along the sequence. All points that are pairwise connected specify an empirical decision set. This decision set estimation procedure is similar in spirit to the semi-supervised learning algorithm proposed in <ref type="bibr" target="#b14">[15]</ref>. In practice, these sequences only need to be evaluated for the test and labeled training points.</p><p>The following lemma shows that if the margin is large relative to the average spacing m −1/d between unlabeled data points, then with high probability, two points are connected if and only if they lie in the same decision set D ∈ D, provided the points are not too close to the decision boundaries. The proof sketch of the lemma and all other results are deferred to Section 7. Lemma 1. Let ∂D denote the boundary of D and define the set of boundary points as</p><formula xml:id="formula_12">B = {x : inf z∈∪D∈D ∂D x − z ≤ 2 √ dh m }.</formula><p>If |γ| &gt; C o (m/(log m) 2 ) −1/d , where C o = 6 √ dκ 0 , then for all p ∈ P X , all pairs of points x 1 , x 2 ∈ supp(p) \ B and all D ∈ D, with probability &gt; 1 − 1/m, x 1 , x 2 ∈ D if and only if x 1 ↔ x 2 for large enough m ≥ m 0 , where m 0 depends only on the fixed parameters of the class P XY (γ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SSL Performance and the Value of Unlabeled Data</head><p>We now state our main result that characterizes the performance of SSL relative to supervised learning and follows as a corollary to the lemma stated above. Let R denote a risk of interest and </p><formula xml:id="formula_13">E( f ) = R( f ) − R * ,</formula><formula xml:id="formula_14">PXY (γ) E[E( f D,n )] ≤ ǫ 2 (n).</formula><p>Then there exists a semi-supervised learner f m,n such that if |γ|</p><formula xml:id="formula_15">&gt; C o (m/(log m) 2 ) −1/d , sup PXY (γ) E[E( f m,n )] ≤ ǫ 2 (n) + O 1 m + n m (log m) 2 −1/d .</formula><p>This result captures the essence of the relative characterization of semi-supervised and supervised learning for the margin based model distributions. It suggests that if the sets D are discernable using unlabeled data (the margin is large enough compared to average spacing between unlabeled data points), then there exists a semi-supervised learner that can perform as well as a supervised learner with clairvoyant knowledge of the decision sets, provided m ≫ n so that (n/ǫ 2 (n)) d = O(m/(log m) 2 ) implying that the additional term in the performace bound for f m,n is negligible compared to ǫ 2 (n). This indicates that if ǫ 2 (n) decays polynomially (exponentially) in n, then m needs to grow polynomially (exponentially) in n.</p><p>Further, suppose that the following finite sample lower bound holds for any supervised learner:</p><formula xml:id="formula_16">inf fn sup PXY (γ) E[E(f n )] ≥ ǫ 1 (n).</formula><p>If ǫ 2 (n) &lt; ǫ 1 (n), then there exists a clairvoyant supervised learner with perfect knowledge of the decision sets that outperforms any supervised learner that does not have this knowledge. Hence, Corollary 1 implies that SSL can provide better performance than any supervised learner provided (i) m ≫ n so that (n/ǫ 2 (n)) d = O(m/(log m) 2 ), and (ii) knowledge of the decision sets simplifies the supervised learning task, so that ǫ 2 (n) &lt; ǫ 1 (n). In the next section, we provide a concrete application of this result in the regression setting. As a simple example in the binary classification setting, if p(x) is supported on two disjoint sets and if P (Y = 1|X = x) is strictly greater than 1/2 on one set and strictly less than 1/2 on the other, then perfect knowledge of the decision sets reduces the problem to a hypothesis testing problem for which ǫ 2 (n) = O(e −ζ n ), for some constant ζ &gt; 0. However, if γ is small relative to the average spacing n −1/d between labeled data points, then ǫ 1 (n) = cn −1/d where c &gt; 0 is a constant. This lower bound follows from the minimax lower bound proofs for regression in the next section. Thus, an exponential improvement is possible using semi-supervised learning provided m grows exponentially in n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Density-adaptive Regression</head><p>Let Y denote a continuous and bounded random variable. Under squared error loss, the target function is f (x) = E[Y |X = x], and E( This implies that the overall regression function f (x) is piecewise Hölder-α smooth; i.e., it is Hölder-α smooth on each D ∈ D, except possibly at the component boundaries. <ref type="bibr" target="#b1">2</ref> Since a Hölder-α smooth function can be locally well-approximated by a Taylor polynomial, we propose the following semi-supervised learner that performs local polynomial fits within each empirical decision set, that is, using training data that are connected as per the definition in Section 3. While a spatially uniform estimator suffices when the decision sets are discernable, we use the following spatially adaptive estimator proposed in Section 4.1 of <ref type="bibr" target="#b11">[12]</ref>. This ensures that when the decision sets are indiscernible using unlabeled data, the semi-supervised learner still achieves an error bound that is, up to logarithmic factors, no worse than the minimax lower bound for supervised learners.</p><formula xml:id="formula_17">f ) = E[( f (X) − f (X)) 2 ]. Recall that p k (Y |X = x)</formula><p>f m,n,x (·) = arg min</p><formula xml:id="formula_18">f ′ ∈Γ n i=1 (Y i − f ′ (X i )) 2 1 x↔Xi + pen(f ′ ) and f m,n (x) ≡ f m,n,x (x)</formula><p>Here 1 x↔Xi is the indicator of x ↔ X i and Γ denotes a collection of piecewise polynomials of degree [α] (the maximal integer &lt; α) defined over recursive dyadic partitions of the domain X = [0, 1] d with cells of sidelength between 2 −⌈log(n/ log n)/(2α+d)⌉ and 2 −⌈log(n/ log n)/d⌉ . The penalty term pen(f ′ ) is proportional to log( n i=1 1 x↔Xi ) #f ′ , where #f ′ denotes the number of cells in the recursive dyadic partition on which f ′ is defined. It is shown in <ref type="bibr" target="#b11">[12]</ref> that this estimator yields a finite sample error bound of n −2α/(2α+d) for Hölder-α smooth functions, and max{n −2α/(2α+d) , n −1/d } for piecewise Hölder-α functions, ignoring logarithmic factors.</p><p>Using these results from <ref type="bibr" target="#b11">[12]</ref> and Corollary 1, we now state finite sample upper bounds on the semisupervised learner (SSL) described above. Also, we derive finite sample minimax lower bounds on the performance of any supervised learner (SL). Our main results are summarized in the following table, for model distributions characterized by various values of the margin parameter γ. A sketch of the derivations of the results is provided in Section 7.3. Here we assume that dimension d ≥ 2α/(2α − 1). If d &lt; 2α/(2α − 1), then the supervised learning error due to to not resolving the decision sets (which behaves like n −1/d ) is smaller than error incurred in estimating the target function itself (which behaves like n −2α/(2α+d) ). Thus, when d &lt; 2α/(2α − 1), the supervised regression error is dominated by the error in smooth regions and there appears to be no benefit to using a semi-supervised learner. In the table, we suppress constants and log factors in the bounds, and also assume that m ≫ n 2d so that (n/ǫ 2 (n)) d = O(m/(log m) 2 ). The constants c o and C o only depend on the fixed parameters of the class P XY (γ) and do not depend on γ.</p><p>Margin range SSL upper bound SL lower bound SSL helps γ ǫ 2 (n)</p><formula xml:id="formula_19">ǫ 1 (n) γ ≥ γ 0 n −2α/(2α+d) n −2α/(2α+d) No γ ≥ c o n −1/d n −2α/(2α+d) n −2α/(2α+d) No c o n −1/d &gt; γ ≥ C o ( m (log m) 2 ) −1/d n −2α/(2α+d) n −1/d Yes C o ( m (log m) 2 ) −1/d &gt; γ ≥ −C o ( m (log m) 2 ) −1/d n −1/d n −1/d No −C o ( m (log m) 2 ) −1/d &gt; γ n −2α/(2α+d) n −1/d Yes −γ 0 &gt; γ n −2α/(2α+d) n −1/d Yes</formula><p>If γ is large relative to the average spacing between labeled data points n −1/d , then a supervised learner can discern the decision sets accurately and SSL provides no gain. However, if γ &gt; 0 is small relative to n −1/d , but large with respect to the spacing between unlabeled data points m −1/d , then the proposed semi-supervised learner provides improved error bounds compared to any supervised learner. If |γ| is smaller than m −1/d , the decision sets are not discernable with unlabeled data and SSL provides no gain. However, notice that the performance of the semi-supervised learner is no worse than the minimax lower bound for supervised learners. In the γ &lt; 0 case, if −γ larger than m −1/d , then the semi-supervised learner can discern the decision sets and achieves smaller error bounds, whereas these sets cannot be as accurately discerned by any supervised learner. For the overlap case (γ &lt; 0), supervised learners are always limited by the error incurred due to averaging across decision sets (n −1/d ). In particular, for the collection of distributions with γ &lt; −γ 0 , a faster rate of error convergence is attained by SSL compared to SL, provided m ≫ n 2d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we develop a framework for evaluating the performance gains possible with semisupervised learning under a cluster assumption using finite sample error bounds. The theoretical characterization we present explains why in certain situations unlabeled data can help to improve learning, while in other situations they may not. We demonstrate that there exist general situations under which semi-supervised learning can be significantly superior to supervised learning in terms of achieving smaller finite sample error bounds than any supervised learner, and sometimes in terms of a better rate of error convergence. Moreover, our results also provide a quantification of the relative value of unlabeled to labeled data. While we focus on the cluster assumption in this paper, we conjecture that similar techniques can be applied to quantify the performance of semi-supervised learning under the manifold assumption as well. In particular, we believe that the use of minimax lower bounding techniques is essential because many of the interesting distinctions between supervised and semi-supervised learning occur only in finite sample regimes, and rates of convergence and asymptotic analyses may not capture the complete picture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Proofs</head><p>We sketch the main ideas behind the proofs here, please refer to <ref type="bibr" target="#b12">[13]</ref> for details. Since the component densities are bounded from below and above, define p min := b min k a k ≤ p(x) ≤ B =: p max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Proof of Lemma 1</head><p>First, we state two relatively straightforward results about the proposed kernel density estimator.</p><p>Theorem 1 (Sup-norm density estimation of non-boundary points). Consider the kernel density estimator p(x) proposed in Section 3. If the kernel G satisfies supp(</p><formula xml:id="formula_20">G) = [−1, 1] d , 0 &lt; G ≤ G max &lt; ∞, [−1,1] d G(u)du = 1 and [−1,1] d u j G(u)du = 0 for 1 ≤ j ≤ [α]</formula><p>, then for all p ∈ P X , with probability at least 1 − 1/m,</p><formula xml:id="formula_21">sup x∈supp(p)\B |p(x) − p(x)| = O h min(1,α) m + log m/(mh d m ) =: ǫ m . Since E[(f (X) − f D,n (X)) 2 ] = D∈D E[(f (X) − f D,n (X)) 2 1 X∈D ]P (D)</formula><p>, taking expectation over n D ∼Binomial(n, P (D)) and summing over all decision sets recalling that |D| is a finite constant, the overall error of f D,n scales as n −2α/(2α+d) , ignoring logarithmic factors. If |γ| &gt; C o (m/(log m) 2 ) −1/d , using Corollary 1, the same performance bound holds for f m,n provided m ≫ n 2d . See <ref type="bibr" target="#b12">[13]</ref> for further details. If |γ| &lt; C o (m/(log m) 2 ) −1/d , the decision sets are not discernable using unlabeled data. Since the regression function is piecewise Hölder-α smooth on each empirical decision set, Using Theorem 9 in <ref type="bibr" target="#b11">[12]</ref> and similar analysis, an upper bound of max{n −2α/(2α+d) , n −1/d } follows, which scales as n −1/d when d ≥ 2α/(2α − 1).</p><p>2) Supervised Learning Lower Bound: The formal minimax proof requires construction of a finite subset of distributions in P XY (γ) that are the hardest cases to distinguish based on a finite number of labeled data n, and relies on a Hellinger version of Assouad's Lemma (Theorem 2.10 (iii) in <ref type="bibr" target="#b15">[16]</ref>). Complete details are given in <ref type="bibr" target="#b12">[13]</ref>. Here we present the simple intuition behind the minimax lower bound of n −1/d when γ &lt; c o n −1/d . In this case the decision boundaries can only be localized to an accuracy of n −1/d , the average spacing between labeled data points. Since the boundaries are Lipschitz, the expected volume that is incorrectly assigned to any decision set is &gt; c 1 n −1/d , where c 1 &gt; 0 is a constant. Thus, if the expected excess risk at a point that is incorrectly assigned to a decision set can be greater than a constant c 2 &gt; 0, then the overall expected excess risk is &gt; c 1 c 2 n −1/d . This is the case for both regression and binary classification. If γ &gt; c o n −1/d , the decision sets can be accurately discerned from the labeled data alone. In this case, it follows that the minimax lower bound is equal to the minimax lower bound for Hölder-α smooth regression functions, which is cn −2α/(d+2α) , where c &gt; 0 is a constant <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Margin γ measures the minimum width of a decision set or separation between the support sets of the component marginal mixture densities. The margin is positive if the component support sets are disjoint, and negative otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Consider a uniform square grid over the domain X = [0, 1] d with spacing 2h m , where h m = κ 0 ((log m) 2 /m) 1/d and κ 0 &gt; 0 is a constant. For any point x ∈ X , let [x] denote the closest point on the grid. Let G denote the kernel and H m = h m I, then the estimator of p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is the conditional density on the k-th component and let E k denote expectation with respect to the corresponding conditional distribution. The regression function on each component is f k (x) = E k [Y |X = x] and we assume that for k = 1, . . . , K 1. f k is uniformly bounded, |f k | ≤ M . 2. f k is Hölder-α smooth on C k with Hölder constant K 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>D, for which the following finite sample upper bound holds sup</head><label></label><figDesc></figDesc><table>where R  *  is the infimum risk over all possible learners. 

Corollary 1. Assume that the excess risk E is bounded. Suppose there exists a clairvoyant super-
vised learner 
f D,n , with perfect knowledge of the decision sets </table></figure>

			<note place="foot" n="1"> This form is a slight generalization of the boundary fragment class of sets which is used as a common tool for analysis of learning problems [11]. Boundary fragment sets capture the salient characteristics of more general decision sets since, locally, the boundaries of general sets are like fragments in a certain orientation.</note>

			<note place="foot" n="2"> If the component marginal densities and regression functions have different smoothnesses, say α and β, the same analysis holds except that f (x) is Hölder-min(α, β) smooth on each D ∈ D.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Proof of Corollary 1</head><p>Let Ω 1 denote the event under which Lemma 1 holds. Then P (Ω c 1 ) ≤ 1/m. Let Ω 2 denote the event that the test point X and training data X 1 , . . . , X n ∈ L don't lie in B. Then P (Ω c 2 ) ≤ (n + 1)P (B) ≤ (n + 1)p max vol(B) = O(nh m ). The last step follows from the definition of the set B and since the boundaries of the support sets are Lipschitz, K is finite, and hence vol(B) = O(h m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now observe that</head><p>f D,n essentially uses the clairvoyant knowledge of the decision sets D to discern which labeled points X 1 , . . . , X n are in the same decision set as X. Conditioning on Ω 1 , Ω 2 , Lemma 1 implies that X, X i ∈ D iff X ↔ X i . Thus, we can define a semi-supervised learner f m,n to be the same as f D,n except that instead of using clairvoyant knowledge of whether X, X i ∈ D, f m,n is based on whether X ↔ X i . It follows that</p><p>, and since the excess risk is bounded:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Density adaptive Regression results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Semi-Supervised Learning Upper Bound: The clairvoyant counterpart of</head><p>, and is a standard supervised learner that performs piecewise polynomial fit on each decision set, where the regression function is Hölder-α smooth. Let n D = 1 n n i=1 1 Xi∈D . It follows <ref type="bibr" target="#b11">[12]</ref> that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A PAC-style model for learning from labeled and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference on Learning Theory, COLT</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization error bounds using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kääriäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference on Learning Theory, COLT</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalization error bounds in semi-supervised classification under the cluster assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1369" to="1392" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical analysis of semi-supervised regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Annual Conference on Learning Theory, COLT</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Manifold regularization and semi-supervised learning: Some theoretical analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<idno>TR-2008-01</idno>
		<ptr target="http://people.cs.uchicago.edu/∼niyogi/papersps/ssminimax2.pdf" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Chicago</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning with labeled and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<ptr target="http://www.dai.ed.ac.uk/∼seeger/papers.html" />
		<imprint>
			<date type="published" when="2000" />
			<pubPlace>Edinburgh, UK. URL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for ANC</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the exponential value of labeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="111" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The relative value of labeled and unlabeled samples in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2102" to="2117" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local polynomial regression on unknown manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMS Lecture NotesMonograph Series, Complex Datasets and Inverse Problems: Tomography, Networks and Beyond</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Korostelev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
		<title level="m">Minimax Theory of Image Reconstruction</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Faster rates in regression via active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Finite sample analysis of semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The asymptotic minimax constant for sup-norm loss in nonparametric density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korostelev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nussbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1099" to="1118" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Introduction a l&apos;estimation non-parametrique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal rates of convergence for nonparametric estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
