<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Impact of Network Sharing in Multi-core Architectures *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narayanaswamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
							<email>feng@cs.vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science Virginia Tech</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Impact of Network Sharing in Multi-core Architectures *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>As commodity components continue to dominate the realm of high-end computing, two hardware trends have emerged as major contributors-high-speed networking technologies and multi-core ar-chitectures. Communication middleware such as the Message Passing Interface (MPI) uses the network technology for communicating between processes that reside on different physical nodes, while using shared memory for communicating between processes on different cores within the same node. Thus, two conflicting possibilities arise: (i) with the advent of multi-core architectures, the number of processes that reside on the same physical node and hence share the same physical network can potentially increase significantly, resulting in increased network usage, and (ii) given the increase in intra-node shared-memory communication for processes residing on the same node, the network usage can potentially decrease significantly. In this paper, we address these two conflicting possibilities and study the behavior of network usage in multi-core environments with sample scientific applications. Specifically, we analyze trends that result in increase or decrease of network usage, and we derive insights into application performance based on these. We also study the sharing of different resources in the system in multi-core environments and identify the contribution of the network in this mix. In addition, we study different process allocation strategies and analyze their impact on such network sharing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High-end computing (HEC) systems are increasingly being characterized by nodes built out of commodity components. Two of the significant trends in the HEC domain have been the dramatic improvements in networking technology (using high-speed network accelerators) and in processor technology (with the advent of multi-core architectures). With respect to the networks, several technologies are available in the market, including 10-Gigabit Ethernet <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, Myrinet <ref type="bibr" target="#b9">[10]</ref>, and InfiniBand (IB) <ref type="bibr" target="#b7">[8]</ref>. With respect to multi-core processors, quad-core processors from Intel and AMD are considered commodity today. Processors with higher number of cores (e.g., Intel Xscale) and multithreading within each core (e.g., SUN Niagara) are also becoming available. As these two trends emerge, it is becoming increasingly important to analyze their interaction.</p><p>Scientists typically use standard parallel programming models to develop their applications over HEC systems in a portable manner. The Message Passing Interface (MPI) is the de facto standard in such programming models and is used by a vast majority of scientific applications. With the growing importance of * This work was supported in part by the National Science Foundation Grant #0702182, the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Advanced Scientific Computing Research, Office of Science, U.S. Department of Energy, under Contract and the Department of Computer Science at Virginia Tech. multi-core environments, most implementations of MPI are optimized on such environments by using the network technology for communicating between processes that reside on different physical nodes, while using shared memory for communicating between processes on different cores within the same node. Using shared memory within the node typically reduces the network overhead, resulting in higher performance. Based on such a design for MPI implementations, two conflicting possibilities arise: (i) with the advent of multi-core architectures, the number of processes that reside on the same physical node and hence share the same physical network can potentially increase significantly resulting in increased network usage and (ii) given the increase in intra-node shared-memory communication for processes residing on the same node, network usage can potentially decrease significantly.</p><p>Based on these two conflicting possibilities, it is not clear whether modern multi-core architectures add extra requirements on networks, thus requiring future HEC systems to scale up network capacity further, or whether the increase in intranode shared memory communication compensates for the increase in network sharing, thus not requiring any changes. Thus, depending on the application communication pattern and the layout of processes across nodes, interesting questions about network sharing and scalability need to be studied.</p><p>In this paper, we address these two conflicting possibilities and study the behavior of network usage in multi-core environments with sample scientific applications within the NAS parallel benchmark suite. Specifically, we analyze trends that result in increase or decrease of network usage and derive insights into application performance based on these. We also study the sharing of different resources in the system in multi-core environments and identify the contribution of the network in this mix. Further, we study different process allocation strategies and analyze their impact on such network sharing. Our experimental results demonstrate that for some applications multi-core architectures can significantly hamper performance because of the increased network sharing, while for others the performance can stay constant or even improve because of the better intranode communication.</p><p>The rest of the paper is organized as follows. Section 2 presents some background on multi-core architectures and Myrinet. Section 3 explains some of the networking issues in multi-core architectures that are of interest to us. Our experimental evaluation is presented in section 4. In Section 5 we briefly discuss related work and conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we present an overview of multi-core architectures and the Myri-10G Myrinet network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of Multi-core Architectures</head><p>For many years, hardware manufacturers have been replicating components on processors to create multiple pathways allowing more than one instruction to run concurrently with others. Duplicate arithmetic and floating-point units, coprocessing units, and multiple thread contexts on the same processing die are examples of such replication. Multi-core processors are considered to be the next step in such hardware replication, where two or more (mostly) independent execution units are combined onto the same integrated circuit.</p><p>Multi-core architectures are at a high level similar to multiprocessor architectures. The operating system deals with multiple cores in the same way as multiple processors, by allocating one process to each core at a time. Arbitration of shared resources between the cores happens completely in hardware, with no intervention from the OS. However, multi-core processors also differ significantly from multi-processor systems. For example, in multi-core processors, both computation units are integrated on the same die. Thus, communication between these computation units does not have to go outside the die and hence is independent of the die pin overhead. Further, architectures such as the current Intel multi-cores, as shown in <ref type="figure">Figure 1</ref>, provide a shared cache between the different cores on the same die. This makes communication even simpler by eliminating the need for complicated cache-coherency protocols. Figure 1: Intel dual-core dual-processor system However, multi-core processors also have the disadvantage of more shared resources as compared to multi-processor systems. That is, multi-core processors might require different cores on a processor die to block waiting for local shared resources to get freed when it is being used by a different core. Such contention is even higher when the ratio of the number of cores on the system increases as compared to the other resources (e.g., multi-core systems with multiple thread contexts). Further, for architectures such as AMD NUMA, each processor in a multiprocessor system has access to its own memory, and hence overall memory bandwidth essentially doubles with the number of processors. For multi-core systems however, the overall memory bandwidth does not change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overview of Myrinet Network</head><p>Myri-10G <ref type="bibr" target="#b9">[10]</ref>, the latest generation Myrinet developed by Myricom, is a low-latency wormhole routing based high-speed interconnect and supporting data transfers at the rate of 10 Gbps. The Myrinet network interface card (NIC) has a userprogrammable processor and DMA engines that eases the design and customization of software communication stacks. MX (Myrinet Express) is a high-performance, low-level, messagepassing software interface tailored for Myrinet. The Myri-10G NICs, switches, and associated software support both Ethernet (MXoE) and Myrinet (MXoM) protocols at the link level. The basic MX-10G communication primitives are non-blocking send and receive operations. Our network consists of the Myri-10G NICs connected by a 24-port Myrinet switch. The NICs are connected to the host via a 133 MHz/64 bit PCI-X bus. They have a programmable LANai processor running at 300 MHz with 2 MB on-board SRAM memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Networking Issues in Multi-cores</head><p>In this section, we cover some of the challenges faced in multicore environments with respect to networking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sharing of Network Resources</head><p>One of the important questions when designing high-end systems based on commodity components is network scalability, specifically, whether the network can cope up with the CPU in terms of the network data being sent. An important advantage of multi-core architectures is the ability to multiplex network data streams over a single network hardware medium, which potentially helps in better use of network resources. Also, latency between application processes can decrease as more and more traffic goes over intra-node communication media instead of over the network. This is good for commodity applications but may affect performance of scientific applications because of sharing of network resources. Similarly, sharing of processor resources can be both beneficial and harmful. For example, shared caches in multi-core architectures can reduce latencies between processes to the scale of nanoseconds, but at the same time introduce contention for those resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Process Allocation Schemes</head><p>In a multi-core cluster, the processes can be arranged among the nodes in several ways. Applications typically have fixed communication patterns, and allocation schemes provide us the flexibility of modifying which processes get colocated on the same node. Thus, depending on the allocation scheme, the amount of network sharing might increase or decrease. We look at two common allocation schemes in this paper: cyclic and blocked allocation.</p><p>Cyclic allocation allocates each subsequent process cyclically to the next node in the ring of nodes. For example, with a total of 16 processes and 8 nodes, process ranks 0 and 8 will get assigned to node 0, ranks 1 and 9 to node 1, and so on. This allocation ensures good load balance among all nodes. In blocked allocation, blocks of processes are assigned to each node in turn. For example, with 16 processes, 8 nodes and a block size of 2, process ranks 0 and 1 get assigned to node 0, ranks 2 and 3 to node 1, and so on.</p><p>The process allocation scheme can play an important role in the kind of communication performed by a process. For example, for an application that does mostly neighbor communication in a 1-D chain of processes, blocked allocation will probably turn out to be better. The reason is that the neighbor processes that a process communicates with are more likely to be on the same node. The result can be significant reduction in network communication, thereby potentially improving performance. With more cores on a node, the situation doesn't improve further, however, since there are only a constant number of neighbors.</p><p>In a 2-D grid of N × N processes performing neighbor communication with M cores in a node, again blocked allocation works better than cyclic allocation in localizing more neighbors when N &gt; M . When M and N are equal, the same number of neighbors coexist with both cyclic and blocked allocation. The same holds true for a 3-D grid of processes as well. Thus, for neighbor communication, there are higher chances that more neighbors will co-exist with blocked allocation.</p><p>As another example, for an application which performs tree-like regular long distance communication, a cyclic allocation strategy might be a better choice, as it might localize many of the communicating processes within a node. For applications running on large clusters with hierarchical layers of switches, allocation schemes that localize branches of trees within the lowest hierarchy might be more beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Evaluation</head><p>In this section, we present our performance evaluation results of the NAS Parallel benchmark suite. We follow two different evaluation methodologies. In Section 4.3, we analyze the impact of network and processor sharing in the performance of applications. In Section 4.4, we show results with different process allocation schemes. We show results with class B of the NAS benchmarks, but we note that we got similar results for classes A and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Each node in our 16-node cluster setup is a custom-built, dualprocessor, dual-core AMD Opteron 2.55 GHz system having 4 GB of DDR2 667 MHz SDRAM. The four cores in each system are organized as cores 0 and 1 on processor 0 and cores 2 and 3 on processor 1. Each core has a separate 1 MB L2 cache. All machines run Ubuntu Fiesty with kernel version 2.6.19 and are equipped with Myri-10G network interface cards connected to a Myrinet switch. The MPI library used is MPICH2-MX v1.0.6. All experiments were run at least three times with the processor affinity of each process set to a fixed core to remove the impact of operating system scheduling anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Configurations Used in Experiments</head><p>This section describes the configurations on which we ran our experiments. We use 16 processes for all the NPB benchmarks because this covers the maximum number of benchmarks and configurations for our setup. We note that 16 processes can be run on different configurations on a multi-core architecture with four cores. Picking only those with constant number of processes on a node, we end up with three configurations:</p><p>• 16X1 -16 nodes, one process on one of the four cores • 8X2 -8 nodes, 2 processes, on two of the four cores • 4X4 -4 nodes, 4 processes, one on each core We start by observing that between each of the three configurations there are increased levels of network sharing. With 16X1, there is no network sharing since each node runs only one application process. With 8X2, however, two processes in each node use the same network interface card. Hence there is two times more network sharing than with the 16X1 case. With 4X4, four processes use the same NIC, thus making the network sharing four times greater than with the 16X1 case. In our experiments, we ran the 4X4 configuration with cyclic allocation of processes between nodes.</p><p>To consider the effects of processor sharing, we split the 8X2 into two cases again. Our setup consists of a dual-core dualprocessor system and hence the two processes can be run in two different modes:</p><p>• 8X2 co-processor mode: two processes, each running on a different processor</p><p>• 8X2 virtual processor mode: two processes, both run on the same processor In the virtual processor mode, there is increased sharing of processor resources because both processes are run on the same processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Network Sharing</head><p>We start by evaluating the impact of network sharing by running the various NPB benchmarks over each of the three configurations described above. <ref type="figure" target="#fig_1">Figure 2</ref> shows the impact network resource sharing can have on the performance. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), as we move from 16X1 to 8X2 co-processor mode, the performance of all the benchmarks drops (as much as 27% for IS). The reason is the increased network sharing in the 8X2 configuration, where two processes have to share the same network device. Since only one process has been added to every node, the chances that a process will communicate predominantly with the process colocated in its node are slim.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>(b) on the other hand, the performance drop is seen mainly for CG, FT, and IS, while the other benchmarks perform similarly or show improved performance (in the case of MG) between the two configurations. Here we see mixed benefit in moving to 4X4 because more number of processes are collocated in the same node. Thus, potentially, more shared memory communication can happen reducing the possibility of network sharing.</p><p>To analyze the level of network sharing in the above results, we profile the network communication time in each of these configurations. Since we are using Myrinet's MX protocol, we profile the time spent in the mx isend() and mx test() calls. This time represents the time spent by the network in sending the data out and thus is an indicator of the overhead of network sharing. <ref type="figure" target="#fig_2">Figure 3</ref> shows the normalized total time spent in mx isend() and mx test() calls for the various configurations. As seen in <ref type="figure" target="#fig_2">Figure 3(a)</ref>, there is an increase in the network communication time for all the benchmarks between 16X1 and 8X2 co-processor mode. In other words, moving to the 8X2 co-processor mode results in more time being spent for network communication because the network resources are being shared. Also, the amount of intra-node communication remains comparatively low, so it is difficult to observe any significant benefit from the reduced latency. Of 15 other possible processes with which a process can communicate, only one results in intra-node communication. Thus, there is a 93% chance that a process will communicate over the network with another process. These results mimic the performance results where all benchmarks observe a decrease in performance when moving to 8X2 co-processor mode.</p><p>In <ref type="figure" target="#fig_2">Figure 3(b)</ref>, however, the network communication increases only for the CG, FT, and IS benchmarks, while for all others it drops. This again clearly mimics the performance results as seen in <ref type="figure" target="#fig_1">Figure 2(</ref>  <ref type="bibr" target="#b10">[11]</ref>.</p><p>To make our analysis of network sharing more comprehensive, we also need to analyze the effect of processor sharing. To do this, we compare the performances of 8X2 co-processor and 8X2 virtual processor modes. For the co-processor mode, we run the processes in cores 0 and 2, while for the virtual processor mode we run the processes on cores 2 and 3. <ref type="figure">Figure 4</ref>(a) shows the performance of co-processor and virtual processor modes in the 8X2 configuration for all the benchmarks. We observe a substantial performance difference between the two modes for all the benchmarks (up to 53% as in the case of SP). This shows that sharing of processor resources can be very detrimental for the application.</p><p>We verify our results with processor sharing by using PAPI to count various hardware performance counters. We first measure the number of L2 cache misses. As shown in <ref type="figure">Figure 4</ref>(b), the virtual processor mode sees increased L2 cache misses ranging from 27% more misses in the case of FT up to 48% more in the case of MG.</p><p>We profile the benchmarks for two types of CPU stall cycles as well: those stalling for any resource and those stalling for memory accesses. Here we show results only for the CG and SP benchmarks; the results for the other benchmarks are similar. <ref type="figure">Figure 5</ref> shows the normalized number of CPU stall cycles waiting for resource and memory for CG and SP benchmarks. From the graphs, we can see that the virtual processor mode has more resource stalls than does the co-processor mode. SP observes up to 73% more resource stalls cycles and 66% more memory stalls, whereas in the case of CG, it is 14% and 17%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Allocation Schemes</head><p>In this section, we take a different approach for investigating network sharing impacts, by performing a comparative study of using the cyclic and blocked allocation schemes with the NPB benchmarks. We run the experiments on 64 processes, with four processes on each of the 16 nodes. To further understand the reasons behind the trends observed, we profile the network communication time of the benchmarks similar to the profiling done in section 4.3. <ref type="figure">Figure 6</ref>(b) shows the normalized total communication time for each of the benchmarks for cyclic and blocked cases. From the graph, we observe that CG observes a substantial reduction in communication time when running in blocked allocation mode. For all other benchmarks, there is an increase in network communication time. We note here that MG observes more than a six fold increase in communication time, which explains why the performance of MG drops heavily when using blocked allocation. The data size analysis shows that the amount of data communicated over the network for CG halves when moving from cyclic to blocked allocation, while MG sees a slight increase. This also verifies the performance results that we observe. Refer to <ref type="bibr" target="#b10">[11]</ref> for the data size analysis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Application Processing Pattern Analysis</head><p>The previous sections evaluated application performance from the viewpoint of system and network characteristics. In this section, we tie in the analysis developed in previous sections to the application communication patterns.</p><p>The CG benchmark performs communication within groups of four processes with certain boundary nodes communicating between the groups. As an example, <ref type="figure" target="#fig_4">Figure 7</ref> shows the communication pattern of CG with 16 processes. This pattern clearly shows that any allocation scheme that localizes the groups of four processes within a node will have good performance improvement. For example, if each of the group of four processes are localized within a node, the only network communication is between the boundary nodes. Thus any allocation scheme that optimizes this strategy will get better performance. We see this result with blocked allocation in the 16X4 case, which performs better than the cyclic allocation (see <ref type="figure">Figure 6</ref>).</p><p>The FT benchmark performs an all-to-all exchange within subcommunicators along the row and column in a processor grid. Thus, having more cores in a node allows some processes in either the row or the column subcommunicators to be local to a node. But the communication as part of the other subcommunicator still has to go through the network. Although some amount of network communication is saved, there is still sufficient sharing of network resources. Similarly, choosing an appropriate allocation scheme might help in localizing all the nodes of a sub-communicator, but still there is enough network traffic between the other subcommunicator to nullify this advantage. In our results, we see a similar behavior, where the performance drops for FT when moving from 16X1 to 4X4 because of the increased sharing of the network but remains the same for the cyclic and blocked allocation strategies. The IS benchmark has a similar analysis as FT as it also does predominantly all-to-all exchanges. This analysis for FT and IS ties in well with the network data size analysis results shown in <ref type="bibr" target="#b10">[11]</ref>. Designing efficient network topologies for FT and IS can be a challenging task given the all-to-all pattern.</p><p>MG has an interesting pattern wherein there is some clustered communication in groups of 4, but these clusters themselves are grouped in clusters of 16. Each process communicates with another process which is at increasing distances of increasing powers of two from it. Thus, any process allocation strategy that puts processes at distances of powers of two on the same node will be beneficial for the application. For example, when the number of nodes is a power of two, cyclic allocation will put such processes on the same node. This situation explains why MG performs better with cyclic allocation than blocked allocation with 64 processes and also why the 4X4 cyclic configuration performs better than the 8X2 configuration.</p><p>BT, LU, and SP follow complex communication patterns that make analysis from the processing pattern difficult. Changes in configurations or allocation schemes may not significantly  In summary, we saw in Section 4.3 that network sharing does affect the performance of applications, although the results might pale in comparison with the effects of processor sharing. Nevertheless, network sharing is an important concern that has to be addressed. We also saw that using a different process allocation strategy has the potential to reduce the effects of network sharing. Furthermore, knowledge of the application pattern can give better ideas for designing better run-time configurations for applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>A lot of work has been proposed on optimizing application performance on multi-core architectures. In <ref type="bibr" target="#b2">[3]</ref>, Curtis-Maury et al. look at OpenMP communication on multi-core processors. <ref type="bibr">Chai et al., in [2]</ref>, look at the performance of applications based on the amount of intra-CMP, inter-CMP and inter-node communication performed. We investigate the problem with a different approach by looking at the amount of sharing of network resources. In <ref type="bibr" target="#b0">[1]</ref>, Alam et al. perform extensive characterization of various scientific workloads on the AMD multi-core processor. But their work looks only at a single multi-core node, whereas we look at a cluster of nodes and at the impact of the network as well.</p><p>Similarly, many articles and papers have investigated the communication patterns of various applications and benchmarks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. But none of these papers focus on multi-core architectures in their evaluation, which we address here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>With the advent of multi-core architectures, designers of highend systems are faced with the challenge of ensuring that the interconnection network scales well with more processing cores. We analyze this problem by studying the impact of network sharing on multi-core architectures. Our results indicate that network sharing can have a significant impact on performance, although sharing of processor resources has a much bigger impact. With a good understanding of the application communication pattern, a different process allocation strategy could potentially reduce the effects of network sharing. For future work, the network sharing analysis studied in this paper can be incorporated into MPI process managers (such as mpd, lamboot etc) which can lay out processes more intelligently across nodes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evaluation of network sharing: (a) 16X1 vs 8X2 co-processor and (b) 8X2 virtual processor vs 4X4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Network communication time: (a) 16X1 vs 8X2 co-processor and (b) 8X2 virtual processor vs 4X4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 (Figure 4 :Figure 5 :</head><label>645</label><figDesc>Figure 4: Analysis of processor sharing: (a) performance and (b) L2 cache misses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CG pattern</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>b). In this case, moving from the 8X2 vir- tual processor mode to 4X4 mode results in two processes get- ting added to the same node. Thus there is increased capabil- ity to perform intra-node communication. Compared to a 93% chance of network communication with the 8X2 case, there is only 80% chance with the 4X4 case that a process will com- municate over the network with another process. We analyze our results further by profiling the amount of data sent over the network as compared to intra-node communication for all the benchmarks. The data-size analysis shows a similar trend as the network communication time and corroborates the perfor- mance results we get. For dearth of space, we present those results in</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Characterization of scientific workloads on systems with multi-core processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Kuehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IISWC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding the impact of multi-core architecture in cluster computing: A case study with intel dual-core system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Symposium on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
	<note>Cluster Computing and the Grid</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An evaluation of openmp on current and emerging multithreaded/multicore processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Curtis-Maury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Antonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on OpenMP</title>
		<meeting><address><addrLine>Eugene, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Architectural requirements of parallel scientific applications with explicit communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cypher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Konstantinidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1993-05" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Initial performance evaluation of the neteffect 10 gigabit iwarp adapter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dalessandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wyckoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RAIT &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Performance characterization of a 10-gigabit ethernet toe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE HotI</title>
		<meeting><address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing 10-gigabit ethernet for networks of workstations, clusters and grids: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hurwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Coccetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">InfiniBand Trade Association</title>
		<ptr target="http://www.infinibandta.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterization of communication patterns in message-passing parallel scientific application programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CANPC &apos;98: Proceedings of the Second International Workshop on Network-Based Parallel Computing</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="202" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Myricom</surname></persName>
		</author>
		<ptr target="http://www.myri.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Impact of network sharing in multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-03" />
		</imprint>
		<respStmt>
			<orgName>Computer Science department, Virginia Tech</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Communication patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Riesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Communication Architecture for Clusters (CSC 2006)</title>
		<meeting><address><addrLine>Rhode Island, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Communication characteristics of large-scale scientific applications for contemporary cluster architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="853" to="865" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
