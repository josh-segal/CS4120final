<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Classification using Rectified Nearest Feature Line Segment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><forename type="middle">Qiu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Classification using Rectified Nearest Feature Line Segment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a new classification method termed Rectified Nearest Feature Line Segment (RNFLS). It overcomes the drawbacks of the original Nearest Feature Line (NFL) classifier and possesses a novel property that centralizes the probability density of the initial sample distribution, which significantly enhances the classification ability. Another remarkable merit is that RNFLS is applicable to complex problems such as two-spirals, which the original NFL cannot deal with properly. Experimental comparisons with NFL, NN(Nearest Neighbor), k-NN and NNL (Nearest Neighbor Line) using artificial and real-world datasets demonstrate that RNFLS offers the best performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nearest Feature Line (NFL) <ref type="bibr" target="#b0">[1]</ref>, a newly developed nonparametric pattern classification method, has recently received considerable attention. It attempts to enhance the representational capacity of a sample set of limited size by using the lines passing through each pair of the samples belonging to the same class. Simple yet effective, NFL shows good performance in many applications, including face recognition <ref type="bibr" target="#b0">[1]</ref> [2], audio retrieval <ref type="bibr" target="#b2">[3]</ref>, image classification <ref type="bibr" target="#b3">[4]</ref>, speaker identification <ref type="bibr" target="#b4">[5]</ref> and object recognition <ref type="bibr" target="#b5">[6]</ref>.</p><p>On the other hand, feature lines may produce detrimental effects that lead to increased decision errors. Compared with the well-known Nearest Neighbor (NN) classifier <ref type="bibr" target="#b6">[7]</ref>, NFL has obvious drawbacks under certain situations that limit its further potential. The authors of <ref type="bibr" target="#b7">[8]</ref> pointed out one of the problemsextrapolation inaccuracy, and proposed a solution called Nearest Neighbor Line (NNL). This extrapolation inaccuracy may lead to enormous decision errors in a low dimensional feature space while a simple NN classifier easily reaches a perfect correct classification rate of 100%. Another drawback of NFL is interpolation inaccuracy. Distributions assuming a complex shape (two-spiral problem for example) often fall into this category, where, by the original NFL, the interpolating parts of the feature lines of one class break up the area of another class and severely damage the decision region.</p><p>In this paper, a new nonparametric classification method, Rectified Nearest Feature Line Segment (RNFLS), is proposed that addresses both of the abovementioned drawbacks and significantly improves the performance of NFL. The original NFL can conceptually be viewed as a two-stage algorithm -building representational subspaces for each class and then performing the nearest distance classification. We focus mainly on the first stage. To overcome extrapolation inaccuracy, Nearest Feature Line Segment subspace (NFLS-subspace) is developed. For the interpolation inaccuracy, the "territory" of each sample point and each class is defined, and we obtain Rectified Nearest Feature Line Segment subspace(RNFLS-subspace) from NFLS-subspace by eliminating those feature line segments trespassing the territory of other classes. As a result, RNFLS works well for all shapes of sample distribution, which is a significant improvement.</p><p>Another remarkable advantage of RNFLS is that it centralizes the probability density of the initial sample distribution. We show, in an experiment, that the decision region created by RNFLS gets closer to the one built by using the optimal Bayesian rule, bringing the correct classification rate higher. Comparisons with NN, k-NN, NFL, NNL using artificial and real-world datasets demonstrate that the proposed RNFLS method offers remarkably superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Nearest Feature Line Method</head><p>The Nearest Feature Line (NFL) <ref type="bibr" target="#b0">[1]</ref> method constructs a feature subspace for each class, consisting of straight lines passing through every pair of the samples belonging to that class. The straight line passing through samples x i , x j of the same class, denoted by x i x j , is called a feature line of that class. All the feature lines of class ω constitute an NFL-subspace to represent class ω, denoted by</p><formula xml:id="formula_0">S ω = {x ω i x ω j |x i , x j ∈ ω, x i = x j },</formula><p>which is a subset of the entire feature space. During classification, a query point q is classified to class ω if q assumes the smallest distance to S ω than to any other S ω , (ω = ω ) . The distance from q to S ω is</p><formula xml:id="formula_1">d(q, S ω ) = min xixj ∈Sω d(q, x i x j ).<label>(1)</label></formula><p>= min</p><formula xml:id="formula_2">xixj ∈Sω q − p ij (2)</formula><p>where p ij is the projection point of q onto line x i x j . The projection point can be computed by</p><formula xml:id="formula_3">p ij = (1 − µ)x i + µx j ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">µ = (q − x i ).(x j − x i ) (x j − x i ).(x j − x i ) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shortcomings</head><p>NFL extends the samples of one class by adding the straight lines linking each pair. A good argument for doing this is that it adds extra information to the sample set. The extra information, however, is a double-edged sword. When a straight line of one class trespasses into the territory of another class, it will lead to increased error probability. There are two types of trespassing, causing two types of inaccuracies: extrapolation inaccuracy and interpolation inaccuracy. <ref type="figure" target="#fig_0">Fig.1(a)</ref> shows a classification problem in which the extrapolation inaccuracy occurs. The query q, surrounded by four "cross" sample points, is in the territory of the "cross" class, leading to the expectation that q should be classified to the "cross" class. But the extrapolating part of feature line x 1 x 2 makes the distance from q to x 1 x 2 smaller. Thus, d(q, S circle ) &lt; d(q, S cross ), and NFL will assign q the label "circle", not "cross". This is very likely to be a decision error. Similarly, the interpolation inaccuracy caused by the interpolating part of a feature line is illustrated in <ref type="figure" target="#fig_0">Fig.1</ref>(b). The above inaccuracies are drawbacks that limit the applicability of NFL. In the following section we pursue a more systematic approach in which a new feature subspace for each class is constructed to avoid both drawbacks. The original advantage of NFL that linearly extending the representational capacity of the original samples is retained in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rectified Nearest Feature Line Segment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using Feature Line Segments</head><p>To avoid extrapolation inaccuracy, we propose to use line segments between pairs of the sample points to construct a Nearest Feature Line Segment subspace (NFLS-subspace) instead of the original NFL-subspace to represent each class.</p><formula xml:id="formula_5">Let X ω = {x ω i |1 ≤ i ≤ N ω } be the set of N ω samples belonging to class ω. The NFLS-subspace ( S ω ) representing class ω is S ω = { x ω i x ω j |1 ≤ i, j ≤ N ω },<label>(5)</label></formula><p>where x ω i x ω j denotes the line segment connecting point x ω i and x ω j . Note that a degenerative line segment</p><formula xml:id="formula_6">x ω i x ω i (1 ≤ i ≤ N ω )</formula><p>, which is a point in the feature space, is also a member of S ω . The distance from a query point q to an NFLS-subspace S ω is defined as</p><formula xml:id="formula_7">d(q, S ω ) = min xixj ∈ Sω k d(q, x i x j )<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">d(q, x i x j ) = min y∈ xixj q − y.<label>(7)</label></formula><p>And to calculate d(q, x i x j ), there are two cases. If x i = x j , the answer is simply the point to point distance,</p><formula xml:id="formula_9">d(q, x i x i ) = q − x i .<label>(8)</label></formula><p>Otherwise, the projection point p of q onto x i x j is to be located first by using Equ. <ref type="formula" target="#formula_3">(3)</ref> and Equ.(4). Then, different reference points are chosen to calculate <ref type="figure" target="#fig_1">Fig.2</ref> shows an example.</p><formula xml:id="formula_10">d(q, x i x j ) according to the position parameter µ. When 0 &lt; µ &lt; 1, p is an interpolation point between x i and x j , so d(q, x i x j ) = q − p. When µ &lt; 0, p is a "backward" extrapolation point on the x i side, so d(q, x i x j ) = q − x i . When µ &gt; 1, p is a "forward" extrapolation point on the x j side, so d(q, x i x j ) = q − x j .</formula><p>In the classification stage, a query q is classified to class ω k when d(q, S ω k ) is smaller than the distance from q to any other S ωi (ω i = ω k ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rectifying the Feature Line Segment Subspace</head><p>The next step is to rectify the NFLS-subspace to eliminate interpolation inaccuracy. Our motivation is to have the inappropriate line segments removed from the NFLS-subspace S ω k for each class ω k ,. The resulting subspace denoted by S * ω k is a subset of S ω k termed Rectified Nearest Feature Line Segment subspace (RNFLS-subspace).</p><p>Territory We begin with the definitions of two types of territories. One is sample-territory, T x ∈ n , that is the territory of a sample point x; the other is class-territory, T ω ∈ n , that is the territory of class ω. Suppose the sample set X is {(x 1 , θ 1 ), (x 2 , θ 2 ), ..., (x m , θ m )}, which means x i belongs to class θ i . The radius r x k of the sample-territory T x k is,</p><formula xml:id="formula_11">r x k = min ∀xi,θi =θ k x i − x k .<label>(9)</label></formula><p>Thus,</p><formula xml:id="formula_12">T x k = {y ∈ n ||y − x k &lt; r x k }.<label>(10)</label></formula><p>The class-territory T ω k is defined to be</p><formula xml:id="formula_13">T ω k = θi=ω k T xi , (x i , θ i ) ∈ X.<label>(11)</label></formula><p>In <ref type="figure" target="#fig_2">Fig.3</ref>, the points denoted by "circle" and "cross" represent the samples from two classes. Each of the "cross"-points (y 1 , y 2 , y 3 ) has its own sample-territory as shown by the dashed circle. The union of these sample-territories is T cross .</p><p>T circle is obtained in a similar way.</p><p>Building RNFLS-subspace For class ω k , its RNFLS-subspace S * ω k is built from the NFLS-subspace S ω k by having those line segments trespassing the classterritories of other classes removed. That is</p><formula xml:id="formula_14">S * ω k = S ω k − U ω k ,<label>(12)</label></formula><p>where '−' is the set difference operator, and</p><formula xml:id="formula_15">U ω k = { x i x j |∃ω y , ω k = ω y ∧ x i x j ∈ S * ω k ∧ x i x j ∩ T ωy = φ} = { x i x j |∃(x y , θ y ) ∈ X, x i x j ∈ S * ω k ∧ ω k = θ y ∧ d(x y , x i x j ) &lt; r xy }.<label>(13)</label></formula><p>Classifying using RNFLS-subspaces To perform classification using RNFLSsubspaces is similar to using NFLS-subspaces, since the only difference between an RNFLS-subspace and an NFLS-subspace is is still a set consisting of line segments. The distance measure from a query point to the RNFLS-subspace remains the same.</p><formula xml:id="formula_16">S * ω k = S ω k − U ω k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analyzing the Centralization Property</head><p>In many real-world pattern recognition problems, samples from one class tend to scatter around a certain center point because of systematic error and random noise. Gaussian distribution is an example. Two scattered classes may overlap each other, causing decision errors. Compared with the original ideal sample distribution without noise, the NFLS-subspace of each class has an impressive property -distribution centralization, which can be viewed as the converse of scattering. With the help of NFLS-subspace, the distribution overlapping is reduced, and the probability distribution grows closer to the original. And so, we get a higher correct classification rate.</p><p>The simplest case to show the centralization property is when the distribution is uniform in a two-dimensional feature space. Suppose that the sample points of class ω are uniformly distributed in a disk D whose radius is R and the center is at O, as shown in <ref type="figure" target="#fig_3">Fig.4</ref>. For the NFLS-subspace of the class, consider a small region M (a, r) (a ≤ R), that is a round area with an arbitrarily small radius r and distance a from O. Let N ω a be the probability of a randomly selected feature line segment of class ω passing through M (a, r). Proposition 1 Given an arbitrarily small r, N ω a is decreasing on a.</p><p>Proof We calculate N ω a in a polar coordinate system by choosing the center of M (a, r) as pole and − − → OM as polar axis. For a line segment XY passing through M (a, r), given one endpoint X(ρ, θ) in D, the other endpoint Y has to appear in the corresponding 2 M1M2HG , as shown in <ref type="figure" target="#fig_3">Fig.4</ref>. Thus we obtain</p><formula xml:id="formula_17">N ω a = D 1 πR 2 A(ρ, θ)ρdρdθ = 2π 0 |M C| 0 1 πR 2 A(ρ, θ)ρdρdθ<label>(14)</label></formula><p>where A(ρ, θ) is the probability that the randomly generated endpoint Y appears in 2 M1M2HG ,</p><formula xml:id="formula_18">A(ρ, θ) = 1 πR 2 1 2 (2r + |GH|) · |M G| + o(r)<label>(15)</label></formula><p>According to Equ. <ref type="formula" target="#formula_1">(14)</ref> and <ref type="formula" target="#formula_1">(15)</ref> </p><formula xml:id="formula_19">N ω a = 2r(R 2 − a 2 ) (πR 2 ) 2 2π 0 R 2 − a 2 sin 2 θ · dθ + o(r).<label>(16)</label></formula><p>Thus, for a fixed r, N ω a gets smaller when a gets larger. Proposition 1 indicates that the distribution of line segments in the NFLSsubspace is denser at the center than at the boundary if the original sample points distribution is under a uniform density. A Gaussian distribution can be viewed as a pile-up of several uniform distribution disks with the same center but different radius. It is conjectured that this centralization property also applies to the Gaussian case, and can be extended to classification problems in which the overlapping is caused by noise scattering of two or more classes under similar distribution but different centers. It reverses the scattering and achieves a substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results and Discussions</head><p>The performance of the RNFLS method is compared with four classifiers -NN, k-NN, NFL and NNL -using two artificial datasets as well as a group of real-world benchmarks widely used to evaluate classifiers. The results on these datasets, representing various distributions and different dimensions, demonstrate that RNFLS possesses remarkably stronger classification ability than the other four methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Two-Spiral Problem</head><p>The two-spiral problem is now included by many authors as one of the benchmarks for evaluation of new classification algorithms. The two-spiral curves in a two-dimensional feature space is described as follows</p><formula xml:id="formula_20">spiral1 : x = kθ cos(θ) y = kθ sin(θ) spiral2 : x = kθ cos(θ + π) y = kθ sin(θ + π)<label>(17)</label></formula><p>where θ ≥ π/2 is the parameter. If the probability density of each class is uniform along the corresponding curve, an instance of such distribution is shown in <ref type="figure" target="#fig_4">Fig.5(a)</ref>. In our experiment, Gaussian noise is added to the samples so that the distribution regions of the two classes may overlap each other, as shown in <ref type="figure" target="#fig_4">Fig.5(b)</ref>. If the prior distribution density were known, according to the optimal Bayesian rule, <ref type="figure" target="#fig_4">Fig.5(d)</ref> should be the optimal decision region. This, however, can hardly be achieved because the only information we have is from a finite number of sample points.</p><formula xml:id="formula_21">(a) (b) (c) (d) (e) (f)</formula><p>The original NFL is not a good choice for this classification problem. We may imagine how fragmented the decision region is carved up because of its interpolation and extrapolation inaccuracy. The decision region created by NN rule is shown in <ref type="figure" target="#fig_4">Fig.5(e)</ref>. When it comes to RNFLS, <ref type="figure" target="#fig_4">Fig.5(c)</ref> is the RNFLSsubspaces and <ref type="figure" target="#fig_4">Fig.5(f)</ref> is the corresponding decision region. Compared with the decision region created by NN, RNFLS produces a much better one in which the boundary is smoother and some incorrect regions caused by isolated noise points is smaller. This significant enhancement can be attributed to the centralization property.</p><p>As a concrete test, let θ ∈ [π/2, 3π] and the Gaussian noise is of a variance σ = 1.7 and an expectation µ = 0. We produce 500 points according to the well-defined distribution, where 250 belong to class ω 1 and the other 250 belong to class ω 2 . Then, half of them are randomly chosen to form the sample set and the remaining half constitute the test set. The classifiers, NN, k-NN(k=3), NFL, NNL and RNFLS, are applied to this task for 10 times, and <ref type="table" target="#tab_1">Table 1</ref> shows the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-World Classification Problems</head><p>We test the RNFLS classifier on a group of real-world datasets as listed in <ref type="table" target="#tab_2">Table 2</ref>. All of the datasets are obtained from the U.C. Irvine repository <ref type="bibr" target="#b8">[9]</ref>.</p><p>Since we do not deal with the issue of missing data, instances with missing values are removed. For the fairness of the procedure, attributes of the instances are standardized (normalized) by their means and standard deviations before submitted to the classifiers. The performance in CCR is obtained using the leave-one-out procedure. It can be seen that RNFLS performs well on both two-category and multicategory classification problems in both low and high dimensional feature spaces. This is encouraging since these datasets represent real-world problems and none of them is specially designed to suit a specific classifier. Since one common characteristic of real-world problems is distribution dispersing caused by noise, the centralization property of RNFLS helps improving the correct classification rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>A new classification method RNFLS is developed. It enhances the representational capacity of the original sample points and constitutes a substantial improvement to NFL. It works well independent of the distribution shape and the feature-space dimension. In particular, viewed as the converse of sample scattering, RNFLS is able to centralize the initial distribution of the sample points and offers a higher correct classification rates for common classification problems.</p><p>Further investigation into RNFLS seems warranted. In the rectification process it would be helpful to reduce the runtime-complexity, perhaps using some kind of probability algorithms. It may also be helpful to treat the trespassing feature line segments more specifically, for example, finding a way to cut off a part of a trespasser instead of eliminating the whole feature line segments. Also worth more investigation is the centralization property, which might be of great potential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a)Extrapolation inaccuracy. (b)Interpolation inaccuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distance (solid lines) from feature points to feature line segment xixj.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The territory of "cross"-samples shown in dashed circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Calculating N ω a for a uniform sample point density on a disk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a)Two-spiral problem. (b)Two-spiral problem with Gaussian noise. (c)RNFLS subspaces. (d)Bayesian decision region. (e)NN classification result. (f)RNFLS classification result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 . Performance evaluation on the two-spiral problem using NN, 3-NN, NFL[1], NNL[8] and RNFLS. (CCR: correct classification rate, percentage)</head><label>1</label><figDesc></figDesc><table>Classifier CCR (average) CCR (min) CCR(max) 
NN 
83.2 
80.4 
85.3 
k-NN(k=3) 
85.3 
83.2 
87.3 
NFL 
53.2 
49.8 
56.7 
NNL 
72.4 
69.0 
78.0 
RNFLS 
86.1 
84.0 
88.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>CCR(%) for NN, 3-NN, NFL, NNL and RNFLS on the real-world datasets 

Dataset 
#Classes #Instances #Attributes NN 3NN NFL NNL RNFLS 
1 iris 
3 
150 
4 
94.7 94.7 88.7 94.7 95.3 
2 housing 
6 
506 
13 
70.8 73.0 71.1 67.6 73.5 
3 pima 
2 
768 
8 
70.6 73.6 67.1 62.8 73.0 
4 wine 
3 
178 
13 
95.5 95.5 92.7 78.7 97.2 
5 bupa 
2 
345 
6 
63.2 65.2 63.5 57.4 66.4 
6 ionosphere 
2 
351 
34 
86.3 84.6 85.2 87.2 94.3 
7 wpbc 
2 
194 
32 
72.7 68.6 72.7 54.1 75.8 
8 wdbc 
2 
569 
30 
95.1 96.5 95.3 64.0 97.2 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work presented in this paper is supported by National Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using the nearest feature line method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminant waveletfaces and nearest feature classifiers for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1644" to="1649" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Content-based audio classification and retrieval using the nearest feature line method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="619" to="625" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance evaluation of the nearest feature line method in image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1335" to="1339" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the use of nearest feature line for speaker identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1735" to="1746" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object recognition based on image sequences by using inter-feature-line consistencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1913" to="1923" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Locally nearest neighbor classifiers for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1307" to="1309" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/∼mlearn/MLRepository.html" />
		<title level="m">UCI repository of machine learning databases</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
