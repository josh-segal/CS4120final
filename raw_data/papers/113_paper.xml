<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-based value iteration: An anytime algorithm for POMDPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
							<email>jpineau@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Robotics Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
							<email>ggordon@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Robotics Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
							<email>thrun¡@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Robotics Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Point-based value iteration: An anytime algorithm for POMDPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces the Point-Based Value Iteration (PBVI) algorithm for POMDP planning. PBVI approximates an exact value iteration solution by selecting a small set of representative belief points and then tracking the value and its derivative for those points only. By using stochastic trajectories to choose belief points, and by maintaining only one value hyper-plane per point, PBVI successfully solves large problems: we present results on a robotic laser tag problem as well as three test domains from the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The value iteration algorithm for planning in partially observable Markov decision processes (POMDPs) was introduced in the 1970s <ref type="bibr">[Sondik, 1971]</ref>. Since its introduction numerous authors have refined it <ref type="bibr" target="#b0">[Cassandra et al., 1997;</ref><ref type="bibr" target="#b0">Kaelbling et al., 1998;</ref><ref type="bibr">Zhang and Zhang, 2001]</ref> so that it can solve harder problems. But, as the situation currently stands, POMDP value iteration algorithms are widely believed not to be able to scale to real-world-sized problems.</p><p>There are two distinct but interdependent reasons for the limited scalability of POMDP value iteration algorithms. The more widely-known reason is the so-called curse of dimensionality <ref type="bibr" target="#b0">[Kaelbling et al., 1998]</ref> -dimensional continuous space. So, naive approaches like discretizing the belief space scale exponentially with the number of states.</p><p>The less-well-known reason for poor scaling behavior is what we will call the curse of history: POMDP value iteration is in many ways like breadth-first search in the space of belief states. Starting from the empty history, it grows a set of histories (each corresponding to a reachable belief) by simulating the POMDP. So, the number of distinct actionobservation histories considered grows exponentially with the planning horizon. Various clever pruning strategies <ref type="bibr" target="#b0">[Littman et al., 1995;</ref><ref type="bibr" target="#b0">Cassandra et al., 1997]</ref> have been proposed to whittle down the set of histories considered, but the pruning steps are usually expensive and seem to make a difference only in the constant factors rather than the order of growth.</p><p>The two curses, history and dimensionality, are related: the higher the dimension of a belief space, the more room it has for distinct histories. But, they can act independently: planning complexity can grow exponentially with horizon even in problems with only a few states, and problems with a large number of physical states may still only have a small number of relevant histories. In most domains, the curse of history affects POMDP value iteration far more strongly than the curse of dimensionality <ref type="bibr" target="#b0">[Kaelbling et al., 1998;</ref><ref type="bibr" target="#b2">Zhou and Hansen, 2001]</ref>. That is, the number of distinct histories which the algorithm maintains is a far better predictor of running time than is the number of states. The main claim of this paper is that, if we can avoid the curse of history, there are many real-world POMDPs where the curse of dimensionality is not a problem.</p><p>Building on this insight, we present Point-Based Value Iteration (PBVI), a new approximate POMDP planning algorithm. PBVI selects a small set of representative belief points and iteratively applies value updates to those points. The point-based update is significantly more efficient than an exact update (quadratic vs. exponential), and because it updates both value and value gradient, it generalizes better to unexplored beliefs than interpolation-type grid-based approximations which only update the value <ref type="bibr" target="#b1">[Lovejoy, 1991;</ref><ref type="bibr" target="#b0">Brafman, 1997;</ref><ref type="bibr" target="#b0">Hauskrecht, 2000;</ref><ref type="bibr" target="#b2">Zhou and Hansen, 2001;</ref><ref type="bibr" target="#b0">Bonet, 2002]</ref>). In addition, exploiting an insight from policy search methods and MDP exploration <ref type="bibr" target="#b1">[Ng and Jordan, 2000;</ref><ref type="bibr" target="#b1">Thrun, 1992]</ref>, PBVI uses explorative stochastic trajectories to select belief points, thus reducing the number of belief points necessary to find a good solution compared to earlier approaches. Finally, the theoretical analysis of PBVI included in this paper shows that it is guaranteed to have bounded error. This paper presents empirical results demonstrating the successful performance of the algorithm on a large (870 states) robot domain called Tag, inspired by the game of lasertag. This is an order of magnitude larger than other problems commonly used to test scalable POMDP algorithms. In addition, we include results for three well-known POMDPs, where PBVI is able to match (in control quality, but with fewer belief points) the performance of earlier algorithms</p><formula xml:id="formula_0">. l £  h i ' j l k h i  j k m ¤  c     e      B   f y  g    3    ¤    %  8 ¥ ¦  q  c      8 r  c    c </formula><p>A number of algorithms have been proposed to implement this backup by directly manipulating  -vectors, using a combination of set projection and pruning operations <ref type="bibr">[Sondik, 1971;</ref><ref type="bibr" target="#b0">Cassandra et al., 1997;</ref><ref type="bibr">Zhang and Zhang, 2001]</ref>. We now describe the most straight-forward version of exact POMDP value iteration.</p><p>To implement the exact update</p><formula xml:id="formula_1">  E   f  A , we first generate intermediate sets § 5 ¨ © ª and § V ¨ © «  ¬ ¦ 9 s ­ ®   ¬ b ­ ¯ (Step 1): ° | S ± ² ´ ³ ¥ | S ± ²  c  i  x u   c     %  (5) ° | ± ³ ¥ | S ± µ  c    ¶ u `  h i  j l k m ¤  c   8  ·  3       f y  g    '   c  3  %  8 ¥ ¦  µ  c   B   q ¸ e ¥ ¦  µ º ¹ w  Next we create § ¨ ( ¬ ¦ 9 » ­ ¼</formula><p>), the cross-sum over observations, which includes one</p><formula xml:id="formula_2"> ¨ © « from each § ¨ © « (Step 2): ° | u ° | ± ² x ½ ° | ± # ¾ ¶ ½ ° | ± ' ¿ 5 ½   <label>(6)</label></formula><p>Finally we take the union of § V ¨ sets (Step 3):</p><formula xml:id="formula_3">w u À | j } ° |<label>(7)</label></formula><p>In practice, many of the vectors in the final set  may be completely dominated by another vector (</p><formula xml:id="formula_4">V Á ¦ Â " ( Ã  x Ä y Â " l  ¬ º "</formula><p>), or by a combination of other vectors. Those vectors can be pruned away without affecting the solution. Finding dominated vectors can be expensive (checking whether a single vector is dominated requires solving a linear program), but is usually worthwhile to avoid an explosion of the solution size.</p><p>To better understand the complexity of the exact update, let </p><formula xml:id="formula_5">Y Y S Y   A  Y É AEÇ Ê AE (time Y Y Ë % Y Y S Y   A 8 Y  AEÇ È AE</formula><p>). Given that this exponential growth occurs for every iteration, the importance of pruning away unnecessary vectors is clear. It also highlights the impetus for approximate solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Point-based value iteration</head><p>It is a well understood fact that most POMDP problems, even given arbitrary action and observation sequences of infinite length, are unlikely to reach most of the points in the belief simplex. Thus it seems unnecessary to plan equally for all beliefs, as exact algorithms do, and preferable to concentrate planning on most probable beliefs.</p><p>The point-based value iteration (PBVI) algorithm solves a POMDP for a finite set of belief points</p><formula xml:id="formula_6">Ì Í E " $ % # " U S p  p  p  # "  Î l 4</formula><p>. It initializes a separate  -vector for each selected point, and repeatedly updates (via value backups) the value of that  -vector. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, by maintaining a full  -vector for each belief point, PBVI preserves the piece-wise linearity and convexity of the value function, and defines a value function over the entire belief simplex. This is in contrast to grid-based approaches <ref type="bibr" target="#b1">[Lovejoy, 1991;</ref><ref type="bibr" target="#b0">Brafman, 1997;</ref><ref type="bibr" target="#b0">Hauskrecht, 2000;</ref><ref type="bibr" target="#b2">Zhou and Hansen, 2001;</ref><ref type="bibr" target="#b0">Bonet, 2002]</ref>, which update only the value at each belief grid point. The complete PBVI algorithm is designed as an anytime algorithm, interleaving steps of value iteration and steps of belief set expansion. It starts with an initial set of belief points for which it applies a first series of backup operations. It then grows the set of belief points, and finds a new solution for the expanded set. By interleaving value backup iterations with expansions of the belief set, PBVI offers a range of solutions, gradually Ï trading off computation time and solution quality. We now describe how we can efficiently perform point-based value backups and how we select belief points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Point-based value backup</head><p>To plan for a finite set of belief points Ì , we modify the backup operator (Eqn 4) such that only one  -vector per belief point is maintained. For a point-based update</p><formula xml:id="formula_7">  E Ñ Ð  f  A</formula><p>, we start by creating projections (exactly as in Eqn 5)</p><formula xml:id="formula_8">¬ ¦ 9 » ­ 3 ¬ b ­ W (Step 1): ° | S ± ² ³ ¥ | S ± ²  c  i  x u   c     %  (8) ° | ± ³ ¥ | S ± µ  c    ¶ u `  h i  j l k m ¤  c   8  ·  3       f y  g    '   c  3  %  8 ¥ ¦  µ  c   B   q ¸ e ¥ ¦  µ º ¹ w </formula><p>Next, the cross-sum operation (Eqn 6) is much simplified by the fact that we are now operating over a finite set of points.</p><p>We construct</p><formula xml:id="formula_9">¬ º " n ­ Ì  ¬ º 9 D ­ W (Step 2): ° |  u ° | ± ²  h j ¡ y z i Ò 3 Ó S y z i { ¢ j Ô Õ Ö ×  g ¥ D Ø  r   (9)</formula><p>Finally, we find the best action for each belief point (Step 3):</p><formula xml:id="formula_10">w ³ z  Ò 3 Ó y z  { Ô Õ Ù ±Ú  | j }  ° |  Ø r   Û ¸ e r ¹ D Ü<label>(10)</label></formula><p>When performing point-based updates, the backup creates</p><formula xml:id="formula_11">Y Y i Y Y i Y  Å A 8 Y</formula><p>projections as in exact VI. However the final solution  is limited to containing only</p><formula xml:id="formula_12">Y Ì Ý Y components (in time Y Y i Y Y S Y   A 8 Y S Y Y S Y Ì ® Y</formula><p>) . Thus a full point-based value update takes only polynomial time, and even more crucial, the size of the solution set  remains constant. As a result, the pruning of  vectors (and solving of linear programs), so crucial in exact POMDP algorithms, is now unnecessary. The only pruning step is to refrain from adding to  any vector already included, which arises when two nearby belief points support the same vector (e.g. "</p><p>U " Ë in <ref type="figure" target="#fig_0">Fig. 1</ref>). In problems with a finite horizon Þ , we run Þ value backups before expanding the set of belief points. In infinite-horizon problems, we select the horizon so that</p><formula xml:id="formula_13">£ 1  ¨  ß ¥ 1  Á  · © 2 x à á Ã â .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Belief point set expansion</head><p>As explained above, PBVI focuses its planning on relevant beliefs. More specifically, our error bound below suggests that PBVI performs best when its belief set is uniformly dense in the set of reachable beliefs. So, we initialize the set Ì to contain the initial belief "  $ and expand Ì by greedily choosing new reachable beliefs that improve the worst-case density as rapidly as possible.</p><p>For a given " n ­ Ì , PBVI stochastically simulates a singlestep forward trajectory using each action to produce new beliefs " which is farthest away from any point already in Ì . <ref type="bibr">2</ref> PBVI tries to generate one new belief from each previous belief; so, Ì at most doubles in size on each expansion. <ref type="bibr">3</ref> Since expansion phases are interleaved with value iteration, PBVI offers an anytime solution. </p><formula xml:id="formula_14">¨  ã # " ¨ ¾ i p  p  p </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convergence and error bounds</head><formula xml:id="formula_15">ë   è ae  ¥ ì  ª  ë i í ï î  ë   ª  ¥ h  ª ë  í .</formula><p>The first term is bounded by our theorem below; the second is bounded by</p><formula xml:id="formula_16">2  ë   ª $ ¥ ð  ª ë</formula><p>.) The remainder of this section states and proves our error bound.</p><p>Define the density â ae of a set of belief points Ì to be the maximum distance from any legal belief to Ì . More precisely,</p><formula xml:id="formula_17">â ae E   á    e ñ    ò s  á ó  ô @ ñ '  ae ë " ¥ " A  ë i U</formula><p>. Then, we can prove: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 The error introduced by PBVI's pruning step is at most</head><formula xml:id="formula_18">o  õ i ö ¤ ÷ S ø i ù Ê E a ú  û º ü  ý q þ  û º ü  ÿ ¡ £ ¢ £ ¤ U  ¦</formula><formula xml:id="formula_19">¨ §  f Â " . So, d © ¥  Ø  r  ! ¥ D Ø r  u ¥  Ø  r  ¥ D Ø r   w  g ¥  Ø r ¥  Ø r  add zero ¥  Ø  r  ¥ D Ø r   p ¥ D Ø # r ¥  Ø # r ¥ opt. at r u  g ¥  ¥ º  º Ø  g r  r   collect terms " ¥  ¥ # " $ % " ' r  r "  Hölder " ¥  ¥ # " $ ' &amp; def'n of &amp; ( ü  ý q þ  ( ü  ÿ    0 ) &amp; see text</formula><p>The last inequality holds because each  -vector represents the reward achievable starting from some state and following some sequence of actions and observations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 For any belief set</head><formula xml:id="formula_20">£ 1  ¨  ß ¥ 1  Á   © â ae £ 3 § ¥ 2 © Ë 2</formula><p>The actual choice of norm doesn't appear to matter in practice; some of our experiments below used Euclidean distance (instead of 1  ) and the results appear identical. <ref type="bibr">3</ref> We experimented with other strategies such as adding a fixed number of new beliefs, but since value iteration is much more expensive than belief computation the above algorithm worked best. If desired, we can impose a maximum size on Ü based on time constraints or performance requirements. <ref type="bibr">4</ref> If </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>The domain of Tag is based on the popular game of lasertag. The goal is to search for and tag a moving opponent <ref type="bibr">[Rosen- crantz et al., 2003</ref>]. <ref type="figure" target="#fig_1">Figure 2a</ref> shows the live robot as it moves in to capture an opponent. In our POMDP formulation, the opponent moves stochastically according to a fixed policy. The spatial configuration of the domain used for planning is illustrated in <ref type="figure" target="#fig_1">Figure 2b</ref>. This domain is an order of magnitude larger (870 states) than most other POMDP problems considered thus far in the literature <ref type="bibr" target="#b0">[Cassandra, 1999]</ref>, and is proposed as a new challenge for fast, scalable, POMDP algorithms. A single iteration of optimal value iteration on a problem of this size could produce over § The state space is described by the cross-product of two features, Robot otherwise. Throughout the game, the Robot's position is fully observable, and the effect of a Move action has the predictable deterministic effect, e.g.:</p><formula xml:id="formula_21">v y x     i r #  U Y x u w   c  (    i r #  U Y ¶ u w    S  W ` !  i x a Y c b e  ¶ u e d</formula><p>The position of the opponent is completely unobservable unless both agents are in the same cell. At each step, the opponent (with omniscient knowledge) moves away from the robot with  <ref type="figure" target="#fig_2">Figure 3</ref> shows the performance of PBVI on the Tag domain. Results are averaged over 10 runs of the algorithm, times 100 different (randomly chosen) start positions for each run. It shows the gradual improvement in performance as samples are added (each shown data point represents a new expansion of the belief set with value backups). In addition to PBVI, we also apply the QMDP approximation as a baseline comparison <ref type="bibr" target="#b0">[Littman et al., 1995]</ref>. The QMDP approximation is calculated by solving a POMDP as though it were fully observable:</p><p>. This approximation is quick to compute, and is remarkably effective in some domains. In the Tag domain, however, it lacks the representational power to compute a good policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Additional experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison on well-known problems</head><p>To further analyze the performance of PBVI, we applied it to three well-known problems from the POMDP literature. We selected Maze33, Hallway and Hallway2 because they are commonly used to test scalable POMDP algorithms <ref type="bibr" target="#b0">[Littman et al., 1995;</ref><ref type="bibr" target="#b0">Brafman, 1997;</ref><ref type="bibr" target="#b1">Poon, 2001]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> presents results for each domain. Replicating earlier experiments, results for Maze33 are averaged over 151 runs (reset after goal, terminate after 500 steps); results for Hallway and Hallway2 are averaged over 251 runs (terminate at goal, max 251 steps). In all cases, PBVI is able to find a good policy. <ref type="table" target="#tab_9">Table 1</ref> compares PBVI's performance with previously published results, comparing goal completion rates, sum of rewards, policy computation time, and number of required belief points. In all domains, PBVI achieves competitive performance, but with fewer samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Validation of the belief set expansion</head><p>To further investigate the validity of our approach for generating new belief states (Section 3.2), we compared our approach with three other techniques which might appear promising. In all cases, we assume that the initial belief " $ (given as part of the model) is the sole point in the initial set, and consider four expansion methods:</p><p>1. Random (RA) 2. Stochastic Simulation with Random Action (SSRA) 3. Stochastic Simulation with Greedy Action (SSGA) 4. Stochastic Simulation with Explorative Action (SSEA)</p><p>The RA method consists of sampling a belief point from a uniform distribution over the entire belief simplex. SSEA is the standard PBVI expansion heuristic (Section 3.2). SSRA similarly uses single-step forward simulation, but rather than try all actions, it randomly selects one and automatically accepts the posterior belief unless it was already in Ì . Finally, SSGA uses the most recent value function solution to pick the greedy action at the given belief " , and performs a single-step simulation to get a new belief " A  é Ì . We revisited the Hallway, Hallway2, and Tag problems from sections 4 and 5.1 to compare the performance of these  four heuristics. For each problem we apply PBVI using each of the belief-point selection heuristics, and include the QMDP approximation as a baseline comparison. <ref type="figure" target="#fig_3">Figure 4</ref> shows the computation time versus the reward performance for each domain.</p><p>The key result from <ref type="figure" target="#fig_3">Figure 4</ref> is the rightmost panel, which shows performance on the largest, most complicated domain. In this domain our SSEA rule clearly performs best. In smaller domains (left two panels) the choice of heuristic matters less: all heuristics except random exploration (RA) perform equivalently well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Significant work has been done in recent years to improve the tractability of POMDP solutions. A number of increasingly efficient exact value iteration algorithms have been proposed <ref type="bibr" target="#b0">[Cassandra et al., 1997;</ref><ref type="bibr" target="#b0">Kaelbling et al., 1998;</ref><ref type="bibr">Zhang and Zhang, 2001]</ref>. They are successful in finding optimal solutions, however are generally limited to very small problems (a dozen states) since they plan optimally for all beliefs. PBVI avoids the exponential growth in plan size by restricting value updates to a finite set of (reachable) beliefs.</p><p>There are several approximate value iteration algorithms which are related to PBVI. For example, there are many gridbased methods which iteratively update the values of discrete belief points. These methods differ in how they partition the belief space into a grid <ref type="bibr" target="#b0">[Brafman, 1997;</ref><ref type="bibr" target="#b2">Zhou and Hansen, 2001]</ref>.</p><p>More similar to PBVI are those approaches which update both the value and gradient at each grid point <ref type="bibr" target="#b1">[Lovejoy, 1991;</ref><ref type="bibr" target="#b0">Hauskrecht, 2000;</ref><ref type="bibr" target="#b1">Poon, 2001]</ref>. While the actual point-based update is essentially the same between all of these, the overall algorithms differ in a few important aspects. Whereas Poon only accepts updates that increase the value at a grid point (requiring special initialization of the value function), and Hauskrecht always keeps earlier  -vectors (causing the set to grow too quickly), PBVI requires no such assumptions. A more important benefit of PBVI is the theoretical guarantees it provides: our guarantees are more widely applicable and provide stronger error bounds than those for other pointbased updates.</p><p>In addition, PBVI is significantly smarter than previous algorithms about how it selects belief points. PBVI selects only reachable beliefs; other algorithms use random beliefs, or (like Poon's and Lovejoy's) require the inclusion of a large number of fixed beliefs such as the corners of the probability simplex. Moreover, PBVI selects belief points which improve its error bounds as quickly as possible. In practice, our experiments on the large domain of lasertag demonstrate that PBVI's belief-selection rule handily outperforms several alternate methods. (Both Hauskrecht and Poon did consider using stochastic simulation to generate new points, but neither found simulation to be superior to random point placements. We attribute this result to the smaller size of their test domains. We believe that as more POMDP research moves to larger planning domains, newer and smarter belief selection rules will become more and more important.) Gradient-based policy search methods have also been used to optimize POMDP solutions <ref type="bibr" target="#b0">[Baxter and Bartlett, 2000;</ref><ref type="bibr" target="#b0">Kearns et al., 1999;</ref><ref type="bibr" target="#b1">Ng and Jordan, 2000]</ref>, successfully solving multi-dimensional, continuous-state problems. In our view, one of the strengths of these methods lies in the fact that they restrict optimization to reachable beliefs (as does PBVI). Unfortunately, policy search techniques can be hampered by low-gradient plateaus and poor local minima, and typically require the selection of a restricted policy class. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper presents PBVI, a scalable anytime algorithm for approximately solving POMDPs. We applied PBVI to a robotic version of lasertag, where it successfully developed a policy for capturing a moving opponent. Other POMDP solvers had trouble computing useful policies for this domain. PBVI also compared favorably with other solvers on three well-known smaller test problems. We attribute PBVI's success to two features, both of which directly target the curse of history. First, by using a trajectory-based approach to select belief points, PBVI focuses planning on reachable beliefs. Second, because it uses a fixed set of belief points, it can perform fast value backups.</p><p>In experiments, PBVI beats back the curse of history far enough that we can solve POMDPs an order of magnitude larger than most previous algorithms. With this success, we can now identify the next hurdle for POMDP research: contrary to our expectation, it turns out to be the old-fashioned MDP problem of having too many distinct physical states. This problem hits us in the cost of updating the point-based value function vectors. (This cost is quadratic in the number of physical states.) While this problem is not necessarily easy to overcome, we believe that sparse matrix computations, together with other approaches from the existing literature <ref type="bibr" target="#b1">[Poupart and Boutilier, 2003;</ref><ref type="bibr" target="#b1">Roy and Gordon, 2003]</ref>, will allow us to to scale PBVI to problems which are at least another order of magnitude larger. So, PBVI represents a considerable step towards making POMDPs usable for realworld problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: POMDP value function representation using PBVI (on the left) and a grid (on the right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Tag domain (870 states, 5 actions, 30 observations)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PBVI performance for four problems: Tag(left), Maze33(center-left), Hallway(center-right) and Hallway2(right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Belief expansion results for three problems: Hallway(left), Hallway2(center) and Tag(right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Robot E Opponent, or</head><label></label><figDesc></figDesc><table>E 

7 
$ 
 
i p 
S p 
i p 
 
7 
Ë 

Q P 

4 

and Opponent 

E 

7 
$ 

i p 
i p 
S p 
 
7 
Ë 

P 


7 
i R 
¨ 

S R 
R 
U T 
W V 

4 

. Both agents start in independently-
selected random positions, and the game finishes when 
Opponent 

E 
7 
R 
¨ 

R 
R 
U T 
W V 

. The robot can select from five actions: 

North, South, East, West, Tag 

4 

. A reward of 

¥ 
è  § 

is imposed 
for each motion action; the Tag action results in a 

î 
Å  § 

X I 

re-
ward if ¥ 
è  § 

X I 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results for POMDP domains. Those marked [*] were com-
puted by us; other results were likely computed on different plat-
forms, and therefore time comparisons may be approximate at best. 
All results assume a standard (not lookahead) controller. 

</table></figure>

			<note place="foot" n="3"> 2 .</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maze33 / Tiger-Grid</head><p>QMDP <ref type="bibr">[*]</ref> n.a. 0.198 0.19 n.a. Grid <ref type="bibr" target="#b0">[Brafman, 1997]</ref> n.a. 0.94 n.v. 174 PBUA <ref type="bibr" target="#b1">[Poon, 2001]</ref> n.a.  <ref type="bibr" target="#b0">[Littman et al., 1995]</ref> 47.4 n.v. n.v. n.a. PBUA <ref type="bibr" target="#b1">[Poon, 2001]</ref> 100  <ref type="bibr" target="#b0">[Littman et al., 1995]</ref> 25.9 n.v. n.v. n.a. Grid <ref type="bibr" target="#b0">[Brafman, 1997]</ref> 98 n.v. n.v. 337 PBUA <ref type="bibr" target="#b1">[Poon, 2001]</ref> 100 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; B</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">I</forename><surname>Bonet ; R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cassandra</surname></persName>
		</author>
		<ptr target="http://www.cs.brown.edu/research/ai/pomdp/code/index.html" />
	</analytic>
	<monogr>
		<title level="m">Learning policies for partially obsevable environments: Scaling up. In ICML</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="99" to="134" />
		</imprint>
	</monogr>
	<note>Approximate planning in large POMDPs via reusable trajectories. NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Handbook for Intelligent Control: Neural, Fuzzy and Adaptive Approaches, chapter The Role of Exploration in Learning Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">W S</forename><surname>Lovejoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lovejoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan ; A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; K.-M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; P</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boutilier ; M. Rosencrantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">N</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computationally feasible bounds for partially observed Markov decision processes</title>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1971" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="29" to="51" />
		</imprint>
		<respStmt>
			<orgName>The Hong-Kong University of Science and Technology ; Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
	<note>Operations Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An improved grid-based approximation algorithm for POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
