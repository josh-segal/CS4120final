<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Markov Logic Network Structure via Hypergraph Lifting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
							<email>koks@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
							<email>pedrod@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Markov Logic Network Structure via Hypergraph Lifting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. Learning MLN structure from a relational database involves learning the clauses and weights. The state-of-the-art MLN structure learners all involve some element of greedily generating candidate clauses, and are susceptible to local optima. To address this problem , we present an approach that directly utilizes the data in constructing candidates. A relational database can be viewed as a hypergraph with constants as nodes and relations as hyperedges. We find paths of true ground atoms in the hyper-graph that are connected via their arguments. To make this tractable (there are exponentially many paths in the hypergraph), we lift the hypergraph by jointly clustering the constants to form higher-level concepts, and find paths in it. We variabilize the ground atoms in each path, and use them to form clauses, which are evaluated using a pseudo-likelihood measure. In our experiments on three real-world datasets, we find that our algorithm outperforms the state-of-the-art approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, there has been a surge of interest in combining statistical and relational learning approaches <ref type="bibr">(Getoor &amp; Taskar, 2007)</ref>, driven by the realization that many applications require both. Recently, <ref type="bibr" target="#b17">Richardson and Domingos (2006)</ref> introduced Markov logic networks (MLNs), a statistical relational language combining first-order logic and Markov networks. An MLN consists of weighted first-order logic formulas, viewed as templates for Markov network features. Learning MLN structure is an important but challenging task, and to date only a few approaches have been proposed <ref type="bibr" target="#b6">(Kok &amp; Domingos, 2005;</ref><ref type="bibr">Mi- halkova &amp; Mooney, 2007;</ref><ref type="bibr" target="#b1">Biba et al., 2008b;</ref><ref type="bibr">etc.</ref>).</p><p>Most of these approaches systematically enumerate candidate clauses by starting from an empty clause, greedily adding literals to it, and testing the resulting clause's empirical fit to training data. Such a strategy has two shortcomings: searching the large space of clauses is computationally expensive; and it is susceptible to converging to a local optimum, missing potentially useful clauses. These shortcomings can be ameliorated by using the data to a priori constrain the space of candidates. This is the basic idea in relational pathfinding <ref type="bibr" target="#b16">(Richards &amp; Mooney, 1992)</ref>, which finds paths of true ground atoms that are linked via their arguments and then generalizes them into firstorder rules. Each path corresponds to a conjunction that is true at least once in the data. Since most conjunctions are false, this helps to concentrate the search on regions with promising rules. However, pathfinding potentially amounts to exhaustive search over an exponential number of paths. Hence, systems using relational pathfinding (e.g., BUSL <ref type="bibr" target="#b10">(Mihalkova &amp; Mooney, 2007)</ref>) typically restrict themselves to very short paths, creating short clauses from them and greedily joining them into longer ones.</p><p>In this paper, we present LHL, an approach that uses relational pathfinding to a fuller extent than previous ones. It mitigates the exponential search problem by first inducing a more compact representation of data, in the form of a hypergraph over clusters of constants. Pathfinding on this 'lifted' hypergraph is typically at least an order of magnitude faster than on the ground training data, and produces MLNs that are more accurate than previous state-of-the-art approaches. LHL is short for Learning via Hypergraph Lifting.</p><p>We begin by reviewing Markov logic in the next section. We then describe our structure learning algorithm (Section 3) and report our experiments with it (Section 4). Finally, we discuss related work (Section 5) and future work (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Markov Logic</head><p>In first-order logic <ref type="bibr" target="#b3">(Genesereth &amp; Nilsson, 1987)</ref>, formulas are constructed using four types of symbols: constants, variables, functions, and predicates. (In this paper we use only function-free logic.) Constants represent objects in a domain of discourse (e.g., people: Anna, Bob, etc.). Variables (e.g., x, y) range over the objects. Predicates represent relations among objects (e.g., Advises), or attributes of objects (e.g., Student). (In this paper, we use predicate and relation interchangeably.) Variables and constants may be typed. An atom is a predicate symbol applied to a list of arguments, which may be variables or constants (e.g., Advises(x, Bob)). A ground atom is an atom all of whose arguments are constants. A world is an assignment of truth values to all possible ground atoms. A database is a partial specification of a world; each atom in it is true, false or (implicitly) unknown. In this paper, we make a closed-world assumption: a ground atom not in the database is assumed to be false. A clause is a disjunction of non-negated/negated atoms. A Markov network <ref type="bibr" target="#b13">(Pearl, 1988)</ref> represents the joint distribution of a set of variables X = (X 1 , . . . , X n ) ∈ X as a product of factors:</p><formula xml:id="formula_0">P (X = x) = 1 Z k f k (x k )</formula><p>, where each factor f k is a non-negative function of a subset of the variables x k , and Z is a normalization constant. As long as P (X = x) &gt; 0 for all x, the distribution can be equivalently represented as a log-linear model:</p><formula xml:id="formula_1">P (X = x) = 1 Z exp ( i w i g i (x))</formula><p>, where the features g i (x) are arbitrary functions of (a subset of) the variables' state. A Markov logic network (MLN) is a set of weighted first-order formulas. Together with a set of constants representing objects in the domain, it defines a Markov network with one variable per ground atom and one feature per ground formula. The probability distribution over possible worlds x is given by</p><formula xml:id="formula_2">P (X = x) = 1 Z exp( i∈F j∈Gi w i g j (x))</formula><p>where Z is the partition function, F is the set of all first-order formulas in the MLN, G i is the set of groundings of the ith first-order formula, and g j (x) = 1 if the jth ground formula is true and g j (x) = 0 otherwise. Markov logic enables us to compactly represent complex models in non-i.i.d. domains. General algorithms for inference and learning in Markov logic are discussed in Richardson and Domingos (2006).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning via Hypergraph Lifting</head><p>We call our algorithm LHL, for Learning via Hypergraph Lifting. In LHL, we make use of hypergraphs. A hypergraph is a straightforward generalization of a graph in which an edge can link any number of nodes, rather than just two. More formally, we define a hypergraph as a pair (V, E) where V is a set of nodes, and E is a multiset of labeled non-empty ordered subsets of V called hyperedges. In LHL, we find paths in a hypergraph. A path is defined as a set of hyperedges such that for any two hyperedges e 0 and e n in the set, there exists an ordering of (a subset of) hyperedges in the set e 0 , e 1 , . . . , e n−1 , e n such that e i and e i+1 share at least one node.</p><p>A database can be viewed as a hypergraph with constants as nodes, and true ground atoms as hyperedges. Each hyperedge is labeled with a predicate symbol. Nodes (constants) are linked by a hyperedge (true ground atom) if and only if they appear as arguments in the hyperedge. (Henceforth we use node and constant interchangeably, and likewise for hyperedge and true ground atom.) A path of hyperedges can be generalized into a first-order clause by variabilizing their arguments. To avoid tracing the exponential number of paths in the hypergraph, LHL first jointly clusters the nodes into higher-level concepts, and by doing so it also clusters the hyperedges (i.e., the ground atoms containing the clustered nodes). The 'lifted' hypergraph has fewer nodes and hyperedges, and therefore fewer paths, reducing the cost of finding them. <ref type="figure" target="#fig_1">Figure 1</ref> provides an example. We have a database describing an academic department where professors tend to have students whom they are advising as teaching assistants (TAs) in the classes the professors are teaching. The left graph is created from the database, and after lifting, results in the right graph. Observe that the lifted graph is simpler and the clustered constants correspond to the high-level concepts of Professor, Student, and Course.</p><p>Algorithm 1 gives the pseudocode for LHL. LHL begins by lifting a hypergraph (Algorithm 2). Then it finds paths in the lifted hypergraph (Algorithm 3). Finally it creates candidate clauses from the paths, and learn their weights to create an MLN (Algorithm 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hypergraph Lifting</head><p>We call our hypergraph lifting algorithm LiftGraph. LiftGraph is similar to the MRC and SNE algorithms <ref type="bibr" target="#b7">(Kok &amp; Domingos, 2007;</ref><ref type="bibr" target="#b8">Kok &amp; Domingos, 2008)</ref>. It differs from them in the following ways. LiftGraph can handle relations of arbitrary arity, whereas</p><formula xml:id="formula_3">Algorithm 1 LHL(D, T, ω, µ, ν, π, π , σ) input: D, a relational database</formula><p>T , a set of types, where a type is a set of constants ω, maximum number of hyperedges in a path µ, minimum number of ground atoms per hyperedge in a path in order for it to be selected ν, maximum number of ground atoms to sample in a path π, π , length penalties on clauses σ, fraction of atoms to sample from D output: (Clauses, W eights), an MLN containing a set of learned clauses and their weights note: Index H maps from each node γi to the set of hyperedges r(γ1, . . . , γi, . . . , γn) containing γi E is a set of hyperedges in a lifted hypergraph P aths is a set of paths, each path being a set of hyperedges (H, E) ← Lif tGraph(D, T ) P aths ← ∅ for each r(γ1, . . . , γn) ∈ E P aths ← P aths ∪ F indP aths({r(γ1, . . . , γn)}, {γ1, . . . , γn}, ω, H)</p><formula xml:id="formula_4">(Clauses, W eights) ← CreateM LN (P aths, D, µ, ν, π, π , σ) return (Clauses, W eights) Algorithm 2 Lif tGraph(D, T )</formula><p>note: The inputs and output are as described in Algorithm 1</p><formula xml:id="formula_5">for each t ∈ T Γt ← ∅ for each x ∈ t Γt ← Γt ∪ {γx} (γx is a unit cluster containing x) H[γx] ← ∅ (H maps from nodes to hyperedges) E ← ∅ (E contains hyperedges) for each true ground atom r(x1, . . . , xn) ∈ D E ← E ∪ {r(γx 1 , . . . , γx n )} for each xi ∈ {x1, . . . , xn} H[γx i ] ← H[γx i ] ∪ {r(γx 1 , . . . , γx n )} repeat for each t ∈ T (γ best , γ best ) ← ClusterP airW ithBestGain(Γt) if {(γ best , γ best )} = ∅ γnew ← γ best ∪ γ best Γt ← (Γt \ {γ best , γ best }) ∪ γnew H[γnew] ← ∅ for each γ ∈ {γ best , γ best } for each r(γ1, . . . , γ, . . . , γn) ∈ H[γ] H[γnew] ← H[γnew] ∪ {r(γ1, . . . , γnew, . . . , γn)} E ← E \ {r(γ1, . . . , γ, . . . , γn)} E ← E ∪ {r(γ1, . . . , γnew, . . . , γn)} H[γ] ← ∅ until no clusters are merged for all t return (H, E)</formula><p>SNE can only handle binary relations. Unlike MRC, LiftGraph finds a single clustering of constant symbols rather than multiple clusterings. While both SNE and MRC can cluster predicate symbols, in this paper, for simplicity, we do not cluster predicates. (However, it is straightforward to extend LiftGraph to do so.) Most domains contain many fewer predicates than objects, and structure learning alone suffices to capture the dependencies among them, which is what LHL does. (Because SNE and MRC do not have a structure learning component, it is essential for them to cluster predicates in order to learn the dependencies among them.)</p><p>LiftGraph works by jointly clustering the constants in a hypergraph in a bottom-up agglomerative manner, allowing information to propagate from one cluster to another as they are formed. The number of clusters need not be pre-specified. As a consequence of clus- </p><formula xml:id="formula_6">P aths ∪ {CurP ath} V ← ∅ for each γj ∈ {γ1, . . . , γn} if γj ∈ V V ← V ∪ {γj } V ← V ∪ {γj } P aths ← P aths ∪ F indP ath(CurP ath, V, ω, H) CurP ath ← CurP ath \ {r(γ1, . . . , γn)} V ← V \ V return P aths</formula><p>tering the constants, the ground atoms in which the constants appear are also clustered. Each hyperedge in the lifted hypergraph contains at least one true ground atom.</p><p>LiftGraph is defined using Markov logic. We use the variable r to represent a predicate, x i for the ith argument of a predicate, γ i for a cluster of ith arguments of a predicate (i.e., a set of constant symbols), and Γ t for a clustering of constant symbols of type t (i.e., a set of clusters or, equivalently, a partitioning of a set of symbols). If x i is in γ i , we say that (x 1 , . . . , x n ) is in the cluster combination (γ 1 , . . . , γ n ), and that (γ 1 , . . . , γ n ) contains the atom r(x 1 , . . . , x n ). r(γ 1 , . . . , γ n ) denotes a hyperedge connecting nodes γ 1 , . . . , γ n . A hypergraph representing the true ground atoms r(x 1 , . . . , x n ) in a database is simply (V = {{x i }}, E = { r({x 1 }, . . . , {x n }) }) with each constant x i in its own cluster, and a hyperedge for each true ground atom.</p><p>The learning problem in LiftGraph consists of finding the cluster assignment {Γ} that maximizes the posterior probability P ({Γ}|D) ∝ P ({Γ})P (D|{Γ}), where D is a database of truth assignments to the observable r(x 1 , . . . , x n ) ground atoms. The prior P ({Γ}) is simply an MLN containing two rules. The first rule states that each symbol belongs to exactly one cluster. This rule is hard, i.e., it has infinite weight and cannot be violated.</p><p>∀x</p><formula xml:id="formula_7">∃ 1 γ x ∈ γ</formula><p>The second rule is</p><formula xml:id="formula_8">∀γ 1 , . . . , γ n ∃x 1 , . . . , x n x 1 ∈ γ 1 ∧ . . . ∧ x n ∈ γ n</formula><p>with negative weight −∞ &lt; −λ &lt; 0, which imposes an exponential prior on the number of cluster combinations to prevent overfitting. The parameter λ is fixed during learning, and is the penalty in log-posterior incurred by adding a cluster combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Markov Logic Network Structure via Hypergraph Lifting</head><p>Algorithm 4 CreateM LN (P aths, D, µ, ν, π, π , σ) calls: V ariabilizeP aths(P aths), replaces the nodes in each path in P aths with variables M akeClauses(P ath), creates clauses from hyperedges in P ath Sample(P ath, ν), uniformly samples ν ground atoms from P ath SampleDB(D, σ), uniformly samples a fraction σ of atoms from database D N umT rueGroundAtoms(P ath), counts the number of true ground atoms in P ath note: The inputs and output are as described in Algorithm 1 (only select paths with enough true ground atoms (heuristic 1)) P aths ← V ariabilizeP aths(P aths) SelectedP aths ← ∅ for each p ∈ P aths if (N umT rueGroundAtoms(p) &gt;= P athLength(p) * µ) SelectedP aths ← SelectedP aths ∪ {p} (evaluate candidates with ground atoms in P ath (heuristic 2))</p><formula xml:id="formula_9">CandidateClauses ← ∅ for each p ∈ SelectedP aths D ← Sample(p, ν) for each c ∈ M akeClauses(p) if Score(c, D ) &gt; Score(∅, D ) CandidateClauses ← CandidateClauses ∪ {c} CandidateClauses ← SortByLength(CandidateClauses) (Evaluate candidates with ground atoms in database D) D ← SampleDB(D, σ) SelectedClauses ← ∅ for each c ∈ CandidateClauses BetterT hanSubClauses ← T rue for each c ∈ (SubClauses(c) ∩ SelectedClauses) if Score(c, D ) &lt; Score(c , D ) BetterT hanSubClauses ← F alse break if (BetterT hanSubClauses) SelectedClauses ← SelectedClauses ∪ {c} AddClausesT oM LN (SelectedClauses) W eights ← LearnW eights(SelectedClauses) return (SelectedClauses, W eights)</formula><p>The main MLN for the likelihood P (D|{Γ}) contains the following rules. For each predicate r and each cluster combination (γ 1 , . . . , γ n ) that contains a true ground atom of r, the MLN contains the rule:</p><formula xml:id="formula_10">∀x 1 , . . . , x n x 1 ∈ γ 1 ∧ . . . ∧ x n ∈ γ n ⇒ r(x 1 , . . . , x n )</formula><p>We call these atom prediction rules because they state that the truth value of an atom is determined by the cluster combination it belongs to. These rules are soft. At most there can be one such rule for each true ground atom (i.e., when each constant is in its own cluster).</p><p>For each predicate r, we create a rule</p><formula xml:id="formula_11">∀x 1 , . . . , x n m i=1 ¬(x 1 ∈ γ i 1 ∧ . . . ∧ x n ∈ γ i n ) ⇒ r(x 1 , . . . , x n )</formula><p>where (γ 1 1 , . . . , γ 1 n ), . . . , (γ m 1 , . . . , γ m n ) are cluster combinations containing true ground atoms of r. This rule accounts for all atoms (all false) that are not in any cluster combination with true ground atoms of r. We call such a rule a default atom prediction rule because its antecedents are analogous to a default cluster combination that contains all atoms that are not in the cluster combinations of any atom prediction rule.</p><p>LiftGraph simplifies the learning problem by performing hard assignments of constant symbols to clusters (i.e., instead of computing probabilities of cluster membership, a symbol is simply assigned to its most likely cluster). The weights and the log-posterior can now be computed in closed form. 1 LiftGraph thus simply searches over cluster assignments, evaluating each one by its posterior probability. It begins by assigning each constant symbol x i to its own cluster {x i }, and creating a hyperedge r <ref type="figure" target="#fig_1">({x 1 }, .</ref> . . , {x n }) for each true ground atom r(x 1 , . . . , x n ). Next it creates candidate pairs of clusters of each type, and for each pair, it evaluates the gain in posterior probability if its clusters are merged. It then chooses the pair that gives the largest gain to be merged. , . . . , γ n ) must do too. To avoid trying all possible candidate pairs of clusters, LiftGraph only tries to merge γ i and γ i if they appear in hyperedges r(γ 1 , . . . , γ i , . . . , γ n ) and r(γ 1 , . . . , γ i , . . . , γ n ). In this manner, it incrementally merges clusters until no merges can be performed to improve posterior probability. It then returns a lifted hypergraph whose hyperedges all contain at least one true ground atom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Path Finding</head><p>FindPaths constructs paths by starting from each hyperedge in a hypergraph. It begins by adding a hyperedge to an empty path, and then recursively adds hyperedges linked to nodes already present in the path (hyperedges already in the path are not re-added). Its search terminates when the path reaches a maximum length or when no new hyperedge can be added. Each time a hyperedge is added to the path, FindPaths stores the resulting path as a new one. All the paths are passed on to the next step to create clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Clause Creation and Pruning</head><p>A path in the hypergraph corresponds to a conjunction of r(γ 1 , . . . , γ n ) hyperedges, and it guarantees that the conjunction has at least one support in the hypergraph (i.e., there is at least one true ground atom in each hyperedge). We replace each γ i in a path with a variable, thereby creating a variabilized atom for each hyperedge. We convert the conjunction of positive literals to a clause because that is the form that is typically used by ILP (inductive logic programming) and MLN structure learning and inference algorithms. (In Markov logic, a conjunction of positive literals with weight w is equivalent to a clause of negative literals with weight −w). In addition, we add clauses with the signs of up to n literals flipped (where n is a user-defined parameter), since the resulting clauses may also be useful. (Notice that if all but one of the literals are negative, this is a definite clause whose antecedent is supported by a path in the hypergraph.)</p><p>We evaluate each clause using weighted pseudolog-likelihood (WPLL) <ref type="bibr" target="#b6">(Kok &amp; Domingos, 2005</ref>). WPLL is defined as:</p><formula xml:id="formula_12">log P • w,F,D (X = x) = r∈R c r g∈G D r log P w,F (X g = x g |M B x (X g ))</formula><p>where F is a set of clauses, w is a set of clause weights, R is the set of first-order predicates, G D r is a set of ground atoms of predicate r in database D, and x g is the truth value (0 or 1) of ground atom g, and</p><formula xml:id="formula_13">P w,F (X g = x g |M B x (X g )) = exp i∈F wini(x) exp i∈F wini(x [Xg=0] ) +exp i∈F wini(x [Xg=1] )</formula><p>. M B x (X g ) is the state of X g 's Markov blanket in the data, n i (x) is the number of true groundings of the ith clause in x, n i (x <ref type="bibr">[Xg=0]</ref> ) is the number of true groundings of the ith clause when we force X g = 0 and leave the remaining data unchanged, and similarly for n i (x <ref type="bibr">[Xg=1]</ref> ). Following Kok &amp; Domingos, we set c r = 1/|G D r | to weight all first-order predicates equally, and penalize the WPLL with a length penalty −πd, where d is the number of atoms in a clause. Summing over all ground atoms in WPLL is computationally expensive, so we only sum over a randomly-sampled fraction σ of them. We define the score of a clause c as Score(c, D) = log P • w ,F ,D (X = x) − πd, where F is a set containing c and one unit clause for each predicate in R, and w is a set of optimal weights for the clauses in F .</p><p>We iterate over the clauses from shortest to longest. For each clause, we compare its scores against those of its sub-clauses (considered separately) that have already been retained. If the clause scores higher than all of these sub-clauses, it is retained; otherwise, it is discarded. In this manner, we discard clauses which are unlikely to be useful. Note that this process is efficient because the score of a clause only needs to be computed once, and can be cached for future comparisons. (Alternatively, we could evaluate a clause against all its sub-clauses taken together, but this would require re-optimizing the weights for each combination of sub-clauses for every comparison, which is computationally expensive.)</p><p>Finally we add the retained clauses to an MLN. We have the option of doing this in several ways. We could greedily add the clauses one at a time in order of decreasing score. After adding each clause, we relearn the weights, and keep the clause in the MLN if it improves the overall WPLL. Alternatively, we could add all the clauses to the MLN, and learn weights using L1 regularization to prune away 'bad' clauses by giving them zero weights <ref type="bibr" target="#b5">(Huynh &amp; Mooney, 2008)</ref>. Lastly, we could use L2-regularization instead if the number of clauses is not too large, and rely on the regularization to give 'bad' clauses low weight. Optionally, we discard clauses containing 'dangling' variables (i.e., variables which only appear once in a clause), since these are unlikely to be useful.</p><p>We use two heuristics to speed up clause evaluation. First we discard a path at the outset if it contains fewer than µ true ground atoms per hyperedge. This cuts the time we spend evaluating clauses that are not well supported by data. Second, before evaluating a clause's WPLL with respect to a database, we evaluate it with respect to the smaller number of ground atoms contained in the paths that gave rise to it. (Note that a clause can be created from different paths.) We limit the number of such ground atoms to a maximum of ν.</p><p>We use a smaller structure prior π to avoid prematurely removing good clauses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We carried out experiments to investigate whether LHL performs better than previous approaches, and to evaluate the contributions of its components. We used three datasets publicly available at http://alchemy.cs.washington.edu. Their details are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>IMDB. This dataset, created by <ref type="bibr" target="#b10">Mihalkova and Mooney (2007)</ref> from the IMDB.com database, describes a movie domain. It contains predicates describing movies, actors, directors, and their relationships (e.g, Actor(person), WorkedIn(person, movie), etc.) It is divided into 5 independent folds. We omitted 4 equality predicates (e.g., SameMovie(movie, movie)) that are true if and only if their arguments are the same. They are superseded by the equality operator in the systems we are comparing, and can be easily predicted with a unit clause (e.g., SameMovie(x, x)), trivially boosting the systems' performances.</p><p>UW-CSE. This dataset, prepared by <ref type="bibr" target="#b17">Richardson and Domingos (2006)</ref>, describes an academic department.</p><p>Its predicates describe students, faculty, and their relationships (e.g, Professor(person),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Markov Logic Network Structure via Hypergraph Lifting</head><p>TaughtBy(course, person, quarter), etc.).</p><p>The dataset is divided into 5 independent areas/folds (AI, graphics, etc.). We omitted 9 equality predicates for the same reasons as above.</p><p>Cora. This dataset is a collection of citations to computer science papers, created by Andrew McCallum, and later processed by Singla and Domingos (2006) into 5 folds for the task of deduplicating the citations, and their title, author, and venue fields. Predicates include: SameCitation(cit1, cit2), TitleHasWord(title, word), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Systems</head><p>We compared LHL to two state-of-the-art systems: BUSL <ref type="bibr" target="#b10">(Mihalkova &amp; Mooney, 2007)</ref> and MSL <ref type="bibr" target="#b6">(Kok &amp; Domingos, 2005</ref>). Both systems are implemented in the Alchemy software package ( <ref type="bibr" target="#b9">Kok et al., 2009</ref>).</p><p>BUSL. BUSL uses a form of relational pathfinding to find a path of ground atoms in the training data, but restricts itself to very short paths (length 2) to avoid fully searching the large space of paths. It variabilizes each ground atom in the path, and constructs a Markov network whose nodes are the paths viewed as Boolean variables (conjunctions of atoms). It uses the Grow-Shrink Markov network learning algorithm to find the edges between the nodes. For each node, the algorithm greedily adds and removes nodes from its Markov blanket using the χ 2 measure of dependence. From the cliques thus created in the Markov network, BUSL creates clauses. For each clique, it forms disjunctions of the atoms in the clique's nodes, and creates clauses with all possible negation/non-negation combinations of the atoms. BUSL then computes the WPLL of the clauses, and adds them one at a time, in order of decreasing WPLL, to an MLN containing only unit clauses. After adding a clause, the weights of all clauses in the MLN are relearned to compute the new WPLL. If a clause increases the overall WPLL, it is retained in the MLN.</p><p>MSL. We used the beam search version of MSL that is implemented in Alchemy. MSL begins by adding all possible unit clauses to an MLN. MSL maintains a set of n clauses that give the best score improvement over the current MLN. Initially, the set is empty. MSL creates all possible clauses of length two, and adds the n clauses with the highest improvement in WPLL to the set. It then repeatedly adds literals to the clauses in the set, and evaluates the WPLL of the newly formed clauses, always maintaining the n highest-scoring ones in the set. When none can be added to the set, it adds the best performing clause in the set to the MLN. It then restarts the search from an empty set. MSL terminates when it cannot find a clause that improves upon the current MLN's WPLL.</p><p>To investigate the importance of hypergraph lifting, we removed the LiftGraph component from LHL, and let FindPaths run on the unlifted hypergraph. The rules it learned were pruned by CreateMLN as normal. We call this system LHL-FindPaths. We also investigated the contribution of hypergraph lifting alone by applying LiftGraph's MLN on the test sets. We call this system LHL-LiftGraph. We also investigated the effectiveness of the two heuristics in CreateMLN, by disabling them and observing the performance of the MLN thus learned by LHL. We call this system LHLNoHeu. Altogether we compared six systems: LHL, LHL-NoHeu, LHL-FindPaths, LHL-LiftGraph, BUSL and MSL. All systems are implemented in C++.</p><p>The following parameter values were used for the LHL systems on all datasets: λ = 1, µ = 50, ν = 500, σ = 0.5. The other parameters were set as follows: ω =5 (IMDB, UW-CSE) and 4 (Cora); π = 0.01 (UW-CSE, Cora) and 0.1 (IMDB); and π = 0.001 (UW-CSE, Cora) and 0.01 (IMDB). (See Algorithm 1 for the parameter descriptions.) For BUSL and MSL, we set their parameters corresponding to π to values we used for LHL. We also set their minW eight parameter to zero (empirically we found that this value performed better than their defaults). All other BUSL and MSL parameters were set to their default values. For all LHL systems, we created clauses with all combinations of negated/non-negated atoms in a variabilized path; greedily added clauses one at a time in order of decreasing score to an MLN (initially empty); and excluded clauses with dangling variables from the final MLN. (To ensure fairness, we also tried excluding dangling clauses in BUSL and MSL, and report the best results for each.) The parameters were set in an ad hoc manner, and per-fold optimization using a validation set could conceivably yield better results. All systems were run on identically configured machines (2.8GHz, 4GB RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Methodology</head><p>For each dataset, we performed cross-validation using the five previously defined folds. For IMDB and UW-CSE, we performed inference over the groundings of each predicate to compute their probabilities of being true, using the groundings of all other predicates as evidence. Exceptions are the predicates Actor and Director (IMDB), and Student and Professor (UW-CSE). We evaluated groundings for those predicates together, using all other predicates as evidence. This is because groundings of those predicates for the same constant are mutually exclusive and exhaustive (e.g., Actor(Bob) and Director(Bob)). Knowing one deter-  <ref type="bibr" target="#b14">(Poon &amp; Domingos, 2006</ref>) because it has been shown to give better results for MLNs containing deterministic rules, which LHL-LiftGraph does. Each run of the inference algorithms drew 1 million samples, or ran for a maximum of 24 hours, whichever came earlier. To evaluate the performance of the systems, we measured the average conditional log-likelihood of the test atoms (CLL), and the area under the precision-recall curve (AUC). The advantage of the CLL is that it directly measures the quality of the probability estimates produced. The advantage of the AUC is that it is insensitive to the large number of true negatives (i.e., atoms that are false and predicted to be false). The precision-recall curve for a predicate is computed by varying the threshold CLL above which an atom is predicted to be true. The results for MSL on UW-CSE and Cora are much worse than those reported by <ref type="bibr" target="#b6">Kok and Domingos (2005)</ref>. They evaluated MSL by computing the probability that a ground atom is true given all other ground atoms as evidence, a much easier task than ours. We also did not use their domain-specific declarative bias to guide clause construction. (Notice how LHL is able to overcome the myopia of greedy search without the help of this bias.) The results for BUSL on IMDB and UW-CSE are also worse than that reported by <ref type="bibr">Mi- halkova and Mooney (2007)</ref>. Unlike them, we omitted the equality predicates (as mentioned earlier) because they are superfluous and can be easily predicted with a single unit clause. We also infer the groundings of Actor/Director and Professor/Student simultaneously, which is a harder task than theirs. The last two reasons also contribute to MSL's poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Huynh and Mooney <ref type="bibr">(2008)</ref>, and <ref type="bibr" target="#b0">Biba et al. (2008a)</ref> proposed discriminative structure learning algorithms for MLNs. These algorithms learn clauses that predict a single target predicate, unlike LHL, which models the full joint distribution of the predicates. Besides relational pathfinding <ref type="bibr" target="#b16">(Richards &amp; Mooney, 1992)</ref>, ILP approaches with bottom-up aspects include <ref type="bibr">Muggle- ton &amp; Buntine (1988)</ref>, <ref type="bibr" target="#b12">Muggleton &amp; Feng (1992)</ref>, etc. These approaches are vulnerable to noise in the data, and also only create clauses to predict a single target predicate. <ref type="bibr" target="#b15">Popescul and Ungar (2004)</ref> have also used clustering to improve probabilistic rule induction. Their approach is limited to logistic regression and SQL rules, uses a very simple clustering method (k-means), and requires pre-specifying the number of clusters. <ref type="bibr" target="#b2">Craven and Slattery (2001)</ref> learn first-order rules for hypertext classification using naive Bayes models as invented predicates. The idea of lifting comes from theorem-proving in first-order logic. In recent years, it has been extended to inference in MLNs and other probabilistic languages. In lifted belief propagation <ref type="bibr" target="#b19">(Singla &amp; Domingos, 2008)</ref>, the algorithm forms clusters of ground atoms and clusters of ground clauses. It performs inference over the more compact network of clusters, thereby improving efficiency. This is analogous to LHL's approach of forming clusters of ground atoms to create a lifted hypergraph in which the search for clauses is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We proposed LHL, a novel algorithm for learning MLN structure. LHL lifts the training data into a compact hypergraph over clusters of constants, and uses relational pathfinding over the hypergraph to find clauses. Empirical comparisons with two state-of-the-art systems on three datasets show the promise of LHL. Future work includes: more tightly integrating the components of LHL; scaling it up further; applying LHL to larger, richer domains (e.g., the Web); etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Appearing in Proceedings of the 26 th International Confer- ence on Machine Learning, Montreal, Canada, 2009. Copy- right 2009 by the author(s)/owner(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Lifting a hypergraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 3 F</head><label>3</label><figDesc>indP aths(CurP ath, V, ω, H) input: CurP ath, set of connected hyperedges V , set of nodes in CurP ath note: The other inputs &amp; output are as described in Algorithm 1 if |CurP ath| = ω return ∅ P aths ← ∅ for each γi ∈ V for each r(γ1, . . . , γn) ∈ H[γi] if r(γ1, . . . , γn) ∈ CurP ath CurP ath ← CurP ath ∪ {r(γ1, . . . , γn)} P aths ←</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>When clusters γ i and γ i are merged to form γ new i , each hyperedge r(γ 1 , . . . , γ i , . . . , γ n ) is replaced with r(γ 1 , . . . , γ new i , . . . , γ n ) (and similarly for hyperedges containing γ i ). Since r(γ 1 , . . . , γ i , . . . , γ n ) contains at least one true ground atom, r(γ 1 , . . . , γ new i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 . Information on datasets.</head><label>1</label><figDesc></figDesc><table>Const-
Predi-
True 
Total 
Dataset 
Types 
ants 
cates 
Atoms 
Atoms 

IMDB 
4 
316 
6 
1224 
17,793 
UW-CSE 
9 
929 
12 
2112 
260,254 
Cora 
5 
3079 
10 
42,558 
687,422 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 . Experimental results.</head><label>2</label><figDesc></figDesc><table>IMDB 
UW-CSE 
Cora 
System 
AUC 
CLL 
Time (min) 
AUC 
CLL 
Time (hr) 
AUC 
CLL 
Time (hr) 
LHL 
0.69±0.01 −0.13±0.00 15.63±1.88 0.22±0.01 −0.04±0.00 
7.55±1.53 
0.87±0.00 −0.26±0.00 
14.82±1.78 
LHL-NoHeu 
0.69±0.01 −0.13±0.00 39.00±13.56 0.22±0.01 −0.04±0.00 158.24±46.70 0.87±0.00 −0.26±0.00 
33.99±3.86 
LHL-FindPaths 0.69±0.01 −0.13±0.00 242.41±30.31 0.19±0.01 −0.04±0.00 56.69±19.98 0.91±0.00 −0.17±0.00 5935.50±39.21 
LHL-LiftGraph 0.45±0.01 −0.27±0.01 
0.18±0.01 0.14±0.01 −0.06±0.00 0.001±0.000 
-
-
0.01±0.01 
BUSL 
0.47±0.01 −0.14±0.00 
4.69±1.02 0.21±0.01 −0.05±0.00 12.97±9.80 
0.17±0.00 −0.37±0.00 
18.65±9.52 
MSL 
0.41±0.01 −0.17±0.00 
2.79±0.59 0.18±0.01 −0.57±0.00 
2.13±0.38 
0.17±0.00 −0.37±0.00 
65.60±1.82 

mines the value of the other. For Cora, we ran infer-
ence over each of the four predicates SameCitation, 
SameTitle, SameAuthor, and SameVenue in turn, us-
ing the groundings of all other predicates as evidence. 
We used Alchemy's Gibbs sampling for all systems ex-
cept LHL-LiftGraph. For LHL-LiftGraph, we used 
Alchemy's MC-SAT algorithm </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 reports</head><label>2</label><figDesc></figDesc><table>the AUCs, CLLs and runtimes. The 
AUC and CLL results are averages over all atoms in 
the test sets and their standard deviations. Runtimes 
are averages over the five folds. 

We first compare LHL to BUSL and MSL. In both 
AUC and CLL, LHL outperforms BUSL and MSL 
on all datasets. The differences between LHL and 
BUSL/MSL on all datasets are statistically significant 
according to one-tailed paired t-tests (p-values ≤ 0.02 
for both AUC and CLL). LHL is slower than BUSL 
and MSL on the smallest dataset (IMDB), mixed on 
the medium one (UW-CSE), and faster on the largest 
one (Cora). This suggests that LHL scales better than 
BUSL and MSL. 

Next we compare LHL to its components LHL-
LiftGraph and LHL-FindPaths. Comparing the run-
times of LHL and LHL-FindPaths, we see that LHL 
is much faster than LHL-FindPaths. LHL's AUC and 
CLL are similar to or better than LHL-FindPaths's 
on IMDB and UW-CSE, but are worse on Cora. 
These results suggest that: LHL is a lot faster than 
LHL-FindPaths without any loss in accuracy on some 
datasets; and when LHL-FindPaths does better, it 

</table></figure>

			<note place="foot" n="1"> See http://alchemy.cs.washington.edu/papers/kok09a for the derivation of the log-posterior.</note>

			<note place="foot" n="2"> For each test fold, we ran FindPaths in parallel on all training folds, and added the runtimes. 3 LHL-LiftGraph on Cora crashed by running out of memory. Alchemy automatically converts the default atom prediction rules into clausal form and represents each clause separately, causing a blow-up in the number of clauses.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
		<title level="m">Discriminative structure learning of Markov logic networks. 18th Int. Conf. on Ind. Logic Prog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="59" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structure learning of Markov logic networks through iterated local search. 18th Euro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Art. Intel</title>
		<imprint>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational learning with statistical predicate invention: Better models for hypertext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="97" to="119" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Logical foundations of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<editor>&amp; Taskar, B.</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative structure and parameter learning for Markov logic networks. 25th Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="416" to="423" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the structure of Markov logic networks. 22th Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="441" to="448" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical predicate invention. 24th Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="443" to="440" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting semantic networks from text via relational clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euro. Conf. on Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="624" to="639" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<title level="m">The Alchemy system for statistical relational AI</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Dept. of Comp. Sci. &amp; Eng., Univ. of Washington</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up learning of Markov logic network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mihalkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int. Conf. on Mach. Learn</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine invention of first-order predicates by inverting resolution. 5th Int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. on Mach. Learn</title>
		<imprint>
			<biblScope unit="page" from="339" to="352" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient induction in logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inductive logic programming</title>
		<editor>S. Muggleton</editor>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="281" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems: Networks of plausible inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sound and efficient inference with probabilistic and deterministic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Nat. Conf. on Art. Intel</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="458" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster-based concept invention for statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Int. Conf. on Know. Disc. and Data Min</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="665" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning relations by pathfinding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Nat. Conf. on Art. Intel</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<title level="m">Entity resolution with Markov logic. 6th Int. Conf. on Data Min</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lifted first-order belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23th AAAI Conf. on Art. Intel</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1094" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
