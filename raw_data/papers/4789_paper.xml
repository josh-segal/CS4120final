<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural-Pattern Databases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering &amp; Management Technion</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmel</forename><surname>Domshlak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering &amp; Management Technion</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural-Pattern Databases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Explicit abstraction heuristics, notably pattern-database and merge-and-shrink heuristics, are employed by some state-of-the-art optimal heuristic-search planners. The major limitation of these abstraction heuristics is that the size of the abstract space has to be bounded by a (large) constant. Targeting this issue, Katz and Domshlak (2008b) introduced structural, and in particular fork-decomposition, abstractions, in which the planning task is abstracted by an instance of a tractable fragment of optimal planning. At first view, however, the lunch was not free. Some of the power of the explicit abstraction heuristics comes from pre-computing the heuristic function offline, and then determine h(s) for each evaluated state s by a very fast lookup in a &quot;database&quot;. In contrast, fork-decomposition offer a poly-time, yet far from being fast, computation. In this contribution, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. Specifically, we show that an equivalent of the explicit abstractions&apos; notion of &quot;database&quot; exists for the fork-decomposition abstractions as well, and this despite of their exponential-size abstract spaces. Experimentally , we show that heuristic search with such &quot;databased&quot; fork-decomposition heuristics favorably competes with the state-of-the-art of optimal planning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Heuristic search is nowadays one of the leading approaches to cost-optimal planning. The difference between various cost-optimizing heuristic-search planners is mainly in the employed admissible heuristic functions. In state-space search, such a heuristic estimates the cost of achieving the goal from a given state, and being admissible, it guarantees not to overestimate that cost. During the last decade, numerous computational ideas evolved into new admissible heuristics for domain-independent planning; this includes deleterelaxing max heuristic h max <ref type="bibr" target="#b1">(Bonet &amp; Geffner 2001)</ref>, critical path heuristics h m <ref type="bibr" target="#b8">(Haslum &amp; Geffner 2000)</ref>, landmark heuristics h L and h LA <ref type="bibr" target="#b16">(Karpas &amp; Domshlak 2009)</ref>, and abstraction heuristics such as pattern-database <ref type="bibr" target="#b5">(Edelkamp 2001)</ref>, merge-and-shrink <ref type="bibr" target="#b13">(Helmert, Haslum, &amp; Hoffmann 2007)</ref>, and structural-pattern <ref type="bibr" target="#b18">(Katz &amp; Domshlak 2008b)</ref> heuristics. In addition, these heuristics have been combined via additive ensembles of admissible heuristics (see, e.g., <ref type="bibr" target="#b5">Edelkamp (2001)</ref>, <ref type="bibr" target="#b10">Haslum et al. (2005)</ref>, <ref type="bibr">Katz &amp; Domsh- lak (2008a)</ref>, <ref type="bibr" target="#b16">Karpas &amp; Domshlak (2009)</ref>).</p><p>Apart from being as informative as possible, the heuristic should also be computable in polynomial time. In fact, the latter is a necessary, yet, in practice, insufficient condition. If any reasonable time cap is put on the planner, the heuristic computation per search node has to be really fast. For many problems, this has to be the case even if the heuristic is perfect, and this because of a large number of states evaluated along the optimal path. This well-known issue can be exemplified by the results of the cost-optimal planning track of the IPC-2008 competition in which even the bestperforming heuristic search planner HSP * F (Haslum 2008) solved less problems within the time limit of 30 minutes than the baseline breadth-first search. With this "heuristic selection" criterion of the fast per-search-node computation in mind, here we focus on abstraction heuristics, and most closely, on structural-pattern heuristics.</p><p>Probably the most well-known abstraction heuristics are pattern database (PDB) heuristics that are based on projecting the planning task onto a subset of its state variables and then explicitly searching for optimal plans in the abstract space. The limitation of PDB heuristics in practice is that both the size of the abstract space and its dimensionality have to be fixed. <ref type="bibr">1</ref> The more recent, merge-and-shrink abstractions, generalize PDB heuristics to overcome the latter limitation; instead of perfectly reflecting just a few state variables, merge-and-shrink abstractions allow for imperfectly reflecting all variables. The abstract space of mergeand-shrink is still searched explicitly, and thus it still has to be of fixed size.</p><p>Targeting both limitations of the PDB abstractions, <ref type="bibr" target="#b18">Katz and Domshlak (2008b)</ref> introduced the framework of structural patterns in which the planning task is abstracted to a tractable fragment of optimal planning. In the extreme case where the problem itself belongs to that fragment, nothing will be abstracted at all, and the perfect heuristic will be computed in polynomial time. <ref type="bibr">Katz and Domshlak in- troduced</ref> a concrete instance of the structural patterns idea, called fork decomposition, in which the problem is projected (in a proper meaning of this term) onto fork and inverted fork subgraphs of the problem's causal graph. The promise of fork-decomposition has been shown via a formal asymptotic performance analysis on selected domains, but the empirical relevance has yet to be shown.</p><p>Examining the empirical effectiveness of the forkdecomposition heuristics, we have implemented and evaluated them using A * on numerous IPC domains. On many domains, the quality of the fork-decomposition heuristics appeared to be very encouraging, to say the least. However, in all domains we pretty quickly bumped into the wall of the per-node computation complexity. While the planner was able to solve many challenging problems within a standard memory cap, it was typically running out of reasonable time limits. Surprising it was not. While pattern database and merge-and-shrink heuristics are also computationally prohibitive, they allow for pre-computing the heuristic values for all abstract states simultaneously, storing them in a lookup table, and then reusing that pre-search computation between search states. In contrast, the abstract spaces induced by structural patterns can be as large as the original state space, and thus they cannot be pre-solved and prestored exhaustively.</p><p>Overall, while the empirical (in)effectiveness of forkdecomposition was not surprising, given the good quality of its induced estimates, disappointing it was. Here, however, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. Specifically, we show that an equivalent of the PDB's and merge-and-shrink's notion of "database" exists for the fork-decomposition abstractions as well, and this despite of their exponential-size abstract spaces. In fact, the same effect could possibly be achieved also for other (to be developed in the future) structural-pattern heuristics.</p><p>Unlike for PDBs and merge-and-shrink abstractions, structural-pattern databases (and in particular these for the fork-decomposition's fork and inverted fork abstractions) do not (and cannot) result in purely lookup computations of h(s). In contrast, structural-pattern databases are based on a proper partition of heuristic computation (h-partition) into parts that can be shared between search states, and parts that shall be computed per individual state. We prove existence of such effective h-partitions for both fork and inverted fork abstractions. We then formally and empirically show that these h-partitions lead to fast pre-search and pernode computations that allow for exploiting the informativeness of the fork-decomposition heuristics in practice. Experimentally, we show that A * equipped with the "databased" fork-decomposition heuristics favorably competes with the state-of-the-art of cost-optimal planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We consider planning problems captured by the SAS + formalism <ref type="bibr" target="#b0">(Bäckström &amp; Nebel 1995</ref>) with non-negative action costs. Such a problem is given by a quintuple Π = V, A, I, G, c, where:</p><p>• V is a set of state variables, each v ∈ V is associated with a finite domain D(v); the value provided to a variable v by a (possibly partial) assignment p to V is denoted by p <ref type="bibr">[v]</ref>. Each complete assignment to V is called a state, and S = D(V ) is the state space of Π. I is an initial state. The goal G is a partial assignment to V ; a state s is a goal state iff G ⊂ s.</p><p>• A is a finite set of actions. Each action a is a pair pre(a), eff(a) of partial assignments to V called preconditions and effects, respectively. By A v ⊆ A we denote the actions affecting the value of v. c : A → R + 0 is a real-valued, non-negative action cost function.</p><p>An action a is applicable in a state s iff s[v] = pre(a) <ref type="bibr">[v]</ref> whenever pre(a) <ref type="bibr">[v]</ref> is specified. Applying a changes the value of v to eff(a) <ref type="bibr">[v]</ref> if eff(a) <ref type="bibr">[v]</ref> is specified. The resulting state is denoted by sa; by sa 1 , . . . , a k we denote the state obtained from sequential application of the (respectively applicable) actions a 1 , . . . , a k starting at state s. Such an action sequence is an s-plan if G ⊆ sa 1 , . . . , a k , and it is a cost-optimal (or, in what follows, optimal) s-plan if the sum of its action costs is minimal among all s-plans. The purpose of (optimal) planning is finding an (optimal) I-plan.</p><p>For a pair of states s 1 , s 2 ∈ S, by c(s 1 , s 2 ) we refer to the cost of a cheapest action sequence taking us from s 1 to s 2 in the transition system induced by Π; h * (s) = min s ⊇G c(s, s ) is the custom notation for the cost of the optimal s-plan in Π. We also use two well-known graphical structures induced by a planning task Π with variables V .</p><p>• The causal graph CG(Π) of Π is a digraph over nodes V .</p><p>An arc (v, v ) is in CG(Π) iff v = v and there exists an action a ∈ A such that both eff(a) <ref type="bibr">[v ]</ref> and either pre(a) <ref type="bibr">[v]</ref> or eff(a) <ref type="bibr">[v]</ref> are specified. Our focus here is on state-dependent, admissible abstraction heuristics. A heuristic is state-dependent if its estimate for a search node depends only on the problem state associated with that node, that is, h : S → R + 0 ∪ {∞}. Most heuristics in use these days are state-dependent (though see, e.g., <ref type="bibr" target="#b19">Richter et al. (2008)</ref> for a different case). A statedependent heuristic h is admissible if h(s) ≤ h * (s) for all states s. Finally, an abstraction heuristic is based on mapping Π's transition system over states S to an abstract transition system over states S α . The mapping is defined by an abstraction function α : S → S α that guarantees c α (α(s), α(s )) ≤ c(s, s ) for all states s, s ∈ S. The (admissible) abstraction heuristic h α (s) is then the distance from α(s) to the closest abstract goal state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristic Complexity</head><p>Accuracy and low time complexity are both desired yet competing properties of heuristic functions. For many powerful heuristics, and in particular abstraction heuristics, computing h(s) for each state s in isolation is infeasible-while computing h(s) is polynomial in the description size of Π, it is often not efficient enough to be performed at each search node. Fortunately, for some heuristics this obstacle can be overcome to a large extent by sharing most of the computation between the evaluations of h on different states. If that is possible, the shared parts of computing h(s) for all problem states s are pre-computed and memorized before the search, and then reused during the search by the evaluations of h on different states. This mixed offline/online heuristic computation, called henceforth h-partition, is adopted by various optimal planners using backward critical-path heuristics h m for m &gt; 1 (Haslum, Bonet, &amp; Geffner 2005), landmark heuristics h L and h LA <ref type="bibr" target="#b16">(Karpas &amp; Domshlak 2009)</ref>, and abstraction pattern-database and merge-and-shrink heuristics <ref type="bibr" target="#b5">(Edelkamp 2001;</ref><ref type="bibr" target="#b13">Helmert, Haslum, &amp; Hoffmann 2007)</ref>. Without an effective h-partition, optimal search with these heuristics would not scale up well, while with such an hpartition these heuristics constitute the state of the art of heuristic search planning.</p><p>We define the time complexity of an h-partition as the complexity of computing h for a set of states. Given a subset of k problem states S ⊆ S, the h-partition's time complexity of computing {h(s) | s ∈ S } is expressed as O(X + kY ), where O(X) and O(Y ) are, respectively, the complexity of the (offline) pre-search and (online) per-node parts of computing h(s) for a state s ∈ S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstraction Heuristics and h-Partitions</head><p>As we briefly mentioned above, an abstraction heuristic for a problem Π maps Π's transition system over states S to an abstract transition system over states S α . The mapping is defined by an abstraction function α : S → S α that guarantees c α (α(s), α(s )) ≤ c(s, s ) for all states s, s ∈ S. Such "distance conservancy" is in particular guaranteed by homomorphism abstractions, obtained (only) by systematically contracting groups of states into abstract states. The abstraction heuristic h α (s) is the distance from α(s) to the closest abstract goal state, and admissibility of h α is implied by the distance conservancy of α.</p><p>The most well-studied abstraction heuristics are pattern database (PDB) heuristics. Given a planning task Π over state variables V , such a heuristic is based on projecting Π onto a subset of its variables V α ⊆ V . Such a homomorphism abstraction α maps two states s 1 , s 2 ∈ S into the same abstract state iff</p><formula xml:id="formula_0">s 1 [V α ] = s 2 [V α ].</formula><p>Inspired by the (similarly named) domain-specific heuristics for search problems such as (k 2 −1)-puzzles, Rubik's Cube, etc. <ref type="bibr" target="#b2">(Culberson &amp; Schaeffer 1998;</ref><ref type="bibr" target="#b15">Hernadvölgyi &amp; Holte 1999;</ref><ref type="bibr" target="#b7">Felner, Korf, &amp; Hanan 2004</ref>), PDB heuristics have been successfully exploited in domain-independent planning <ref type="bibr" target="#b5">(Edelkamp 2001;</ref><ref type="bibr" target="#b6">2002;</ref><ref type="bibr" target="#b9">Haslum et al. 2007)</ref>.</p><p>Apart from the need to automatically select good projections, the two limitations of PDB heuristics are the size of the abstract space and its dimensionality. First, the number of abstract states should be small enough to allow reachability analysis in S α by exhaustive search, and thus we must have |S α | = O(1). The bound on |S α | is typically set explicitly given the time and memory limitations of the system. Second, since PDB abstractions are projections, the explicit constraint on |S α | implies a fixed-dimensionality constraint on the abstract space, that is, |V α | = O(1). In problems with, informally, many alternative resources, this limitation is a pitfall. For instance, suppose {Π i } is a sequence of Logistics problems of growing size with |V i | = i. If each package in Π i can be transported by some Θ(i) vehicles, then starting from some j, h α will not account at all for movements of vehicles essential for solving Π j <ref type="bibr" target="#b12">(Helmert &amp; Mattmüller 2007)</ref>.</p><p>Note that, while the first limitation of projection abstractions can be overcome to a certain extent with additive ensembles of numerous projections, the dimensionality limitation remains an issue in additive PDBs as well. Within these limitations, however, a very attractive side of PDB abstractions is the complexity of their natural h α -partition. Instead of computing h α (s) = h * (α(s)) from scratch for each evaluated state s (which would not be practical for all but tiny projections), the practice is to pre-compute and store h * (α(s)) for all abstract states α(s) ∈ S α , and then the pernode computation of h α (s) boils down to hash-table lookup with a perfect hash function. In our terms, the time and space complexity of that PDB h α -partition for a set of k states is</p><formula xml:id="formula_1">O (|S α | (log (|S α |) + |A|) + k) and O(|S α |), respectively.</formula><p>This is precisely what makes PDB heuristics so attractive in practice.</p><p>Aiming at preserving the attractiveness of the PDB h α -partition while eliminating the bottleneck of fixed dimensionality, Helmert, Haslum, and <ref type="bibr" target="#b13">Hoffmann (2007)</ref> generalize the methodology of <ref type="bibr" target="#b3">Dräger et al. (2006)</ref> and introduce what we call merge-and-shrink (MS) planning abstractions. MS abstractions are homomorphisms that generalize PDB abstractions by allowing for more flexibility in selection of pairs of state to be contracted. The problem's state space is viewed as the synchronized product of its projections onto the single state variables. This product can be computed by iteratively composing two abstract spaces, replacing them with their product. While in a PDB the size of the abstract space S α is controlled by limiting the number of product compositions, in MS abstractions it is controlled by interleaving the iterative composition of projections with abstraction of the partial composites.</p><p>The accuracy of the MS heuristics crucially depends on the order in which composites are formed and the choice of abstract states to contract. This flexibility in the MS abstraction strategy also adds variability to the complexity of its natural h α -partition. The time and space complexity for the linear abstraction strategy of Helmert et al. are respectively</p><formula xml:id="formula_2">O (|V ||S α | (log (|S α |) + |A|) + k · |V |) and O(|S α |).</formula><p>Similarly to PDB abstractions, the per-node computation of h α (s) with an MS abstraction α is just a lookup in a data structure storing h * (α(s)) for all abstract states α(s) ∈ S α . Hence, while the pre-search computation with MS abstractions can be more costly than with PDBs, the online part of computing heuristic values is still extremely efficient. This per-node efficiency allows Helmert et al. to demonstrate impressive practical effectiveness of MS abstractions on several IPC domains <ref type="bibr" target="#b13">(Helmert, Haslum, &amp; Hoffmann 2007)</ref>.</p><p>Merge-and-shrink abstractions escape the fixeddimensionality constraint of PDBs, but not the constraint on the abstract space to be of a fixed size. In attempt to eliminate both these constraints of "explicit" abstractions, <ref type="bibr" target="#b18">Katz and Domshlak (2008b)</ref> introduce a framework of structural-pattern abstractions. A structural-pattern abstraction α maps the problem in hand to an abstract problem from a tractable fragment of optimal planning. This way, the dimensionality of S α is not limited even by |V |, and the size of the abstract space is not limited even by the size of the original space. <ref type="bibr">2</ref> In particular, Katz and Domshlak (2008b) introduce a concrete family of structural-pattern abstraction, fork-decomposition, corresponding to two classes of tractable optimal planning. A digraph forms fork/inverted-fork if it is connected, loop-less, and have all its arcs outgoing/incoming from/to a single node (called the root node of the graph). Propositions 3-4 of <ref type="bibr" target="#b18">Katz and Domshlak (2008b)</ref> state that an optimal plan for a planning task Π can be found in polynomial time if 1. the causal graph of Π forms a fork, and the root variable is binary-valued, or 2. the causal graph of Π forms an inverted fork 3 , and the domain of the root variable is fixed. Now, given a general planning task Π, for each variable v i ∈ V , let V f i ⊆ V contain v i and all its immediate successors in CG(Π), and V i i ⊆ V contain v i and all its immediate predecessors in CG(Π). At a high-level summary, the forkdecomposition of Π with |V | = n is obtained by (1) schematically constructing a set of projection abstractions</p><formula xml:id="formula_3">{Π f i , Π i i } n i=1 , (2) reformulating the actions of {Π f i , Π i i } n i=1</formula><p>to single-effect actions so that the causal graphs of Π f i and Π i i become respectively forks and inverted forks rooted in v i , and (3) within each Π f i , abstracting the domain of v i to {0, 1}, and within each Π i i , abstracting the domain of v i to {0, 1, . . . , b} with b = O(1).</p><formula xml:id="formula_4">The problems {Π f i , Π i i } n i=1</formula><p>correspond to distanceconservative (though, after step (2), not necessarily homomorphic) abstractions of Π, and, similarly to PDB and MS abstractions, can be used either separately or as parts of heuristic ensembles.</p><p>To see that the fork-decomposition escapes both limitations of the projection abstractions, note that both |V f i | and |V i i | can be Θ(|V |), and thus each induced |S α | can be Θ(|S|). The latter is a blessing, but a mixed one, and both sides of the story are demonstrated in <ref type="table">Table 1</ref>, showing the performance of four different planners on the planning tasks from the Logistics domain of IPC-2000. The first planner, h F , is A * equipped with the additive fork-decomposition heuristic "all forks" under uniform action cost partitioning <ref type="bibr" target="#b18">(Katz &amp; Domshlak 2008b)</ref>. It is compared to (1) MS-10 5 -Helmert et al.'s A * with a merge-and-shrink abstrac-2 While dimensionality larger than |V |, and abstract-space size larger than |S| may appear excessive, see <ref type="bibr" target="#b17">(Katz &amp; Domshlak 2008a</ref>) for concrete examples of why this can be useful.</p><p>3 Proposition 4 of <ref type="bibr" target="#b18">Katz and Domshlak (2008b)</ref> also requires the problem to be what is called 1-dependent, but this requirement was later found unnecessary.  <ref type="formula">(3)</ref> Gamer planner. MS-10 5 represents the stateof-the-art in optimal planning with abstraction heuristics, while Gamer and HSP * F were the two best-performing costoptimal planners at IPC-2008.</p><p>As it appears from <ref type="table">Table 1</ref>, at least on the Logistics domain, the abstraction-based heuristic search favorably competes with the leading optimal-search planners: Both h F and MS-10 5 outperform HSP * F and Gamer on these planning tasks. The comparison between h F and MS-10 5 , however, is more important to us here. On the one hand, h F almost consistently expands less search nodes than MS-10 5 , with the difference hitting four orders of magnitude. On the other hand, the time complexity of h F per search node is substantially higher than this of MS-10 5 , with the two expanding (approximately) 4 and 100000 nodes per second, respectively. The outcome is simple: while with no time limits (and only memory limit of 1.5 GB) h F solves more tasks than MS-10 5 , the inverse holds with a time limit of one hour (see tasks 11-1 and 12-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural-Pattern Databases</head><p>Empirical (in)effectiveness of the fork-decomposition heuristics was not surprising, yet it was disappointing to see an informative heuristic being out of reach of practical planning. Fortunately, this is not the end of the story. We now show that the time-per-node complexity bottleneck of fork-decomposition heuristics can be successfully overcome. Specifically, we show that an equivalent of PDB's and merge-and-shrink notion of "database" exists for fork-decomposition abstractions as well, despite of their exponential-size abstract spaces. Of course, unlike with PDB and merge-and-shrink abstractions, structuralpattern databases (and in particular these for the forkdecomposition's fork and inverted fork abstractions) do not (and cannot) result in purely lookup computations of h α (s). The online part of the h α -partition has to be non-trivial in the sense that its complexity cannot be O(1).</p><p>In the remainder of this section we prove existence of such effective h-partitions for both fork and inverted fork abstractions. In the next section we empirically show that these hpartitions lead to fast pre-search and per-node computations that allow for successfully exploiting the informativeness of the fork-decomposition heuristics in practice.</p><p>Theorem 1 Let Π = V, A, I, G, c be a planning task with a fork causal graph rooted in a binary-valued variable r. There exists an h * -partition for Π such that, for any set of k states, the time and space complexity of that h * -partition is, respectively, For each v ∈ V \ {r}, let DTG 0 v and DTG 1 v be the subgraphs of DTG(v, Π) obtained by removing from the latter all the arcs labeled with 1 and 0, respectively. </p><formula xml:id="formula_5">O(d 3 |V | + |A r | + k · d|V |) and O(d 2 |V |), where d = max v D(v).</formula><formula xml:id="formula_6">DTG 0 v /DTG 1 v if i is odd/even. For each ϑ ∈ L i−1 , ϑ ∈ L i , L v (σ) contains an arc (ϑ, ϑ ) weighted with the dis- tance from ϑ to ϑ in DTG 0 v /DTG 1 v if i is odd/even. 3. Set R = ∅. For each σ ∈ * [σ(r)], a plan ρ σ for Π is constructed as follows. i For each v ∈ V \ {r}, find a shortest path from s[v] to G[v] in L v (σ).</formula><p>If no such path exists, go to the next σ ∈ * [σ(r)]. The i-th edge on this path (say, from ϑ ∈ L i−1 to ϑ ∈ L i ) corresponds to the shortest path from ϑ to ϑ in either DTG 0 v or DTG 1 v , and we denote this path by</p><formula xml:id="formula_7">S i ϑ . ii Set R = R∪{ρ σ }, ρ σ = S 1 ·a σ[2] ·S 2 ·. . .·a σ[m] ·S m ,</formula><p>where m = |σ|, S i is obtained by an arbitrary merge of {S i v } v∈V \{r} , and a x is the action that changes the value of r to x. 4. If R = ∅, then "unsolvable". Otherwise, return ρ = argmin ρσ∈R c(ρ σ ).</p><p>This algorithm can be formulated as follows.</p><p>(1) For each ϑ r ∈ D(r), v ∈ V \ {r}, and ϑ, ϑ ∈ D(v), let p ϑ,ϑ ;ϑr be the cost of the cheapest sequence of actions ϑ to ϑ provided r = ϑ r . The whole set {p ϑ,ϑ ;ϑr } can be computed by a straightforward variant of the allpairs-shortest-paths, Floyd-Warshall algorithm in time</p><formula xml:id="formula_8">O(d 3 |V |).</formula><p>(2) For each v ∈ V \ {r}, 1 ≤ i ≤ d + 1, and ϑ ∈ D(v), let g ϑ;i be the cost of the cheapest sequence of actions changing s[v] to ϑ provided a sequence σ ∈ * [σ(r)], |σ| = i, of value changes of r. Given {p ϑ,ϑ ;ϑr }, the set {g ϑ;i } is given by the solution of the recursive equation</p><formula xml:id="formula_9">g ϑ;i =            p s[v],ϑ;s[r] , i = 1 min ϑ g ϑ ;i−1 + p ϑ ,ϑ;s[r] , 1 &lt; i ≤ δ ϑ , i is odd min ϑ g ϑ ;i−1 + p ϑ ,ϑ;¬s[r] , 1 &lt; i ≤ δ ϑ , i is even g ϑ;i−1 , δ ϑ &lt; i ≤ d + 1</formula><p>, where</p><formula xml:id="formula_10">δ ϑ = |D(v)| + 1 if ϑ ∈ D(v). Given that, h * (s) = min σ∈ * [σ(r)] [ c(σ) + v∈V \{r} g G[v];|σ| ].</formula><p>Notice that step (1) is already state-independent, but the heavy step <ref type="formula" target="#formula_12">(2)</ref> is not. However, the state dependence of step (2) can mostly be overcome. For each v ∈ V \ {r}, ϑ ∈ D(v), 1 ≤ i ≤ d + 1, and ϑ r ∈ D(r), let g ϑ;i (ϑ r ) be the cost of the cheapest sequence of actions changing ϑ to G <ref type="bibr">[v]</ref> provided the value changes of r induce a 0/1 sequence of length i starting with ϑ r . Very similarly to before, the set {g ϑ;i (ϑ r )} is given by the solution of the recursive equation</p><formula xml:id="formula_11">g ϑ;i (ϑ r ) =      p ϑ,G[v];ϑr , i = 1 min ϑ g ϑ ;i−1 (¬ϑ r ) + p ϑ,ϑ ;ϑr , 1 &lt; i ≤ δ ϑ g ϑ;i−1 (ϑ r ), δ ϑ &lt; i ≤ d + 1 ,</formula><p>(1) which can be solved in time O(d 3 |V |). Note that this equation is now independent of the evaluated state s, and yet {g ϑ;i (ϑ r )} allow for computing h * (s) for a given state s via</p><formula xml:id="formula_12">h * (s) = min σ∈ * [σ(r)] [ c(σ) + v∈V \{r} g s[v];|σ| (s[r]) ]<label>(2)</label></formula><p>With the new formulation, the only computation that has to be performed per search node is this of the final minimization over * [σ(r)], which is the lightest part of the whole algorithm anyway. The major computation, notably this of {p ϑ,ϑ ;ϑr } and {g ϑ;i (ϑ r )}, can now be performed offline, and shared between the evaluated states. The space required to store this information is O(  <ref type="table">0  201  200  101  100  1  0  2  24  100  2  1  0  201  200  101  100  1  0   3  48  100  2  1  0  53  102  3  2  1  0  4  72  100  2  1  0  53</ref>  Theorem 2 Let Π = V, A, I, G, c be a planning task with an inverted fork causal graph rooted in r with |D(r)| = b = O(1). There exists an h * -partition for Π such that, for any set of k states, the time and space complexity of that</p><formula xml:id="formula_13">h * -partition is O(b|V ||A r | b−1 + d 3 |V | + k · |V ||A r | b−1 ) and O(|V ||A r | b−1 + d 2 |V |), respectively, where d = max v D(v).</formula><p>Similarly to the proof of Theorem 1, the proof of Theorem 2 is also based on a modification of the poly-time algorithm of <ref type="bibr" target="#b18">Katz and Domshlak (2008b)</ref> for computing h * (s) for state s of a task Π as in the theorem. The latter algorithm finds h * (s) as follows.</p><p>1. For each v ∈ V \ {r}, and each ϑ, ϑ ∈ D(v), compute shortest (i.e., cost-minimal) paths from ϑ to ϑ in DTG(v, Π).</p><p>2. Enumerate all Θ(|A r | b−1 ) cycle-free paths from s[r] to G <ref type="bibr">[r]</ref> in DTG(r, Π). For each such path, construct a plan for Π based on that path for r, and the shortest paths computed in (1). The cheapest such plan is a cost-optimal plan from s in Π, and thus induces h * (s).</p><p>This algorithm can be formulated as follows.</p><p>(1) For each v ∈ V \ {r}, and ϑ, ϑ ∈ D(v), let p ϑ,ϑ be the cost of the cheapest sequence of actions changing ϑ to ϑ . The whole set {p ϑ,ϑ } can be computed using the Floyd-Warshall algorithm in time</p><formula xml:id="formula_14">O(d 3 |V |). (2) For each cycle-free path π = a 1 · . . . · a m from s[r]</formula><p>to G <ref type="bibr">[r]</ref> in DTG(v, Π), let g π be the cost of the cheapest plan from s in Π based on π, and the shortest paths computed in (1). Each g π can be computed as</p><formula xml:id="formula_15">g π = m i=1 c(a i ) + m i=0 v∈V \{r} p pre i [v],pre i+1 [v] ,</formula><p>where pre 0 · . . . · pre m+1 are the values needed from the parents of r along the path π. That is, for each v ∈ V \ {r}, and 0 ≤ i ≤ m + 1, <ref type="bibr">and G[v]</ref> is specified pre(ai) <ref type="bibr">[v]</ref>, 1 ≤ i ≤ m, and pre(ai) <ref type="bibr">[v]</ref> is specified pre i−1 <ref type="bibr">[v]</ref> otherwise</p><formula xml:id="formula_16">pre i [v] = 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : s[v], i = 0 G[v], i = m + 1,</formula><p>From that, we have h * (s) = min π g π .</p><p>Notice that step (1) is state-independent, but step <ref type="formula" target="#formula_12">(2)</ref> is not so. However, the dependence of step (2) on the evaluated state can be substantially relaxed. As there are only O(1) different values of r, it is possible to consider cyclefree paths to G <ref type="bibr">[r]</ref> from all values of r. For each such path π, and each parent variable v ∈ V \ {r}, we know what would be the first value of v required by π. Given that, we can pre-compute the cost-optimal plans induced by each π and assuming the parents start at their first needed values. The remainder of the computation of h * (s) is delegated to online, and the modified step (2) is as follows.</p><p>For each ϑ r ∈ D(r) and each cycle-free path π = a 1 ·. . .· a m from ϑ r to G <ref type="bibr">[r]</ref> in DTG(v, Π), let a "proxy" state s π be <ref type="table">ipc4  20  20  100  17  85  17  85  16  80  16  80  15  75  11  55  17  85  20  100  blocks-ipc2  30  21  70  18  60  18  60  18  60  20</ref>   <ref type="table">-ipc1  2  2  100  1  50  1  50  2  100  2  100  0  0  2  100  1  50  2  100  gripper-ipc1  20  7  35  7  35  7  35  7  35  7  35  6  30  20  100  7  35  7  35  logistics-ipc1  7  6  86  4  57  5  71  4  57  5  71  3  43  6  86  2  29  2  29  logistics-ipc2  22  22  100  16  73Table 2</ref>: A summary of the experimental results. Per domain, S denotes the number of tasks solved by any planner. Per planner/domain, the number of tasks solved by that planner is given both by the absolute number (s) and by the percentage from "solved by any" (%S). Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the number of solved instances in total (s), the domain-normalized measure of solved instances in total (ˆ s), and the number of domains in which the planners achieved superior performance (w).</p><formula xml:id="formula_17">sπ[v] = 8 &gt; &lt; &gt; : ϑr, v = r G[v], ∀1 ≤ i ≤ m : pre(ai)[v] is unspecified pre(ai)[v], i = argmin j {pre(aj)[v] is specified} , domain (D) S(D) h F h I h FI MS-10 4 MS-10 5 HSP * F Gamer blind hmax s %S s %S s %S s %S s %S s %S s %S s %S s %S airport-</formula><p>that is, the non-trivial part of s π captures the first values of V \ {r} needed along π. <ref type="bibr">4</ref> Given that, let g π be the cost of the cheapest plan from s π in Π based on π, and the shortest paths {p ϑ,ϑ } computed in (1). Each g π can be computed as</p><formula xml:id="formula_18">g π = m i=1   c(a i ) + v∈V \{r} p pre i [v],pre i+1 [v]   ,</formula><p>where, for each v ∈ V \ {r}, and 1 ≤ i ≤ m + 1, <ref type="bibr">and G[v]</ref> is specified pre(ai) <ref type="bibr">[v]</ref>, 2 ≤ i ≤ m, and pre(ai) <ref type="bibr">[v]</ref> is specified pre i−1 <ref type="bibr">[v]</ref> otherwise Storing the pairs (g π , s π ) accomplishes the offline part of the computation. Now, given a search state s, we can compute</p><formula xml:id="formula_19">pre i [v] = 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : sπ[v], i = 1 G[v], i = m + 1,</formula><formula xml:id="formula_20">h * (s) = min π:sπ[r]=s[r]   g π + v∈V \{r} p s[v],sπ[v]   .</formula><p>The dominant offline computation is computing {g π }. and |S α | &lt; 10 5 . These four (baseline and MS) heuristics were implemented by  within the same planning system as our fork-decomposition heuristics, allowing for a fairly unbiased comparison. We also compare to the <ref type="bibr">Gamer (Edelkamp &amp; Kissmann 2009)</ref> and HSP * F (Haslum 2008) planners that were respectively the winner and the runner-up at the sequential optimization track of IPC-2008. On the algorithmic side, Gamer is based on a bidirectional blind search using sophisticated symbolic-search techniques, and HSP * F uses A * with an additive critical-path heuristic.</p><p>The summary of our experimental results is shown in <ref type="table">Table 2</ref>. The experiments were conducted on 3GHz Intel E8400 CPU with 2 GB memory, using a 1.5 GB memory limit and 30 minute timeout. The only exception from that was Gamer for which we used similar machines but with 4 GB memory, and 2 GB memory limit; this was done to provide Gamer with the environment for which it was configured. For each domain D, S(D) is the number of tasks in D that were solved by at least one planner in the suite. For each domain D and planner p, s(D, p) (in columns s) is the number of tasks in D solved by p. Columns %S provide the same information in terms of percentage from "solved by any". Boldfaced results indicate the best performance within the corresponding domain. The last three rows summarize the performance of the planners via three measures. First, s(p) is the total number of tasks solved in all the 23 domains; this is basically the measure used in planner evaluation in IPC-2008. As domains are not equally challenging, and do not provide the same discrimination between the planners' performance, the second, "domain-normalized" measure of solved instances in totaî s(p) = D s(D, p)/S(D). Finally, w(p) is the number of domains in which p achieved superior performance.</p><p>Overall, <ref type="table">Table 2</ref> clearly suggests that heuristic search with "databased" fork-decomposition heuristics favorably competes with the state-of-the-art of optimal planning. In particular, A * with the fork-decomposition heuristic h F exhibited the best overall performance according to all three measures. The overall performance of A * with the other two heuristics h I and h FI was not as good as with h F , and yet, in terms of, e.g., absolute number of solved instances, h FI and h I came second and forth, respectively. Likewise, h I still slightly outperforms h F on the Rovers and Trucks domains, so no clear-cut dominance between them.</p><p>Finally, the contribution of employing h-partitions to the success of the structural-pattern heuristics was dramatic. Looking back on the results for Logistics-ipc2 in <ref type="table">Table 1</ref>, note that the timeout of 30 minutes would prevent us from solving instances 11-1 and 12-1, ending up with only 20 solved instances. In contrast, with structuralpattern databases we manage to solve these two instances within the time limit, and the difference in running time is spectacular: While without employing h α -partition, solving instance 12-1 took us more than 7 hours, with structuralpattern databases we solved all the 22 instances from <ref type="table">Table 1</ref> together in less than 35 seconds. This difference in runtimes is representative for all the domains in our evaluation, and thus h-partitions play a key role in bringing structural patterns to the state of the art in cost-optimal planning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>The domain transition graph DTG(v, Π) of v ∈ V is an arc-labeled digraph over the nodes D(v) such that an arc (ϑ, ϑ ) labeled with a ∈ A belongs to DTG(v, Π) iff both eff(a)[v] = ϑ and either pre(a)[v] = ϑ or pre(a)[v] is unspecified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The proof of Theorem 1 is based on a modification of the poly-time algorithm of Katz and Domshlak (2008b) for computing h * (s) for state s of such a task Π. Let D(r) = {0, 1}, where s[r] = 0; for ϑ ∈ D(r), let ¬ϑ denote the opposite value 1 − ϑ. Let σ(r) be a 0/1 sequence of length 1 + d, such that, for 1 ≤ i ≤ |σ(r)|, σ(r)[i] = 0/1 if i is odd/even. Let * [σ(r)] be the set of all non-empty pre- fixes of σ(r) if G[r] is unspecified, and the set of all non- empty prefixes of σ(r) ending with G[r], otherwise. For each σ ∈ * [σ(r)], c(σ) denotes the (straightforwardly computable) cost of achieving the respective sequence of value changes of r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>For each v ∈ V \{r}, and each ϑ, ϑ ∈ D(v), compute the shortest (i.e., cost-minimal) paths from ϑ to ϑ in DTG 0 v and DTG 1 v . 2. For each σ ∈ * [σ(r)], and each v ∈ V \ {r}, build a layered digraph L v (σ) with |σ| + 1 layers L 0 , . . . , L |σ| , where L 0 consists of only s[v], and for 1 ≤ i ≤ |σ|, L i consists of all nodes reachable from the nodes L i−1 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>d 2 |V |) as it contains only a fixed amount of information per each pair of values of a same variable. The time complexity of the offline compu- tation is O(d 3 |V | + |A r |); the |A r | component stems from pre-computing the pair of values w 0 and w 1 . The time com- plexity of the online computation per state is O(d|V |); |V |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structural-pattern database for a fork-structured problem with a binary-valued root variable r and two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5. (a) depicts the domain transition graphs of v (top) and u (bottom); the numbers above and below each edge are the precondition on r and the cost of the respective action. (b) depicts the database created by the algorithm. For instance, the entry in row r = 0 ∧ |σ| = 5 and column v = 0 captures the value of g v=0;5 (r = 0) computed as in Eq. 1. The shaded entries are these examined at the online computation of h * (r = 0, v = 0, u = 0). comes from the internal summation and d comes from the size of * [σ(r)]. Figure 1(b) shows the structural-pattern database created for a fork-structured problem with a binary-valued root r, two children v and u, and G[r] = 0, G[v] = 3, and G[u] = 5; the domain transition graphs of v and u are depicted in Figure 1(a). Online computation of h * (s) as in Eq. 2 for s = (r = 0, v = 0, u = 0) sums over the shaded entries of each of the four rows having such entries, and minimizes over the resulting four sums, with the minimum being obtained in the row r = 0 ∧ |σ| = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The number of cycle-free paths to G[r] in DTG(v, Π) is Θ(|A r | b−1 ), and computing g π for each such path π can be done in time O(b|V |). Hence, the overall offline time complexity is O(b|V ||A r | b−1 ), and space complexity (in- cluding the storage of the proxy states s π ) is O(|V ||A r | b−1 ). The time complexity of the online computation per state is O(|V ||A r | b−1 ); |V | comes from the internal summation and |A r | b−1 from the upper bound on the number of cycle-free paths from s[r] to G[r].</figDesc></figure>

			<note place="foot" n="1"> This does not necessarily apply to symbolic PDBs that, on some tasks, may provide exponential reduction of the PDB&apos;s representation (Edelkamp 2002).</note>

			<note place="foot" n="4"> For ease of presentation, we omit here the case where v is needed neither along π, nor by the goal; such variables should be simply ignored when accounting for the cost of π.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Evaluation</head><p>To evaluate the practical attractiveness of structural-pattern heuristics in general, and the corresponding h-partitions in particular, we have conducted an empirical study on a wide sample of planning domains from the international planning competitions <ref type="bibr">[1998]</ref><ref type="bibr">[1999]</ref><ref type="bibr">[2000]</ref><ref type="bibr">[2001]</ref><ref type="bibr">[2002]</ref><ref type="bibr">[2003]</ref><ref type="bibr">[2004]</ref><ref type="bibr">[2005]</ref><ref type="bibr">[2006]</ref>. The selection of the domains was aimed at allowing a comparative evaluation with other baseline and state of the art approaches/planners that, at the moment, not all support all PDDL features.</p><p>We implemented three additive fork-decomposition heuristics within a standard heuristic forward search framework of the Fast Downward planner <ref type="bibr" target="#b14">(Helmert 2006</ref>), using the A * algorithm with full duplicate elimination. The three heuristics were these outlined by <ref type="bibr">Katz and Domsh- lak (2008b)</ref>. The h F heuristic corresponds to the ensemble of all (not clearly redundant) fork subgraphs of the causal graph, with the domains of the roots being abstracted using the "leave-one-value-out" binary-valued domain decompositions. The h I heuristic is the same but for the inverted fork subgraphs, with the domains of the roots being abstracted using the "distance-to-goal-value" ternary-valued domain decompositions. The ensemble of the h FI heuristic is the union of these for h F and h I . The action cost partitioning for all three ensembles was set to "uniform".</p><p>We compare with two baseline approaches, namely blind search (A * with a heuristic function which is 0 for goal states and 1 otherwise) and A * with the h max heuristic <ref type="bibr" target="#b1">(Bonet &amp; Geffner 2001)</ref>, as well as with state of the art abstraction heuristics, represented by the merge-and-shrink abstractions of <ref type="bibr" target="#b13">Helmert, Haslum, and Hoffmann (2007)</ref>. The latter were constructed under the linear, f -preserving abstraction strategy suggested by these authors, and this under two fixed bounds on the size of the abstraction, |S α | &lt; 10 4</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Complexity results for SAS + planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bäckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Intell</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="655" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Planning as heuristic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIJ</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="33" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pattern databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Culberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="318" to="334" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directed model checking with distance-preserving abstractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dräger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Finkbeiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Podelski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIN</title>
		<imprint>
			<date type="published" when="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal symbolic planning with action costs and preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edelkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kissmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Planning with pattern databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edelkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECP</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Symbolic pattern databases in heuristic search planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edelkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="274" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Additive pattern database heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Korf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="279" to="318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Admissible heuristics for optimal planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPS</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-independent construction of pattern database heuristics for cost-optimal planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Botea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1007" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New admissible heuristics for domain-independent planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Geffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1163" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additive and reversed relaxed reachability heuristics revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haslum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accuracy of admissible heuristic functions in selected planning domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mattmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="938" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flexible abstraction heuristics for optimal sequential planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Fast Downward planning system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="191" to="246" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PSVN: A vector representation for production systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hernadvölgyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holte</surname></persName>
		</author>
		<idno>1999-07</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Ottawa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cost-optimal planning with landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Karpas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal additive composition of abstraction-based admissible heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structural patterns heuristics via fork decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domshlak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Landmarks revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Westphal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
