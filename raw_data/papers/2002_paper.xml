<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
							<email>nlao@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
							<email>tom.mitchell@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL&apos;s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although there is a great deal of recent research on extracting knowledge from text <ref type="bibr" target="#b0">(Agichtein and Gravano, 2000;</ref><ref type="bibr" target="#b8">Etzioni et al., 2005;</ref><ref type="bibr" target="#b18">Snow et al., 2006</ref>; <ref type="bibr" target="#b14">Pantel and Pennacchiotti, 2006;</ref><ref type="bibr" target="#b20">Yates et al., 2007)</ref>, much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge.</p><p>In particular, traditional logical inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods ( <ref type="bibr">Richardson and Domingos, 2006</ref>) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large KB's, and that are robust to imperfect knowledge. The KB we consider is a large triple store, which can be represented as a labeled, directed graph in which each entity x is a node, each binary relation R(x, y) is an edge labeled R between x and y, and unary concepts C(x) are represented as an edge labeled "isa" between the node for the entity x and a node for the concept C. We present a trainable inference method that learns to infer relations by combining the results of different random walks through this graph, and show that the method achieves good scaling properties and robust inference in a KB containing over 500,000 triples extracted from the web by the NELL system ( <ref type="bibr" target="#b6">Carlson et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The NELL Case Study</head><p>To evaluate our approach experimentally, we study it in the context of the NELL (Never Ending Language Learning) research project, which is an effort to develop a never-ending learning system that operates 24 hours per day, for years, to continuously improve its ability to read (extract structured facts from) the web ( <ref type="bibr" target="#b6">Carlson et al., 2010)</ref>. NELL began operation in January 2010. As of March 2011, NELL has built a knowledge base containing several million candidate beliefs which it has extracted from the web with varying confidence. Among these, NELL has fairly high confidence in approximately half a million, which we refer to as NELL's (confident) beliefs. NELL has lower confidence in a few million others, which we refer to as its candidate beliefs.</p><p>NELL is given as input an ontology that defines hundreds of categories (e.g., person, beverage, athlete, sport) and two-place typed relations among these categories (e.g., atheletePlaysSport(athlete, sport)), which it must learn to extract from the web. It is also provided a set of 10 to 20 positive seed examples of each such category and relation, along with a downloaded collection of 500 million web pages from the ClueWeb2009 corpus <ref type="bibr" target="#b5">(Callan and Hoy, 2009)</ref> as unlabeled data, and access to 100,000 queries each day to Google's search engine. Each day, NELL has two tasks: (1) to extract additional beliefs from the web to populate its growing knowledge base (KB) with instances of the categories and relations in its ontology, and (2) to learn to perform task 1 better today than it could yesterday. We can measure its learning competence by allowing it to consider the same text documents today as it did yesterday, and recording whether it extracts more beliefs, more accurately today than it did yesterday.</p><p>NELL uses a large-scale semi-supervised multitask learning algorithm that couples the training of over 1500 different classifiers and extraction methods (see <ref type="bibr" target="#b6">(Carlson et al., 2010)</ref>). Although many of the details of NELL's learning method are not central to this paper, two points should be noted. First, NELL is a multistrategy learning system, with components that learn from different "views" of the data <ref type="bibr" target="#b3">(Blum and Mitchell, 1998)</ref>: for instance, one view uses orthographic features of a potential entity name (like "contains capitalized words"), and another uses free-text contexts in which the noun phrase is found (e.g., "X frequently follows the bigram 'mayor of' "). Second, NELL is a bootstrapping system, which self-trains on its growing collection of confident beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Knowledge Base Inference: Horn Clauses</head><p>Although NELL has now grown a sizable knowledge base, its ability to perform inference over this knowledge base is currently very limited. At present its only inference method beyond simple inheritance involves applying first order Horn clause rules to infer new beliefs from current beliefs. For example, it may use a Horn clause such as</p><formula xml:id="formula_0">AthletePlaysForTeam(x, y) (1) ∧ TeamPlaysInLeague(y, z) ⇒ AthletePlaysInLeague(x,z)</formula><p>to infer that AthletePlaysInLeague(HinesWard,NFL), if it has already extracted the beliefs in the preconditions of the rule, with variables x, y and z bound to HinesWard, PittsburghSteelers and NFL respectively as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. NELL currently has a set of approximately 600 such rules, which it has learned by data mining its knowledge base of beliefs. Each learned rule carries a conditional probability that its conclusion will hold, given that its preconditions are satisfied.</p><p>NELL learns these Horn clause rules using a variant of the FOIL algorithm <ref type="bibr" target="#b15">(Quinlan and Cameron-Jones, 1993)</ref>, henceforth N-FOIL. N-FOIL takes as input a set of positive and negative examples of a rule's consequent (e.g., +AthletePlaysInLeague(HinesWard,NFL), −AthletePlaysInLeague(HinesWard,NBA)), and uses a "separate-and-conquer" strategy to learn a set of Horn clauses that fit the data well. Each Horn clause is learned by starting with a general rule and progressively specializing it, so that it still covers many positive examples but covers few negative examples. After a clause is learned, the examples covered by that clause are removed from the training set, and the process repeats until no positive examples remain.</p><p>Learning first-order Horn clauses is computationally expensive-not only is the search space large, but some Horn clauses can be costly to evaluate <ref type="bibr" target="#b7">(Cohen and Page, 1995)</ref>. N-FOIL uses two tricks to improve its scalability. First, it assumes that the consequent predicate is functional-e.g., that each Athlete plays in at most one League. This means that explicit negative examples need not be provided ( <ref type="bibr" target="#b21">Zelle et al., 1995</ref>): e.g., if AthletePlaysInLeague(HinesWard,NFL) is a positive example, then AthletePlaysInLeague(HinesWard,z ) for any other value of z is negative. In general, this constraint guides the search algorithm toward Horn clauses that have fewer possible instantiations, and hence are less expensive to match. Second, N-FOIL uses "relational pathfinding" ( <ref type="bibr" target="#b16">Richards and Mooney, 1992</ref>) to produce general rules-i.e., the starting point for a clause of the form R(x, y) is found by looking at positive instances R(a, b) of the consequent, and finding a clause that corresponds to a bounded-length path of binary relations that link a to b. In the example above, a start clause might be the clause (1). As in FOIL, the clause is then (potentially) specialized by greedily adding additional conditions (like ProfessionalAthlete(x)) or by replacing variables with constants (eg, replacing z with NFL).</p><p>For each N-FOIL rule, an estimated conditional probabilityˆPprobabilityˆ probabilityˆP (conclusion|preconditions) is calculated using a Dirichlet prior according tô</p><formula xml:id="formula_1">tô P = (N + + m * prior)/(N + + N − + m) (2)</formula><p>where N + is the number of positive instances matched by this rule in the FOIL training data, N − is the number of negative instances matched, m = 5 and prior = 0.5. As the results below show, N-FOIL generally learns a small number of high-precision inference rules. One important role of these inference rules is that they contribute to the bootstrapping procedure, as inferences made by N-FOIL increase either the number of candidate beliefs, or (if the inference is already a candidate) improve NELL's confidence in candidate beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Knowledge Base Inference: Graph Random Walks</head><p>In this paper, we consider an alternative approach, based on the Path Ranking Algorithm (PRA) of Lao and Cohen (2010b), described in detail below. PRA learns to rank graph nodes y relative to a query node x. PRA begins by enumerating a large set of bounded-length edge-labeled path types, similar to the initial clauses used in NELL's variant of FOIL. These path types are treated as ranking "experts", each performing a random walk through the graph, constrained to follow that sequence of edge types, and ranking nodes y by their weights in the resulting distribution. Finally PRA combines these "experts" using logistic regression.</p><p>As an example, consider a path from x to y via the sequence of edge types isa, isa −1 (the inverse of isa), and AthletePlaysInLeague, which corresponds to the Horn clause</p><formula xml:id="formula_2">isa(x, c) ∧ isa −1 (c, x ) (3) ∧ AthletePlaysInLeague(x , y) ⇒ AthletePlaysInLeague(x, y)</formula><p>Suppose a random walk starts at a query node x (say x=HinesWard). If HinesWard is linked to the single concept node ProfessionalAthlete via isa, the walk will reach that node with probability 1 after one step. If A is the set of ProfessionalAthlete's in the KB, then after two steps, the walk will have probability 1/|A| of being at any x ∈ A. If L is the set of athletic leagues and ∈ L, let A be the set of athletes in league : after three steps, the walk will have probability |A |/|A| of being at any point y ∈ L. In short, the ranking associated with this path gives the prior probability of a value y being an athletic league for x-which is useful as a feature in a combined ranking method, although not by itself a high-precision inference rule.</p><p>Note that the rankings produced by this "expert" will change as the knowledge base evolves-for instance, if the system learns about proportionally more soccer players than hockey players over time, then the league rankings for the path of clause <ref type="formula">(3)</ref> will change. Also, the ranking is specific to the query node x. For instance, suppose the KB contains facts which reflect the ambiguity of the team name "Giants" 1 as in <ref type="figure" target="#fig_0">Figure 1</ref>. Then the path for clause (1) above will give lower weight to y = NFL for x = EliManning than to y = NFL for x = HinesWard.</p><p>The main contribution of this paper is to introduce and evaluate PRA as an algorithm for making probabilistic inference in large KBs. Compared to Horn clause inference, the key characteristics of this new inference method are as follows:</p><p>• The evidence in support of inferring a relation instance R(a, b) is based on many existing paths between a and b in the current KB, combined using a learned logistic function.</p><p>• The confidence in an inference is sensitive to the current state of the knowledge base, and the specific entities being queried (since the paths used in the inference have these properties).</p><p>• Experimentally, the inference method yields many more moderately-confident inferences than the Horn clauses learned by N-FOIL.</p><p>• The learning and inference are more efficient than N-FOIL, in part because we can exploit efficient approximation schemes for random walks <ref type="bibr" target="#b11">(Lao and Cohen, 2010a)</ref>. The resulting inference is as fast as 10 milliseconds per query on average.</p><p>The Path Ranking Algorithm (PRA) we use is similar to that described elsewhere ( <ref type="bibr" target="#b12">Lao and Cohen, 2010b)</ref>, except that to achieve efficient model learning, the paths between a and b are determined by the statistics from a population of training queries rather than enumerated completely. PRA uses random walks to generate relational features on graph data, and combine them with a logistic regression model. Compared to other relational models (e.g. FOIL, Markov Logic Networks), PRA is extremely efficient at link prediction or retrieval tasks, in which we are interested in identifying top links from a large number of candidates, instead of focusing on a particular node pair or joint inferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Work</head><p>The TextRunner system <ref type="bibr" target="#b4">(Cafarella et al., 2006</ref>) answers list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type information, which is important for achieving high accuracy predictions.</p><p>The HOLMES system ( <ref type="bibr" target="#b17">Schoenmackers et al., 2008)</ref> derives new assertions using a few manually written inference rules.</p><p>A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules automatically from training data. Below we describe our approach in greater detail, provide experimental evidence of its value for performing inference in NELL's knowledge base, and discuss implications of this work and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we first describe how we formulate link (relation) prediction on a knowledge base as a ranking task. Then we review the Path Ranking Algorithm (PRA) introduced by <ref type="bibr" target="#b12">Lao and Cohen (2010b;</ref><ref type="bibr" target="#b11">2010a)</ref>. After that, we describe two improvement to the PRA method so that it is more suitable for the task of link prediction on knowledge base. The first improvement helps PRA deal with the large number of relations which is typical of large knowledge bases. The second improvement aims at improving the quality of inference by applying low variance sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning with NELL's Knowledge Base</head><p>For each relation R in the knowledge base we train a model for the link prediction task: given a concept x, find all other concepts y which potentially have the relation R(x, y). This prediction is made based on an existing knowledge base extracted imperfectly from the web. Although a model can potentially benefit from predicting multiple relations jointly, such joint inference is beyond the scope of this work.</p><p>To ensure reasonable number of training instances, we generate labeled training queries from 48 relations which have more than 100 instances in the knowledge base. We create two tasks for each relation-i.e., predicting y given x and predicting x given y-yielding 96 tasks in all. Each node x which has relation R in the knowledge base with any other node is treated as a training query, the actual nodes y in the knowledge base known to satisfy R(x, y) are treated as labeled positive examples, and any other nodes are treated as negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Path Ranking Algorithm Review</head><p>We now review the Path Ranking Algorithm introduced by <ref type="bibr" target="#b12">Lao and Cohen (2010b)</ref>. A relation path P is defined as a sequence of relations R 1 . . . R , and in order to emphasize the types associated with each step, P can also be written as T 0</p><formula xml:id="formula_3">R 1 − − → . . . R −→ T , where T i = range(R i ) = domain(R i+1 )</formula><p>, and we also define domain(P ) ≡ T 0 , range(P ) ≡ T . In the experiments in this paper, there is only one type of nodes called concept, which can be connected through different types of relations. In this notation, relations like "the team certain player plays for", and "the league certain player's team is in" can be expressed by the paths below (respectively): </p><p>If P = R 1 . . . R is nonempty, then let P = R 1 . . . R −1 , and define</p><formula xml:id="formula_5">h s,P (e) = e ∈range(P ) h s,P (e ) · P (e|e ; R ),<label>(5)</label></formula><p>where P (e|e ; R ) = R (e ,e) |R (e ,·)| is the probability of reaching node e from node e with a one step random walk with edge type R . R(e , e) indicates whether there exists an edge with type R that connect e to e.</p><p>More generally, given a set of paths P 1 , . . . , P n , one could treat each h s,P i (e) as a path feature for the node e, and rank nodes by a linear model</p><formula xml:id="formula_6">θ 1 h s,P 1 (e) + θ 2 h s,P 2 (e) + . . . θ n h s,Pn (e)</formula><p>where θ i are appropriate weights for the paths. This gives a ranking of nodes e related to the query node s by the following scoring function</p><formula xml:id="formula_7">score(e; s) = P ∈P h s,P (e)θ P ,<label>(6)</label></formula><p>where P is the set of relation paths with length ≤ . Given a relation R and a set of node pairs {(s i , t i )}, we can construct a training dataset D = {(x i , r i )}, where x i is a vector of all the path features for the pair (s i , t i )-i.e., the j-th component of x i is h s i ,P j (t i ), and r i indicates whether R(s i , t i ) is true. Parameter θ is estimated by maximizing the following regularized objective function</p><formula xml:id="formula_8">O(θ) = i o i (θ) − λ 1 |θ| 1 − λ 2 |θ| 2 ,<label>(7)</label></formula><p>where λ 1 controls L 1 -regularization to help structure selection, and λ 2 controls L 2 -regularization to prevent overfitting. o i (θ) is a per-instance objective function, which is defined as</p><formula xml:id="formula_9">o i (θ) = w i [r i ln p i + (1 − y i ) ln(1 − p i )],<label>(8)</label></formula><p>where p i is the predicted relevance defined as</p><formula xml:id="formula_10">p i = p(r i = 1|x i ; θ) = exp(θ T x i ) 1+exp(θ T x i )</formula><p>, and w i is an importance weight to each example. A biased sampling procedure selects only a small subset of negative samples to be included in the objective (see <ref type="bibr" target="#b12">(Lao and Cohen, 2010b</ref>) for detail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data-Driven Path Finding</head><p>In prior work with PRA, P was defined as all relation paths of length at most . When the number of edge types is small, one can generate P by enumeration; however, for domains with a large number of relations (e.g., a knowledge base), it is impractical to enumerate all possible relation paths even for small . For instance, if the number of relations related to each node is 100, even the number of length three paths easily reaches millions. For other domains like parsed natural language sentences, useful paths can be as long as ten relations <ref type="bibr" target="#b13">(Minkov and Cohen, 2008)</ref>. In this case, even with smaller number of possible relations, the total number of relation path is still too large for systematic enumeration.</p><p>In order to apply PRA to these domains, we modify the path generation procedure in PRA to produce only paths which are potentially useful for the task. Define a query s to be supporting a path P if h s,P (e) = 0 for any entity e. We require that any node created during path finding needs to be supported by at least a fraction α of the training queries s i , as well as being of length no more than (In the experiments, we set α = 0.01) We also require that a path included in the PRA model only if it retrieves at least one target entity t i in the training set. As we can see from <ref type="table" target="#tab_0">Table 1</ref>, together these two constraints dramatically reduce the number of paths that need to be considered, relative to systematically enumerating all possible paths. L1 regularization reduces the size of the model even more.</p><p>The idea of finding paths that connects nodes in a graph is not new. It has been embodied previously in first-order learning systems ( <ref type="bibr" target="#b16">Richards and Mooney, 1992)</ref> as well as N-FOIL, and relational database searching systems ( <ref type="bibr" target="#b2">Bhalotia et al., 2002</ref>). These approaches consider a single query during path finding. In comparison, the data-driven path finding method we described here uses statistics from a population of queries, and therefore can potentially determine the importance of a path more reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Low-Variance Sampling</head><p>Lao and Cohen (2010a) previously showed that sampling techniques like finger printing and particle filtering can significantly speedup random walk without sacrificing retrieval quality. However, the sampling procedures can induce a loss of diversity in the particle population. For example, consider a node in the graph with just two out links with equal weights, and suppose we are required to generate two walkers starting from this node. A disappointing result is that with 50 percent chance both walkers will follow the same branch, and leave the other branch with no probability mass.</p><p>To overcome this problem, we apply a technique called Low-Variance Sampling (LVS) ( <ref type="bibr" target="#b19">Thrun et al., 2005</ref>), which is commonly used in robotics to improve the quality of sampling. Instead of generating independent samples from a distribution, LVS uses a single random number to generate all </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section reports empirical results of applying random walk inference to NELL's knowledge base after the 165th iteration of its learning process. We first investigate PRA's behavior by cross validation on the training queries. Then we compare PRA and N-FOIL's ability to reliably infer new beliefs, by leveraging the Amazon Mechanical Turk service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross Validation on the Training Queries</head><p>Random Walk with Restart (RWR) (also called personalized PageRank <ref type="bibr" target="#b9">(Haveliwala, 2002)</ref>) is a general-purpose graph proximity measure which has been shown to be fairly successful for many types of tasks. We compare PRA to two versions of RWR on the 96 tasks of link prediction with NELL's knowledge base. The two baseline methods are an untrained RWR model and a trained RWR model as described by <ref type="bibr" target="#b12">Lao and Cohen (2010b)</ref>. (In brief, in the trained RWR model, the walker will probabilistically prefer to follow edges associated with different labels, where the weight for each edge label is chosen to minimize a loss function, such as Equation 7. In the untrained model, edge weights are uniform.) We explored a range of values for the regularization parameters L 1 and L 2 using cross validation on the training data, and we fix both L 1 and L 2 parameters to 0.001 for all tasks. The maximum path length is fixed to 3. 2 <ref type="table" target="#tab_1">Table 2</ref> compares the three methods using 5-fold cross validation and the Mean Reciprocal Rank (MRR) 3 measure, which is defined as the inverse rank of the highest ranked relevant result in a set of results.</p><p>If the the first returned result is relevant, then MRR is 1.0, otherwise, it is smaller than 1.0. Supervised training can significantly improve retrieval quality (p-value=9 × 10 −8 comparing untrained and trained RWR), and leveraging path information can produce further improvement (p-value=4 × 10 −4 comparing trained RWR with PRA). The average training time for a predicate is only a few seconds.</p><p>We also investigate the effect of low-variance sampling on the quality of prediction. <ref type="figure" target="#fig_2">Figure 2</ref> compares independent and low variance sampling when applied to finger printing and particle filtering <ref type="bibr" target="#b11">(Lao and Cohen, 2010a</ref>). The horizontal axis corresponds to the speedup of random walk compared with exact inference, and the vertical axis measures the quality of prediction by MRR with three fold cross validation on the training query set. Low-variance sampling can improve prediction for both finger <ref type="bibr">2</ref> Results with maximum length 4 are not reported here. Generally models with length 4 paths produce slightly better results, but are 4-5 times slower to train <ref type="bibr">3</ref> For a set of queries Q, M RR = 1 |Q| q∈Q 1 rank of the first correct answer for q printing and particle filtering. The numbers on the curves indicate the number of particles (or walkers). When using a large number of particles, the particle filtering methods converge to the exact inference. Interestingly, when using a large number of walkers, the finger printing methods produce even better prediction quality than exact inference. Lao and Cohen noticed a similar improvement on retrieval tasks, and conjectured that it is because the sampling inference imposes a regularization penalty on longer relation paths (2010a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation by Mechanical Turk</head><p>The cross-validation result above assumes that the knowledge base is complete and correct, which we know to be untrue. To accurately compare PRA and N-FOIL's ability to reliably infer new beliefs from an imperfect knowledge base, we use human assessments obtained from Amazon Mechanical Turk. To limit labeling costs, and since our goal is to improve the performance of NELL, we do not include RWR-based approaches in this comparison. Among all the 24 functional predicates, N-FOIL discovers confident rules for 8 of them (it produces no result for the other 16 predicates). Therefore, we compare the quality of PRA to N-FOIL on these 8 predicates only. Among all the 72 non-functional predicates-which N-FOIL cannot be applied to-PRA exhibits a wide range of performance in cross-validation. The are 43 tasks for which PRA obtains MRR higher than 0.4 and builds a model with more than 10 path features. We randomly sampled 8 of these predicates to be evaluated by Amazon Mechanical Turk. <ref type="table">Table 3</ref> shows the top two weighted PRA features for each task on which N-FOIL can successfully learn rules. These PRA rules can be categorized into broad coverage rules which behave like priors over correct answers (e.g. 1-2, 4-6, 15), accurate rules which leverage specific relation sequences (e.g. 9, 11, 14), rules which leverage information about the synonyms of the query node (e.g. 7-8, 10, 12), and rules which leverage information from a local neighborhood of the query node (e.g. 3, 12-13, 16). The synonym paths are useful, because an entity may have multiple names on the web. We find that all 17 general rules (no specialization) learned by N-FOIL can be expressed as length two relation <ref type="table">Table 3</ref>: The top two weighted PRA paths for tasks on which N-FOIL discovers confident rules. c stands for concept.  <ref type="table">Table 4</ref>: Amazon Mechanical Turk evaluation for the promoted knowledge. Using paired t-test at task level, PRA is not statistically different than N-FOIL for p@10 (p-value=0.3), but is significantly better for p@100 (p-value=0.003) PRA N-FOIL Task P majority #Paths p@10 p@100 p@1000 #Rules #Query p@10 p@100 p@1000  paths such as path 11. In comparison, PRA explores a feature space with many length three paths. For each relation R to be evaluated, we generate test queries s which belong to domain(R). Queries which appear in the training set are excluded. For each query node s, we applied a trained model (either PRA or N-FOIL) to generate a ranked list of candidate t nodes. For PRA, the candidates are sorted by their scores as in Eq. (6). For N-FOIL, the candidates are sorted by the estimated accuracies of the rules as in Eq. (2) (which generate the candidates). Since there are about 7 thousand (and 13 thousand) test queries s for each functional (and non-functional) predicate R, and there are (potentially) thousands of candidates t returned for each query s, we cannot evaluate all candidates of all queries. Therefore, we first sort the queries s for each predicate R by the scores of their top ranked candidate t in descending order, and then calculate precisions at top 10, 100 and 1000 positions for the list of result R(s R,1 , t R,1 1 ), R(s R,2 , t R,2 1 ), ..., where s R,1 is the first query for predicate R, t R,1 1 is its first candidate, s R,2 is the second query for predicate R, t R,2 1 is its first candidate, so on and so forth. To reduce the labeling load, we judge all top 10 queries for each predicate, but randomly sample 50 out of the top 100, and randomly sample 50 out of the top 1000. Each belief is evaluated by 5 workers at Mechanical Turk, who are given assertions like "Hines Ward plays for the team Steelers", as well as Google search links for each entity, and the combination of both entities. Statistics shows that the workers spend on average 25 seconds to judge each belief. We also remove some workers' judgments which are obviously incorrect <ref type="bibr">4</ref> . We sampled 100 beliefs, and compared their voted result to gold-standard labels produced by one author of this paper. <ref type="table" target="#tab_3">Table 5</ref> shows that 74% of the time the workers' voted result agrees with our judgement. <ref type="bibr">4</ref> Certain workers label all the questions with the same answer <ref type="table">Table 4</ref> shows the evaluation result.</p><p>The P majority column shows for each predicate the accuracy achieved by the majority prediction: given a query R(x, ?), predict the y that most often satisfies R over all possible x in the knowledge base. Thus, the higher P majority is, the simpler the task.</p><p>Predicting the functional predicates is generally easier predicting the non-functional predicates. The #Query column shows the number of queries on which N-FOIL is able to match any of its rules, and hence produce a candidate belief. For most predicates, N-FOIL is only able to produce results for at most a few hundred queries. In comparison, PRA is able to produce results for 6,599 queries on average for each functional predicate, and 12,519 queries on average for each non-functional predicate. Although the precision at 10 (p@10) of N-FOIL is comparable to that of PRA, precision at 10 and at 1000 (p@100 and p@1000) are much lower <ref type="bibr">5</ref> .</p><p>The #Path column shows the number of paths learned by PRA, and the #Rule column shows the number of rules learned by N-FOIL, with the numbers before brackets correspond to unspecialized rules, and the numbers in brackets correspond to specialized rules. Generally, specialized rules have much smaller recall than unspecialized rules. Therefore, the PRA approach achieves high recall partially by combining a large number of unspecialized paths, which correspond to unspecialized rules. However, learning more accurate specialized paths is part of our future work. A significant advantage of PRA over N-FOIL is that it can be applied to non-functional predicates. The last eight rows of <ref type="table">Table 4</ref> show PRA's performance on eight of these predicates. Compared to the result on functional predicates, precisions at 10 and at 100 of non-functional predicates are slightly lower, but precisions at 1000 are comparable. We note that for some predicates precision at 1000 is better than at 100. After some investigation we found that for many relations, the top portion of the result list is more diverse: i.e. showing products produced by different companies, journalist working at different publications.</p><p>While the lower half of the result list is more homogeneous: i.e. showing relations concentrated on one or two companies/publications. On the other hand, through the process of labeling the Mechanical Turk workers seem to build up a prior about which company/publication is likely to have correct beliefs, and their judgments are positively biased towards these companies/publications. These two factors combined together result in positive bias towards the lower portion of the result list. In future work we hope to design a labeling strategy which avoids this bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We have shown that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. We applied this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL. This new system improves significantly over NELL's earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100. The inference and learning are both very efficient-our experiment shows that the inference time is as fast as 10 milliseconds per query on average, and the training for a predicate takes only a few seconds.</p><p>There are some prominent directions for future work. First, inference starting from both the query nodes and target nodes ( <ref type="bibr" target="#b16">Richards and Mooney, 1992)</ref> can be much more efficient in discovering long paths than just inference from the query nodes. Second, inference starting from the target nodes of training queries is a potential way to discover specialized paths (with grounded nodes). Third, generalizing inference paths to inference trees or graphs can produce more expressive random walk inference models. Overall, we believe that random walk is a promising way to scale up relational learning to domains with very large data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example subgraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>P</head><label></label><figDesc>1 : concept AtheletePlayesForTeam −−−−−−−−−−−−−−→ concept P 2 : concept AtheletePlayesForTeam −−−−−−−−−−−−−−→ concept TeamPlaysInLeagure − −−−−−−−−−−−− → concept For any relation path P = R 1 . . . R and a seed node s ∈ domain(P ), a path constrained random walk defines a distribution h s,P recursively as follows. If P is the empty path, then define h s,P (e) = 1, if e = s 0, otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compare inference speed and quality over 96 tasks. The speedup is relative to exact inference, which is on average 23ms per query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(teams with many players in the athlete's league) 2 c athletePlaysInLeague − −−−−−−−−−−−−−− → c leagueTeams − −−−−−−−− → c teamAgainstTeam − −−−−−−−−−−− → c (teams that play against many teams in the athlete's league) athletePlaysInLeague 3 c athletePlaysSport − −−−−−−−−−−− → c players − −−−− → c athletePlaysInLeague − −−−−−−−−−−−−−− → c (the league that players of a certain sport belong to) 4 c isa − − → c isa −1 − −−− → c athletePlaysInLeague − −−−−−−−−−−−−−− → c (popular leagues with many players) athletePlaysSport 5 c isa − − → c isa −1 − −−− → c athletePlaysSport − −−−−−−−−−−− → c (popular sports of all the athletes) 6 c athletePlaysInLeague − −−−−−−−−−−−−−− → c superpartOfOrganization −−−−−−−−−−−−−−−−−→ c teamPlaysSport − −−−−−−−−−− → c (popular sports of a certain league) stadiumLocatedInCity 7 c stadiumHomeTeam − −−−−−−−−−−−− → c teamHomeStadium − −−−−−−−−−−−− → c stadiumLocatedInCity − −−−−−−−−−−−−−− → c (city of the stadium with the same team) 8 c latitudeLongitude − −−−−−−−−−−− → c latitudeLongitudeOf − −−−−−−−−−−−−− → c stadiumLocatedInCity − −−−−−−−−−−−−−− → c (city of the stadium with the same location) teamHomeStadium 9 c teamPlaysInCity − −−−−−−−−−−− → c cityStadiums − −−−−−−−− → c (stadiums located in the same city with the query team) 10 c teamMember − −−−−−−−− → c athletePlaysForTeam − −−−−−−−−−−−−−− → c teamHomeStadium − −−−−−−−−−−−− → c (home stadium of teams which share players with the query) teamPlaysInCity 11 c teamHomeStadium − −−−−−−−−−−−− → c stadiumLocatedInCity − −−−−−−−−−−−−−− → c (city of the team's home stadium) 12 c teamHomeStadium − −−−−−−−−−−−− → c stadiumHomeTeam − −−−−−−−−−−−− → c teamPlaysInCity − −−−−−−−−−−− → c (city of teams with the same home stadium as the query) teamPlaysInLeague 13 c teamPlaysSport − −−−−−−−−−− → c players − −−−− → c athletePlaysInLeague − −−−−−−−−−−−−−− → c (the league that the query team's members belong to) 14 c teamPlaysAgainstTeam − −−−−−−−−−−−−−−− → c teamPlaysInLeague −−−−−−−−−−−−−→ c (the league that the query team's competing team belongs to) teamPlaysSport 15 c isa − − → c isa −1 − −−− → c teamPlaysSport − −−−−−−−−−− → c (sports played by many teams) 16 c teamPlaysInLeague −−−−−−−−−−−−−→ c leagueTeams − −−−−−−−− → c teamPlaysSport − −−−−−−−−−− → c (the sport played by other teams in the league)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Number of paths in PRA models of maximum path length 3 and 4.</head><label>1</label><figDesc></figDesc><table>Averaged over 96 tasks. 
=3 
=4 
all paths up to length L 
15, 376 1, 906, 624 
+query support≥ α = 0.01 
522 
5016 
+ever reach a target entity 
136 
792 
+L 1 regularization 
63 
271 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Comparing PRA with RWR models.</head><label>2</label><figDesc></figDesc><table>MRRs and 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Compare Mechanical Turk workers' voted 
assessments with our gold standard labels based on 100 
samples. 
AMT=F AMT=T 
Gold=F 
25% 
15% 
Gold=T 
11% 
49% 

</table></figure>

			<note place="foot" n="1"> San Francisco&apos;s Major-League Baseball and New York&apos;s National Football League teams are both called the &quot;Giants&quot;.</note>

			<note place="foot" n="5"> If a method makes k predictions, and k &lt; n, then p@n is the number correct out of the k predictions, divided by n</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by NIH under grant R01GM081293, by NSF under grant IIS0811562, by DARPA under awards FA8750-08-1-0009 and AF8750-09-C-0179, and by a gift from Google.</p><p>We thank Geoffrey J. Gordon for the suggestion of applying low variance sampling to random walk inference. We also thank Bryan Kisiel for help with the NELL system.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: extracting relations from large plain-text collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM conference on Digital libraries -DL &apos;00</title>
		<meeting>the fifth ACM conference on Digital libraries -DL &apos;00<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open Information Extraction from the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bhalotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Hulgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charuta</forename><surname>Nakhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudarshan</surname></persName>
		</author>
		<title level="m">Keyword searching and browsing in databases using banks. ICDE</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory -COLT&apos; 98</title>
		<meeting>the eleventh annual conference on Computational learning theory -COLT&apos; 98<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Relational Web Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Clueweb09 data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<ptr target="http://boston.lti.cs.cmu.edu/Data/clueweb09/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-Ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Polynomial learnability and inductive logic programming: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3&amp;4</biblScope>
			<biblScope unit="page" from="369" to="409" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised namedentity extraction from the web: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamaria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Er</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Factrank: Random walks on a web of facts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alpa</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast query execution for retrieval models based on path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning graph walk based similarity measures for parsed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="907" to="916" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FOIL: A Midterm Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. Mike Cameron-Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning relations by pathfinding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92)</title>
		<meeting>the Tenth National Conference on Artificial Intelligence (AAAI-92)<address><addrLine>San Jose, CA, July. Matthew Richardson and Pedro Domingos</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="50" to="55" />
		</imprint>
	</monogr>
	<note>Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling Textual Inference to the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic Taxonomy Induction from Heterogenous Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TextRunner: Open Information Extraction on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL (Demonstrations)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inducing logic programs without explicit negative examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Workshop on Inductive Logic Programming (ILP-95)</title>
		<meeting>the Fifth International Workshop on Inductive Logic Programming (ILP-95)<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="403" to="416" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
