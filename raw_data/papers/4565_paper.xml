<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating Genetic Algorithms with Conditional Random Fields to Enhance Question Informer Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yuh</forename><surname>Day</surname></persName>
							<email>myday@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<postCode>115</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<postCode>106</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Hung</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<postCode>115</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<postCode>106</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chorng-Shyong</forename><surname>Ong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Management</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<postCode>106</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
							<email>shwu@cyut.edu.tw</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Chaoyang Univ. of Technology</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Lian</forename><surname>Hsu</surname></persName>
							<email>hsu@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<postCode>115</postCode>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating Genetic Algorithms with Conditional Random Fields to Enhance Question Informer Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Question informers play an important role in enhancing question classification for factual question answering. Previous works have used conditional random fields (CRFs) to identify question informer spans. However, in CRF-based models, the selection of a feature subset is a key issue in improving the accuracy of question informer prediction. In this paper, we propose a hybrid approach that integrates Genetic Algorithms (GAs) with CRF to optimize feature subset selection in CRF-based question informer prediction models. The experimental results show that the proposed hybrid GA-CRF model improves the accuracy of question informer prediction of traditional CRF models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Question informers play an important role in enhancing question classification for factual question answering <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">Krishnan et al. [3]</ref> introduced the notion of the answer type informer span of a question for question classification and showed that human-annotated informer spans lead to large improvements in the accuracy of question classification. They defined that choosing a minimal, appropriate contiguous span of a question token, or tokens, as the informer span of a question, which is adequate for question classification. For example, in the question: "What is the biggest city in the United State?" the question informer is "city". Thus "city" is the most important clue in the question for question classification. <ref type="bibr">Krishnan et al.</ref> reported that perfect knowledge of informer spans can enhance the predictive accuracy from 79.4% to 88% using linear Support Vector Machines (SVMs) on standard benchmarks of question classification.</p><p>Question informers are only useful if question informer spans can be identified automatically. Previous works have used Conditional Random Fields (CRFs) to identify question informer spans. By using a parse of the question, Krishnan et al. <ref type="bibr" target="#b2">[3]</ref> derived a set of multiresolution features to train a CRF model and achieved 85%-87% accuracy for question informer prediction.</p><p>Krishnan et al. <ref type="bibr" target="#b2">[3]</ref> also showed that the effect of the features chosen by a CRF model varies significantly depending on the accuracy of the CRF model. In a machine learning approach, feature selection is an optimization problem that involves choosing an appropriate feature subset. In CRF-based models, selection of the feature subset is a key issue in improving the accuracy of question informer prediction. Genetic Algorithms (GAs) <ref type="bibr" target="#b1">[2]</ref> have been widely used in feature selection in machine learning <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper, we propose a hybrid approach that integrates GA with CRF to optimize feature subset selection in CRF-based question informer prediction models.</p><p>The remainder of this paper is organized as follows. Section 2 describes the background to question informers and previous works. In section 3, we propose the hybrid GA-CRF approach for question informer prediction. Section 4 discusses the experiment and the test bed, and Section 5 contains the experimental results. Finally, in Section 6 we present our conclusions and indicate some future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Research Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditional Random Fields (CRFs)</head><p>Lafferty et al. <ref type="bibr" target="#b3">[4]</ref> proposed using Conditional Random Fields (CRFs), a framework for building probabilistic models, to segment and label sequence data. A CRF models Pr(y|x) using a Markov random field, with nodes corresponding to elements of the structured object y, and potential functions that are conditional on features of x. Learning is performed by setting parameters to maximize the likelihood of a set of (x,y) pairs given as training data <ref type="bibr" target="#b7">[8]</ref>.</p><p>CRFs are widely used for sequential learning problems like NP chunking, POS tagging, and name entity recognition (NER). Recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> have shown that CRFs have a consistent advantage over traditional Hidden Markov Models (HMMs) and Maximum Entropy Markov Models (MEMMs) <ref type="bibr" target="#b5">[6]</ref> in the face of many redundant features. <ref type="bibr">Krishnan et al. [3]</ref> reported that they achieved 85%-87% accuracy of question informer prediction by using CRF model with a set of features.</p><p>CRF++, which is developed by Taku Kudo, is a simple, customizable, and open source implementation of CRFs for segmenting and labeling sequenced data (CRF++ is available at: http://chasen.org/~taku/software/CRF++/). It was designed for generic purposes and can be applied to a variety of NLP tasks, such as Named Entity Recognition, Information Extraction, and Text Chunking. The benefit of using CRF++ is that it enables us to redefine feature sets and specify the feature templates in a flexible way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Genetic Algorithm (GA)</head><p>Genetic Algorithms (GAs) are a class of heuristic search methods and computational models of adaptation and evolution based on the mechanics of natural selection and genetics <ref type="bibr" target="#b1">[2]</ref>. GAs have been widely used for feature selection in machine learning <ref type="bibr" target="#b9">[10]</ref> methods, such as SVM. Feature selection is an optimization problem that involves the process of picking a subset of features that are relevant to the target concept and removing irrelevant or redundant features. This is an important factor that determines the performance of the machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Hybrid GA-CRF Model</head><p>In this paper, we propose integrating the GA architecture with CRF to optimize feature selection for CRF-based question informer prediction. <ref type="figure" target="#fig_0">Fig 1 shows</ref> the architecture of the proposed hybrid GA-CRF approach for question informer prediction. There are two phases in the architecture. The first is the GA-CRF learning phase with a training dataset, while the second is the CRF test phase with a test dataset.</p><p>The application of GA to CRF-based question informer prediction comprises the following steps.</p><p>1) Encoding a feature subset of CRF with the structure of chromosomes: To apply GA to the search for the optimal feature subsets of CRF, the subsets must be encoded on a chromosome in the form of binary strings.</p><p>The gene structure of the chromosomes for feature subset selection is presented in <ref type="figure">Figure 2</ref>.</p><p>The value of the codes for feature subset selection is set to a one-bit digit '0' or '1', where '0' means the corresponding feature is not selected, and '1' means that it is selected. The length of each chromosome is n bits, where n is the number of features. We use f i-2 , f i-1 , f i+0 , f i+1 , f i+2 to represent the sliding windows of each feature. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of feature subset encoding for GA.   3) Population: The population is a set of seed chromosomes used to find the optimal feature subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Evaluate (Fitness Function): In this step, we</head><p>calculate the fitness score of each chromosome. In addition, the population is searched to find the encoded chromosome that maximizes the specific fitness function. The values of the fitness functions for the items in the evaluation set are calculated and used to determine the suitability of each chromosome. 5) CRF model 10-fold Cross validation: In this procedure, the feature subsets derived by the previous procedure are applied to the CRF module. The fitness function is determined by the F-score of 10-fold cross validation of the CRF model. We use 10-fold cross validation on the training dataset of the CRF model as the fitness function of each chromosome to avoid over-fitting on the test dataset.</p><p>6) Stopping criteria satisfied? If the stopping criteria are satisfied, the best chromosome and a near optimal feature subset of CRF model is obtained; otherwise, apply GA operators and produce a new generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7) Apply GA operators and produce a new generation:</head><p>In this procedure, we use three GA operators, namely, reproduction, crossover, and mutation to produce a new generation.</p><p>8) Apply the selected feature subsets to the CRF test dataset: After the GA-CRF learning process, we can obtain a near optimal feature subset of CRF. We then train the whole training set on that feature subset to obtain a near optimal CRF prediction model, which we use to test the test dataset for CRF-based question informer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data set</head><p>We use the UIUC QC dataset from Li and Roth <ref type="bibr" target="#b4">[5]</ref> and the corresponding question informer dataset from Krishnan et al. <ref type="bibr" target="#b2">[3]</ref>. There are 5,500 training questions, 500 test questions, and corresponding question informers <ref type="bibr" target="#b0">1</ref>   <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. It has 6 coarse and 50 fine answer types in a two level taxonomy, together with 5,500 training and 500 test questions. Krishnan et al. <ref type="bibr" target="#b2">[3]</ref> reported that they had two volunteers to tag 6,000 UIUC questions with informer spans, which they call human-annotated "perfect" informer spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Features of Question Informer</head><p>We adopt the 3-state transition model suggested by Krishnan et al <ref type="bibr" target="#b2">[3]</ref> and follow the "begin/in/out" (BIO) model proposed by Ramshaw and Marcus <ref type="bibr" target="#b6">[7]</ref> to tag question informers. In our dataset for the CRF model, "O-QIF0" indicates outside and before a question informer, "B-QIF1" indicates the start of a question informer, while "O-QIF2" indicates outside and after a question informer.  <ref type="bibr" target="#b2">[3]</ref> indicate that using features with 2 levels is adequate. Features 12 to 18 are derived from heuristics that are suggested by Krishnan et al. We add Features 19 to 21, namely, question wh-word (6W1H1O: who, what, when, where, which, why, how, and others), question length, and token position.</p><p>In this study, we regard each feature candidate as a gene, and treat the corresponding F-score as the performance value of the feature (gene) for the CRF model. <ref type="table" target="#tab_1">Table 1</ref> shows that word, POS, parser level 1, and parser level 2 have better performance for the CRF model. For example, the F-score of a single feature used with the "word" feature is 58.35%, while using the "parser level 2" feature solely achieves a score of 48.13%. The experiment result shows that each feature candidate has a different effect on the performance of CRF-based question informer prediction. <ref type="figure">Figure 4</ref> shows an example of a feature with sliding windows for a CRF model. For example, "city" is the feature f ij for x i , where i = 4 and j=0. Given that x 4 = "city", the label of prediction y 4 = "B-QIF1". <ref type="figure" target="#fig_4">Figure 5</ref> shows an example of feature generation and a feature template for CRF++. For example, we can specify the feature "city" in feature f 0,0 as the feature template "U02:%[0,0]", and the previous feature "oldest" in feature f -1,0 as the feature template "U01:%[-1,0]". Features fij for xi</p><formula xml:id="formula_0">Sliding Windows i -2 i -1 i +0 i +1 i +2 O-QIF2 DT [+2, 1] the [+2, 0] +2 O-QIF2 IN [+1, 1] in [+1, 0] +1 B-QIF1 NN [ 0, 1] city [ 0, 0] 0 O-QIF0 JJS [-1, 1] oldest[-1, 0] -1 O-QIF0 DT [-2, 1] the [-2, 0] -2 yi POS xi i 1 0 j</formula><p>Features fij for xi <ref type="figure">Figure 4</ref>. An example of feature with sliding windows for CRF   <ref type="table" target="#tab_1">Null_1 SBARQ_1 IsTag0 IsNum0 IsPrevTag1 IsNextTag0 IsEdge0 IsBegin0 IsEnd0 Wh_what 10 1 O-QIF0  1</ref>  We encode all 21 feature candidates and sliding windows with the structure of chromosomes to form the feature subset for GA. The candidates corresponding sliding window size is 5 (w= -2, -1, 0, +1, +2), We use f i-2 , f i-1 , f i+0 , f i+1 , f i+2 to represent the windows of each feature candidate. Since there are 21 basic features and 5 sliding windows, we can generate 105 (21 basic features * 5 sliding windows) features (genes) for each chromosome. <ref type="figure" target="#fig_6">Figure 7</ref> shows an example of encoding a feature subset with the structure of chromosomes for GA. For example, the chromosome "1 0 1 1 0 0 1 1 0 1" represents the selected feature subset is {F1, F3, F4, F7, F8 , F10}, and the corresponding feature template for CRF++ is { U00: </p><formula xml:id="formula_1">F10 U09:%x[+2,1] f+2,1 DT F9 U08:%x[+1,1] f+1,1 IN F8 U07:%x[ 0,1] f0,1 NN F7 U06:%x[-1,1] f-1,1 JJS F6 U05%x[-2,1] f-2,1 DT F5 U04:%x[+2,0] f+2,0 the F4 U03:%x[+1,0] f+1,0 in F3 U02:%x[ 0,0] f0,0 city F2 U01:%x[-1,0] f-1,0 oldest F1 U00:%x[-2,0] f-2,0 the</formula><formula xml:id="formula_2">%x[-2,0] U02:%x[ 0,0] U03:%x[+1,0] U06:%x[-1,1] U07:%x[ 0,1] U09:%x[+2,1]}. F10 U09:%x[+2,1] f+2,1 DT F9 U08:%x[+1,1] f+1,1 IN F8 U07:%x[ 0,1] f0,1 NN F7 U06:%x[-1,1] f-1,1 JJS F6 U05%x[-2,1] f-2,1 DT F5 U04:%x[+2,0] f+2,0 the F4 U03:%x[+1,0] f+1,0 in F3 U02:%x[ 0,0] f0,0 city F2 U01:%x[-1,0] f-1,0 oldest F1 U00:%x[-2,0] f-2,0 the</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fitness function of GA</head><p>In the learning and validation phases, we use 10-fold cross validation with 5,500 UIUC training data to reduce the over-fitting problem, and use the selected near optimal feature subset for the CRF model. <ref type="figure">Figure 8</ref> shows the experimental results of 10-fold cross validation on the training dataset and the corresponding performance on the test dataset using GA for feature subset selection of CRF-based question informer prediction. The F-score is approximately 95% for 10-fold cross validation on the training dataset (UIUC Q5500), and approximately 88% on the test dataset (UIUC Q500). <ref type="figure">Figure 9</ref> shows the experimental results of CRF-based question informer prediction using GA for feature subset selection for a population whose characteristics are: size = 40, crossover rate = 80%, and mutation rate = 10%. The F-score for question informer prediction is 93.87%. It should be noted that the fitness function is used to evaluate on the test dataset (UIUC Q500) with the training dataset (UIUC Q5500). F Score: 93.87, Population: 40, Crossover: 80%, Mutation: 10%, Generation: 100 After 100 generations of GA, we obtain the near optimal chromosomes and their corresponding feature subset for CRF++. <ref type="figure" target="#fig_0">Figure 10</ref> shows the near optimal chromosomes and the corresponding feature subset for the CRF model selected by GA. The experimental results show that we can improve the F-score of CRF-based question informer prediction from 88.9% to 93.87% using GA to reduce the number of features from 105 to a 40-feature subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>The accuracy of our proposed GA-CRF model for UIUC dataset is 95.58% compared with 87% for the traditional CRF model reported by <ref type="bibr">Krishnan et al. [3]</ref>. The experimental results show that our proposed hybrid GA-CRF model for question informer prediction outperforms the traditional CRF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a hybrid approach that integrates Genetic Algorithm (GA) with Conditional Random Field (CRF) to optimize feature subset selection in a CRFbased model for question informer prediction. The experimental results show that the proposed hybrid GA-CRF model of question informer prediction improves the accuracy of the traditional CRF model. By using GA to optimize the selection of the feature subset in CRF-based question informer prediction, we can improve the F-score from 88.9% to 93.87%, and reduce the number of features from 105 to 40.   <ref type="figure" target="#fig_0">Figure 10</ref>. The near optimal chromosome and the corresponding feature subset for the CRF model selected by GA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GA-CRF Model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The architecture of proposed hybrid GA-CRF approach for question informer prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fi- 2 FiFigure 2 .</head><label>22</label><figDesc>Figure 2. Gene structure of chromosomes for a feature subset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An example of feature subset encoding for GA 2) Initialization: In this step, we generate the initial population, which is initialed into random values before the search process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>We generate the 21 feature candidates in a two- dimensional matrix and add the question informer tag (O- QIF0, B-QIF1, O-QIF2) for each question sentence in the last column. Figure 6 shows the data format of a question informer and the corresponding features candidates in a CRF model for the question "What is the oldest city in the United States?".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. An example of feature generation and a feature template for CRF++ 4.3. Encoding a feature subset with the structure of chromosomes for GA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. An example of the data format of a question informer and the corresponding features for a CRF model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Encoding a feature subset with the structure of chromosomes for GA. There are 105 feature subsets in total (21 basic features * 5 sliding windows)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure8. 10 -Figure 9 .</head><label>109</label><figDesc>Figure8. 10-fold cross validation on the training dataset and their corresponding performance on the test dataset using GA for feature subset selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Near Optimal Feature Subsets for CRF model: U001:%x[-2,1] U002:%x[0,1] U003:%x[1,1] U004:%x[-1,2] U005:%x[0,2] U006:%x[1,2] U007:%x[2,2] U008:%x[-2,3] U009:%x[-2,5] U010:%x[-1,5] U011:%x[-2,6] U012:%x[1,6] U013:%x[2,6] U014:%x[2,7] U015:%x[0,8] U016:%x[1,8] U017:%x[-2,9] U018:%x[1,9] U019:%x[1,10] U020:%x[-2,11] U021:%x[-2,12] U022:%x[0,12] U023:%x[0,13] U024:%x[2,13] U025:%x[-2,14] U026:%x[-1,14] U027:%x[2,14] U028:%x[0,16] U029:%x[1,16] U030:%x[2,16] U031:%x[-2,17] U032:%x[-1,17] U033:%x[0,17] U034:%x[1,17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>the UIUC QC data set, which is now the standard dataset for question classification</head><label></label><figDesc></figDesc><table>. 
Li and Roth used supervised learning for question 

1 

Li and Roth (2002), UIUC QC Datasets: 
http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/ 
Vijay Krishnan, Sujatha Das and Soumen Chakrabarti 
(2005), 
UIUC 
Informers 
Datasets 
: 
http://hake.stanford.edu/~kvijay/UIUC_Informers/ 

classification of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows</head><label>1</label><figDesc>21 basic feature candidates for question informer prediction. Features 1 to 4 are word, POS, heuristic informer, and question segmentation information, respectively. Features 5 to 11 are parser information about the question. We use the OpenNLP Parser (available at: http://opennlp.sourceforge.net/) to parse a question and translate the parse tree into a two- dimensional matrix with 6 levels as features from the parse of a question. Although the parse tree can be arbitrarily deep, Krishnan et al.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 . Features for question informer predictionPOS HQI Token ParL0 ParL1 ParL2 ParL3 ParL4 ParL5 ParL6 IsTag IsNum IsPrevTag IsNextTag IsEdge IsBegin IsEnd Wh-word L P y i 0 What WP city What What WP_1 WHNP_1 Null_1 Null_1</head><label>1</label><figDesc></figDesc><table>ID 
Feature 
name 
Description 

Feature 
Template for 
CRF ++ 

F-score 
Feature 
Rank 

1 Word 
Word 
U01:%x[0,0] 
58.35 
1 

2 POS 
POS 
U01:%x[0,1] 
48.29 
6 

3 HQI 
Heuristic Informer U01:%x[0,2] 
52.21 
4 

4 Token 
Token 
U01:%x[0,3] 
58.35 
2 

5 ParserL0 Parser Level 0 
U01:%x[0,4] 
58.35 
3 

6 ParserL1 Parser Level 1 
U01:%x[0,5] 
50.98 
5 

7 ParserL2 Parser Level 2 
U01:%x[0,6] 
48.13 
7 

8 ParserL3 Parser Level 3 
U01:%x[0,7] 
37.76 
9 

9 ParserL4 Parser Level 4 
U01:%x[0,8] 
38.45 
8 

10 ParserL5 Parser Level 5 
U01:%x[0,9] 
21.45 
17 

11 ParserL6 Parser Level 6 
U01:%x[0,10] 
22.43 
13 

12 IsTag 
Is Tag 
U01:%x[0,11] 
21.57 
15 

13 IsNum 
Is Number 
U01:%x[0,12] 
21.57 
16 

14 IsPrevTag Is Previous Tag 
U01:%x[0,13] 
21.21 
18 

15 IsNextTag Is Next Tag 
U01:%x[0,14] 
28.75 
11 

16 IsEdge 
Is Edge 
U01:%x[0,15] 
21.58 
14 

17 IsBegin 
Is Begin 
U01:%x[0,16] 
15.45 
20 

18 IsEnd 
Is End 
U01:%x[0,17] 
28.26 
12 

19 Wh-word 
Question Wh-
word (6W1H1O) 
U01:%x[0,18] 
30.17 
10 

20 Length 
Question Length U01:%x[0,19] 
20.93 
19 

21 Position 
Token Position 
U01:%x[0,20] 
13.17 
21 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We would like to thank Vijay Krishnan for providing the question informer dataset and informative discussions. This research was supported in part by the thematic program of Academia Sinica under Grant AS 95ASIA02, the National Science Council under Grant NSC 95-2752-E-001-001-PAE, NSC 95-2416-H-002-047, and NSC94-2218-E-324-003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL 2003)</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL 2003)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization and Machine Learning</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced Answer Type Inference from Questions using Sequential Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Question Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum Entropy Markov Models for Information Extraction and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="591" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACL Workshop on Very Large Corpora</title>
		<meeting>the Third ACL Workshop on Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-markov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Question Classification using HDAG Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multilingual Summarization and Question Answering 2003 (post-conference workshop in conjunction with ACL-2003)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature subset selection using a genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
