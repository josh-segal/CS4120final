<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training an Active Random Field for Real-Time Image Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
						</author>
						<title level="a" type="main">Training an Active Random Field for Real-Time Image Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-MRF training</term>
					<term>CRF training</term>
					<term>Fields of Experts</term>
					<term>image denoising EDICS: TEC-RST</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Many computer vision problems can be formulated in a Bayesian framework based on Markov Random Fields (MRF) or Conditional Random Fields (CRF). Generally, the MRF/CRF model is learned independently of the inference algorithm that is used to obtain the final result. In this paper, we observe considerable gains in speed and accuracy by training the MRF/CRF model together with a fast and suboptimal inference algorithm. An Active Random Field (ARF) is defined as a combination of a MRF/CRF based model and a fast inference algorithm for the MRF/CRF model. This combination is trained through an optimization of a loss function and a training set consisting of pairs of input images and desired outputs. We apply the Active Random Field concept to image denoising, using the Fields of Experts MRF together with a 1-4 iteration gradient descent algorithm for inference. Experimental validation on unseen data shows that the Active Random Field approach obtains an improved benchmark performance as well as a 1000-3000 times speedup compared to the Fields of Experts MRF. Using the ARF approach, image denoising can be performed in real-time, at 8fps on a single CPU for a 256×256 image sequence, with close to state-of-the-art accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many real-world applications can be regarded as graphbased optimization problems, where the graph nodes are some smaller granularities of the system, such as atoms for material science and pixels for computer vision. In some cases (e.g. material science), a unique energy function that can be described mathematically exists and can accurately represent the relationship between the graph nodes. In computer vision, the natural images exhibit very complex structures for which it is difficult if not impossible to find an exact mathematical model that is computationally feasible.</p><p>Many of these computer vision problems are approached by constructing models based on Markov Random Field (MRF) or Conditional Random Field (CRF) energy functions and obtaining the solution through an optimization procedure. The optimization is one of the available MRF/CRF Maximum A Posteriori (MAP) inference algorithms such as gradient descent, Belief Propagation <ref type="bibr" target="#b43">[44]</ref>, Graph Cuts <ref type="bibr" target="#b4">[5]</ref>, Iterated Conditional Modes <ref type="bibr" target="#b2">[3]</ref>, etc. However, such an approach faces two challenges when applied to real-world problems.</p><p>First, the energy function must be computationally feasible in the sense that the minimum should be found in polynomial time. This does not usually happen in reality, since finding the global minimum for most energy functions associated to realworld applications is NP hard. For example, finding the global minimum of the Potts model <ref type="bibr" target="#b27">[28]</ref>, used in Stereo Matching as a prior term <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, is NP hard <ref type="bibr" target="#b4">[5]</ref>. In such cases, polynomial-time algorithms are not expected to be found.</p><p>Second, it is very hard to find energy functions that always have a global minimum exactly at the desired solution. For example, even though the Potts model has been widely used in Stereo, the energy level of the desired result is higher than the energy obtained by different optimization algorithms <ref type="bibr" target="#b31">[32]</ref>, or the global minimum <ref type="bibr" target="#b22">[23]</ref>. Recent work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> introduced methods for training the MRF parameters such that the MRF energy minimum is as close as possible to the desired output on a training set.</p><p>The goal of this paper is to observe that when an approximate model is sought, it is sometimes not necessary to find the global minimum of the MRF energy. It has been shown in <ref type="bibr" target="#b38">[39]</ref> that for applications with limited computational budget, the MAP parameter estimation does not give the best accuracy, and training biased estimators could compensate some of the errors introduced by the fast and approximate inference algorithm. How much can the biased estimators compensate for the suboptimal algorithm? In this paper we attempt to answer this question for image denoising with a target on real-time performance. The energy model and the inference algorithm are no longer independent, so we consider them as parts of an Active Random Field, and their parameters are learned so that they work best together to obtain the desired results. For the image denoising application, we use the Fields of Experts <ref type="bibr" target="#b28">[29]</ref> Markov Random Field (MRF) model and a 1-4 iteration gradient descent inference algorithm. The algorithm is restricted to be 1000-3000 times faster than the one previously used for image denoising and the best model-algorithm parameters are trained using a dataset of training pairs consisting of input images corrupted with noise and the desired denoised output (the images without the noise). A comprehensive evaluation on 68 standard benchmark images that were not used for training revealed that the trained model-algorithm combination obtains improved denoising performance compared to the equivalent MRF model while being thousands of times faster.</p><p>Section II presents an overview of Markov Random Fields, Energy Based Models and introduces the Active Random Field concept. Section III applies the Active Random Field to image denoising using the Fields of Experts model, presenting a detailed overview of the training procedure and results. Finally, Section IV presents conclusions and future directions.</p><p>A shorter version of this paper appeared in CVPR <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ACTIVE RANDOM FIELD: JOINTLY TRAINING THE MODEL WITH A FAST AND SUBOPTIMAL INFERENCE ALGORITHM</head><p>Markov Random Fields (MRF) are used extensively in many areas of computer vision, signal processing and beyond. They are capable of enforcing strong regularization on the desired results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of Markov Random Fields and Conditional Random Fields</head><p>Let G = (V, E) be a graph with nodes V and edges E, x = (x v ) v∈V be a set of random variables representing some hidden attributes (e.g. labels) of the graph nodes v ∈ V , and C be a set of cliques (fully connected subgraphs) of G. In a Bayesian framework, the posterior probability of the hidden variables x given input data (image, signal) y is</p><formula xml:id="formula_0">P (x|y) ∝ P (y|x)P (x)<label>(1)</label></formula><p>The Markov Random Field (C, φ) models the prior on the hidden variables x</p><formula xml:id="formula_1">P (x) = 1 Z exp[ c∈C φ c (x c )]<label>(2)</label></formula><p>where φ c (x c ) are potential functions that enforce the regularization between the variables x c corresponding to the clique c. The cliques can be as small as graph edges (order 2), however larger cliques are preferred, since they are capable of representing more complex relationships. In our denoising application, the graph G is the pixel lattice and the clique set C contains all the 5 × 5 pixel patches of the image, thus each clique c ∈ C contains 25 nodes.</p><p>Quite recently, Conditional Random Fields (CRF) <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref> were developed as an extension of the MRF so that the clique potentials depend on the observed data y. A CRF is also a pair (C, φ) with φ depending on y, aimed at directly modeling the posterior P (x|y) (thus the task that is being solved).</p><formula xml:id="formula_2">P (x|y) = 1 Z(y) exp[ c∈C φ c (x c , y)]<label>(3)</label></formula><p>The MRFs and CRFs have the following advantages and disadvantages:</p><p>+ They are capable of encoding complex relationships between the graph attributes x resulting in flexible yet powerful models -Inferring the optimal state is computationally demanding. For example, the exact inference is NP hard <ref type="bibr" target="#b4">[5]</ref> even for one of the simplest pairwise MRF priors: the Potts model <ref type="bibr" target="#b27">[28]</ref>. Hence, approximate solutions are used in practice. -The MRF is difficult to train, since the normalization constant Z is needed to comparing different MRF models. -The the MRF/CRF is always used with an inference algorithm, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. However, the MRF/CRF is usually trained independently of the inference algorithm, through a procedure illustrated <ref type="figure" target="#fig_1">Figure 2</ref>. We will observe that by training the MRF/CRF together with the inference algorithm, significant improvements in both speed and accuracy can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy Based Models and Loss Functions</head><p>Recent work on Energy Based Models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref> deals with the normalization constant by training the MRF parameters θ so that the MAP estimates are as similar as possible to the corresponding desired outputs. The differences between the MAP estimates x i and the desired outputs t i are measured using a loss function L(x i , t i ) and the training procedure for the Energy Based Models can be written as:</p><formula xml:id="formula_3">min θ i L(x i , t i ), with x i = arg max x p(x|y i ; θ) (4)</formula><p>This approach eliminates the need to compute the normalization constant by comparing models using the loss function. However, these methods still deal with an idealized situation, since in reality the minimum energy MRF point is often too expensive to compute (e.g. NP-hard for the Potts model) obtaining a suboptimal point instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Active Random Fields</head><p>Since most fast inference algorithms obtain a sub-optimal solution anyway, we follow <ref type="bibr" target="#b38">[39]</ref> and propose a different approach in which the model parameters are trained such that the inference algorithm output (and not the "ideal" MAP estimate as in the Energy Based Models) is close to the desired output. This way, the suboptimal inference algorithm is involved in the parameter learning phase. This combined approach can be written as: where x = A(y, θ) is the result of the algorithm A with the model and algorithm parameters θ = (θ m , θ a ) on the input image y. As with the Energy Based Models from Section II-B, the training data consists of pairs (y i , t i ) consisting of input images y i and the corresponding desired outputs t i . This approach is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Since the MRF model and the inference algorithm are now inseparable, we define an Active Random Field (ARF) as a triplet (C, φ, A) consisting of a MRF or CRF (C, φ) together with an inference algorithm A ∈ A. The algorithm A is selected from a family of algorithms A that provides inference on the input data y using the model (C, φ). The algorithm family A can include any type of algorithm that can be used for MRF/CRF inference: gradient descent, Belief Propagation <ref type="bibr" target="#b43">[44]</ref>, Graph Cuts <ref type="bibr" target="#b4">[5]</ref>, etc. However, in contrast to the standard MRF/CRF approaches, the algorithms in the family A are restricted to be very fast, by sacrificing accuracy. For example, the number of gradient descent iterations in our image denoising application is kept small, on the order of 1 to 4, as opposed to 3000-10000 iterations used in <ref type="bibr" target="#b28">[29]</ref>. The inaccuracy of the algorithm is compensated by training the model to give best results on this algorithm, resulting in a fast and accurate combination.</p><formula xml:id="formula_4">min θ i L(x i , t i ), with x i = A(y i , θ)<label>(5)</label></formula><p>The performance of the Active Random Field is measured using a loss function L that is a generally accepted benchmark in the community. In image denoising we use the average PSNR (peak signal-to-noise ratio) over the set of images (training or testing) and replace the minimization in Equation (5) with a maximization. Other more appropriate loss functions could be used instead of the PSNR, for example the Structural Similarity Index (SSIM) <ref type="bibr" target="#b39">[40]</ref>.</p><p>The differences from the standard MRF/CRF approaches and the proposed ARF approach are 1) The normalization constant Z is not important in the ARF since different models are compared using the loss function L instead of the likelihood or posterior probability.</p><p>2) The training set consists of pairs of input images and desired results. With the loss functions, this avoids the need for sampling from the learned distribution as in the MRF/CRF training. The new training approach gives a better idea on when the training is completed or whether overfitting occurs.</p><p>3) The trained model and algorithm complement each other and result in a fast and accurate system. 4) The MRF/CRF are just models that are always used with the help of an inference algorithm. On the other hand, the ARF is a trained model+algorithm combination that given an input, returns a result, thus it is a full computational solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Related Work</head><p>In the literature, a substantial amount of work combines models with algorithms in different ways. Active Appearance Models <ref type="bibr" target="#b7">[8]</ref> are iterative algorithms driven by data and a PCAlike model to find objects of interest in the image. The solution depends on the starting location, so they are usually used in cooperation with other algorithms or with user initialization. A more complete solution for object or shape detection is offered by the Shape Regression Machine <ref type="bibr" target="#b45">[46]</ref>, where an image based regression algorithm is trained to find a vector toward the object of interest from any random location inside the image. Fast and robust object detection is obtained by using hundreds of random initializations and a verification step based on Adaboost. The Shape Regression Machine can thus be seen as a trained model-algorithm combination for object or shape detection. Our work differs from the Regression Machine because it is aimed at training models and algorithms for MRF/CRF inference instead of object/shape detection. Another related work is <ref type="bibr" target="#b11">[12]</ref>, learning detectors for faces and face parts by exploiting the context between them, but without an explicit MRF formulation.</p><p>The ARF resembles the energy based models <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, in that only the energy part of the MRF is used without the normalization constant, and a loss function is used for training. The energy based models are models trained in such a way that the minimum energy is at the desired location on the training set, independent of the optimization (inference) algorithm used. In order for that to happen, specific conditions on the energy function are imposed <ref type="bibr" target="#b18">[19]</ref>. By contrast, the ARF training finds the model parameters that give best results on a training set using a preselected inference algorithm. As a consequence, no conditions on the energy function or the loss function are imposed on the ARF. The applicability of the ARF is only limited by the existence of a fast inference algorithm that obtains results in a matter of seconds, since it will have to be applied many times during training.</p><p>A number of works use the model-algorithm combination for learning the model, but without imposing any computational complexity constraints. In this category is <ref type="bibr" target="#b35">[36]</ref>, where a CRF based on pairwise potentials is trained for object classification using boosting and a pixelwise loss function. On a similar note, <ref type="bibr" target="#b36">[37]</ref> trains a sequence of classifiers for object segmentation. Each classifier is based on features from the data and on the probability map obtained so far. These two methods train MRF model-algorithm combinations that slowly decrease in speed at each training iteration, because the models become more and more complex. In <ref type="bibr" target="#b29">[30]</ref>, an approximate posterior was maximized by gradient optimization for learning a pairwise MRF for stereo matching.</p><p>There exist a number of works that train model-algorithm combinations with a loss function that is used to report the results. However, these works use inference algorithms that are focused on exact MAP estimation, which is different than what is proposed in this paper. The same quantity from Eq. <ref type="formula" target="#formula_4">(5)</ref> is minimized in <ref type="bibr" target="#b33">[34]</ref> for image denoising, but as an attempt to obtain a stronger MRF optimum than the gradient descent. For that, a more complex inference algorithm, based on variational optimization, is derived. On a similar note, in <ref type="bibr" target="#b32">[33]</ref> Gaussian Conditional Random Fields are defined and used for image denoising. They allow exact computation of the MAP solution as well as an analytic gradient of a loss function (the MSE) comparing the solution and the desired result. The analytic computation of the MAP solution and of the gradient are possible by making some compromises in the model (the GCRF). The results presented in <ref type="bibr" target="#b32">[33]</ref> are comparable to the two iterations of ARF though obtained at least one hundred times slower. We show in Section III-F that MAP estimation for the Fields Of Experts MRF model can be obtained by a sequence of GCRF estimations. Finally, a model-algorithm combination for optical flow was trained in <ref type="bibr" target="#b19">[20]</ref> using stochastic optimization and a loss function based on the average endpoint error. The MRF model was based on 3-cliques and inference was obtained by limited memory BFGS <ref type="bibr" target="#b20">[21]</ref>. A common theme in these works is the fact that all use a lot of computation in the inference algorithm for obtaining an strong MRF optimum. This paper differs in this regard by using a fast (close to real-time) and suboptimal inference algorithm which does not try to obtain a strong optimum. We argue in this paper that it is more important to prevent overfitting than to obtain a strong MRF optimum.</p><p>Even when using a fast inference algorithm such as one iteration of gradient descent, through appropriate training and with a complex and flexible enough model, the model will adapt to the simple descent algorithm as predicted by <ref type="bibr" target="#b38">[39]</ref>. Consequently, the image denoising results presented in this paper surpass any previous results based on MRF models in both speed and accuracy.</p><p>Similar goals in obtaining good results with low computational expense are explored in cost-sensitive learning. In <ref type="bibr" target="#b37">[38]</ref>, a decision tree was trained to minimize a cost function with terms for accuracy and computational expense for each feature. Also related is <ref type="bibr" target="#b41">[42]</ref>, where for each instance of the wellknown SAT problem, the most efficient algorithm is selected from a pool of SAT solvers using regressors that estimate the algorithm running time. These regressors have been trained beforehand on a dataset of SAT instances.</p><p>In general, parameter tuning for a specific application based on a training dataset can be viewed as related work, but we are unaware of any work specifically aimed at studying parameter tuning and ways to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training the Active Random Field</head><p>Training of the Active Random Field, is achieved using examples in the form of pairs (y i , t i ) of the observed images y i and the corresponding desired outputs t i . Given a training set T = {(y i , t i ), i = 1, ..., n} consisting of such pairs, the loss function L(y, t) is used to evaluate how well the model and algorithm solve the given problem on this training set.</p><p>If the model-algorithm combination is parametrized by θ = (θ m , θ a ), the training is an optimization procedure to find</p><formula xml:id="formula_5">θ = arg min θ n i=1 L(A(y i , θ), t i )<label>(6)</label></formula><p>Depending on the problem, different optimization algorithms (coordinate descent, conjugate gradient, simulated annealing, genetic algorithm, etc) could be appropriate.</p><p>There are two main concerns regarding this Active Random Field approach.</p><p>1) The main concern is overfitting the training data. This happens when an increased performance on the training data is reflected in a decreased performance on an unseen dataset. Overfitting can be detected using a validation set and appropriate measures can be taken. Possible measure include increasing the number of training examples or changing the type of the training examples (e.g. larger images to avoid boundary effects). 2) Another concern is the computational complexity of the applying the algorithm on all the training examples for each optimization iteration. This concern is addressed in three ways. First, for certain problems, different design strategies (e.g. memorization of partial results) can be used to reduce the computation to a fraction of the full evaluation cost. Second, efficient optimization algorithms such as conjugate gradient or genetic algorithms, can make good use of each function evaluation. Third, the computational demand is less of an issue every day due to the exponential growth in computational power of a standard PC. Even though the CPU frequency has reached a limit recently, the number of CPU cores in a standard PC still increases exponentially. Furthermore, the training can be easily parallelized, resulting in a good utilization of all available computing power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPLICATION: IMAGE DENOISING</head><p>We apply the ARF idea to image denoising, where given an image corrupted with noise, the goal is to obtain an image from which the noise was removed. This problem has been addressed using wavelets in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref> and by learning a MRF prior model known as Fields of Experts on 5 × 5 pixel cliques in <ref type="bibr" target="#b28">[29]</ref>. Non-local image denoising methods include <ref type="bibr" target="#b5">[6]</ref> and especially 3D collaborative filtering (BM3D) <ref type="bibr" target="#b8">[9]</ref>, the latter obtaining very good results with low computational expense. An example of an image denoising problem and results obtained using the above mentioned methods as well as the ARF approach proposed in this paper are shown in <ref type="figure" target="#fig_3">Figure  4</ref>, together with the CPU time required to obtain each result. Another approach <ref type="bibr" target="#b9">[10]</ref> uses a sparse representation based on a learned dictionary of primitives and is more computationally expensive. The ARF approach to image denoising proposed in this paper uses the Fields of Experts MRF model and the gradient descent algorithm that were presented in <ref type="bibr" target="#b28">[29]</ref> and will be briefly mentioned in the next section. The loss function used for training the ARF is the average PSNR (Peak Signal to Noise Ratio) over the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fields of Experts</head><p>The Fields of Experts <ref type="bibr" target="#b28">[29]</ref> is a Markov Random Field prior model with potential functions based on a collection of convolution kernels (filters) J f , f = 1, ..., N and coefficients</p><formula xml:id="formula_6">α f , f = 1, ..., N p F OE (x, θ) = 1 Z(θ) exp(−E F OE (x, θ)), E F OE (x, θ) = k N f =1 α f log(1 + 1 2 (J T f x (k) ) 2 )<label>(7)</label></formula><p>The first sum is taken over the cliques k of the denoised image x, and x (k) are the pixels of x corresponding to clique k. There is a clique centered at each pixel location inside the image. Basically, each expert is a convolution followed by a robust potential function. A convolutional approach is also taken in the FRAME model for texture modeling <ref type="bibr" target="#b46">[47]</ref>. This is a Maximum Entropy Model with learned potential functions and convolutions of the image with predefined filters such as Laplacian of Gaussian, Gabor, etc. The difference is that in the FRAME model the convolution filters are predefined and the potential functions are learned, while in the FOE the potential functions are fixed and the convolution filters are learned.</p><p>For image denoising, this prior is used together with a likelihood that assumes i.i.d. Gaussian noise:</p><formula xml:id="formula_7">p(y|x) ∝ exp(−E data (x|y)), E data (x|y) = 1 2σ 2 j (y j −x j ) 2<label>(8)</label></formula><p>where x j is the value of pixel j of image x.</p><p>The beauty of the Fields of Experts formulation consists of an analytical solution for the gradient of the energy with respect to x.</p><formula xml:id="formula_8">∇ x E F OE (x, θ) = N f =1 α f J − f * J T f x 1 + 1 2 (J T f x) 2 ∇ x E data (x|y) = 1 2σ 2 (x − y)<label>(9)</label></formula><p>where J − f is the mirror image of filter J f around its center pixel.</p><p>Given a noisy image and learned parameters θ, the denoising is obtained by gradient descent in the energy E data (x|y) + E F OE (x, θ). Thus, by taking small steps in the direction of the energy gradient, a denoised imagê x is obtained in about 3000 iterations. For more details, see <ref type="bibr" target="#b28">[29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Active Random Field Approach</head><p>In the Fields of Experts formulation, the model (MRF prior +likelihood) is trained independently from the MAP inference algorithm (gradient descent). In what follows, they will be trained together in a joint optimization.</p><p>For image denoising with Active Random Fields, we use the model and family of algorithms A from the Fields of Experts formulation presented above. By ignoring the normalization constant, from the gradient equation <ref type="formula" target="#formula_8">(9)</ref> we obtain the iterative gradient descent inference algorithm that is used for MAP estimation.</p><formula xml:id="formula_9">x ← x + δ β 2σ 2 (x − y) + N f =1 α f J − f * J T f x 1 + 1 2 (J T f x) 2<label>(10)</label></formula><p>These iterative algorithms from equation (10) form an algorithm family A, parametrized by N convolution kernels J f , f = 1, ..., N with corresponding coefficients α f , the data coefficient β, the number n iter of gradient update iterations (10), and the update parameter δ. Therefore θ = (θ m , θ a ) = (N, J 1 , α 1 , ..., J N , α N , β, n iter , δ). <ref type="formula" target="#formula_0">(11)</ref> When training for a particular noise level σ, we observed a very modest contribution of at most 0.01dB of the data term β 2σ 2 (x − y) to the final result. Hence we keep β = 0 until section III-E.</p><p>In our approach, instead of taking n iter = 3000 iterations with small steps (δ = 0.2) as in the FOE model, the algorithms in the family A have a small number of iterations n iter ∈ {1, 2, 3, 4} with δ = 400/n iter . Since the number of iteration is small, the result is obtained between 800 and 3000 times faster than the FOE. At the same time we observe that the denoising performance actually increases compared to FOE for an appropriately trained system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training the Active Fields of Experts</head><p>In <ref type="bibr" target="#b28">[29]</ref>, the Fields of Experts model is trained using Contrastive Divergence <ref type="bibr" target="#b14">[15]</ref> and Markov Chain Monte Carlo sampling. The procedure involves gradient descent in the parameter space to minimize the KL divergence between the model probability and the empirical prior probability obtained from the training examples. The parameters are updated based on expected values with respect to the current probability distribution, obtained using MCMC sampling. The training procedure is computationally intensive and yields a generic prior model for natural images.</p><p>In <ref type="bibr" target="#b33">[34]</ref>, the same FOE model is used and trained using a loss function and stochastic gradient descent. With the help of a family of upper bounds of the nonlinear function log(1+x 2 ), another inference algorithm is obtained, with the hope that it can obtain a stronger optimum than the gradient descent <ref type="bibr" target="#b9">(10)</ref>.</p><p>In what follows, we will show that this is not necessary, since by appropriately training the ARF (i.e. the FOE model together with the steepest descent algorithm), the model will adapt to make the simple gradient descent work very well, making it unnecessary to use a more powerful inference algorithm. This was predicted by Wainwright in <ref type="bibr" target="#b38">[39]</ref> but the extent to which this statement is true is quite surprising.</p><p>1) Dataset: The same images as <ref type="bibr" target="#b28">[29]</ref> are used for training, namely 40 natural images from the Berkeley dataset <ref type="bibr" target="#b21">[22]</ref>. The training examples consist of the 40 pairs (y i , t i ) of input images y i and desired results t i , i = 1, ..., 40. The desired results t i are the original noise-free training images. The input images y i are the original training images t i corrupted with Gaussian noise of similar variance as expected at testing time. Since each training example contains 150, 000 cliques, the training set contains 6, 000, 000 cliques. We experimented with smaller patches (e.g. of size 15 × 15 as in <ref type="bibr" target="#b28">[29]</ref>) and observed that overfitting occurs when the patches are smaller than 250×250 pixels. This could be due to the boundary effect since the graph nodes close to the patch boundary don't have all the neighbors to communicate with and behave differently than the interior nodes.</p><p>For testing, we use the same 68 natural images from the Berkeley dataset as <ref type="bibr" target="#b28">[29]</ref> as well as some standard image denoising test images. These testing images were not used for training.</p><p>2) Loss Function: The ARF is trained by optimizing the same criterion that is used for evaluating the denoising system performance, namely the average PSNR over the images in the set. Thus the loss function is L(x, t) = 20 log 10 (255/std(t − x))</p><p>where std(t − x) is the standard deviation of the difference between the original image t and the denoised image x. More appropriate loss functions could be used instead of the PSNR, for example the Structural Similarity Index (SSIM) <ref type="bibr" target="#b39">[40]</ref>. Learning is an optimization on the parameters θ to maximize</p><formula xml:id="formula_11">M (θ) = 1 n n i=1 L(A(y i , θ), t i ),<label>(13)</label></formula><p>the average PSNR obtained after running the denoising algorithm A(y i , θ) with parameters θ on the 40 training examples y i .</p><p>3) Optimization: In this work, coordinate ascent was used for maximizing the loss function. Coordinate ascent is a greedy iterative optimization algorithm in which at each step, one of the variables θ i of the current state θ is chosen at random and its value is modified by a small amount (0.0001 to 0.001 in our experiments) if M (θ) does not decrease. If the M (θ) decreases, the variable θ i is rolled back to its old value. For our problem, each filter is constrained to have a zero-sum so we modified the coordinate ascent so that when a filter is selected to be modified, two locations inside the filter are chosen randomly and modified by the same small amount, but with opposite signs. This way the filters always remain zerosum.</p><p>We also experimented with gradient ascent, conjugate gradient and the simplex method <ref type="bibr" target="#b23">[24]</ref>. For this particular application, we observed that these other methods could not find such a strong optimum as the coordinate ascent. This is probably because the optimum path is very narrow and a fast algorithm could not follow it properly. Other optimization methods such as genetic algorithms <ref type="bibr" target="#b13">[14]</ref> or simulated annealing <ref type="bibr" target="#b15">[16]</ref> could be more appropriate for avoiding local optima and are subject to further investigation. The one iteration parameters were trained first, for the level of noise σ = 25. For the one iteration parameters, the coefficients α f can be well approximated analytically as the solution of the least squares problem:</p><formula xml:id="formula_12">40 i=1 ||t i − x i − δ N f =1 α f g i f || 2 ,</formula><p>where</p><formula xml:id="formula_13">(g i f ) j = J − f * J T f x (j) i 1 + 1 2 (J T f x (j) i ) 2<label>(14)</label></formula><p>This leaves only the value of the filters F f , f = 1, ..., N for optimization. At each step of the optimization, the coefficients α f are obtained by solving the above least squares problem and then M (θ) is evaluated. This technique is know as RaoBlackwellization <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Since the function M (θ) is not convex, the optimization is prone to be stuck in local maxima. To alleviate this problem, the one iteration filters for σ = 25 are trained using a simplified version of Marginal Space Learning <ref type="bibr" target="#b44">[45]</ref>. Marginal Space Learning is an optimization procedure aimed at finding optima in high dimensional spaces by propagating a set of particles in a sequence of spaces of increasing dimensions until the full parameter space is reached. In our case, a  <ref type="figure">Fig. 8</ref>. PSNR evolution on the training and test set observed while training the one iteration (n iter = 1) ARF parameters for the level of noise σ = 25. Also displayed in dotted lines are the training and testing PSNR of a GMRF with the same model complexity, described in section III-G. single particle (maximum) is propagated starting from the small dimensional space of parameters of only one filter and gradually increasing the dimensionality by adding filters or by increasing the filter size. Each time the dimensionality is increased, the position of the particle in the larger Marginal Space is searched by Coordinate Ascent.</p><p>More specifically, the Marginal Space Learning procedure is started with one filter of size 3 × 3 with all entries 0 except on the first row, F 1 (1, 1) = 0.1, F 1 (1, 2) = −0.1. Starting from this initial setting, the PSNR optimization was run until not much improvement in M (θ) was observed. This is the location of the particle in the first Marginal Space. Then the parameter space was enlarged by adding another filter with all entries 0 and optimizing for 3000 steps, obtaining the particle position in the second space. The process of increasing the Marginal Space by adding one filter and retraining was repeated until there were a total of five 3 × 3 filters. Then the Marginal Space was enlarged by increasing the filter size to 5 × 5 by padding zeros on the border of each filter. The new position of the particle (maximum) was searched through 3000 steps of optimization. The process of enlarging the Marginal Space by adding filters (now of size 5 × 5) and retraining was repeated until the number of filters reached N = 13. This</p><formula xml:id="formula_14">nn iterr =11 σσ =155 nn iterr =22 σσ =155 nn iterr =11 σσ =100 nn iterr =22 σσ =100 nn iterr =11 σσ =200 nn iterr =22 σσ =200 nn iterr =11 σσ =255 nn iterr =22 σσ =255 nn iterr =11 σσ =500 nn iterr =22 σσ =500 nn iterr =33 σσ =100 nn iterr =44 σσ =100 nn iterr =44 σσ =155 nn iterr =33 σσ =155 nn iterr =33 σσ =200 nn iterr =44 σσ =200 nn iterr =44 σσ =255 nn iterr =33 σσ =255 nn iterr =33 σσ =500</formula><p>nn iterr =44 σσ =500 <ref type="figure">Fig. 9</ref>. Diagram of the training of the ARF parameters for different levels of noise and numbers of iterations of the steepest descent inference algorithm. The double lines mean that the filters are the same. number was chosen by observing on the validation set that no further improvement in PSNR could be obtained. The whole procedure is illustrated in <ref type="figure" target="#fig_5">Figure 7</ref>. The evolution of the PSNR over all this training, starting with one 3 × 3 filter and ending with thirteen 5 × 5 filters is plotted in <ref type="figure">Figure 8</ref>. Training the 5 filters of size 3 × 3 takes about 7 hours on a dual-core 2.4Ghz PC while the whole training for the one iteration σ = 25 filters takes about 3 days.</p><p>Since the optimization is prone to be stuck in local optima, the other filters are initialized from already trained filters in the order presented in <ref type="figure">Figure 9</ref>. The 3-iteration filters are well trained to perform iterative denoising and can also be used for 4-iterations without any modifications.</p><p>Training each of the arrows in <ref type="figure">Figure 9</ref> takes about one day on a 8-core 2Ghz PC. We believe that by using better optimization algorithms, the training time can be further improved. The trained 5 × 5 filters for σ = 20 and n iter = 3 are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. 4) Overfitting: As already mentioned, we initially used images of size 15×15 as in <ref type="bibr" target="#b28">[29]</ref> and observed that the training PSNR was increasing significantly while the PSNR on the validation set was actually decreasing, a sign of overfitting. We experimented with increasing the size of the training images and observed that this alleviated the overfitting problem. Finally we observed that when the training images were at least 250 × 250, there were no signs of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>The performance of the Active Random Field system is evaluated on the same datasets as <ref type="bibr" target="#b28">[29]</ref>. First, results on some standard images -Lena, Barbara, Boats, House and Peppersat the noise level σ = 25 are shown in <ref type="table" target="#tab_1">Table I</ref>. Note that these images were not used for training. The ARF results are obtained between 7 and 3000 times faster than the other methods.</p><p>For further comparison, <ref type="table" target="#tab_1">Table II</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> present results on the same 68 test images from the Berkeley dataset as <ref type="bibr" target="#b28">[29]</ref>. Note that these images were also not used for training. We present results for 1: Wiener filter, 2: Nonlinear Diffusion [41] using the nonlinear diffusion Matlab toolbox provided by Frederico D'Almeida, with parameters λ = 8, σ = 1, diffusivity = 3, step size = 8, steps = 2, and with the aos option, 3: Non-local means <ref type="bibr" target="#b5">[6]</ref> with search window 17 × 17, similarity window 9 × 9 and h tuned for best results for each σ, 4: Fields of Experts (FOE) <ref type="bibr" target="#b28">[29]</ref> with 3000 iterations, 5,6,7,8: our algorithm with 1,2,3,4 iterations, 9: wavelet based denoising <ref type="bibr" target="#b26">[27]</ref>, 10: Overcomplete DCT <ref type="bibr" target="#b9">[10]</ref>, 11: KSVD <ref type="bibr" target="#b9">[10]</ref> and 12: BM3D <ref type="bibr" target="#b8">[9]</ref>. Since this evaluation is on 68 images, it should be regarded as a more thorough evaluation than the results on 5 specific images. We should mention that all other algorithms were run on Matlab code provided by their authors and were not implemented by us.</p><p>From the evaluation, it is clear that the one iteration ARF is on par with the FOE while being 3000 times faster. Therefore, training the MRF model together with a suboptimal inference algorithm offers significant advantages in speed and accuracy. One could also observe that the ARF is within 0.5dB from the best method and it is outperformed by two methods: KSVD [10], BM3D <ref type="bibr" target="#b8">[9]</ref> and for some noise levels by wavelet denoising <ref type="bibr" target="#b26">[27]</ref> and overcomplete DCT <ref type="bibr" target="#b9">[10]</ref>.</p><p>Depending on application, trade-offs between speed and accuracy might be important. <ref type="figure" target="#fig_0">Figure 11</ref> shows a plot of the PSNR performance in dB of the algorithms compared above as a function of the processing speed in fps. From the figure, one can see that the Active Random Fields are very competitive candidates when high processing speeds are required such as in real-time medical applications.</p><p>The computation complexity of the ARF image denoising algorithm is due to the necessity of performing 2N convolutions (where N is the number of filters) for each iteration. A standard Matlab implementation takes about 0.8s for each iteration on a 256×256 image and a 2.4GHz PC. A better C++ implementation using IPL (Intel Image Processing Library)   <ref type="table" target="#tab_1">Table II.</ref> cuts computation time to 0.12s per iteration for the same image size. Furthermore, a parallel implementation on multiple CPUs and/or a GPU implementation could bring this algorithm to real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. A Study of the One Iteration Algorithms</head><p>It is intriguing that such results could be obtained in a single gradient descent iteration. Could this be due to the specially trained filters F j or to the filter coefficients α j ? In this section we perform more experiments on different one iteration algorithms to determine the cause of this performance. First, we plot in <ref type="figure" target="#fig_0">Figure 12</ref> the performance of the 1-iteration ARF in the range σ ∈ <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">50]</ref>. For comparison, the FOE performance is also displayed. It is clear that the ARF algorithms are specialized at the levels of noise they were trained for.</p><p>Then we evaluated the FOE model with a single iteration gradient descent algorithm on the same 68 images from the Berkeley dataset, choosing the step size δ to maximize the PSNR. The results are presented in the second row in <ref type="table" target="#tab_1">Table  III</ref> and in <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>Another 1-iteration FOE model was obtained by retraining the coefficients α j for each σ to maximize the PSNR on the training data, while keeping the original filters F j . These results are posted in the third row in Table III and in <ref type="figure" target="#fig_0">Figure 13</ref>. For each algorithm are also displayed in Table III the number of parameters that are trained for each noise level σ.</p><p>The fourth row of the table displays an evaluation of the FOE filters when the filter norms ν j (i.e. scalar multipliers</p><formula xml:id="formula_15">F j = ν j F F OE j</formula><p>) and their coefficients α j were trained for each value of σ by maximizing the PSNR on the training set.</p><p>The fifth row in <ref type="table" target="#tab_1">Table III</ref> presents the performance of a 1-iteration ARF that was trained on images corrupted with noise levels σ = 15 and 25 and no data term. Observe that the performance decreased slightly while the noise range was extended.</p><p>In the 1-iteration ARF, the data term has theoretically no influence, since before the iteration x − y = 0. We slightly modified the 1-iteration ARF to a 2-step version that bears the same computational expense but can take into account the data term:</p><formula xml:id="formula_16">1. x ← y + δ N f =1 α f J − f * J T f y 1 + 1 2 (J T f y) 2 2. x ← x + δ β σ 2σ 2 (x − y)<label>(15)</label></formula><p>where the data term has a coefficient β σ that depends on σ, as in <ref type="bibr" target="#b28">[29]</ref>. This can be also written in a single iteration as</p><formula xml:id="formula_17">x ← y + δ N f =1 α f (1 + β σ 2σ 2 )J − f * J T f y 1 + 1 2 (J T f y) 2 (16)</formula><p>The last two rows of <ref type="table" target="#tab_1">Table III</ref> and the red and green solid lines in <ref type="figure" target="#fig_0">Figure 13</ref> show results obtained with this modified 1-iteration ARF. The first one is trained with images corrupted with noise levels σ = 15 and 25, while the second one with images corrupted with noise levels σ = 15 and 40. One can see that the band-pass behavior disappeared after the introduction of the data term.</p><p>The FOE with the coefficients and norms retrained at each noise level (fourth row in <ref type="table" target="#tab_1">Table III</ref>) has a very good overall performance. However, compared to the ARF algorithms displayed in <ref type="table" target="#tab_1">Table III</ref>, it has 48 times more parameters (24 norms and 24 coefficients) that are trained at each noise level.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Relation to M-estimation and Gaussian CRF</head><p>In this section we will use a standard statistical method named M-estimation <ref type="bibr" target="#b10">[11]</ref>, p. 99, which maps robust regression to an iterative weighted regression.</p><p>The FOE energy with a data term can be written as:</p><formula xml:id="formula_18">E F OE (x) = N f =1 j α f ρ(J T f x (j) ) + β(x − y) T (x − y) = N f =1 j α f ρ(J T f,j x) + β(x − y) T (x − y)<label>(17)</label></formula><p>where ρ(x) = log(1 + x 2 /2) is the Lorentzian and J f,j is the j-th column of J f .</p><p>Thus minimizing E F OE (x) means solving a robust regression problem. One commonly used algorithm for robust regression is the iterative weighted regression using M-estimation, <ref type="bibr" target="#b10">[11]</ref>, p. 99.</p><p>Taking the derivative with respect to x (k) and setting it to zero, gives:   Filter responses, sigma=25 which can be written as</p><formula xml:id="formula_19">∂ ∂x (k) E F OE (x) = N f =1 j α f ρ ′ (J T f,j x)J f,j,k +2β(x−y) = 0,<label>(18)</label></formula><formula xml:id="formula_20">N f =1 j α f 1 1 + (J T f,j x) 2 /2 J f,j,k J T f,j x + 2β(x − y) = 0 (19)</formula><p>By fixing w j,f = α f /(1 + (J T f,j x) 2 /2), observe that (19) can be regarded as solving (minimizing) the weighted least squares</p><formula xml:id="formula_21">E w (x) = N f =1 j w jf (J T f,j x) 2 + β(x − y) T (x − y)<label>(20)</label></formula><p>This leads to a fixed point solution to the M-estimation problem by starting with an initial x = x 0 and iteratively minimizing <ref type="bibr" target="#b19">(20)</ref> and updating the weights w j,f = α f /(1 + (J T f,j x) 2 /2) based on the newly obtained x.</p><p>Since the weighted sum of squares from eq. <ref type="formula" target="#formula_1">(20)</ref> is exactly the energy of a Gaussian Conditional Random Field <ref type="bibr" target="#b32">[33]</ref> </p><formula xml:id="formula_22">E GCRF (x) = N f =1 j w f (y) (j) (J f * x (j) − r f (y) (j) ) 2 (21)</formula><p>we obtain that the FOE energy can be minimized in an iterative way by alternatively minimizing a GCRF energy <ref type="bibr" target="#b19">(20)</ref> and updating the weights as described above.</p><p>In the ARF approach presented in this paper, a simpler approach to minimizing the FOE energy is taken as a few iterations of gradient descent. This eliminates the need to solve costly least squares problems and it works very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Comparison to Gaussian MRF</head><p>Finally, to see how close the one iteration ARF is to exact inference on a Gaussian MRF with energy</p><formula xml:id="formula_23">E GRF (x) = x T Σ −1 x + (x − y) T (x − y),<label>(22)</label></formula><p>we get the exact minimum atˆx atˆ atˆx = (2Σ −1 + I) −1 y = Ay.</p><p>By limiting to only short-range interactions, (23) can be seen as a filtering operationˆx</p><formula xml:id="formula_25">operationˆ operationˆx = F * y.<label>(24)</label></formula><p>Comparing this to the one-iteration ARF equation e.g. (16), the Gaussian MRF solution above <ref type="formula" target="#formula_1">(24)</ref> is linear whereas the one iteration ARF is non-linear. To see how close the one iteration ARF is to being a linear operator, we obtained a histogram of the filter responses over the test set, displayed in <ref type="figure" target="#fig_0">Figure 14</ref>. Approximately 8% of the filter responses fall outside the interval <ref type="bibr">[−1, 1]</ref> where the Lorentzian ρ(x) = log(1 + x 2 /2) can be well approximated with a Gaussian, i.e. where the derivative ρ ′ (x) (overlaid as a dashed curve in <ref type="figure" target="#fig_0">Figure 14)</ref> is approximately linear.</p><p>To compare how well the GMRF can denoise with the same model complexity, we trained it on the same 40 images from the Berkeley dataset. We used the same Marginal Space Learning approach as for training the ARF, but now there is only one filter that is started as the 1 × 1 identity filter and is enlarged by placing zeros on the border followed by 3000 iterations of coordinate ascent with the same PSNR loss function as the ARF. The enlarging and 3000 iteration optimization were alternated until the filter had size 19 × 19, which has slightly more parameters as 13 FOE filters of size 5 × 5. The PSNR on the training and test set are plotted in <ref type="figure">Figure 8</ref>. One could see that the Fields of Experts has better modeling power and it is better suited for image denoising, achieving more than 1.5dB improvement when compared to the GMRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION AND CONCLUSIONS</head><p>Wainwright <ref type="bibr" target="#b38">[39]</ref> predicted that in computationally restricted applications, models trained using MAP estimation might not be the best choice and biased models might give better results. In this paper, we studied what biased models can give us for real-time image denoising. We defined Active Random Field as the combination of a MRF/CRF with a fast and suboptimal inference algorithm and trained this combination using pairs of input and desired output as well as a benchmark error measure (loss function). This training approach does not need to evaluate the MRF normalization constant and can use a validation set to detect when the training is completed and whether overfitting occurs.</p><p>Applied to image denoising, experiments show that considerable gains in speed and accuracy are obtained when compared to the standard MRF formulation. Moreover, the obtained results are comparable in terms of accuracy with the state of the art while being faster.</p><p>A direct practical application of this method is denoising fluoroscopy (X-ray) sequences, where one could use pairs of low-dose (noisy) and high-dose (good quality) X-rays obtained from cadavers or phantoms to train a similar Active Random Field based real-time image denoising system. This type of training can be used in other applications where fast MRF inference is desired on models with a large number of parameters, for example super-resolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b42">[43]</ref>. If the number of model parameters is small, the model might not be flexible enough to adapt to the fast inference algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Markov Random Field model makes use of an inference algorithm to solve a given task (image denoising in this example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Markov Random Field model is usually trained independent of the inference algorithm. The impact of training the model with the inference algorithm will be studied in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An Active Random Field is a MRF/CRF model trained together with a fast inference algorithm using pairs of input and desired output as training examples. Training is achieved by optimizing a loss function that evaluates how well the given model+algorithm combination solves the given task (image denoising in this example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Image denoising example. Top, from left to right: original image, image corrupted with additive Gaussian noise with σ = 25, PSNR=20.17; our result, PSNR=28.94, 0.6 seconds and Fields of Experts result [29], PSNR=28.67, 2280 seconds. Bottom results, from left to right: wavelet denoising [27], PSNR=29.05, 16 seconds; overcomplete DCT, PSNR=28.81, 38 seconds; KSVD [10], PSNR=29.02, 250 seconds and BM3D [9], PSNR=29.60, 4.3 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The thirteen 5 × 5 Active Random Field filters trained for the level of noise σ = 20 and for a one iteration (n iter = 1, top) and three iteration (n iter = 3, bottom) steepest descent inference algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Training diagram for Active Random Field parameters for the level of noise σ = 25 and the one iteration (n iter = 1) steepest descent inference algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Average PSNR in dB for different image denoising algorithms at different noise levels on 68 images from the Berkeley dataset. 1: Wiener Filter, 2: nonlinear diffusion, 3: Non-local means [6] 4: Fields of Experts [29], 5,6,7,8: our algorithm with 1,2,3 and 4 iterations, 9: wavelet based denoising [27], 10: Overcomplete DCT [10], 11: KSVD [10] and 12: BM3D [9]. The results are also shown in Table II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. PSNR (dB) vs speed (fps) of different denoising algorithms on the 68 Berkeley test images for σ = 25. The colors are the same as in Figure 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. PSNR (dB) curve of the 1-iteration denoising algorithms from Table II and Figure 10 for all noise levels in the range σ ∈ [10, 50] on the 68 Berkeley test images. The 3000 iteration FOE algorithm was also included for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. PSNR (dB) curve of different 1-iteration denoising algorithms at different noise levels on the 68 Berkeley test images. The 3000 iteration FOE algorithm was also included for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Histogram of the filter responses over the training set for the one iteration algorithm for σ = 25. Approximately 8% of the responses fall outside the interval where the robust function ρ(x) = log(1 + x 2 /2) behaves like a gaussian, i.e. where the derivative ρ ′ (x) (displayed as a dashed curve) is almost linear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TABLE I PERFORMANCE EVALUATION AND COMPARISON OF OUR METHOD (1-4 ITERATIONS) WITH OTHER METHODS ON SOME STANDARD BENCHMARK IMAGES, σ = 25. THE ARF RESULTS ARE OBTAINED 7-3000 TIMES FASTER.</head><label>I</label><figDesc></figDesc><table>Image 
Lena 
Barbara Boats House 
Peppers 
Average 
Fields of Experts [29] 
30.82 
27.04 
28.72 
31.11 
29.20 
29.38 
1-iteration ARF 
30.15 
27.10 
28.66 
30.14 
28.90 
28.99 
2-iteration ARF 
30.66 
27.49 
28.99 
30.80 
29.31 
29.45 
3-iteration ARF 
30.76 
27.57 
29.08 
31.04 
29.45 
29.58 
4-iteration ARF 
30.86 
27.59 
29.14 
31.18 
29.51 
29.66 
Wavelet Denoising [27] 
31.69 
29.13 
29.37 
31.40 
29.21 
30.16 
Overcomplete DCT [10] 
30.89 
28.65 
28.78 
31.03 
29.01 
29.67 
Globally Trained Dictionary [10] 
31.20 
27.57 
29.17 
31.82 
29.84 
29.92 
KSVD [10] 
31.32 
29.60 
29.28 
32.15 
29.73 
30.42 
BM3D [9] 
32.08 
30.72 
29.91 
32.86 
30.16 
31.15 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>TABLE II PERFORMANCE EVALUATION OF DIFFERENT DENOISING METHODS ON 68 IMAGES FROM THE BERKELEY DATASET. AVERAGE PSNR OF THE</head><label>II</label><figDesc></figDesc><table>DENOISING RESULTS OBTAINED BY THE METHODS AT DIFFERENT NOISE 
LEVELS. 

Level of Noise σ 
10 
15 
20 
25 
50 
1. Wiener Filter 
31.65 29.18 27.53 26.37 22.94 
2. Nonlinear Diffusion [41] 
32.03 29.83 28.28 27.25 24.73 
3. Non-local [6] 
31.48 29.86 28.62 27.59 24.22 
4. Fields of Experts [29] 
32.68 30.50 28.78 27.60 23.25 
5. 1-iteration ARF 
32.74 30.57 28.92 27.77 24.58 
6. 2-iteration ARF 
32.74 30.70 29.23 28.10 24.88 
7. 3-iteration ARF 
32.84 30.76 29.29 28.17 25.11 
8. 4-iteration ARF 
32.82 30.76 29.33 28.24 25.14 
9. Wavelet Denoising [27] 
33.05 30.73 29.18 28.03 25.37 
10. Overcomplete DCT [10] 33.19 30.75 29.15 27.98 24.86 
11. KSVD [10] 
33.30 30.96 29.43 28.33 25.20 
12. BM3D [9] 
33.53 31.21 29.71 28.63 25.47 1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

Algorithm, sigma=10 (PSNR=28.13) 

PSNR 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

Algorithm, sigma=15 (PSNR=24.61) 

PSNR 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 

25 

26 

27 

28 

29 

30 

31 

32 

33 

34 

35 

36 

Algorithm, sigma=20 (PSNR=22.11) 

PSNR 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 11 12 
21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>TABLE III TESTING ERRORS ON 68 UNSEEN BERKELEY IMAGES WHEN TRAINING DIFFERENT 1-ITERATION ARF ALGORITHMS.</head><label>III</label><figDesc></figDesc><table>Algorithm 
Number of σ-dependent params 
10 
15 
20 
25 
50 
FOE [29] 
1 
32.68 30.50 28.78 27.60 23.25 
1-iteration FOE 
1 
29.57 25.96 23.35 21.31 14.92 
1-iter. FOE, retrained coeffs 
24 
30.23 26.63 24.00 21.92 15.39 
1-iter. FOE, retrained coeffs &amp; norms 
48 
32.29 29.73 27.99 26.69 22.39 
ARF, no data term 
0 
30.13 29.89 28.99 27.40 18.69 
ARF w/data term, trained with σ ∈ [15, 25] 
1 
31.99 30.37 28.99 27.63 20.26 
ARF w/data term, train with σ ∈ [15, 40] 
1 
31.13 29.55 28.56 27.72 23.38 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hidden Markov Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Real-Time MRF Inference for Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the statistical analysis of dirty pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="302" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Doksum</surname></persName>
		</author>
		<title level="m">Mathematical Statistics: Basic Ideas and Selected Topics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Non-Local Algorithm for Image Denoising. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rao-Blackwellisation of sampling schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Linear Models with R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Faraway</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mutual boosting for contextual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization, and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley Pub. Co Reading</publisher>
			<pubPlace>Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training Products of Experts by Minimizing Contrastive Divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Optimization by Simulated Annealing. Biology and Computation: A Physicist&apos;s Choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Gelati</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vecchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative random fields: a discriminative framework for contextual interaction in classification. Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Ninth IEEE International Conference on</title>
		<meeting>Ninth IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning table of contents</title>
		<meeting>the Eighteenth International Conference on Machine Learning table of contents</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10-thInternational Workshop on Artificial Intelligence and Statistics (AIStats 05</title>
		<meeting>of the 10-thInternational Workshop on Artificial Intelligence and Statistics (AIStats 05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning for optical flow using stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV01</title>
		<meeting>of ICCV01</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yanover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="428" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simplex method for function minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The computer journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">308</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward automatic phenotyping of developing embryos from videos. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Delhomme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Barbano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1360" to="1371" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating the Probability of the Presence of a Signal of Interest in Multiresolution Single-and Multiband Image Denoising. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pizurica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="654" to="665" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of Gaussians in the wavelet domain. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some generalized order-disorder transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Camb. Phil. Soc</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="106" to="109" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fields of Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning conditional random fields for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR &apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching using belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="787" to="800" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Comparison of graph cuts with belief propagation for stereo, using identical MRF parameters. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="900" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning gaussian conditional random fields for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR &apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Utilizing Variational Optimization to Learn Markov Random Fields. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured Prediction via the Extragradient Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1345</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contextual models for object detection using boosted random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="369" to="409" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating the &quot;wrong&quot; graphical model: Benefits in the computation-limited setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1829" to="1859" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Review of Nonlinear Diffusion Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Scale-Space Theory in Computer Vision</title>
		<meeting>the First International Conference on Scale-Space Theory in Computer Vision</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="3" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SATzilla: Portfoliobased Algorithm Selection for SAT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="565" to="606" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="689" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Four-Chamber heart modeling and automatic segmentation for 3-D cardiac CT volumes using marginal space learning and steerable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheuering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1668" to="1681" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shape Regression Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Information Processing in Medical Imaging</title>
		<meeting>Information Processing in Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
