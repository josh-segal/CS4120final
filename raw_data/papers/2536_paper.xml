<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Domain Knowledge into Topic Modeling via Dirichlet Forest Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Sciences</orgName>
								<orgName type="department" key="dep2">†Department of Biostatistics and Medical Informatics</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Sciences</orgName>
								<orgName type="department" key="dep2">†Department of Biostatistics and Medical Informatics</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
							<email>craven@biostat.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Sciences</orgName>
								<orgName type="department" key="dep2">†Department of Biostatistics and Medical Informatics</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Domain Knowledge into Topic Modeling via Dirichlet Forest Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model&apos;s ability to follow and generalize beyond user-specified domain knowledge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Topic modeling, using approaches such as Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, has enjoyed popularity as a way to model hidden topics in data. However, in many applications, a user may have additional knowledge about the composition of words that should have high probability in various topics. For example, in a biological application, one may prefer that the words "termination", "disassembly" and "release" appear with high probability in the same topic, because they all describe the same phase of biological processes. Furthermore, a biologist could automatically extract these preferences from an existing biomedical ontology, such as the Gene Ontology (GO) (The Gene Ontology <ref type="bibr">Consortium, 2000</ref>). As another example, an analyst may run topic modeling on a corpus of people's wishes, inspect the resulting topics, and notice that "into, college" and "cure, cancer" all ap- pear with high probability in the same topic. The analyst may want to interactively express the preference that the two sets of words should not appear together, re-run topic modeling, and incorporate additional preferences based on the new results. In both cases, we would like these preferences to guide the recovery of latent topics. Standard LDA lacks a mechanism for incorporating such domain knowledge.</p><p>In this paper, we propose a principled approach to the incorporation of such domain knowledge into LDA. We show that many types of knowledge can be expressed with two primitives on word pairs. Borrowing names from the constrained clustering literature ( <ref type="bibr" target="#b0">Basu et al., 2008)</ref>, we call the two primitives Must-Links and Cannot-Links, although there are important differences. We then encode the set of Must-Links and Cannot-Links associated with the domain knowledge using a Dirichlet Forest prior, replacing the Dirichlet prior over the topic-word multinomial p(word|topic). The Dirichlet Forest prior is a mixture of Dirichlet tree distributions with very specific tree structures. Our approach has several advantages: (i) A Dirichlet Forest can encode Must-Links and Cannot-Links, something impossible with Dirichlet distributions. (ii) The user can control the strength of the domain knowledge by setting a parameter η, allowing domain knowledge to be overridden if the data strongly suggest otherwise. (iii) The Dirichlet Forest lends itself to efficient inference via collapsed Gibbs sampling, a property inherited from the conjugacy of Dirichlet trees. We present experiments on several synthetic datasets and two real domains, demonstrating that the resulting topics not only successfully incorporate the specified domain knowledge, but also generalize beyond it by including/excluding other related words not explicitly mentioned in the Must-Links and Cannot-Links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Domain Knowledge into Topic Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review LDA using the notation of <ref type="bibr" target="#b6">Griffiths and Steyvers (2004)</ref>. Let there be T topics. Let w = w 1 . . . w n represent a corpus of D documents, with a total of n words. We use d i to denote the document of word w i , and z i the hidden topic from which w i is generated. Let φ (w) j = p(w|z = j), and θ</p><formula xml:id="formula_0">(d) j = p(z = j) for document d. The LDA generative model is then: θ ∼ Dirichlet(α) (1) z i |θ (di) ∼ Multinomial(θ (di) ) (2) φ ∼ Dirichlet(β) (3) w i |z i , φ ∼ Multinomial(φ zi )<label>(4)</label></formula><p>where α and β are hyperparameters for the documenttopic and topic-word Dirichlet distributions, respectively. For simplicity we will assume symmetric α and β, but asymmetric hyperparameters are also possible.</p><p>Previous work has modeled correlations in the LDA document-topic mixtures using the logistic Normal distribution <ref type="bibr" target="#b1">(Blei &amp; Lafferty, 2006</ref>), DAG (Pachinko) structures <ref type="bibr" target="#b8">(Li &amp; McCallum, 2006</ref>), or the Dirichlet Tree distribution <ref type="bibr" target="#b10">(Tam &amp; Schultz, 2007)</ref>. In addition, the concept-topic model ( <ref type="bibr" target="#b3">Chemudugunta et al., 2008</ref>) employs domain knowledge through special "concept" topics, in which only a particular set of words can be present. Our work complements the previous work by encoding complex domain knowledge on words (especially arbitrary Cannot-Links) into a flexible and computationally efficient prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Topic Modeling with Dirichlet Forest</head><p>Our proposed model differs from LDA in the way φ is generated. Instead of (3), we have</p><formula xml:id="formula_1">q ∼ DirichletForest(β, η) φ ∼ DirichletTree(q)</formula><p>where q specifies a Dirichlet tree distribution, β plays a role analogous to the topic-word hyperparameter in standard LDA, and η ≥ 1 is the "strength parameter" of the domain knowledge. Before discussing DirichletForest(β, η) and DirichletTree(q), we first explain how knowledge can be expressed using MustLink and Cannot-Link primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Must-Links and Cannot-Links</head><p>Must-Links and Cannot-Links were originally proposed for constrained clustering to encourage two instances to fall into the same cluster or into separate clusters, respectively. We borrow the notion for topic modeling. Informally, the Must-Link primitive prefers that two words tend to be generated by the same topic, while the Cannot-Link primitive prefers that two words tend to be generated by separate topics. However, since any topic φ is a multinomial over words, any two words (in general) always have some probability of being generated by the topic. We therefore propose the following definition:</p><p>Must-Link (u, v): Two words u, v have similar probability within any topic, i.e., φ</p><formula xml:id="formula_2">(u) j ≈ φ (v) j for j = 1 . . . T .</formula><p>It is important to note that the probabilities can be both large or both small, as long as they are similar. For example, for the earlier biology example we could say Must-Link (termination, disassembly).</p><p>Cannot-Link (u, v): Two words u, v should not both have large probability within any topic. It is permissible for one to have a large probability and the other small, or both small. For example, one primitive for the wish example can be Cannot-Link (college, cure).</p><p>Many types of domain knowledge can be decomposed into a set of Must-Links and Cannot-Links. We demonstrate three types in our experiments: we can Split two or more sets of words from a single topic into different topics by placing Must-Links within the sets and Cannot-Links between them. We can Merge two or more sets of words from different topics into one topic by placing Must-Links among the sets. Given a common set of words which appear in multiple topics (such as stopwords in English, which tend to appear in all LDA topics), we can Isolate them by placing Must-Links within the common set, and then placing Cannot-Link between the common set and the other high-probability words from all topics. It is important to note that our Must-Links and Cannot-Links are preferences instead of hard constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoding Must-Links</head><p>It is well-known that the Dirichlet distribution is limited in that all words share a common variance parameter, and are mutually independent except the normalization constraint <ref type="bibr" target="#b9">(Minka, 1999</ref>). However, for MustLink (u, v) it is crucial to control the two words u, v differently than other words.</p><p>The Dirichlet tree distribution <ref type="bibr" target="#b4">(Dennis III, 1991</ref>) is a generalization of the Dirichlet distribution that allows such control. It is a tree with the words as leaf nodes; see <ref type="figure" target="#fig_2">Figure 1</ref>(a) for an example. Let γ (k) be the Dirichlet tree edge weight leading into node k. Let C(k) be the immediate children of node k in the tree, L the leaves of the tree, I the internal nodes, and L(k) the leaves in the subtree under k. To generate a sample φ ∼ DirichletTree(γ), one first draws a multinomial at each internal node s ∈ I from Dirichlet(γ C(s) ), i.e., using the weights from s to its children as the Dirichlet parameters. One can think of it as re-distributing the probability mass reaching s by this multinomial (initially, the mass is 1 at the root). The probability φ (k) of a word k ∈ L is then simply the product of the multinomial parameters on the edges from k to the root, as shown in <ref type="figure" target="#fig_2">Figure 1(b)</ref>. It can be shown <ref type="bibr" target="#b4">(Dennis III, 1991</ref>) that this procedure gives</p><formula xml:id="formula_3">DirichletTree(γ) ≡ p(φ|γ) = L k φ (k) γ (k) −1    I s Γ C(s) k γ (k) C(s) k Γ γ (k)   L(s) k φ (k)   ∆(s)   </formula><p>where Γ(·) is the standard gamma function, and the notation</p><formula xml:id="formula_4">L k means k∈L . The function ∆(s) ≡ γ (s) − k∈C(s) γ (k)</formula><p>is the difference between the in-degree and out-degree of internal node s. When this difference ∆(s) = 0 for all internal nodes s ∈ I, the Dirichlet tree reduces to a Dirichlet distribution.</p><p>Like the Dirichlet, the Dirichlet tree is conjugate to the multinomial. It is possible to integrate out φ to get a distribution over word counts directly, similar to the multivariate Pólya distribution:</p><formula xml:id="formula_5">p(w|γ) = I s   Γ C(s) k γ (k) Γ C(s) k γ (k) + n (k) C(s) k Γ γ (k) + n (k) Γ(γ (k) )   (5)</formula><p>Here n (k) is the number of word tokens in w that appear in L(k).</p><p>We encode Must-Links using a Dirichlet tree. Note that our definition of Must-Link is transitive: MustLink (u, v) and Must-Link (v, w) imply Must-Link (u, w). We thus first compute the transitive closures of expressed Must-Links. Our Dirichlet tree for MustLinks has a very simple structure: each transitive closure is a subtree, with one internal node and the words in the closure as its leaves. The weights from the internal node to its leaves are ηβ. The root connects to these internal nodes s with weight |L(s)|β, where | · | represents the set size. In addition, the root directly connects to other words not in any closure, with weight β. For example, the transitive closure for a Must-Link (A,B) on vocabulary {A,B,C} is simply {A,B}, corresponding to the Dirichlet tree in <ref type="figure" target="#fig_2">Figure 1</ref>(a).</p><p>To understand this encoding of Must-Links, consider first the case when the domain knowledge strength parameter is at its weakest η = 1. Then in-degree equals out-degree for any internal node s (both are |L(s)|β), and the tree reduces to a Dirichlet distribution with symmetric prior β: the Must-Links are turned off in this case. As we increase η, the re-distribution of probability mass at s (governed by a Dirichlet under s) has increasing concentration |L(s)|ηβ but the same uniform base-measure. This tends to redistribute the mass evenly in the transitive closure represented by s. Therefore, the Must-Links are turned on when η &gt; 1. Furthermore, the mass reaching s is independent of η, and can still have a large variance. This properly encodes the fact that we want Must-Linked words to have similar, but not always large, probabilities. Otherwise, Must-Linked words would be forced to appear with large probability in all topics, which is clearly undesirable. This is impossible to represent with Dirichlet distributions. For example, the blue dots in <ref type="figure">Fig</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encoding Cannot-Links</head><p>Cannot-Links are considerably harder to handle. We first transform them into an alternative form that is amenable to Dirichlet trees. Note that Cannot-Links are not transitive: Cannot-Link (A,B) and CannotLink (B,C) does not entail Cannot-Link (A,C). We define a Cannot-Link-graph where the nodes are words 1 , and the edges correspond to the Cannot-Links. Then the connected components of this graph are independent of each other when encoding Cannot-Links. We will use this property to factor a Dirichlet-tree selection probability later. For example, the two CannotLinks (A,B) and (B,C) form the graph in <ref type="figure" target="#fig_2">Figure 1</ref>(e) with a single connected component {A,B,C}.</p><p>Consider the subgraph on connected component r.</p><p>We define its complement graph by flipping the edges (on to off, off to on), as shown in <ref type="figure" target="#fig_2">Figure 1</ref>(f). Let there be Q (r) maximal cliques M r1 . . . M rQ (r) in this complement graph. In the following, we simply call them "cliques", but it is important to remember that they are maximal cliques of the complement graph, not the original Cannot-Link-graph. In our example, Q (r) = 2 and M r1 = {A, C}, M r2 = {B}. These cliques have the following interpretation: each clique (e.g., M r1 = {A, C}) is the maximal subset of words in the connected component that can "occur together". That is, these words are allowed to simultaneously have large probabilities in a given topic without violating any Cannot-Link preferences. By the maximality of these cliques, allowing any word outside the clique (e.g., "B") to also have a large probability will violate at least 1 Cannot-Link (in this example 2).</p><p>We discuss the encoding for this single connected component r now, deferring discussion of the complete encoding to section 3.4. We create a mixture model of Q (r) Dirichlet subtrees, one for each clique. Each topic selects exactly one subtree according to probability</p><formula xml:id="formula_6">p(q) ∝ |M rq |, q = 1 . . . Q (r) .<label>(6)</label></formula><p>Conceptually, the selected subtree indexed by q tends to redistribute nearly all probability mass to the words within M rq . Since there is no mass left for other cliques, it is impossible for a word outside clique M rq to have a large probability. Therefore, no CannotLink will be violated. In reality, the subtrees are soft rather than hard, because Cannot-Links are only preferences. The Dirichlet subtree for M rq is structured as follows. The subtree's root connects to an internal node s with weight η|M rq |β. The node s connects to words in M rq , with weight β. The subtree's root also directly connects to words not in M rq (but in the connected component r) with weight β. This will send most probability mass down to s, and then flexibly redistribute it among words in M rq . For example, <ref type="figure" target="#fig_2">Figures 1(g,h)</ref> show the Dirichlet subtrees for M r1 = {A, C} and M r2 = {B} respectively. Samples from this mixture model are shown in <ref type="figure" target="#fig_2">Figure 1(</ref> Finally, we mention that although in the worst case the number of maximal cliques Q (r) in a connected component of size |r| can grow exponentially as O(3 |r|/3 ) ( <ref type="bibr" target="#b7">Griggs et al., 1988)</ref>, in our experiments Q (r) is no larger than 3, due in part to Must-Linked words "collapsing" to single nodes in the Cannot-Link graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The Dirichlet Forest Prior</head><p>In general, our domain knowledge is expressed by a set of Must-Links and Cannot-Links. We first compute the transitive closure of Must-Links. We then form a Cannot-Link-graph, where a node is either a Must-Link closure or a word not present in any Must-Link. Note that the domain knowledge must be "consistent" in that no pairs of words are simultaneously Cannot-Linked and Must-Linked (either explicitly or implicitly through Must-Link transitive closure.) Let R be the number of connected components in the Cannot-Link-graph. Our Dirichlet Forest consists of R r=1 Q (r) Dirichlet trees, represented by the template in <ref type="figure" target="#fig_3">Figure 2</ref>. Each Dirichlet tree has 2 Dirichlet distributions with very small concentration do have some selection effect. For example, Beta(0.1,0.1) tends to concentrate probability mass on one of the two variables. However, such priors are weak -the "pseudo counts" in them are too small because of the small concentration. The posterior will be dominated by the data, and we would lose any encoded domain knowledge.</p><p>R branches beneath the root, one for each connected component. The trees differ in which subtrees they include under these branches. For the r-th branch, there are Q (r) possible Dirichlet subtrees, corresponding to cliques M r1 . . . M rQ <ref type="bibr">(r)</ref> . Therefore, a tree in the forest is uniquely identified by an index vector q = (q (1) . . . q (R) ), where q (r) ∈ {1 . . . Q (r) }. To draw a Dirichlet tree q from the prior DirichletForest(β, η), we select the subtrees independently because the R connected components are independent with respect to Cannot-Links: p(q) = R r=1 p(q (r) ). Each q (r) is sampled according to <ref type="bibr">(6)</ref>, and corresponds to choosing a solid box for the r-th branch in <ref type="figure" target="#fig_3">Figure 2</ref>. The structure of the subtree within the solid box has been defined in Section 3.3. The black nodes may be a single word, or a Must-Link transitive closure having the subtree structure shown in the dotted box. The edge weight leading to most nodes k is γ (k) = |L(k)|β, where L(k) is the set of leaves under k. However, for edges coming out of a Must-Link internal node or going into a Cannot-Link internal node, their weights are multiplied by the strength parameter η. These edges are marked by "η * " in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>We now define the complete Dirichlet Forest model, integrating out ("collapsing") θ and φ. Let n (d) j be the number of word tokens in document d that are assigned to topic j. z is generated the same as in LDA:</p><formula xml:id="formula_7">p(z|α) = Γ(T α) Γ(α) T D D d=1 T j=1 Γ(n (d) j + α) Γ(n (d) · + T α) .</formula><p>There is one Dirichlet tree q j per topic j = 1 . . . T , sampled from the Dirichlet Forest prior p(q j ) = R r=1 p(q (r) j ). Each Dirichlet tree q j implicitly defines its tree edge weights γ (·) j using β, η, and its tree structure L j , I j , C j (·). Let n (k) j be the number of word tokens in the corpus assigned to topic j that appear under the node k in the Dirichlet tree q j . The probability of generating the corpus w, given the trees q 1:T ≡ q 1 . . . q T and the topic assignment z, can be derived using (5): p(w|q 1:T , z,</p><formula xml:id="formula_8">β, η) = T j=1 Ij s   Γ Cj (s) k γ (k) j Γ Cj (s) k (γ (k) j + n (k) j ) Cj (s) k Γ(γ (k) j + n (k) j ) Γ(γ (k) j )   .</formula><p>Finally, the complete generative model is p(w, z, q 1:T |α, β, η) = p(w|q 1:T , z, β, η)p(z|α) T j=1 p(q j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference for Dirichlet Forest</head><p>Because a Dirichlet Forest is a mixture of Dirichlet trees, which are conjugate to multinomials, we can efficiently perform inference by Markov Chain Monte Carlo (MCMC). Specifically, we use collapsed Gibbs sampling similar to <ref type="bibr" target="#b6">Griffiths and Steyvers (2004)</ref>. However, in our case the MCMC state is defined by both the topic labels z and the tree indices q 1:T . An MCMC iteration in our case consists of a sweep through both z and q 1:T . We present the conditional probabilities for collapsed Gibbs sampling below.</p><p>(Sampling z i ): Let n </p><formula xml:id="formula_9">p(z i = v|z −i , q 1:T , w) ∝ (n (d) −i,v + α) Iv(↑i) s γ (Cv(s↓i)) v + n (Cv(s↓i)) −i,v Cv(s) k γ (k) v + n (k) −i,v</formula><p>, where I v (↑ i) denotes the subset of internal nodes in topic v's Dirichlet tree that are ancestors of leaf w i , and C v (s↓i) is the unique node that is s's immediate child and an ancestor of w i (including w i itself).</p><p>(Sampling q </p><formula xml:id="formula_10">p(q (r) j = q ′ |z, q −j , q (−r) j , w) ∝   M rq ′ k β k   × I j,r=q ′ s   Γ Cj (s) k γ (k) j Γ Cj (s) k (γ (k) j + n (k) j ) Cj (s) k Γ(γ (k) j + n (k) j ) Γ(γ (k) j )  </formula><p>where I j,r=q ′ denotes the internal nodes below the r-th branch of tree q j , when clique M rq ′ is selected.</p><p>(Estimating φ and θ): After running MCMC for sufficient iterations, we follow standard practice (e.g. ( <ref type="bibr" target="#b6">Griffiths &amp; Steyvers, 2004)</ref>) and use the last sample (z, q 1:T ) to estimate φ and θ. Because a Dirichlet tree is a conjugate distribution, its posterior is a Dirichlet tree with the same structure and updated edge weights. The posterior for the Dirichlet tree of the j-th topic is γ post (k)</p><formula xml:id="formula_11">j = γ (k) j +n (k)</formula><p>j , where the counts n (k) j are collected from z, q 1:T , w. We estimate φ j by the first moment under this posterior <ref type="bibr" target="#b9">(Minka, 1999)</ref>:</p><formula xml:id="formula_12">φ (w) j = Ij (↑w) s γ post (Cj (s↓w)) j   Cj (s) s ′ γ post (s ′ ) j   −1 . (7)</formula><p>The parameter θ is estimated the same way as in standard LDA:</p><formula xml:id="formula_13">θ (d) j = (n (d) j + α)/(n (d) · + T α).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Synthetic Corpora: We present results on synthetic datasets to show how the Dirichlet Forest (DF) incorporates different types of knowledge. Recall that DF with η = 1 is equivalent to standard LDA (verified with the code of ( <ref type="bibr" target="#b6">Griffiths &amp; Steyvers, 2004)</ref>).</p><p>Previous studies often take the last MCMC sample (z and q 1:T ), and discuss the topics φ 1:T derived from that sample. Because of the stochastic nature of MCMC, we argue that more insight can be gained if multiple independent MCMC samples are considered.</p><p>For each dataset, and each DF with a different η, we run a long MCMC chain with 200,000 iterations of burn-in, and take out a sample every 10,000 iterations afterward, for a total of 200 samples. We have some indication that our chain is well-mixed, as we observe all expected modes, and that samples with "label switching" (i.e., equivalent up to label permutation) occur with near equal frequency. For each sample, we derive its topics φ 1:T with <ref type="formula">(7)</ref> and then greedily align the φ's from different samples, permuting the T topic labels to remove the label switching effect. Within a dataset, we perform PCA on the baseline (η = 1) φ and project all samples into the resulting space to obtain a common visualization (each row in <ref type="figure" target="#fig_6">Figure 3</ref>. Points are dithered to show overlap.). That is, simply adding one more topic does not clearly separate AB and CD. On the other hand, with η increasing, DF eventually concentrates on cluster 7, which satisfies the Split operation.</p><p>Wish Corpus: We now consider interactive topic modeling with DF. The corpus we use is a collection of 89,574 New Year's wishes submitted to The Times Square Alliance ( <ref type="bibr" target="#b5">Goldberg et al., 2009)</ref>. Each wish is treated as a document, downcased but without stopword removal. For each step in our interactive example, we set α = 0.5, β = 0.1, η = 1000, and run MCMC for 2000 iterations before estimating the topics from the final sample. The domain knowledge in DF is accumulative along the steps.</p><p>Step 1: We run LDA with T = 15. Many of the most probable words in the topics are conventional ("to, and") or corpus-specific ("wish, 2008") stopwords, which obscure the meaning of the topics.</p><p>Step 2: We manually create a 50-word stopword list, and issue an Isolate preference. This is compiled into Must-Links among this set and Cannot-Links between this set and all other words in the top 50 for all topics. T is increased to 16. After running DF, we end up with two stopword topics. Importantly, with the stopwords explained by these two topics, the top words for the other topics become much more meaningful.</p><p>Step 3: We notice that one topic conflates two concepts: enter college and cure disease (top 8 words: "go school cancer into well free cure college"). We issue Split("go,school,into,college", "cancer,free,cure,well") to separate the concepts. This is compiled into MustLinks within each quadruple, and a Cannot-Link between them. T is increased to 18. After running DF, one of the topics clearly takes on the "college" concept, picking up related words which we did not explicitly encode in our prior. Another topic does likewise for the "cure" concept (many wishes are like "mom stays cancer free"). Other topics have minor changes. Step 4: We then notice that two topics correspond to romance concepts. We apply Merge("love, forever, marry, together, loves", "meet, boyfriend, married, girlfriend, wedding"), which is compiled into MustLinks between these words. T is decreased to 17. After running DF, one of the romance topics disappears, and the remaining one corresponds to the merged romance topic ("lose", "weight" were in one of them, and remain so). Other previous topics survive with only minor changes. <ref type="table" target="#tab_1">Table 1</ref> shows the wish topics after these four steps, where we place the DF operations next to the most affected topics, and color-code the words explicitly specified in the domain knowledge.</p><p>Yeast Corpus: Whereas the previous experiment illustrates the utility of our approach in an interactive setting, we now consider a case in which we use background knowledge from an ontology to guide topic modeling. Our prior knowledge is based on six concepts. The concepts transcription, translation and replication characterize three important processes that are carried out at the molecular level. The concepts initiation, elongation and termination describe phases of the three aforementioned processes. Combinations of concepts from these two sets correspond to concepts in the Gene Ontology (e.g., GO:0006414 is translational elongation, and GO:0006352 is transcription initiation). We guide our topic modeling using Must-Links among a small set of words for each concept. Moreover, we use Cannot-Links among words to specify that we prefer (i) transcription, translation and replication to be represented in separate topics, and (ii) initiation, elongation and termination to be represented in separate topics. We do not set any preferences between the "process" topics and the "phase" topics, however. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">3 4 5 6 7 8 o 1 3 4 5 6 7 8 9 10 transcription</head><p>•</p><formula xml:id="formula_14">• • 1 • • • transcriptional • • • 2 • • • template • 1 • • • translation • • • • translational • • • tRNA 1 • • replication • 2 • • • cycle • • • • • division • 3 • • • initiation • • • • • • • • • start • • • • • • • assembly • • 7 • • • • elongation • • 1 • termination • • • disassembly • release 2 • stop • •</formula><p>The corpus that we use for our experiments consists of 18,193 abstracts selected from the MEDLINE database for their relevance to yeast genes. We induce topic models using DF to encode the Must-Links and Cannot-Links described above, and use standard LDA as a control. We set T = 100, α = 0.5, β = 0.1, η = 5000. For each word that we use to seed a concept, Table 2 shows the topics that include it among their 50 most probable words. We make several observations about the DF-induced topics. First, each concept is represented by a small number of topics and the MustLink words for each topic all occur as highly probable words in these topics. Second, the Cannot-Link preferences are obeyed in the final topics. Third, the topics use the process and phase topics in a compositionally. For example, DF Topic 4 represents transcription initiation and DF Topic 8 represents replication initiation. Moreover, the topics that are significantly influenced by the prior typically include highly relevant terms among their most probable words. For example, the top words in DF Topic 4 include "TATA", "TFIID", "promoter", and "recruitment" which are all specifically germane to the composite concept of transcription initiation. In the case of standard LDA, the seed concept words are dispersed across a greater number of topics, and highly related words, such as "cycle" and "division" often do not fall into the same topic. Many of the topics induced by ordinary LDA are semantically coherent, but the specific concepts suggested by our prior do not naturally emerge without using DF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Appearing in Proceedings of the 26 th International Confer- ence on Machine Learning, Montreal, Canada, 2009. Copy- right 2009 by the author(s)/owner(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>- ure 1(c) are φ samples from the Dirichlet tree in Fig- ure 1(a), plotted on the probability simplex of dimen- sion three. While it is always true that p(A) ≈ p(B), their total probability mass can be anywhere from 0 to 1. The most similar Dirichlet distribution is per- haps the one with parameters (50,50,1), which gener- ates samples close to (0.5, 0.5, 0) (Figure 1(d).)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Encoding Must-Links and Cannot-Links with a Dirichlet Forest. (a) A Dirichlet tree encoding Must-Link (A,B) with β = 1, η = 50 on vocabulary {A,B,C}. (b) A sample φ from this Dirichlet tree. (c) A large set of samples from the Dirichlet tree, plotted on the 3-simplex. Note p(A) ≈ p(B), yet they remain flexible in actual value, which is desirable for a Must-Link. (d) In contrast, samples from a standard Dirichlet with comparable parameters (50,50,1) force p(A) ≈ p(B) ≈ 0.5, and cannot encode a Must-Link. (e) The Cannot-Link-graph for Cannot-Link (A,B) and Cannot-Link (B,C). (f) The complementary graph, with two maximal cliques {A,C} and {B}. (g) The Dirichlet subtree for clique {A,C}. (h) The Dirichlet subtree for clique {B}. (i) Samples from the mixture model on (g,h), encoding both Cannot-Links, again with β = 1, η = 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Template of Dirichlet trees in the Dirichlet Forest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>−i,j be the number of word to- kens in document d assigned to topic j, excluding the word at position i. Similarly, let n (k) −i,j be the number of word tokens in the corpus that are under node k in topic j's Dirichlet tree, excluding the word at position i. For candidate topic labels v = 1 . . . T , we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Since the connected components are independent, sampling the tree q j factors into sam- pling the cliques for each connected component q (r) j . For candidate cliques q ′ = 1 . . . Q(r), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. PCA projections of permutation-aligned φ samples for the four synthetic data experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 . Wish topics from interactive topic modeling</head><label>1</label><figDesc></figDesc><table>Topic Top words sorted by φ = p(word|topic) 
Merge love lose weight together forever marry meet 
success health happiness family good friends prosperity 
life 
life happy best live time long wishes ever years 
-
as do not what someone so like don much he 
money out make money up house work able pay own lots 
people no people stop less day every each other another 
iraq 
home safe end troops iraq bring war return 
joy 
love true peace happiness dreams joy everyone 
family happy healthy family baby safe prosperous 
vote 
better hope president paul ron than person bush 
Isolate and to for a the year in new all my 
god 
god bless jesus everyone loved know heart christ 
peace peace world earth win lottery around save 
spam com call if u 4 www 2 3 visit 1 
Isolate i to wish my for and a be that the 
Split job go great school into good college hope move 
Split mom hope cancer free husband son well dad cure 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 . Yeast topics. The left column shows the seed words in the DF model. The middle columns indicate the topics in which at least 2 seed words are among the 50 highest probability words for LDA, the "o" column gives the number of other topics (not shared by another word).</head><label>2</label><figDesc></figDesc><table>The right columns show the same topic-word relationships 
for the DF model. 
LDA 
DF 
1 </table></figure>

			<note place="foot" n="1"> When there are Must-Links, all words in a Must-Link transitive closure form a single node in this graph.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Constrained clustering: Advances in algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<editor>&amp; Wagstaff, K.</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling documents by combining semantic concepts with unsupervised statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Semantic Web Conf</title>
		<imprint>
			<biblScope unit="page" from="229" to="244" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the hyper-Dirichlet type 1 and hyper-Liouville distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4069" to="4081" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">May all your wishes come true: A study of wishes and how to recognize them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Proc. of the Annual Conf. of the North American Chapter of the Assoc. for Computational Linguistics</title>
		<imprint>
			<publisher>ACL Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Nat. Academy of Sciences of the United States of America</title>
		<meeting>of the Nat. Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The number of maximal independent sets in a connected graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Griggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Grinstead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Guichard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Math</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="211" to="220" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pachinko allocation: DAG-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd Intl. Conf. on Machine Learning</title>
		<meeting>of the 23rd Intl. Conf. on Machine Learning</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Dirichlet-tree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/∼minka/papers/dirichlet/minka-dirtree.pdf" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Correlated latent semantic model for unsupervised LM adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Conf. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gene Ontology: Tool for the unification of biology</title>
	</analytic>
	<monogr>
		<title level="j">The Gene Ontology Consortium</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Nature Genetics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
