<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Approximate Factoring for A * Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
							<email>aria42@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<email>denero@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Approximate Factoring for A * Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel method for creating A * estimates for structured search problems. In our approach , we project a complex model onto multiple simpler models for which exact inference is efficient. We use an optimization framework to estimate parameters for these projections in a way which bounds the true costs. Similar to Klein and Manning (2003), we then combine completion estimates from the simpler models to guide search in the original complex model. We apply our approach to bitext parsing and lexicalized parsing, demonstrating its effectiveness in these domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inference tasks in NLP often involve searching for an optimal output from a large set of structured outputs. For many complex models, selecting the highest scoring output for a given observation is slow or even intractable. One general technique to increase efficiency while preserving optimality is A * search <ref type="bibr" target="#b7">(Hart et al., 1968)</ref>; however, successfully using A * search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models <ref type="bibr" target="#b3">(Collins, 1999)</ref>, though in certain cases admissible heuristics are known <ref type="bibr" target="#b11">(Och and Ney, 2000;</ref><ref type="bibr" target="#b14">Zhang and Gildea, 2006</ref>). For example, <ref type="bibr" target="#b8">Klein and Manning (2003)</ref> show a class of projection-based A * estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A * estimates for models which do not factor in this restricted way.</p><p>Like <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>, we focus on search problems where there are multiple projections or "views" of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization techniques <ref type="bibr">(Boyd and Van- denberghe, 2005</ref>) to approximately factor a model over these projections. Solutions to the projected problems yield heuristics for the original model. This approach is flexible, providing either admissible or nearly admissible heuristics, depending on the details of the optimization problem solved. Furthermore, our approach allows a modeler explicit control over the trade-off between the tightness of a heuristic and its degree of inadmissibility (if any). We describe our technique in general and then apply it to two concrete NLP search tasks: bitext parsing and lexicalized monolingual parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Approach</head><p>Many inference problems in NLP can be solved with agenda-based methods, in which we incrementally build hypotheses for larger items by combining smaller ones with some local configurational structure. We can formalize such tasks as graph search problems, where states encapsulate partial hypotheses and edges combine or extend them locally. 1 For example, in HMM decoding, the states are anchored labels, e.g. VBD <ref type="bibr">[5]</ref>, and edges correspond to hidden transitions, e.g. VBD <ref type="bibr">[5]</ref> → DT <ref type="bibr">[6]</ref>.</p><p>The search problem is to find a minimal cost path from the start state to a goal state, where the path cost is the sum of the costs of the edges in the path. , the top matrix is an example cost matrix, which specifies the cost of each local configuration. The bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost. Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.</p><formula xml:id="formula_0">񮽙 a a 񮽙 񮽙 → 񮽙 b b 񮽙 񮽙 񮽙 b a 񮽙 񮽙 → 񮽙 c b 񮽙 񮽙 1 񮽙 a a 񮽙 񮽙 → 񮽙 b b 񮽙 񮽙 񮽙 b b 񮽙 񮽙 → 񮽙 c c 񮽙 񮽙 1 񮽙 a a 񮽙 񮽙 → 񮽙 b b 񮽙 񮽙 񮽙 a b 񮽙 񮽙 → 񮽙 b c 񮽙 񮽙 1 񮽙 a a 񮽙 񮽙 → 񮽙 b b 񮽙 񮽙 񮽙 a b 񮽙 񮽙 → 񮽙 b c 񮽙 񮽙 1 Local Configurations a' → b' b' → c' a → b b → c 3</formula><p>For probabilistic inference problems, the cost of an edge is typically a negative log probability which depends only on some local configuration type. For instance, in PCFG parsing, the (hyper)edges reference anchored spans X <ref type="bibr">[i, j]</ref>, but the edge costs depend only on the local rule type X → Y Z. We will use a to refer to a local configuration and use c(a) to refer to its cost. Because edge costs are sensitive only to local configurations, the cost of a path is a c(a). A * search requires a heuristic function, which is an estimate h(s) of the completion cost, the cost of a best path from state s to a goal.</p><p>In this work, following <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>, we consider problems with projections or "views," which define mappings to simpler state and configuration spaces. For instance, suppose that we are using an HMM to jointly model part-of-speech (POS) and named-entity-recognition (NER) tagging. There might be one projection onto the NER component and another onto the POS component. Formally, a projection π is a mapping from states to some coarser domain. A state projection induces projections of edges and of the entire graph π(G).</p><p>We are particularly interested in search problems with multiple projections {π 1 , . . . , π } where each projection, π i , has the following properties: its state projections induce well-defined projections of the local configurations π i (a) used for scoring, and the projected search problem admits a simpler inference. For instance, the POS projection in our NER-POS HMM is a simpler HMM, though the gains from this method are greater when inference in the projections have lower asymptotic complexity than the original problem (see sections 3 and 4).</p><p>In defining projections, we have not yet dealt with the projected scoring function. Suppose that the cost of local configurations decomposes along projections as well. In this case,</p><formula xml:id="formula_1">c (a) = i=1 c i (a) , ∀a ∈ A (1)</formula><p>where A is the set of local configurations and c i (a) represents the cost of configuration a under projection π i . A toy example of such a cost decomposition in the context of a Markov process over two-part states is shown in <ref type="figure" target="#fig_0">figure 1(b)</ref>, where the costs of the joint transitions equal the sum of costs of their projections. Under the strong assumption of equation <ref type="formula">(1)</ref>, <ref type="bibr" target="#b8">Klein and Manning (2003)</ref> give an admissible A * bound. They note that the cost of a path decomposes as a sum of projected path costs. Hence, the following is an admissible additive heuristic <ref type="bibr" target="#b4">(Felner et al., 2004</ref>),</p><formula xml:id="formula_2">h(s) = i=1 h * i (s)<label>(2)</label></formula><p>where h * i (s) denote the optimal completion costs in the projected search graph π i (G). That is, the completion cost of a state bounds the sum of the completion costs in each projection.</p><p>In virtually all cases, however, configuration costs will not decompose over projections, nor would we expect them to. For instance, in our joint POS-NER task, this assumption requires that the POS and NER transitions and observations be generated independently. This independence assumption undermines the motivation for assuming a joint model. In the central contribution of this work, we exploit the projection structure of our search problem without making any assumption about cost decomposition.</p><p>Rather than assuming decomposition, we propose to find scores φ for the projected configurations which are pointwise admissible:</p><formula xml:id="formula_3">i=1 φ i (a) ≤ c(a), ∀a ∈ A (3)</formula><p>Here, φ i (a) represents a factored projection cost of π i (a), the π i projection of configuration a. Given pointwise admissible φ i 's we can again apply the heuristic recipe of equation <ref type="formula" target="#formula_2">(2)</ref>. An example of factored projection costs are shown in figure 1(c), where no exact decomposition exists, but a pointwise admissible lower bound is easy to find.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim. If a set of factored projection costs</head><p>{φ 1 , . . . , φ } satisfy pointwise admissibility, then the heuristic from <ref type="formula" target="#formula_2">(2)</ref> is an admissible A * heuristic.</p><p>Proof. Assume a 1 , . . . , a k are configurations used to optimally reach the goal from state s. Then,</p><formula xml:id="formula_4">h * (s) = k X j=1 c(aj) ≥ k X j=1 X i=1 φi(aj) = X i=1 k X j=1 φi(aj) ! ≥ X i=1 h * i (s) = h(s)</formula><p>The first inequality follows from pointwise admissibility. The second inequality follows because each inner sum is a completion cost for projected problem π i and therefore h * i (s) lower bounds it. Intuitively, we can see two sources of slack in such projection heuristics. First, there may be slack in the pointwise admissible scores. Second, the best paths in the projections will be overly optimistic because they have been decoupled (see <ref type="figure" target="#fig_6">figure 5</ref> for an example of decoupled best paths in projections).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Finding Factored Projections for Non-Factored Costs</head><p>We can find factored costs φ i (a) which are pointwise admissible by solving an optimization problem.</p><p>We think of our unknown factored costs as a block vector φ = [φ 1 , .., φ ], where vector φ i is composed of the factored costs, φ i (a), for each configuration a ∈ A. We can then find admissible factored costs by solving the following optimization problem,</p><formula xml:id="formula_5">minimize φ γ (4) such that, γ a = c(a) − i=1 φ i (a), ∀a ∈ A γ a ≥ 0, ∀a ∈ A</formula><p>We can think of each γ a as the amount by which the cost of configuration a exceeds the factored projection estimates (the pointwise A * gap). Requiring γ a ≥ 0 insures pointwise admissibility. Minimizing the norm of the γ a variables encourages tighter bounds; indeed if γ = 0, the solution corresponds to an exact factoring of the search problem. In the case where we minimize the 1-norm or ∞-norm, the problem above reduces to a linear program, which can be solved efficiently for a large number of variables and constraints. <ref type="bibr">2</ref> Viewing our procedure decision-theoretically, by minimizing the norm of the pointwise gaps we are effectively choosing a loss function which decomposes along configuration types and takes the form of the norm (i.e. linear or squared losses). A complete investigation of the alternatives is beyond the scope of this work, but it is worth pointing out that in the end we will care only about the gap on entire structures, not configurations, and individual configuration factored costs need not even be pointwise admissible for the overall heuristic to be admissible.</p><p>Notice that the number of constraints is |A|, the number of possible local configurations. For many search problems, enumerating the possible configurations is not feasible, and therefore neither is solving an optimization problem with all of these constraints. We deal with this situation in applying our technique to lexicalized parsing models (section 4).</p><p>Sometimes, we might be willing to trade search optimality for efficiency. In our approach, we can explicitly make this trade-off by designing an alternative optimization problem which allows for slack in the admissibility constraints. We solve the following soft version of problem <ref type="formula">(4)</ref>:</p><formula xml:id="formula_6">minimize φ γ + + Cγ − (5) such that, γ a = c(a) − i=1 φ i (a), ∀a ∈ A</formula><p>where γ + = max{0, γ} and γ − = max{0, −γ} represent the componentwise positive and negative elements of γ respectively. Each γ − a &gt; 0 represents a configuration where our factored projection estimate is not pointwise admissible. Since this situation may result in our heuristic becoming inadmissible if used in the projected completion costs, we more heavily penalize overestimating the cost by the constant C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bounding Search Error</head><p>In the case where we allow pointwise inadmissibility, i.e. variables γ − a , we can bound our search error. Suppose γ − max = max a∈A γ − a and that L * is the length of the longest optimal solution for the original problem. Then,</p><formula xml:id="formula_7">h(s) ≤ h * (s) + L * γ − max , ∀s ∈ S.</formula><p>This -admissible heuristic <ref type="bibr" target="#b6">(Ghallab and Allard, 1982</ref>) bounds our search error by L * γ − max . 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bitext Parsing</head><p>In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence w s and its translation w t ( <ref type="bibr" target="#b10">Melamed et al., 2004;</ref><ref type="bibr" target="#b13">Wu, 1997)</ref>. Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G.</p><p>Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n 6 ) in the length of the sentence ( <ref type="bibr" target="#b13">Wu, 1997)</ref>. This high degree of complexity makes exhaustive bitext parsing infeasible for all but the shortest sentences. In contrast, monolingual CFG parsing requires time O(n 3 ) in the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A * Parsing</head><p>Alternatively, we can search for an optimal parse guided by a heuristic. The states in A</p><note type="other">* bitext parsing are rooted bispans, denoted X [i, j] :: Y [k, l]. States represent a joint parse over subspans [i, j] of w s and [k, l] of w t rooted by the nonterminals X and Y respectively.</note><p>Given a WSG G, the algorithm prioritizes a state (or edge) e by the sum of its inside cost β G (e) (the negative log of its inside probability) and its outside estimate h(e), or completion cost. <ref type="bibr">4</ref> We are guaranteed the optimal parse if our heuristic h(e) is never greater than α G (e), the true outside cost of e.</p><p>We now consider a heuristic combining the completion costs of the monolingual projections of G, and guarantee admissibility by enforcing point-wise admissibility.</p><formula xml:id="formula_8">Each state e = X [i, j] :: Y [k, l]</formula><p>projects a pair of monolingual rooted spans. The heuristic we propose sums independent outside costs of these spans in each monolingual projection.</p><formula xml:id="formula_9">h(e) = α s (X [i, j]) + α t (Y [k, l])</formula><p>These monolingual outside scores are computed relative to a pair of monolingual WCFG grammars G s and G t given by splitting each synchronous rule</p><formula xml:id="formula_10">r = X (s) Y (t) → α β γ δ</formula><p>into its components π s (r) = X → αβ and π t (r) = Y → γδ and weighting them via optimized φ s (r) and φ t (r), respectively. 5 To learn pointwise admissible costs for the monolingual grammars, we formulate the following optimization problem: 6 minimize γ,φs,φt  </p><formula xml:id="formula_11">γ 1 such that, γ r = c(r) − [φ s (r) + φ t (r)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers ( <ref type="bibr" target="#b9">Knight and Graehl, 2004</ref>). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. <ref type="figure" target="#fig_3">Figure 3(a)</ref> shows an example rule.</p><p>Following <ref type="bibr" target="#b5">Galley et al. (2004)</ref>, we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in <ref type="figure" target="#fig_3">figure 3(b)</ref>. <ref type="bibr">7</ref> We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus. 8 <ref type="figure" target="#fig_5">Figure 4</ref>(a) shows that A * expands substantially fewer states while searching for the optimal parse with our op-7 The bilingual corpus consists of translation pairs with fixed English parses and word alignments. Rules were scored by their relative frequencies.</p><p>8 Rare words were replaced with their parts of speech to limit the memory consumption of the parser. timization heuristic. The exhaustive curve shows edge expansions using the null heuristic. The intermediate result, labeled English only, used only the English monolingual outside score as a heuristic. Similar results using only Spanish demonstrate that both projections contribute to parsing efficiency. All three curves in <ref type="figure" target="#fig_5">figure 4</ref> represent running times for finding the optimal parse.</p><p>Zhang and Gildea <ref type="formula" target="#formula_2">(2006)</ref> offer a different heuristic for A * parsing of ITG grammars that provides a forward estimate of the cost of aligning the unparsed words in both sentences. We cannot directly apply this technique to our grammar because tree-to-tree transducers only align non-terminals. Instead, we can augment our synchronous grammar model to include a lexical alignment component, then employ both heuristics. We learned the following two-stage generative model: a tree-to-tree transducer generates trees whose leaves are parts of speech. Then, the words of each sentence are generated, either jointly from aligned parts of speech or independently given a null alignment. The cost of a complete parse under this new model decomposes into the cost of the synchronous tree over parts of speech and the cost of generating the lexical items. Given such a model, both our optimization heuristic and the lexical heuristic of <ref type="bibr" target="#b14">Zhang and Gildea (2006)</ref> can be computed independently. Crucially, the sum of these heuristics is still admissible. Results appear in <ref type="figure" target="#fig_5">figure 4(b)</ref>. Both heuristics (lexical and optimization) alone improve parsing performance, but their sum opt+lex substantially improves upon either one.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lexicalized Parsing</head><p>We next apply our technique to lexicalized parsing <ref type="bibr" target="#b2">(Charniak, 1997;</ref><ref type="bibr" target="#b3">Collins, 1999</ref>). In lexicalized parsing, the local configurations are lexicalized rules of the form</p><formula xml:id="formula_12">X[h, t] → Y [h , t ] Z[h, t]</formula><p>, where h, t, h , and t are the head word, head tag, argument word, and argument tag, respectively. We will use r = X → Y Z to refer to the CFG backbone of a lexicalized rule. As in Klein and Manning (2003), we view each lexicalized rule, , as having a CFG projection, π c () = r, and a dependency projection, π d () = (h, t, h , t )(see figure 5). <ref type="bibr">9</ref> Broadly, the CFG projection encodes constituency structure, while the dependency projection encodes lexical selection, and both projections are asymptotically more efficient than the original problem. <ref type="bibr" target="#b8">Klein and Manning (2003)</ref> present a factored model where the CFG and dependency projections are generated independently (though with compatible bracketing):</p><formula xml:id="formula_13">P (Y [h, t]Z[h , t ] | X[h, t]) =<label>(6)</label></formula><formula xml:id="formula_14">P (Y Z|X)P (h , t |t, h)</formula><p>In this work, we explore the following non-factored model, which allows correlations between the CFG and dependency projections:</p><formula xml:id="formula_15">P (Y [h, t]Z[h , t ] | X[h, t]) = P (Y Z|X, t, h) (7) P (t |t, Z, h , h) P (h |t , t, Z, h , h)</formula><p>This model is broadly representative of the successful lexicalized models of Charniak (1997) and <ref type="bibr">9</ref> We assume information about the distance and direction of the dependency is encoded in the dependency tuple, but we omit it from the notation for compactness.</p><p>Collins (1999), though simpler. <ref type="bibr">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Choosing Constraints and Handling Unseen Dependencies</head><p>Ideally we would like to be able to solve the optimization problem in (4) for this task. Unfortunately, exhaustively listing all possible configurations (lexical rules) yields an impractical number of constraints. We therefore solve a relaxed problem in which we enforce the constraints for only a subset of the possible configurations, A ⊆ A. Once we start dropping constraints, we can no longer guarantee pointwise admissibility, and therefore there is no reason not to also allow penalized violations of the constraints we do list, so we solve (5) instead.</p><p>To generate the set of enforced constraints, we first include all configurations observed in the gold training trees. We then sample novel configurations by choosing (X, h, t) from the training distribution and then using the model to generate the rest of the configuration. In our experiments, we ended up with 434,329 observed configurations, and sampled the same number of novel configurations. Our penalty multiplier C was 10.</p><p>Even if we supplement our training set with many sample configurations, we will still see new projected dependency configurations at test time. It is therefore necessary to generalize scores from training configurations to unseen ones. We enrich our procedure by expressing the projected configuration costs as linear functions of features. Specifically, we define feature vectors f c (r) and f d (h, t, h t ) over the CFG and dependency projections, and intro- </p><formula xml:id="formula_16">S       $ $ $ $ $ $ NP S     3</formula><formula xml:id="formula_17">minimize γ,wc,w d γ + 2 + Cγ − 2 (8) such that, w c ≥ 0, w d ≥ 0 γ = c() − [w T c f c (r) + w T d f d (h, t, h , t )] for = (r, h, t, h , t ) ∈ A</formula><p>Our CFG feature vector has only indicator features for the specific rule. However, our dependency feature vector consists of an indicator feature of the tuple (h, t, h , t ) (including direction), an indicator of the part-of-speech type (t, t ) (also including direction), as well as a bias feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>We tested our approximate projection heuristic on two lexicalized parsing models. The first is the factored model of <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>, given by equation <ref type="formula" target="#formula_13">(6)</ref>, and the second is the non-factored model described in equation <ref type="formula">(7)</ref>. Both models use the same parent-annotated head-binarized CFG backbone and a basic dependency projection which models direction, but not distance or valence. <ref type="bibr">11</ref> In each case, we compared A * using our approximate projection heuristics to exhaustive search. We measure efficiency in terms of the number of expanded hypotheses (edges popped); see <ref type="figure">figure 6</ref>. <ref type="bibr">12</ref> In both settings, the factored A * approach substantially outperforms exhaustive search. For the fac- <ref type="bibr">11</ref> The CFG and dependency projections correspond to the PCFG-PA and DEP-BASIC settings in <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>. <ref type="bibr">12</ref> All models are trained on section 2 through 21 of the English Penn treebank, and tested on section 23. tored model of <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>, we can also compare our reconstructed bound to the known tight bound which would result from solving the pointwise admissible problem in (4) with all constraints. As <ref type="figure">figure 6</ref> shows, the exact factored heuristic does outperform our approximate factored heuristic, primarily because of many looser, backedoff cost estimates for unseen dependency tuples. For the non-factored model, we compared our approximate factored heuristic to one which only bounds the CFG projection as suggested by <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>. They suggest,</p><formula xml:id="formula_18">φ c (r) = min ∈A:πc()=r c()</formula><p>where we obtain factored CFG costs by minimizing over dependency projections. As <ref type="figure">figure 6</ref> illustrates, this CFG only heuristic is substantially less efficient than our heuristic which bounds both projections.</p><p>Since our heuristic is no longer guaranteed to be admissible, we evaluated its effect on search in several ways. The first is to check for search errors, where the model-optimal parse is not found. In the case of the factored model, we can find the optimal parse using the exact factored heuristic and compare it to the parse found by our learned heuristic. In our test set, the approximate projection heuristic failed to return the model optimal parse in less than 1% of sentences. Of these search errors, none of the costs were more than 0.1% greater than the model optimal cost in negative log-likelihood. For the non-factored model, the model optimal parse is known only for shorter sentences which can be parsed exhaustively. For these sentences up to length 15, there were no search errors. We can also check for violations of pointwise admissibility for configurations encoun-  <ref type="figure">Figure 6</ref>: Edges popped by exhaustive versus factored A * search. The chart in (a) is using the factored lexicalized model from <ref type="bibr" target="#b8">Klein and Manning (2003)</ref>. The chart in (b) is using the non-factored lexicalized model described in section 4.</p><p>tered during search. For both the factored and nonfactored model, less than 2% of the configurations scored by the approximate projection heuristic during search violated pointwise admissibility. While this is a paper about inference, we also measured the accuracy in the standard way, on sentences of length up to 40, using EVALB. The factored model with the approximate projection heuristic achieves an F 1 of 82.2, matching the performance with the exact factored heuristic, though slower. The non-factored model, using the approximate projection heuristic, achieves an F 1 of 83.8 on the test set, which is slightly better than the factored model. <ref type="bibr">13</ref> We note that the CFG and dependency projections are as similar as possible across models, so the increase in accuracy is likely due in part to the nonfactored model's coupling of CFG and dependency projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a technique for creating A * estimates for inference in complex models. Our technique can be used to generate provably admissible estimates when all search transitions can be enumerated, and an effective heuristic even for problems where all transitions cannot be efficiently enumerated. In the future, we plan to investigate alternative objective functions and error-driven methods for learning heuristic bounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row and column of the cell). In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration. The bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost. Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The gap between the heuristic (left) and true completion cost (right) comes from relaxing the synchronized problem to independent subproblems and slack in the factored models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 diagrams</head><label>2</label><figDesc>Figure 2 diagrams the two bounds that enforce the admissibility of h(e). For any outside cost α G (e), there is a corresponding optimal completion structure o under G, which is an outer shell of a synchronous tree. o projects monolingual completions o s and o t which have well-defined costs c s (o s ) and c t (o t ) under G s and G t respectively. Their sum c s (o s ) + c t (o t ) will underestimate α G (e) by pointwise admissibility. Furthermore, the heuristic we compute underestimates this sum. Recall that the monolingual outside score α s (X [i, j]) is the minimal costs for any completion of the edge. Hence, α s (X [i, j]) ≤ c s (o s ) and α t (X [k, l]) ≤ c t (o t ). Admissibility follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) A tree-to-tree transducer rule. (b) An example training sentence pair that yields rule (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem. (b) Including a lexical model and corresponding heuristic further increases parsing efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Lexicalized parsing projections. The figure in (a) is the optimal CFG projection solution and the figure in (b) is the optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.</figDesc></figure>

			<note place="foot" n="1"> In most complex tasks, we will in fact have a hypergraph, but the extension is trivial and not worth the added notation.</note>

			<note place="foot" n="2"> We used the MOSEK package (Andersen and Andersen, 2000).</note>

			<note place="foot" n="3"> This bound may be very loose if L is large.</note>

			<note place="foot" n="4"> All inside and outside costs are Viterbi, not summed. 5 Note that we need only parse each sentence (monolingually) once to compute the outside probabilities for every span. 6 The stated objective is merely one reasonable choice among many possibilities which require pointwise admissibility and encourage tight estimates.</note>

			<note place="foot" n="10"> All probability distributions for the non-factored model are estimated by Witten-Bell smoothing (Witten and Bell, 1991) where conditioning lexical items are backed off first.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The MOSEK interior point optimizer for linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Optimization</title>
		<editor>H. Frenk et al.</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical parsing with a context-free grammar and word statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Additive pattern database heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Felner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Korf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Hanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A * -an efficient near admissible heuristic search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Ghallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><forename type="middle">G</forename><surname>Allard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A formal basis for the heuristic determination of minimum cost paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems Science and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factored A* search for models over sequences and trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized multitext grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Melamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">C</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient search for inversion transduction grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
