<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ranking Refinement and Its Application to Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>rongjin@cse.msu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Valizadegan</surname></persName>
							<email>valizade@cse.msu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<email>hangli@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Michigan State University East Lansing</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<address>
									<postCode>48824, 48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI, MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<addrLine>Asia 4F, Sigma Center No.49 Zhichun Road, Haidian Beijing</addrLine>
									<postCode>100080</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ranking Refinement and Its Application to Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Infor- mation Systems]: Information Search and Retrieval; I26 [Artificial Intelligence]: Learning General Terms: Design</term>
					<term>Experimentation</term>
					<term>Theory Keywords: Learning to Rank</term>
					<term>Background Information</term>
					<term>Boosting</term>
					<term>Incremental Learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We consider the problem of ranking refinement, i.e., to improve the accuracy of an existing ranking function with a small set of labeled instances. We are, particularly, interested in learning a better ranking function using two complementary sources of information, ranking information given by the existing ranking function (i.e., the base ranker) and that obtained from users&apos; feedbacks. This problem is very important in information retrieval where feedbacks are gradually collected. The key challenge in combining the two sources of information arises from the fact that the ranking information presented by the base ranker tends to be imperfect and the ranking information obtained from users&apos; feedbacks tends to be noisy. We present a novel boosting algorithm for ranking refinement that can effectively leverage the uses of the two sources of information. Our empirical study shows that the proposed algorithm is effective for ranking refinement, and furthermore it significantly outper-forms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Learning to rank is a relatively new area of study in machine learning. It aims to learn an assignment of scores to objects and rank the objects on the basis of the scores. It has received much attention in recent years because of its important role in information retrieval. Most research in learning to rank is conducted in the supervised fashion, in which a ranking function is learned from a given set of training instances. The drawback with the supervised approach is that they tend to fail when the number of training instances is small.</p><p>In several real-world applications, in addition to the labeled training instances, a base ranker is available that can be used to rank the objects. Then, the research question is how to exploit the outputs from the base ranker when learning a ranking function from a small number of labeled instances. We refer to this problem as Ranking Refinement to distinguish it from supervised learning for ranking. Below we show two examples for the application of ranking refinement:</p><p>Relevance feedback In information retrieval, documents are often ordered by a predefined relevance ranking function, such as BM25 <ref type="bibr" target="#b0">[1]</ref> and Language Model for IR <ref type="bibr" target="#b1">[2]</ref>, that assesses the relevancy of documents to a given query. Relevance feedback techniques are proposed to improve the retrieval accuracy by allowing users to provide relevance judgments for the first a few retrieved documents. The research question here is how to enhance the accuracy of relevance feedback by combining the uses of the two types of information. In this case, the base ranker is the relevance ranking function, and the training instances are the documents that are judged by the users.</p><p>Recommender system The goal of a recommender system is to rank the items according to the interest of an active user (i.e., the test user). Usually, a few rated items are provided to indicate the preference of the active user. Using the collaborative filtering techniques <ref type="bibr" target="#b2">[3]</ref>, we can come up with a preliminary list of items ranked by using the preference information of other users. The research question here is how to enhance the final ranking accuracy by leveraging the two types of information. In this case, the base ranker is the collaborative filtering algorithm, and the labeled instances are the items labeled by the active user.</p><p>Furthermore, any online learning of ranking functions can be viewed as a ranking refinement problem in that the ranking function is updated iteratively with new training instances collected on the fly.</p><p>A straightforward approach toward ranking refinement is to view the scores of the base ranker as an additional feature, and learn a ranking function over the augmented features. As will be shown in the experiments, this is not the best approach for exploiting the information hidden in the base ranking function. We believe that the most valuable information behind the base ranker is not its scores but the ranked list of objects it produces. We therefore view the base ranker and the labeled instances as two complementary sources of information. The key challenge in combining these two sources of information is that the ranked list generated by the base ranker tends to be imperfect while the labeled instances tend to be noisy. In this paper, we present a boosting algorithm for ranking refinement that can effectively utilize the two types of information. Our empirical study with relevance feedback and recommender system show that the proposed algorithm is effective for ranking refinement, and it significantly outperforms the baseline algorithms that incorporate the outputs from the base ranker as an additional feature for the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Most learning to rank algorithms are designed for the setting of supervised learning, in which a ranking function is learned from labeled instances. The problem of learning to rank is often cast as a classification problem where the goal is to correctly classify the ordering relationship between any two instances. Three well-known approaches in this category are Ranking-SVM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, RankBoost <ref type="bibr" target="#b5">[6]</ref>, and RankNet <ref type="bibr" target="#b6">[7]</ref>. Ranking-SVM minimizes the number of incorrectly ordered pairs within the maximum margin framework. Several variants <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> are developed to further enhance the performance of Ranking-SVM. RankBoost learns a ranking model based on the same consideration, but by means of Boosting. RankNet <ref type="bibr" target="#b6">[7]</ref> is a neural network based approach that uses cross entropy as its loss function. Recently, Xu et al. <ref type="bibr" target="#b9">[10]</ref> proposed another approach that is aimed at directly optimizing the performance measures in information retrieval, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). A special group of ranking problem is called ordinal regression <ref type="bibr" target="#b3">[4]</ref>, in which the output of the ranking function is restricted to a few ordinal categories. Example algorithms for ordinal regression include the maximum margin based approach <ref type="bibr" target="#b10">[11]</ref> and the Gaussian process based approach <ref type="bibr" target="#b11">[12]</ref>. The ranking refinement problem differs from the supervised ranking problem in that an imperfect base ranker is provided in addition to the labeled training instances.</p><p>The ranking problem is essential to information retrieval, whose goal is to rank a collection of documents by their relevance to a given query. In particular, relevance feedback techniques <ref type="bibr" target="#b12">[13]</ref> are developed to improve the accuracy of the existing retrieval algorithms. There are two types of relevance feedback. The first type, termed user relevance feedback, enhances the retrieval accuracy by collecting the user relevance judgments for the documents that are ranked on the top of the list. As pointed out in the introduction section, the user relevance feedback problem can be treated as a problem of ranking refinement. In the empirical study, we will show that the proposed algorithm for ranking refinement significantly outperforms the standard relevance feedback algorithm (i.e., the Rocchio algorithm) over several datasets. The second type of relevance feedback, often termed pseudo relevance feedback, does not explicitly collect the user relevance judgments. Instead, it treats the top ranked documents as relevant to the given query, and the documents ranked at the bottom as irrelevant. These pseudo relevance judgments are used to improve the existing ranking function. It is well known in information retrieval that pseudo relevance feedback may result in degradation of retrieval performance given the high probability of errors in pseudo relevance judgments <ref type="bibr" target="#b12">[13]</ref>. This is similar to the noise of training instances in ranking refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RANKING REFINEMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let D = (x1, x2, . . . , xn) denote the set of instances to be ordered, where each instance x i ∈ R d is a vector of d dimensions. Let G : R d → R denote the base ranking function (base ranker), and g i = G(x i ) denote the ranking score assigned to xi by the base ranking function G. Instance xi is ranked before xj if gi &gt; gj. To make our problem general, we assume the label information collected from user feedback is presented as a set of ordered pairs, denoted by O = {(xi k xj k )|k = 1, . . . , m} where each pair xi xj indicates that instance xi is ranked before xj 1 . The goal of ranking refinement is to learn a ranking function F : R d → R by exploiting both the labeled pairs in O and the ranking information given by G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Ranking Information</head><p>The first important question for ranking refinement is how to encode the ranking information provided by the base ranking function G. A straightforward approach is to use the ranking scores computed by G as an additional feature, and apply the existing algorithms, such as RankBoost and Ranking-SVM, to learn a ranking function from the labeled instances. The drawback of this approach is twofold:</p><p>• First, this approach only utilizes the ranking scores of the labeled instances. The ranking information generated by the base ranking function for the unlabeled instances is completely ignored by this approach. Since the number of labeled instances collected from users' feedbacks is considerably smaller than the number of unlabeled instances, this approach is not optimal in exploiting the information provided by the base ranking function.</p><p>• Second, we believe that the ranking orders generated by the base ranking function is substantially more reliable than the numerical values of the ranking scores. Similar observation is found in the study of meta search whose goal is to combine the retrieval results of multiple search engines to create a better ranking list <ref type="bibr" target="#b13">[14]</ref>. Empirical studies <ref type="bibr" target="#b13">[14]</ref> showed that the meta search algorithms based on the document ranks often outperform the algorithms that directly use the relevance scores.</p><p>To address the above problems, we encode the order information generated by the base ranking function G with matrix W ∈ [0, 1] n×n . Each Wi,j in the matrix represents the probability of ranking xi before xj and is defined as follows</p><formula xml:id="formula_0">Wi,j = exp(λgi) exp(λg i ) + exp(λg j )<label>(1)</label></formula><p>In the above, Wi,j is defined by a softmax function and the parameter λ ≥ 0 represents the confidence of the base ranking function. To see the effect of λ, we consider two extreme cases:</p><p>• λ = 0. In this case, we have Wi,j = 0.5, which indicates that the ordering information generated by the base ranker is completely ignored.</p><p>• λ = ∞. In this case, we have</p><formula xml:id="formula_1">Wi,j =    1 gi &gt; gj 0.5 gi = gj 0 g i &lt; g j<label>(2)</label></formula><p>Thus, W is almost a binary matrix, which implies that we completely trust ranked list generated by the base ranking function.</p><p>By varying the parameter λ, we are able to alleviate the negative effect from the base ranking function. In our experiment, we set λ to be inverse to the standard deviation of ranking scores of the first 10 retrieved documents. Similarly, we encode the ordering information inside the set O with matrix T as follows:</p><formula xml:id="formula_2">T i,j = 1 − η/2 (x i x j ) ∈ O η/2 otherwise (3)</formula><p>where parameter η ∈ <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Ti,j represents the probability of ranking ranking xi before xj in the training data. The parameter η reflects the error rate of training data, and is particularly useful when the labeled instances are derived from implicit user feedback that is usually noisier. In our experiment, we set η = 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Objective Function</head><p>The goal of ranking refinement is to learn a ranking function F : R d → R from matrix W and T that produces a more accurate ranked list than the base ranking function G. In particular, the optimal ranking function F should be consistent with the ranking information in W and T . To this end, we measure the ranking errors of F with respect to both W and F , i.e.,</p><formula xml:id="formula_3">errw = n i,j=1 Wi,jI(Fj ≥ Fi) (4) errt = n i,j=1</formula><p>Ti,jI(Fj ≥ Fi)</p><p>In the above, we introduce F i = F (x i ) and the indicator function I(x) that outputs 1 when the input boolean variable x is true and zero otherwise. There are two problems with directly using the ranking errors err w and err t as the objective function:</p><p>• First, both error functions are non-smooth functions since the indicator function I(x) is non-smooth. It is well know that optimizing a non-smooth function is computationally more challenging than optimizing a smooth function because the derivative of a nonsmooth function is not well defined <ref type="bibr" target="#b14">[15]</ref>.</p><p>• Second, with two objectives at hand, the problem is essentially a multi-objective optimization problem <ref type="bibr" target="#b15">[16]</ref>.</p><p>Thus, another important question is how to combine multiple objectives into one single objective.</p><p>In the following subsections, we will address these two questions separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Relaxation with Exponential Functions</head><p>To address the problem with non-smooth objective functions, we follow the idea of boosting by replacing the indicator function I(x ≥ y) with an exponential function exp(x − y). The resulting new objective functions are:</p><formula xml:id="formula_5">errw = n i,j=1 Wi,j exp(Fj − Fi) (6) errt = n i,j=1</formula><p>Ti,j exp(Fj − Fi)</p><p>Note that since exp(x − y) ≥ I(x ≥ y), by minimizing the errors err w and err t , we are effective in reducing the original ranking errors errw and errt. Another advantage of using errw and errt comes from the theoretic result of AdaBoost <ref type="bibr" target="#b16">[17]</ref>, i.e., by minimizing the exponential loss function, the resulting classifier will not only reduce the training errors but also maximize the classification margin. The enlarged classification margin is the key to guarantee a low generalization error for testing instances <ref type="bibr" target="#b16">[17]</ref>.</p><p>Remark: It is interesting to examine the effect of the smoothing parameter η on the ranking error err t . By substituting the expression (3) for Ti,j in <ref type="formula" target="#formula_6">(7)</ref>, we have errt expressed as follows:</p><formula xml:id="formula_7">errt = (1 − η) (x i x j )∈O exp(Fj − Fi) + η 2 n i,j=1 [exp(Fi − Fj) + exp(Fj − Fi)] ≈ (1 − η) (x i x j )∈O exp(Fj − Fi) + η 2 n i,j=1 (Fi − Fj) 2 = (1 − η)   (x i x j )∈O exp(Fj − Fi) + η 2(1 − η) F 2 S   (8)</formula><p>where F 2 S is a norm of vector F = (F1, . . . , Fn) defined as follows:</p><formula xml:id="formula_8">F 2 S = F (nI − ee)F</formula><p>where I is the identity matrix and e is a vector of all ones. The approximation of the second step in the above derivation follows the Taylor expansion of the exponential function. Clearly, the second term in (8), i.e., ηF 2 S /2(1 − η), is similar to the regularization term used by Support Vector Machines (SVM) <ref type="bibr" target="#b17">[18]</ref>. Thus, the parameter η plays the role of coefficient in the regularized ranking error errt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Combination of Two Objectives</head><p>The problem of optimizing multiple objectives is usually called multi-objective optimization problem <ref type="bibr" target="#b15">[16]</ref>. The most common approach is to linearly combine objectives, which in our case is to linearly combine the two error functions, i.e.,</p><formula xml:id="formula_9">L a = γ err w + err t = n i,j=1 (γW i,j + T i,j ) exp(F j − F i ) (9)</formula><p>where parameter γ is used to combine two classification errors. We refer to the approach based on the above objective function as "Linear Ranking Refinement", or LRR, for short. The main problem with using the linearly combined objectives is how to decide an appropriate value for γ. In our experiments, we will show that different γ could result in very different performance in information retrieval.</p><p>To resolve the difficulty, we consider another approach which makes combination of the two errors by their products, i.e.,</p><formula xml:id="formula_10">Lp = errw × errt = n i,j=1 Ti,j exp(Fj − Fi) n i,j=1</formula><p>Wi,j exp(Fj − Fi)</p><p>We refer to the approach as "Multiplicative Ranking Refinement", or MRR for short. The first concern on using the product is whether the resulting solution is Pareto efficient <ref type="bibr" target="#b15">[16]</ref>. A solution F = (F1, . . . Fn) is Pareto efficient for the objectives errw and err t if there does not exist any other solution</p><formula xml:id="formula_12">F = (F 1 , . . . , F n ) that is either 1. err w (F ) &lt; err w (F ) and err t (F ) ≤ err t (F ), or 2. errw(F ) ≤ errw(F ) and errt(F ) &lt; errt(F ).</formula><p>In other words, if F is Pareto efficient, it guarantees that no solution is able to further reduce the two objectives simultaneously than F. It is well known that, according to multi-objective optimization theory <ref type="bibr" target="#b15">[16]</ref>, the solution found by minimizing L a is guaranteed to be Pareto efficient. Regarding the Pareto efficiency when minimizing L p in <ref type="formula" target="#formula_0">(10)</ref>, we have the following theorem:</p><formula xml:id="formula_13">Theorem 1. The optimal solution F = (F 1 , . . . , F n ) found by minimizing the objective function L p is Pareto efficient.</formula><p>The proof of this theorem can be found in Appendix A. The main advantage of using L p rather than L a is that it does not need a weight parameter. This will be revealed in our empirical studies in that minimization of Lp usually significantly outperforms minimization of L a even when the optimal combination weight γ is used for L a .</p><p>In order to compare the properties of the two different approaches for combination, we examine their first order derivatives. Let ξ denote the parameters used by the ranking function F (x). Then, the first order derivatives of L a and L p with respect to ξ are given as follows:</p><formula xml:id="formula_14">ξ La = n i,j=1 (T i,j + γW i,j ) exp(F j − F i )( ξ F (x j ) − ξ F (x i )) ξ L p = Lp i,j=1 (ai,j + bi,j) exp(Fj − Fi)( ξ F (xj) − ξ F (xi))</formula><p>where</p><formula xml:id="formula_15">a i,j = W i,j exp(F j − F i ) n i,j=1 W i,j exp(F j − F i ) (11) b i,j = Ti,j exp(Fj − Fi) n i,j=1 T i,j exp(F j − F i )<label>(12)</label></formula><p>Note that both derivative shares similar structures. The key difference between ξ L a and ξ L p is that in ξ L p , a i,j and bi,j are used to weight the contribution from W and T for instance pair (xi, xj) when computing the derivative. This is in contrast to ξ L a where the weights for instance pair</p><formula xml:id="formula_16">(x i , x j ) are γW i,j exp(F j − F i ) and T i,j exp(F j − F i ). The</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Boosting algorithm for minimizing L p</head><p>1: Compute W i,j and T i,j based on the ranked list and the set of labeled pairs 2: Initialize F (x) = 0 for all instances 3: repeat 4:</p><p>Compute γi,j for each instance pair as</p><formula xml:id="formula_17">γ i,j = a i,j + b i,j<label>(13)</label></formula><p>where a i,j and b i,j are defined in (11) and (12). 4:</p><p>Compute the weight for each instance as</p><formula xml:id="formula_18">wi = n j=1 γi,j − γj,i<label>(14)</label></formula><p>4: Assign each instance the class label y i = sign(w i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Train a classifier f (x) : R d → {0, 1} that maximizes the following quantity</p><formula xml:id="formula_19">θ = n i=1 |w i |f (x i )y i<label>(15)</label></formula><p>6:</p><p>Predict fi for all instances in D 7:</p><p>Compute combination weight α as follows:</p><formula xml:id="formula_20">α = 1 2 log n i,j=1 γ i,j δ(f i , 1)δ(f j , 0) n i,j=1 γi,jδ(fj, 1)δ(fi, 0)<label>(16)</label></formula><p>where Update the ranking function as</p><formula xml:id="formula_21">f i = f (x i ). δ(x,</formula><formula xml:id="formula_22">F (x) ← F (x) + αf (x)<label>(17)</label></formula><p>9: until reach the maximum number of iterations main advantage of using a i,j and b i,j comes from the fact that they are normalized, i.e., n i,j=1 a i,j = n i,j=1 b i,j = 1, and therefore the contributions from W and T are naturally balanced when calculating the derivative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Boosting Algorithm for Ranking Refinement</head><p>In this section, we will consider algorithms for learning the ranking function F (x) by respectively minimizing the objective function L a and L p . The objective function L a is similar to the objective function used by Rank-Boost except that a weight (Ti,j +γWi,j) is used for each instance pair. We thus can simply modify the Rank-Boost algorithm to learn the optimal ranking function F (x). Hence, in the sequel, we will focus on the boosting algorithm for minimizing L p .</p><p>To learn the optimal ranking function F (x), we follow the greedy approach of boosting algorithms. Since the information for training is a set of labeled instance pairs, a straightforward boosting approach is to iteratively update the weights of instance pairs and train a new ranking function for the given weighted pairs. This is the strategy employed in the Rank-Boost algorithm <ref type="bibr" target="#b5">[6]</ref>. However, since the number of instance pairs is O(n 2 ), this approach could be computationally expensive when the number of instance n is large.</p><p>To address the above problem, we present a new boosting algorithm that converts the weights of instance pairs into weights for individual instances. for the target objective that decouples functions for pairs of instances into functions for individual instances. It is this decoupling that makes it possible to infer weights for individual instances from weights for instance pairs. In addition, the new boosting algorithm is able to derive an appropriate binary class label for each instance using the computed weights. Using both the weights and the class labels of instances, we can train a binary classifier f : R d → {0, +1} and update the overall ranking function by</p><formula xml:id="formula_23">F (x) = F (x)+αf (x)</formula><p>where α is the combination weight. Note that by converting a ranking problem into a series of binary classification problems, the new boosting algorithm avoids the high computational cost arising from the large number of instance pairs.</p><p>Algorithm 1 summarizes the overall procedures for the proposed boosting algorithm minimizing Lp. As the first step in the iteration, we compute γi,j for every pair of instances that measures the uncertainty of ranking instance x i ahead of x j . Next, we calculate the weight for instance x i as w i = n j=1 γ i,j − γ j,i . It is important to note that w i can be both positive and negative. In particular, wi &gt; 0 indicates that it is more likely to have xi ranked on the top of the ranked list than on the bottom of the list; w i ≤ 0 indicates the opposite. Hence, we can derive the class label y i for x i based on the sign of wi: a positive class for placing instances on the top of the ranked list, and a negative class for placing instances on the bottom of the list. Since |w i | indicates the overall confidence in deciding the ranking position of x i in the list, it is used to weight the importance of individual instances. With this information, we will train a classifier that maximizes θ in (15), which can be interpreted as a sort of classification accuracy. Since most binary classifiers are unable to take weights into consideration, we will divide the training procedure into two steps: in the first step, we sample s instances according to the distribution that is proportional to the weights |wi|; we then train a binary classifier f : R d → {0, +1} using the sampled instances. We manually set s = max(20, n/5) in our empirical study. A similar strategy is employed in the AdaBoost algorithm <ref type="bibr" target="#b5">[6]</ref> and its effectiveness has been verified in empirical studies.</p><p>In the remaining of this section, we will give justification to the proposed algorithm described in <ref type="table">Table 1</ref>. The main result is summarized in Theorem 2.</p><p>Theorem 2. Let f k (x) denote the binary classification function obtained in the kth iteration, and γ k i,j denote γ i,j learned in that iteration. The objective function after T iterations, denoted by L T p , is bounded as follows:</p><formula xml:id="formula_24">L T p ≤ n i,j=1</formula><p>Ti,j n i,j=1</p><formula xml:id="formula_25">Wi,j exp − T k=1 ( √ µ k − √ ν k ) 2<label>(18)</label></formula><p>where</p><formula xml:id="formula_26">µ k = n i,j=1 γ k i,j δ(f k (x i ), 1)δ(f k (x j ), 0) ν k = n i,j=1 γ k i,j δ(f k (x i ), 0)δ(f k (x j ), 1)</formula><p>The above theorem essentially shows that by using the proposed algorithm, the objective function L p will be reduced exponentially.</p><p>The key to proving Theorem 2 is to establish the relationship between the objective function Lp of two consecutive iterations. This is because by upper bounding the log-ratio between L p of two consecutive iterations, i.e.,</p><formula xml:id="formula_27">rt ≥ log L t p − log L t−1 p ,<label>(19)</label></formula><p>we will have</p><formula xml:id="formula_28">L T p = L 0 p T t=1 L t p L t−1 p ≤ L 0 p exp T t=1 r t<label>(20)</label></formula><p>For the convenience of presentation, in the following, we only consider two consecutive iterations without specifying the index of iteration. Instead, we denote the quantities of the current iteration by symbol˜to differentiate the quantities of the previous iteration. In order to establish an upper bound for the log ratio, we first introduce the following lemma Lemma 1. Assume˜FAssume˜ Assume˜F (x) = F (x)+αf (x) where˜Fwhere˜ where˜F (x) and F (x) are the ranking functions of two consecutive iterations, respectively. f : R d → {0, 1} is a binary classifier and α is the combination weight. We have the following inequality hold for any F , f , and α:</p><formula xml:id="formula_29">log˜L log˜ log˜L p Lp ≤ −2 + n i,j=1 (a i,j + b i,j ) exp(α(f j − f i )) (21)</formula><p>where a i,j and b i,j are defined <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>, respectively.</p><p>The proof of Lemma 1 can be found in Appendix B. Using the Lemma 1, we present the proof of Theorem 2 in Appendix C. Finally, we can show the relationship between the objective function Lp and the quantity θ (in <ref type="bibr" target="#b14">(15)</ref>) that is used to guide the training of binary classifiers in iterations. This result is summarized in the following theorem:</p><p>Theorem 3. Let θ k denote the value of the quantity θ (in <ref type="bibr" target="#b14">(15)</ref>) that is maximized by the binary classifier f k (x) learned in the kth iteration. Assume that θ k ≥ 0 for each iteration. Then, the objective function after T iterations, denoted by L T p , is bounded as follows:</p><formula xml:id="formula_30">L T p ≤ n i,j=1 T i,j n i,j=1 W i,j exp − T k=1 θ k<label>(22)</label></formula><p>The proof of the above theorem can be found in Appendix D. Theorem 3 provides a theoretical justification for Algorithm 1.</p><p>In particular, by maximizing θ, Algorithm 1 effectively reduces the objective function L p . This is further confirmed by our empirical study. <ref type="figure">Figure 1</ref> shows an example of reduction in the objective function Lp. We clearly see that the objective function is reduced exponentially and receives the largest reduction during the first few iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we evaluate the proposed algorithm for ranking refinement by two tasks, i.e., user relevance feedback and recommender system. The objectives of our experiments are: (1) to compare the proposed algorithm for ranking refinement to the existing ranking algorithms, <ref type="formula" target="#formula_1">(2)</ref> to examine the performance of the proposed algorithm for ranking refinement with different numbers of training instances, (3) to examine the effect of different base rankers on the performance of the proposed algorithm, and (4) to examine the time efficiency of the proposed algorithm for ranking refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For the Relevance Feedback experiment, we used the LETOR testbed <ref type="bibr" target="#b19">[20]</ref> that includes the OHSUMED dataset and the datasets from TREC 2003 and 2004. The OHSUMED dataset consists of 106 queries. For each query, a number of documents are retrieved and their relevance to the query is given at three levels: definitely (2), possibly (1), or irrelevant (0). There are a total of 16, 140 query-document relevance judgments. For each query-document pair, a total of 25 ranking features are extracted. There are 50 queries in the dataset of <ref type="bibr">TREC 2003</ref>, and 75 queries in TREC 2004. For each query, about 1000 documents are retrieved, which amounts to a total of 49, 171 query-document pairs for TREC 2003 and 74, 170 query-document pairs for TREC 2004. A binary relevance judgment is provided for each query-document pair. There are 44 features extracted for each query-document pair. The detailed information about OHSUMED and TREC data sets are available in <ref type="bibr" target="#b19">[20]</ref>.</p><p>For the Recommender System experiment, we used the MovieLens dataset, available at <ref type="bibr" target="#b18">[19]</ref>, contains 100, 000 ratings (from 1 to 5) for 1682 movies given by 943 users. Each movie is represented by 51 binary features: 19 features are derived from the genres of movies and the rest 32 features are derived from the keywords that are used to describe the content of movies 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Algorithms</head><p>To examine the effectiveness of the proposed algorithm for ranking refinement, we compared the following ranking algorithms:</p><p>Base Ranker: It is the base ranker used in the ranking refinement.</p><p>Rocchio: This algorithm extends the standard Rocchio algorithm for user relevance feedback and it creates a new query vector by linearly combining the query vector and vectors of feedback documents. Given the initial query Q 0 , the relevant documents (R 1 , R 2 , ..., R n 1 ) <ref type="bibr" target="#b1">2</ref> We downloaded the keywords of each movie from the online movie database IMBD. The 32 most popular keywords used by the 1682 movies were selected. and non-relevant documents (S 1 , S 2 , ..., S n 2 ), the new query according to Rocchio is:</p><formula xml:id="formula_31">Q = Q0 + α n 1 i=1 Ri n1 − β n 2 i=1</formula><p>Si n2</p><p>Note, in our case, that each document is not represented by a vector of word frequency, but a vector of features that are computed based on its match to the query. Hence, we don't have Q0, i.e., the representation vector for query itself. We therefore set Q0 to be a vector of all zeros. We used the inner product between the new query and documents as the scores to rank the documents. To obtain the best performance, we vary α and β from 1 to 10 and choose the best setting.</p><p>SVM: This implements the Ranking-SVM algorithm using the SVM light package. Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM. The experimental results provided in the LETOR collection also confirm this. Hence, we only compare the proposal algorithm with Ranking-SVM, but not Rank-Boost.</p><p>MRR: This is the Multiplicative Ranking Refinement algorithm that minimizes Lp in (10).</p><p>LRR: This is the Linear Ranking Refinement algorithm that minimizes La in (9). Since the performance of LRR depends on the parameter γ, we run LRR with 100 different values from 0.1 to +10 and choose the best and worst performance. We referred them to as LRR-Worst and LRR-Best, respectively.</p><p>For a fair comparison, the output from the base ranker is used as an extra feature when using SVM (i.e., Ranking-SVM) and Rocchio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation Metrics</head><p>To evaluate the performance of different algorithms, we used precision and normalized discounted cumulative gain (NDCG) at rank position k. Let (d R 1 , d R 2 , ..., d R n ) denote the top ranked documents according to the ranker R, and (r R 1 , r R 2 , .., r Rn ) denote their binary judgments. The precision at rank position k measures the relevancy of the first k documents and is defined as follows</p><formula xml:id="formula_33">P R @k = k i=1 r R i /k</formula><p>For the OHSUMED dataset, a document is deemed relevant when its score is two. In the case of MovieLense data, a movie is deemed to be interesting to a test user when its rating is no less than 4 3 . Since the first top documents are more important than the other documents, we also employ the NDCG metric <ref type="bibr" target="#b20">[21]</ref> that is defined as follows:</p><formula xml:id="formula_34">N DCG R @k = DCGR@k DCG T @k</formula><p>where T stands for the oracle ranker and DCG X @k is defined as follows <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_35">DCG X @k = rX 1 if k = 1 r X 1 + k i=2 r X i log 2 i if k &gt; 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation Protocol</head><p>Unless specified, for all the experiments with relevance feedback, we used the standard BM25 retrieval algorithm (i.e., the 21st feature in OHSUMED data set and 16th in TREC data sets) as the base ranker. We followed the common practice of user relevance feedback by collecting the relevance judgments for the first 10 retrieved documents. These user relevance judgments served as labeled instances in ranking refinement.</p><p>For the experiment with recommender system, the base ranker was created by applying a collaborative filtering algorithm, more specifically, the Personality Diagnosis algorithm <ref type="bibr" target="#b2">[3]</ref>, to the user rating data. In particular, 20 users were randomly selected as the training users, and the remaining 923 users were used for testing. For each test user, 10 rated movies were randomly selected and were used by the collaborative filtering algorithm to identify the 20 training users who share the common interests with the test user. Note that we did not compare the proposed algorithm to other information filtering algorithms because the focus of this study is to examine the effectiveness and the generality of the proposed approach for ranking refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results for Relevance Feedback</head><p>Figure 2 and 3 show the performance results of different algorithms in terms of precision and NDCG for the first 25 ranked documents. First, by comparing the performance of the two variants of ranking refinement, we observed that the Multiplicative Ranking Refinement (MRR) algorithm is significantly more effective than the Linear Ranking Refinement (LLR) algorithm. Indeed, MRR performs significantly better than the best case of LRR (i.e., LRR-best). The key difference between MRR and LRR is that MRR minimizes the product of the two error functions while LRR minimizes the weighted sum. We believe it is the normalization scheme brought by MRR (see equations in <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>) that makes it performing better than LRR. Second, comparing to the other three baseline algorithms, i.e., the base ranker, Rocchio, Ranking-SVM, we observed that MRR always significantly outperforms the baseline algorithms in all the cases. More noticeable is the improvement made by the ranking refinement over the first a few ranking positions. We thus conclude that Multiplicative Ranking Refinement is more effective than the baseline algorithms for user relevance feedback in information retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Quality of Base Rankers</head><p>To examine the robustness of the proposed algorithm with respect to the imperfectness of the base ranker, we tested the MRR algorithms with three different base rankers. We plotted the results of all different features and selected three features which cover a good range of ranking quality. The chosen base rankers for OHSUMED data set are the features 7, 11, and 21 and those for the TREC data sets are features 16, 21, and 36. <ref type="figure" target="#fig_6">Figure 4</ref> shows how the MRR algorithm performs using different base rankers (NDCG shows similar results). The result indicates that the quality of base rankers has a direct impact on the performance of the MRR algorithm. We also observed that the proposed algorithm is able to significantly improve the performance even with a poor base ranker. More impressively, by comparing <ref type="figure" target="#fig_6">Figure 4</ref> to <ref type="figure" target="#fig_4">Figure 2</ref>, we observed that even using the worst base ranker (i.e., feature 7 for OHSUMED, 21 for TREC 2003 ,and 36 for TREC 2004), the retrieval accuracy of MRR is comparable to the other methods using the best base ranker (i.e., the BM25 retrieval algorithm). We thus conclude that the MRR algorithm is resilient to the imperfectness of base rankers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Size of Feedback Data</head><p>To investigate the effect of the number of feedback documents on the performance, we ran the MRR algorithm by varying the number of feedback documents from 5 to 20. <ref type="figure">Figure 5</ref> shows the result using varied number of feedback documents. We clearly observed that the number of feedback documents have a direct effect on the performance of ranking refinement. However, even with a small amount of feedback, MRR is able to improve the retrieval performance considerably, particularly for the accuracy of the first few ranked documents. We thus conclude that the proposed algorithm for ranking refinement is robust to the size of feedback data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results for Recommender System</head><p>We evaluated the generality of the proposed algorithm by applying it to recommender system (movie recommendation). <ref type="figure">Figure 6</ref>(a) and <ref type="figure">Figure 6(b)</ref> show the results of different algorithms when applied on the MovieLens dataset. It is surprising to observe that the results of LRR, the linear ranking refinement algorithm, even with the tuned parameter γ, is not even comparable to the the performance of the base ranker. In contrast, the MRR algorithm is able to significantly improve the accuracy of the base ranker and outperform the other baseline algorithms considerably. This result further indicates the importance of appropriately combining the two information sources, i.e., the ranking information behind the base ranker and the feedback information provided by users. <ref type="figure">Figure 6</ref>(c) shows the sensitivity of MRR to the size of feedback data by varying the number of rated movies by the test user from 5 to 20. Similar to the result for relevance feedback, we observed that the size of feedback data affects the performance of MRR considerably. However, even with 5 rated movies, the MRR algorithm is able to make a noticeable improvement in the prediction accuracy compared to the base ranker. This result further confirms the robustness of the proposed algorithm for ranking refinement with respect to the size of feedback data. <ref type="figure">Figure 7</ref> shows the efficiency of the MRR algorithm in terms of the running time for different numbers of rated movies for each test user. We chose movies data set for the experiment because it provides a good range for the number of objects. We partitioned the test users into groups where each group of users has a different number of rated movies. The running time of MRR for each group is calculated by averaging it across all the users in the group. As pointed in Section 3.4 and seen in <ref type="figure">Figure 7</ref>, the running time is linear in the number of instances. Note that the relatively long running time is due to the MATLAB implementation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Time Efficiency of Ranking Refinement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose the problem of ranking refinement, whose goal is to improve a given ranking function by a small number of labeled instances. The key challenge in combining the ranking information from the base ranker and the labeled instances arises from the fact that the information in the base ranker tends to be inaccurate and the information from the training data tends to be noisy. We present a boosting algorithm for ranking refinement that is resilient to the errors. Empirical studies with relevance feedback and recommender system show promising performance of the proposed algorithm.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><formula xml:id="formula_36">W i,j exp(F j − F i + α(f j − f i )) × n i,j=1</formula><p>Ti,j exp(Fj − Fi + α(fj − fi))</p><formula xml:id="formula_37">= n i,j=1 a i,j exp(α(f j − f i ) n i,j=1 b i,j exp(α(f j − f i )</formula><p>where a i,j and b i,j are defined in <ref type="formula" target="#formula_0">(11)</ref>  Ti,j + Wi,j, we obtain the result in Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. PROOF OF THEOREM 3</head><p>Proof. We rewrite the quantity θ as follows:</p><formula xml:id="formula_38">θ = n i=1 fiyi|wi| = n i=1 fi n j=1 γi,j − γj,i = n i,j=1 γi,j(fi − fj) = µ − ν Since µ − ν = √ µ − √ ν √ µ + √ ν ≥ √ µ − √ ν 2 ,</formula><p>we have θ ≥ √ µ − √ ν 2 . Substituting this result into the expression of Theorem 2, we have Theorem 3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Copyright is held by the International World Wide Web Conference Com- mittee (IW3C2). Distribution of these papers is limited to classroom use, and personal use by others. WWW 2008, April 21-25, 2008, Beijing, China. ACM 978-1-60558-085-2/08/04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y) outputs 1 if x = y and zero otherwise. Break the loop if α ≤ 0 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>The key idea behind the new boosting algorithm is to derive an upper bound 400 WWW 2008 / Refereed Track: Search -Ranking &amp; Retrieval Enhancement April 21- 25 , 2008 Figure 1 :</head><label>4002520081</label><figDesc>Figure 1: Reduction of the objective function L p using the OHSUMED Data Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision of relevance feedback for different algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 7 :</head><label>37</label><figDesc>Figure 3: NDCG of relevance feedback for different algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision of MRR with different base rankers for relevance feedback</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Precision of MRR with different numbers of feedback documents for relevance feedback</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>and (12). Thus, we have an upper bound of the log ratio as follows log˜L log˜ log˜L p L = log n i,j=1 ai,j exp(α(fj − fi)) + log n i,j=1 b i,j exp(α(f j − f i )) ≤ −2 + n i,j=1 (ai,j + bi,j) exp(α(fj − fi)) The second inequality follows the concaveness of the loga- rithm function, i.e., log x ≤ x − 1 for any x &gt; 0. C. PROOF OF THEOREM 2 Proof. Using the upper bound expressed in Lemma 1, we have log˜L log˜ log˜L p Lp + 2 ≤ n i,j=1 γ i,j exp(α(f j − f i )) = n i,j=1 γ i,j δ(f j , 1)δ(f i , 0) exp(α) + n i,j=1 γ i,j δ(f j , 0)δ(f i , 1) exp(−α) Using the definition of α in (16), we have log˜L log˜ log˜L p L p ≤ −2+ 2 n i,j=1 γ i,j δ(f j , 1)δ(f i , 0) n i,j=1 γ i,j δ(f j , 0)δ(f i , 1) = −2 + 2 √ µν In the above, we use the definitions of µ and ν in Theorem 1 to simplify the expression. Since 2 = n i,j=1 γ i,j ≥ µ + ν, we have log˜L log˜ log˜L p L p ≤ −2 + 2 √ µν ≤ −µ − ν + 2 √ µν = − √ µ − √ ν 2 We thus have log L t p L t−1 p ≤ rt = − ( √ µt − √ νt) 2 Substituting the above expression for rt into (20), and fur- ther using the fact L0 = n i,j=1</figDesc></figure>

			<note place="foot" n="1"> This is because any labeled instances can be converted into ordered pairs while the converse is not true.</note>

			<note place="foot" n="3"> We have experimented with other thresholds and find similar results in our empirical study. We did not present results for the other thresholds due to the space limitation.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. PROOF OF THEOREM 1</head><p>Proof. First, note that the objective function L p is convex in terms of F. This is because L p can be expanded as follows:</p><p>Since Lp is a convex function, the solution found by minimizing L p will always be global optimal, instead of local optimal. Second, to show that the optimal solution found by minimizing Lp is Pareto efficient, we prove by contradiction. Let F * denote the global minimizer of function L p . By assuming that Theorem 1 is not correct, there will exist a solution F = F * that either (1)</p><p>. We can easily infer Lp(F) &lt; Lp(F * ) since (1) both err w and err t are non-negative for any solution F, and (2) L p = err w × err t . Clearly, this conclusion contracts the fact that F * is a global minimizer of L p .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The trec-9 filtering track final report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC9</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="25" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collaborative filtering by personality diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="115" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient boosting algorithm for combining preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="969" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">N</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminant model for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adapting ranking svm to document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A boosting algorithm for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ranking with large margin principle: Two approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gaussian processes for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relevance feedback revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Condorcet fusion for improved retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;02: Proceedings of the eleventh international conference on Information and knowledge management</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="538" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis</title>
		<meeting><address><addrLine>Princeton, N.J.</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple Criteria Optimization: Theory, Computation and Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Steuer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>John Wiley</publisher>
			<biblScope unit="page">546</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Theoretical views of boosting and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory, 10th International Conference, ALT &apos;99</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1720</biblScope>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MovieLens Data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grouplens</surname></persName>
		</author>
		<ptr target="http://www.grouplens.org/node/12" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">LETOR: Benchmark Datasets for Learning to Rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia</forename><surname>Microsoft Research</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
