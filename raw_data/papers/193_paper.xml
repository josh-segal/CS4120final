<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similarity Search: A Matching Based Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 12-15, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Univ. of Singapore ‡ Univ. of Melbourne § Univ. of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Univ. of Singapore ‡ Univ. of Melbourne § Univ. of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Koudas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Univ. of Singapore ‡ Univ. of Melbourne § Univ. of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">§</forename><surname>Beng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Univ. of Singapore ‡ Univ. of Melbourne § Univ. of Toronto</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Ooi</surname></persName>
							<email>ooibc@comp.nus.edu.sgrui@csse.unimelb.edu.aukoudas@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National Univ. of Singapore ‡ Univ. of Melbourne § Univ. of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Similarity Search: A Matching Based Approach</title>
					</analytic>
					<monogr>
						<title level="m">VLDB &apos;06</title>
						<meeting> <address><addrLine>Seoul, Korea</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">September 12-15, 2006</date>
						</imprint>
					</monogr>
					<note>1-59593-385-9/06/09.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Similarity search is a crucial task in multimedia retrieval and data mining. Most existing work has modelled this problem as the nearest neighbor (NN) problem, which considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks: 1) it leaves many partial similarities uncovered; 2) the distance is often affected by a few dimensions with high dissimilarity. To overcome these drawbacks, we propose the k-n-match problem in this paper. The k-n-match problem models similarity search as matching between the query object and the data objects in n dimensions , where n is a given integer smaller than dimension-ality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. The k-n-match query is expected to be superior to the kNN query in discovering partial similarities, however, it may not be as good in identifying full similarity since a single value of n may only correspond to a particular aspect of an object instead of the entirety. To address this problem, we further introduce the frequent k-n-match problem, which finds a set of objects that appears in the k-n-match answers most frequently for a range of n values. Moreover, we propose search algorithms for both problems. We prove that our proposed algorithm is optimal in terms of the number of individual attributes retrieved , which is especially useful for information retrieval from multiple systems. We can also apply the proposed al-gorithmic strategy to achieve a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we show that: 1) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities; 2) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Similarity search is a crucial task in many multimedia and data mining applications and extensive studies have been performed in the area. Usually, the objects are mapped to multi-dimensional points and similarity search is modelled as a nearest neighbor search in a multi-dimensional space. In such searches, comparison between two objects is performed by computing a score based on a similarity function like Euclidean distance <ref type="bibr" target="#b4">[8]</ref> which essentially aggregates the difference between each dimension of the two objects. The nearest neighbor model considers the distance between the query object and the data objects over a fixed set of features. Such an approach has two drawbacks: 1) it leaves many partial similarities uncovered since the distance computation is based on the fixed set of features; 2) the distance is often affected by a few dimensions with high dissimilarity. For example, consider the 10-dimensional database consisting of four data objects as shown in <ref type="figure">Figure 1</ref> and the query object (1,1,1,1,1,1,1,1,1,1 20 20 20 20 20 20 20 20 20 20</p><p>Figure 1: An Example Database based on Euclidean distance will return object 4 as the answer. However, it is not difficult to see that the other three objects are actually more similar to the query object in 9 out of the 10 dimensions but are considered to be further away due to large dissimilarity in only one dimension (the dimension with value 100). Such a phenomenon becomes more obvious for high-dimensional data when the probability of encountering big differences in some of the dimensions is higher. These high dissimilarity dimensions happen often in real applications such as bad pixels, wrong readings or noise in a signal. Moreover, think of the example database as features extracted from pictures, and suppose the first three dimensions represent the color, the second three dimensions represent the texture and the last four dimensions represent the shape (there may be more number of features in reality). We can see that the nearest neighbor based on Euclidean distance returns a picture which is not that similar to the query picture in any aspects despite those pictures that have exact matches on certain aspects (e.g., picture 3 matches the query's color and texture exactly). This shows how finding similarity based on a fixed set of features overlook the partial similarities.</p><p>To overcome these drawbacks, we propose the k-n-match problem in this paper. For ease of illustration, we will start with the n-match problem, which is the special case when k equals 1. Alternatively, we can view the k-n-match problem as finding the top k answers for the n-match problem.</p><p>The n-match problem models similarity search as matching between the query object and the data objects in n dimensions, where n is a given integer smaller than dimensionality d and these n dimensions are determined dynamically to make the query object and the data objects returned in the answer set match best. A key difference here is that we are using a small number (n) of dimensions which are determined dynamically according to the query object and a particular data object, so that we can focus on the dimensions where these two objects are most similar. By this means, we overcome drawback <ref type="bibr">(2)</ref>, that is, the effects of the dimensions with high dissimilarities are suppressed. Further, by using n dimensions, we are able to discover partial similarities so that drawback (1) is overcome. To further give an intuition on the method that we are proposing in this paper, let us consider the process of judging the similarity between two persons. Given the large number of features (eye color, shape of face etc.) and the inability to give a very accurate measure of the similarity for each feature, a quick approach is to approximate the number of features in which we judge to be very close and claim that the two persons are similar if the number of such features is high. Still consider the example in <ref type="figure">Figure 1</ref>, if we issue a 6-match query, object 3 will be returned, which is a more reasonable answer than object 4. However, we may not always have exact matches in reality especially for continuous value domains. For example, objects 1 and 2 are also close to the query in many dimensions although they are not exact matches. Therefore, we use a more flexible match scheme, that is, pi (data value in dimension i) matches q i (query value in dimension i) if their difference is within a threshold δ. If we set δ to 0.2, we would have an additional answer, object 1, for the 6-match query. A new problem here is how we determine δ. We still leave this choice self-adaptive with regard to the data and query. Specifically, for a data object P and a query Q, we first sort the differences |p i − q i | in all the dimensions and obtain the n-th smallest difference, called P 's n-match difference with regard to Q. Then, among all the data objects, the one with the smallest n-match difference determines δ, that is, δ is this smallest n-match difference. For the k-nmatch query, δ equals the k-th smallest n-match difference and therefore k objects will be returned as the answer.</p><p>While the k-n-match query is expected to be superior than the kNN query in discovering partial similarities, it may not be as good in finding full similarity since a single value of n may only correspond to one aspect of an object instead of the entirety. To address the problem, we further introduce the frequent k-n-match query. In the frequent k-n-match query, we first find out the k-n-match solutions for a range of n values, say, from 1 to d. Then we choose k objects that appear most frequently in the k-n-match answer sets for all the n values.</p><p>A naive algorithm for processing the k-n-match query is to compute the n-match difference of every point and return the top k answers. The frequent k-n-match query can be done similarly. We just need to maintain a top k answer set for each n value required by the query while checking every point. However, the naive algorithm is expensive since we have to scan the whole database and hence every attribute of every point is accessed. In this paper we propose an algorithm that works on a different organization of the data objects, namely, each dimension of the data set is sorted. Our algorithm accesses the attributes in each dimension in ascending order of their differences to the query in corresponding dimensions. We call it the ascending difference (AD) algorithm. We prove that the AD algorithm is optimal for both query types in terms of the number of individual attributes retrieved given our data organization. Our model of organizing data as sorted dimensions and using number of attributes retrieved as cost measure matches very well the setting of information retrieval from multiple systems <ref type="bibr" target="#b7">[11]</ref>. Our cost measure also conforms with the cost model of disk based algorithms, where the number of disk accesses is the major measure of performance, and the number of disk accesses is proportional to the attributes retrieved. So we also apply our algorithmic strategy to achieve an efficient disk based algorithm.</p><p>By a thorough experimental study using both real and synthetic data sets, we show that: 1) the k-n-match query yields better result than the kNN query in identifying similar objects by partial similarities; 2) our proposed method (for processing the frequent k-n-match query) outperforms existing techniques for similarity search in terms of both effectiveness and efficiency.</p><p>The rest of the paper is organized as follows. First, we formulate the k-n-match and the frequent k-n-match problems in Section 2. Then we propose the AD algorithm for processing the (frequent) k-n-match problem and discuss properties of the AD algorithm in Section 3. In section 4, we apply our algorithmic strategy to achieve a disk based solution. At the same time, we give an adapted algorithm from the VA-file technique as a competitive method for the disk based version of the problem. The experimental study is reported in Section 5 and related work is discuss in Section 6. Finally, we conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head><p>In this section, we formulate the k-n-match and the frequent k-n-match problems. As an object is represented as a multi-dimensional point, we will use object and point interchangeably in the remainder of the paper. Then a database is a set of d-dimensional points, where d is the dimensionality. The notation used in this paper is summarized in <ref type="table" target="#tab_1">Table  1</ref> for easy reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The K-N -Match Problem</head><p>For ease of illustration, we start with the simplest form of the k-n-match problem, that is, the n-match problem, which is the special case when k equals 1. Before giving the definition, we first define the n-match difference of a point P with regard to another point Q as follows: The number of n-match points to return n</p><p>The number of dimensions to match P A point pi</p><p>The coordinate of P in the i-th dimension Q</p><p>The query point q i</p><p>The coordinate of Q in the i-th dimension S A set of points Following our notation, Q represents the query point, therefore in the sequel, we simply say P 's n-match difference for short and by default it is with regard to the query point Q. Obviously, the n-match difference of point P with regard to Q is the same as the n-match difference of point Q with regard to P .</p><formula xml:id="formula_0">Definition 1. N -match difference Given two d-dimensional points P (p1, p2, ..., p d ) and Q (q 1 , q 2 , ..., q d ), let δ i = |p i − q i |, i = 1, ..., d</formula><p>Next, we give the definition of the n-match problem as follows:</p><p>Definition 2. The n-match problem Given a d-dimensional database DB, a query point Q and an integer n (1 ≤ n ≤ d), find the point P ∈ DB that has the smallest n-match difference with regard to Q. P is called the n-match point of Q. 2</p><p>In the example of <ref type="figure">Figure 1</ref>, point 3 is the 6-match (δ=0) of the query, point 1 is the 7-match (δ=0.2) and point 2 is the 8-match (δ=0.4). <ref type="figure" target="#fig_0">Figure 2</ref> shows a more intuitive example in 2-dimensional space. A is the 1-match of Q because it has the smallest Intuitively, the query finds a point P that matches Q in n dimensions. If we consider the maximum difference in these n dimensions, P has the smallest maximum difference to Q among all the points in DB.</p><p>This definition conforms with our reasoning in the example of <ref type="figure">Figure 1</ref>, which actually uses a modified form of Hamming distance <ref type="bibr" target="#b11">[15]</ref> in judging the similarity exhibited by the first three points. The difference however is that we are working on spatial attributes while Hamming distance is typically used for categorical data <ref type="bibr">1</ref> .</p><p>If we view n-match difference as a distance function, we can see that the n-match problem is still looking for the nearest neighbor of Q. The key difference is that the distance is not defined on a fixed set of dimensions, but dynamically determined based on the query and data points. The n-match difference differs from existing similarity scores in two ways. First, the attributes are discretized dynamically by determining a value of δ on the fly. Given a value of δ, determining a match or a mismatch is performed independently without aggregating the actual differences among the dimensions. Because of this, dimensions with high dissimilarities are not accumulated, making comparison more robust to these artifacts. Second, in the n-match problem, the number of dimensions that are deemed close are captured in the final result. Existing similarity measure can generally be classified into two approaches. For categorical data, the total number of dimensions in which there are matches are usually used. For spatial data, a distance function like Euclidean distance simply aggregates the differences without capturing any dimensional matching information. The approach of n-match can be seen as combination of the two, capturing the number of dimensional matches in terms of n and the spatial distances in terms of δ. This makes sense especially in high dimensional data in which we can leverage on a high value of d to provide statistical evidence that two points are similar if they are deemed to be close in most of the d dimensions.</p><p>Note that our distance function is not a generalization of the Chebyshev distance (or the L ∞ norm), which returns the maximum difference (made to positive) of attributes in the same dimension. The radical difference is that our function is not metric, particularly, it does not satisfy the triangular inequality. Consider the three 3-dimensional points F (0.1, 0.5, 0.9), G(0.1, 0.1, 0.1) and H(0.5, 0.5, 0.5). The 1-match difference between F and G, F and H, G and H are 0, 0, 0.4, respectively; they do not satisfy the triangular inequality of 0 + 0 &gt; 0.4.</p><p>In analogy to kNN with regard to NN, we further introduce the k-n-match problem as follows.</p><p>Definition 3. The k-n-match problem Given a d-dimensional database DB of cardinality c, a query point Q, an integer n (1 ≤ n ≤ d), and an integer k ≤ c, find a set S which consists of k points from DB so that for any point P 1 ∈ S and any point P 2 ∈ DB − S, the n-match difference between P 1 and Q is less than or equal to the nmatch difference between P 2 and Q. The S is the k-n-match set of Q.</p><p>For the example in <ref type="figure" target="#fig_0">Figure 2</ref>, {A, D, E} is the 3-1-match of Q while {A, B} is the 2-2-match of Q.</p><p>Obviously the k-n-match query is different from the skyline query, which returns a set points so that any point in the returned set is not dominated by any other point in the database. The skyline query returns {A, B, C} for the example in <ref type="figure" target="#fig_0">Figure 2</ref>, while the k-n-match query returns k points depending on the query point and the k value. None of the k-n-match query example shown above has the same answer as the skyline query.</p><p>While the k-n-match problem may find us similar objects through partial similarity, the choice of n introduces an additional parameter to the solution. It is evident that the most similar points returned are sensitive to the choice of n. To address this, we will further introduce the frequent k-nmatch query, which is described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Frequent K-N -Match Problem</head><p>The k-n-match query can help us find out similar objects through partial similarity when an appropriate value of n is selected. However, it is not obvious how such a value of n can be determined. Instead of trying to find such a value of n directly, we will instead vary n within a certain range (say, 1 to d) and try to compute some statistics on the set of matches that are returned for each n. Specifically, we first find out the k-n-match answer sets for a range [n 0 , n 1 ] of n values. Then we choose the k points that appear most frequently in the k-n-match answer sets for all the n values. Henceforth, we will say that the similar points generated from the k-n-match problem are based on partial similarity (only one value of n) while those generated from the frequent k-n-match problem are based on full similarity (all possible values of n). We use an example to illustrate the intuition behind such a definition. Suppose we are looking for objects similar to an orange. The objects are all represented by its features including color (described by 1 attribute), shape (described by 2 attributes) and other characteristics. When we issue a k-1-match query, we may get a fire and a sun in the answer set. When we issue a k-2-match query, we may get a volleyball and a sun in the answer set. The sun appears in both answer sets while none of the volleyball or the fire does, because the sun is more similar to the orange than the others, in both color and shape.</p><p>The definition of the frequent k-n-match problem is given below:</p><p>Definition 4. The frequent k-n-match problem Given a d-dimensional database DB of cardinality c, a query point Q, an integer k ≤ c, and an integer range [n 0 , n 1 ] within <ref type="bibr">[1, d]</ref>, let S 0 , ..., S i be the answer sets of k-n 0 -match, ..., k-n 1 -match, respectively. Find a set T of k points, so that for any point P1 ∈ T and any point P2 ∈ DB − T , P1's number of appearances in S0, ..., Si is larger than or equal to P 2 's number of appearances in S 0 , ..., S i .</p><p>The range [n 0 , n 1 ] can be determined by users. We can simply set it as <ref type="bibr">[1, d]</ref>. As in our previous discussion, full number of dimensions usually contains dimensions of large dissimilarity, therefore setting n 1 as d may not help much in the effectiveness. On the other hand, too few features can hardly determine a certain aspects of an object and matching on a small number of dimensions may be caused by noises. Therefore, we may set n 0 as a small number, say 3, instead of 1. We will investigate more on the effects of n 0 and n 1 in Section 5 through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ALGORITHMS</head><p>In this section, we propose an algorithm to process the (frequent) k-n-match problem with optimal cost under the following model, namely, attributes are sorted in each dimension and the cost is measured by the number of attributes retrieved. This model makes sense in a number of settings. For example, in information retrieval from multiple systems <ref type="bibr" target="#b7">[11]</ref>, objects are stored in different systems and given scores by each system. Each system will sort the objects according to their scores. A query retrieves the scores of objects (by sorted access) from different systems and then combines them using an aggregation function to obtain the final result. In this whole process, the major cost is the retrieval of the scores from the systems, which is proportional to the number of scores retrieved. <ref type="bibr" target="#b7">[11]</ref> has focused on aggregation functions such as min and max. Besides these functions, we could also perform similarity search over the systems and implement similarity search as the (frequent) k-n-match query. Then the scores from different systems become the attributes of different dimensions in the (frequent) k-n-match problem, and the algorithmic goal is to minimize the number of attributes retrieved. Further, the cost measure also conforms with the cost model of disk based algorithms, where the number of disk accesses is the major measure of performance, and the number of disk accesses is proportional to the attributes retrieved. However, unlike the multiple system information retrieval case, disk based schemes may make use of indexes to reduce disk accesses, which adds some complexity to judge which strategy is better. We will analyze these problems in more detail in Section 4.</p><p>A naive algorithm for processing the k-n-match query is to compute the n-match difference of every point and return the top k answers. The frequent k-n-match query can be done similarly. We just need to maintain a top k answer set for each n value required by the query while checking every point. However, the naive algorithm is expensive since every attribute of every point is retrieved. We hope to do better and access less than all the attributes. We will propose an algorithm, called the AD algorithm, that retrieves minimum number of attributes in Section 3.1.</p><p>Note that the algorithm proposed in <ref type="bibr" target="#b7">[11]</ref> for aggregating scores from multiple systems, called FA, does not apply to our problem. They require the aggregation function to be monotone, but the aggregation function used in k-n-match (that is, n-match difference) is not monotone. We use an example to explain this. Consider the database in <ref type="figure" target="#fig_1">Figure 3</ref>  </p><formula xml:id="formula_1">(p 1 , ..., p d ) ≤ f (p 1 , ..., p d ) whenever p i ≤ p i for every i = 1, ..., d (or p i ≥ p i for every i = 1, ..., d).</formula><p>In the example, point 1 is smaller than point 2 in every dimension, but its 1-match difference (2.6) is larger than point 2's 1-match difference (0.2). Point 4 is larger than point 2 in every dimension, but its 1-match difference (2.0) is still larger than point 2's 1-match difference (0.2). This example shows that the n-match difference is not a monotone aggregation function. If we use the FA algorithm here, we get point 1, which is a wrong answer (the correct answer is point 2). The reason is that the sorting of the attributes in each dimension is based on the attribute values, but our ranking is based on the differences to the query. Furthermore, the score we obtained from the aggregation function (n-match difference) is based on a dynamically determined dimension set instead of all the dimensions. Next we present the AD algorithm, which guarantees correctness of the answer and retrieves minimum number of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The AD Algorithm for K-N -Match Search</head><p>Recall the model that the attributes are sorted in each dimension; each attribute is associated with its point ID. Therefore, we have d sorted lists. Our algorithm works as follows. We first locate each dimension of the query Q in the d sorted lists. Then we retrieve the individual attributes in ascending order of their differences to the corresponding attributes of Q. When a point ID is first seen n times, this point is the first n-match. We keep retrieving the attributes until k point ID's have been seen at least n times. Then we can stop. We call this strategy of accessing the attributes in Ascending order of their Differences to the query point's attributes as the AD algorithm. Besides the applicability due to the aggregation function, the AD algorithm has another key difference from the FA algorithm in the accessing style. The FA algorithm accesses the attributes in parallel, that is, if we think of the sorted dimensions as columns and combine them into a table, the FA algorithm would access the "records" in the table one row after another. But the AD algorithm access the attributes in ascending order of their differences to the corresponding query attributes. If a parallel access was used, we would retrieve more attributes than necessary as can be seen from the optimality analysis in Theorem 3.2.</p><p>The detailed steps of the AD algorithm for k-n-match search, namely "KNMatchAD", is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. Line 1 initializes some structures used in the algorithm. appear <ref type="bibr">[i]</ref> maintains the number of appearances of point i. It has c elements, where c is the cardinality of the database 2 , and all the elements are initialized to 0. h maintains the number of point ID's that have appeared n times and is initialized to 0. S is the answer set and initialized to ∅. Line 3 finds the position of qi in dimension i using a binary search, since each dimension is sorted.  Each element of g[ ] is a triple (pid, pd, dif ) where pid is the point ID of the attribute, pd is the dimension and dif is the difference between q pd and the next attribute to access in dimension pd. For example, first we retrieve the largest attribute in dimension 1 that is smaller than q 0 , let it be a0 and let its point ID be pid0. We use them to form the triple (pid0, 0, q0 − a0), and put this triple into g <ref type="bibr">[0]</ref>. Similarly, we retrieve the smallest attribute in dimension 1 that is larger than q 0 and form a triple to be put into g <ref type="bibr">[1]</ref>. We do the same thing for other dimensions. After initializing g[ ], we begin to pop out values from it in the ascending order of dif . The function "smallest" in line 6 returns the triple with the smallest dif from g[ ]. Whenever we see a pid, we increase its number of appearance by 1 (line 7). When a pid appears n times, an n-match is found, therefore h is increased by 1 and the pid is added to S. After popping out an attribute from g[ ], we retrieve the next attribute in the same dimension to fill the slot. Next, we continue to pop out triples from g[ ] until h reaches k, and then the algorithm terminates.</p><p>We use the database in <ref type="figure" target="#fig_1">Figure 3</ref> as a running example to explain the algorithm, and suppose we are searching 2-2-match for the query (3.0, 7.0, 4.0). Hence k=n=2 in this query. First, we have each dimension sorted as in <ref type="figure" target="#fig_3">Figure  5</ref>, where each entry in each dimension represents a (point ID, attribute) pair. We locate qi in each dimension. q1 is between (2, 2.8) and (5, 3.5); q 2 is between (2, 5.5) and (3, 7.8); q 3 is between (2, 2.0) and (3, 5.0). Then we calculate the differences of these attributes to qi in the corresponding dimension and form triples, which are put into the array g[ ].</p><p>g[ ] becomes {(2, 0, 0.2), (5, 1, 0.5), (2, 2, 1.5), (3, 3, 0.8), (2, 4, 2.0), (3, 5, 1.0)}. Then we start popping triples out of g[ ] from the one with the smallest difference. First, (2, 0, We get (5, 1, 0.5), so appear[5] becomes 1 and (3, 1, 3.5) is put into g <ref type="bibr">[1]</ref>. Next, we get (3, 3, 0.8) from g[ ], so appear <ref type="bibr">[3]</ref> becomes 1 and (4, 3, 2.0) is put into g <ref type="bibr">[3]</ref>. Now g[ ]={(1, 0, 2.6), (3, 1, 3.5), (2, 2, 1.5), (4, 3, 2.0), (2, 4, 2.0), (3, 5, 1.0)}. Next, we get (3, 5, 1.0), so appear <ref type="bibr">[3]</ref> becomes 2, which equals n, and so h becomes 1. At this time we have found the first 2-match point, that is, point 3. (5, 5, 4.0) is put into g <ref type="bibr" target="#b1">[5]</ref>. Next we get (2, 2, 1.5) from g[ ], so appear <ref type="bibr">[2]</ref> becomes 2, which also equals n, and so h becomes 2, which equals k. At this time we have found the second 2-match, therefore the algorithm stops. The 2-2-match set is {point 2, point 3} and we also get the 2-2-match difference, 1.5.</p><p>In the implementation, we do not have to actually store pd in the triple since we can tell which dimension a (pid, dif ) pair is from when we get it from the sorted dimensions or from g[ ].</p><p>In what follows, we will prove the correctness and optimality of the AD algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.1. Correctness of KNMatchAD</head><p>The points returned by algorithm KNMatchAD compose the k-n-match set of Q.</p><p>Proof. We will prove that the k-th point that appears n times has the k-th smallest n-match difference.</p><p>First, we consider k = 1. Let the first point that appears n times be P , and when it appears the n-th time, let the difference between p i and q i be dif (i is the corresponding dimension). We are accessing the attributes in ascending order of their differences to qi, therefore dif is the n-match difference of P . Suppose P does not have the smallest nmatch difference, then there must exist a point P that has a smaller n-match difference, that is, P has at least n dimensions smaller than dif , and then P should have appeared n times. This result is contradictory to the fact that P is the first point that appears n times. Therefore, the supposition that P does not have the smallest n-match difference is wrong.</p><p>We can use a similar method as above to prove that the second point that appears n times must have the second smallest k-n-match difference, and so on. Therefore, the points returned by the algorithm KNMatchAD compose the k-n-match set of Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.2. Optimality of KNMatchAD</head><p>Among all algorithms that guarantee correctness for any data set instances, algorithm KNMatchAD retrieves the least attributes for the k-n-match problem.</p><p>Proof. Suppose another algorithm A retrieves one less attribute a than the attributes retrieved by KNMatchAD. Suppose a is dimension i of point pid 1 (for convenience, we may simply use a point ID to represent the point). a − q i must be smaller than the k-n-match difference δ (otherwise it would not be retrieved by KNMatchAD). In our model, data are sorted according to the attribute values. The algorithm only has information on the attribute value range but no information on the associated point ID at all. Therefore, as long as we keep the attribute values the same, an algorithm will retrieve the same values no matter how the associated point ID change. In other words, the set of attributes retrieved is irrespective to the point ID's. Given this observation, we can construct a data set instance as follows, which will make A produce wrong k-n-match answers.</p><p>Let point pid 2 be the point with the k-th smallest n-match difference, that is, it should be the last point to join the kn-match answer set. Let b be an attribute of point pid 2 that is less than δ. Suppose b is in dimension j, hence b − qj &lt; δ. Further, let point pid3 be the point with the (k + 1)-th smallest n-match difference and let this difference be smaller than point pid 2 's (n + 1)-match difference. If A returns the correct answer, then (b, pid2) is already retrieved when A finished searching. Now consider two (attribute, point ID) pairs in the sorted dimensions: (a, pid 1 ) and (b, pid 2 ). We exchange the point ID's of these two pairs and obtain a new data set instance with (a, pid 2 ) and (b, pid 1 ), while everything else is the same as the original data set. According to our observation, A is not aware of the change of the point ID's, and still will not retrieve the pair with attribute a. In this case, A can only find n − 1 dimensions less than δ for point pid 2 . Because of not retrieving (a, pid2), A thinks pid2's n + 1-match difference is its n-match difference, and hence will return point pid3 as the point with the k-th smallest n-match difference. Therefore, A will return wrong answers if it retrieves any less attribute than KNMatchAD does.</p><p>More generally, as long as an algorithm A knows nothing about the point ID before retrieving an attribute (the dimensions not necessarily sorted), A still have to retrieve all the attributes that KNMatchAD retrieves to guarantee correctness for any data set. The proof is the same as above. The multiple system information retrieval model satisfies the condition here, therefore KNMatchAD is optimal among all the algorithms that search k-n-match correctly, including those not based on attributes sorted at each system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The AD Algorithm for Frequent K-N -Match Search</head><p>For frequent k-n-match search, the AD algorithm works in a similar fashion as for k-n-match search. The difference is that, instead of monitoring point ID's that appear n times, we need to monitor point ID's whose number of appearances are in the range [n 0 , n 1 ].</p><p>The AD algorithm for frequent k-n-match search, namely "FKNMatchAD", is illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>. <ref type="bibr">Line</ref>   Before k points appear at least n 1 times, they must have already appeared n0 times, ..., n1 −1 times. Therefore, when algorithm FKNMatchAD stops, that is, when it finds the kn 1 -match answer set, it must have found all the k-i-match answer sets, where i = n 0 , ..., n 1 . Then we simply need to scan the k-i-match answer sets for i = n0, ..., n1 to get the k points that appear most frequently. This shows the correctness of the algorithm. At the same time, we can see that algorithm FKNMatchAD retrieves the same number of attributes as if we are performing a k-n 1 -match search by algorithm KNMatchAD. Since we have to at least retrieve the attributes necessary for answering the k-n1-match query, and we only need to retrieve this many to answer the frequent k-n-match query, we consequently have the following theorem:</p><formula xml:id="formula_2">Theorem 3.3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Optimality of FKNMatchAD</head><p>Algorithm FKNMatchAD retrieves the least attributes for the frequent k-n-match problem.</p><p>We can see that the frequent k-n-match search is no harder than a k-n-match search with the same k and n values. However, the frequent k-n-match query can take advantage of the results of a range of n values to obtain answers based on full similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISK BASED SOLUTIONS</head><p>In the previous section, we have investigated AD algorithms that are optimal in the model where cost is measured by the number of attributes retrieved. This model directly applies to the multiple system information retrieval problem. In this section, we would like to study how the AD algorithm works in the disk based model. Our cost measure still conforms with the disk model, where the number of disk accesses is the major measure of performance, and disk accesses is proportional to the attributes retrieved. However, one complication is that disk based algorithms may make use of auxiliary structures such as indexes or compression to prune data. R-tree based approaches have been shown to perform badly with high dimensional data due to too much overlap between page regions, and also no other indexing techniques can be applied directly to out problem because of the dynamic dimensions used for aggregating the score. Only compression techniques still apply such as the one used in VA-file <ref type="bibr" target="#b17">[21]</ref>, which does not rely on a fixed set of dimensions. Therefore, we will describe the disk based AD algorithm and an adaption from the VA-file algorithm for our problem in the following. We will use the VA-file based algorithm as a competitor when evaluating the efficiency of the AD algorithm in the experimental study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disk Based AD Algorithm</head><p>As we can see from Section 3.2, algorithm KNMatchAD is actually a special case of FKNMatchAD (when n 0 = n 1 ). Therefore we will focus on the frequent k-n-match search. First, we sort each dimension and store them sequentially on disk. Then we can use the same FKNMatchAD algorithm except that, when reading the next attribute from the sorted dimensions, if we reach the end of a page, we will read the next page from disk. Note that FKNMatchAD accesses the pages sequentially when search forwards, which makes the processing more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compression Based Approach</head><p>Compression based techniques such as the VA-file <ref type="bibr" target="#b17">[21]</ref> can be adapted to process the frequent k-n-match query. The algorithm runs in two phases. The first phase scans an approximation of the points, that is, the VA-file. During the first phase, the algorithm calculate lower and upper bounds of the k-n-match difference of each point based on the VAfile and utilizes these bounds to prune some points. Only the points that go through the first phase will be actually retrieved from the database for further checking in the second phase. We omit the implementation details here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL STUDY</head><p>In this section, we evaluate both the effectiveness and the efficiency of the (frequent) k-n-match query by an extensive experimental study. We use both synthetic uniform data sets and real data sets of different dimensionalities. The data values are all normalized to the range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. All the experiments were run on a desktop computer with 1.1GHz CPU and 500M RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effectiveness</head><p>To validate the statement that the traditional kNN query leaves many partial similarities uncovered, we first use a image database to visually show this fact in Section 5.1.1. In comparison, we show that the k-n-match query can identify similar objects by partial similarities if a proper n is chosen. But note that we are not using this to argue the effectiveness of the k-n-match approach for full similarity. The technique we use for full similarity search is the frequent k-n-match query and we will evaluate its effectiveness statistically in Section 5.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Searching by K-N -Match</head><p>We shall first visually show that the k-n-match query yields better result than the k-n-match search if a proper value of n is chosen. To do so, we use the COIL-100 database <ref type="bibr">[1]</ref>, which consists of 100 images. Some of the images in the database are shown in <ref type="figure" target="#fig_5">Figure 7</ref> (the numbers under the images are the image ID's). We extracted 54 features from these images such as color histograms and moments of area. Below we show a sample of the experiments we conducted and the results of other searches on the database exhibit similar behavior.</p><p>In this experiment, we used image 42 as the query object.     <ref type="table" target="#tab_6">Table 2</ref> shows the results returned by k-n-match with k = 4 and sampled n values varying from 5 to 50. The results of the kNN search is shown in <ref type="table" target="#tab_7">Table 3</ref> and the 10 nearest neighbors returned based on Euclidean distance are given. Comparing the two tables, the most obvious difference is the existence of image 78 in the k-n-match frequently which is not found in the 10 nearest neighbors of kNN search. Image 78 is a boat which is obviously more similar to image 42 compared to images 13, 64, 85, and 88 in the kNN result set. In fact, we did not find image 78 in the kNN result set even when finding 20 nearest neighbors. The difference in color between image 42 and 78 is clearly dominating all other aspects of comparison. The k-n-match query successfully identifies this object because of the use of partial matches.</p><p>Among the remaining k-n-match result, perhaps less noticeable is the existence of image 3. It is obviously more similar to image 42 than many images in the kNN result set and image 3 is in fact a yellow color and bigger version of image 42. However, it appears only once in the k-n-matches of different n values. If we are not using a good n value, we may miss this answer.</p><p>As can be seen from these results, k-n-match can yield better result than kNN search, but it also depends on a good choice of n. This motivates the use of the frequent kn-match query, which returns objects that have many partial matches with the query object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Searching by Frequent K-N -Match</head><p>We next evaluate the effectiveness of our proposed method, the frequent k-n-match query, for finding objects of full similarity. In order to evaluate effectiveness from a (statistically) quantitative view, we use the class stripping technique <ref type="bibr" target="#b2">[6]</ref>, which is described as follows. We use five real data sets from the UCI machine learning repository <ref type="bibr">[2]</ref> with dimensionalities varying from 4 to 34: 1) the ionosphere data set contains 351 34-dimensional points with 2 classes; 2) the image segmentation data set contains 300 19-dimensional points with 7 classes; 3)the wdbc data set contains 569 30-dimensional points with 2 classes; 4) the glass data set contains 214 9-dimensional points with 7 classes; 5) the iris data set contains 150 4-dimensional points with 3 classes. Each record has an additional variable indicating which class it belongs to. By the class stripping technique, we strip this class tag from each point and use different techniques to find the similar objects to the query objects. If the answer and the query belong to the same class, then the answer is correct. The more correct ones in the returned answers, statistically, the better the quality of the similarity searching method.</p><p>We run 100 queries which are sampled randomly from the data sets, k set as 20. We count the number of the answers with correct classification and divide it by 2000 to obtain the accuracy rates. Two techniques proposed previously: <ref type="bibr">IGrid [6]</ref> and the Human-Computer Interactive NN search (HCINN for short) <ref type="bibr" target="#b0">[4]</ref> have been shown to obtain more accurate results than the traditional kNN query. Therefore, we will compare the frequent k-n-match query with these two techniques. [n 0 ,n 1 ] for the frequent k-n-match query is simply set to <ref type="bibr">[1,d]</ref>. The results are shown in <ref type="table" target="#tab_8">Table 4</ref>. As the code of HCINN is not available, its accuracies on the ionosphere and segmentation data sets are adopted directly from <ref type="bibr" target="#b0">[4]</ref> while results on other data sets are not available.</p><p>We can see that frequent k-n-match constantly obtains higher accuracy than the other two techniques. It improves the accuracy up to 9.2% over IGrid. Therefore, we argue that the frequent k-n-match query is a more accurate model for similarity search.  <ref type="formula">(30)</ref> 87.1% N.A. 92.5% Glass <ref type="formula">(9)</ref> 58.6% N.A. 67.8% Iris <ref type="formula">(4)</ref> 88.9% N.A. 89.6%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Efficiency</head><p>In Section 3, we have proved that the AD algorithm is optimal in terms of number of attributes retrieved. However, is it also efficient in a disk based model? To answer this question, we would like to conduct the following experimental studies. First, we will study how to choose parameters, particularly, the range of frequent k-n-match, <ref type="bibr">[n0,n1]</ref>, to optimize its performance (we will focus on frequent k-n-match instead of k-n-match, since frequent k-n-match is the technique we finally use to perform similarity search). Second, we will study, using well chosen parameters, which searching scheme is the best for frequent k-n-match search. Third, we would like to study how the efficiency of frequent k-nmatch search is compared to other similarity search techniques, such as IGrid, since IGrid is more effective than kNN and can be processed very efficiently as reported in <ref type="bibr" target="#b2">[6]</ref>.  <ref type="figure" target="#fig_6">Figure 8</ref> illustrates the effects of the range of the frequent k-n-match, [n 0 , n 1 ], on the accuracy of the results of the three high dimensional machine learning data sets: ionosphere (iono), image segmentation (seg), and wdbc, still using the class stripping technique described in Section 5.1.2. <ref type="figure" target="#fig_6">Figure 8 (a)</ref> plots the accuracy as a function of n0 while fixing n 1 as d. We can see that, as n 0 increases, the accuracy first increases, and then decreases. This is because when n is too small, there are not enough attributes to capture any feature of the object but some random matches. Using such small n values decreases the accuracy. When there are enough number of dimensions, the results begin to make sense and accuracy increases. However, when n 0 is too large, the range of <ref type="bibr">[n0, n1]</ref> becomes too small to identify frequently appearing objects, and therefore the accuracy decreases again. As the accuracy on the ionosphere data set starts to decrease from n 0 = 4, we have chosen n 0 conservatively as 4 in the following experiments. <ref type="figure" target="#fig_6">Figure 8 (b)</ref> shows the accuracy of frequent k-n-match as a function of n 1 while fixing the value of n 0 as 4. The accuracy decreases as n 1 decreases. This is expected since the larger the range, the more stable the frequent appearing objects that we find. We observe that the accuracy decreases very slowly when n0 is large. As n0 becomes smaller, the accuracy decreases more and more rapidly. The reason is that, when n is large, more dimensions of high dissimilarities are taken into account. These dimensions do not help in finding similarities between objects. In another experiment, we would like to see the relationship between the number of attributes retrieved and n 1 , which is revealed in <ref type="figure">Figure 9</ref>. This figure shows that there is a tradeoff between the accuracy and performance of the AD algorithm in terms of number of attributes retrieved. <ref type="figure">Figure 9</ref> (a) plots the number of attributes retrieved (in terms of percentage of the cardinality of the data set) by the AD algorithm as a function of n 1 . The number of attributes retrieved increases as n 1 increases since the larger the n1, the larger the k-n-match difference and hence the more attributes smaller than this k-n-match difference. An interesting phenomenon is that, in contrary to the trend of the accuracy, the increase of the number of attributes retrieved is slower when n1 is small than when n1 is large. This means that, by decreasing n1 slightly from d, we can achieve a large performance gain without sacrificing much accuracy. And we have plotted this tradeoff between accuracy and performance more clearly in <ref type="figure">Figure 9</ref> (b). This figure shows the accuracy of the AD algorithm versus the percentage of attributes retrieved on the ionosphere data set. We can see that the accuracy increases most rapidly when about 10% of the attributes are retrieved. After this, the accuracy increases much slower. We also draw the accuracy of the IGrid technique on the same data set. When the AD algorithm achieves the same accuracy as IGrid, less than 15% of the attributes are retrieved. Results on other data sets have the similar trend and all retrieve about 15% attributes when getting the same accuracy as IGrid. Therefore, we choose the n1 value according to the accuracy of IGrid when comparing efficiency with IGrid. By this means, n 1 is about 8 for the high dimensional real data sets, varying 1 or 2 depending on the dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Choosing Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation of Disk Based Algorithms for Frequent K-N -Match</head><p>As the data sets used for the above studies are too small for run time testing on disk based solutions (the queries finish too fast to make any difference in time for different techniques), we use data sets with more points for efficiency evaluation. We generated uniformly distributed data sets of various dimensionalities and also a real data set, the Cooccurrence Texture from the UCI KDD archive <ref type="bibr">[3]</ref>. All uniform data sets contain 100,000 points. The Texture data set contains 68040 16-dimensional points. Data page size is 4096 bytes. Because frequent k-n-match search is the final technique we use to performance similarity search, we focus on frequent k-n-match search instead of k-n-match search. The range <ref type="bibr">[n0, n1]</ref> of frequent k-n-match search is chosen according to the results on real data sets as described in Section 5.2.1.</p><p>First, we evaluate the VA-file based algorithm as described in Section 4.2. In our implementation of the VA-file, we use 8 bits to code the data, which make the size of VA-file 25% of the size of the original data set. <ref type="figure" target="#fig_8">Figure 10</ref> shows As the total number of points is 100,000 and 68,040 for the uniform and the Texture data sets respectively, there are about 10% of points retrieved. For these about 10% points, the algorithm needs to do random page accesses to retrieve them, therefore the final response time turns out to be about twice that of the scan algorithm, as shown in 10 (b). Results of data sets of other dimensionalities have similar behavior. Therefore, VA-file based algorithm does not work for the frequent k-n-match query. Next, we evaluate our proposed AD algorithm. The number of page accesses and response time on a 16-dimensional uniform and the Texture data sets are shown in <ref type="figure">Figure 11</ref> (a) and (b), respectively. The number of page accesses of AD is 10∼20% of the sequential scan and the result of response time is similar. Because the AD algorithm retrieves only the necessary attributes for evaluating the frequent k-n-match query and search forwards in a dimension take advantage of sequential accesses, it beats sequential scan on the total response time. This shows the efficiency of the AD algorithm. We also plotted the number of page accesses and response time as functions of n 1 in <ref type="figure" target="#fig_0">Figure 12</ref> (a) and (b), respectively. While the AD algorithm can achieve the same accuracy as IGrid when n1 as low as 8, the AD algorithm beats the sequential scan even when n1 is much larger (up to 14). This means that our technique can achieve high accuracy in similarity search while still being very efficient.</p><p>From the above comparison with VA-file based algorithm and sequential scan, we can draw the conclusion that the AD algorithm is still the best choice among the competitors in the disk based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparison with Other Similarity Search Techniques</head><p>In this section, we compare the efficiency of the frequent k-n-match query using the AD algorithm (that is, FKNMatchAD) with other similarity search techniques. Both IGrid <ref type="bibr" target="#b2">[6]</ref> and the Human-Computer Interactive NN search (HCINN for short) <ref type="bibr" target="#b0">[4]</ref> have been reported to have better accuracy than the kNN query. We have shown that frequent k-n-match search has better accuracy than them in Section 5.1.2. Therefore, we would like to further see how is the efficiency of our method compared with these two techniques.</p><p>The HCINN search algorithm needs to access all the data in the data set and moreover, it requires human interaction, therefore it is less efficient than FKNMatchAD. In the following, we will only compare FKNMatchAD with IGrid. IGrid <ref type="bibr" target="#b2">[6]</ref> was proposed as an inverted file on the grid partition of the database. The analysis in <ref type="bibr" target="#b2">[6]</ref> shows that the accessed data size is 2/d of the original data, therefore the data accessed decreases as the dimensionality increases. However, in their analysis, they only considered the sum of the size of the data accessed, but not how the data are distributed on the disk. In fact, the accessed data are fragmented and distributed all over the data set. Random accesses of all the fragments are much more expensive than when they are clustered together and accessed sequentially. So a mere comparison in the size of the accessed data is not enough to show its efficiency. In view of this, we have compared the response time of FKNMatchAD and IGrid, using both synthetic and real data sets. The response time of the two techniques, FKNMatchAD and IGrid, on a 16-dimensional uniform data set with varying k and data set sizes are shown in <ref type="figure" target="#fig_1">Figure 13</ref>. We also plotted the response time of the sequential scan algorithm for frequent k-n-match search as a reference for FKNMatchAD. We see that the FKNMatchAD is more efficient than IGrid. And FKNMatchAD is scalable with regard to k and data set size. We also compared them for on data sets of varying  <ref type="figure" target="#fig_2">Figure 14</ref>.</p><p>Finally, we compare them on the real data set (the Texture data set). The result of response time is shown in <ref type="figure" target="#fig_3">Figure  15</ref> (a). We can see that FKNMatchAD beats the other two techniques even when n 1 equals the dimensionality 16. By examining the number of attributes retrieved as shown in <ref type="figure" target="#fig_3">Figure 15 (b)</ref>, we can see that when n1 = 16, there is only 25% of the attributes retrieved due to the high skew of the real data. This is the reason for the especially good performance exhibited here. From the above results, we draw the conclusion that the frequent k-n-match query can be processed more efficiently (by our proposed FKNMatchAD algorithm) than the existing techniques while achieving better accuracy than them in similarity search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>A popular method for similarity search is to first extract from objects some features such as image colors <ref type="bibr" target="#b10">[14]</ref>, shapes <ref type="bibr" target="#b13">[17]</ref> and texts <ref type="bibr" target="#b15">[19]</ref>, and then use nearest neighbor queries to search similar objects <ref type="bibr" target="#b6">[10,</ref><ref type="bibr" target="#b10">14]</ref>. In the last decade, many structures and algorithms have been proposed aiming at accelerating the processing of (k) nearest neighbor queries. Early methods are based on R-tree-like structures such as the SS-tree <ref type="bibr" target="#b18">[22]</ref> and the X-tree <ref type="bibr" target="#b3">[7]</ref>. However, the R-tree-like structures all suffer from the "dimensionality curse", that is, their performance deteriorates dramatically as dimensionality becomes high. <ref type="bibr" target="#b17">[21]</ref> has shown this phenomenon both analytically and experimentally. Therefore, the authors of <ref type="bibr" target="#b17">[21]</ref> proposed an algorithm based on compression, called the vector approximation-file (VA-file) to accelerate sequential scan.</p><p>While the papers above mainly emphasize on the efficiency of kNN search, other works look at kNN from the aspect of effectiveness. In <ref type="bibr" target="#b4">[8]</ref>, <ref type="bibr">Beyer et. al.</ref> show that at very high dimensionality, the distance between two nearest points and two furthest points in a data set are almost the same. At the same time however, they also show that points that are generated from distinct clusters do not obey such rules. Various studies <ref type="bibr" target="#b12">[16,</ref><ref type="bibr" target="#b1">5,</ref><ref type="bibr" target="#b2">6,</ref><ref type="bibr" target="#b0">4]</ref> have been performed subsequently to address the issue raised in <ref type="bibr" target="#b4">[8]</ref>. Among these, only <ref type="bibr" target="#b2">[6]</ref> addresses the efficiency issue. In <ref type="bibr" target="#b2">[6]</ref>, the IGrid index was proposed, in which each dimension is discretized based on equi-depth partitioning in a pre-processing phase. When comparing two points, the actual difference between matching dimensions are aggregated to judge their similarity. This is different from our work which performs the discretization dynamically while counting only the matches. The most effective among these works is reported in <ref type="bibr" target="#b0">[4]</ref> where human computer interaction is needed to find meaningful neighbors.</p><p>In <ref type="bibr" target="#b14">[18]</ref>, dynamic partial function (DPF) was proposed to compute similarity based on the closest n dimensions. Our work employs the similar strategy in defining the n-match problem. However, in view of the hardness to define a good n value in reality, we propose the frequent k-n-match problem which captures the full similarity of objects and the result is not sensitive to the choice of n. <ref type="bibr" target="#b14">[18]</ref> used an n value observed from experiments over the data set, which is an ad hoc method. Moreover, the algorithm proposed in <ref type="bibr" target="#b14">[18]</ref> has no correctness guarantee and their accuracy is measured by recall of the actual kNN, that is, how many actual kNNs are included in their answers. In other words, the algorithm finds approximations to the exact kNN answers, but without any approximation guarantee. This is different from our effectiveness evaluation, which measures the extent of similarity of answers to the query, while the answers are exact correct nearest neighbors under our similarity model.</p><p>We have discussed our problem in the multiple system information retrieval model, which was described in <ref type="bibr" target="#b7">[11]</ref>. As discussed in Section 3, the algorithm proposed in <ref type="bibr" target="#b7">[11]</ref> and variations in <ref type="bibr" target="#b9">[13]</ref> were for other types of queries and they assume a monotone aggregation function, which is not satisfied by the aggregation function of our problem. More recently, <ref type="bibr" target="#b8">[12]</ref> has used rank aggregation to answer kNN approximately and the quality measure is the approximation factor. Again, the difference is that we are answering the query defined by our similarity model exactly.</p><p>The skyline query <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b16">20]</ref> has also been proposed to find close objects based on various feature sets, but the answer of the skyline query is a set of objects that do not dominate each other. In our model, we still find top k answers according to one score, the n-match difference. In this sense, it is closer to the traditional kNN query that the answer with higher score dominates the ones with lower scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this paper, we proposed a new approach to model the similarity search problem, namely the k-n-match problem. The k-n-match problem models the similarity search as matching between the query object and the data objects in n dimensions, where these n dimensions are determined dynamically to make the query object and the data objects in the answer set match best. While the k-n-match query is expected to be superior than the kNN query in discovering partial similarities, it depends on a good choice of the n value. To address the problem, we further introduced the frequent k-n-match query, which returns the objects that appear most frequently in the answer sets of k-n-match queries with a range of n values. Moreover, we proposed algorithms (called the AD algorithm) for both problems. We proved that the AD algorithm is optimal in the multiple system information retrieval model. We also applied the strategy to obtain a disk based algorithm for the (frequent) k-n-match query. By a thorough experimental study using both real and synthetic data sets, we validated that the k-n-match query finds better result than the kNN query through partial similarity if a good value of n is chosen; we showed that the frequent k-n-match query is more effective in similarity search than existing techniques such as IGrid and HumanComputer Interactive NN search, which have been reported to be more effective than traditional kNN queries based on Euclidean distance. We also showed that the frequent kn-match query can be processed more efficiently than the other techniques by our proposed AD algorithm in the disk based cost model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The n-match problem difference from Q in dimension x. B is the 2-match of Q because when we consider 2 dimensions, B has the smallest difference. Intuitively, the query finds a point P that matches Q in n dimensions. If we consider the maximum difference in these n dimensions, P has the smallest maximum difference to Q among all the points in DB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An Example Database function f is monotone means that f (p 1 , ..., p d ) ≤ f (p 1 , ..., p d ) whenever p i ≤ p i for every i = 1, ..., d (or p i ≥ p i for every</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Algorithm KNMatchAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A Running Example 0.2) is popped out, so appear[2] is increased by 1 and equals 1 now. We read the next pair in dimension 1 towards the smaller attribute direction, that is, (1, 0.4), and form the triple (1, 0, 2.6), which is put back into g[0]. Next, we pop the triple with the smallest difference from the current g[ ]. We get (5, 1, 0.5), so appear[5] becomes 1 and (3, 1, 3.5) is put into g[1]. Next, we get (3, 3, 0.8) from g[ ], so appear[3] becomes 1 and (4, 3, 2.0) is put into g[3]. Now g[ ]={(1, 0, 2.6), (3, 1, 3.5), (2, 2, 1.5), (4, 3, 2.0), (2, 4, 2.0), (3, 5, 1.0)}. Next, we get (3, 5, 1.0), so appear[3] becomes 2, which equals n, and so h becomes 1. At this time we have found the first 2-match point, that is, point 3. (5, 5, 4.0) is put into g[5]. Next we get (2, 2, 1.5) from g[ ], so appear[2] becomes 2, which also equals n, and so h becomes 2, which equals k. At this time we have found the second 2-match, therefore the algorithm stops. The 2-2-match set is {point 2, point 3} and we also get the 2-2-match difference, 1.5. In the implementation, we do not have to actually store pd in the triple since we can tell which dimension a (pid, dif ) pair is from when we get it from the sorted dimensions or from g[ ]. In what follows, we will prove the correctness and optimality of the AD algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Algorithm FKNMatchAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Images in the COIL-100 database</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Effects of n 0 and n 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Attributes retrieved vs. n1 (b) Accuracy vs. Attributes retrieved Figure 9: Tradeoff between accuracy and perfor- mance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Performance of VA-file based algorithm the results on a 16-dimensional uniform and the Texture data sets. Figure 10 (a) shows the number of points that are actually retrieved from the database in the refinement phase of the VA-file based algorithm for frequent k-n-match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Performance of the AD algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Response</head><label></label><figDesc>time (sec) Data set size (thousand) scan AD IGrid (a) Response time vs. k (b) Response time vs. data set size Figure 13: Comparison with IGrid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Effect of dimensionality dimensionalities from 8 to 48. FKNMatchAD always outperforms the other two techniques as shown in Figure 14. Finally, we compare them on the real data set (the Texture data set). The result of response time is shown in Figure 15 (a). We can see that FKNMatchAD beats the other two techniques even when n 1 equals the dimensionality 16. By examining the number of attributes retrieved as shown in Figure 15 (b), we can see that when n1 = 16, there is only 25% of the attributes retrieved due to the high skew of the real data. This is the reason for the especially good performance exhibited here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(</head><label></label><figDesc>a) Response time vs. n 1 (b) Attributes retrieved vs. n 1 Figure 15: Comparison with IGrid on real data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Notation Notation Meaning c Cardinality of the database DB The database, which is a set of points d Dimensionality of the data space k</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Then starting from the posi- tion of q i , we access the attributes one by one towards both directions along dimension i. Here, we use an array g[ ] (line 4) of size 2d to maintain the next attribute to access in each dimension, in both directions (attributes smaller than Algorithm KNMatchAD 1 Initialize appear[ ], h and S. 2 for every dimension i 3 Locate qi in dimension i. 4= n 9 h++; 10 S=S ∪ pid; 11 Read next attribute from dimension pd and form a new triple (pid, pd, dif ). If end of the dimension is reached, let dif be ∞. Put the triple to g[pd]. while h &lt; k 12 return S. End KNMatchAD</head><label></label><figDesc></figDesc><table>Calculate the differences between qi and its 
closest attributes in dimension i along both 
directions. Form a triple (pid, pd, dif ) for each 
direction. Put this triple to g[pd]. 
5 do 
6 
(pid, pd, dif ) = smallest(g); 
7 
appear[pid]++; 
8 
if appear[pid] </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>appear[ ], h[ ] and S[ ] have the same meanings as in algorithm KNMatchAD except that h and S are arrays, each has d elements. After initialization, we locate the query's attributes in each dimen- Algorithm FKNMatchAD 1 Initialize appear[ ], h[ ] and S[ ]. 2 for every dimension i 3 Locate qi in dimension i. 4 Calculate the differences between qi and11 Read next attribute from dimension pd and form a new triple (pid, pd, dif ). If end of the dimension is reached, let dif be ∞. Put the triple to g[pd]. while h[n1] &lt; k 12 scan Sn 0 , ..., Sn 1 to obtain the k point ID</head><label></label><figDesc>1 initializes some structures used in the algorithm.</figDesc><table>its 
closest attributes in dimension i along both 
directions. Form a triple (pid, pd, dif ) for each 
direction. Put this triple to g[pd]. 
5 do 
6 
(pid, pd, dif ) = smallest(g); 
7 
appear[pid]++; 
8 
if n 0 ≤ appear[pid] ≤ n 1 
9 
h[appear[pid]]++; 
10 
S[appear[pid]]=S[appear[pid]] ∪ pid; 
's that 
appear most times 
End FKNMatchAD 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 : k-n-match results, k = 4, Query Image 42 n images returned n images</head><label>2</label><figDesc></figDesc><table>returned 
5 
36, 42, 78, 94 
30 
10, 35, 42, 94 
10 
27, 35, 42, 78 
35 
35, 42, 94, 96 
15 
3, 38, 42, 78 
40 
35, 42, 94, 96 
20 
27, 38, 42, 78 
45 
35, 42, 94, 96 
25 
35, 40, 42, 94 
50 
35, 42, 94, 96 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 : kNN results, k = 10,</head><label>3</label><figDesc></figDesc><table>Query Image 42 
k 
images returned 
10 13, 35, 36, 40, 42 
64, 85, 88, 94, 96 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy on Real data sets 
data sets (d) 
IGrid HCINN Freq. k-n-match 
Ionosphere (34) 
80.1% 
86% 
87.5% 
Segmentation (19) 79.9% 
83% 
87.3% 
Wdbc </table></figure>

			<note place="foot" n="1"> A side effect of our work will be that we can have a uniform treatment for both type of attributes in the future.</note>

			<note place="foot" n="2"> We only use 1 byte for each element of appear[ ], which can work for up to 256 dimensions. For a data set of 1 million records, the memory usage is 1 Megabytes. This should be acceptable given the memory size of today&apos;s computer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards meaningful high-dimensional nearest neighbor search by human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the surprising behavior of distance metrics in high dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The igrid index: Reversing the dimensionality curse for similarity indexing in high dimensional space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The x-tree: An index structure for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berchtold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">When is nearest neighbors meaningful? In ICDT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The skyline operator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Börzsönyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Content-based image indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining fuzzy information from multiple systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient similarity search and classification via rank aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal aggregation algorithms for middleware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lotem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient and effective querying by image content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Equitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Petkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="262" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Error detecting and error correcting codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Hamming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Systems Technical Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What is the nearest neighbor in high dimensional spaces? In VLDB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A retrieval technique for similar shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dyndex: a dynamic and non-metric space indexer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Techniques for automatically correcting words in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing survey</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="439" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progressive skyline computation in database systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="82" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Schek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Similarity indexing with the ss-tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
