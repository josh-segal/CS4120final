<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy of Search Logs: How Hard is Search? With Personalization? With Backoff?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana Champaign Urbana</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<postCode>61801, 98052</postCode>
									<region>IL, WA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
							<email>church@microsoft.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana Champaign Urbana</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<postCode>61801, 98052</postCode>
									<region>IL, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy of Search Logs: How Hard is Search? With Personalization? With Backoff?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Informa- tion Search and Retrieval]: Text Mining General Terms: Measurements Keywords: entropy</term>
					<term>search log</term>
					<term>search difficulty</term>
					<term>personal- ization with backoff</term>
					<term>demographics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>How many pages are there on the Web? 5B? 20B? More? Less? Big bets on clusters in the clouds could be wiped out if a small cache of a few million urls could capture much of the value. Language modeling techniques are applied to MSN&apos;s search logs to estimate entropy. The perplexity is surprisingly small: millions, not billions. Entropy is a powerful tool for sizing challenges and opportunities. How hard is search? How hard are query suggestion mechanisms like auto-complete? How much does personalization help? All these difficult questions can be answered by estimation of entropy from search logs. What is the potential opportunity for personalization? In this paper, we propose a new way to personalize search, personalization with backoff. If we have relevant data for a particular user, we should use it. But if we don&apos;t, back off to larger and larger classes of similar users. As a proof of concept, we use the first few bytes of the IP address to define classes. The coefficients of each backoff class are estimated with an EM algorithm. Ideally, classes would be defined by market segments, demographics and surrogate variables such as time and geography.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>How many pages are there on the Web? 5B? 20B? More? Less? How hard is search? How much does personalization help? All are difficult but crucial questions to search business.</p><p>Scale is hard. The bigger the web, the harder the search. Search engines make large investments in expensive computer centers in the cloud to index billions of pages. Could these large investments be wiped out if a small cache of a few million pages could capture much of the value? What if someone found a way to squeeze much of the value of the cluster into a desktop or a mobile device? Is search more like an Everest expedition (clusters in the clouds) or a walk in the park (a little flash memory on a mobile device)?</p><p>Related questions come up in language. How big is English? One can find simple answers on the covers of many dictionaries, but we would feel more comfortable with answers from a more authoritative source than a marketing department. Many academics have contributed to this discussion from many perspectives: Education, Psychology, Statistics, Linguistics, and Engineering. Chomsky and Shannon proposed two different ways to think about such questions:</p><p>• Chomsky: language is infinite <ref type="bibr" target="#b3">[4]</ref> • Shannon: 1.3 bits per character <ref type="bibr" target="#b23">[24]</ref> These two answers are very different. Chomsky's answer is about the total number of words; and Shannon's answer is about the perplexity, or the difficulty of using a language. A dictionary could cover a lot of words, but not all of them are actively used. Using a Chomskian argument, we could argue that there are infinitely many urls. For example, one could write a spider trap such as successor.aspx?x=0 which links to successor.aspx?x=1 which links to successor.aspx?x=2. In addition to intentionally malicious spider traps, there are perfectly benign examples such as calendars 1 , where there are infinitely many pages, one for each month, with links from each month to the next. It is all too easy to build a web crawler that finds itself attempting to materialize an infinite set with finite resources. The crawler can easily consume all available time and space.</p><p>Shannon offers a more practical answer. Although there are a lot of pages out there, there are not that many pages that people actually go to. This paper will estimate entropy of urls (and queries and IP addresses) based on logs from Microsoft's www.live.com. We find that it takes just 22 bits to guess the next url (or the next query or the next IP address). That is a walk in the park (millions, not billions). With all the talk about the long tail, one would think that the web was astronomical. But the logs are tiny, far less than Carl Sagan's billions and billions <ref type="bibr" target="#b21">[22]</ref>.</p><p>As we will see, entropy is a powerful tool for sizing challenges and opportunities. How hard is search? How much does personalization help?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Personalization</head><p>Personalization is a hot topic, with a large body of work, not only in the scientific literature, but also in commercial practice. Many people use personalized search products every day. A query for "personalized search" returns millions of page hits. The first few pages of results are dominated by the commercial practice. If you want to find the scientific literature such as <ref type="bibr" target="#b28">[29]</ref>, you'll have to refine the query considerably by adding a keyword like "SIGIR."</p><p>Why does personalization help? It is useful to know your audience. Consider the ambiguous query: "MSG". Depending on the user, this query could be looking for the sports arena (Madison Square Garden) or the food additive (Monosodium Glutamate). The search engine could do a better job answering ambiguous queries like this if it had access to demographic data and/or log data such as click logs.</p><p>Many acronyms are ambiguous. ACS can refer to the "American Chemical Society," the "American Cancer Society," the "American College of Surgeons" and more. Acronyms take on special meanings inside many large organizations and private enterprises. For example, for most people, MSR means "Mountain Safety Research," but inside Microsoft, it means "Microsoft Research." And of course, it means other things to other people including: "Montessori School of Raleigh," "Mom Service Representative" and "My Sports Radio." PSS is a stock ticker for "Payless Shoes," as well an abbreviation of several different companies: "Physicians Sales and Service," "Phoenix Simulation Software," "Personal Search Syndication," "Professional Sound System," etc. But inside Microsoft, PSS refers to "Product Support Services." It helps to know your audience in order to know:</p><p>• what the terminology means</p><p>• which questions are likely to come up, and</p><p>• which answers are likely to be appreciated.</p><p>If we have the relevant data (such as click logs) for a particular user, we should use it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Personalization With Backoff</head><p>But what if we do not have data for a particular user (or we cannot use it because of privacy concerns)? This paper takes a backoff approach to personalization <ref type="bibr" target="#b16">[17]</ref>. If we do not have data for a particular user, back off to larger and larger groups of similar users. As a proof of concept, users are grouped into equivalence classes based on the most significant bytes of their IP address. Personalization is then conducted by combining estimates based on all four bytes of the IP address, the first three bytes, the first two, and so on. It would be even better to group customers by market segments and/or collaborative filtering (users who ask similar questions and click on similar urls). We leave these suggestions for future work.</p><p>Segmentation is a traditional goal in marketing. Customers are assigned to equivalence classes based on profile features such as age, income, occupation, etc. It is useful for an advertiser to know who it is talking to so that it can target the message appropriately to the audience. An advertiser such as Ford, for example, has a wide range of products. Some products are more attractive to some customers, and other products are more attractive to other customers. For example, the firm may wish to target small trucks to a rural audience and hybrids to a green audience. Companies would like to know if they are talking to college students, teenagers, parents with young children, etc. It is useful to know the class of your audience.</p><p>We find that a little bit of personalization is better than too much or too little. Specifically, personalization with backoff to higher bytes of IP addresses (especially the second and third bytes) is better than 100% personalization or no personalization. Too little personalization misses the opportunity and too much runs into sparse data <ref type="bibr">(and pri- vacy)</ref>. It isn't feasible to know everything about everyone (and they might not like it, if we knew too much).</p><p>Instead of assigning each customer to his own class (i.e., 100% personalization), it is common to assign customers to market segments. Market segments are typically defined in terms of surrogate variables such as geography (e.g., zip code), and time of day and day of week. These surrogate variables are easy to work with, and hopefully, they are well correlated with the more sensitive demographic variables such as those mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ENTROPY ESTIMATION</head><p>How big is the web? How hard is search? How much does personalization help?</p><p>To answer these questions and more, we collected a sample of logs from the Live search engine of about 1.5 year up to July 2007. This 1.5 year data, denoted as the "bigger" dataset, contains 193 million unique IP addresses, 637 million unique queries, and 585 million unique urls. The sample contains about 10 million Q, U RL, IP triples per day. Each triple corresponds to a click from a particular IP address on a particular url for a particular query.</p><p>We separated the logs between 1/1/2006 and 2/6/2006 specifically for personalization experiments. The January data (the "smaller" data) was used for training the model of personalization and the February data was used for validation and testing. This one month training set contains 26 million unique IP addresses, 36 million unique queries, and 63 million unique urls. Entropy was estimated based on both the smaller data and the bigger data.</p><p>We assume that these sets are reasonably representative of the tasks of interest, though of course, such assumptions can be highly problematic. It is possible, for example, that users could share the same IP address, and a user would click on different urls under different conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>• U: a user</p><p>• IP: an IP address. IP will be used as a convenient surrogate for U, though of course, it is possible for multiple users to share the same IP address.</p><p>• C: a class of users. Users are grouped into equivalence classes based on variables such as IP prefixes, time and location. These variables are treated as convenient surrogates for variables of interest such as demographics, market segments, etc.</p><p>• URL: Uniform Resource Locator, the name of a web document.</p><p>• Q, U RL, IP : a triple from the search logs, indicating that there was a click on a particular URL in response to a particular query Q from a particular IP address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entropy (H)</head><p>Entropy 2 is commonly used in information theory to characterize the size of the search space. The larger the entropy of a distribution is, the harder it is to predict the next event <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, Shannon used entropy to measure the difficulty of predicting the next character of English. Shannon's entropy provides bounds, but does not say how to achieve these bounds.</p><p>Similarly, we introduce entropy to measure the difficulty faced by a search engine. Note that entropy measures the size of the search space of the web, but not the number of particular urls. This is practical since what a search business cares about is the difficulty of search. How many bits does it take to guess the next url that will be clicked on?</p><formula xml:id="formula_0">H(U RL) = − U RL p(U RL) log p(U RL)</formula><p>Conditional entropy measures the remaining entropy of the target random variable given the value of another related random variable. We can thus use conditional entropy to measure the difficulty of web search, when we know the query, the user, etc. The search task, of course, is much easier, because we are given the query, which is a huge hint.</p><formula xml:id="formula_1">H(U RL|Q) = H(U RL, Q) − H(Q)</formula><p>How much does personalization help? That is, suppose we give the search engine not only the query, but also the IP address. How much does the IP address help?</p><formula xml:id="formula_2">H(U RL|Q, IP ) = H(U RL, Q, IP ) − H(Q, IP )</formula><p>These quantities and more can be estimated from the training data, a sequence of triples: Q, U RL, IP . We will present estimates of the entropy of urls, queries and IP addresses, taken one at a time. In addition, we will present estimates of the joint entropies of all pairs of these quantities, as well as the three-way joint. From these quantities, we can easily derive estimates of conditional entropies of any combination of these variables (X) given any other combination of these variables (Y ) using the rule 3</p><formula xml:id="formula_3">H(Y |X) = H(X, Y ) − H(X)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross Entropy</head><p>It is common practice to split the data into two pieces, one for training and the other for validation. Entropy estimation is fundamentally a prediction task. The task is to use historical logs to estimate search experiences in the future. Splitting up the data into separate training and validation sets tend to produce larger (and more credible) estimates of entropy. Cross entropy 4 can be applied to measure the average number of bits needed to guess the next url in new logs (validation), given the distribution estimated from the historical logs (training).</p><p>For example, the cross entropy of url given the query and IP address is Hc(U RL|Q, IP )</p><formula xml:id="formula_4">= − U RL,IP,Q pv(U RL, IP, Q) log pt(U RL|IP, Q)</formula><p>where pt(U RL|IP, Q) is estimated from the training set (historical log data) and pv(U RL, IP, Q) is estimated from the 2 http://en.wikipedia.org/wiki/Information entropy 3 http://en.wikipedia.org/wiki/Conditional entropy 4 http://en.wikipedia.org/wiki/Cross entropy validation set (new log data). Please note that minimizing this cross entropy is equivalent to maximizing the likelihood of the new log data, given the estimates from the history. Based on these evaluation measures, we present a series of experimental results which answers the questions in Section 1. How big is the web? How hard is search? How much does personalization help?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HOW LARGE IS THE WEB?</head><p>Entropy estimates for Q (query), URL and IP addresses are shown in <ref type="table">Table 1</ref>  <ref type="figure">Figure 1</ref>: Entropy of logs grows much more slowly than its upperbound <ref type="figure">Figure 1</ref> shows a clearer trend. With the bigger dataset, the maximum entropies (upperbounds) increase significantly ( &gt; 3 bits, 1 bit corresponds to a twice larger search space). The actual entropies, however, stay around 22 to 23.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Population Bound</head><p>How large could the web become? Chris Anderson points out in "The Long Tail" (www.thelongtail.com) that online distribution channels are making it possible for NetFlix, Amazon and others to sell less of more <ref type="bibr" target="#b0">[1]</ref>. But there are limits to this process. NetFlix offers just 70,000 products 5 and Amazon has just 8 million. That's millions, not billions.</p><p>How about vanity searches? Even if everyone looks for their home page, how many pages is that? Right now, there are home pages for famous people and academics, but not ordinary people like our neighbors. There aren't that many famous people and academics: perhaps millions, but certainly not billions.</p><p>The telephone business is a mature business that has saturated the market with universal service. Most people (and most businesses) are listed in the white pages and/or yellow pages (unless they opt out). Our neighbors are likely to be in the phonebook, but they don't have a home page on the web.</p><p>Phonebooks are limited by the population. According to the FCC, 6 there are about 173 million telephone lines in the US, or less than one per person.</p><p>Eventually, when billions of people have universal web service and everyone has their own home page, the web will be bounded by the population, billions of pages worldwide. But, for the foreseeable future (a decade), the web will be a growth market, far from saturation. Millions (not billions) will be good enough until the market saturates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Equilibrium: Supply = Demand</head><p>In addition to supply side accounting, we can also use a demand side argument to justify the population bound.</p><p>Users have only so much time to surf the web. Suppose that each user is willing to spend a few hours per day on the web, assuming that they value the web about as much as telephone (1 hour of usage per day per telephone number) <ref type="bibr" target="#b6">7</ref> and television (8 hours of usage per day per household) 8 . Assume further that users can only visit so many pages, given these time constraints. Thus, when the web eventually saturates the market, the total number of page hits will be constrained by the size of the population: O(population).</p><p>Let's assume there is an equilibrium constraint between suppliers and consumers. Consumers have limited time. They can visit only so many pages within that time limit. Suppliers will compete for these hits. Excluding illegitimate suppliers (spam), reasonable suppliers depend on these hits for their livelihood. Reasonable suppliers will post as many pages as consumers can consume (and no more). Thus, if there is a population bound on hits, then there will also be a population bound on the supply of reasonable pages (that people will look for and click on and value).    <ref type="table" target="#tab_2">Table 2</ref>. The search task is to guess the URL that the user is looking for from a Query Q. Based on the one month data, that is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">How Hard is Search?</head><formula xml:id="formula_5">H(U RL|Q) = H(Q, U RL) − H(Q) = 23.9 − 21.1 = 2.8</formula><p>This number becomes 3.5 with the bigger data. In other words, search is doable. A user can often find the url he is looking for somewhere in the top 10 search results. That is reassuring, though not surprising. We would expect an upper bound around log210 ≈ 3.3 bits, given the source of the data (click logs). Users tend to click somewhere on the first page of results, or not at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">How much does Personalization Help?</head><p>Suppose we give the search engine not only the query, but also the IP address. How much does that help? Using the one month estimates in <ref type="table" target="#tab_2">Table 2</ref> above,</p><formula xml:id="formula_6">H(U RL|Q, IP ) = H(Q, U RL, IP ) − H(Q, IP ) = 27.2 − 26.0 = 1.2</formula><p>In other words, personalization cuts the search space in half.</p><p>That is a huge opportunity. This entropy becomes 1.3 with the bigger data. Please note that although the joint entropy of the three variables increases a lot, this conditional entropy remains very small. Why does personalization help? Consider the ambiguous query: MSG. Some users, especially those near New York City, are looking for the sports arena (Madison Square Garden), whereas other users are looking for the food additive (Mono-sodium Glutamate). The search engine should use the user's history of queries and clicks (when possible) to disambiguate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">How Hard are Query Suggestions?</head><p>There are a number of applications that search the space of queries as opposed to the space of answers. For example, a number of query suggestion mechanisms have been proposed suggest as Google Suggests 9 and The Wild Thing <ref type="bibr" target="#b6">[7]</ref>. How hard is it to guess the next question, as opposed to guessing the next answer? H(Q) = 21.1 bits (22.9 from 1.5 year).</p><p>How much does personalization help?</p><formula xml:id="formula_7">H(Q|IP ) = H(Q, IP ) − H(IP ) = 26 − 22 = 4</formula><p>This number becomes 7.8 with the bigger data. In other words, personalization cuts the search space in more than a half. This is a really huge opportunity. The entropy estimates in this section assume that the search log data is seen, and thus correspond to the lower bounds of search difficulty. In reality, when a search engine tries to predict unseen data, the actual entropies (cross entropies) would be higher. Smoothing methods have to be applied on the personalization language models. We will introduce one possible choice in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PERSONALIZATION WITH BACKOFF</head><p>The entropy numbers are really exciting. They make a strong case for plausibility, but there are many remaining challenges that need to be addressed including privacy and data sparsity. In fact, Shannon's entropy gives a lower bound of the search difficulty, but does not provide an operational procedure to achieve it. Personalization is very attractive when we have plenty of data, but what if we do not have enough data, or we cannot use much of the data that we have because of privacy concerns? In this section, we introduce one possible operational procedure of approaching this lower bound: personalization with backoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">User Modeling with Backoff</head><p>If we don't have enough data for a particular user, or we can't use the data we have, we recommend backing off to classes of users. As a proof of concept, this paper will form classes of users based on the first few bytes of the IP address. Even better is to back off based on market segments and collaborative filtering (other users who click similarly). Time and geography can be viewed as surrogate variables for demographics in market segmentation analysis.</p><p>The model assumes that users in a class share similar interests. For example, users from the same company are likely to ask similar questions and click on similar answers. Consequently, if we lack adequate historical data for a particular user, we can backoff to a larger class of similar users.</p><p>Formally, assume a query Q, a user U, and a web document URL. Let Γ = {C0, C1, ..., Cn−1} be a set of n classes of users. Under personalization with backoff, the probability p(U RL|Q, U ) is estimated as a simple linear combination of the class models, for each class that the user is a member of. The weights, λ, can be fit with EM <ref type="bibr" target="#b8">[9]</ref>. That is,</p><formula xml:id="formula_8">p(U RL|Q, U ) = C i ∈Γ λU,ip(U RL|Q, Ci)</formula><p>where i λU,i = 1, and λU,i = 0 if U ∈ Ci. Note that the classes need not form a partition. In particular, we will place IP addresses into a nested hierarchy. Each IP address can be a member of multiple nested classes. Certainly, IP hierarchy is not the only possible choice of the user classes, and perhaps not the best choice either.</p><p>This model allows for a wide range of personalization. Two extreme special cases are 0% personalization and 100% personalization. We will refer to 0% personalization as nonpersonalized, and 100% personalization as complete personalization.</p><p>Non-personalization (or 0% personalization) is the special case where n = 1. There is just one super-class of users: Γ = {C0}. All users are members of this single super-class. In this special case, the model becomes p(U RL|Q, U ) = p(U RL|Q, C0), where C0 can be dropped.</p><p>At the other extreme, 100% personalization, n = |U |. Every class contains exactly one user, and every user belongs to exactly one class. In this special case, the model becomes p(U RL|Q, U ) = p(U RL|Q, Cu), where Cu = {U }.</p><p>Between these two extreme cases, there is plenty of middle ground, where users are grouped into more than one class, but less than |U |. Class assignments are typically determined by variables such as IP addresses, time and geography, and combinations thereof. These variables can be treated as surrogates for demographic variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Nested Classes Based on IP Addresses</head><p>Users are assigned to 5 nested classes based on their IP address. It is assumed that the prefix of an IP address is a convenient surrogate for some more meaningful variables such as geography. An IP address consists of four sections, each of which is typically encoded with a byte. This representation suggests the following 5 classes:</p><p>• IP4: Users are assigned to classes based on all 4 bytes of the IP address.</p><p>• IP3: Users are assigned to classes based on the 3 most significant bytes of the IP address.</p><p>• IP2: Users are assigned to classes based on the 2 most significant bytes of the IP address.</p><p>• IP1: Users are assigned to classes based on the most significant byte of the IP address.</p><p>• IP0: All users are assigned to a single super-class.</p><p>With this construction, every user is assigned to exactly 5 classes. IP4 and IP0 are the two extreme cases mentioned above: 100% personalization and 0% personalization, respectively.</p><p>We further simplify the model to use just 5 λ's. That is,</p><formula xml:id="formula_9">p(U RL|Q, IP ) = λ0p(U RL|Q, CIP,0) + λ1p(U RL|Q, CIP,1) + λ2p(U RL|Q, CIP,2) + λ3p(U RL|Q, CIP,3) + λ4p(U RL|Q, CIP,4)</formula><p>where C IP,k is the class of IP addresses that share the most significant k bytes. There are many ways to fit the λ's. We used the standard Expectation-Maximization(EM) algorithm <ref type="bibr" target="#b8">[9]</ref>, an iterative procedure which estimates the parameters of the model from the training set, and also finds the λ's that maximize the validation set V given the model. On each iteration, we perform both an estimation (E) step as well as a maximization (M) step with the following two updating formulae:</p><formula xml:id="formula_10">z (n+1) Q,U RL,IP ,i = λ (n) i p(U RL|Q, CIP,i) 4 k=0 λ (n) k p(U RL|Q, C IP,k ) λ (n+1) i = Q,U RL,IP ∈V z (n+1) Q,U RL,IP ,i C(Q, U RL, IP , V ) Q,U RL,IP ∈V C(Q, U RL, IP , V )</formula><p>where p(U RL|Q, C IP,k ) denotes probability estimates based on the training set, and C(Q, U RL, IP , V ) denotes counts of triples based on the validation set. <ref type="figure">Figure 2</ref> shows the resulting estimates of λ. The training set is a month of logs <ref type="bibr">(January 2006</ref>). The validation set is a single day of logs (February 1st). <ref type="figure">Figure 2</ref> shows that a little bit of personalization is better than too much or too little. Note that λ3 and λ2 are considerably larger than λ4, λ1 and λ0. Too much personalization suffers from sparse data whereas too little personalization misses the opportunity of personalization.</p><p>Interestingly, we also see that λ0 is larger than λ1 and λ4. We see this because the validation set contains many IP addresses that do not appear in the training set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>̄4</head><p>̄3 ̄2 ̄1 ̄0 <ref type="figure">Figure 2</ref>: A little bit of personalization is better than too much or too little. Note that λ2 and λ3 are larger than the other λ's. Too much personalization (λ4) runs into sparse data, whereas too little (λ0 and λ1) misses the opportunity. The EM algorithm assigns more weight to classes of users that share a few bytes of their IP address than to classes that share more (100% personalization) or less (0% personalization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of the Backoff Model</head><p>How well does this model of personalization perform on future queries? With appropriate smoothing (backoff), personalization should do no harm. Hopefully, personalization improves (reduces) entropy by enough to justify the effort. But no matter what, it should never hurt.</p><p>To evaluate the model, we used the logs between February 2, 2006 and February 6, 2006 as a test set, T . We constructed 5 properly nested subsets:</p><formula xml:id="formula_11">T4 ⊆ T3 ⊆ T2 ⊆ T1 ⊆ T0 ⊆ T</formula><p>The 5 subsets exclude queries that were not seen in the training set because they could not benefit from this model of personalization. The remainder of the Q, U RL, IP triples in T were assigned to T0, T1, T2, T3, T4 based on the most significant k bytes of the IP address. The smallest subset, T4, contains the triples where all 4 bytes of the IP address were observed in the training set. This set is properly nested within T3, which contains triples where the first 3 bytes of the of the IP address were observed in training. And so on. <ref type="figure" target="#fig_2">Figure 3</ref> shows that T0, T1, T2, T3, T4 cover between 8.8% and 51.0% of the triples in T . <ref type="figure" target="#fig_2">Figure 3</ref> shows that our proposal, personalization with backoff (dashed lines), does not harm, as we would hope. That is, the dashed line improves (lowers) cross entropy over the "no personalization" baseline, across all 5 test subsets.</p><p>In addition, personalization with backoff beats the "complete personalization" baseline in 4 of the 5 subsets. Obviously, backoff can't beat 100% personalization when you have the relevant data (T4), but even in that case, backoff isn't much worse.</p><p>This section proposed a novel backoff approach to personalization. Backoff is a classic smoothing technique bor- rowed from language modeling. We show the effectiveness of personalization with backoff using IP addresses. Users are assigned to nested classes, based on the most significant bytes of their IP address. This approach is just one possible way to approach the lower bound of search difficulty estimated with Shannon's entropy. To build a real personalized search engine, this log-based backoff model has to be combined with other features associated with search engines, such as static rank and content relevance. IP address is by no means the only possible surrogate variable for assigning users to classes. The next section will explore some other possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SEGMENTATION VARIABLES</head><p>In addition to IP addresses, there are many other variables that could be used for backing off. The next two subsections will explore day of week and time of day, two variables that have been used to segment telephone traffic into businesses and consumers <ref type="bibr" target="#b7">[8]</ref>. Consumers and businesses issue different queries at different times. Different market segments have different needs, and ask different questions at different times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Day of Week</head><p>In well-understood mature businesses like telephony, it is common to observe large and important dependencies on day-of-week. Volumes are typically higher on weekdays than weekends. Volumes are especially high on Mondays. Friday afternoon is almost a weekend. The Monday after a long weekend is even bigger than a typical Monday. There are strong interactions between these trends and market segments. Businesses tend to do most of their work on business days, whereas consumers tend to be more active during Prime Time television hours. <ref type="figure" target="#fig_3">Figure 4</ref> shows that there are similar day-of-week patterns  Weekends are harder (larger entropy) than weekdays. <ref type="figure">Fig- ure 5</ref> shows six lines, one for each of the 6 test days. Note that the solid lines (business days) are consistently below the dashed lines (weekends). There are 24 points along the x-axis in <ref type="figure">Figure 5</ref>, one for each of the 24 training days. All curves peak on weekends. Weekends are harder, both when used for training as well as testing. From the solid lines, we also learn that future weekdays are better predicted using previous weekdays than using previous weekends.</p><p>This analysis suggests that it is potentially beneficial to include the market segmentation of weekdays/weekends along with IP addresses in personalized search, and treat them accordingly. Volumes follow the expected pattern, with more queries during the day and fewer at night. Yet again, entropy is a surprise. Recall that volumes and entropies were out of phase with one another in <ref type="figure" target="#fig_3">Figure 4</ref>. This time, they are in phase with one another. It appears that queries are different from clicks. The dashed line highlights the hourly time structure. The dash line is the solid line shifted right by 24 hours. An autocorrelation analysis would compare these two lines, showing that there is a strong periodicity with a lag of 24 hours, not surprisingly. The plot makes it clear that the daily periodicity is stronger on weekdays than weekends, which again, is not unexpected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Time of Day</head><p>To test whether a market segmentation with time of day could help the personalization of future search activity, we From the plots, we see that the search history during different hours of a day shows different predictive ability over a testing set of different hours. From the two dashed lines, it is easy to see that search in the day time can be better predicted by history of the day time, and nights are better predicted by nights. When the time of the training set is closer to the time of the testing set, the cross entropy becomes lower. When the time of the training set departs from the time of the testing set, Hc(URL|IP, Q) becomes larger. This suggests that the best training data set to personalize future search activity at a given time of day is the search history at the closest time period of a day. In this analysis, it is clear that segmenting the search business with the time of day is potentially beneficial on top of IP address and day of week. "yahoo" "mapquest" "cnn" We already presented that market segmentation with geographic variables such as IP addresses, and time variables such as day/week and hour/day are beneficial for personalization. All these variables are metadata variables in search. How about those core variables in search? Is it also beneficial to differentiate the core variable in search, the query? <ref type="figure" target="#fig_8">Figure 9</ref> and <ref type="figure" target="#fig_9">Figure 10</ref> present the day-of-week frequency patterns for two groups of queries. The first group includes three query strings "yahoo," "mapquest," and "cnn," and the second group includes "sex," "mp3," and "movies." From <ref type="figure" target="#fig_8">Figure 9</ref>, we see that the frequency of the first group of queries, which we shall call business queries, has clear dayof-week patterns. There are significantly more business queries on weekdays than weekends. The second group of queries, which we shall call consumer queries, however, does not show clear day-week patterns. As in <ref type="figure" target="#fig_9">Figure 10</ref>, the frequency of consumer queries on weekends is comparable, sometimes even higher than their frequency on weekdays. It is interesting (but not unexpected), that the query "movies" is asked most frequently on Fridays.  However, the frequency of these two queries gets highest and lowest at different hours. This analysis shows that besides metadata variables such as geography and time, the search business can also potentially benefit from segmenting the search space with the type of core search variables, such as the queries. In general, users ask more questions and simpler questions during business days and business hours. It isn't clear why this is so, but we might hypothesize that businesses are more business-like, more likely to ask direct questions that have direct answers, like navigational queries. "cnn" is an example of a navigational query. The answer to the "cnn" query is simply: "www.cnn.com." In contrast, consumers ask less direct questions. They may be seeking employment, health, wealth, happiness, entertainment, etc. They may be shopping or just browsing, with no particular place to go, and lots of time on their hands. In extreme cases, one can even find Eliza-like 10 queries in the logs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query Types</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consumer Query: Weekends &gt; Weekday</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hour-of-day Pattern of Different Queries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>This paper draws connections across a wide range of fields including: Information Theory, backoff smoothing of language models, query suggestions, personalization and marketing. There is a huge body of work in each of these areas, though relatively little work that connects all of them (or even many of the pairs).</p><p>Entropy has a long history dating back to Shannon, and perhaps, earlier. See <ref type="bibr" target="#b10">[11]</ref> for an excellent retrospective on Shannon's life, work and impact. Entropy has been applied to many data sources, though there is still plenty of room for novel applications such as web logs.</p><p>There is a considerable body of work on estimating the size of the web including: <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. These references attempt to estimate supply (the size of the reachability graph of links from one url to the next), as opposed to demand (clicks). By taking demand into account, we can come up with much more feasible estimates, millions not billions, suggesting that a small cache of a few million pages could capture much of the value.</p><p>Entropy analysis is once again well accepted in Computational Linguistics. Back in the late 1940s and early 1950s, Shannon's Information Theory was having a dramatic impact on a wide range of fields from Engineering to Psychology and more. Shannon published his remarkable estimate of the size of English language in 1951 <ref type="bibr" target="#b23">[24]</ref>. Chomsky's Syntactic Structures <ref type="bibr" target="#b3">[4]</ref> came out shortly thereafter in 1957, arguing that language was unbounded (infinite) and that ngram approximations (such as Shannon's) do not come closer and closer to the truth. Chomsky's work dominated much of the thinking in Computational Linguistics over the next few decades, but Information Theory regained popularity in Computational Linguistics in the 1990s with the successes of trigram language models in speech recognition <ref type="bibr" target="#b5">[6]</ref>. The speech application motivated researchers to think about smoothing methods such as backoff: <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Query suggestions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref> and personalization <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> are somewhat related topics, though the connection between those two topics and backoff smoothing of language models is novel. Many personalized search techniques have been proposed, both server-side <ref type="bibr" target="#b28">[29]</ref> 11 and client-side <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>, as well as with long term query history <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> and short term implicit feedback <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref>. Much of this work takes advantage of search engine query logs. This work tends to be focused more on methods of improving user experience, and less on sizing challenges and opportunities.</p><p>Market segmentation (and demographics) come from a completely different tradition than Language Modeling and backoff. Marketing is relatively more central to this con-10 http://en.wikipedia.org/wiki/ELIZA 11 See also www.google.com/psearch. ference than Information Theory and Language Modeling. Pregibon and Cortes <ref type="bibr" target="#b7">[8]</ref>, for example, were concerned with marketing applications in telephony. Marketing was eager to find ways to segment customers based on usage (demand). Pregibon and Cortes found that businesses and consumers make calls at different times for different purposes. Marketing could take advantage of such insights to target offers more appropriately since businesses and consumers respond differently to different offers such as various pricing plans and discounts. The connection between, marketing, a well-established KDD application, and Language Modeling is novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, we showed how entropy can be used to address a number of fundamental questions in web search. Entropy was estimated from search logs, a sequence of triples: Q, U RL, IP , indicating that a click was observed from a particular URL and a particular IP address in response to a particular query Q.</p><p>How big is the web? Answer: millions, not billions. When the web eventually saturates the market, then the number of home pages, businesses and products will be bounded by O(population). However, unlike telephony, the web is a growth business, far from saturation. For the foreseeable future, we will be able to find millions of famous people and academics, but not everyone (not billions of ordinary people like our neighbors).</p><p>Large investments in clusters in the cloud could be wiped out if someone found a way to capture much of the value of billions with a small cache of millions.</p><p>While there are lots of pages out there (infinitely many, in a Chomskian sense), there are not that many pages that people actually go to. Shannon's entropy (H) is a powerful tool for sizing challenges and opportunities.</p><p>How hard is search? It takes around 22 bits to guess the next url (or the next query or the next user). 22 bits is millions, not billions. When we give the search engine the query, we cut the 22 bits down to around 3 bits.</p><p>What is the opportunity for personalization? Personalization cuts the search space in half (from 3 down to less than 1.5 bits). That is a huge opportunity. A personalized cache is an even bigger threat to the cluster in the cloud than a cache without personalization.</p><p>Shannon's entropy provides a novel way to think about sizing challenge and opportunity in search business. It gives the lower bound of the difficulty of personalized search but not an operational procedure to approach this lower bound. In reality, when we do not have data for a particular user, a smoothed version of the language model P (U RL|Q, U ) has to be applied. While different smoothing methods lead to different personalization approaches, we introduce one of those choices: personalization with backoff.</p><p>Personalization with backoff is more effective than personalization without backoff. As a proof of concept, we discussed backing off to classes of users based on IP addresses. A little bit of personalization was found to be better than too much or too little.</p><p>Rather than backing off based on prefixes of IP addresses, it would be better to back off based on market segmentation (demographics) and/or collaborative filtering (other users who click like you). Different segments have different needs (ask different questions at different times) and different values (willingness to pay and advertising opportunities). Businesses and consumers ask different questions at different times. We showed that query volumes and search difficulty (entropy) vary by time of day and day of week. Variables such as IP addresses and time and geography can be viewed as convenient surrogates for more sensitive market segmentation variables.</p><p>There are many possible future extensions to this work. It is interesting to introduce alternative principled smoothing methods, probably with backing off based on combinations of demographic variables. We are particularly excited by the possibility of backing off based on collaborative filtering (other users with similar search interests). It is interesting to combine the language model of personalization with backoff with other well known features in search, build a real personalized search engine, and evaluate the effectiveness of personalization with real user experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For for example, the IP address, 156 .</head><label>156</label><figDesc>111.188.243, belongs to 5 nested classes, namely: CIP,4 = {156.111.188.243} CIP,3 = {156.111.188. * } CIP,2 = {156.111. * . * } CIP,1 = {156. * . * . * } CIP,0 = { * . * . * . * }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: In 4 of the 5 test subsets, our proposal, personalization with backoff (dashed lines), has better (lower) cross entropy, Hc(U RL|IP, Q), than two baselines: too much personalization (triangles) and too little personalization (solid lines with squares).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Day-of-week patterns could be used to segment the search market into businesses and consumers. Note that click volumes are out of phase with click entropies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 showsFigure 6 :</head><label>66</label><figDesc>Figure 6 shows query volumes and entropies, H(Q|IP ) by hour for the first 15 days of January 2006. There are clear hour of day effects, especially on weekdays. Day time: More &amp; Diversified Queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 7: Search is simpler at work hours and more difficult at television hours Figure 7 shows entropies of clicks, H(U RL|IP, Q), by hour from January 7, 2006 to January 16, 2006. There is a strong hourly time structure. Entropy peaks during prime time TV hours. The valleys are very early in the morning. The dashed line highlights the hourly time structure. The dash line is the solid line shifted right by 24 hours. An autocorrelation analysis would compare these two lines, showing that there is a strong periodicity with a lag of 24 hours, not surprisingly. The plot makes it clear that the daily periodicity is stronger on weekdays than weekends, which again, is not unexpected. To test whether a market segmentation with time of day could help the personalization of future search activity, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Day/Nights better predict future DayFigure 8 :</head><label>8</label><figDesc>Figure 8: Cross validation of the predictive ability of hours in a day</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Business queries are issued on business days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Unlike business queries, consumer queries do not select for business days.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11</head><label>11</label><figDesc>Figure 11 presents the comparison of the time-of-day patterns of the frequency of two queries, "yahoo" and "sex". It is clear that both queries have clear time-of-day patterns. However, the frequency of these two queries gets highest and lowest at different hours. This analysis shows that besides metadata variables such as geography and time, the search business can also potentially benefit from segmenting the search space with the type of core search variables, such as the queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Different types of queries have different hour-day patterns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>The entropy estimates are surprisingly 
small; 22 bits is millions, not billions. A cache of a few 
million pages will cover much of the demand. 

Combination One Month 
1.5 Year 
H(Q) 
21.14 (25.1) 22.94 (29.2) 
H(URL) 
22.06 (25.9) 22.44 (29.1) 
H(IP) 
22.09 (24.6) 22.64 (27.5) 

Table 1: The search space of the web is surprisingly 
small; 22 bits of entropy corresponds to a perplexity 
of millions, not billions. 

The numbers in brackets correspond to the entropy if the 
data is uniformly distributed, or the maximum entropy. The 
actual estimates are significantly smaller than these upper-
bounds, and change very slowly with the increase of data. 

15 
17 
19 
21 
23 
25 
27 
29 

Small Log 
Big Log 
Small Log 
Upperbound 

Big Log 
Upperbound 

H(Q) 
H(URL) 
H(IP) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Entropy estimates of all combinations of Q 
(query), URL and IP addresses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 is like Table 1 ,</head><label>2like1</label><figDesc></figDesc><table>but adds joint entropies for all 
combinations of Q (query), URL and IP address. The size 
6 See Table 7.3 of http://www.fcc.gov/Bureaus/Common 
Carrier/Reports/FCC-State Link/IAD/trend605.pdf. 
7 Table 10.2 of http://www.fcc.gov/Bureaus/Common Carrier/ 
Reports/FCC-State Link/IAD/trend803.pdf reports that 
the average telephone line is used for 71 minutes per day. 
8 http://www.nielsenmedia.com/newsreleases/2005/ 
AvgHoursMinutes92905.pdf 

of the search space for search can be estimated from </table></figure>

			<note place="foot" n="1"> http://www.timeanddate.com/calendar/monthly.html? year=2005&amp;month=12&amp;country=11</note>

			<note place="foot" n="5"> http://www.netflix.com/BrowseSelection?lnkctr=nmhbs</note>

			<note place="foot" n="9"> http://labs.google.com/suggests</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>The authors thank the anonymous reviewers for their useful comments. We thank Mike Schultz for his help on preparing the search log data. We thank people in the Text Mining, Search, and Navigation Research (TMSN) group of Microsoft research for their valuable discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Long Tail: Why the Future of Business is Selling Less of More. Hyperion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A technique for measuring the relative size and overlap of public web search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on World Wide Web 7</title>
		<meeting>the seventh international conference on World Wide Web 7</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 34th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Syntactic Structures. The Hague/Paris: Mouton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of the enhanced good-turing and deleted estimation methods for estimating probabilities of english bigrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="54" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on computational linguistics using large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The wild thing!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thiesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2005</title>
		<meeting>the ACL 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="93" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signature-based methods for data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pregibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="182" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of web metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dhyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="503" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A retrospective on his life, work, and impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2681" to="2695" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The indexable web is more than 11.5 billion pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Signorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special interest tracks and posters of the 14th international conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="902" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
		<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Query word deletion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Fain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="435" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating query substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speeech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">5360</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accessibility of information on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="issue">6740</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accessibility of information on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="39" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query chains: learning to rank from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>eeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Billions and Billions: Thoughts on Life and Death at the Brink of the Millennium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Systems Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction and entropy of printed english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Systems Technical Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Context-sensitive information retrieval using implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ucair: a personalized search toolbar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="681" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive web search based on user profile constructed without any effort from users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hatano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on World Wide Web</title>
		<meeting>the 13th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining long-term search history to improve search accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="718" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Personalizing search via automated analysis of interests and activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Examining the effectiveness of real-time query expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing and Management (IPM)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="685" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
