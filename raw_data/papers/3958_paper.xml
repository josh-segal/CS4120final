<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Near-Duplicate Detection Using Simhash</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadhan</forename><surname>Sood</surname></persName>
							<email>sadhan@cs.tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Texas A&amp;M University College Station</orgName>
								<orgName type="institution" key="instit2">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843, 77843</postCode>
									<region>TX, TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Loguinov</surname></persName>
							<email>dmitri@cs.tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Texas A&amp;M University College Station</orgName>
								<orgName type="institution" key="instit2">Texas A&amp;M University College Station</orgName>
								<address>
									<postCode>77843, 77843</postCode>
									<region>TX, TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Near-Duplicate Detection Using Simhash</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Information Search and Retrieval]: Clustering General Terms Algorithms Keywords Hamming distance</term>
					<term>similarity</term>
					<term>simhash</term>
					<term>clustering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper offers a novel look at using a dimensionality-reduction technique called simhash [8] to detect similar document pairs in large-scale collections. We show that this algorithm produces interesting intermediate data, which is normally discarded, that can be used to predict which of the bits in the final hash are more susceptible to being flipped in similar documents. This paves the way for a probabilistic search technique in the Hamming space of simhashes that can be significantly faster and more space-efficient than the existing simhash approaches. We show that with 95% recall compared to deterministic search of prior work [16], our method exhibits 4-14 times faster lookup and requires 2-10 times less RAM on our collection of 70M web pages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many clustering problems in data mining involve similarity matching, which is a process of finding pairs of documents in collection D that are similar (in some sense) to each other. To decouple similarity from page semantics and ultimately human judgement, it is common to preprocess the data by extracting d-dimensional feature vectors (i.e., syntactic elements perceived to be important) from each page and solving the matching problem on them.</p><p>When the number of dimensions d is large, many traditional clustering approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref> are a Θ(n 2 ) endeavor, where n = |D|, that requires an all-to-all comparison across the entire dataset. Since this is infeasible for collections * Supported by NSF grants CNS-0720571 and CNS-1017766.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. larger than a few thousand pages, another approach <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref> is to devise approximate algorithms that can compute similarity using much smaller data structures and subquadratic overhead. One promising technique in this category, called simhash <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, relies on replacing feature vectors with b-bit fingerprints that preserve cosine similarity of the vector space.</p><p>The main challenge with simhash is to quickly find all pairs of fingerprints within a certain Hamming distance h of each other. For large collections (i.e., billions of pages), the currently fastest and most space-efficient solution <ref type="bibr" target="#b15">[16]</ref>, which we call Block-Permuted Hamming Search (BPHS), relies on permuting chunks of each hash and performing lookups in multiple similarly-permuted copies of the dataset. However, since D must be replicated at least four times in RAM, BPHS may be unsuitable for certain memory-constrained systems. Even if enough RAM exists to hold 4n items, unless the number of copies is scaled with dataset size, the search complexity of this method is still quadratic in n.</p><p>Noticing that simhash is already probabilistic in nature and does not guarantee 100% recall on vector pairs, it makes sense to ask whether by sacrificing a small additional percentage of recall one can reduce RAM consumption of BPHS, speed up the search, and improve scalability as n → ∞. We investigate this question next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>We first analyze the construction process of simhash and observe that certain bits are much more volatile (i.e., likely to be flipped by modification of features) than others. This leads to a conjecture that the Hamming-distance problem on simhash can be solved efficiently by an iterative process that intelligently searches to distance h within the volatile bits and performs lookups in a single copy of the dataset, keeping RAM overhead at the theoretical minimum.</p><p>Thus, the main question with using this approach is how to design an efficient algorithm that decides which bits to flip, in what order, and when to stop (if less than 100% recall is desired). For a given page u, let pi(u) be the probability that bit i of u's hash is different from that in other pages of the collection and S ⊆ {1, 2, . . . , b} be some subset of hash bits. Then, the probability that there exists another simhash that differs from u only in bits contained in S can be estimated as:</p><formula xml:id="formula_0">p(u, S) = ∏ i∈S pi(u) ∏ j / ∈S (1 − pj(u)),<label>(1)</label></formula><p>where independence between the bits is assumed from the simhash algorithm <ref type="bibr" target="#b7">[8]</ref>.</p><p>To maximize useful work and allow the search to stop after k ≪ ( b h ) attempts, one requires a method for obtaining the top-k subsets S (1 ≤ |S| ≤ h) in decreasing order of their probability p(S), but with much lower complexity than exponential needed to compute all possible values (1) and sort them. To solve this problem, we offer a novel algorithm we call Volatility Ordered Set Heap (VOSH), which performs the job in the optimal Θ(k log k) time and Θ(k) space. For the common setting h = 3 and b = 64 suggested for large datasets <ref type="bibr" target="#b15">[16]</ref>, we compare VOSH with a matching algorithm that flips bits in random order without repetition. Our results show that for 100% recall, the former requires 61 times fewer attempts than the latter; for 80% recall, 151 times fewer; and for 50% recall, 347 times fewer.</p><p>We incorporate VOSH in a system we call Probabilistic Simhash Matching (PSM) for finding similar documents in large collections D under two modes of operation -online and batch. The former case, assuming that D fits in RAM, aims to minimize the latency of answering queries about incoming pages u. The latter case, assuming that D is kept on disk, aims to maximize the query throughput rate. We compare our method against BPHS <ref type="bibr" target="#b15">[16]</ref> in terms query speed and RAM usage using a web collection with 70M documents. For 95% recall, our results show that PSM requires 2 − 10 times less RAM, while being 5 − 14 times faster than BPHS. We also model both approaches and show that with a fixed number of copies of the dataset in RAM, PSM scales as O(n log h (n) log log n) as opposed to BPHS's Θ(n 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The first approach to matching similar documents is to convert them to some canonical form in which they become exact duplicates. By removing certain tokens perceived dispensable (e.g., high/low IDF words <ref type="bibr" target="#b8">[9]</ref>) and hashing the resulting page to a single fingerprint, similar documents can be found in Θ(n log n) time using sorting.</p><p>Detecting similar pages over a general space of d-dimensional feature vectors is a more challenging task, especially at nontrivial scale. In the small number of dimensions d ≪ log 2 n, special data structures (e.g., R-tree <ref type="bibr" target="#b11">[12]</ref>, Kd-tree <ref type="bibr" target="#b3">[4]</ref>) can find near neighbors in Θ(n log n) time; however, their performance deteriorates to that of an exhaustive search (or worse) as d increases. Since web-scale collections usually exhibit dictionaries with millions of unique words, each typically mapping to a feature, this approach is generally inapplicable to such cases.</p><p>A more viable solution for large d and n involves approximate algorithms that sacrifice some precision and recall in favor of manageable speed. These include Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b13">[14]</ref>, Min-Wise Independent Permutations <ref type="bibr" target="#b6">[7]</ref>, simhash <ref type="bibr" target="#b7">[8]</ref>, and many variations <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. This is the direction we pursue below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FUNDAMENTALS</head><p>We start by motivating the usage of fingerprints, analyze the brute-force approach to finding near duplicates, and formulate the problem we aim to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework and Motivation</head><p>Since our focus is on large web crawls (i.e., billions of pages), simhash's low space and time complexity <ref type="bibr" target="#b15">[16]</ref>, as well as high precision <ref type="bibr" target="#b12">[13]</ref> compared to other hashing techniques  <ref type="table" target="#tab_5">Time  RAM  1M  91 days  1.1 GB  34 min  8 MB  64M 1,020 years 70 GB  97 days 512 MB  8B  261K years  9 TB  68 years  64 GB   Table 1</ref>: Extrapolated delay to compute cosine similarity and Hamming distance on all pairs (one core of AMD Phenom II 2.8 GHz).</p><p>[7], are quite appealing. With this in mind, we next set up the terminology and notation that will be needed later. Let F = {1, 2, . . . , d} be the set of all unique features across the entire collection and F(u) ⊆ F be the set of features present on page u ∈ D. The number of features f (u) = |F(u)| can vary from a few hundred to a few thousand depending on which features are selected and the specific dataset. Each feature i ∈ F (u) is described by a certain real weight w i (u) ∈ R, which measures the importance and contribution of i to u. A combination of F (u) and weights {wi(u)} i∈F (u) represents the feature vector of page u.</p><p>Define V to be the set of all feature vectors produced by D and assume some similarity measure s : V 2 → <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> that maps pairs of vectors to real numbers (i.e., values close to 0 indicate dissimilar documents and those close to 1 indicate similar ones). In the context of shingles and min-wise hashing <ref type="bibr" target="#b6">[7]</ref>, s(u, v) is usually Jaccard's coefficient; however, for simhash and non-integer weights it is more common to utilize cosine similarity <ref type="bibr" target="#b7">[8]</ref>, which we also do below whenever comparison in the vector space is needed.</p><p>To understand the scale at which similarity can be sought in an all-to-all search among feature vectors, we have the following analysis. Each pair of vectors (u, v) requires f (u) + f (v) operations, some of which can be quite expensive (e.g., multiplication for cosine similarity), for a total overhead of E[f (u)]n 2 . Assuming ζ bytes are needed to store each feature index and its weight, the expected space requirement of D is nE[f (u)]ζ. Using our dataset of webpages with E[f (u)] = 141 and ζ = 8, the left side of <ref type="table">Table 1</ref> shows the amount of time and RAM needed to find all duplicate vectors under cosine s(u, v). Notice in the table that both space and time are prohibitive, even for relatively small datasets.</p><p>To make storage and computation more reasonable, a second level of approximation (i.e., simhash) is a one-way mapping V → {0, 1} b that converts feature vectors to b-bit binary strings. Assuming H is the collection of hashes produced by V, the main similarity metric on H is Hamming distance H(x, y) whose benefit over s(u, v) lies in the fact that it can be computed with a handful of CPU instructions <ref type="bibr" target="#b19">[20]</ref>.</p><p>Replacing each feature vector with its 64-bit simhash, the right side of <ref type="table">Table 1</ref> shows an improvement in computational speed by a factor of 3800 (i.e., from 70K pairs/sec to 268M pairs/sec) and a reduction in space by a factor of 141, both of which are substantial. While this solves the problem for n up to a few million pages, the 3 months for medium-sized collections and the 68 years for large datasets shown in the table are still undesirable in practice. This explains our interest in sub-quadratic techniques for computing Hamming distances over large document sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Formulation</head><p>There are two classes of matching problems we consider in this paper. In the first class (e.g., clustering <ref type="bibr" target="#b10">[11]</ref>), the</p><formula xml:id="formula_1">Algorithm 1 Simhash (u) 1: W ← array of b zeros 2: for i ∈ F (u) do ◃ Examine each feature 3: ϕ i ← UniformHash(i) ◃ Compute b-bit hash</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for j = 1 to b do ◃ Iterate through each bit 5: goal is to find all matches for a given page. Knowing pairwise similarity among pages, one can use separate clustering algorithms, which we do not consider here, to combine the various documents into groups.</p><formula xml:id="formula_2">if ϕ ij = 1 then ◃ j-th bit of ϕ i 6: W [j] ← W [j] + w i ◃ Add</formula><formula xml:id="formula_3">Objective 1. Given x, find all y ∈ H s.t. H(x, y) ≤ h.</formula><p>In the second class (e.g., duplicate elimination <ref type="bibr" target="#b12">[13]</ref>, plagiarism detection <ref type="bibr" target="#b18">[19]</ref>), the goal is to determine if there exists at least one similar page in the dataset, without finding all matching documents, which often can save significant processing overhead and increase performance.</p><formula xml:id="formula_4">Objective 2. Given x, find any y ∈ H s.t. H(x, y) ≤ h.</formula><p>It should be noted that these goals are not specific to what features are being used (e.g., individual words, shingles, HTML tags, anchor text, URLs), their weights (e.g., frequency, TF-IDF, HTML highlight), dimensionality of the vectors, or classification purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">UNDERSTANDING SIMHASH</head><p>We next explain the simhash construction algorithm and dissect the properties of its hashes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithm</head><p>Recall that simhash is a feature fingerprinting technique that uses random projections to generate compact representation of high-dimensional vectors. Its main characteristic is that the Hamming distance between two document fingerprints is positively correlated with cosine similarity between the corresponding feature vectors.</p><p>Using the notation of the previous section, Algorithm 1 explains the simhash process of <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, which we discuss in more detail next. Denote by W a vector of temporary weights that the simhash function generates. As we will see later, these weights are important elements of the proposed framework. For each feature i in the current document u, Algorithm 1 first computes its uniformly random hash ϕi and then decides to either add or subtract the feature's weight wi to/from Wj based on whether the j-th bit ϕij of the uniform hash is zero or one. After all features have been processed, bits of simhash with non-negative Wj are set to 1 and the remaining bits are set to 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weights and Hamming Distance</head><p>We are now ready to examine the distribution of weights {Wj} and lay the foundation for understanding which bits are more likely to be different between similar documents.</p><p>Personalizing each variable in the algorithm with its page u, it is not difficult to notice that simhash computes a dotproduct between a global vector of iid random variables and a vector of u's specific feature weights:</p><formula xml:id="formula_5">Wj(u) = ∑ i∈F (u) (2ϕij − 1)wi(u),<label>(2)</label></formula><p>where {ϕij}ij are iid Bernoulli variables with E[ϕij] = 1/2. This makes each weight Wj(u) zero-mean:</p><formula xml:id="formula_6">E[Wj(u)] = ∑ i∈F (u) E[2ϕij − 1]E[wi(u)] = 0.<label>(3)</label></formula><p>Using normalized TF-IDF weights wi(u), <ref type="figure" target="#fig_1">Figure 1</ref>(a) shows the distribution of W1(u) from (2) using our collection of 70M documents. We focus only on the first bit since the others produce identical results. While the Central Limit Theorem suggests that constant feature weights (e.g., used in <ref type="bibr" target="#b12">[13]</ref>) are expected to converge Wj(u) to a Gaussian distribution, TF-IDF weights interestingly lead to a Laplace-like distribution (log-linear plots of tails are omitted for brevity).</p><p>To control the variance and distribution of resulting sums, one can generalize (2) to non-Bernoulli cases. Assume νi = (νi1, . . . , ν ib ) is a vector permanently associated with feature i ∈ F whose elements are drawn from a zero-mean symmetric distribution. Then, (2) becomes:</p><formula xml:id="formula_7">Wj(u) = ∑ i∈F (u) νijwi(u),<label>(4)</label></formula><p>while the rest of the algorithm remains the same. Assuming θ ∈ <ref type="bibr">[0, π]</ref> is the angle between two d-dimensional vectors, <ref type="bibr" target="#b7">[8]</ref> shows that if νij are zero-mean Gaussian variables, the probability that the corresponding fingerprints collide on a given bit is q = 1 − θ/π ≈ (cos θ + 1)/2. While no similar closed-form result is available for (2), intuition suggests that a correlation between cos θ and q still exists.</p><p>Since wi(u)'s positive contribution to bit j1 has no bearing on whether its contribution to another bit j2 will be positive or negative, it follows that bits in the final simhash are pairwise independent, which implies that H(x, y) is a binomial random variable with parameters b and 1 − q. While a-priori this tells us nothing about which bits are likely to differ in a random pair (x, y), we next show that utilizing the knowledge of x's simhash weights {Wj} allows an informed decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Why Bits Flip</head><p>Suppose page u undergoes a re-write, which includes deletion of α existing features and addition of β new features. This modifies each simhash weight j to:</p><formula xml:id="formula_8">W ′ j (u) = Wj(u) − X α j + X β j ,<label>(5)</label></formula><p>where X α j and X β j are summations in the form of <ref type="formula" target="#formula_5">(2)</ref> that correspond to the deleted/added features. Define Yj = X α j − X β j to be the random weight change that occurs in response to these modifications. Since we are interested in changes to u that exist in our collection D (as opposed to an infinite number of hypothetical modifications), Yj can be viewed as a random variable with the same distribution as {Wj(u) − Wj(v)}u,v∈D, which is shown in <ref type="figure" target="#fig_1">Figure 1</ref></p><formula xml:id="formula_9">(b). The main dif- ference from part (a) is the doubled variance (i.e., V ar[Yj] = V ar[Wj(u)] + V ar[Wj(v)] = 2V ar[Wj(u)])</formula><p>and a slightly more Gaussian shape; however, the log-scale shows that both tails remain exponential rather than Gaussian.</p><p>Conditioned on Wj(u) = c, it follows that W ′ j (u) = c − Yj, which suggests that the difficulty of flipping bit j is directly related to the magnitude of c. Indeed, first notice that Yj</p><formula xml:id="formula_10">is zero-mean since E[X α j ] = E[X β j ] = 0</formula><p>, which follows from (3). Second, averaged over all possible pairs (α, β), the distribution of Yj is symmetric around 0 since Xα and X β are iid. We can make this argument due to the non-adversarial nature of changes applied to the page. As a result, we have P (Yj &gt; |c|) = P (Yj &lt; −|c|). Finally, in order to flip bit j, one must encounter a modification that satisfies both a) |Yj| &gt; |c| and b) sign(Yj) = sign(c). The probability of this happening is P (Yj &gt; |c|) = 1 − Fj(|c|), where Fj(x) is the CDF of Yj. Due to the monotonicity of CDFs, this probability monotonically decays with |c|.</p><p>This brings us to our main result of the section. Observe that each simhash contains certain bits that are much more volatile than others. Assume page v is similar to u according to cosine similarity at some threshold (e.g., 0.9). Then, if v manages to flip some bits in u's simhash, the likely order of these flips follows from the smallest |Wj(u)| to the largest. For example, consider simhash weights (1.9, 0.01, −0.5) for the first three bits and the distribution of Yj from <ref type="figure" target="#fig_1">Figure  1(b)</ref>. If anything at all is flipped by small changes, it will most likely be a single bit 2. Much more effort is needed to flip bit 3, while bit 1 requires massive summations of added/deleted features in (2) to overpower its weight 1.9.</p><p>While it is straightforward to generate lookups for Hamming distance h = 1 (i.e., by sorting all bits in the increasing order of |Wj(u)|), multi-bit flips are more difficult. For example, is the two-bit combination with weights (1.9, 0.01) more likely than a single-bit combination with weight −0.5? We study this question next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">BIT ORDER</head><p>This section develops an efficient technique for deciding the order in which bits should be attempted during similarity search on H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model</head><p>Define pj(u) = P (Yj &gt; |Wj(u)|) to be the probability of that another page in D has flipped bit j in the simhash of a given page u. While obtaining a closed-form model for pj(u) might be possible in future work, an empirical distribution of Yj sampled from pages in D, e.g., as in <ref type="figure" target="#fig_1">Figure 1</ref>(b), is sufficient for estimating pj(u) in practice.</p><p>Let S ⊆ {1, 2, . . . , b} be a non-empty subset of hash bits. Then, the probability that there exists a page in D that differs from u only in the bits contained in S is:</p><formula xml:id="formula_11">p(u, S) = ∏ i∈S pi(u) ∏ j / ∈S (1 − pj(u)).<label>(6)</label></formula><p>To limit the Hamming search to the most likely subsets, one requires an ordering of {S} according to <ref type="bibr" target="#b5">(6)</ref>. The bruteforce approach would be to generate all possible subsets S, sort them in the decreasing order of p(u, S), and then select the top-k elements, where k controls the tradeoff between overhead and recall. The main problem with this method is that it requires Θ(l log l) operations and Θ(l) space, where</p><formula xml:id="formula_12">l = ∑ h i=1 ( b i )</formula><p>is often quite large. We next offer a better algorithm that solves this problem in optimal space and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Volatility Heap</head><p>We start by considering the problem at fixed Hamming distance h. We later generalize the solution to all distances up to h. Throughout this section, it is convenient to refer to bits in decreasing order of their probability pj(u) rather than their physical position in the hash (i.e., bit 1 is the most volatile and bit b is the least). Similarly, each set S is assumed to be sorted in decreasing volatility of its bits (e.g., S = (1, 3, 7) refers to the first, third, and seventh most volatile bits of the hash).</p><p>Define ≺ to be a lexicographical comparison operator on {S}. If S1 ≺ S2, then there exists an index i &lt; h such that the two sets share the leading i elements, but S2 is larger in the (i + 1)-st element. For example, (1, 3, 7, 15) ≺ (1, 3, 9, 10). Then, we have the following important result. Lemma 1. If two sets S1, S2 of the same size h differ in exactly 1 bit and S1 ≺ S2, then ∀u : p(u, S1) ≥ p(u, S2).</p><p>Proof. Suppose the bits that differ are i in S1 and j in S2. Since (6) has h − 1 terms in each product that are common between p(u, S1) and p(u, S2), we get:</p><formula xml:id="formula_13">p(u, S1) p(u, S2) = pi(1 − pj) pj(1 − pi) = pi − pipj pj − pipj .<label>(7)</label></formula><p>Recalling that S1 ≺ S2, notice that i &lt; j and pi ≥ pj, which immediately shows that (7) is lower-bounded by 1.</p><p>This result paves the way for an algorithm that sorts bit combinations using a structure we call Volatility Ordered Set Heap (VOSH). It starts with the optimal set S0 = (1, 2, . . . , h) in the root and iteratively generates for each existing node Si two children, each of whom succeeds Si according to ≺ and differ from the parent in exactly one bit. From Lemma 1, observe that each child's p(u, S) is always no larger than the parent's. Using transitivity of ≺ and ≥, we obtain that each Si contains bits whose combination is at least as likely as that of any node in its entire subtree.</p><p>We next describe how the tree is constructed using Algorithm 2. Given node S with h bits, VOSH attempts to generate two children. The left child always increments the last bit of the parent as long as the result does not exceed b (if it does, the left branch stops). The right child scans the parent's bits from right to left until it finds the first gap </p><formula xml:id="formula_14">Algorithm 2 ProduceChildren(S) 1: if S[h] ≤ b − 1 then ◃ Attempt left child 2: S L ← S; S L [h] ← S L [h] + 1 ◃ Add 1 to last bit 3: else 4: S L ← ∅ ◃</formula><formula xml:id="formula_15">gap = S[j + 1] − S[j]</formula><p>8:</p><p>if gap = 2 then ◃ Should increment bit j?</p><p>9: ◃ Produce both children in bit numbers of size at least 2. If the gap is exactly 2, the bit on the left of the gap is incremented. Otherwise, the right child is omitted. The last nuance is necessary to prevent generation of duplicate nodes along different branches of the tree. <ref type="figure" target="#fig_2">Figure 2</ref> shows the top five levels of three VOSH heaps that correspond to h = 1, 2, 3.</p><formula xml:id="formula_16">S R ← S; S L [j] ← S L [j] +</formula><note type="other">1 2 3 4 1,2 1,3 1,4 1,5 2,3 2,4 3,4 2,5 1,2,3 1,2,4 1,2,5 1,2,6 1,3,4 1,3,5 1,3,6 1,4,5 2,3,4 5 2,3,5 1,2,7 1,6 h=2 h=3 h=1</note><p>Upper-bounding each heap size by k, space and time complexity of constructing all VOSH trees to depth h is Θ(k). We next explain how to use them during Hamming search to decide the order of bit flips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PROBABILISTIC SEARCH</head><p>In this section, we present our algorithm for near-duplicate search in the simhash space. We start with the bit-generation process, verify its effectiveness, and then discuss online/batchmode operation on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bit Selection</head><p>While VOSH ensures that each subtree should not be traversed before its root, it does not (and cannot) decide which of the two siblings at each level is more optimal. As explained next, we make this decision during run-time using an additional max-heap M that operates on (key, value) pairs, where the key is p(u, S) and the value is set S.</p><p>Given a page u, we first compute its weights {Wj(u)} and the corresponding {pj(u)}. We then populate M with tuples (p(u, S), S) generated by h root nodes of VOSH trees, each corresponding to a different number of bits. At each bit-flip, we extract from M the node with the largest p(u, S), obtain its children from the corresponding VOSH tree, compute their probabilities p(u, S), and insert their tuples into M . This guarantees traversal of bit combinations in the decreasing order of p(u, S) and keeps the total complexity of k flips at the optimal Θ(k log k).</p><p>In <ref type="bibr" target="#b15">[16]</ref>, b = 64 and h = 3 were tested in an 8B-page col-  lection and found to work well. We later verify that these numbers are also quite appropriate for our dataset; in the meantime, use them as our target combination for all experiments and analysis. To understand the performance of VOSH combined with M , we extract from our dataset 8M random pairs of simhashes at Hamming distance h = 1, 2, 3. We then compare our VOSH-driven approach to random flipping of bits, where the latter considers all possible combinations of exactly h bits, ensuring there is no repetition. <ref type="figure">Figure 3(a)</ref> shows the CDF of the number of bit attempts needed to find each of the matches at distance h = 1. Observe in the figure that the first VOSH flip finds the matching pair in over 30% of the cases and four flips accomplish the same 80% of the time. All pairs at Hamming distance 1 are discovered in 17 or fewer attempts, while the random approach requires 64 flips to achieve the same 100% recall. For the exact distances h = 2 and h = 3 (drawn on a log-linear scale), VOSH finds all pairs in 152 and 675 flips, respectively. This compares favorably to 2,016 and 41,664 attempts needed by the random approach.</p><p>The difference between VOSH and random flipping becomes more pronounced as h increases and recall decreases. This is illustrated in <ref type="figure">Figure 3(d)</ref>, which plots the ratio of the number of attempts needed between the two methods for 50%, 80%, and 100% recall. At h = 1, VOSH is 3.7 times faster than random at 100% recall and 16 times faster at 50%. These numbers increase to 13 and 37 respectively for h = 2, eventually becoming 61 and 347 for h = 3.</p><p>With these encouraging results in mind, we next describe how VOSH can be applied to large-scale datasets in a framework we call Probabilistic Simhash Matching (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Online Queries</head><p>In online mode, the existing fingerprints H are stored in memory to support similarity queries about arriving pages. The main performance metric in this setup is delay τ needed to compare a new document against a set H of n simhashes. To support efficient lookups, assume H is sorted before the algorithm starts and let n = 2 d be a power of 2. Using VOSH across all b bits is extremely wasteful as it produces a large number of hashes that do not exist in the collection, which is especially noticeable when n ≪ 2 b . Given b = 64, most datasets D fall into this category. It thus makes sense to limit the random lookups to some small range of t ≤ d uppermost bits, which we call the header, and perform a linear scan to match the remaining bits.</p><p>The matching process may perform a d-step binary search on H or utilize a hash table of size 2 t for efficient header lookups. We use the latter approach in the paper and illustrate the resulting system in <ref type="figure" target="#fig_5">Figure 4</ref>(a). Given a query page u with simhash x, the first lookup is x itself at Hamming distance 0. We then use VOSH to generate k new queries x1, . . . , x k by working with t most-significant bits of x. In each lookup, if i bits are flipped in the header, the linear scan flags all pages whose Hamming distance to x in the remaining b − t bits is no more than h − i.</p><p>If only a single-match is desired, the linear scan stops as soon as it identifies the first similar page; otherwise, it continues until a header mismatch. To differentiate these cases, we use PSMF (first) and PSMA (all), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Batch Queries</head><p>In batch mode, the existing collection H is stored on disk. New fingerprints are accumulated in set Q until it reaches size m. After Q becomes full, PSM scans file H by reading it in chunks of m fingerprints and matching each x ∈ Q against the loaded chunk using the online method described above. After Q is processed to completion, it is sorted and appended to the existing file. This removes the need to sort chunks again when they are loaded in RAM.</p><p>It should be noted that PSMF can stop reading the file as soon as finds at least one near-duplicate for each fingerprint in Q, which in certain cases can save significant amounts of overhead. In the worst case, however, both PSMF and PSMA read the entire file for each Q. In a multi-core system, the lookups can be easily parallelized by splitting Q across threads. It also makes sense to pre-process Q by identifying the volatile bits in each hash and retaining probabilistic information only related to them. This not only saves memory, but also saves time in identifying top bit-combinations every time a new chunk is loaded into memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">PERFORMANCE MODELING</head><p>This section models the currently fastest and most spaceefficient simhash matching approach <ref type="bibr" target="#b15">[16]</ref> and compares its overhead to that of PSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Online BPHS</head><p>Efficient search in the Hamming space is an old problem <ref type="bibr" target="#b16">[17]</ref> that has remained difficult to solve at large scale. For small h, <ref type="bibr" target="#b15">[16]</ref> offers an algorithm, which we call BlockPermuted Hamming Search (BPHS), for dealing with large collections D. Suppose we are interested in finding all hashes y ∈ H within Hamming distance h ≥ 1 of x. BPHS first splits the b-bit hash x into G ≥ h + 1 non-overlapping blocks of γ = b/G consecutive bits. It then selects an integer g between 1 and G − h. If H(x, y) ≤ h, then block-by-block comparison between x and y is guaranteed to have at least g exact matches. The goal of BPHS is to group all possible combinations of g blocks at the front of the hash and perform the search only on them. This is shown in <ref type="figure" target="#fig_5">Figure  4</ref>(b) for G = 8 and g = 3, which is a combination that can support all h ≤ 5.</p><p>Similar to PSM, define the leading g blocks of the hash as its header. Then, there are T = ( G g ) ways of selecting the g blocks for the header. Call each of these selections a block permutation πi(x) of the original hash. Since the order of blocks neither in the header nor in the rest of the hash matters, there are exactly T unique permutations, which applied to H produce T copies of the dataset H1, . . . , HT .</p><p>After sorting the copies (which are called tables in <ref type="bibr" target="#b15">[16]</ref>), the lookup proceeds in T iterations. For the i-th step, BPHS uses a binary search to find the numerically smallest hash in Hi whose header matches that of πi(x). Starting with that hash, it scans Hi linearly until the first header mismatch. During this scan, it computes the Hamming distance in the lower b − (gγ) bits between each target hash and x, with the rest of the algorithm being similar to PSM. We consider two versions: BPHSF stops after finding the first match, while BPHSA always checks every eligible hash.</p><p>For the analysis, we assume a sufficiently large b to ignore round-off effects during division and model only the overhead of BPHSA. Define δR to be the RAM latency during random access, δP to be the delay needed to permute the blocks of x, and δL to be the per-hash delay needed to perform Hamming-distance calculations during linear scan. Then, define the number of bits that are matched during binary search as:</p><formula xml:id="formula_17">t = min(gγ, d) = min ( gb G , d ) ,<label>(8)</label></formula><p>which has a similar meaning to t in PSM. Note that the min function is necessary since the binary search exhausts the entire dataset in d steps. Then, the total the lookup latency of BPHS in T tables is:</p><formula xml:id="formula_18">τ = ( G g ) ( δP + tδR + 2 d−t δL ) .<label>(9)</label></formula><p>The storage overhead (in bytes) of BPHS is simply:</p><formula xml:id="formula_19">Ω = ( G g ) nb 8 .<label>(10)</label></formula><p>We next analyze the issue of optimally selecting G. For a fixed g and assuming gγ ≤ d, (9) breaks into two terms:</p><formula xml:id="formula_20">τ = (G − 1)!b (G − g)!(g − 1)! δR + ( G g ) ( δP + 2 d−gb/G δL ) ,<label>(11)</label></formula><p>both of which monotonically increase in G. For gγ &gt; d, we have an even simpler situation:</p><formula xml:id="formula_21">τ = ( G g ) (δP + dδR + δL),<label>(12)</label></formula><p>which also shows that choosing the smallest possible G leads to best performance. From (10), we can make the same observation about RAM overhead. Since both space and time decrease with G, it follows that its optimal value is its lower bound g + h. Re-writing (9), we have:</p><formula xml:id="formula_22">τ = ( g + h g</formula><p>) (</p><formula xml:id="formula_23">δP + tδR + 2 d−t δL ) .<label>(13)</label></formula><p>Next, notice that increasing gγ beyond d hurts performance in <ref type="formula" target="#formula_0">(13)</ref> as it keeps t = d constant, but increases the leading binomial coefficient, which makes the final model:</p><formula xml:id="formula_24">τ (d) = ( g + h g ) ( δP + gb g + h δR + 2 d−gb/(g+h) δL ) ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_25">g ≤ dh b − d .<label>(15)</label></formula><p>Determination of optimal g is impossible without knowing the relationship between δR and δL, as well as the value of b in comparison to d. For a RAM-restricted system that cannot grow T to infinity, notice that τ (d) scales exponentially with d, or in other words, linearly with set size n. Thus, in such cases, similarity search for all document pairs in H requires complexity Θ(n 2 ). We verify this finding in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Batch BPHS</head><p>Batch mode in BPHS is similar to that in PSM, with two exceptions explained in <ref type="bibr" target="#b15">[16]</ref>. First, matching in BPHS proceeds by checking each fingerprint y from the loaded chunk against T tables built around simhashes in Q. This is necessary to prevent repeated construction of T permuted copies of each loaded chunk. One peculiar side-effect of this optimization is that BPHS cannot stop when it finds the first match for y since there might be fingerprints x ∈ Q that still do not have any matches. Thus, in batch mode, BPHSF is identical to BPHSA. The second difference from PSM is that batches Q do not have to be sorted before being appended to the file since the search runs against Q rather than H.</p><p>To process a batch of m = |Q| hashes, the I/O delay is the time needed to read the entire file:</p><formula xml:id="formula_26">τ disk = nb 8D ,<label>(16)</label></formula><p>where D is the read speed of the hard drive. For highperformance RAID-based configurations and overlapped I/O, computation can be executed while the next chunk is being read. In such cases, the bottleneck is in the CPU portion of the overhead, which consists of the time to permute the m incoming hashes T times, sort the corresponding tables, and perform n lookups in them: where τ (.) is given by the online model <ref type="bibr" target="#b13">(14)</ref>. Finally, the throughput of this system in hashes per second is:</p><formula xml:id="formula_27">τCP U = T m(δP + log 2 m) + nτ (log m),<label>(17)</label></formula><formula xml:id="formula_28">r = m max(τ disk , τCP U ) .<label>(18)</label></formula><p>In the experiments below, τCP U ≈ 20τ disk dominates and rate r is determined solely by the performance of the studied algorithms rather than the disk speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Online PSM</head><p>Recall that VOSH flips k combinations in the upper t bits of each fingerprint x. PSM then searches for these combinations in a single copy of the dataset H in RAM. The latency of each lookup consists of the VOSH overhead δV per examined element in the heap (which hash depth log 2 k), a single visit to the header hash table, and the linear scan:</p><formula xml:id="formula_29">τ ′ (d) = k(t)(δV log 2 k(t) + δR + 2 max(d−t,0) δL),<label>(19)</label></formula><p>where we make k explicitly depend on t. Since increasing t increases k(t), the following simple analysis helps choose the optimal t. Recall that each VOSH combination is limited to h bits out of t possible, which means that k(t) is upperbounded by:</p><formula xml:id="formula_30">k(t) ≤ h ∑ i=1 ( t i ) = Θ(t h ).<label>(20)</label></formula><p>Therefore, ignoring the small terms δV (k) and δR in (21) and only considering t ≤ d, the dominating term of the delay is Θ(t h 2 −t ), which is minimized when t is maximized. This shows that the optimal choice is t = d and:</p><formula xml:id="formula_31">τ ′ (d) = k(d)(δV log 2 k(d) + δR + δL).<label>(21)</label></formula><p>To understand the growth of k(t) with t, we partition our 70M-page web collection into two parts -10M random pages are selected to arrive in online mode and the remaining 60M are chosen for the main dataset H. We first pass each online page through BPHS to find all matches in H within Hamming distance h = 3. We then select for each t such k(t) that achieve 95% recall in PSM compared to BPHS.</p><p>The result is plotted in <ref type="figure" target="#fig_7">Figure 5</ref>(a). As t grows from 16 bits to 26, the number of combinations required by PSMA increases from 6 to 23 and that for PSMF from 3 to 15. The exact growth rate cannot be readily ascertained over this small range, but it is visibly super-linear. The total run time to verify 10M hashes in online mode is shown in <ref type="figure" target="#fig_7">Figure 5</ref>(b). As predicted, the delay is minimized when t is at its maximum (i.e., d), in which case the processing speed reaches 1.6M arriving hashes/sec for PSMF and 765K/sec for PSMA.</p><p>Since PSM maintains a hash table with 2 t entries and a single copy of H, it storage requirement for t = d is:</p><formula xml:id="formula_32">Ω ′ = (2 t + 2 d ) b 8 = 2nb 8 ,<label>(22)</label></formula><p>which is equivalent to 2 permuted tables in BPHS. For h = 3 and the minimum four tables in BPHS, our approach is at least twice as efficient. Furthermore, for a fixed number of additional tables (i.e., just one), PSM's latency (21) scales as</p><formula xml:id="formula_33">Θ(k(d) log k(d)), where d = log 2 n. Since k(d) is O(d h )</formula><p>, we obtain that its overall complexity for processing all hashes in H is O(n log h (n) log log n). This compares favorably to Θ(n 2 ) of BPHS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Batch PSM</head><p>Given a batch Q of new pages, PSM loads chunks of m = |Q| fingerprints from the file, sets up a hash table for each element of the chunk, and performs lookups for all x ∈ Q. Note that after the first match, PSMF removes x from Q, which prevents its being checked against subsequent chunks. This allows PSMF to become much faster as it progresses through the file.</p><p>In the worst case, PSM's I/O delay is the same as (16), which in our tests is again much smaller than the CPU latency, where the latter can be broken down into three parts -producing the permutations for m hashes, performing m lookups in each of n/m chunks, and finally sorting the hashes in Q before writing them to disk:</p><formula xml:id="formula_34">τ ′ CP U = mδV log 2 k + nτ ′ (log m) + mδS log 2 m, (23)</formula><p>where δS is the mean latency of moving hashes while sorting. As important result of this analysis is how batch size m affects rate r in (18). For a fixed number of tables T , BPHS's τCP U scales as Θ(m) and keeps r virtually unchanged. On the other hand, ignoring the VOSH and sorting overhead, PSM's τ ′ CP U scales as Θ(log h (m) log log m). This increases the overall rate r slightly slower than linear, i.e., as Θ(m/ log h (m) log log m), but nevertheless significantly faster than in BPHS. We re-examine this issue and confirm this result in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">EXPERIMENTS</head><p>We next describe our dataset D, examine whether simhash indeed approximates cosine similarity on D, select the optimal h, and evaluate PSM in comparison to BPHS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Dataset</head><p>All our experiments involve a set of 100M web pages crawled by IRLbot <ref type="bibr" target="#b14">[15]</ref> in April 2008. We process the collection by removing pages that have size less than 5 KB, contain no URLs, have an exact duplicate (identified using a standard hash function), or consist of non-English words, all of which shrinks D to a total of 70M pages.</p><p>We parse each remaining page, removing stop-words and stemming all text outside of HTML tags. We then create feature vectors with weights ti(u) being the normalize TF-IDF score of each word i on page u. For calculating simhash fingerprints, we use the 64-bit MurmurHash function <ref type="bibr" target="#b0">[1]</ref>. dataset with 8B pages, we aim to verify that the same parameters work well for our D, but using the similarity measure of the feature-vector space and at a much larger scale. To our knowledge this has not been done before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Usability of Simhash</head><p>We randomly sample 100M document pairs (out of a total of 4.9 quadrillion) and place them into set Υ. We then define set Ncos to contain all pairs whose cosine similarity is above threshold θ = 0.9, i.e., Ncos = {(u, v) ∈ Υ : cos(u, v) ≥ θ}. We also define sets N h (for h = 1, 2, . . . , 64) to contain all sampled pairs of documents within Hamming distance h of each other, i.e.,</p><formula xml:id="formula_35">N h = {(x, y) ∈ Υ : H(x, y) ≤ h}.</formula><p>Then, precision at distance h is defined as the fraction of pages in N h that belong to Ncos:</p><formula xml:id="formula_36">P (h) = |N h ∩ Ncos| |N h | (24)</formula><p>and recall at distance h is the fraction of pages in Ncos that belong to N h : <ref type="figure" target="#fig_8">Figure 6</ref>(a) plots both metrics as a function of h. Observe that small h produces a high rate of false-negatives, but keeps the false positive rate low. Large values of h offer the opposite condition -many false positives, but few false negatives. Taking the point where both curves intersect, we arrive at h = 3 as a sensible balance for this tradeoff. </p><formula xml:id="formula_37">R(h) = |N h ∩ Ncos| |Ncos| (25)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Implementation Details</head><p>We implemented both PSM and BPHS <ref type="bibr" target="#b15">[16]</ref> in Visual Studio C++, using similar optimizations and running them on the same hardware, which consisted of a desktop machine with the AMD Phenom II X6 CPU (2.8 GHz), 16 MB of RAM, 5 TB of disk space, and Windows Server 2008 R2.</p><p>Since PSM is an approximation to an exact simhash search, its relative recall R ′ is computed against the matches found by BPHS. Thus, PSM's total recall against cosine similarity is R(h)R ′ , where R(h) is given in (25) and plotted in <ref type="figure" target="#fig_8">Figure  6</ref>(a). It should be noted that PSM's relative precision is 100% against BPHS and is not a factor in our comparison.</p><p>Relative recall R ′ for our experiments is defined slightly differently for PSMA and PSMF . In the former case, we take the number of matching pairs found by PSMA and normalize  it by that found by BPHSA. In the latter case, we record the number of simhashes for which PSMF found at least one similar pair and divide it by the same number in BPHSF .</p><p>All our experiments use k(t) such that R ′ achieves recall 95%, unless otherwise specified. As discussed earlier, we divide the full dataset of 70M fingerprints by random sampling into two parts: 10M hashes are used as queries and the remaining 60M are used as the existing collection H.</p><p>We experimented with replacing the binary search in BPHS with extrapolation search suggested in <ref type="bibr" target="#b15">[16]</ref>. For d = 26, this reduced the number of RAM lookups from 26 to 11, but the overall runtime of the algorithm increased due to the larger number of multiplications/divisions needed to compute each jump. We thus do not include it in our comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Online Mode</head><p>In our experiments, we use two most space-efficient BPHS designs suggested in <ref type="bibr" target="#b15">[16]</ref>, i.e., T = 4 and T = 10 tables. To make comparison easier to follow, we convert our RAM overhead to the same notation by dividing Ω ′ by the size of H, i.e., nb/8. Thus, PSM's number of tables becomes:</p><formula xml:id="formula_38">T ′ = 1 + 1 2 d−t .<label>(26)</label></formula><p>By varying t, we can achieve any overhead T ′ ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. We study how this selection impacts the result using the total run-time of each algorithm over the entire set of 10M queries. <ref type="table" target="#tab_5">Table 2</ref> shows our results. The top half of the table focuses on finding all matches, where PSMA is 3.4 − 5 times faster than BPHSA, while reducing its RAM consumption by a factor of 2-10 depending on the choice of t. In the lower half, PSMF is even better (i.e., 3.7 − 8.7 times faster than BPHSF ) with the same savings in RAM.</p><p>Another interesting observation in the table is that PSM is relatively insensitive to RAM usage. After dropping T ′ from 2 to 1.06, PSMA loses only 15% in speed and PSMF only about 9%. Focusing on the ratio of speed (in thousands of queries/sec) to the number of tables used, PSMA peaks at 617 with T ′ = 1.125 and PSMF at 1367 with T ′ = 1.06. The best numbers from BPHS are 38 and 46, respectively.</p><p>We next focus on the scalability of each method. In the modeling section, we showed that with a fixed number of tables, BPHS's lookup delay for each query scaled linearly with set size n, which is equivalent to an exponential increase when plotted against d = log 2 n. At the same time, we showed that PSM's CPU overhead could be crudely upperbounded by O(d h ), which at least in theory should be significantly better. <ref type="figure">Figure 7</ref> confirms this result in our implementation. Specifically, in part (a) of the figure, we fix T = 4 tables for <ref type="bibr" target="#b15">[16]</ref> and keep T ′ = 2 in our method. Notice the aggressive increase in delay for both versions of BPHS and an almost linear increase for PSM. In part (b), the BPHS design calls for g = 2 and G = 5, which means that the exponential term in (14) does not become active until d exceeds gb/(b+h) = 25.6. Thus, most of the visible increase in <ref type="figure">Figure 7</ref>(b) is due to the binary search; however, once the dataset becomes substantially larger, these curves will become exponential in d. Note that in part (a) of the figure, the situation was dramatically different because g was 1 and gb/(b + h) was 16.</p><p>We finish this section by keeping the number of attempted combinations k(t) constant and examining how recall R ′ changes with dataset size d. This demonstrates the decay rate of recall as a function of |H|, which might be interesting to applications that intend to keep per-query CPU overhead constant as n → ∞. In these experiments, we scale the number of header bits as t = d and plot the result in <ref type="figure">Figure  8</ref>. First, notice that recall of PSMF decreases slightly slower than that of PSMA. Second, observe that with just k = 5 combinations of bit-flips, the former method achieves 90% recall for all datasets up to 60M pages. The latter technique can maintain 93% recall with just k = 10 flips, which implies that by lowering the target R ′ to 90%, our method can become 2.5 − 3 times faster than already demonstrated.</p><p>In real-time systems with hard memory and performance constraints that are ready to sacrifice a small percentage of recall, these results show that PSM offers a significantly faster and more space-efficient solution than currently available in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Batch Mode</head><p>In these experiments, the existing dataset of 60M fingerprints is read from disk and matched against a query batch with m = |Q| = 10M fingerprints stored in memory. Since in our system τ disk is at least 20 times smaller than τCP U , the overall performance is dominated by the throughput of the studied algorithms. While we are not concerned with RAM as much as before, simple calculations show that PSM still uses less RAM than BPHS. <ref type="figure" target="#fig_11">Figure 9</ref>(a) shows the effect of batch size m on the processing speed. As discussed earlier, BPHSA and BPHSF are the same method in batch mode, which we plot as a single curve in the figure. We use the 4-table design for BPHS since it proved faster than the 10-table version for dataset sizes below 2 25 (see <ref type="figure">Figure 7)</ref>. We now come back to the prediction in the modeling section that BPHS's speed should saturate and remain constant with m, while that of PSM should increase sublinearly, but no worse than m/ log h 2 (m) log log m. <ref type="figure" target="#fig_11">Figure 9</ref>(a) confirms both findings, showing that the processing rate of BPHS stabilizes at 55K/sec, while our technique scales from 99K/sec to 727K/sec for PSMF and from 70K/sec to 373K/sec for PSMA. At the final batch size (i.e., m = 10M), PSM outperforms BPHS by a factor of 7.7 − 14, which is expected to continue increasing as m → ∞.</p><p>It should be noted that even for the largest batch m = 10M, both methods in <ref type="figure" target="#fig_11">Figure 9</ref>(a) are still 50-70% slower than in online mode. For BPHS, this can be explained by the much larger number of binary searches it performs compared to the online version. For PSM, the model predicts that splitting the dataset into small chunks reduces performance since each new hash must be looked up in n/m hash tables.</p><p>We finish the paper by examining how both methods behave when n increases, but m stays fixed. The model shows that the run-time of both techniques should be linear in n, which means that their throughput should be Θ(1/n). <ref type="figure" target="#fig_11">Fig- ure 9</ref>(b) confirms this fact and shows that the ratio between the PSM and BPHS curves remains constant at approximately 7 for PSMA and 14 for PSMF . Combining the various observations, we can conclude that as n → ∞, batchmode PSM will be able to use larger m and its performance gains over fixed-table BPHS will grow even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we presented a novel way of utilizing simhash to find near-duplicates in large collections of documents. We showed that by sacrificing a small percentage of recall the proposed approach consistently outperformed <ref type="bibr" target="#b15">[16]</ref> in terms of query speed and space consumption, which it was able to simultaneously lower by a factor that ranged from 2 to 14 in various configurations.</p><p>Future work involves analysis of feature-selection techniques for better clustering, improvement of simhash recall against cosine similarity, and further overhead reduction in our bit-flipping algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CIKM' 11 ,</head><label>11</label><figDesc>October 24-28, 2011, Glasgow, Scotland, UK. Copyright 2011 ACM 978-1-4503-0717-8/11/10 ...$10.00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histograms of simhash weights (bin size 1/150, collection of 70M documents).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 : Top five levels of three volatility heaps.</head><label>2</label><figDesc>Figure 2: Top five levels of three volatility heaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 : Bit flips needed in VOSH compared to random ( 64 -bit hashes).</head><label>364</label><figDesc>Figure 3: Bit flips needed in VOSH compared to random (64-bit hashes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 : Lookup in PSM and BPHS.</head><label>4</label><figDesc>Figure 4: Lookup in PSM and BPHS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Assuming 8 GB RAM and the absolute minimum number of tables T = 4 for h = 3 (i.e., G = 4, g = 1, γ = 16 bits), BPHS admits datasets up to n = 2 28 with an expected length of linear scans 2 28−16 = 2 12 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Optimal selection of t in PSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 : Comparison of simhash against cosine sim- ilarity at different Hamming distances.</head><label>6</label><figDesc>Figure 6: Comparison of simhash against cosine similarity at different Hamming distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 (</head><label>6</label><figDesc>Figure 6(a) plots both metrics as a function of h. Observe that small h produces a high rate of false-negatives, but keeps the false positive rate low. Large values of h offer the opposite condition -many false positives, but few false negatives. Taking the point where both curves intersect, we arrive at h = 3 as a sensible balance for this tradeoff. Figure 6(b) shows the expected cosine E[cos(u, v)] between document pairs at different Hamming distances. The correlation between the two is very clear, with H(x, y) ∈ [1, 3] producing E[cos(u, v)] that ranges from 0.95 down to 0.89, which confirms the applicability of simhash as a substitute for s(u, v) on this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 : Scalability with dataset size (online mode, 10M queries, 60M existing hashes)Figure 8 : Relative recall in PSM with a fixed num- ber of bit flips (online mode, 10M queries, 60M ex- isting hashes).</head><label>78</label><figDesc>Figure 7: Scalability with dataset size (online mode, 10M queries, 60M existing hashes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Throughput in batch mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>n Cosine s(u, v) Hamming H(x, y) Time RAM</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>= 1 to b do ◃ Revisit all bits</head><label></label><figDesc></figDesc><table>feature weight 

7: 

else 

8: 

W [j] ← W [j] − w i 
◃ Subtract feature weight 

9: 

end if 

10: 

end for 

11: end for 
12: for j 13: 

if W [j] ≥ 0 then 

14: 

B[j] ← 1 
◃ Positive weight, set bit to 1 

15: 

else 

16: 

B[j] ← 0 
◃ Negative weight, set bit to 0 

17: 

end if 

18: end for 
19: return array B[1 . . . b] 

◃ simhash 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>= h − 1 downto 1 do ◃ Attempt right child 7:</head><label></label><figDesc></figDesc><table>Left child is empty 

5: end if 
6: for j </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of PSM to BPHS (online 
mode, 10M queries, 60M existing hashes). 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">MurmurHash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Appleby</surname></persName>
		</author>
		<ptr target="http://sites.google.com/site/murmurhash/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning &apos;Forgiving&apos; Hash Functions: Algorithms and Large Scale Tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2007-01" />
			<biblScope unit="page" from="2663" to="2669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LSH Forest: SelfTuning Indexes for Similarity Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Condie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ganesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2005-05" />
			<biblScope unit="page" from="651" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">K-D Trees For Semi-Dynamic Point Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Computational Geometry (SCG)</title>
		<meeting>ACM Symposium on Computational Geometry (SCG)</meeting>
		<imprint>
			<date type="published" when="1990-06" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying and Filtering Near-Duplicate Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CPM</title>
		<meeting>CPM</meeting>
		<imprint>
			<date type="published" when="2000-06" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Min-Wise Independent Permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM STOC</title>
		<meeting>ACM STOC</meeting>
		<imprint>
			<date type="published" when="1998-05" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntactic Clustering of the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8-13</biblScope>
			<biblScope unit="page" from="1157" to="1166" />
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity Estimation Techniques from Rounding Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM STOC</title>
		<meeting>ACM STOC</meeting>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collection Statistics for Fast Duplicate Document Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mccabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding Interesting Associations without Support Pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDE</title>
		<meeting>IEEE ICDE</meeting>
		<imprint>
			<date type="published" when="2000-02" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental Clustering for Mining in a Data Warehousing Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="1998-08" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">R-Trees: A Dynamic Index Structure for Spatial Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guttman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="1984-06" />
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM STOC</title>
		<meeting>ACM STOC</meeting>
		<imprint>
			<date type="published" when="1998-05" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">IRLbot: Scaling to 6 Billion Pages and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Loguinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2008-04" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting Near Duplicates for Web Crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2007-05" />
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Perceptrons. MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic Hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Strategies for Retrieving Plagiarized Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Zu Eissen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2007-07" />
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Technique for Counting Ones in a Binary Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wegner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">322</biblScope>
			<date type="published" when="1960-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
