<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PERFORMANCE MEASURES FOR RANKING AND SELECTION PROCEDURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Waeber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<postCode>14850</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Frazier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<postCode>14850</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Henderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research and Information Engineering</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<postCode>14850</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PERFORMANCE MEASURES FOR RANKING AND SELECTION PROCEDURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To efficiently compare Ranking and Selection procedures, we present a three-layer performance evaluation process. The two most popular formulations, namely the Bayes and Indifference Zone formulations, have a common representation analogous to convex risk measures used in quantitative risk management. We study decision makers&apos; acceptance sets via an axiomatic approach and introduce a new performance measure using computational cost. 1 MOTIVATION In ranking and selection (R&amp;S), we use stochastic simulation to determine which of several simulated systems is the best. This need arises, for example, if we have a simulation of a call center that we would like to use to choose among several possible staffing policies. We would first decide how many samples of the customer waiting time to take using each staffing policy, and would then use these samples to decide which system has the most desirable waiting time distribution. The crucial decision in R&amp;S is choosing how many samples to take from each system. If we make this decision intelligently, we can find a good system more reliably and in less time than can be found using naive sampling strategies. An enormous number of R&amp;S procedures have been introduced in the literature, and two opposing theoretical frameworks have been introduced for evaluating the performance of these procedures. The first framework is the Indifference Zone (IZ) framework, which considers the worst-case performance of an R&amp;S policy over a specific set of configurations. The second framework is the Bayesian framework, which considers average performance under a prior probability distribution. Each framework has advantages and disadvantages. The IZ framework provides a statistical guarantee on performance that provides peace of mind, but is often extremely conservative in the number of samples that it requires an R&amp;S procedure to take. The Bayesian framework, in contrast, results in R&amp;S procedures that perform well in many cases, but might perform badly in certain configurations. The two frameworks seem to be at odds with each other, presenting radically different approaches to the R&amp;S problem. In this paper, we introduce a new and more general theoretical framework that includes both the IZ and Bayesian formulations of the R&amp;S problem, as well as a continuum of other frameworks that strike a balance between the conservatism of IZ and the Bayesian emphasis on average-case performance. This allows choosing, in a cohesive way, among the enormous number of available R&amp;S procedures. Surprisingly, this theoretical framework is structurally similar to that of convex risk measures studied in mathematical finance. We begin by investigating how the performance of selection policies can be studied using a three layer performance evaluation process. We then, inspired by convex risk measures, introduce a new performance measure based solely on the number of simulation iterations required for the policy to become acceptable to a decision maker. A decision maker determines what performance standards an R&amp;S policy should satisfy, and the set of policies that meet these standards then comprise her acceptance set. When performance can be controlled by the total number of simulation iterations budgeted, the smallest simulation budget for which a policy&apos;s performance falls within the acceptance set defines a new performance measure for R&amp;S policies. The existing literature on R&amp;S policies is extensive. R&amp;S problems have been heavily studied in sequential statistical testing and experiment design, see Santner and Tamhane (1984) and Bechhofer, Santner, and Goldsman (1995) for overviews. In simulation literature, selection procedures are popular tools for simulation optimization</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">MOTIVATION</head><p>In ranking and selection (R&amp;S), we use stochastic simulation to determine which of several simulated systems is the best. This need arises, for example, if we have a simulation of a call center that we would like to use to choose among several possible staffing policies. We would first decide how many samples of the customer waiting time to take using each staffing policy, and would then use these samples to decide which system has the most desirable waiting time distribution. The crucial decision in R&amp;S is choosing how many samples to take from each system. If we make this decision intelligently, we can find a good system more reliably and in less time than can be found using naive sampling strategies.</p><p>An enormous number of R&amp;S procedures have been introduced in the literature, and two opposing theoretical frameworks have been introduced for evaluating the performance of these procedures. The first framework is the Indifference Zone (IZ) framework, which considers the worst-case performance of an R&amp;S policy over a specific set of configurations. The second framework is the Bayesian framework, which considers average performance under a prior probability distribution. Each framework has advantages and disadvantages. The IZ framework provides a statistical guarantee on performance that provides peace of mind, but is often extremely conservative in the number of samples that it requires an R&amp;S procedure to take. The Bayesian framework, in contrast, results in R&amp;S procedures that perform well in many cases, but might perform badly in certain configurations. The two frameworks seem to be at odds with each other, presenting radically different approaches to the R&amp;S problem.</p><p>In this paper, we introduce a new and more general theoretical framework that includes both the IZ and Bayesian formulations of the R&amp;S problem, as well as a continuum of other frameworks that strike a balance between the conservatism of IZ and the Bayesian emphasis on average-case performance. This allows choosing, in a cohesive way, among the enormous number of available R&amp;S procedures. Surprisingly, this theoretical framework is structurally similar to that of convex risk measures studied in mathematical finance.</p><p>We begin by investigating how the performance of selection policies can be studied using a three layer performance evaluation process. We then, inspired by convex risk measures, introduce a new performance measure based solely on the number of simulation iterations required for the policy to become acceptable to a decision maker. A decision maker determines what performance standards an R&amp;S policy should satisfy, and the set of policies that meet these standards then comprise her acceptance set. When performance can be controlled by the total number of simulation iterations budgeted, the smallest simulation budget for which a policy's performance falls within the acceptance set defines a new performance measure for R&amp;S policies.</p><p>The existing literature on R&amp;S policies is extensive. R&amp;S problems have been heavily studied in sequential statistical testing and experiment design, see <ref type="bibr" target="#b15">Santner and Tamhane (1984)</ref> and <ref type="bibr" target="#b0">Bechhofer, Santner, and Goldsman (1995)</ref> for overviews. In simulation literature, selection procedures are popular tools for simulation optimization Waeber, <ref type="bibr">Frazier and Henderson problems (Boesel, Nelson, and Kim 2003)</ref> and discrete-event simulation problems <ref type="bibr" target="#b16">(Swisher, Jacobson, and Yücesan 2003)</ref>. <ref type="bibr" target="#b12">Malone (2004)</ref> provides an overview for the Bernoulli R&amp;S problem. For the normal R&amp;S problem, there exist mainly two research sreams: one is the IZ approach <ref type="bibr" target="#b11">(Kim and Nelson 2006</ref>), the other uses Bayesian methods <ref type="bibr" target="#b7">(Chick 2006</ref>). This paper focuses on the underlying performance evaluation for selection procedures and uses similar methods as in statistical decision theory, see <ref type="bibr" target="#b1">Berger (1985)</ref> and <ref type="bibr" target="#b9">DeGroot (1970)</ref>. By far the most popular performance measure for selection procedures is the probability of correct selection (PCS), resp. its counterpart the probability of incorrect selection (PICS). <ref type="bibr" target="#b6">Chick (1997)</ref>, <ref type="bibr" target="#b8">Chick and Inoue (2001)</ref> and Chen, He, Fu, and <ref type="bibr" target="#b5">Lee (2008)</ref> also consider other performance measures. <ref type="bibr" target="#b3">Branke, Chick, and Schmidt (2007)</ref> provided a comparison of existing R&amp;S policies for the normal R&amp;S problem based on an extensive set of examples. Our idea is that performance measures used in quantitative finance, namely convex risk measures, could prove effective for comparison of selection procedures, see <ref type="bibr" target="#b10">Föllmer and</ref><ref type="bibr" target="#b10">Schied (2004) and</ref><ref type="bibr" target="#b13">McNeil, Frey, and</ref><ref type="bibr" target="#b13">Embrechts (2005)</ref> for an introduction to risk measures used in finance, and <ref type="bibr" target="#b14">Rockafellar (2007)</ref> for use of convex risk measures in optimization.</p><p>The outline of this paper is as follows. Section 2 introduces the R&amp;S problem and outlines the three-layer performance evaluation process. Section 3 shows the unified representation for existing performance measures analogous to convex risk measures. Section 4 introduces acceptance sets and the new performance measure based on computational cost. In Section 5 the new performance measure is used to evaluate three popular R&amp;S policies for independent normal systems with unknown means and variances. Section 6 presents conclusions and future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PERFORMANCE EVALUATION FOR RANKING AND SELECTION PROCEDURES</head><p>In this section, we introduce the R&amp;S problem and necessary notation. We then outline the performance evaluation process for R&amp;S policies and show how the Bayes and the IZ formulation fit into a decision theoretic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mathematical Framework</head><p>We consider a setting with k independent systems. Each system i corresponds to a simulation experiment producing iid random outcomes Y i . The distribution of these outcomes is described by a vector ψ i , which is also called the configuration of system i. From each configuration a corresponding performance parameter θ i ∈ R may be computed. In general, θ i is unknown, and all or part of ψ i is unknown as well. The performance parameters across all systems are summarized in a vector (θ 1 , . . . , θ k ) = θ ∈ Θ ⊂ R k , where Θ denotes the corresponding parameter space, and the underlying configurations are summarized (ψ 1 , . . . , ψ k ) = ψ ∈ Ψ. We are interested in identifying the system with the highest performance parameter θ i , i.e., we want to select system i * where θ i * = max i∈{1,...,k} θ i . For example, if sampling distributions are normal, then we could take ψ i = (µ i , σ 2 i ) to contain the mean and the variance of samples from system i, and θ i = µ i to be the mean. Although θ i is often taken to be the mean of the sampling distribution in the R&amp;S literature, our framework allows other choices as well. For example, in finance, one is often interested in a portfolio composition with the highest Sharpe ratio, which can be estimated by θ i = µ i /σ i . Because θ depends on ψ, we sometimes write θ (ψ) to emphasize this dependence. More often, however, we simply write θ when the context is clear.</p><p>We denote by N the total number of measurements budgeted across all systems. For the moment, we assume that the simulation budget N is given at the beginning of the experiment. In Section 4, we relax this assumption and use N to control the performance of the policy. The goal of a selection procedure is to allocate these N measurements to the k systems to best support the ultimate selection decision.</p><p>An R&amp;S policy π consists of an allocation rule, which determines which systems to simulate, and a selection rule, which determines which system to select based on the simulation results. We allow adaptive allocation rules. Formally, for n = 1, . . . , N, we define x n ∈ {1, . . . , k} to be the system simulated at time n and y n (x n ) ∈ R the corresponding outcome. The decision x n depends upon the previously observed outcomes, i.e., x n (x 1:n−1 , y 1:n−1 ), although we suppress this in the notation. Define i π ∈ {1, . . . , k} to be the selection made once all N simulations have been carried out. This decision is a function of all the observed outcomes y 1:N , i.e., i π (x 1:N , y 1:N ). A policy π can be written as π = (x 1:N , i π ), where we use the notation x 1:n := (x 1 , . . . , x n ). Finally, let Π be the set of all possible policies π.</p><p>The essential question in R&amp;S is this: which policy π should we use? The performance evaluation analysis in the next section discusses the issues that arise in answering this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Three-Layer Performance Evaluation</head><p>When we use an R&amp;S procedure, we hope that it selects the best alternative. Inevitably, however, it will sometimes fail to do so. In this section, we discuss ways to measure the risk associated with this failure to select the best. Together, there are three layers of analysis that determine this risk: the loss of the decision, the configuration-specific risk, and the overall risk. In this section, we discuss and give examples of these layers of risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Loss of the Decision (Outcome-Specific Risk)</head><p>For a known θ , we can assign a quantity L(i, θ ) ∈ R to decision i, which reflects the loss (cost) associated with choosing system i given the true parameter θ . Examples of such loss functions L include:</p><p>1. L(i, θ ) := {θ i 񮽙 =θ i * } . This is the 0-1 loss function, used throughout the R&amp;S literature. It penalizes all incorrect selections equally. The corresponding expected loss is the probability of incorrect selection (PICS).</p><formula xml:id="formula_0">2. L(i, θ ) := {θ i ≤(θ i * −δ )} , for δ &gt; 0.</formula><p>This loss function only penalizes incorrect selection when the selected alternative is more than δ worse than the best. This reflects the idea that we may be indifferent to differences in performance of δ or less. This notion is similar to the IZ approach in R&amp;S. 3. L(i, θ ) := θ i * − θ i . This is the linear loss function used in <ref type="bibr" target="#b8">Chick and Inoue (2001)</ref>, and is also referred to as regret. It is equal to the difference in performance between the best system and the selected system. 4. L(i, θ ) := c − θ i , where c ∈ R is a constant. This can be used when our loss depends not on the difference in performance between the selected and the best, but between the selected and some known threshold. In some cases, e.g., the Bayesian framework using expected loss, this induces the same preference order as the linear loss function. In other cases, it does not.</p><formula xml:id="formula_1">5. L(i, θ ) := f (θ i * − θ i ),</formula><p>where f is a convex and increasing function on R. This generalizes linear loss, with the function f modeling the risk aversion of the decision-maker. Possible choices for f include</p><formula xml:id="formula_2">f (x) = {x≥0} x p for some constant p ≥ 1 or f (x) = exp(x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Configuration-Specific Risk</head><p>The loss L(i, θ ) quantifies the risk associated with a single outcome of the selection decision i π . However, this decision i π (x 1:N , y 1:N ) made by an R&amp;S policy π depends on random simulation samples, and so L(i π , θ ) is a random variable. Thus, an additional source of risk is this randomness induced by stochastic simulation, given a single fixed configuration ψ. We call this configuration-specific risk, and refer to it as R(π, ψ).</p><p>To quantify the configuration-specific risk, one possibility is to use the expected loss,</p><formula xml:id="formula_3">R(π, ψ) = E[L(i π , θ (ψ))]</formula><p>. This is a reasonable measure when the policy is used repeatedly for R&amp;S in a large number of similar situations, or by a decision-maker with little aversion to risk. However, when selection errors are costly, or if a policy is only to be used once or a small number of times (for example in medical trials), more conservative functionals of the loss distribution, such as quantiles or expected loss above a given quantile, should be considered <ref type="bibr" target="#b6">(Chick 1997)</ref>. Generally, we write the configuration-specific risk as</p><formula xml:id="formula_4">R(π, ψ) := r(L(i π , θ (ψ))),<label>(1)</label></formula><p>where r is a functional of the distribution of the random variable L(i π , θ (ψ)), given a fixed ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Overall Risk</head><p>The configuration-specific risk R(π, ψ) defined in (1) is still dependent on the unknown underlying configuration ψ ∈ Ψ. There is also risk associated with the fact that this configuration is unknown. A risk-averse decision-maker would prefer a policy π that has moderately low R(π, ψ) across all possible configurations ψ. In contrast, a decision-maker indifferent to risk might prefer a policy π with extremely low R(π, ψ) in most configurations, but high R(π, ψ) in a certain few problematic configurations.</p><p>To quantify this risk, we use a functional ρ on the function ψ 񮽙 → R(π, ψ). With a slight abuse of notation, we write ρ(π) = ρ(R(π, ·)) ∈ R. This performance mapping ρ depends only on the policy π, and not on the configuration ψ. It can then be used to compare different policies and induces a preference order on Π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>Depending on the risk tolerance of the decision-maker there are different ways to define ρ. Three popular choices are:</p><p>1. Worst-Case Performance:</p><formula xml:id="formula_5">ρ WC (π) := sup ψ∈Ψ R(π, ψ),</formula><p>This performance measure prefers policies which perform well in the worst underlying configuration of the experiment. This is the most conservative performance measure. 2. Indifference Zone:</p><formula xml:id="formula_6">ρ IZ (π) := sup ψ / ∈IZ R(π, ψ),<label>(2)</label></formula><p>This formulation is closely related to the worst-case performance measure. A set IZ ⊂ Ψ, the so-called Indifference zone, is defined as a subset of the set of configurations. The IZ performance measure prefers policies that perform well in the worst underlying configuration outside the Indifference zone. The intuition behind this approach is that some configurations of the experiment have performance measures so close to each other that it is not worth detecting the difference. This formulation is still quite conservative, but not as restrictive as the worst-case analysis. 3. Bayes Risk: The Bayesian approach incorporates a prior distribution P 0 on the configuration ψ ∈ Ψ. This prior reflects a risk weighting on ψ and does not necessarily correspond to the prior belief regarding ψ or θ .</p><p>In the Bayesian approach, ψ is considered to be a random vector so R(π, ψ) is a random variable. Different functionals of the distribution of R(π, ψ) with respect to P 0 can be used to quantify the performance of a policy. The most popular approach is to use the expected value, i.e.,</p><formula xml:id="formula_7">ρ Bayes (π) := E P 0 [R(π, ψ)].</formula><p>In contrast to the worst-case and the indifference zone formulation, the Bayes formulation focuses on an average performance rather than a conservative worst-case performance. When the risk weighting P 0 is rather diffuse, such an average case analysis might be too optimistic. One could introduce more risk aversion by using a different loss function L, a more conservative risk weighting P 0 or a different functional of R(π, ψ) with respect to P 0 , e.g. quantiles or expected shortfall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Decision-Theoretic Formulation for R&amp;S Procedures</head><p>We briefly summarize the process needed to induce a preference order on the set of policies Π. To compare different policies, three quantities must be defined:</p><formula xml:id="formula_8">1. The loss of a decision L(i, θ (ψ)); 2. The configuration-specific risk R(π, ψ); 3. The overall risk ρ(π).</formula><p>Figure 1 provides an overview of these three concepts and how they interact. There is no single best way to define these quantities. Rather, they should be chosen carefully according to the decision-maker's risk tolerance for the particular problem instance addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REPRESENTATION THEOREM FOR PERFORMANCE MEASURES</head><p>In the previous section, we described the process that underlies the performance evaluation of selection procedures. We now focus on the connections between the three popular performance measures defined in Section 2.2.3. These performance measures have a common representation analogous to convex risk measures studied in finance. This representation then suggests other performance measures in-between the measures given in Section 2.2.3 that may potentially overcome the drawbacks of these popular performance measures.</p><p>We first define convex risk measures as they are used in finance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>Figure 1: Quantities used to define a preference order on the set of policies. </p><formula xml:id="formula_9">Definition 1. A mapping ρ : L 2 → R</formula><formula xml:id="formula_10">1. Monotonicity: If X ≤ Y , then ρ(X) ≥ ρ(Y ). 2. Cash Invariance: If m ∈ R, then ρ(X + m) = ρ(X) − m. 3. Convexity: ρ(λ X + (1 − λ )Y ) ≤ λ ρ(X) + (1 − λ )ρ(Y ), for 0 ≤ λ ≤ 1.</formula><p>Here the restriction to the L 2 space is sufficient but not necessary. At a first glance, this definition of convex risk measures does not seem related to our problem setting. However, a strong connection can be identified through the following representation property of convex risk measures. </p><formula xml:id="formula_11">Q∈Q (E Q [−X] − α(Q)),<label>(3)</label></formula><p>defines a convex risk measure on L 2 .</p><p>With further technical assumptions the reverse direction also holds, i.e., every convex risk measure has a representation of the form <ref type="bibr">(3)</ref>. See Föllmer and Schied (2004) for more details and a proof of Theorem 1. We now state a similar representation theorem for the performance measures used for R&amp;S procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. The Worst-Case, Indifference zone and Bayes formulations for Ranking and Selection procedures defined in Section 2.2.3 can each be represented as</head><formula xml:id="formula_12">ρ(π) = sup Q∈Q (E Q [R(π, ψ)] − α(Q)) ,<label>(4)</label></formula><p>where Q is some appropriate set of probability measures on Ψ and its associated Borel sigma-field, and α : Q 񮽙 → R ∪ {∞} is some penalty function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>The representation <ref type="formula" target="#formula_12">(4)</ref> is not unique. We show two ways in which the performance measures in Section 2.2.3 can be represented according to <ref type="bibr">(4)</ref>. In the first method, we take Q to be the set of all probability measures on Ψ and adjust the penalty function α accordingly. An unconstrained optimization problem must be solved to determine ρ(π). In the second method, which may be more intuitive, we restrict the set of probability measures Q considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>This approach has the advantage that the penalty function α is equal to zero, but it requires solving a constrained optimization problem.</p><p>We first show the theorem using the first method. Let Q be the set of all probablity measures on Ψ.</p><p>• If the penalty function α is We now show the theorem using the second method. Let α ≡ 0. Then</p><formula xml:id="formula_13">α WC (Q) = 񮽙 0, if</formula><p>• the set Q WC = {all Dirac point measures for ψ ∈ Ψ} yields ρ(π) = ρ WC (π);</p><p>• the set Q IZ = {all Dirac point measures for ψ ∈ Ψ\IZ} yields ρ(π) = ρ IZ (π);</p><p>• the set Q Bayes = {P 0 } yields ρ(π) = ρ Bayes (π).</p><p>Theorem 2 suggests that the class of convex risk measures is helpful in analyzing the performance of R&amp;S policies. For example, if a decision-maker is interested in analyzing the performance of a policy under Bayes and IZ formulations she could consider ρ(π) with α ≡ 0 and Q = Q IZ ∪ Q Bayes . Another decision-maker might be interested only in the performance of a policy under the Bayes formulation, but be unsure about how robust the performance is with respect to the risk weighting P 0 . She could then use Q C = {P : P = (1 − ε)P 0 + εC,C ∈ C }, for 0 &lt; ε &lt; 1 and a class of possible 'contaminations' C . Such a class of 'contaminated' priors is studied in Bayesian robustness theory, see <ref type="bibr" target="#b1">Berger (1985)</ref>. A third decision-maker might put emphasis on average performance, but also be somewhat concerned about worst-case performance. He might take Q = Q WC ∪ Q Bayes , α(Q) = 0 for Q ∈ Q Bayes , and α(Q) equal to a large strictly positive constant c for Q ∈ Q WC . With this large value of the penalty function on Q WC , the performance measure ρ(π) would be equal to the Bayes performance unless the worst-case performance was extremely bad, when ρ(π) would equal the worst-case performance minus c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACCEPTANCE SETS AND A NEW PERFORMANCE MEASURE</head><p>In the previous sections we described how, given a fixed simulation budget N, the performance of a policy π can be evaluated via a performance measure ρ. Another important aspect of R&amp;S is controlling performance. By relaxing the assumption of a fixed simulation budget we can use the "necessary" number of simulation iterations as a performance control of the policy. To do this, we must first define what it means for a policy to be acceptable. We then quantify the performance of a policy as the smallest simulation budget necessary to make the policy acceptable.</p><p>We use the representation of ρ in Theorem 2 to define an acceptable policy. While the IZ and Bayes formulations are special cases of (4), this representation can be used to define much more general performance measures. Each probability measure Q ∈ Q corresponds to a scenario of the underlying experiment configuration Ψ and Q is the set of all scenarios considered. Examples of Q are Q WC , Q IZ , Q P 0 and Q C as defined in Section 3. For this paper we assume that |Q| = m &lt; ∞. The extension to infinitely many scenarios is mathematically more challenging and left open for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>For a given loss function L, a risk function R(π, ψ), and a number of measurements N, the risk of a policy π N that takes N measurements can be represented as a mapping</p><formula xml:id="formula_14">f π N : Q → R, Q 񮽙 → E Q [R(π N , ψ)].</formula><p>The set of all risks generated by arbitrary combinations of L, R(π, ψ) and N corresponds to the functional space G = { f : Q → R}, which can be identified with R m . The risk of a particular policy π N can then be represented as a risk vector</p><formula xml:id="formula_15">f (π N ) =    f π N (Q 1 ) . . . f π N (Q m )    =    E Q 1 [R(π N , θ )] . . . E Q m [R(π N , θ )]    ∈ R m .</formula><p>To introduce the concept of acceptance sets, we take the viewpoint of a decision-maker. If the decision-maker wants to use an R&amp;S procedure, she should specify an acceptance set A ⊂ R m such that a policy π is acceptable if and only if f (π) ∈ A . The acceptance set represents the decision-maker's risk attitude towards different risk scenarios Q. The set A should be defined in a coherent way, i.e., all reasonable acceptance sets should satisfy certain axioms after being appropriately normalized. We assume that after normalization, f π (Q) ∈ [0, 1] for all Q in Q, where f π (Q) = 0 corresponds to the minimal risk and f π (Q) = 1 to the maximal risk. Further, we assume that the policy that randomly chooses a system has risk 1 − 1/k. Such a normalization is adapted to the PICS formulation. We propose that any coherent acceptance set A should satisfy the following axioms:</p><formula xml:id="formula_16">Axiom 1:</formula><p>0 ∈ A . A policy with no risk under any scenario should always be acceptable. Axiom 2:</p><formula xml:id="formula_17">񮽙 k−1 k · 1, 1 񮽙 m ∩ A = / 0</formula><p>, where 1 represents a vector of ones in R m . Any acceptable policy should be as good as randomly identifying a system as the best. Clearly, this axiom depends on our choice of normalization. Axiom 3:</p><p>For any point q ∈ A , [0, q] m ⊆ A . All policies with risks smaller than some acceptable policy must also be acceptable.</p><p>Note that vector notation is used to define subsets of R m , e.g. for two vectors p, q ∈ R m , we define the set <ref type="bibr">[p, q]</ref> </p><formula xml:id="formula_18">m := [p 1 , q 1 ] × [p 2 , q 2 ] × · · · × [p m , q m ] ⊆ R m .</formula><p>Once a decision-maker has identified the acceptance set according to her risk tolerance, we can check whether or not a given policy π N is acceptable. Now, instead of taking N as a predetermined simulation budget, we can treat N as a control chosen by the decision-maker. This interpretation motivates the new performance measure</p><formula xml:id="formula_19">ρ A (π) := inf {n ∈ N| f (π n ) ∈ A } ,<label>(5)</label></formula><p>which corresponds to the minimum number of simulation iterations required to make the policy acceptable to the decision-maker. The intuition behind this performance measure stems from the fact that the cost of a simulation can be measured as the number of iterations required to achieve a certain confidence level. If an infinite number of simulations is allowed, then any reasonable policy can be satisfactory. Here a reasonable policy refers to an algorithm that converges to the right answer, i.e., f (π N ) → 0 as N → ∞. A performance measure for an R&amp;S policy should depend on the convergence rate of that policy. <ref type="figure">Figure 2</ref> shows conceptually how the risk measure ρ A can be interpreted for two policies π (1) and π <ref type="bibr">(2)</ref> , where the grey area A represents a possible acceptance set. We consider only two scenarios, Q 1 and Q 2 . Assume that the decision-maker wants to compare policy π (1) and π <ref type="bibr">(2)</ref> . As N varies between 0 and ∞, each policy traces out a trajectory in R 2 indicated by the dashed arrows. When the trajectory crosses the boundary of the acceptance set A for the first time, the corresponding number of iterations N is precisely the performance measure ρ A .</p><p>The IZ and Bayesian formulations can be combined effectively in this framework. Consider scenarios Q 1 = LFC and Q 2 = P 0 , where LFC represents the least favorable configuration of the experiment where the supremum in (2) is attained. It is not clear whether such a configuration can always be identified and in many cases the LFC may depend on the policy itself. For illustrative purposes, we can think of the LFC as one single challenging configuration. To be more rigorous one should use Q IZ defined in the proof to Proposition 2. <ref type="figure" target="#fig_2">Figure 3</ref> shows the acceptance set A 1 of a decision-maker using the Bayes formulation, as well as the acceptance set A 2 of a decision-maker applying the IZ formulation. A decision-maker who wants to satisfy both the IZ and the Bayesian formulation would use the intersection of A 1 and A 2 as her acceptance set. The dashed arrows indicate possible trajectories of two R&amp;S policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>Figure 2: Possible acceptance set A and trajectories of two policies π (1) and π <ref type="bibr">(2)</ref> . The trajectories outline the function f (π N ) for N → ∞. Note that the trajectories are not continuous curves as N ∈ N, but we drew them continuously here for simplicity. It is instructive to analyze the connection between the performance measure ρ(π) = sup Q∈Q E Q [R(π, θ )] − α(Q) defined in Theorem 2 and the new performance measure ρ A (π) = inf {n ∈ N| f (π n ) ∈ A } introduced in (5). For simplicity, consider the case where Q encompasses all relevant scenarios so the penalty function in (4) can be ignored, i.e., ρ(π) = sup Q∈Q E Q [R(π, Q)]. An acceptance set can then be defined directly by setting a bound on ρ(π), i.e., ρ(π) ≤ c for some c ∈ R, and this corresponds to a hypercube in R m with side-length c. On the other hand, if an acceptance set A is defined independently of ρ, then ρ(π ρ A ) corresponds to the maximal risk among all scenarios as the policy trajectory enters the acceptance set A . Remark 1. ρ A does not directly induce a preference order on the set of policies. In R&amp;S algorithms, not only the rate of convergence needs to be considered, but also the cost of determining the measurement allocation rule. For example, complex sequential R&amp;S procedures require extensive computation at each allocation step, whereas a simple equal allocation does not require any computation between simulations. If simulation of the systems is relatively inexpensive compared to the computation of the allocation rule, an equal allocation policy is most likely preferred. If it is possible to determine a cost c π for an allocation decision of policy π, then a preference order can be formed via the quantity c π ρ A (π), i.e., a policy π 1 is preferred to a policy π 2 if and only if c</p><formula xml:id="formula_20">π 1 · ρ A (π 1 ) ≤ c π 2 · ρ A (π 2 ).</formula><p>If all the allocation costs are equal, or negligible compared to simulation costs, then the risk measure ρ A itself induces a preference order on the set of policies.</p><p>The cost of simulating each individual system should also be considered. If the systems require different computational budget, different allocations should be considered and policies that spend too much time on expensive systems should be penalized. This aspect has been considered in some policies in <ref type="bibr" target="#b8">Chick and Inoue (2001)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>Remark 2. In order to extend the present framework so that policies with random stopping times can also be compared, the assumption of a fixed simulation budget N needs to be relaxed. This could be done by considering the expected running time. Although it is possible to determine whether or not a policy is acceptable to a decision-maker based on the expected running time, it is not obvious how these policies can be controlled. For policies with fixed simulation budgets, the parameter N provides this important control and can be used as a performance measure in (5). For policies with random runlength, a different control such as a confidence parameters can be used.</p><p>Remark 3. R&amp;S procedures can also be useful for infinitely many systems, especially in continuous simulation optimization problems. In this case, the convergence f (π N ) → 0 as N → ∞ is often not guaranteed. However, our framework can still be used by setting ρ A = ∞ if the trajectory f (π N ) does not reach the acceptance set A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIMULATION EXAMPLE</head><p>In this section, we show via a small simulation example how the performance measure ρ A can be used to induce a preference order on a set of policies for the independent normal R&amp;S problem with unknown means and variances. Since explicit calculations of ρ A can be quite challenging, we rely on estimation procedures.</p><p>We consider k = 10 systems with independent normal output</p><formula xml:id="formula_21">Y i ∼ N(µ i , σ 2 i ), i = 1, . . . , k,</formula><p>where the underlying configuration of the experiment is ψ i = (µ i , σ 2 i ) and we are interested in identifying the system with the highest mean, i.e., θ i = µ i . This problem has been studied extensively in the R&amp;S literature and there exist many competing policies. <ref type="bibr" target="#b3">Branke, Chick, and Schmidt (2007)</ref> compared different policies for this problem. We follow their outline but use the new performance measure introduced in <ref type="formula" target="#formula_19">(5)</ref> to induce a preference order on Π.</p><p>An extensive comparison of existing policies would go beyond the scope of this paper. Instead, we compare three popular policies, namely the equal allocation policy, the optimal computing budget allocation policy (OCBA), and the (0 − 1)(S) expected value of information policy. The equal allocation policy simply allocates the allowed measurements equally among the k systems, OCBA is a policy introduced in Chen (1996) that dynamically allocates simulation replications, thereby essentially screening out bad systems, and (0 − 1)(S) is a Bayesian policy introduced in <ref type="bibr" target="#b8">Chick and Inoue (2001)</ref>. For detailed descriptions of these policies, see <ref type="bibr" target="#b3">Branke, Chick, and Schmidt (2007)</ref>. We explore whether or not our framework can give insights regarding their relative performance.</p><p>First, it is necessary to define the considered set of scenarios Q. For illustrative purposes we only consider two scenarios, the IZ formulation (Q 1 ) and the Bayes formulation (Q 2 ). As mentioned earlier, infinitely many scenarios must be considered in order to properly capture the IZ formulation. Instead we only focus on one challenging configuration of the experiment and refer to it as LFC even though it is not necessarily the least favorable configuration. We define the two scenarios as Q 1 := A point mass at {ψ 1 = (0.4, 1), ψ i = (0, 1) for i = 2, . . . , 10} , and</p><formula xml:id="formula_22">Q 2 := {ψ i = (µ i , σ i ), where µ i (iid) ∼ U(0, 2), σ i (iid) ∼ U(0.2, 1) for i = 1, . . . , 10} .</formula><p>Scenario Q 2 is approximated by 100 independent random replications of ψ. We choose the 0-1 loss function L(i, θ ) := {θ i 񮽙 =θ i * } and the risk mapping R(π, ψ) = E[L(i π , θ (ψ))] which corresponds to the PICS for a single configuration ψ.</p><p>Based on these assumptions, we first analyze the performance under IZ and Bayes formulations. <ref type="figure" target="#fig_3">Figure 4</ref> shows the estimated trajectories of the three policies as N increases from 40 to 530. Throughout we use a batchsize of 10, i.e., the policy is updated after 10 simulation runs. Batching speeds up the simulations since some allocation rules require significant computation and updating the allocation after each single observation would be too time consuming. The dashed line indicates the acceptance set for a decision-maker adopting an IZ approach and the dotted line indicates the acceptance set for a decision-maker adopting a Bayes approach. The confidence level is set to be c = 0.22, i.e., a policy is acceptable if and only if ρ IZ (π) ≤ c resp. ρ Bayes (π) ≤ c. A preference order among these three policies cannot be deduced based on the trajectories alone. <ref type="table">Table 1</ref> gives the estimation results for the performance measure ρ A . <ref type="figure" target="#fig_4">Figure 5</ref> shows the convergence towards the acceptance set for each of the two scenarios.</p><p>Let us now consider a decision-maker who seeks good performance in both scenarios Q 1 and Q 2 . <ref type="figure" target="#fig_5">Figure 6</ref> shows four possible acceptance sets that combine the two scenarios. The set A 1 is most conservative, where the decision-maker requires that both the IZ and the Bayes performances are bounded by a certain confidence level c. The set A 4 is the least conservative, where the decision-maker requires that at least one performance measure <ref type="table">Table 1</ref>: Estimated performance measures ρ A (with standard deviations) for a decision-maker with IZ or Bayes acceptance set. We used bootstrapping techniques to estimate ρ A and its standard deviations based on a sample of 100 trajectories. The standard deviation for the Bayes scenario is larger than that for the LFC. This is due to the fact that the slope at the entry point of the trajectory in the Bayes scenario is much smaller than the slope for the LFC scenario, see <ref type="figure" target="#fig_4">Figure 5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head><p>satisfies a given confidence level c. <ref type="table" target="#tab_1">Table 2</ref> summarizes the estimation results for the performance measure ρ A defined in (5) and the corresponding estimated performance measure ρ = ρ(π ρ A ) defined in (4) from Proposition 2.   <ref type="formula" target="#formula_12">(4)</ref> are also given. We omit standard deviations for ρ because they were small (&lt; 5%). From these results we may conclude that OCBA outperforms the other policies on these scenarios and acceptance sets. Having said that, our primary goal is to indicate how a comprehensive performance analysis might be carried out.</p><formula xml:id="formula_23">A 1 A 2 A 3 A 4 ρ A 1 ρ ρ A 2 ρ ρ A 3 ρ ρ A 4 ρ<label>Equal</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FURTHER RESEARCH</head><p>We presented a performance-analysis process for Ranking and Selection procedures. This process states all assumptions needed to incorporate a preference order on the set of policies. The three most popular approaches have a common representation analogous to convex risk measures used in mathematical finance. Based on the performance evaluation process, we introduced a new performance measure using computational cost and the definition of acceptable policies. This easy-to-interpret performance measure can be used to compare selection procedures for different acceptance sets specified by the decision-maker.</p><p>Extensions to this framework involve the treatment of infinitely many scenarios in the scenario approach and the analysis of policies with random simulation lengths. Analogous to <ref type="bibr" target="#b3">Branke, Chick, and Schmidt (2007)</ref>, a comprehensive comparison of existing R&amp;S policies using different acceptance sets would be helpful in identifying robust selection procedures. This paper provides a framework regarding how such a comparison could be carried out, but we have yet to identify good or optimal policies for specific acceptance sets. The computation required for the illustrative example was nontrivial, which suggests that efficient computation of these performance measures should also be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 .</head><label>1</label><figDesc>Denote by Q the set of all finitely additive probability measures on a measurable space (Ω, F ). Let α : Q → R ∪ {∞} be a functional with inf Q∈Q α(Q) ∈ R. Then the mapping ρ(X) := sup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Q is a Dirac point measure for ψ ∈ Ψ, +∞, otherwise, then ρ is the Worst-Case performance measure, i.e., ρ(π) = ρ WC (π). • If the penalty function α is α IZ (Q) = 񮽙 0, if Q is a Dirac point measure for ψ ∈ Ψ\IZ, +∞, otherwise, then ρ is the IZ performance measure, i.e., ρ(π) = ρ IZ (π). • If the penalty function α is α Bayes (Q) = 񮽙 0, if Q = P 0 ; +∞, otherwise, then ρ is the Bayes performance measure, i.e., ρ(π) = ρ Bayes (π).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Acceptance sets corresponding to Bayes and IZ formulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Trajectories for the Equal, OCBA and VIP policies, along with the IZ and Bayes acceptance sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence in terms of N for each scenario separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Trajectories for Equal, OCBA and VIP policy and different acceptance sets A 1 through A 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>is called a convex risk measure if it satisfies the following conditions for all X,Y ∈ L 2 (L 2 denotes the space of random variables with finite variance).</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Estimated risk measures ρ A (along with standard deviations) for different acceptance sets. We used bootstrapping techniques to estimate ρ A and its standard deviations based on a sample of 100 trajectories. The corresponding risk measures ρ defined in</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported, in part, by National Science Foundation Grant CMMI-0800688.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Waeber, Frazier and Henderson</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Design and analysis of experiments for statistical selection, screening, and multiple comparisons. Wiley series in probability and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bechhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldsman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Ranking and Selection to &quot;Clean Up&quot; after Simulation Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="814" to="825" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selecting a selection procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1916" to="1932" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A lower bound for the correct subset-selection probability and its application to discrete-event system simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on automatic control</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1227" to="1231" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient simulation budget allocation for selecting an optimal subset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">579</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selecting the best system: A decision-theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th conference on Winter simulation</title>
		<meeting>the 29th conference on Winter simulation</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Subjective probability and Bayesian methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbooks in Operations Research and Management Science</title>
		<editor>Simulation, ed. S. G. Henderson and B. L. Nelson</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New two-stage and sequential procedures for selecting the best simulated system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="732" to="743" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optimal Statistical Decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Degroot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stochastic Finance, An Introduction in Discrete Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Föllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schied</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Walter de Gruyter &amp; Co</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selecting the best system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbooks in Operations Research and Management Science</title>
		<editor>Simulation, ed. S. G. Henderson and B. L. Nelson</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ranking and Selection Procedures for Bernoulli and Multinomial Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Georgia Technical Institute, Atlanta GA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Quantitative Risk Management: Concepts, Techniques, and Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Embrechts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Princeton university press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coherent approaches to risk in optimization under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorials in Operations Research: OR Tools and Applications. Glimpses of Future Technologies</title>
		<imprint>
			<publisher>INFORMS</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Design of experiments: ranking and selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamhane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Marcel Dekker, inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discrete-event simulation optimization using ranking, selection, and multiple comparison procedures: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Modeling and Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="154" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>TOMACS)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">AUTHOR BIOGRAPHIES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">D. student in the School of Operations Research and Information Engineering at Cornell University. His research interest is in quantitative risk management. His e-mail address can be found via &lt;www</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Waeber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ph</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>orie.cornell.edu&gt;</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">FRAZIER is an assistant professor in the School of Operations Research and Information Engineering at Cornell University. He received a Ph.D. in Operations Research and Financial Engineering from Princeton University in 2009. His research interest is in the optimal acquisition of information, with applications in simulation, medicine and operations management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>His web page can be found via &lt;www.orie.cornell.edu&gt;</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">His research interests include discrete-event simulation and simulation optimization, and he has worked for some time with emergency services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><forename type="middle">G</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Winter Simulation Conference</title>
		<meeting>the 2007 Winter Simulation Conference</meeting>
		<imprint/>
		<respStmt>
			<orgName>Operations Research and Information Engineering at Cornell University</orgName>
		</respStmt>
	</monogr>
	<note>He is the simulation area editor at Operations Research, and an associate editor for Management Science. His web page can be found via &lt;www.orie.cornell.edu&gt;</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
