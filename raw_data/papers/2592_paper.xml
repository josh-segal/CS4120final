<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seeing stars when there aren&apos;t many stars: Graph-based semi-supervised learning for sentiment categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
							<email>goldberg@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Sciences Department</orgName>
								<orgName type="department" key="dep2">Computer Sciences Department</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin-Madison Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison Madison</orgName>
								<address>
									<postCode>53706, 53706</postCode>
									<region>W.I., W.I</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Sciences Department</orgName>
								<orgName type="department" key="dep2">Computer Sciences Department</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin-Madison Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison Madison</orgName>
								<address>
									<postCode>53706, 53706</postCode>
									<region>W.I., W.I</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Seeing stars when there aren&apos;t many stars: Graph-based semi-supervised learning for sentiment categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., &quot;4 stars&quot;), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve rating-inference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis of text documents has received considerable attention recently <ref type="bibr" target="#b11">(Shanahan et al., 2005;</ref><ref type="bibr" target="#b14">Turney, 2002;</ref><ref type="bibr" target="#b3">Dave et al., 2003;</ref><ref type="bibr" target="#b5">Hu and Liu, 2004;</ref><ref type="bibr" target="#b2">Chaovalit and Zhou, 2005</ref>). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem <ref type="bibr">(2005)</ref>. Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of "stars" that a critic gave to a movie. Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating inference with large amounts of training data.</p><p>However, review documents often do not come with numerical ratings. We call such documents unlabeled data. Standard supervised machine learning algorithms cannot learn from unlabeled data. Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed. Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled. Supervised learning algorithms trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? <ref type="bibr" target="#b9">Pang and Lee (2005)</ref> suggested that doing so should be useful.</p><p>We demonstrate that the answer is 'Yes.' Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions ( <ref type="bibr" target="#b16">Zhu, 2005;</ref><ref type="bibr" target="#b10">Seeger, 2001)</ref>. This paper contains three contributions:</p><p>to the sentiment analysis domain, extending past supervised learning work by <ref type="bibr" target="#b9">Pang and Lee (2005)</ref>;</p><p>• We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3;</p><p>• We show the benefit of semi-supervised learning for rating inference with extensive experimental results in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Graph for Sentiment Categorization</head><p>The semi-supervised rating-inference problem is formalized as follows. There are n review documents x 1 . . . x n , each represented by some standard feature representation (e.g., word-presence vectors). Without loss of generality, let the first l ≤ n documents be labeled with ratings y 1 . . . y l ∈ C. The remaining documents are unlabeled. In our experiments, the unlabeled documents are also the test documents, a setting known as transduction. The set of numerical ratings are C = {c 1 , . . . , c C }, with c 1 &lt; . . . &lt; c C ∈ R. For example, a one-star to four-star movie rating system has C = {0, 1, 2, 3}. We seek a function f : x → R that gives a continuous rating f (x) to a document x. Classification is done by mapping f (x) to the nearest discrete rating in C. Note this is ordinal classification, which differs from standard multi-class classification in that C is endowed with an order. In the following we use 'review' and 'document,' 'rating' and 'label' interchangeably.</p><p>We make two assumptions:</p><p>1. We are given a similarity measure w ij ≥ 0 between documents x i and x j . w ij should be computable from features, so that we can measure similarities between any documents, including unlabeled ones. A large w ij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>), and mutual-information modulated word-vector cosine similarity. Details can be found in section 4.</p><p>2. Optionally, we are given numerical rating predictionsˆydictionsˆ dictionsˆy l+1 , . . . , ˆ y n on the unlabeled documents from a separate learner, for instance -insensitive support vector regression <ref type="bibr" target="#b6">(Joachims, 1999;</ref><ref type="bibr" target="#b13">Smola and Schölkopf, 2004</ref>) used by <ref type="bibr" target="#b9">(Pang and Lee, 2005)</ref>. This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is required.)</p><p>We now describe our graph for the semisupervised rating-inference problem. We do this piece by piece with reference to <ref type="figure" target="#fig_0">Figure 1</ref>. Our undirected graph G = (V, E) has 2n nodes V , and weighted edges E among some of the nodes.</p><p>• Each document is a node in the graph (open circles, e.g., x i and x j ). The true ratings of these nodes f (x) are unobserved. This is true even for the labeled documents because we allow for noisy labels. Our goal is to infer f (x) for the unlabeled documents.</p><p>• Each labeled document (e.g., x j ) is connected to an observed node (dark circle) whose value is the given rating y j . The observed node is a 'dongle' ( <ref type="bibr" target="#b15">Zhu et al., 2003</ref>) since it connects only to x j . As we point out later, this serves to pull f (x j ) towards y j . The edge weight between a labeled document and its dongle is a large number M . M represents the influence of y j : if M → ∞ then f (x j ) = y j becomes a hard constraint.</p><p>• Similarly each unlabeled document (e.g., x i ) is also connected to an observed dongle nodê y i , whose value is the prediction of the separate learner. Therefore we also require that f (x i ) is close tô y i . This is a way to incorporate multiple learners in general. We set the weight between an unlabeled node and its dongle arbitrarily to 1 (the weights are scale-invariant otherwise). As noted earlier, the separate learner is optional: we can remove it and still carry out graph-based semi-supervised learning. • Each unlabeled document</p><formula xml:id="formula_0">x i is connected to kN N L (i), its k nearest labeled documents.</formula><p>Distance is measured by the given similarity measure w. We want f (x i ) to be consistent with its similar labeled documents. The weight between</p><formula xml:id="formula_1">x i and x j ∈ kN N L (i) is a · w ij .</formula><p>• Each unlabeled document is also connected to k N N U (i), its k nearest unlabeled documents (excluding itself). The weight between x i and</p><formula xml:id="formula_2">x j ∈ k N N U (i) is b · w ij .</formula><p>We also want f (x i ) to be consistent with its similar unlabeled neighbors. We allow potentially different numbers of neighbors (k and k ), and different weight coefficients (a and b). These parameters are set by cross validation in experiments.</p><p>The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph-Based Semi-Supervised Learning</head><p>With the graph defined, there are several algorithms one can use to carry out semi-supervised learning ( <ref type="bibr" target="#b15">Zhu et al., 2003;</ref><ref type="bibr" target="#b4">Delalleau et al., 2005;</ref><ref type="bibr" target="#b7">Joachims, 2003;</ref><ref type="bibr" target="#b1">Blum and Chawla, 2001;</ref>). The basic idea is the same and is what we use in this paper. That is, our rating function f (x) should be smooth with respect to the graph. f (x) is not smooth if there is an edge with large weight w between nodes x i and x j , and the difference between f (x i ) and f (x j ) is large. The (un)smoothness over the particular edge can be defined as w f (</p><formula xml:id="formula_3">x i ) − f (x j ) 2 .</formula><p>Summing over all edges in the graph, we obtain the (un)smoothness L(f ) over the whole graph. We call L(f ) the energy or loss, which should be minimized. Let L = 1 . . . l and U = l + 1 . . . n be labeled and unlabeled review indices, respectively. With the graph in <ref type="figure" target="#fig_0">Figure 1</ref>, the loss L(f ) can be written as</p><formula xml:id="formula_4">i∈L M (f (x i ) − y i ) 2 + i∈U (f (x i ) − ˆ y i ) 2 + i∈U j∈kN N L (i) aw ij (f (x i ) − f (x j )) 2 + i∈U j∈k N N U (i) bw ij (f (x i ) − f (x j )) 2 .<label>(1)</label></formula><p>A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers. This is how unlabeled data can participate in learning. The optimization problem is min f L(f ). To understand the role of the parameters, we define α = ak + bk and β = b a , so that L(f ) can be written as</p><formula xml:id="formula_5">i∈L M (f (x i ) − y i ) 2 + i∈U (f (x i ) − ˆ y i ) 2 + α k + βk j∈kN N L (i) w ij (f (x i ) − f (x j )) 2 + j∈k N N U (i) βw ij (f (x i ) − f (x j )) 2 .<label>(2)</label></formula><p>Thus β controls the relative weight between labeled neighbors and unlabeled neighbors; α is roughly the relative weight given to semi-supervised (nondongle) edges. We can find the closed-form solution to the optimization problem. Defining an n × n matrix ¯ W ,</p><formula xml:id="formula_6">¯ W ij =    0, i ∈ L w ij , j ∈ kN N L (i) βw ij , j ∈ k N N U (i).<label>(3)</label></formula><p>Let W = max( ¯ W , ¯ W ) be a symmetrized version of this matrix. Let D be a diagonal degree matrix with</p><formula xml:id="formula_7">D ii = n j=1 W ij .<label>(4)</label></formula><p>Note that we define a node's degree to be the sum of its edge weights. Let ∆ = D − W be the combinatorial Laplacian matrix. Let C be a diagonal dongle weight matrix with</p><formula xml:id="formula_8">C ii = M, i ∈ L 1, i ∈ U .<label>(5)</label></formula><p>Let f = (f (x 1 ), . . . , f (x n )) and y = (y 1 , . . . , y l , ˆ y l+1 , . . . , ˆ y n ) . We can rewrite L(f ) as</p><formula xml:id="formula_9">(f − y) C(f − y) + α k + βk f ∆f .<label>(6)</label></formula><p>This is a quadratic function in f . Setting the gradient to zero, ∂L(f )/∂f = 0 , we find the minimum loss function</p><formula xml:id="formula_10">f = C + α k + βk ∆ −1</formula><p>Cy.</p><p>Because C has strictly positive eigenvalues, the inverse is well defined. All our semi-supervised learning experiments use <ref type="formula" target="#formula_11">(7)</ref> in what follows. Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and <ref type="bibr" target="#b9">Lee, 2005)</ref>, which formulates rating inference as a metric labeling problem <ref type="bibr">(Klein- berg and Tardos, 2002</ref>). Consider a special case of our loss function (1) when b = 0 and M → ∞. It is easy to show for labeled nodes j ∈ L, the optimal value is the given label: f (x j ) = y j . Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node</p><formula xml:id="formula_12">i ∈ U : L b=0,M →∞ (f (x i )) = (f (x i ) − ˆ y i ) 2 + j∈kN N L (i) aw ij (f (x i ) − y j ) 2 . (8)</formula><p>The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>) used absolute difference. Indeed in experiments comparing the two (not reported here), their differences are not statistically significant. From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We performed experiments using the movie review documents and accompanying 4-class (C = {0, 1, 2, 3}) labels found in the "scale dataset v1.0" available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>). We chose 4-class instead of 3-class labeling because it is harder. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents. We ran experiments individually for each author. Each document is represented as a {0, 1} word-presence vector, normalized to sum to 1. We systematically vary labeled set size |L| ∈ {0.9n, 800, 400, 200, 100, 50, 25, 12, 6} to observe the effect of semi-supervised learning. |L| = 0.9n is included to match 10-fold cross validation used by <ref type="bibr" target="#b9">(Pang and Lee, 2005</ref>). For each |L| we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets. We ensure that all four classes are represented in each labeled set. The same random splits are used for all methods, allowing paired t-tests for statistical significance. All reported results are average test set accuracy.</p><p>We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regression</head><p>We ran linear -insensitive support vector regression using Joachims' SVM light package (1999) with all default parameters. The continuous prediction on a test document is discretized for classification. Regression results are reported under the heading 'reg.' Note this method does not use unlabeled data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metric labeling</head><p>We ran Pang and Lee's method based on metric labeling, using SVM regression as the initial label preference function. The method requires an itemsimilarity function, which is equivalent to our similarity measure w ij . Among others, we experimented with PSP-based similarity. For consistency with (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>), supervised metric labeling results with this measure are reported under 'reg+PSP.' Note this method does not use unlabeled data for training either.</p><p>PSP i is defined in (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>) as the percentage of positive sentences in review x i . The similarity between reviews x i , x j is the cosine angle  <ref type="figure">Figure 2</ref>: PSP for reviews expressing each fine-grain rating. We identified positive sentences using SVM instead of Na¨ıveNa¨ıve Bayes, but the trend is qualitatively the same as in <ref type="bibr" target="#b9">(Pang and Lee, 2005</ref>). between the vectors (PSP i , 1−PSP i ) and (PSP j , 1− PSP j ). Positive sentences are identified using a binary classifier trained on a separate "snippet data set" located at the same URL as above. The snippet data set contains 10662 short quotations taken from movie reviews appearing on the rottentomatoes.com Web site. Each snippet is labeled positive or negative based on the rating of the originating review. Pang and Lee (2005) trained a Na¨ıveNa¨ıve Bayes classifier. They showed that PSP is a (noisy) measure for comparing reviews-reviews with low ratings tend to receive low PSP scores, and those with higher ratings tend to get high PSP scores. Thus, two reviews with a high PSP-based similarity are expected to have similar ratings. For our experiments we derived PSP measurements in a similar manner, but using a linear SVM classifier. We observed the same relationship between PSP and ratings ( <ref type="figure">Figure 2)</ref>.</p><p>The metric labeling method has parameters (the equivalent of k, α in our model). Pang and Lee tuned them on a per-author basis using cross validation but did not report the optimal parameters. We were interested in learning a single set of parameters for use with all authors. In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, α for all authors at all labeled set sizes in experiments involving PSP. Because c is fixed, k varies directly with |L| (i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples). In an attempt to reproduce the findings in <ref type="bibr" target="#b9">(Pang and Lee, 2005</ref>), we tuned c, α with cross validation. Tuning ranges are c ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and α ∈ {0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and α = 1.5. <ref type="figure">(In section 4.4</ref>, we discuss an alternative similarity measure, for which we re-tuned these parameters.)</p><p>Note that we learned a single set of shared parameters for all authors, whereas (Pang and <ref type="bibr" target="#b9">Lee, 2005</ref>) tuned k and α on a per-author basis. To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters. <ref type="table">Table 1</ref> shows the accuracy obtained over 20 trials with |L| = 0.9n for each author, using SVM regression, reg+PSP using shared c, α parameters, and reg+PSP using authorspecific c, α parameters (listed in parentheses). The best result in each row of the table is highlighted in bold. We also show in bold any results that cannot be distinguished from the best result using a paired t-test at the 0.05 level.</p><p>( <ref type="bibr" target="#b9">Pang and Lee, 2005)</ref> found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d). Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results. Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning. The optimal shared parameters produced almost the same results as the optimal author-specific parameters, and were used in subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semi-Supervised Learning</head><p>We used the same PSP-based similarity measure and the same shared parameters c = 0.2, α = 1.5 from our metric labeling experiments to perform graph-based semi-supervised learning. The results are reported as 'SSL+PSP.' SSL has three  <ref type="table">Table 1</ref>: Accuracy using shared (c = 0.2, α = 1.5) vs. author-specific parameters, with |L| = 0.9n.</p><p>additional parameters k , β, and M . Again we tuned k , β with cross validation. Tuning ranges are k ∈ {2, 3, 5, 10, 20} and β ∈ {0.001, 0.01, 0.1, 1.0, 10.0}. The optimal parameters are k = 5 and β = 1.0. These were used for all authors and for all labeled set sizes. Note that unlike k = c|L|, which decreases as the labeled set size decreases, we let k remain fixed for all |L|. We set M arbitrarily to a large number 10 8 to ensure that the ratings of labeled reviews are respected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alternate Similarity Measures</head><p>In addition to using PSP as a similarity measure between reviews, we investigated several alternative similarity measures based on the cosine of word vectors. Among these options were the cosine between the word vectors used to train the SVM regressor, and the cosine between word vectors containing only words with high (top 1000 or top 5000) mutual information values. The mutual information is computed with respect to the positive and negative classes in the 10662-document "snippet data set." Finally, we experimented with using as a similarity measure the cosine between word vectors containing all words, each weighted by its mutual information. We found this measure to be the best among the options tested in pilot trial runs using the metric labeling algorithm. Specifically, we scaled the mutual information values such that the maximum value was one. Then, we used these values as weights for the corresponding words in the word vectors. For words in the movie review data set that did not appear in the snippet data set, we used a default weight of zero (i.e., we excluded them. We experimented with setting the default weight to one, but found this led to inferior performance.)</p><p>We repeated the experiments described in sections 4.2 and 4.3 with the only difference being that we used the mutual-information weighted word vector similarity instead of PSP whenever a similarity measure was required. We repeated the tuning procedures described in the previous sections. Using this new similarity measure led to the optimal parameters c = 0.1, α = 1.5, k = 5, and β = 10.0. The results are reported under 'reg+WV' and 'SSL+WV,' respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We tested the five algorithms for all four authors using each of the nine labeled set sizes. The results are presented in table 2. Each entry in the table represents the average accuracy across 20 trials for an author, a labeled set size, and an algorithm. The best result in each row is highlighted in bold. Any results on the same row that cannot be distinguished from the best result using a paired t-test at the 0.05 level are also bold.</p><p>The results indicate that the graph-based semisupervised learning algorithm based on PSP similarity (SSL+PSP) achieved better performance than all other methods in all four author corpora when only 200, 100, 50, 25, or 12 labeled documents were available. In 19 out of these 20 learning scenarios, the unlabeled set accuracy by the SSL+PSP algorithm was significantly higher than all other methods. While accuracy generally degraded as we trained on less labeled data, the decrease for the SSL approach was less severe through the mid-range labeled set sizes. SSL+PSP remains among the best methods with only 6 labeled examples.</p><p>Note that the SSL algorithm appears to be quite sensitive to the similarity measure used to form the graph on which it is based. In the experiments where we used mutual-information weighted word vector similarity (reg+WV and SSL+WV), we notice that reg+WV remained on par with reg+PSP at high labeled set sizes, whereas SSL+WV appears significantly worse in most of these cases. It is clear that PSP is the more reliable similarity measure. SSL uses the similarity measure in more ways than the metric labeling approaches (i.e., SSL's graph is denser), so it is not surprising that SSL's accuracy would suffer more with an inferior similarity measure.</p><p>Unfortunately  <ref type="table">Table 2</ref>: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and methods. In each row, we list in bold the best result and any results that cannot be distinguished from it with a paired t-test at the 0.05 level.</p><p>is due to two factors: a) the baseline SVM regressor trained on a large labeled set can achieve fairly high accuracy for this difficult task without considering pairwise relationships between examples; b) PSP similarity is not accurate enough. Gain in variance reduction achieved by the SSL graph is offset by its bias when labeled data is abundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have demonstrated the benefit of using unlabeled data for rating inference. There are several directions to improve the work: 1. We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews' sentiment patterns. For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work <ref type="bibr" target="#b9">(Pang and Lee, 2005</ref>). 2. Our method is transductive: new reviews must be added to the graph before they can be classified. We will extend it to the inductive learning setting based on <ref type="bibr">(Sind- hwani et al., 2005</ref>). 3. We plan to experiment with cross-reviewer and cross-domain analysis, such as using a model learned on movie reviews to help classify product reviews.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The graph for semi-supervised rating inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>, our SSL approach did not do as well with large labeled set sizes. We believe this PSP word vector |L| regression reg+PSP SSL+PSP reg+WV SSL+WV</figDesc><table>Author (a) 

1593 
0.592 
0.592 
0.546 
0.592 
0.544 
800 
0.553 
0.554 
0.534 
0.553 
0.517 
400 
0.522 
0.525 
0.526 
0.522 
0.497 
200 
0.494 
0.498 
0.521 
0.494 
0.472 
100 
0.463 
0.477 
0.511 
0.462 
0.450 
50 
0.439 
0.458 
0.499 
0.438 
0.429 
25 
0.408 
0.421 
0.465 
0.400 
0.404 
12 
0.401 
0.378 
0.451 
0.335 
0.398 
6 
0.390 
0.359 
0.422 
0.314 
0.389 

Author (b) 

811 
0.501 
0.498 
0.481 
0.503 
0.473 
800 
0.501 
0.497 
0.478 
0.503 
0.474 
400 
0.471 
0.471 
0.465 
0.471 
0.450 
200 
0.447 
0.449 
0.452 
0.447 
0.429 
100 
0.415 
0.423 
0.443 
0.415 
0.397 
50 
0.388 
0.396 
0.434 
0.387 
0.376 
25 
0.373 
0.380 
0.418 
0.364 
0.367 
12 
0.354 
0.360 
0.399 
0.313 
0.353 
6 
0.348 
0.352 
0.380 
0.302 
0.347 

Author (c) 

1176 
0.592 
0.589 
0.566 
0.594 
0.514 
800 
0.579 
0.585 
0.559 
0.579 
0.509 
400 
0.550 
0.556 
0.544 
0.551 
0.491 
200 
0.513 
0.519 
0.532 
0.513 
0.479 
100 
0.484 
0.495 
0.521 
0.484 
0.466 
50 
0.462 
0.476 
0.504 
0.461 
0.456 
25 
0.459 
0.472 
0.484 
0.439 
0.454 
12 
0.420 
0.405 
0.477 
0.356 
0.414 
6 
0.320 
0.382 
0.366 
0.334 
0.322 

Author (d) 

924 
0.496 
0.498 
0.495 
0.499 
0.490 
800 
0.500 
0.501 
0.495 
0.504 
0.483 
400 
0.474 
0.478 
0.486 
0.477 
0.463 
200 
0.459 
0.459 
0.468 
0.459 
0.445 
100 
0.444 
0.445 
0.460 
0.444 
0.437 
50 
0.429 
0.431 
0.445 
0.429 
0.428 
25 
0.411 
0.411 
0.425 
0.400 
0.409 
12 
0.393 
0.362 
0.405 
0.335 
0.391 
6 
0.393 
0.357 
0.403 
0.312 
0.393 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Bo Pang, Lillian Lee and anonymous reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTAT</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th International Conf. on Machine Learning</title>
		<meeting>18th International Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Movie review mining: a comparison between supervised and unsupervised classification approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pimwadee</forename><surname>Chaovalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICSS</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;03: Proceedings of the 12th international conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient non-parametric function induction in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTAT</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD &apos;04, the ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>KDD &apos;04, the ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making large-scale svm learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>B. Schölkopf, C. Burges, and A. Smola</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-03, 20th International Conference on Machine Learning</title>
		<meeting>ICML-03, 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="616" to="639" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning with labeled and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computing attitude and affect in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<pubPlace>Dordrecht, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond the point cloud: from transductive to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML05, 22nd International Conference on Machine Learning</title>
		<meeting><address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>ACL-02, 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-03, 20th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Computer Sciences</publisher>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
