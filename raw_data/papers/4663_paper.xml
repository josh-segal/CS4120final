<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Valizadegan</surname></persName>
							<email>valizade@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Michigan State University East Lansing</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<address>
									<postCode>48824, 48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI, MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
							<email>rongjin@cse.msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Michigan State University East Lansing</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<address>
									<postCode>48824, 48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI, MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Maximum margin clustering was proposed lately and has shown promising performance in recent studies [1, 2]. It extends the theory of support vector machine to unsupervised learning. Despite its good performance, there are three major problems with maximum margin clustering that question its efficiency for real-world applications. First, it is computationally expensive and difficult to scale to large-scale datasets because the number of parameters in maximum margin clustering is quadratic in the number of examples. Second, it requires data preprocessing to ensure that any clustering boundary will pass through the origins, which makes it unsuitable for clustering unbalanced dataset. Third, it is sensitive to the choice of kernel functions, and requires external procedure to determine the appropriate values for the parameters of kernel functions. In this paper, we propose &quot;generalized maximum margin clustering&quot; framework that addresses the above three problems simultaneously. The new framework generalizes the maximum margin clustering algorithm by allowing any clustering boundaries including those not passing through the origins. It significantly improves the computational efficiency by reducing the number of parameters. Furthermore, the new framework is able to automatically determine the appropriate kernel matrix without any labeled data. Finally, we show a formal connection between maximum margin clustering and spectral clustering. We demonstrate the efficiency of the generalized maximum margin clustering algorithm using both synthetic datasets and real datasets from the UCI repository.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data clustering, the unsupervised classification of samples into groups, is an important research area in machine learning for several decades. A large number of algorithms have been developed for data clustering, including the k-means algorithm <ref type="bibr" target="#b2">[3]</ref>, mixture models <ref type="bibr" target="#b3">[4]</ref>, and spectral clustering <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. More recently, maximum margin clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> was proposed for data clustering and has shown promising performance. The key idea of maximum margin clustering is to extend the theory of support vector machine to unsupervised learning. However, despite its success, the following three major problems with maximum margin clustering has prevented it from being applied to real-world applications:</p><p>• High computational cost. The number of parameters in maximum margin clustering is quadratic in the number of examples. Thus, it is difficult to scale to large-scale datasets. <ref type="figure" target="#fig_0">Figure 1</ref> shows the computational time (in seconds) of the maximum margin clustering algorithm with respect to different numbers of examples. We  clearly see that the computational time increases dramatically when we apply the maximum margin clustering algorithm to even modest numbers of examples.</p><p>• Requiring clustering boundaries to pass through the origins. One important assumption made by the maximum margin clustering in <ref type="bibr" target="#b0">[1]</ref> is that the clustering boundaries will pass through the origins. To this end, maximum margin clustering requires centralizing data points around the origins before clustering data. It is important to note that centralizing data points at the origins does not guarantee clustering boundaries to go through origins, particularly when cluster sizes are unbalanced with one cluster significantly more popular than the other.</p><p>• Sensitive to the choice of kernel functions. <ref type="figure" target="#fig_1">Figure 2(b)</ref> shows the clustering error of maximum margin clustering for the synthesized data of two overlapped Gaussians clusters <ref type="figure" target="#fig_1">(Figure 2(a)</ref>) using the RBF kernel with different kernel width. We see that the performance of maximum margin clustering depends critically on the choice of kernel width. The same problem is also observed in spectral clustering <ref type="bibr" target="#b9">[10]</ref>. Although a number of studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6]</ref> are devote to automatically identifying appropriate kernel matrices in clustering, they are either heuristic approaches or require additional labeled data.</p><p>In this paper, we propose "generalized maximum margin clustering" framework that resolves the above three problems simultaneously. In particular, the proposed framework reformulates the problem of maximum margin clustering to include the bias term in the classification boundary, and therefore remove the assumption that clustering boundaries have to pass through the origins. Furthermore, the new formulism reduces the number of parameters to be linear in the number of examples, and therefore significantly reduces the computational cost. Finally, it is equipped with the capability of unsupervised kernel learning, and therefore, is able to determine the appropriate kernel matrix and clustering memberships simultaneously. More interestingly, we will show that spectral clustering, such as the normalized cut algorithm, can be viewed as a special case of the generalized maximum margin clustering. The remainder of the paper is organized as follows: Section 2 reviews the work of maximum margin clustering and kernel learning. Section 3 presents the framework of generalized maximum margin clustering. Our empirical studies are presented in Section 4. Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The key idea of maximum margin clustering is to extend the theory of support vector machine to unsupervised learning. Given the training examples D = (x 1 , x 2 , . . . , x n ) and their class labels y = (y 1 , y 2 , . . . , y n ) ∈ {−1, +1} n , the dual problem of support vector machine can be written as:</p><formula xml:id="formula_0">max α∈R n α e − 1 2 α diag(y)Kdiag(y)α s. t. 0 ≤ α ≤ C, α y = 0 (1)</formula><p>where K ∈ R n×n is the kernel matrix and diag(y) stands for the diagonal matrix that uses the vector y as its diagonal elements. To apply the above formulism to unsupervised learning, the maximum margin clustering approach relaxes class labels y to continuous variables, and searches for both y and α that maximizes the classification margin. This leads to the following optimization problem:</p><formula xml:id="formula_1">min y,λ,ν,δ t s. t. (yy ) • K e + ν − δ + λy (e + ν − δ + λy) t − 2Cδ e 0 ν ≥ 0, δ ≥ 0</formula><p>where • stands for the element wise product between two matrices. To convert the above problem into a convex programming problem, the authors of <ref type="bibr" target="#b0">[1]</ref> makes two important relaxations. The first one relaxes yy into a positive semi-definitive (PSD) matrix M 0 whose diagonal elements are set to be 1. The second relaxation sets λ = 0, which is equivalent to assuming that there is no bias term b in the expression of classification boundaries, or in other words, classification boundaries have to pass through the origins of data. These two assumption simplify the above optimization problem as follows:</p><formula xml:id="formula_2">min M,ν,δ t s. t. M • K e + ν − δ (e + ν − δ) t − 2Cδ e 0 ν ≥ 0, δ ≥ 0, M 0 (2)</formula><p>Finally, a few additional constraints of M are added to the above optimization problem to prevent skewed clustering sizes <ref type="bibr" target="#b0">[1]</ref>. As a consequence of these two relaxations, the number of parameters is increased from n to n 2 , which will significantly increase the computational cost. Furthermore, by setting λ = 0, the maximum margin clustering algorithm requires clustering boundaries to pass through the origins of data, which is unsuitable for clustering data with unbalanced clusters. Another important problem with the above maximum margin clustering is the difficulty in determining the appropriate kernel similarity matrix K. Although many kernel based clustering algorithms set the kernel parameters manually, there are several studies devoted to automatic selection of kernel functions, in particular the kernel width for the RBF kernel, <ref type="bibr" target="#b7">[8]</ref> recommended choosing the kernel width as 10% to 20% of the range of the distance between samples. However, in our experiment, we found that this is not always a good choice, and in many situations it produces poor results. Ng et al. <ref type="bibr" target="#b8">[9]</ref> chose kernel width which provides the least distorted clusters by running the same clustering algorithm several times for each kernel width. Although this approach seems to generate good results, it requires running seperate experiments for each kernel width, and therefore could be computationally intensive. Manor et al. in <ref type="bibr" target="#b9">[10]</ref> proposed a self-tuning spectral clustering algorithm that computes a different local kernel width for each data point x i . In particular, the local kernel width for each x i is computed as the distance of x i to its kth nearest neighbor. Although empirical study seems to show the effectiveness of this approach, it is unclear how to find the optimal k in computing the local kernel width. As we will see in the experiment section, the clustering accuracy depends heavily on the choice of k. Finally, we will briefly overview the existing work on kernel learning. Most previous work focus on supervised kernel learning. The representative approaches in this category include the kernel alignment <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, semi-definitive programming <ref type="bibr" target="#b12">[13]</ref>, and spectral graph partitioning <ref type="bibr" target="#b5">[6]</ref>. Unlike these approaches, the proposed framework is designed for unsupervised kernel learning.</p><formula xml:id="formula_3">i.e., σ in exp − xi−xj 2 2 2σ 2 . Shi et al.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalized Maximum Margin Clustering and Unsupervised Kernel Learning</head><p>We will first present the proposed clustering algorithm for hard margin, followed by the extension to soft margin and unsupervised kernel learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hard Margin</head><p>In the case of hard margin, the dual problem of SVM is almost identical to the problem in Eqn.</p><p>(1) except that the parameter α does not have the upper bound C. Following <ref type="bibr" target="#b12">[13]</ref>, we further convert the problem in (1) into its dual form:</p><formula xml:id="formula_4">min ν,y,λ 1 2 (e + ν + λy) T diag(y)K −1 diag(y)(e + ν + λy) s. t. ν ≥ 0, y ∈ {+1, −1} n<label>(3)</label></formula><p>where e is a vector with all its elements being one. Unlike the treatment in <ref type="bibr" target="#b12">[13]</ref>, which rewrites the above problem as a semi-definitive programming problem, we introduce variables z that is defined as follows: z = diag(y)(e + ν) Given that ν ≥ 0, the above expression for z is essentially equivalent to the constraint |z i | ≥ 1 or z 2 i ≥ 1 for i = 1, 2, . . . , n. Then, the optimization problem in (3) is rewritten as follows:</p><formula xml:id="formula_5">min z,λ 1 2 (z + λe) T K −1 (z + λe) s. t. z 2 i ≥ 1, i = 1, 2, . . . , n<label>(4)</label></formula><p>Note that the above problem may not have unique solutions for z and λ due to the translation invariance of the objective function. More specifically, given an optimal solution z and λ, we may be able to construct another solution z and λ such that: z = z + e, λ = λ − . Evidently, both solutions result in the same value for the objective function in (4). Furthermore, with appropriately chosen , the new solution z and λ will be able to satisfy the constraint z 2 i ≥ 1. Thus, z and λ is another optimal solution for (3). This is in fact related to the problem in SVM where the bias term b may not be unique <ref type="bibr" target="#b13">[14]</ref>. To remove the translation invariance from the objective function, we introduce an additional term C e (z e) 2 into the objective function, i.e.</p><formula xml:id="formula_6">min z,λ 1 2 (z + λe) T K −1 (z + λe) + C e (z e) 2 s. t. z 2 i ≥ 1, i = 1, 2, . . . , n<label>(5)</label></formula><p>where constant C e weights the important of the punishment factor against the original objective. It is set to be 10, 000 in our experiment. For the simplicity of our expression, we further define w = (z; λ) and P = (I n , e). Then, the problem in (4) becomes min w∈R n+1</p><formula xml:id="formula_7">w T P T K −1 P w + C e (e 0 w) 2 s. t. w 2 i ≥ 1, i = 1, 2, . . . , n<label>(6)</label></formula><p>where e 0 is a vector with all its elements being 1 except its last element which is zero. We then construct the Lagrangian as follows</p><formula xml:id="formula_8">L(w, γ) = w T P T K −1 P w + C e (e 0 w) 2 − n i=1 γ i (w I i n+1 w − 1) = w P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 w + n i=1 γ i</formula><p>where I i n+1 is an (n + 1) × (n + 1) matrix with all the elements being zero except the ith diagonal element which is 1. Hence, the dual problem of <ref type="formula" target="#formula_7">(6)</ref> is</p><formula xml:id="formula_9">max γ∈R n n i=1 γ i s. t. P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 γ i ≥ 0, i = 1, 2, .</formula><p>. . , n (7) Finally, the solution w can be computed using the KKT condition, i.e.,</p><formula xml:id="formula_10">P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 w = 0 n+1</formula><p>In other words, the solution w is proportional to the eigenvector of matrix</p><formula xml:id="formula_11">P T K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1</formula><p>for the zero eigenvalue. Since w i = (1 + ν i )y i , i = 1, 2, . . . , n and ν i ≥ 0, the class labels {y i } n i=1 can be inferred directly from the sign of {w i } n i=1 . Remark I It is important to realize that the problem in (5) is non-convex due to the nonconvex constraint w 2 i ≥ 1. Thus, the optimal solution found by the dual problem in <ref type="formula" target="#formula_20">(7)</ref> is not necessarily the optimal solution for the prime problem in (5). Our hope is that although the solution found by the dual problem is not optimal for the prime problem, it is still a good solution for the prime problem in (5). This is similar to the SDP relaxation made by the maximum margin clustering algorithm in (2) that relaxes a non-convex programming problem into a convex one. However, unlike the relaxation made in (2) that increases the number of variables from n to n 2 , the new formulism of maximum margin does not increase the number of parameters (i.e., γ), and therefore will be computational more efficient. This is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in which the computational time of generalized maximum margin clustering is increased much slower than that of the maximum margin algorithm.</p><p>Remark II To avoid the high computational cost in estimating K −1 , we replace K −1 with its normalized graph Laplacian L(K) <ref type="bibr" target="#b14">[15]</ref>, which is defined as</p><formula xml:id="formula_12">L(K) = I −D 1/2 KD 1/2</formula><p>where D is a diagonal matrix whose diagonal elements are computed as D i,i = n j=1 K i,j , i = 1, 2, . . . , n. This is equivalent to defining a kernel matrix˜Kmatrix˜ matrix˜K = L(K) † where † stands for the operator of pseudo inverse. More interesting, we have the following theorem showing the relationship between generalized maximum margin clustering and the normalized cut. Theorem 1. The normalized cut algorithm is a special case of the generalized maximum margin clustering in (7) if the following conditions hold, i.e., (1) K −1 is set to be the normalized Laplacian ¯ L(K), (2) all the γs are enforced to be the same, i.e., γ i = γ 0 , i = 1, 2, . . . , n, and (3) C e ≥ 1. Proof sketch: Given the conditions 1 to 3 in the theorem, the new objective function in <ref type="formula" target="#formula_20">(7)</ref> becomes: max γ≥0 γ s.t. ¯ L(K) γI n and the solution for this problem is the largest eigenvector of ¯ L(K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft Margin</head><p>We extend the formulism in <ref type="formula" target="#formula_20">(7)</ref> to the case of soft margin by considering the following problem:</p><formula xml:id="formula_13">min ν,y,λ,δ 1 2 (e + ν − δ + λy) T diag(y)K −1 diag(y)(e + ν − δ + λy) + C δ n i=1 δ 2 i s. t. ν ≥ 0, δ ≥ 0, y ∈ {+1, −1} n (8)</formula><p>where C δ weights the importance of the clustering errors against the clustering margin. Similar to the previous derivation, we introduce the slack variable z and simplify the above problem as follows:</p><formula xml:id="formula_14">min z,δ,λ 1 2 (z + λe) T K −1 (z + λe) + C e (z e) 2 + C δ n i=1 δ 2 i s. t. (z i + δ i ) 2 ≥ 1, δ i ≥ 0, i = 1, 2, . . . , n<label>(9)</label></formula><p>By approximating (z i + δ i ) 2 as z 2 i + δ 2 i , we have the dual form of the above problem written as:</p><formula xml:id="formula_15">max γ∈R n n i=1 γ i s. t. P K −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n<label>(10)</label></formula><p>The main difference between the above formulism and the formulism in <ref type="formula" target="#formula_20">(7)</ref> is the introduction of the upper bound C δ for γ in the case of soft margin. In the experiment, we set the parameter C δ to be 100, 000, a very large value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Kernel Learning</head><p>As already pointed out, the performance of many clustering algorithms depend on the right choice of the kernel similarity matrix. To address this problem, we extend the formulism in (10) by including the kernel learning mechanism. In particular, we assume that a set of m kernel similarity matrices K 1 , K 2 , . . . , K m are available. Our goal is to identify the linear combination of kernel matrices, i.e., K = m i=1 β i K i , that leads to the optimal clustering accuracy. More specifically, we need to solve the following optimization problem:</p><formula xml:id="formula_16">max γ,β n i=1 γ i s. t. P m i=1 β i K i −1 P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n, m i=1 β i = 1, β i ≥ 0, i = 1, 2, . . . , m<label>(11)</label></formula><p>Unfortunately, it is difficult to solve the above problem due to the complexity introduced by (</p><formula xml:id="formula_17">m i=1 β i K i ) −1 .</formula><p>Hence, we consider an alternative problem to the above one. We first introduce a set of normalized graph Laplacian ¯</p><formula xml:id="formula_18">L 1 , ¯ L 2 , . . . , ¯ L m .</formula><p>Each Laplacian L i is constructed from the kernel similarity matrix K i . We then defined the inverse of the combined matrix as</p><formula xml:id="formula_19">K −1 = m i=1 β i ¯ L i .</formula><p>Then, we have the following optimization problem Figure 3: Data distribution of the three synthesized datasets By solving the above problem, we are able to resolve both γ (corresponding to clustering memberships) and β (corresponding to kernel learning) simultaneously.</p><formula xml:id="formula_20">max γ,β n i=1 γ i s. t. m i=1 β i P ¯ L i P + C e e 0 e 0 − n i=1 γ i I i n+1 0 0 ≤ γ i ≤ C δ , i = 1, 2, . . . , n, m i=1 β i = 1, β i ≥ 0, i = 1, 2, . . . , m<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We tested the generalized maximum margin clustering algorithm on both synthetic datasets and real datasets from the UCI repository. <ref type="figure">Figure 3</ref> gives the distribution of the synthetic datasets. The four UCI datasets used in our study are "Vote", "Digits", "Ionosphere", and "Breast". These four datasets comprise of 218, 180, 351, and 285 examples, respectively, and each example in these four datasets is represented by 17, 64, 35, and 32 features. Since the "Digits" dataset consists of multiple classes, we further decompose it into four datasets of binary classes that include pairs of digits difficult to distinguish. Both the normalized cut algorithm <ref type="bibr" target="#b7">[8]</ref> and the maximum margin clustering algorithm <ref type="bibr" target="#b0">[1]</ref> are used as the baseline. The RBF kernel is used throughout this study to construct the kernel similarity matrices. In our first experiment, we examine the optimal performance of each clustering algorithm by using the optimal kernel width that is acquired through an exhaustive search. The optimal clustering errors of these three algorithms are summarized in the first three columns of <ref type="table" target="#tab_0">Table 1</ref>. It is clear that generalized maximum margin clustering algorithm achieve similar or better performance than both maximum margin clustering and normlized cut for most datasets when they are given the optimal kernel matrices. Note that the results of maximum margin clustering are reported for a subset of samples(including 80 instances) in UCI datasets due to the out of memory problem. In the second experiment, we evaluate the effectiveness of unsupervised kernel learning. Ten kernel matrices are created by using the RBF kernel with the kernel width varied from 10% to 100% of the range of distance between any two examples. We compare the proposed unsupervised kernel learning to the self-tuning spectral clustering algorithm in <ref type="bibr" target="#b9">[10]</ref>. One of the problem with the self-tuning spectral clustering algorithm is that its clustering error usually depends on the parameter k, i.e., the number of nearest neighbor used for computing the kernel width. To provide a full picture of the self-tuning spectral clustering, we vary k from 1 and 15 , and calculate both best and worst performance using different k. The last three columns of <ref type="table" target="#tab_0">Table 1</ref> summarizes the clustering errors of generalized maximum margin clustering and self-tuning spectral clustering with both best and worst k. First, observe the big gap between best and worst performance of self-tuning spectral clustering with different choice of k, which implies that this algorithm is sensitive to parameter k. Second, for most datasets, generalized maximum margin clustering achieves similar performance as self-tuning spectral clustering with the best k. Furthermore, for a number of datasets, the unsupervised kernel learning method achieves the performance close to the one using the optimal kernel width. Both results indicate that the proposed algorithm for unsupervised kernel learning is effective in identifying appropriate kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a framework for the generalized maximum margin clustering. Compared to the existing algorithm for maximum margin clustering, the new framework has three advantages: 1) it reduces the number of parameters from n 2 to n, and therefore has a significantly lower computational cost, 2) it allows for clustering boundaries that do not pass through the origin, and 3) it can automatically identify the appropriate kernel similarity matrix through unsupervised kernel learning. Our empirical study with three synthetic datasets and four UCI datasets shows the promising performance of our proposed algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Kernel</head><label>1</label><figDesc>Figure 1: The scalability of the original maximum margin clustering algorithm versus the generalized maximum margin clustering algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Clustering error of spectral clustering using the RBF kernel with different kernel width. The horizonal axis of Figure 2(b) represents the percentage of the distance range (i.e., the difference between the maximum and the minimum distance) that is used for kernel width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Clustering error (%) of normalized cut (NC), maximum margin clustering (MMC), 
generalized maximum margin clustering (GMMC) and self-tuning spectral clustering (ST). 
Dataset 
Optimal Kernel Width 
Unsupervised Kernel Learning 
NC MMC GMMC GMMC ST (Best k) ST(Worst k) 
Two Circles 
2 
0 
0 
0 
0 
50 
Two Jointed Circles 
7 
6.25 
0 
0 
1 
45 
Two Gaussian 
1.25 
2.5 
1.25 
3.75 
5 
7.5 
Vote 
25 
15 
9.6 
11.90 
11 
40 
Digits 3-8 
35 
10 
5.6 
5.6 
5 
50 
Digits 1-7 
45 
31.25 
2.2 
3 
0 
47 
Digits 2-7 
34 
1.25 
.5 
5.6 
1.5 
50 
Digits 8-9 
48 
3.75 
16 
12 
9 
48 
Ionosphere 
25 
21.25 
23.5 
27.3 
26.5 
48 
Breast 
36.5 38.75 
36.1 
37 
37.5 
41.5 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised multi-class support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05</title>
		<meeting>the 20th National Conference on Artificial Intelligence (AAAI-05</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Statist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixture densities, maximum likelihood and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Redner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="195" to="239" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A min-max cut algorithm for graph partitioning and data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Data Mining</title>
		<meeting>IEEE Int&apos;l Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 16</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probabilistic approach for optimizing spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On kernel-target alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="367" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonparametric transforms of graph kernels for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uniqueness theorems for kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crisp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="187" to="220" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Amer. Math. Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
