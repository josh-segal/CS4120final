<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple, Pipelined Algorithm for Large, Irregular All-gather Problems ⋆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Larsson Träff</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories Europe</orgName>
								<orgName type="institution">NEC Europe Ltd</orgName>
								<address>
									<addrLine>Rathausallee 10</addrLine>
									<postCode>D-53757</postCode>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ripke</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories Europe</orgName>
								<orgName type="institution">NEC Europe Ltd</orgName>
								<address>
									<addrLine>Rathausallee 10</addrLine>
									<postCode>D-53757</postCode>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Siebert</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories Europe</orgName>
								<orgName type="institution">NEC Europe Ltd</orgName>
								<address>
									<addrLine>Rathausallee 10</addrLine>
									<postCode>D-53757</postCode>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mathematics and Computer Science Division</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<postCode>60439</postCode>
									<settlement>Argonne</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Thakur</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mathematics and Computer Science Division</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<postCode>60439</postCode>
									<settlement>Argonne</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gropp</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple, Pipelined Algorithm for Large, Irregular All-gather Problems ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present and evaluate a new, simple, pipelined algorithm for large, irregular all-gather problems, useful for the implementation of the MPI Allgatherv collective operation of MPI. The algorithm can be viewed as an adaptation of a linear ring algorithm for regular all-gather problems for single-ported, clustered multiprocessors to the irregular problem. Compared to the standard ring algorithm, whose performance is dominated by the largest data size broadcast by a process (times the number of processes), the performance of the new algorithm depends only on the total amount of data over all processes. The new algorithm has been implemented within different MPI libraries. Benchmark results on NEC SX-8, Linux clusters with InfiniBand and Gigabit Ethernet, Blue Gene/P, and SiCortex systems show huge performance gains in accordance with the expected behavior.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The all-gather problem is a basic collective communication operation, in which each participant of a predefined group wants to broadcast personal data to all other group members. In the MPI standard, this functionality is embodied in the regular MPI Allgather collective, in which each process contributes the same amount of data, and in the irregular MPI Allgatherv collective, where the amount of data can be freely chosen for the different processes <ref type="bibr" target="#b7">[8]</ref>. For both MPI collectives, all participating processes know the sizes of the data to be broadcast by all other processes. The irregular all-gather operation is used for instance in linear algebra kernels for matrix multiplication and LU factorization <ref type="bibr" target="#b0">[1]</ref>.</p><p>The regular all-gather problem has been intensively studied (theoretically under the term gossiping, but is also known as broadcast-to-all, all-to-all-broadcast, as well as many other names) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and many algorithms have been proposed and/or implemented as part of MPI libraries for various systems and communication models <ref type="bibr">[1-3, 7, 9, 10]</ref>. The more challenging, irregular all-gather problem has received much less attention, and MPI libraries typically use the same algorithm for both MPI Allgather and MPI Allgatherv. For irregular problems with considerable differences between the amount of data contributed by the processes, this can have huge performance drawbacks. For extreme cases, the resulting performance loss can amount to orders of magnitude (cf. Section 3).</p><p>In this paper, we present an algorithm for large, irregular all-gather problems. The underlying idea is quite simple and can be viewed as an adaptation to the irregular problem of a ring-based algorithm for regular all-gather problems for single-ported, clustered multiprocessors. The algorithm has been implemented for several MPI libraries, and evaluated on diverse systems, namely NEC SX-8, two Linux clusters, IBM Blue Gene/P, and SiCortex 5832. We demonstrate significant performance improvements over a standard MPI Allgatherv algorithm, depending on the amount of irregularity in the benchmark scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Algorithm and Implementation(s)</head><p>In the following, p is the number of participating (MPI) processes, numbered consecutively from 0 to p − 1. We let m i denote the size of the data contributed by process i, and m = p−1 i=0 m i the total amount of data that eventually has to be gathered by all processes. For large data, we assume that the time for transmitting a message of size m ′ is simply O(m ′ ). For most of the following discussion, a detailed communication cost model is unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Standard, linear ring Algorithm</head><p>A basic (folklore) algorithm for large, regular all-gather problems is the linear ring. The algorithm performs p − 1 communication rounds. In each round process i sends (starting with its own data) an already known block of data of size m ′ to process (i + 1) mod p and receives an unknown block of data from process (i − 1) mod p. For regular problems where all blocks are of the same size m i = m ′ , the completion time of the ring algorithm is O((p−1)m ′ ) = O(m−m ′ ). The number of communication start-ups (latency) scales linearly with p. This is unproblematic for large m ′ , but for small problems, an algorithm with a logarithmic number of start-ups is clearly preferable <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>. The linear ring algorithm is straightforward to implement. For systems with single-ported, bidirectional communication capabilities (where each process can at the same time send data to another process and receive data from a possibly different process) it can use the system communication bandwidth to full capacity. For irregular all-gather problems, where the data sizes m i can vary arbitrarily over the processes, the algorithm can however perform poorly. The running time is determined by the largest amount of data m ′ = max p−1 i=0 m i , which has to be sent along the ring in each round, and is therefore O((p − 1)m ′ ). In particular, (p − 1)m ′ can be much larger than the total amount of data m. Node j Node j − 1 <ref type="figure" target="#fig_2">Fig. 1</ref>. The linear ring algorithm on a cluster of SMP nodes with different number of MPI processes per node. The processes are (virtually) ranked such that one process at each node receives data from another node, and one process sends data to another node in each round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pipelined (blocked) ring Algorithm</head><p>We first observe that the linear ring algorithm can also be used for the regular all-gather problems on clustered multiprocessors (like clusters of SMP nodes) with a single-ported communication network. In that case the ring is organized such that exactly one process i per SMP node has its predecessor (i − 1) mod p on another SMP node, and exactly one process j per SMP node has its successor (j + 1) mod p on another node. To accomplish this, a (virtual) reranking of the MPI processes might be necessary. The clustered, linear ring algorithm is now communication-bandwidth optimal, because in each round one process on each node receives a block of data and one process sends a block of data. This holds also for the case where the number of MPI processes per cluster node is not identical, and is illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>In <ref type="bibr" target="#b10">[11]</ref> it is observed that regular collective communication problems like the all-gather problem induce corresponding irregular problems over a set of nodes in a clustered system. Therefore, if the communication capabilities of processors and nodes in a cluster are similar (for instance, single ported), an algorithm for solving a regular problem on a clustered system (with possibly different number of processes per cluster node) can be used to solve its irregular counterpart over a set of processors. This observation can be exploited to convert the clustered linear ring algorithm into an algorithm for the irregular all-gather problem.</p><p>To accomplish this the data of process i of size m i is associated with a virtual cluster node, and divided into b i = max(1, ⌈m i /B⌉) blocks of size at most B. Each block is associated with a virtual processor in the node. The total number of blocks is b = p−1 i=0 b i (note that b ≥ p). Every actual process with data size m i will play the role of a cluster node with b i virtual processors. The linear ring algorithm with regular blocks of size (at most) B now solves the problem in b − 1 instead of p− 1 communication rounds. The resulting, pipelined (or blocked ) ring algorithm is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Compared to the linear ring, the advantage of the pipelined ring algorithm is that (more) regular blocks are sent and received in each round, for a total time of O((b − 1)B). A small value for B increases the number of start-ups, and a large value increases the possible round up error. Therefore a proper balancing needs to be applied to find an optimal value for the block size parameter. We note that for extremely irregular all-gather problems where only one process has all the data, the pipelined ring algorithm is equivalent to a linear broadcast pipeline. For regular problems where m i = m ′ for all i, the Process i block size B can be set to m ′ , in which case the algorithm is identical to the standard, linear ring. Thus, by choosing B properly, the pipelined ring algorithm should never perform worse than the linear ring algorithm.</p><formula xml:id="formula_0">Process i + 1 Process i − 1 j − 1 j j + 1 j + k − 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Determining an optimal block size</head><p>We note that for partially full blocks, only the actual data are sent and received (see again <ref type="figure" target="#fig_1">Figure 2</ref>). In particular, the empty blocks which arise for processes with m i = 0 are neither sent nor received. Nevertheless, they contribute to the total number of communication rounds. We estimate the optimal block size B as follows, assuming that z denotes the number of processes with m i = 0:</p><p>-If z = 0 we take B = min p−1 i=0 m i (as long as this is not too small). This ensures that all processes are both sending and receiving blocks in (almost) all rounds. -If z 񮽙 = 0 we try to minimize the time needed for b − 1 communication rounds.</p><p>Assuming that the remainders in the m i /B terms are equally distributed, we get an average padding of B/2 for all partially full blocks. We can therefore simplify b = p−1 i=0 max(1, ⌈m i /B⌉) to b = m B + p+z 2 . Assuming linear communication costs, where sending and receiving messages of size m ′ takes time α + βm ′ , the estimated total running time is (b − 1)(α + βB). Minimizing this term gives an (approximated) optimal block size of B = 2αm β(p+z−2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>We have benchmarked the new MPI Allgatherv implementations with the following distributions of contiguous data over the p MPI processes. A base count c (which is varied over some interval) is used as seed for the following distributions: In distributions (2) and (3) the same total amount of data m = c is gathered by all processes, so similar running times can be expected (comparable to the regular distribution with p times smaller data size). The case for distributions (1), (4), <ref type="formula">(5)</ref> and <ref type="formula">(6)</ref> is analogous, where the total amount of data is m = pc.</p><p>We compare our implementations of the new MPI Allgatherv algorithm with implementations of the standard linear ring algorithm that is still used in many MPI libraries <ref type="bibr" target="#b8">[9]</ref>. The reported running times are minimum times for the last process to finish over a (small) number of iterations <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on an NEC SX-8 vector system</head><p>The pipelined ring has been implemented for MPI/SX for the NEC SX-series of parallel vector computers. It has been benchmarked with the distributions described above on 30 SX-8 nodes at HLRS in Stuttgart, with 1 and 8 MPI processes per node, respectively. Selected results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>For the extreme broadcast distribution (2) the pipelined ring outperforms the standard linear ring by more than a factor of 10 on 30 SX-8 nodes. For 32 MBytes with a fixed block size B of 1 MByte an improvement of a factor 32×29 29+31 ≈ 15 would have been best possible. Significant improvements can also be observed for the other distributions. The performance of the standard ring and the pipelined ring are similar for the regular (1) and the half full (4) distributions. Running on a randomly permuted communicator instead of MPI COMM WORLD gives almost identical results. This is a desirable property of an algorithm for a symmetric (i.e. non-rooted) collective operation like MPI Allgatherv <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on a Linux Cluster with InfiniBand</head><p>To show the effect of the block size B, the algorithm has also been integrated into NEC's MPI/PC version and evaluated on an Intel Xeon based SMP cluster with InfiniBand interconnect. The running time is compared to the standard, nonpipelined algorithm for B = 32K, 64K, 128K, 512K, 1024K. Results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. For the spike distribution (3) the pipelined algorithm is faster for all block sizes. However, the best block size depends not only on the size of the problem but also on the distribution of data over the processes. This can be seen in the case of the decreasing distribution (5) where a too small block size makes the pipelined algorithm perform worse than the standard ring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on a Linux Cluster with Gigabit Ethernet</head><p>We ran the benchmarks on a Linux cluster at Argonne National Laboratory with 24 nodes, each with two dual-core 2.8 GHz AMD Opteron CPUs (total     . For small problem sizes, the pipelined algorithm performs only slightly better than the standard algorithm, but as problem size increases, the difference in performance becomes considerable. <ref type="figure">Figure 6(right)</ref> shows the distribution of communication and idle times for the two algorithms. As expected, the standard algorithm suffers because many processes remain idle for a long time, whereas in the pipelined algorithm, communication is more balanced. We also collected traces of the program execution and plotted them using the Jumpshot tool, as shown in <ref type="figure">Figure 7</ref>. The penalty due to idle time incurred by the standard algorithm is clearly visible as the yellow bars. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on SiCortex</head><p>Benchmarks were also performed on the SiCortex 5832 system at Argonne. This machine has 972 nodes, each with 6 cores, for a total of 5832 processors. The nodes are connected by a Kautz graph network. In some of our experiments the native SiCortex MPI implementation failed. We therefore implemented the standard linear ring algorithm ourselves and compared it with the pipelined algorithm. <ref type="figure" target="#fig_7">Figure 8</ref> shows the results for a test run with a geometric curve dis- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on IBM Blue Gene/P</head><p>Finally, we performed the tests on one rack of the IBM Blue Gene/P at Argonne National Laboratory (4096 cores). The native implementation of MPI Allgatherv in the Blue Gene/P's MPI library uses a very fast hardware-supported algorithm, which outperforms both standard ring and pipelined ring implementations. Therefore, to fairly compare pipelined and non-pipelined algorithms, we implemented both these algorithms. <ref type="figure" target="#fig_7">Figure 8</ref> shows the results. The pipelined algorithm performs even better on this machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concluding Remarks</head><p>We described a simple, pipelined ring algorithm for large, irregular all-gather problems. The algorithm was implemented within different MPI libraries and benchmarked on various systems, and in all cases showed considerable improvements over a commonly used linear ring algorithm for problems with significant irregularity in the individual message sizes. Determining the best possible pipeline block size for all distributions of input data still requires more (experimental) work. On regular problem instances the pipelined algorithm performs similarly to the linear ring, which is bandwidth optimal for that case. Ring algorithms can likewise be implemented to be largely independent on process placement in an SMP system. This is an important property for users expecting (self-) consistent performance of their MPI library <ref type="bibr" target="#b11">[12]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The clustered, linear ring algorithm viewed as a pipelined (blocked) algorithm for solving the irregular all-gather problem. For each process, the data mi is divided into blocks of some maximum block size B (partially full blocks are partially colored). Process i starts sending block j + k − 1 and receiving block j − 1. After b − 1 rounds, where b represents the total number of blocks, all processes have gathered all the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>Regular: all m i = c are identical, therefore m = pc. 2. Broadcast: m 0 = c, all other m i = 0, therefore m = c. 3. Spike: similar to broadcast but all processes contribute some data, m 0 = c/2 and m i = c 1 2(p−1) , therefore m = c. 4. Half full: m 2⌊i/2⌋ = 2c, and m 2⌊i/2⌋+1 = 0, therefore m = pc. 5. Linearly decreasing: m i = 2c (p−1−i) p−1 , therefore m = pc. 6. Geometric curve: m i−1+j = c p i log p for i = 1, 2, 4, . . . and j = {0, . . . , i−1}, therefore m = pc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results (left to right, top to bottom) for distributions (2), (3), (5) and (6) on an NEC SX-8 with 30 nodes and 1 MPI process per node, and distributions (2) and (3) with 8 MPI processes per node. A fixed block size B = 1 MByte has been used. The base data size is the base count c multiplied by the size of an MPI INT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results from a Linux Xeon/InfiniBand cluster with 16 × 2 processes with spike (left) and linearly decreasing (right) distributions, and block size B = 32K, 64K, 128K, 512K, 1024K compared to the non-pipelined algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Results with 96 processes on Linux cluster: (left) Spike distribution (right) Geometric curve distribution. A fixed block size B = 32 KB has been used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results for the geometric curve distribution: (left) with 5784 processes on the SiCortex machine and a fixed block size of B = 1 MB, (right) with 4096 processes on 1 rack of the Blue Gene/P and a fixed block size of B = 64 KB</figDesc></figure>

			<note place="foot">⋆ This work was supported in part by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Advanced Scientific Computing Research, Office of Science, U.S. Department of Energy, under Contract DE-AC02-06CH11357.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonuniformly communicating noncontiguous data: A case study with PETSc and MPI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Buntinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21th International Parallel and Distributed Processing Symposium (IPDPS 2007)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of MPICH allgather algorithms on switched networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Caglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface. 10th European PVM/MPI Users&apos; Group Meeting</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2840</biblScope>
			<biblScope unit="page" from="335" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient algorithms for all-to-all communications in multiport message-passing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kipnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Upfal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weathersby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1143" to="1156" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reproducible measurements of MPI performance characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface. 6th European PVM/MPI Users&apos; Group Meeting</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1697</biblScope>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of gossiping and broadcasting in communication networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hedetniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hedetniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Liestman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="319" to="349" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gossiping in minimal time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Krumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="139" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient shared memory and RDMA based design for mpi allgather over InfiniBand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mamidala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface. 13th European PVM/MPI Users&apos; Group Meeting</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4192</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MPI -The Complete Reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huss-Lederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The MPI Core</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the performance of collective operations in MPICH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient allgather for regular SMP-clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface. 13th European PVM/MPI Users&apos; Group Meeting</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4192</biblScope>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Relationships between regular and irregular collective communication operations on clustered multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Submitted</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-consistent MPI performance requirements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface. 14th European PVM/MPI Users&apos; Group Meeting</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4757</biblScope>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
