<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparison of Variational Bayes and Gibbs Sampling in Reconstruction of Missing Values with Probabilistic Principal Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Gabriel</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Aalto University School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alba</forename><surname>Rivera</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Aalto University School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Aalto University School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Aalto University School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Science and Technology Department of Information and Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Comparison of Variational Bayes and Gibbs Sampling in Reconstruction of Missing Values with Probabilistic Principal Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Lately there has been the interest of categorization and pattern detection in large data sets, including the recovering of the dataset missing values. In this project the objective will be to recover the subset of missing values as accurately as possible from a movie rating data set. Initially the data matrix is preprocessed and its elements are divided in training and test sets. Thereafter the resulting matrices are factorized and reconstructed according to probabilistic principal component analysis (PCA). We compare the quality of reconstructions done with sampling and variational Bayesian (VB) approach. The results of the experiments showed that sampling improved the quality of the recovered missing values over VB-PCA typically after roughly 100 steps of Gibbs sampling.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human preferences (the quality tags we put on things) are language terms that can be easily translated into a numerical domain. We could assign low values to odd things and high values to enjoyable things, i.e.; rate things according to our experience. These ratings serve us to easily (and grossly) classify and order our preferences from the ones we like the most to the ones we dislike the most. Of course we are limited: we can not rate what we do not know, however; it may be of our interest to know the possible ratings of these unknowns.</p><p>In this project we will be working with large and sparse matrices of movies ratings. The objective will be to recover a subset of the missing values as accurately as possible. Recovering these missing values equal to predicting movies ratings and, therefore; predicting movies preferences for different users. The idea of correctly recovering movies ratings for different users has been a hot topic during the last years motivated by the Netflix prize.</p><p>The concept of mining users preferences to predict a preference of a third user is called Collaborative Filtering, it involves large data sets and has been used by stores like Amazon and iTunes.</p><p>We can start by considering that the preferences of the users are determined by a number of unobserved factors (that later we will call components). These hidden variables can be, for example, music, screenplay, special effects, etc. These variables weight different and are rated independently, however; they, together, sum up for the final rating, the one we observe. Therefore; if we can factorize the original matrix (the one with the ratings) in a set of sub-matrices that represent these hidden factors, we may have a better chance to find the components and values to recover the missing ratings <ref type="bibr" target="#b0">[1]</ref>. One approach to find these matrices is to use SVD (Single Value Decomposition), a matrix factorization method. With SVD the objective is to find matrices U V minimizing the sum-squared distance to the target matrix R <ref type="bibr" target="#b1">[2]</ref>.</p><p>For this project we consider matrix Y to be our only informative input. Matrix Y is, usually, large and disperse, i.e.; with lots of missing values. The observable values are the ratings given to movies (rows) by users (columns). Our objective is to recover the missing values, or a subset of them, with a small error. We can factorize matrix Y such that</p><formula xml:id="formula_0">Y ≈ WX + m,<label>(1)</label></formula><p>where the bias vector m is added to each column of the matrix WX. Matrices W X m will let us recover the missing values, of course, the quality of the recovering depends on the quality of these matrices. Sampling will let us improve the fitness of matrices W X m to better recover matrix Y. We can use VB-PCA (Variational Bayes PCA) for the initial decomposition of the input matrix Y. VB-PCA is known to be less prone to over-fitting and more accurate for lager-scale data sets with lots of missing values compared to traditional PCA methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However; VB-PCA is not compulsory for sampling, a random initialization method is also explored in this project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sampling PCA</head><p>Sampling can be seen as the generation of numerical values with the characteristics of a given distribution. Sampling is used when other approaches are not feasible.</p><p>For high-dimensional probabilistic models Markov chain Monte Carlo methods are used to go over the integrals with good accuracy. Gibbs sampling is a well known MCMC method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In Gibbs approach we sample one variable, for example W, conditioned to the remaining variables, X m. In the following step we sample another variable fixing the rest; we repeat this process generating as many samples as necessary.</p><p>In our project we have matrix Y that is a joint distribution of the form Y = WX + m+noise to predict the missing values in Y we need to solve:</p><formula xml:id="formula_1">P (Y MIS |Y OBS ) = P (Y MIS |W, X, m) (2) P (W, X, m|Y OBS ) dW dX dm.</formula><p>Solving the integral is complex, therefore; we make use of Gibbs sampling to approximate its solution. To recover matrices W X m we need to solve P (W|Y OBS , X, m), P (X|Y OBS , W, m) and P (m|Y OBS , W, X) each one following a Gaussian distribution, contrary to P (W, X, m|Y OBS ) that follows an unknown and complex distribution. The mean matrices, ¯ X ¯ W ¯ m, and covariance matrices, Σ x Σ w ˜ m, are calculated according to the formulas provided at <ref type="bibr" target="#b3">[4]</ref> Appendix D; this is done as follows:</p><formula xml:id="formula_2">¯ X :j = ( ¯ W T j ¯ W j + vI) −1 ¯ W T j ( ˚ Y :j − ¯ m j ) (3) Σ x,j = v( ¯ W T j ¯ W j + vI) −1 (4) ¯ W i: = ( ˚ Y i: − ¯ m i ) T ¯ X T i ( ¯ X i ¯ X T i + v diag(w −1 k ))<label>(5)</label></formula><formula xml:id="formula_3">Σ w,i = v( ¯ X i ¯ X T i + v diag(w −1 k ))<label>(6)</label></formula><formula xml:id="formula_4">¯ m i = w m |O i |(w m + v/|O i |) jǫOi [y ij − ¯ W i: ¯ X :j ] (7) ˜ m i = vw m |O i |(w m + v/|O i |) .<label>(8)</label></formula><p>Indices j = 1, . . . , p and i = 1, . . . , m go over the rows (people) and columns (movies) of matrix Y, and y ij is the ijth element of matrix Y. ¯ X :j is the column j of matrix ¯ X, ¯ W i: is row i of matrix ¯ W, ¯ m i is element i of vector m. v and w m are hyperparameters. ˚ Y is the data matrix where the missing values have been replaced with zeroes. O is the set of indices ij for which y ij is observed. O i is the set of indices j for which y ij is observed. |O i | is the number of elements in O i . I is the identity matrix. diag is the diagonalizing of the referred values. W j is matrix W in which an ith row is replaced with zeros if y ij is missing, m j is vector m in which each ith element is replaced with zero if y ij is missing, and X i is the matrix X in which a jth column is replaced with zeros if y ij is missing.</p><p>Using the mean and covariance matrices we are able to sample W ′ X ′ and m ′ using the methods presented in <ref type="bibr" target="#b5">[6]</ref>. With the sampled and mean matrices we recover a full matrix Y ′ , i.e.; including the missing values; more of this is explained in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recovering the Missing Values</head><p>To recover the matrix Y we need to multiply matrix W by X and add the m bias vector to each column. Referring to the ideas presented by <ref type="bibr" target="#b0">[1]</ref>, matrix W represents the different and weighted factors that conform a movie. On the other hand, matrix X represents the values assigned to each factor by the different users. The resulting matrix Y ′ has, therefore, the ratings given to movies m by users p. The bias term, m, is used to compensate the differences in results from the recovered matrix Y ′ and the original observed values used during the training.</p><p>To prove the quality of the ratings in the recovered matrix Y ′ it is necessary to have a test set different from the training set. At every step during sampling when the values are recovered we calculate the Root Mean Square Error, RMSE, using the test set as baseline. RMSE is a well known measure to quantify the amount by which a predictor differs from the value being predicted.</p><p>The sampling and recovering process is as follows:</p><p>1. Start point i = 1, with matrices W i X i and m i .</p><p>2. Calculate mean matrix ¯ X and covariance matrix Σ x using W i by Eqs. (3)-(4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recover Y ′ with W i and ¯</head><p>X by Eq. (1).</p><p>4. Increase i by one.</p><p>5. Sample X i using from N ( ¯ X, Σ x ).</p><p>6. Calculate mean matrix ¯ W and covariance matrix Σ w using X i by Eqs. (5)-(6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Recover matrix Y ′ with ¯</head><p>W and X i by Eq. (1).</p><p>8. Sample W i from N ( ¯ W, Σ w ).</p><p>9. Calculate bias mean ¯ m and variance˜mvariance˜ variance˜m using W i X i by Eqs. <ref type="formula">(7)</ref> </p><formula xml:id="formula_5">¯ y k+1 = k¯ y k + y k+1 k + 1 ,<label>(9)</label></formula><p>where k is the step, ¯ y is the average of the previous values and y are the new recovered values. Using the average will lead to better results than just using the single-samples alone. The more samples are averaged, the close the approximation is to the true integral in Equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tests and Results</head><p>The Sampling PCA method was tested with an artificial data set and the MovieLens data set. For the MovieLens test the missing values were also predicted randomly to observe how close a random prediction is from the sampling approach, i.e.; to grossly measure the benefit of using sampling. With the artificial data we will focus on recovering all missing values while with MovieLens data only a subset of the missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Artificial Data</head><p>The initial testing was done using artificially gener-    it is in with data set A, PCA FULL recovers the matrix with a small error, therefore; no improving can be expected, or achieved, when sampling. On the other hand, with data set C, where the missing values are many and the matrix is noisy and large the recovering achieved from PCA FULL is just good but it is improved with the Sampling PCA algorithm. An important value affecting the results is the number of components, c. Because we do not know the original number of components we try with 10, 20 and 30, and notice that as we get closer to the original number of components our results improve. At <ref type="figure" target="#fig_3">Figure 2</ref>, are the sampling RMSE error progress through 500 samples compared to the PCA FULL RMSE error using the best results within each data set.</p><p>From the artificial testing we can conclude, first; the number of components used play an important role and, second; as more complex is the problem better results can be expected when using Sampling PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MovieLens Data</head><p>The MovieLens <ref type="bibr" target="#b6">[7]</ref> data set consist of 100,000 ratings given by 943 users to 1682 movies. Each rating is a triplet, the value of the rating, the user giving the rat-  ing and the movie being rated. The ratings go from 1 to 5, not all movies have been rated nor all users have given rates. Having 100,000 ratings mean that less than 10% of the total possible triplets are available. With the recovered matrices and hyper-parameters we perform Sampling PCA. Two options are explored, the first option consist in using all the recovered data as starting point for sampling. The second option consists on only using the hyper-parameters; W ′ X ′ and m ′ matrices are initialized with random values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sampling From PCA Full/Diag</head><p>In this first approach sampling is performed using the recovered matrices and hyper-parameters. For each set of variables 2000 samples are generated, the numeric results can be observed at  <ref type="table" target="#tab_1">Table 2</ref>. The use of 20 components seems to return the best results, also, the use of PCA DIAG shows better results. The best results (shadowed) represent a small improvement, less than   1% against the top result obtained using the VB-PCA approach alone (shadowed at <ref type="table" target="#tab_1">Table 2</ref>). However; a small improvement for recovering missing values tasks its an important gain.</p><p>At <ref type="figure" target="#fig_6">Figure 3</ref>, we can observe the RMSE value of each sample through the 2000 samples taken, with different number of components and using PCA FULL data as baseline; the values are compared against the RMSE of VB-PCA approach. At <ref type="figure" target="#fig_7">Figure 4</ref>, a similar plot is observable but in this case using PCA DIAG data as baseline. For both Figures, in all sub-plots, we can notice that the sampling algorithm is unstable for the initial samples, the RMSE value jumps around the RMSE recovered from the VB-PCA approach. However; for the last hundreds of samples stabilization is noticeable, showing small differences after each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sampling Using Random Initialization</head><p>Another approach to perform Sampling PCA consist in only using the hyper-parameters recov-  The initial samples will be highly deviated from the objective value, therefore; they can be eliminated before the real prediction is made. In our test we remove the initial 30 samples. Later, we generate 1000 new samples to make the predictions of the missing values. Again 10, 20 and 30 components are used and the hyper-parameters from PCA FULL/DIAG. The <ref type="figure">Figure 5</ref>, shows the discarded samples and how spread they were compared to the final RMSE. The first 10 samples are the most disperse ones, latest samples are more stable in their RMSE value, specially, when the number of components is 20 and 30.  <ref type="table" target="#tab_5">Table 4</ref>. The results are similar to those obtained using the first approach, however; its worth noticing that for PCA FULL the results are better in all the instances and only half the samples were generated (the same hyper-parameters were used). This may be related on how the recovered matrices, learning Y t , directly affect the sampling process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This project lead to interesting results. The artificial tests let us know that small matrices with small portion of missing values are not easily improved by sampling. For the MovieLens test we observed that sampling improved the quality of the recovered missing values over VB-PCA using the later as an initial step. We also noticed that the random initialization does not affect sampling and the results are good. The best results were obtained using PCA DIAG and 20 components; the worst results were obtained using PCA FULL and 10 components. A future improvement could be achieved rounding the recovered values that are outside the range of the expected ones, i.e.; values ≤ 1 to 1 and ≥ 5 to 5. A look at the recovered vector, for the best results, shows 6 values below 1 and 32 above 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>Luis De Alba is supported by the Program AlBan, the European Union Program of High Level Scholarships for Latin America, scholarship No. E07M402627MX. Alexander Ilin and Tapani Raiko are supported by the Academy of Finland.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sampling PCA process. Every time matrix Y ′ is calculated (steps 3 and 7) the missing values are recovered. At every recovering step the missing values are averaged with the previously recovered ones</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ated data.</head><label></label><figDesc>The artificial data consists on generating matrices W[m, c] (normally distributed N (0, 1), ran- dom values); X[c, p] (uniformly distributed [0 . . . 1], random values) and, an additional noise matrix N[m, p] (normally distributed N (0, var) where noise variance (var) is given in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sampling progress with artificial data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>c=10 c=20 c=30 PCA FULL Y ′ vs Y t 0.743154 0.743614 0.744083 PCA FULL Y ′ vs Y p 0.892615 0.892397 0.892211 PCA DIAG Y ′ vs Y t 0.762655 0.768235 0.768326 PCA DIAG Y ′ vs Y p 0.889250 0.889069 0.888687</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reconstruction error in the MovieLens data as a function of used samples, initialized by and compared to PCA FULL solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Reconstruction error in the MovieLens data as a function of used samples, initialized by and compared to PCA DIAG solution..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 ,</head><label>6</label><figDesc>Figure 6, shows the RMSE value at each sample during the sampling process. The results of the sampling process are at Table 4. The results are similar to those obtained using the first approach, however; its worth noticing that for PCA FULL the results are better in all the instances and only half the samples were generated (the same hyper-parameters were used). This may be related on how the recovered matrices, learning Y t , directly affect the sampling process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: RMSE for first samples after random initialization (discarded samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table below )</head><label>below</label><figDesc>t . We do this using 10, 20 and 30 components. With the recovered matrices we run the Sampling PCA algorithm; 500 samples are generated from each input. We can observe at Table 1, how the noise, size and proportion of missing values of the original matrix Y affect the quality of the recovered missing values. It is also noticeable that when the problem is simple, as</figDesc><table>. Matrix 
Y[m, p] is generated as Y = WX + N. From ma-
trix Y a given percentage of ratings is selected at ran-
dom and set to N aN in matrix Y t , i.e.; set to be miss-
ing values 1 . 
Three data sets were generated with the following 
characteristics: 

Set 
m 
p 
c Noise Var Missing Values 
A 100 125 8 
0.05 
50% 
B 
150 200 15 
0.3 
70% 
C 
300 450 18 
0.5 
85% 

Using the VB-PCA approach, PCA FULL function 
[4], we recover W X and m (plus hyper-parameters) 
from matrix Y c=10 

c=20 
c=30 

A PCA Full 0.264886 
0.264909 0.264939 
Sampling 
0.265208 
0.265511 0.266457 
B PCA Full 0.965070 
0.865517 0.992878 
Sampling 
0.959550 
0.866838 0.989643 
C PCA Full 1.238677 
1.163651 1.238233 
Sampling 
1.232581 
1.160960 1.230279 

Table 1: RMSE results on artificial data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : RMSE results on PCA FULL/DIAG for Train- ing and Probing parts of the MovieLens data.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Re-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>RMSE results in the MovieLens problem af-
ter sampling (2000 samples). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>RMSE results after sampling, random ini-
tialization. 

trices W ′ X ′ and m ′ values are updated to better fit 
Y t . 

</table></figure>

			<note place="foot" n="1"> Where m stands for number of movies; p for number of people and c for number of components.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Netflix update: Try this at home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Funk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Principal Component Analysis for Large Scale Problems with Lots of Missing Values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th European conference on Machine Learning</title>
		<meeting>the 18th European conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical Approaches to Principal Component Analysis in the Presence of Missing Values. Technical report TKK-ICS-R6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Helsinki University of Technology. Later version accepted for publication in</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian Probabilistic Matrix Factorization using Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pattern recognition and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MovieLens</title>
		<ptr target="http://www.grouplens.org/node/73" />
	</analytic>
	<monogr>
		<title level="m">Movie Recommendations. GroupLens Research at the University of Minnesota</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
