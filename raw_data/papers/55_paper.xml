<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analytical Cache Models with Applications to Cache Partitioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Edward</forename><surname>Suh</surname></persName>
							<email>suh@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Devadas</surname></persName>
							<email>devadas@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Rudolph</surname></persName>
							<email>rudolph@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analytical Cache Models with Applications to Cache Partitioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An accurate, tractable, analytic cache model for time-shared systems is presented, which estimates the overall cache miss-rate of a multiprocessing system with any cache size and time quanta. The input to the model consists of the isolated miss-rate curves for each process, the time quanta for each of the executing processes, and the total cache size. The output is the overall miss-rate. Trace-driven simulations demonstrate that the estimated miss-rate is very accurate. Since the model provides a fast and accurate way to estimate the effect of context switching, it is useful for both understanding the effect of context switching on caches and optimizing cache performance for time-shared systems. A cache partitioning mechanism is also presented and is shown to improve the cache miss-rate up to 25% over the normal LRU replacement policy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This paper presents an analytical model for the behavior of a cache in a multiprocessing system that can accurately estimate overall miss-rate for any cache size and any time quantum. An evaluation method for miss-rate is essential to optimize cache performance. Traditional cache performance evaluation is done by simulations <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b12">12]</ref>, which provide accurate results, but simulation time is often long. Hardware monitoring can dramatically speed up the process <ref type="bibr" target="#b26">[26]</ref>, however, it is limited to the particular cache configuration. As a result, both simulations and hardware monitoring can only be used to evaluate the effect of context switches <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b9">10]</ref>. Moreover, simulations and monitoring rarely provide intuitive understanding making it difficult to improve cache performance. To provide both performance prediction and insight into improving performance, analytical cache models are required.</p><p>We use our model to determine the best cache partitioning so as to improve performance. Partitioning is needed to mitigate the effects of conflicts among concurrently executing processes, especially for large caches. In the past, caches were small and it was best to let each process consume the entire cache space, since process footprints were much larger than the cache. In modern microprocessors, caches are much larger; some Level 1 (L1) caches range up to one MB <ref type="bibr" target="#b4">[5]</ref>, and L2 caches are up to several MB <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13]</ref>. Large caches allow potential performance improvement by partitioning. Since each process may not need the entire cache space, the effect of context switches can be mitigated by keeping useful data in the cache over context switches. It is crucial for modern microprocessors to minimize inter-process conflicts by proper cache partitioning <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b8">9]</ref> or scheduling <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Our model requires information that is relatively easy to acquire. The characteristics for each process are given by the miss-rate as a function of cache size when the process is isolated, which can be easily obtained either on-line or offline. The time quantum for each process and cache size are also given as inputs to the model. With this information, the model estimates the overall miss-rate for a given cache size when an arbitrary combination of processes is run. The model provides good estimates for any cache size and any time quantum, and is easily applied to real problems since the input miss-rate curves are both intuitive and easy to obtain in practice. Therefore, we believe that the model is useful for any study related to the effect of context switches on cache memory.</p><p>After describing related research in Section 2, Section 3 derives an analytical cache model for time-shared systems. Section 4 discusses cache partitioning based on the model and evaluates the model-based partitioning method by simulations. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Several early investigations of the effects of context switches use analytical models. Thiébaut and Stone <ref type="bibr" target="#b20">[20]</ref> modeled the amount of additional misses caused by context switches for set-associative caches. Agarwal, Horowitz and Hennessy <ref type="bibr" target="#b0">[1]</ref> also included the effect of conflicts between processes in their analytical cache model and showed that inter-process conflicts are noticeable for a mid-range of cache sizes that are large enough to have a considerable number of conflicts but not large enough to hold all the working sets. However, these models work only for long enough time quanta, and require information that is hard to collect on-line. <ref type="bibr">Mogul and Borg [14]</ref> studied the effect of context switches through trace-driven simulations. Using a timesharing system simulator, their research shows that system calls, page faults, and a scheduler are the main sources of context switches. They also evaluated the effect of context switches on cycles per instruction (CPI) as well as the cache miss-rate. Depending on cache parameters, the cost of a context switch appears to be in the thousands of cycles, or tens to hundreds of microseconds in their simulations.</p><p>Stone, Turek and Wolf <ref type="bibr" target="#b18">[18]</ref> investigated the optimal allocation of cache memory between two competing processes that minimizes the overall miss-rate of a cache. Their study focuses on the partitioning of instruction and data streams, which can be thought of as multitasking with a very short time quantum. Their model for this case shows that the optimal allocation occurs at a point where the miss-rate derivatives of the competing processes are equal. The LRU replacement policy appears to produce cache allocations very close to optimal for their examples. They also describe a new replacement policy for longer time quanta that only increases cache allocation based on time remaining in the current time quantum and the marginal reduction in missrate due to an increase in cache allocation. However, their policy simply assumes the probability for a evicted block to be accessed in the next time quantum as a constant, which is neither validated nor is it described how this probability is obtained.</p><p>Thiébaut, Stone and Wolf applied their partitioning work <ref type="bibr" target="#b18">[18]</ref> to improve disk cache hit-ratios <ref type="bibr" target="#b21">[21]</ref>. The model for tightly interleaved streams is extended to be applicable for more than two processes. They also describe the problems in applying the model in practice, such as approximating the miss-rate derivative, non-monotonic miss-rate derivatives, and updating the partition. Trace-driven simulations for 32-MB disk caches show that the partitioning improves the relative hit-ratios in the range of 1% to 2% over the LRU policy.</p><p>Our analytical model and partitioning differ from previous efforts that tend to focus on some specific cases of context switches. Our model works for any specific time quanta, whereas the previous models focus only on long time quanta. Also, our partitioning works for any time quanta, whereas Thiébaut's algorithms only works for very short time quanta. Moreover, the inputs of our model (miss-rates) are much easier to obtain compared to footprints or the number of unique cache blocks that previous models require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ANALYTICAL CACHE MODEL</head><p>The analytical cache model estimates the overall cache missrate for a multi-processing system. The cache size and the time quantum length for each job is known. The cache size is given by the number of cache blocks, and the time quantum is given by the number of memory references. Both are assumed to be constants (See <ref type="figure" target="#fig_0">Figure 1 (a)</ref>). In addition, associated with each job is its miss-rate curve, i.e., the number of cache misses as a function of the cache size.</p><p>This section explains the development of the model in several steps. Heavy use is made of the individual, isolated miss-rate curve (iimr). This curve is the miss-rate for a pro-  cess as a function of cache size assuming no other processes are running. There is much information that can be gleaned from this equation. For example, we can compute the miss rate of a process as a function of time (Section 3.2.1) from the miss-rate of a process as a function of space.</p><p>Observe that as a process executes, it either references an item in the cache, in which case its footprint size remains the same, or it gets a cache miss thereby increasing its footprint size. In other words, we know how much cache is allocated to a process as a function of time: from the iimr curve, we compute the independent, isolated footprint as a function of time (iifp) (Section 3.2.2).</p><p>If one knows how much cache is allocated to a process when it begins executing its time quantum and how much more cache it will need during the execution of that time quantum, we can compute how much cache will be left for the next process that is about to begin its time quantum execution. In other words, from the iifp curves of all the concurrent processes, we compute the individual, dependent footprint (dfp) as a function of time (Section 3.2.3).</p><p>At each time step, we know how much cache is allocated to the running process (from dfp(t)) and we know the miss rate for that size (from iimr(S)) for the executing process and so we can get the dependent miss rate as a function of time (dmr(t)) (Section 3.2.1).</p><p>Finally, integrating or summing the dmr(t) over time, gives the overall average miss rate for a given cache size, given time quantum sizes, and a given set of concurrent processes (Section 3.2.4).</p><p>The following subsection gives an overview of our assumptions. The development of the cache model is then presented, following the outline given above. Finally, this section ends with experimental verification of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumptions</head><p>The memory reference pattern of each process is assumed to be represented by a miss-rate curve that is a function of the cache size. Moreover, this miss-rate curve is assumed not to change over time. Although real applications do have  dynamically changing memory reference patterns, our results show that, in practice, an average miss-rate function works very well. For abrupt changes in the reference pattern, multiple miss-rate curves can be used to estimate an overall miss-rate.</p><p>There is no shared address space among processes. This assumption is true for common cases where each process has its own virtual address space and the shared memory space is negligible compared to the entire memory space that is used by a process.</p><p>Finally, a round-robin scheduling policy with a fixed time quantum for each process is assumed (see <ref type="figure" target="#fig_0">Figure 1 (b)</ref>), an LRU replacement policy is used, and the cache is fully associative. Although most real caches are set-associative, a model for fully-associative caches is very useful for understanding the effect of context switches because the model is simple. Moreover, cache partitioning experiments demonstrate that the fully-associative model can also be applied to set-associative caches in practice (Section 4). Elsewhere, we have extended the model to handle set-associative caches <ref type="bibr" target="#b19">[19]</ref>. A model assuming many other scheduling methods and replacement policies can be similarly derived.</p><p>We make use of the following notations:</p><p>t the number of memory references from the beginning of a time quantum.</p><p>x(t) the number of cache blocks that belong to a process after t memory references.</p><p>m(x) the steady-state miss-rate for a process with cache size x.</p><p>T the number of memory references in a time quantum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cache Model</head><p>The goal is to predict the average miss-rate for a multiprocess machine with a given cache size and set of processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Miss rate as function of time</head><p>Given the independent, isolated miss-rate of a process as a function of cache size, we compute its miss-rate as a function of time. Let time t start at the beginning of a time quantum, not at the beginning of execution. Since all time quanta for a process are identical by our assumptions, we consider only one time quantum for each process.</p><p>Although the cache size is C, at certain times, it is possible that only part of the cache is filled with the current process' data ( <ref type="figure" target="#fig_1">Figure 2</ref> (a) shows a snapshot of a cache at time t0). Therefore, the effective cache size at time t0 can be thought of as the amount of the current process' data x(t0) in the cache at that time. The probability of a cache miss in the next memory reference is given by</p><formula xml:id="formula_0">Pmiss(t0) = m(x(t0)).<label>(1)</label></formula><p>Once we have Pmiss(t0), it is easy to estimate the missrate over time during that time quantum. The number of misses for the process over a time quantum can be expressed as a simple integral, <ref type="figure" target="#fig_1">Figure 2</ref> (b), where the miss-rate is expressed as the number of misses divided by the number of memory references.</p><formula xml:id="formula_1">miss-rate = 1 T Z T 0 Pmiss(t)dt = 1 T Z T 0 m(x(t))dt (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Footprint as a function of time</head><p>We now estimate x(t), the amount of a process' data, i.e. its footprint, in a cache as a function of time. Let us begin with the assumption that a process starts executing during a time quantum with an empty cache in order to estimate cache performance for cases when a cache gets flushed for every context switch. Virtual address caches without process ID are good examples of such a case. We show later how to estimate x(t) when the cache is not empty at the start of a time quantum.</p><p>Consider x ∞ (t) as the amount of the current process' data at time t for an infinite size cache. We assume that the process starts with an empty cache at time 0. There are two possibilities for x ∞ (t) at time t + 1. If the (t + 1) th memory reference results in a cache miss, a new cache block is brought into the cache. As a result, the amount of the process's cache data increases by one block. Otherwise, the amount of data remains the same. Therefore, the amount of the process' data in the cache at time t + 1 is given by</p><formula xml:id="formula_2">x ∞ (t + 1) = ( x ∞ (t) + 1 (t + 1) th reference misses x ∞ (t) otherwise.<label>(3)</label></formula><p>Since the probability for the (t + 1) th memory reference to miss is m(x ∞ (t)) from Equation 1, the expected value of x(t + 1) can be written by</p><formula xml:id="formula_3">E[x ∞ (t + 1)] = E[x ∞ (t) · (1 − m(x ∞ (t))) + (x ∞ (t) + 1) · m(x ∞ (t))] = E[x ∞ (t) + 1 · m(x ∞ (t))] = E[x ∞ (t)] + E[m(x ∞ (t))]. (4)</formula><p>Assuming that m(x) is convex 1 , we can use Jensen's inequality <ref type="bibr" target="#b2">[3]</ref> and rewrite the equation as a function of E[x ∞ (t)].</p><formula xml:id="formula_4">E[x ∞ (t + 1)] ≥ E[x ∞ (t)] + m(E[x ∞ (t)]).<label>(5)</label></formula><p>Usually, a miss-rate changes slowly. As a result, for a short interval such as from x to x + 1, m(x) can be approximated as a straight line. Since the equality in Jensen's inequality holds if the function is a straight line, we can approximate the amount of data at time t + 1 as</p><formula xml:id="formula_5">E[x ∞ (t + 1)] E[x ∞ (t)] + m(E[x ∞ (t)]).<label>(6)</label></formula><p>We can calculate the expectation of x ∞ (t) more accurately by calculating the probability for every possible value at time t <ref type="bibr" target="#b19">[19]</ref>. However, calculating a set of probabilities is computationally expensive. Also, our experiments show that the above approximation closely matches simulation results.</p><p>If we further approximate the amount of data x ∞ (t) to be the expected value E[x ∞ (t)], x ∞ (t) can be expressed with a differential equation:</p><formula xml:id="formula_6">x ∞ (t + 1) − x ∞ (t) = m(x ∞ (t)),<label>(7)</label></formula><p>which can be easily calculated in a recursive manner.</p><p>To obtain a closed form solution, we can rewrite the discrete form of the differential equation 7 to a continuous form:</p><formula xml:id="formula_7">dx ∞ dt = m(x ∞ ).<label>(8)</label></formula><p>Solving the differential equation by separating variables, the differential equation becomes</p><formula xml:id="formula_8">t = Z x ∞ (t) x ∞ (0) 1 m(x ) dx .<label>(9)</label></formula><p>We define a function M (x) as an integral of 1/m(x), which means that dM (x)/dx = 1/m(x), and then x ∞ (t) can be written as a function of t:</p><formula xml:id="formula_9">x ∞ (t) = M −1 (t + M (x ∞ (0)))<label>(10)</label></formula><p>where M −1 (x) represents the inverse function of M (x).</p><p>Finally, for a finite size cache, the amount of data in the cache is limited by the size of the cache C. Therefore, x φ (t),</p><p>1 If a replacement policy is smart enough, the marginal gain of having one more cache block monotonically decreases as we increase the cache size.</p><formula xml:id="formula_10">P i,1 P i,3 P i+1,2 ... P i-1,2 P i,2 P i+1,1 ... P i-1,1 MRU data LRU data</formula><p>The snapshot of a cache the amount of a process' data starting from an empty cache, is written by</p><formula xml:id="formula_11">x φ (t) = M IN [x ∞ (t), C] = M IN [M −1 (t + M (0)), C]. (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Individual, Dependent Footprint as a function of time</head><p>We now compute the amount of a process' data at time t when the cache is not flushed at a context switch, i.e., the dependent case. To distinguish between the processes, a subscript i is used to represent Process i. For example, xi(t) represents the amount of Process i's data at time t.</p><p>The estimation of xi(t) is based on round-robin scheduling (See <ref type="figure" target="#fig_0">Figure 1 (b)</ref>) and the LRU replacement policy. Process i runs for a fixed length time quantum Ti. For simplicity, processes are assumed to be of infinite length so that there is no change in the scheduling. Also, the initial startup transient from an empty cache is ignored since it is negligible compared to the steady state.</p><p>To estimate the amount of a process' data at a given time, imagine the snapshot of a cache after executing Process i for time t as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Note that time is 0 at the beginning of the process' time quantum. In the figure, the blocks on the left side show recently used data, and blocks on the right side show old data. P j,k represents the data of Process j, and subscript k specifies the most recent time quantum when the data are referenced. From the figure, we can obtain xi(t) once we know the size of all P j,k blocks.</p><p>The size of each block can be estimated using the x φ i (t) curve from Equation 11, which is the amount of Process i's data when the process starts with an empty cache. Since x φ i (t) can also be thought of as the amount of data that are referenced from time 0 to time t, x φ i (Ti) is the amount of data that are referenced over one time quantum. Similarly, we can estimate the amount of data that are referenced over k recent time quanta to be x φ i (k · Ti). As a result, the size of Block P j,k can be written as</p><formula xml:id="formula_12">P j,k = 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :</formula><formula xml:id="formula_13">x φ j (t + (k − 1) · Tj) − x φ j (t + (k − 2) · Tj ) if j is executing x φ j (k · Tj) − x φ j ((k − 1) · Tj ) otherwise<label>(12)</label></formula><p>where we assume that x φ j (t) = 0 if t &lt; 0.</p><p>xi(t) is the sum of P i,k blocks that are inside the cache of size C in <ref type="figure" target="#fig_2">Figure 3</ref>. If we define li(t) as the maximum integer value that satisfies the following inequality, then li(t) + 1 represents how many P i,k blocks are in the cache.</p><formula xml:id="formula_14">l i (t) X k=1 N X j=1 P j,k = x φ i (t+(li(t)−1)·Ti)+ N X j=1,j񮽙 =i x φ j (li(t)·Tj) ≤ C (13)</formula><p>where N is the number of processes. From li(t) and <ref type="figure" target="#fig_2">Figure 3</ref>, the estimated value of xi(t) is <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the relation between x φ i (t) and xi(t). In the figure li(t) is assumed to be 2. Unlike the cache flushing case, a process can start with some of its data left in the cache. The amount of initial data xi(0) is given by Equation 14. If the least recently used (LRU) data in a cache does not belong to Process i, xi(t) increases the same as x φ i (t). However, if the LRU data belongs to Process i, xi(t) does not increase on a cache miss since Process i's block gets replaced.</p><formula xml:id="formula_15">xi(t) = 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :</formula><formula xml:id="formula_16">x φ i (t + li(t) · Ti) if x φ i (t + li(t) · Ti)+ N X j=1,j񮽙 =i x φ j (li(t) · Tj ) ≤ C C − N X j=1,j񮽙 =i x φ j (li(t) · Tj ) otherwise<label>(14)</label></formula><p>Define tstart(j, k) as the time when the k th MRU block of Process j (P j,k ) becomes the LRU part of a cache, and t end (j, k) as the time when P j,k gets completely replaced from the cache (See <ref type="figure" target="#fig_2">Figure 3)</ref>. tstart(j, k) and t end (j, k) specify the flat segments in <ref type="figure" target="#fig_3">Figure 4</ref> and can be estimated from the following equations that are based on Equation 12.</p><formula xml:id="formula_17">x φ j (tstart(j, k)+(k−1)·Tj )+ N X p=1,p񮽙 =j x φ p ((k−1)·Tp) = C. (15) x φ j (t end (j, k)+(k−2)·Tj)+ N X p=1,p񮽙 =j x φ p ((k−1)·Tp) = C.<label>(16)</label></formula><p>tstart(j, lj (t) + 1) would be zero if Equation 15 is satisfied when tstart(j, lj (t) + 1) is negative, which means that the P (j, lj (t) + 1) block is already the LRU part of the cache at the beginning of a time quantum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Overall Miss-rate</head><p>This section presents the overall miss-rate calculation. When a cache uses virtual address tags and gets flushed for every context switch, each process starts a time quantum with an empty cache. In this case, the miss-rate of a process can be estimated from the results of Section 3.2.1 and 3.2.2. From Equation 2 and 11, the miss-rate for Process i can be written by</p><formula xml:id="formula_18">miss-rate φ i = 1 Ti Z T i 0 mi(M IN [M −1 i (t + Mi(0)), C])dt.<label>(17)</label></formula><p>If a cache uses physical address tags or has a process' ID with virtual address tags, it does not have to be flushed at a context switch. In this case, the amount of data xi(t) is estimated in Section 3.2.3. The miss-rate for Process i can be written by</p><formula xml:id="formula_19">miss-ratei = 1 Ti Z T i 0 mi(xi(t))dt<label>(18)</label></formula><p>where xi(t) is given by Equation 14.</p><p>For actual calculation of the miss-rate, tstart(j, k) and t end (j, k) from Equation 15 and 16 can be used. Since tstart(j, k) and t end (j, k) specify the flat segments in <ref type="figure" target="#fig_3">Figure 4</ref>, the miss-rate of Process i can be rewritten by</p><formula xml:id="formula_20">miss-ratei = 1 Ti { Z T i 0 mi(M IN [M −1 i (t + Mi(xi(0))), C])dt + l i (t)+1 X k=d i mi(x φ i (tstart(i, k) + (k − 1) · Ti)) · (M IN [t end (i, k), Ti] − tstart(i, k))}<label>(19)</label></formula><p>where di is the minimum integer value that satisfies tstart(i, di) &lt; Ti. T i is the time that Process i actually grows.</p><formula xml:id="formula_21">T i = Ti − l i (t)+1 X k=d i (M IN [t end (i, k), Ti] − tstart(i, k)).<label>(20)</label></formula><p>As shown above, calculating a miss-rate could be complicated if we do not flush a cache at a context switch. If we assume that the executing process' data left in a cache is all in the most recently used part of the cache, we can use the  equation for estimating the amount of data starting with an empty cache. Therefore, the calculation can be much simplified as follows,</p><formula xml:id="formula_22">miss-ratei = 1 Ti Z T i 0 mi(M IN [M −1 i (t + Mi(xi(0))), C])dt<label>(21)</label></formula><p>where xi(0) is estimated from Equation 14. The effect of this approximation is evaluated in the experiment section (cf. Section 3.3).</p><p>Once we calculate the miss-rate of each process, the overall miss-rate is straightforwardly calculated from those missrates.</p><formula xml:id="formula_23">Overall miss-rate = P N i=1 miss-ratei · Ti P N i=1 Ti<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Verification</head><p>Our cache model can be validated by comparing estimated miss-rate predictions with simulation results. Several combinations of benchmarks are modeled and simulated for various time quanta. First, we simulate cases when a cache gets flushed at every context switch, and compare the results with the model's estimation. Cases without cache flushing are also tested. For the cases without cache flushing, both the complete model <ref type="bibr">(Equation 19</ref>) and the approximation (Equation 21) are used to estimate the overall miss-rate. Based on the simulation results, the error caused by the approximation is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Cache Flushing Case</head><p>The results of the cache model and simulations are shown in <ref type="figure">Figure 5</ref> in cases when a process starts its time quantum with an empty cache. Four benchmarks from SPEC CPU2000 <ref type="bibr" target="#b6">[7]</ref>, which are vpr, vortex, gcc and bzip2, are tested. The cache is a 32-KB fully-associative cache with 32-Byte blocks. The miss-rate of a process is plotted as a function of the length of a time quantum, and shows a good agreement between the model's estimation and the simulation result.</p><p>As inputs to the cache model, the average miss-rate of each process has been obtained from simulations. Each process has been simulated for 25 million memory references, and the miss-rates of the process for various cache size have been recorded. The simulation results were also obtained by simulating benchmarks for 25 million memory references with flushing a cache every T memory references. As the result shows, the average miss-rate works very well. <ref type="figure">Figure 6</ref> shows the result of the cache model when two processes are sharing a cache. The two benchmarks are vpr and vortex from SPEC CPU2000, and the cache is a 32-KB fully-associative cache with 32-Byte blocks. The overall miss-rates are shown in <ref type="figure">Figure 6</ref> (a). As shown in the figure, the miss-rate estimated by the model shows a good agreement with the results of the simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">General Case</head><p>The figure also shows an interesting fact that a certain range of time quanta could be very problematic for cache performance. For short time quanta, the overall miss-rate is relatively small. For very long time quanta, context switches do not matter since a process spends most of its time in the steady state. However, medium time quanta could severely degrade cache miss-rates as shown in the figure. This problem occurs when a time quantum is long enough to pollute the cache but not long enough to compensate for the misses caused by context switches. The problem becomes clear in <ref type="figure">Figure 6</ref> (b). The figure shows the amount of data left in the cache at the beginning of a time quantum. Comparing <ref type="figure">Figure 6</ref> (a) and (b), we can see that the problem occurs when the initial amount of data rapidly decreases.</p><p>The error caused by our approximation (Equation 21) method can be seen in <ref type="figure">Figure 6</ref>. In the approximation, we assume that the data left in the cache at the beginning of a time quantum are all in the MRU region of the cache. In reality, however, the data left in the cache could be the LRU cache blocks and get replaced before other process' blocks in the cache, although the current process's data are likely to be accessed in the time quantum. As a result, the approximated miss-rate is lower than the simulation result when the initial amount of data is not zero. A four-process case is also tested in <ref type="figure" target="#fig_6">Figure 7</ref>. Two more benchmarks, gcc and bzip2, from SPEC CPU2000 <ref type="bibr" target="#b6">[7]</ref> are added to vpr and vortex, and the same cache configuration is used as the two process case. The figure also shows a very close agreement between the miss-rate estimated by the cache model and the miss-rate from simulations. The problematic time quanta and the effect of the approximation have changed. Since there are more processes polluting the cache as compared to the two process case, a process experiences an empty cache in shorter time quanta. As a result, the problematic time quanta become shorter. On the other hand, the effect of the approximation is less harmful in this case. This is because the error in one process' miss-rate becomes less important as we have more processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CACHE PARTITIONING</head><p>This section shows how the analytical cache model can be used to dynamically partition the cache. A partitioned cache allocates cache space to particular processes. This space is dedicated to the process and cannot be used to satisfy cache misses by other processes. Using trace-driven simulations, we compare partitioning with the normal LRU. The partitioning is based on the fully-associative cache model. However, simulation results demonstrate that this implementation works for both fully-associative caches and setassociative caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recording Memory Reference Patterns</head><p>The miss-rate curves for each process are generated off-line. We record the miss-rate curve for each process to represent its memory reference pattern. For various cache sizes, a single process cache simulator is applied to each process. This information can be reused for any combination of processes as long as the cache configuration is the same 2 .</p><p>To incorporate the dynamically changing behavior of a process, a set of miss-rate curves, one for each time period, are 2 Note that for our fully-associative model, only the cache block size matters  <ref type="figure">Figure 8</ref>: The implementation of on-line cache partitioning.</p><p>produced. At run-time, the miss-rate curve is mapped to the appropriate time quantum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Partitioning Scheme</head><p>The overall flow of the partitioning scheme can be viewed as a set of four modules: off-line recording, scheduler information, allocation, and replacement ( <ref type="figure">Figure 8</ref>). The scheduler provides the partition module with the set of executing processes and their start/end times. The partition module uses the miss-rate information for the processes to calculate cache partitions at the end of each time quantum. Finally, the replacement unit maps these partitions to the appropriate parts of the cache.</p><p>The partition module decides the number of cache blocks that should be dedicated to a process (Di). The Di most recently used cache blocks of Process i are kept in the cache over other process' time quanta, and Process i starts its time quantum with those cache blocks in the cache. During its own time quantum, Process i can use all cache blocks that are not reserved for other processes (S = C − P N j=1,j񮽙 =i Dj).</p><p>In addition to LRU information, our replacement decision depends on the number of cache blocks that currently belong to each process (Xi), that is, the number of cache lines in the cache that currently contain memory of that process. The LRU cache block of an active process (i) is chosen if its actually allocation (Xi) is larger than or equal to the desired one (Di + S ≤ Xi). Otherwise, the LRU cache block of a dormant overallocated process is chosen. For set-associative caches, there may be no cache block of the desired process in the set. In this case, the LRU cache block of the set is replaced.</p><p>For set-associative caches, the fully-associative replacement policy may result in replacing recently used data to keep useless data. Imagine the case when a process starts to heavily access two or more addresses that happen to be mapped to the same set. If the process already has many cache blocks in other sets, our partitioning will allocate only a few cache blocks in the accessed set for the process, causing lots of conflict misses. To solve this problem, we can use better mapping functions <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b5">6]</ref> or a victim cache <ref type="bibr" target="#b7">[8]</ref>.</p><p>When a Process i first starts, Di is set to zero since there is no cache block that belongs to the process. At the end of Process i's time quantum, the partition module updates the information such as the miss-rate curve(mi(x)) and the time quantum <ref type="bibr">(Ti)</ref>. If there is any change, Di is also updated based on the cache model. A cache partition specifies the amount of data in the cache at the beginning of a process' time quantum <ref type="bibr">(Di)</ref>, and the maximum cache space the process can use (C − P N j=1,j񮽙 =i Dj ). Therefore, the number of misses for a process over one time quantum can be estimated from Equation 21:</p><formula xml:id="formula_24">missi = Z T i 0 mi(M IN [M −1 i (t + Mi(Di)), C − N X j=1,j񮽙 =i Dj ])dt (23)</formula><p>where C is cache size, and N is the number of processes sharing the cache.</p><p>The new value of Di is the integer, in the range <ref type="bibr">[0, Xi]</ref>, that minimizes the total number of misses that is given by the following quantity:</p><formula xml:id="formula_25">N X p=1 Z Tp 0 mp(M IN [M −1 p (t + Mp(Dp)), C − N X q=1,q񮽙 =p Dq])dt.</formula><p>(24)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Verification</head><p>The case of eight processes sharing a 32-KB cache is simulated to evaluate model-based partitioning. Seven benchmarks (bzip2, gcc, swim, mesa, vortex, vpr, twolf) are from SPEC CPU2000 <ref type="bibr" target="#b6">[7]</ref>, and one (the image understanding program (iu)) is from a data intensive systems benchmark suite <ref type="bibr" target="#b15">[15]</ref>. The overall miss-rate with partitioning is compared to the miss-rate only using the normal LRU replacement policy.</p><p>The simulations are carried out for fifty million memory references for each time quantum. Processes are scheduled in a round-robin fashion with the fixed number of memory references per time quantum. Also, the number of memory references per time quantum is assumed to be the same for the all eight processes. Finally, two record cycles (P ), of ten million and one hundred thousand memory references, respectively, are used for the model-based partitioning. The record cycle represents how often the miss-rate curve is recorded for the off-line profiling. Therefore, a shorter record cycle implies more detailed information about a process' memory reference pattern.</p><p>The characteristics of the benchmarks are illustrated in <ref type="figure" target="#fig_7">Fig- ure 9</ref>. <ref type="figure" target="#fig_7">Figure 9</ref> (a) shows the change of a miss-rate over time. The x-axis represents simulation time. The y-axis represents the average miss-rate over one million memory references at a given time. As shown in the figure, bzip2, gcc, swim and iu show abrupt changes in their miss-rate, whereas other benchmarks have very uniform miss-rate characteristics over time. <ref type="figure" target="#fig_7">Figure 9</ref> (b) illustrates the miss-rate as a function of the cache size. For a 32-KB fully-associative cache, benchmarks show miss-rates between 1% and 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Fully-Associative Result</head><p>The results of cache partitioning for a fully-associative cache are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. In <ref type="figure" target="#fig_0">Figure 10</ref> (a), the miss-rates are  averaged over 50 million memory references and shown for various time quanta. As discussed in the cache model, the normal LRU replacement policy is problematic for a certain range of time quanta. In this case, the overall miss-rate increases dramatically for time quanta between one thousand and ten thousand memory references. For this problematic region, the model-based partitioning improves the cache miss-rate by lowering it from 4.6% to 3.4%, which is about a 25% improvement. For short time quanta, the relative improvement is about 7%. For very long time quanta, the model-based partitioning shows the exact same result as the normal LRU replacement policy. In general, it is shown by the figure that the model-based partitioning always performs at least as well as or better than the normal LRU replacement policy. Also, the partitioning with a short record cycle performs better than the partitioning with a long record cycle.</p><p>In our example of a 32-KB cache with eight processes ( ure 10), the problematic time quanta are in the order of a thousand memory references, which is very short for modern microprocessors. As a result, only systems with very fast context switching, such as simultaneous multi-threading machines <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b3">4]</ref>, can be improved for this cache size and workload. However, longer time quanta become problematic if a cache is larger. Therefore, conventional time-shared systems with very high clock frequency can also be improved by the same technique if a cache is large. In the figure, the model-based partitioning with the long record cycle (P = 10 7 ) performs worse than LRU at the beginning of a simulation, even though it outperforms the normal LRU replacement policy overall. This is because the model-based partitioning has only one average miss-rate curve for a process. As shown in <ref type="figure" target="#fig_7">Figure 9</ref>, some benchmarks such as bzip2 and gcc have a very different miss-rate at the beginning. Therefore, the average miss-rate curves for those benchmarks do not work at the beginning of the simulation, which results in worse performance than the normal LRU replacement policy. The model-based partitioning with the short record cycle (P = 10 5 ), on the other hand, always outperforms the normal LRU replacement policy. In this case, the model has correct miss-rate curves for all the time quanta, and partitions the cache properly even for the beginning of processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Set-Associative Result</head><p>The result of cache partitioning for a set-associative cache is shown in <ref type="figure" target="#fig_0">Figure 11</ref>. The same set of benchmarks are simulated with a 32-KB 8-way set-associative cache, and the same miss-rate curves generated for a 32-KB fully-associative cache are used. In this case, a 16 entry victim cache is added. In the figure, the model-based partitioning improves the miss-rate about 4% for short time quanta and up to 15% for mid-range time quanta. The figure demonstrates that the model-based partitioning mechanism works reasonably well for set-associative caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>An analytical cache model to estimate overall miss-rate when multiple processes are sharing a cache has been presented. The model obtains the information about each process from its miss-rate curve, and combines it with parameters that define the cache configuration and schedule of processes. Interference among processes under the LRU replacement policy is quickly estimated for any cache size and any time quantum, and the estimated miss-rate is very accurate. A more important result is that the model provides not only the overall miss-rate but also a very good understanding of the effect of context switching. For example, the model clearly shows that the LRU replacement policy is problematic for mid-range time quanta because the policy replaces the blocks of least recently executed process that are more likely to be accessed in the near future.</p><p>The analytical model has been applied to the cache partitioning problem. A model-based partitioning method has been implemented and verified by simulations. Miss-rate curves are recorded off-line and partitioning is performed on-line according to the combination of processes that are executing. Even though we have used an off-line profiling method to obtain miss-rate curves, it should not be hard to approximate the miss-rate curve on-line using a miss-rate monitoring technique. Therefore, a fully on-line cache partitioning method can be developed based on the model.</p><p>Only the cache partitioning problem has been studied in this paper. However, as shown by the study of cache partitioning, our model can be applied to any cache optimization problem that is related to the problem of context switching. For example, it can be used to determine the best combination of processes that can be run on each processor of a multi-processor system. Also, the model is useful to identify areas in which further research in improving cache performance would be fruitful since it can easily provide the maximum improvement we can expect in the area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The overview of an analytical cache model. (b) Round-robin schedule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The probability of a miss at time t0. (b) The number of misses from Pmiss(t) curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The snapshot of a cache after running Process i for time t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The relation between x φ i (t) and xi(t). xi(0) is the amount of Process i's data in the cache when a time quantum starts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The result of the cache model for cache flushing cases. (a) vpr. (b) vortex. (c) gcc. (d) bzip2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The overall miss-rate when four processes (vpr, vortex, gcc, bzip2) are sharing a cache (32 KB, fully-associative).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The characteristics of the benchmarks. (a) The change of a miss-rate over time. (b) The miss-rate as a function of the cache size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The results of the model-based cache partitioning for a set-associative cache when eight processes (bzip2, gcc, swim, mesa, vortex, vpr, twolf, iu) are sharing the cache (32 KB, 8-way associative).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 (</head><label>10</label><figDesc>Figure 10 (b) shows the change of a miss-rate over time rather than an average miss-rate over the entire simulation. It is clear from the figure how the short record cycle helps partitioning. In the figure, the model-based partitioning with the long record cycle (P = 10 7 ) performs worse than LRU at the beginning of a simulation, even though it outperforms the normal LRU replacement policy overall. This is because the model-based partitioning has only one average miss-rate curve for a process. As shown in Figure 9, some benchmarks such as bzip2 and gcc have a very different miss-rate at the beginning. Therefore, the average miss-rate curves for those benchmarks do not work at the beginning of the simulation, which results in worse performance than the normal LRU replacement policy. The model-based partitioning with the short record cycle (P = 10 5 ), on the other hand, always outperforms the normal LRU replacement policy. In this case, the model has correct miss-rate curves for all the time quanta, and partitions the cache properly even for the beginning of processes.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An analytical cache model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Compaq alphastation family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Compaq</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-03" />
			<publisher>John &amp; Sons, Incorporated</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: A platform for next-generation processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The hewlett packard PA-RISC 8500 processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freeburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-10" />
		</imprint>
		<respStmt>
			<orgName>Hewlett Packard Laboratories</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eliminating cache conflict misses through XOR-based placement functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Parcerisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 1997 international conference on Supercomputing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SPEC CPU2000: Measuring CPU performance in the new millennium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 17th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Process dependent static cache partitioning for real-time systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time Systems Symposium</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hurson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effects of multithreading on cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Converting thread-level parallelism to instruction-level parallelism via simultaneous multithreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient memory simulation in SimICS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Annual Simulation Symposium</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Inc. MIPS R10000 Microprocessor User&apos;s Manual</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The effect of context switches on cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the fourth international conference on Architectural support for programming languages and operating systems</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data-Intensive Systems Benchmark Suite Analysis and Specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muoz</surname></persName>
		</author>
		<ptr target="http://www.aaec.com/projectweb/dis" />
		<imprint>
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complete computer system simulation: The SimOS approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Herrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Parallel &amp; Distributed Technology</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using processor-cache affinity information in shared-momory multiprocessor scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Squillante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal partitioning of cache memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1992-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Set-associative cache models for time-shared systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Report CSG Memo</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Footprints in the cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thiébaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1987-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving disk cache hit-ratios through cache partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thiébaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1992-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randomized cache placement for eleminating conflicts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benefits of cache-affinity scheduling in shared-memory multiprocessors: A summary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 1993 ACM SIGMETRICS conference on Measurement and modeling of computer systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: Maximizing on-chip parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trace-driven memory simulation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance analysis using the MIPS R1000</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zagha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Itzkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing&apos;96</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
