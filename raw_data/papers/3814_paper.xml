<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient classification for metric data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee-Ad</forename><surname>Gottlieb</surname></persName>
							<email>lee-ad.gottlieb@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit1">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryeh</forename></persName>
							<email>karyeh@cs.bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit1">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Kontorovich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit1">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krauthgamer</surname></persName>
							<email>robert.krauthgamer@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit1">Weizmann Institute of Science</orgName>
								<orgName type="institution" key="instit2">Ben Gurion University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient classification for metric data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent advances in large-margin classification of data residing in general metric spaces (rather than Hilbert spaces) enable classification under various natural metrics, such as edit and earthmover distance. The general framework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004] left open the question of computational efficiency and providing direct bounds on classification error. We design a new algorithm for classification in general metric spaces, whose runtime and accuracy depend on the doubling dimension of the data points. It thus achieves superior classification performance in many common scenarios. The algorithmic core of our approach is an approximate (rather than exact) solution to the classical problems of Lipschitz extension and of Nearest Neighbor Search. The algorithm&apos;s generalization performance is established via the fat-shattering dimension of Lipschitz classifiers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recent line of work extends the large-margin classification paradigm from Hilbert spaces to less structured ones, such as Banach or even metric spaces <ref type="bibr">[HBS05, vLB04, DL07]</ref>. In this metric approach, data is presented as points with distances but without requiring the additional structure of inner products. The potentially significant advantage is that the metric can be carefully suited to the type of data, e.g. earthmover distance for images, or edit distance for sequences.</p><p>However, much of the existing machinery of generalization bounds <ref type="bibr">[CV95, SS02]</ref> depends strongly on the inner-product structure of the Hilbert space. von <ref type="bibr">Luxburg and Bousquet [vLB04]</ref> developed a powerful framework of large-margin classification for a general metric space X . First, they show that the natural hypotheses (classifiers) to consider in this context are maximally smooth Lipschitz functions; indeed, they reduce classification (of points in a metric space X ) to finding a Lipschitz function (f : X → R) consistent with the data, which is a classic problem in Analysis, known as Lipschitz extension. Next, they establish error bounds in the form of expected-loss. Finally, the computational problem of evaluating the classification function is reduced, assuming zero training error, to exact 1-nearest neighbor search. This matches a common classification heuristic, see e.g. <ref type="bibr">[CH67]</ref>, and the analysis of <ref type="bibr">[vLB04]</ref> may be viewed as a rigorous explanation for the empirical success of this heuristic.</p><p>An important question left open by the work of <ref type="bibr">[vLB04]</ref> is the efficient computation of the classifier. Specifically, exact nearest neighbor search in general metrics might require time that is linear in the sample size, and it is algorithmically nontrivial to deal with training error. In particular, the task of choosing which points will be misclassified by the hypothesis (i.e. optimizing the bias-variance tradeoff) remains to be addressed.</p><p>Our contribution. We solve the problems delineated above by showing that data with a low doubling dimension admits accurate and computationally efficient classification. In fact, this is the first time in which the doubling dimension of the data points is tied to either classification error or algorithmic runtime. (Previously, the doubling dimension of the space of classifiers was controlled by the VC dimension of the classifier space <ref type="bibr">[BLL09]</ref>.) We first give an alternate generalization bound for Lipschitz classifiers, which directly bounds the classification error, rather than expected loss. (A similar bound can in fact be derived from the analysis of <ref type="bibr">[vLB04]</ref>.) Our bound is based on an elementary analysis of the fat-shattering dimension, see Section 3.</p><p>We then present our main contribution, and give an efficient computational implementation of the Lipschitz classifier. In Section 4 we prove that once a Lipschitz classifier has been chosen, the classifier can be computed (evaluated) quickly on any new point x ∈ X , by utilizing approximate nearest neighbor search (which is known to be fast when points have a low doubling dimension). In Section 5 we further show how to quickly compute a near-optimal classifier (in terms of classification error bound), even when the training error is nonzero. In particular, this necessitates the optimization of the number of incorrectly labeled examples -and moreover, their identity -as part of the bias-variance tradeoff. In Section 6 we give an example to illustrate the potential power of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions and notation</head><p>We use standard notation and definitions throughout.</p><p>Metric spaces. A metric ρ on a set X is a positive symmetric function satisfying the triangle inequality ρ(x, y) ≤ ρ(x, z) + ρ(z, y); together the two comprise the metric space (X , ρ). The diameter of a set A ⊆ X , is defined by diam(A) = sup x,y∈A ρ(x, y). The Lipschitz constant of a function f : X → R, denoted by f Lip , is defined to be the smallest L &gt; 0 that satisfies |f (x) − f (y)| ≤ Lρ(x, y) for all x, y ∈ X .</p><p>Doubling dimension. For a metric (X , ρ), let λ be the smallest value such that every ball in X can be covered by λ balls of half the radius. The doubling dimension of X is ddim(X ) = log 2 λ. A metric is doubling when its doubling dimension is bounded. Note that while a low Euclidean dimension implies a low doubling dimension (Euclidean metrics of dimension d have doubling dimension O(d) <ref type="bibr">[GKL03]</ref>), low doubling dimension is strictly more general than low Euclidean dimension.</p><p>The following packing property can be demonstrated via a repetitive application of the doubling property: For set S with doubling dimension ddim(X ), if the minimum interpoint distance in S is at least α, and diam(S) ≤ β, then |S| ≤ ⌈β/α⌉ ddim(X )+1 (see, for example <ref type="bibr">[KL04]</ref>).</p><p>Learning. Our setting in this paper is a generalization of PAC known as probabilistic concept learning <ref type="bibr">[KS94]</ref>. In this model, examples are drawn independently from X × {−1, 1} according to some unknown probability distribution P , and the learner, having observed n such pairs (x, y) produces a hypothesis h :</p><formula xml:id="formula_0">X → {−1, 1}.</formula><p>The generalization error is the probability of misclassifying a new point drawn from P :</p><formula xml:id="formula_1">P {(x, y) : h(x) 񮽙 = y} .</formula><p>The quantity above is random (since it depends on a random sequence) and we wish to upper-bound it in probability. Most bounds of this sort contains a sample error term (corresponding in statistics to bias), which is the fraction of observed examples misclassified by h and a hypothesis complexity term (corresponding to variance in statistics) which measures the richness of the class of all admissible hypotheses <ref type="bibr">[Was06]</ref>. Keeping in line with the literature, we ignore the measure-theoretic technicalities associated with taking suprema over uncountable function classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalization bounds</head><p>In this section, we take a preliminary step towards our efficient classification algorithm by deriving generalization bounds for Lipschitz classifiers over doubling spaces. As noted by <ref type="bibr">[vLB04]</ref> Lipschitz functions are the natural object to consider in an optimization/regularization framework. The basic intuition behind our proofs is that the Lipschitz constant plays the role of the inverse margin in the confidence of the classifier. As in <ref type="bibr">[vLB04]</ref>, small Lipschitz constant corresponds to large margin, which in turn yields low hypothesis complexity and variance. In retrospect, our generalization bound (Corollary 5 below) can be derived as a consequence of [vLB04, <ref type="bibr">Theorem 18]</ref> in conjunction with [BM02, Theorem 5(b)]. We apply tools from generalized Vapnik-Chervonenkis theory to the case of Lipschitz classifiers. Let F be a collection of functions f : X → R and recall the definition of the fat-shattering dimension [ABCH97, BS99]: a set X ⊂ X is said to be γ-shattered by F if there exists some function r : X → R such that for each label assignment y ∈ {−1, 1}</p><p>X there is an f ∈ F satisfying y(x)(f (x) − r(x)) ≥ γ &gt; 0 for all x ∈ X. The γ-fat-shattering dimension of F , denoted by fat γ (F ), is the cardinality of the largest set γ-shattered by F .</p><p>For the case of Lipschitz functions, we will show that the notion of fat-shattering dimension may be somewhat simplified. We say that a set X ⊂ X is γ-shattered at zero by a collection of functions F if for each</p><formula xml:id="formula_2">y ∈ {−1, 1} X there is an f ∈ F satisfying y(x)f (x) ≥ γ for all x ∈ X.</formula><p>(This is the definition above with r ≡ 0.) We write fat 0 γ (F ) to denote the cardinality of the largest set γ-shattered at zero by F and show that for Lipschitz function classes the two complexity measures are the same.</p><formula xml:id="formula_3">Lemma 1 Let F be the collection of all f : X → R with f Lip ≤ L. Then fat γ (F ) = fat 0 γ (F ).</formula><p>Proof: We begin by recalling the classic Lipschitz extension result, essentially due to McShane and Whitney <ref type="bibr">[McS34, Whi34]</ref>. Any real-valued function f defined on a subset X of a metric space X has an extension f * to all of X satisfying f * Lip = f Lip . Thus, in what follows we will assume that any function f defined on X ⊂ X is also defined on all of X via some Lipschitz extension (in particular, to bound f Lip it suffices to bound the restricted f | X Lip ).</p><p>Consider some finite X ⊂ X . If X is γ-shattered at zero by F then by definition it is also γ-shattered. Now assume that X is γ-shattered by F . Thus, there is some function r : X → R such that for each y ∈ {−1, 1}</p><p>X there is an f = f r,y ∈ F such that f r,y (x) ≥ r(x) + γ if y(x) = +1 and f r,y (x) ≤ r(x) − γ if y(x) = −1. Let us define the functioñ f y on X and as per above, on all of X , by˜fby˜ by˜f y (x) = γy(x). It is clear that the collectioñ collectioñ f y : y ∈ {−1, 1} X γ-fat-shatters X at zero; it only remains to verify that˜fthat˜ that˜f y ∈ F, i.e.,</p><formula xml:id="formula_4">sup y∈{−1,1} X ˜ f y Lip ≤ sup y∈{−1,1} X f r,y Lip .</formula><p>Indeed,</p><formula xml:id="formula_5">sup y∈{−1,1} X ,x,x ′ ∈X f r,y (x) − f r,y (x ′ ) ρ(x, x ′ ) ≥ sup x,x ′ ∈X r(x) − r(x ′ ) + 2γ ρ(x, x ′ ) ≥ sup x,x ′ ∈X 2γ ρ(x, x ′ ) = sup y∈{−1,1} X ˜ f y Lip .</formula><p>A consequence of Lemma 1 is that in considering the generalization properties of Lipschitz functions we need only bound the γ-fat-shattering dimension at zero. The latter follows from the observation that the packing number of a metric space controls the fat-shattering dimension of Lipschitz functions defined over the metric space. Let M (X , ρ, ε) be defined as the ε-packing number of X , the cardinality of the largest ε-separated subset of X .</p><p>Theorem 2 Let (X , ρ) be a metric space. Fix some L &gt; 0, and let F be the collection of all f :</p><formula xml:id="formula_6">X → R with f Lip ≤ L. Then for all γ &gt; 0, fat γ (F ) = fat 0 γ (F ) ≤ M (X , ρ, 2γ/L).</formula><p>Proof: Suppose that S ⊆ X is fat γ-shattered at zero. The case |S| = 1 is trivial, so we assume the existence of x 񮽙 = x ′ ∈ S and f ∈ F such that f (x) ≥ γ &gt; −γ ≥ f (x ′ ). The Lipschitz property then implies that ρ(x, x ′ ) ≥ 2γ/L, and the claim follows.</p><p>Corollary 3 Let metric space X have doubling dimension ddim(X ), and let F be the collection of realvalued functions over X with Lipschitz constant at most L. Then for all γ &gt; 0,</p><formula xml:id="formula_7">fat γ (F ) ≤ Ldiam(X ) 2γ ddim(X )+1 .</formula><p>Proof: The claim follows immediately from Theorem 2 and the packing property of doubling spaces.</p><p>Equipped with these estimates for the fat-shattering dimension of Lipschitz classifiers, we can invoke a standard generalization bound stated in terms of this quantity. For the remainder of this section, we take γ = 1 and say that a function f classifies an example (x i , y i ) correctly if</p><formula xml:id="formula_8">y i f (x i ) ≥ 1.<label>(1)</label></formula><p>The following generalization bounds appear in [BS99]:</p><p>Theorem 4 Let F be a collection of real-valued functions over some set X , define d = fat 1/16 (F ) and let and P be some probability distribution on X × {−1, 1}. Suppose that (x i , y i ), i = 1, . . . , n are drawn from X × {−1, 1} independently according to P and that some f ∈ F classifies the n examples correctly, in the sense of (1). Then with probability at least 1 − δ P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ 2 n (d log 2 (34en/d) log 2 (578n) + log 2 (4/δ)) .</p><p>Furthermore, if f ∈ F is correct on all but k examples, we have with probability at least 1 − δ</p><formula xml:id="formula_9">P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).</formula><p>Applying Corollary 3, we obtain the following consequence of Theorem 4:</p><p>Corollary 5 Let metric space X have doubling dimension ddim(X ), and let F be the collection of realvalued functions over X with Lipschitz constant at most L. Then for any f ∈ F that classifies a sample of size n correctly, we have with probability at least 1 − δ P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ 2 n (d log 2 (34en/d) log 2 (578n) + log 2 (4/δ)) .</p><p>Likewise, if f is correct on all but k examples, we have with probability at least 1 − δ</p><formula xml:id="formula_10">P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).<label>(2)</label></formula><p>In both cases,</p><formula xml:id="formula_11">d = fat 1/16 (F ) ≤ ⌈8Ldiam(X )⌉ ddim(X )+1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lipschitz extension classifier</head><p>Given a labeled set (X, Y ) ⊂ X ×{−1, 1}, we construct our classifier in a similar manner to [vLB04, <ref type="bibr">Lemma 12]</ref>, via a Lipschitz extension of the labels Y to all of X . Let S + , S − ⊂ X be the sets of positive and negative labeled points that the classifier correctly labels. Our starting point is the same extension function used in <ref type="bibr">[vLB04]</ref>, namely, for all α ∈ [0, 1]</p><formula xml:id="formula_12">f α = α min i y i + 2 d(x, x i ) d(S + , S − ) + (1 − α) max j y j − 2 d(x, x j ) d(S + , S − ) .</formula><p>However, evaluating the exact value of f α (x) for each point x ⊂ X (or even the sign of f α (x) at this point) requires an exact nearest neighbor search, and in arbitrary metric space nearest neighbor search may require Θ(|X|) time.</p><p>In this section, we give a classifier whose sign can be evaluated using a (1 + ε)-approximate nearest neighbor search. There exists a search structure for an n point set that can be built in 2 O(ddim(X )) n log n time and supports approximate nearest neighbor searches in time 2 O(ddim(X )) log n+ε −O(ddim(X )) <ref type="bibr">[CG06, HM06]</ref> (see also <ref type="bibr">[KL04, BKL06]</ref>). In constructing the classifier, we assume that the sample points have already been partitioned in a manner that yields a favorable bias-variance tradeoff, as in Section 5 below. Therefore, the algorithm below takes as input a set of point S 1 ⊂ X that must be correctly classified, and a set of error points S 0 = X − S 1 that may be ignored in the classifier construction (but which affect the resulting generalization bound).</p><p>Theorem 6 Let X be a metric space, and fix 0 &lt; ε ≤ 1 2 . Given a labeled sample S = (x i , y i ) ∈ X ×{−1, 1}, i = 1, . . . , n, let S be partitioned into S 0 and S 1 , of sizes k and n − k, where S 0 contains points that may be misclassified, and S 1 contains points that may not be misclassified. Define S + 1 , S − 1 ⊂ S 1 according to their labels and define L = 2/d(S + 1 , S − 1 ). Then there exists a binary classification function h : X → {−1, 1} satisfying the following: (a) h(x) can be evaluated at each x ∈ X via a single (1 + ε)-nearest neighbor query. In particular, h(x) can be evaluated in time 2 O(ddim(X )) log n + ε −O(ddim(X )) , after an initial computation of (2 O(ddim(X )) log n + ε −O(ddim(X )) )n time.</p><p>(b) With probability at least 1 − δ</p><formula xml:id="formula_13">P {(x, y) : h(x) 񮽙 = y} ≤ 2 k n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ))</formula><p>where</p><formula xml:id="formula_14">d = ⌈8(1 + ε)Ldiam(X )⌉ ddim(X )+1 .</formula><p>Proof: Let the distance functioñ d(·, ·) be the approximate distance between a point and a set (or between two sets), as determined by a fixed (1 + ε 4 )-nearest neighbor search structure. Let˜f</p><formula xml:id="formula_15">Let˜ Let˜f 1 (x) := min i y i + 2 ˜ d(x, x i ) ˜ d(S + 1 , S − 1 ) ,</formula><p>and let the classifier be h(x) := sgn( ˜ f 1 (x)). h(x) can be evaluated via an approximate nearest neighbor query in time 2 O(ddim(X )) log n + ε −O(ddim(X )) , assuming that a search structure has been precomputed in time 2 O(ddim(X )) n log n, and˜dand˜ and˜d(S + 1 , S − 1 ) has been precomputed via O(n) nearest neighbor searches in time (2 O(ddim(X )) log n + ε −O(ddim(X )) )n.</p><p>It remains to bound the generalization error of h. To this end, define</p><formula xml:id="formula_16">f + 1 (x) = (1 + ε)f 1 (x) + ε = (1 + ε) min i y i + 2 d(x, x i ) d(S + 1 , S − 1 ) + ε, f − 1 (x) = (1 + ε)f 1 (x) − ε = (1 + ε) min i y i + 2 d(x, x i ) d(S + 1 , S − 1 ) − ε.</formula><p>Note that f + 1 (x) &gt; f − 1 (x). Both f + 1 (x) and f − 1 (x) correctly classify all labeled points of S 1 and have Lipschitz constant (1 + ε)L, so their classification bounds are given by Corollary 5 with this Lipschitz constant.</p><p>We claim that h(x) always agrees with the sign of at least one of f + 1 (x) and f − 1 (x): If f + 1 (x) and f − 1 (x) disagree in their sign, then the claim follows trivially. Assume then that the signs of f + 1 (x) and f − 1 (x) agree. Suppose that f + 1 (x) and f − 1 (x) are positive, which implies that y j + 2</p><formula xml:id="formula_17">d(x,xj) d(S + 1 ,S − 1 )</formula><p>&gt; ε 1+ε for all j. Now recall that˜fthat˜ that˜f 1 (x) = min i y i + 2</p><formula xml:id="formula_18">˜ d(x,xi) ˜ d(S + ,S − ) ≥ min i y i + 2 (1+ε/4) 2 d(x,xi) d(S + ,S − )</formula><p>. If y i = +1,</p><formula xml:id="formula_19">then trivially h(x) is positive. If y i = −1, we have that 2 d(x,xi) d(S + ,S − ) &gt; ε 1+ε + 1 = 1+2ε 1+ε</formula><p>, and sõ</p><formula xml:id="formula_20">f 1 (x) ≥ min i y i + 2 (1+ε/4) 2 d(x,xi) d(S + ,S − ) &gt; −1 + 1 (1+ε/4) 2 1+2ε 1+ε</formula><p>&gt; 0, and we are done. Suppose then that f + 1 (x) and f − 1 (x) are negative, which implies that y j + 2</p><formula xml:id="formula_21">d(x,xj) d(S + 1 ,S − 1 )</formula><p>&lt; − ε 1+ε for some fixed j. Now it must be that y j = −1, and so 2</p><formula xml:id="formula_22">d(x,xj) d(S + ,S − ) &lt; − ε 1+ε + 1 = 1 1+ε . Now recall that˜fthat˜ that˜f 1 (x) = min i y i + 2 ˜ d(x,xi) ˜ d(S + ,S − ) ≤ y j + 2(1 + ε/4) 2 d(x,xj) d(S + ,S − ) &lt; −1 + (1 + ε/4) 2 1 1+ε</formula><p>&lt; 0, and we are done.</p><p>It follows that if h(x) misclassifies x, then x must be misclassified by at least one of f + 1 (x) and f − 1 (x). Hence, the generalization bound of h(x) is not greater than the sum of the generalization bounds of f + 1 (x) and f − 1 (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bias-variance tradeoffs</head><p>In this section, we show how to efficiently construct a classifier that optimizes the bias-variance tradeoff implicit in Corollary 5, equation (2). Let X be a metric space, and assume we are given a labeled sample S = (x i , y i ) ∈ X × {−1, 1}. For any Lipschitz constant L, let k(L) be the minimal sample error of S over all classifiers with Lipschitz constant L. We rewrite the generalization bound as follows:</p><formula xml:id="formula_23">G(L) = P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ k(L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ))</formula><p>where d = ⌈8Ldiam(X )⌉ ddim(X )+1 . This bound contains a free parameter, L, which may be tuned to optimize the bias-variance tradeoff. More precisely, decreasing L drives the bias term (number of mistakes) up and the variance term (fat-shattering dimension) down. For some optimal values of L, G(L) achieves a minimum value. The following theorem gives our bias-variance tradeoff.</p><p>Theorem 7 Let X be a metric space. Given a labeled sample S = (x i , y i ) ∈ X × {−1, 1}, i = 1, . . . , n, there exists a binary classification function h : X → {−1, 1} satisfying the following properties:</p><p>(a) h(x) can be evaluated at each x ∈ X in time 2 O(ddim(X )) log n, after an initial computation of O(n 2 log n) time.</p><p>(b) The generalization error of h is bound by</p><formula xml:id="formula_24">P {(x, y) : sgn(f (x)) 񮽙 = y} ≤ c · inf L&gt;0 k(L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)) .</formula><p>for some constant c, and where</p><formula xml:id="formula_25">d = d(L) = ⌈8Ldiam(X )⌉ ddim(X )+1 .</formula><p>We proceed with a description of the algorithm. We will first give an algorithm with runtime O(n 4.376 ), and then improve the runtime to O(n 2 log n).</p><p>Algorithm description. Here we give a randomized algorithm that finds an optimal value L * , that is G(L * ) = inf L&gt;0 G(L). The runtime of this algorithm is O(n 4.376 ) with high probability.</p><p>First note the behavior of k(L) as L increases. k(L) may decrease when the value of L crosses some critical value: This critical value is determined by point pairs x i ∈ S + , x j ∈ S − (that is, L = 2 d(xi,xj) ) and implies that the classification function can correctly classify both these points. There are O(n 2 ) critical values of L, and these can be determined by enumerating all interpoint distances between sets S + , S − ⊂ S.</p><p>Below, we will show that for any given L, the value k(L) can be computed in randomized time O(n 2.376 ). More precisely, we will show how to compute a partition of S into sets S 1 (with Lipschitz constant L) and S 0 (of size k(L)) in this time. Given sets S 0 , S 1 ⊂ S, we can construct the classifier of Corollary 5. Since there are O(n 2 ) critical values of L, we can calculate k(L) for each critical value in O(n 4.376 ) total time, and thereby determine L * . Then by Corollary 5, we may compute a classifier with a bias-variance tradeoff arbitrarily close to optimal.</p><p>It is left to describe how value k(L) is computed for any L in randomized time O(n 2.376 ). Consider the following algorithm: Construct a bipartite graph G = (V + , V − , E). The vertex sets V + , V − correspond to the labeled sets S + , S − ∈ S, respectively. The length of edge e = (u, v) connecting vertices u ∈ V + and v ∈ V − is equal to the distance between the points, and E includes all edges of length less than 2/L. (E can be computed in O(n 2 log n) time.) Now, for all edges e ∈ E, a classifier with Lipschitz constant L necessarily misclassifies at least one endpoint of e. Hence, the problem of finding a classifier with Lipschitz constant L that misclassifies a minimum number of points in S is equivalent to finding a minimum vertex cover for bipartite graph G. By König's theorem, minimum bipartite vertex cover is itself equivalent to the maximum matching problem on bipartite graphs. An exact solution to the bipartite matching problem may be computed in randomized time O(n 2.376 ) <ref type="bibr">[MS04]</ref>. This solution immediately identifies sets S 0 , S 1 , which allows us to compute a classifier with a bias-variance tradeoff arbitrarily close to optimal.</p><p>Improved algorithmic runtime. The runtime given above can be reduced from randomized O(n 4.376 ) to deterministic O(n 2 log n), if we are willing to settle for a generalization bound G(L) within a constant factor of the optimal G(L * ).</p><p>The first improvement is in the runtime of the vertex cover algorithm. It is well known that a 2-approximation to the minimum vertex cover on an arbitrary graph can be computed by a greedy algorithm in time</p><formula xml:id="formula_26">O(|V + + V − | + |E|) = O(n 2 ) [GJ77]. Hence, we may evaluate in O(n 2 ) time a function k ′ (L) which satisfies k(L) ≤ k ′ (L) ≤ 2k(L).</formula><p>The second improvement uses a binary search over the values of L, which allows us to evaluate k ′ (L) for only O(log n) values of L, as opposed to all Θ(n 2 ) values above. Now, we seek the a value of L for which</p><formula xml:id="formula_27">G ′ (L) = k ′ (L)/n + 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)) is minimal. Call this value L ′ . Also note that for all L, G ′ (L) ≤ 2G(L), from which it follows that G ′ (L ′ ) ≤ 2G(L * ).</formula><p>While we cannot efficiently find L ′ , we are able to use a binary search to find a value L for which</p><formula xml:id="formula_28">G ′ (L) ≤ 2G ′ (L ′ ) ≤ 4G(L * ).</formula><p>In particular we seek the minimum value of L for which k ′ (L)/n ≤ 2 n (d ln(34en/d) log 2 (578n) + ln(4/δ)).</p><p>Now, decreasing L can only increase k ′ (L), so the solution to the inequality above necessarily yields an L for which</p><formula xml:id="formula_29">G ′ (L) ≤ 2G ′ (L ′ ) ≤ 4G(L * ).</formula><p>The solution to the inequality can be computed through a binary search on all values of L. By Corollary 5, we can construct a classifier with a bias-variance tradeoff within a factor 4(1 + ε) of optimal. The total runtime is O(n 2 log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Example: Earthmover metric</head><p>To illustrate the potential power of our approach, we now analyze the doubling dimension of an earthmover metric X k that is often used in computer vision applications. (k ≥ 2 is a parameter.) Each point in X k is a multiset of size k in the unit square in the Euclidean plane, formally S ⊂ [0, 1] 2 and |S| = k (allowing and counting multiplicities). The distance between such sets S, T (i.e. two points in X k ) is given by EMD(S, T ) = min</p><formula xml:id="formula_30">π:S→T 1 k s∈S s − π(s) 2 ,</formula><p>where the minimum is over all one-to-one mappings π : S → T . In other words, EMD(S, T ) is the minimum-cost matching between the two sets S, T , where costs correspond to Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 8</head><p>The earthmover metric X above satisfies diam(X k ) ≤ √ 2, and ddim(X k ) ≤ O(k log k).</p><p>Proof: For the rest of this proof, a point refers to the unit square, not X k . Fix r &gt; 0 and consider a ball (in X k ) of radius r around some S. Let N be an r/2-net of the unit square [0, 1] 2 . Now consider all multisets T of size k of the unit square which satisfy the following condition: every point in T belongs to the net N and is within (Euclidean) distance (k + 1/2)r from at least one point of S. Points in such a multiset T are chosen from a collection of size at most k · (k+1/2)r r/2 O(1) ≤ k O(1) (by the packing property in the Euclidean plane).</p></div>		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scale-sensitive dimensions, uniform convergence, and learnability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Noga Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nicoì O Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="631" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cover trees for nearest neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;06: Proceedings of the 23rd international conference on Machine learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using the doubling dimension to analyze the generalization of learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bshouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalization performance of support vector machines and other pattern classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in kernel methods: support vector learning</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching dynamic point sets in spaces with bounded doubling dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee-Ad</forename><surname>Gottlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="574" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-Margin Classification in Banach Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The rectilinear steiner tree problem is np-complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="826" to="834" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bounded geometries, fractals, and low-distortion embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="534" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximal margin classification for metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast construction of nets in low-dimensional metrics and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sariel</forename><surname>Har</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Peled</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manor</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1148" to="1184" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Navigating nets: simple algorithms for proximity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Krauthgamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA &apos;04: Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="798" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient distribution-free learning of probabilistic concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="464" to="497" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extension of range of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mcshane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximum matchings via gaussian elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Sankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS &apos;04: Proceedings of the 45th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distance-based classification with lipschitz functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ulrike Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="669" to="695" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">All of nonparametric statistics. Springer Texts in Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analytic extensions of differentiable functions defined in closed sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassler</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="89" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
