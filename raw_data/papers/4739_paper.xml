<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Performance Models for Evaluation and Automatic Tuning of Symmetric Sparse Matrix-Vector Multiply</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Computer Science Division Berkeley</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Vuduc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Computer Science Division Berkeley</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
							<email>demmel@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Computer Science Division Berkeley</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
							<email>yelick@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Computer Science Division Berkeley</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Performance Models for Evaluation and Automatic Tuning of Symmetric Sparse Matrix-Vector Multiply</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present optimizations for sparse matrix-vector multiply SpMV and its generalization to multiple vectors, SpMM, when the matrix is symmetric: (1) symmetric storage, (2) register blocking, and (3) vector blocking. Combined with register blocking, symmetry saves more than 50% in matrix storage. We also show performance speedups of 2.1× for SpMV and 2.6× for SpMM, when compared to the best non-symmetric register blocked implementation. We present an approach for the selection of tuning parameters , based on empirical modeling and search that consists of three steps: (1) Off-line benchmark, (2) Run-time search, and (3) Heuristic performance model. This approach generally selects parameters to achieve performance with 85% of that achieved with exhaustive search. We evaluate our implementations with respect to upper bounds on performance. Our model bounds performance by considering only the cost of memory operations and using lower bounds on the number of cache misses. Our optimized codes are within 68% of the upper bounds.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We present optimizations, an approach for tuning parameter selection, and performance analyses for sparse matrixvector multiply (SpMV), y ← y + A · x, where A is a symmetric, sparse matrix (i.e., A = A T ) and x,y are dense column vectors called the source and destination. We also consider the generalization of SpMV to multiple vectors where x,y are replaced by matrices X,Y , referring to this kernel as SpMM. Symmetry is traditionally exploited to reduce storage, but performance gains are also possible since the cost of memory accesses dominates the cost of flops on most modern cache-based superscalar architectures.</p><p>The challenge in efficient performance tuning for sparse computational kernels is the considerable variation in the best choice of sparse matrix data structure and code transformations across machines, compilers, and matrices, the latter of which may not be known until run-time. This paper describes a new implementation space for the symmetric case, considering symmetric storage (Section 3), register blocking (Section 3), and vector blocking (Section 4).</p><p>We present an empirical modeling and search procedure to select optimal tuning parameters that characterize a SpMM code for a given matrix and machine (Section 5). Our approach extends models of the SPARSITY system <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>. To evaluate the measured performance of our best implementations, we formulate upper bounds on performance (Section 6). We bound execution time from below by considering only the cost of memory accesses and by modeling data placement in the memory hierarchy for a lower bound on cache misses.</p><p>The following summarizes experimental results from four different computing platforms <ref type="table">(Table 1</ref>) and a test suite of twelve sparse symmetric matrices <ref type="table" target="#tab_2">(Table 2</ref>):</p><p>1. Symmetric register blocking achieves up to 2.1× speedups for SpMV and 2.6× speedups for SpMM over non-symmetric register and vector blocking.</p><p>2. Combining symmetry, register and vector blocking achieves up to 9.9× speedups for a dense matrix in sparse format and up to 7.3× for a true sparse matrix when compared to a na¨ıvena¨ıve code (no optimizations).</p><p>3. The empirical modeling and search procedure generally selects parameters that yield within 85% of the best performance achieved by an exhaustive search over all possible parameter values. 1</p><p>4. Measured performance achieve 68% of the performance upper bound, on average. <ref type="table" target="#tab_2">Intel  Intel  IBM  Property  Ultra 2i  Itanium 1  Itanium 2  Power 4  Clock rate (MHz)  333  800  900  1300  Peak Main Memory  664  2.1  6400  8000  Bandwidth (MB/s</ref> This paper summarizes the key findings of a recent technical report <ref type="bibr" target="#b2">[3]</ref>. We refer the reader to the report for further details. The empirical modeling and search procedure for tuning parameters does not appear in the report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sun</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Methodology</head><p>We conducted experiments on machines based on the microprocessors in <ref type="table">Table 1</ref>. Latency estimates were obtained from a combination of published sources and experimental measurements using the Saavedra-Barrera memory system microbenchmark <ref type="bibr" target="#b9">[10]</ref> and MAPS benchmarks <ref type="bibr" target="#b10">[11]</ref>. <ref type="table" target="#tab_2">Table 2</ref> summarizes the size and application of each symmetric matrix in the matrix benchmark suite used for evaluation. Most of the matrices are available from either the collections at NIST (MatrixMarket <ref type="bibr" target="#b11">[12]</ref>) or the University of Florida <ref type="bibr" target="#b12">[13]</ref>. The size of these matrices exceed the largest cache size for the evaluation platforms.</p><p>We use the PAPI v2.1 library for access to hardware counters on all platforms <ref type="bibr" target="#b13">[14]</ref> except Power 4; not all PAPI counters are available for the Power 4 and HPM counters overcount memory traffic. We use the cycle counters, reported as the median of 25 consecutive trials, as timers.</p><p>Presented performance in Mflop/s always uses "ideal" flop counts. That is, if a transformation of the matrix requires filling in explicit zeros (e.g. register blocking), arith-  metic with these extra zeros are not counted as flops when determining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimizations for Matrix Symmetry</head><p>The baseline implementation stores the full matrix in compressed sparse row (CSR) format and computes SpMV using a non-symmetric kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Symmetric Storage</head><p>Matrix symmetry enables storing only half of the matrix and, without loss of generality, our implementation stores the upper-triangle. Although the symmetric code requires the same number of floating point operations as the baseline, symmetry halves the number of memory accesses to the matrix: a symmetric implementation simultaneously applies each element and its transpose, processing only the stored half of the matrix. In both cases, stores to the destination are indirect and potentially irregular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symmetric Register Blocking</head><p>SPARSITY's register blocking is a technique for improving register reuse <ref type="bibr" target="#b6">[7]</ref>. The sparse m × n matrix is logically divided into aligned r × c blocks, storing only those blocks containing at least one non-zero. SpMV computation proceeds block-by-block. For each block, we can reuse the corresponding c elements of the source by keeping them in registers to increase temporal locality to the source, assuming a sufficient number are available.</p><p>Register blocking uses the blocked variant of compressed sparse row (BCSR) storage format. Blocks within the same block row are stored consecutively, and the elements of each block are stored consecutively in row-major order. When r = c = 1, BCSR reduces to CSR. BCSR potentially stores fewer column indices than CSR implementation (one per block instead of one per non-zero). The effect is to reduce memory traffic by reducing storage overhead. Furthermore, SPARSITY implementations fully unroll the r × c submatrix computation, reducing loop overheads and exposing scheduling opportunities to the compiler.</p><p>A uniform block size may require filling in explicit zero values, resulting in extra computation. We define the fill ratio to be the number of stored values (original non-zeros plus explicit zeros) divided by the number of non-zeros in the original matrix. The profitability of register blocking depends highly on the fill and the non-zero pattern of the matrix.</p><p>In our symmetric implementation of r × c register blocking, we use the following square diagonal blocking scheme ( <ref type="figure" target="#fig_0">Figure 1</ref>). Given a row-oriented storage scheme and r × c register blocks, the diagonal blocks are implemented as square r × r blocks. We align the register blocks from the right edge of the matrix, which may require small degenerate r × c blocks to the right of the diagonal blocks, where c &lt; c and c depends on the block row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimizations for Multiple Vectors</head><p>The baseline implementation, given k vectors, applies the unblocked symmetric SpMV kernel once for each vector. This implementation requires k accesses to the entire matrix and, for large matrices, brings the entire matrix through the memory hierarchy once for each vector.</p><p>Vector blocking is a technique for reducing memory traffic for the SpMM kernel, Y = Y + A · X, where A is a symmetric sparse n × n matrix, and X, Y are dense n × k matrices. <ref type="bibr" target="#b1">2</ref> The k vectors are processed in k v groups of 2 X, Y are collections of k dense column vectors of length n.</p><p>the vector width v and multiplication of each element of A is unrolled by v. The computation of SpMM proceeds sequentially across matrix elements or register blocks, computing results for the corresponding elements in each of the v destinations in the vector block. When v = 1, the subroutine is effectively a single vector implementation of SpMV. Thus, the matrix is accessed at most k v + 1 times in contrast to the k times required by the baseline. The effect is to reduce memory traffic and increase temporal locality to A by amortizing the cost of accessing a matrix element for v vectors. Furthermore, unrolling the multiply for the v vectors reduces loop overhead and exposes scheduling opportunities to the compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Automated Empirical Tuning</head><p>A register and vector blocked SpMM code is characterized by the parameters (r, c, v), indicating optimal register block size r×c and vector width v. The optimal parameters vary significantly across machines and matrices and are difficult to choose by purely analytic modeling <ref type="bibr" target="#b1">[2]</ref>.</p><p>Our approach to selecting (r, c, v) is based on empirical modeling and search, executed partly off-line and partly at run-time. Given a machine, a symmetric matrix A, and a number of vectors k, our tuning parameter selection procedure consists of the following 3 steps:</p><p>1. Off-line benchmark: Once per machine, measure the performance (in Mflop/s) of symmetric SpMM for a dense matrix stored in sparse format, for all (r, c, v) such that 1 ≤ r, c ≤ b max and 1 ≤ v ≤ v max , where k is set equal to v. In practice, we use b max = 8 and v max = 10. Denote this symmetric register profile by</p><formula xml:id="formula_0">{P rcv (dense) |1 ≤ r, c ≤ b max , 1 ≤ v ≤ v max }.</formula><p>2. Run-time "search": When the matrix A is known at run-time, compute an estimatê f rc (A) of the true fill ratio for all 1 ≤ r, c, ≤ b max . Estimating this quantity is a form of empirical search over possible block sizes, and depends only on the matrix non-zero structure.</p><p>3. Heuristic performance model: Choose (r, c, v) that maximizes the following estimate of register blocking performancê P rcv (A),</p><formula xml:id="formula_1">ˆ P rcv (A) = P rcv (dense) ˆ f rc (A) ,<label>(1)</label></formula><p>for all 1 ≤ r, c ≤ b max and 1 ≤ v ≤ min{v max , k}. Intuitively, P rcv (dense) is an empirical estimate of expected performance, andˆfandˆ andˆf rc (A) reduces this value according to the extra flops per non-zero due to fill.</p><p>The key idea is to decouple machine-specific aspects of performance which can be measured off-line (Step 1) from matrix-specific aspects determined at run-time (Step 2), combining these aspects with a heuristic model of performance evaluated at run-time <ref type="bibr">(Step 3)</ref>. This procedure adapts our prior technique for non-symmetric SpMV <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Trying all or even a subset of block sizes is infeasible if the matrix is known only at run-time, due to the cost of converting the matrix to blocked format. In contrast, the fill can be estimated accurately and cheaply <ref type="bibr" target="#b1">[2]</ref>, while the total run-time cost of executing Steps 2 and 3, followed by a single conversion to r×c blocked format, is not much greater than the cost of only the conversion <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Bounds on Performance</head><p>We present performance upper bounds to estimate the potential payoff from low-level tuning given a matrix and a data structure, but independent of instruction mix and ordering. These bounds extend prior bounds for non-symmetric SpMV <ref type="bibr" target="#b3">[4]</ref>. The performance model consists of a lower bound model of execution time and a lower bound model on cache misses at each level in the memory hierarchy. The following are underlying assumptions for these bounds:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Our lower bound model of execution time considers</head><p>only the cost of memory operations, taking SpMV and SpMM to be memory bound. Assuming write-back caches and sufficient store buffer capacity, we account only for the cost of loads and ignore the cost of stores.</p><p>2. For a hit in cache level i, we assign a cost α i to access the data at that level, as determined by microbenchmarks on streaming workloads likely to represent the fastest memory access patterns.</p><p>3. We further bound time from below by obtaining a lower bound on cache misses that considers only compulsory misses, accounts for cache line sizes, and assumes full associativity.</p><p>4. We further bound time from below by ignoring TLB misses. This assumption is justified by the predominantly streaming behavior of SpMV <ref type="bibr" target="#b1">[2]</ref>, but may lead to an optimistic bound in the multiple vector case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Lower Bound Execution Time Model</head><p>If the total execution time is T seconds, the performance P in Mflop/s is</p><formula xml:id="formula_2">P = 4kv T ×10 −6<label>(2)</label></formula><p>where k is the number of stored non-zeros in the n × n sparse matrix A (excluding any fill) and v is the vector width in the vector blocked implementation. To get an upper bound on performance, we require a lower bound on T . Consider a machine with κ cache levels, where the access latency at cache level i is α i in cycles, and the memory access latency is α mem . Let H i and M i be the number of cache hits and misses at each level i, respectively. Also, let L be the total number of loads. The execution time T is</p><formula xml:id="formula_3">T = κ i=1 αiHi+αmemMκ = α1L+ κ−1 i=1 (αi+1−αi)Mi+(αmem−ακ)Mκ<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">H 1 = L − M 1 and H i = M i−1 − M i for 2 ≤ i ≤ κ.</formula><p>According to Equation <ref type="formula" target="#formula_3">(3)</ref>, we can minimize T by minimizing M i , assuming α i+1 ≥ α i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Counting Load Operations</head><p>Let A be an m × m symmetric matrix with k stored non-zeros. Let D r be the number of r × r non-zero diagonal blocks, B rc be the number of r × c non-zero register blocks, and f rc be the fill ratio given these blocks. Let D r and B rc denote the total number of matrix elements (including filled zeros) stored in the diagonal and non-diagonal blocks, respectively. Every matrix element, row index, and column index must be loaded once. We assume that SpMM iterates over block rows in the stored upper triangle and that all r entries of the destinations can be loaded once for each of D r block rows and kept in registers for the duration of the block row multiply. We also assume that all c destination elements can be kept in registers during the multiplication of a given transpose block, thereby requiring B rc c additional loads from the destination. A similar analysis for the block columns in the transpose of the stored triangle yields rD r + cB rc loads. Thus, the number of loads, scaled for multiple vectors, is 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Lower Bounds on Cache Misses</head><p>Beginning with the L1 cache, let l 1 be the L1-cache line size, in double-precision words. One compulsory L1 read miss per cache line is incurred for every matrix element (value and index) and each of the mv destination elements. In considering the vectors, we assume the vector size is less than the L1 cache size, so that in the best case, only one compulsory miss per cache line is incurred for each of the 2mv source and destination elements. Thus, a lower bound M <ref type="formula" target="#formula_1">(1)</ref> lower on L1 misses is</p><formula xml:id="formula_5">M (1) lower (r,c,v) = 1 l 1 [kfrc+ 1 γ (Dr+Brc+ m r +1)+2mv] (5)</formula><p>where the size of one double precision floating point value equals γ integers. In this paper, we use 64-bit doubleprecision floating point data and 32-bit integers, so that γ = 2. The factor of 1 l1 accounts for the L1 line size. An analogous expression applies at other cache levels by substituting the appropriate line size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Figures 2-5 summarize the performance of our optimizations on the four platforms in <ref type="table" target="#tab_2">Table 1 and the matrices in  Table 2</ref>. We compare the following nine implementations and bounds. 3. Non-Symmetric Register Blocked: The blocked single vector implementation with non-symmetric storage where r and c are chosen by exhaustive search to maximize performance. Represented by asterisks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Symmetric Register Blocked:</head><p>The blocked single vector implementation with symmetric storage where r and c are chosen by exhaustive search to maximize performance. Represented by plus signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Non-Symmetric Register Blocked with Multiple</head><p>Vectors: The blocked multiple vector implementation with non-symmetric storage where r, c, and v are chosen by exhaustive search to maximize performance. Represented by upward pointing triangles.</p><p>6. Symmetric Register Blocked with Multiple Vectors: The blocked multiple vector implementation with symmetric storage where r, c, and v are chosen by exhaustive search to maximize performance. We refer to these parameters as r opt , c opt , and v opt . Represented by six-pointed stars.</p><p>7. Tuning Parameter Selection Heuristic: The blocked multiple vector implementation with symmetric storage where r, c, and v are chosen by the tuning parameter selection heuristic described in Section 5. We refer to these parameters as r heur , c heur , and v heur . Represented by hollow circles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Analytic Upper Bound:</head><p>The analytic upper bound on performance implementation 6. We use Equation <ref type="formula">(4)</ref> and Equation <ref type="formula">(5)</ref> to compute the numbers of loads and cache misses. Represented by solid lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">PAPI Upper Bound:</head><p>An upper bound on performance for implementation 6. The number of loads and cache misses are obtained from PAPI event counters. Represented by dashed lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Effects of Symmetry on Performance</head><p>Considering all four platforms, the maximum performance gain from symmetry is 2.08× for register blocked SpMV over the non-symmetric register blocked kernel (4 versus 3). The median speedup is 1.34×. The maximum performance gain from symmetry is 2.58× for register and vector blocked SpMM over the non-symmetric register and vector blocked kernel (6 versus 5). However, the median speedup of 1.09× is appreciably slower. Furthermore, symmetry can reduce performance in the rare worst case. The chosen register block sizes, shown in the appendices, suggest these performance decreases are due to block sizes in the symmetric case that lead to significantly more fill than those in the non-symmetric case.</p><p>An implementation optimized for symmetry, register and vector blocking achieves maximum, median, minimum performance gains of 7.32×, 4.15×, and 1.60× compared to the na¨ıvena¨ıve code (6 versus 1) over all platforms and matrices. Given symmetric storage, the other two optimizations almost always improve and never reduce performance. The increasing performance gains as optimizations are incrementally applied suggest cumulative performance effects of these optimizations.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Accuracy of Automatic Tuning Parameter Selection</head><p>On the Ultra 2i, Itanium 1, and Itanium 2, the block size selection procedure generally chooses (r, c, v) whose performance is 93% or more of the best by exhaustive search. On Power4, the predictions are somewhat less accurate, at 85% of the best or greater. Nevertheless, in nearly all cases the selected implementation yields at least some speedup over the symmetric register blocked single vector case <ref type="bibr">(7 versus 4)</ref>.</p><p>The heuristic does not select near-optimal implementations in the case of Matrix 12 (a linear programming matrix). We discuss this case in detail in the full report <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Proximity of Performance to Upper Bounds</head><p>To evaluate our performance models, we consider the proximity of the upper bounds to the measured performance of the symmetric, register and vector blocked code. The finite element matrices (FEM Matrices 2-8) achieve 72% to 90% of the PAPI bound, but only 53% to 73% of the analytic bound on the Ultra 2i and Itanium 1 . This difference suggests that further performance improvements on these platforms will require reducing the gap between the number of predicted and measured cache misses. The measured performance of these matrices on the Itanium 2 and Power 4 were 38% to 63% of the analytic bound. <ref type="bibr" target="#b3">4</ref> The non-FEM matrices realize relatively lower measured performance, achieving 65% to 120% of the PAPI bound and 44% to 62% of the analytic bound on the Ultra 2i and Itanium 1. Cases in which the measured performance exceeds the PAPI upper bound (Matrix 12 on Ultra 2i and Matrix 1 on Itanium 1) may be caused by limitations in the PAPI counters. The realized performance of these matrices on the Itanium 2 is 29% to 38% and 23% to 32% of the PAPI and analytic bounds, respectively, and 38% to 54% of the analytic bound on the Power 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Effects of Symmetry on Storage</head><p>Symmetry can significantly save storage . We show maximum and median savings of 64.79% and 56.52%, respectively, for symmetric register blocking <ref type="table" target="#tab_4">(Table 3, I</ref>). Symmetry may also use almost 10% more memory in the case of matrix 12 on the Itanium 1 and 2. We show maximum and median savings of 64.79% and 53.70%, respectively, when adding multiple vectors <ref type="table" target="#tab_4">(Table 3, II)</ref>.</p><p>A large register block size in a symmetric code can save more than 50% of storage because the memory for matrix indices decreases by up to a factor of r × c, augmenting the savings from storing half the matrix. An increase in storage from symmetric storage is also possible, however, if the chosen register block size results in significant fill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Temam and Jalby <ref type="bibr" target="#b17">[18]</ref> and Fraguela, et al., <ref type="bibr" target="#b18">[19]</ref> developed probabilistic cache miss models for SpMV, but assumed a uniform distribution of non-zero entries. In contrast, our lower bounds account only for compulsory misses. <ref type="bibr">Gropp, et al.,</ref> use similar bounds to analyze and tune a computational fluid dynamics code <ref type="bibr" target="#b16">[17]</ref> on Itanium 1. However, we tune for a variety of architectures and matrix domains. Work in sparse compilers (e.g. <ref type="bibr">Bik et al. [20]</ref>, Pugh and Spheisman <ref type="bibr" target="#b20">[21]</ref>, and the Bernoulli compiler <ref type="bibr" target="#b21">[22]</ref>) complements our own work. These projects consider expressing sparse kernels and data structures for code generation. In contrast, we use a hybrid off-line, on-line model for selecting transformations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Square diagonal blocking. A 10 × 10 matrix with 2 × 3 register blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The upper bound on D r is m r with at most D r = m r · r(r+1) 2 ≈ m(r+1) 2 diagonal blocked elements in the matrix, since a diagonal block has at most r(r+1) 2 el- ements. Furthermore, we estimate the number of non- diagonal blocks B rc by counting the stored elements ex- cluded from the diagonal blocks so that B rc = Brc rc where B rc ≈ kf rc − m(r+1) 2 and each register block con- tains rc elements. In the case of 1 × 1 register blocking, D r + B rc = k. The matrix requires storage of D r + B rc double precision values, D r + B rc integer column indices, and m r + 1 integer row indices. Since the fill ratio is defined as the number of stored elements (fill included) divided by the number of non-zeros (fill excluded), f rc ≈ Dr+Brc k and is always at least 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>Non-Symmetric Unoptimized Reference: The un- blocked (1, 1) single vector implementation with non- symmetric storage. Represented by crosses. 2. Symmetric Reference: The unblocked (1, 1) single vector implementation with symmetric storage. Rep- resented by five-pointed stars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 . Performance Summary -Sun Ul- tra 2i. Performance (MFlop/s) of various op- timized implementations compared to upper bounds on performance.</head><label>2</label><figDesc>Figure 2. Performance Summary -Sun Ultra 2i. Performance (MFlop/s) of various optimized implementations compared to upper bounds on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 . Performance Summary -Intel Ita- nium 1. Performance data and upper bounds shown in a format analogous to the format in Figure 2 .</head><label>32</label><figDesc>Figure 3. Performance Summary -Intel Itanium 1. Performance data and upper bounds shown in a format analogous to the format in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 . Performance Summary -Intel Ita- nium 2. Performance data and upper bounds shown in a format analogous to the format in Figure 2 . The model predicts machine peak ( 3 . 6 Gflop/s) for matrices 1- 7 .</head><label>42367</label><figDesc>Figure 4. Performance Summary -Intel Itanium 2. Performance data and upper bounds shown in a format analogous to the format in Figure 2. The model predicts machine peak (3.6 Gflop/s) for matrices 1-7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 . Performance Summary -IBM Power 4. Performance data and upper bounds shown in a format analogous to the format in Figure 2 . The analytic upper bound is omitted due to the unavailability of hardware coun- ters.</head><label>52</label><figDesc>Figure 5. Performance Summary -IBM Power 4. Performance data and upper bounds shown in a format analogous to the format in Figure 2. The analytic upper bound is omitted due to the unavailability of hardware counters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Matrix benchmark suite. 1 is a dense 
matrix stored in sparse format; 2-8 arise in 
finite element applications; 9-11 come from 
assorted applications; 12 is a linear program-
ming example. For each matrix, we show the 
number of non-zeros in the upper-triangle. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Percentage savings in matrix storage. 

</table></figure>

			<note place="foot" n="1"> &quot;All possible values&quot; subject to constraints that consider the characteristics of practical applications and architectural parameters.</note>

			<note place="foot" n="3"> where Dr and Brc can be represented in terms of frc, m, r, c, and k.</note>

			<note place="foot" n="4"> Note that no PAPI data was available for a bound on the Power 4.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Future Directions</head><p>Symmetry significantly reduces storage requirements, saving as much as 64.79% when combined with register blocking. Symmetry, register and vector blocking, improves performance by as much as 7.3× (median 4.15×) over a na¨ıvena¨ıve code, 2.08× (median 1.34×) over non-symmetric register blocked SpMV, and 2.6× (median 1.1×) over non-symmetric register and vector blocked SpMM. Moreover, the performance effects of these optimizations appear to be cumulative, making the case to combine these techniques.</p><p>Our heuristic, based on an empirical performance modeling and search procedure, is reasonably accurate, particularly for matrices arising from FEM applications. Heuristic chosen tuning parameters yield performance within 85% of the performance achieved from exhaustive search. Additional refinements may improve the accuracy on matrices with little or no block structure.</p><p>The performance of our optimized implementations are, on average, within 68% of the performance bounds, smaller than previously observed for non-symmetric SpMV. Additional refinements to explictly model low-level code generation and employing automated low-level tuning techniques (e.g., ATLAS/PHiPAC <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>) may close the gap.</p><p>Sparse kernels may be optimized for other forms of symmetry (e.g. structural, skew, hermitian, skew hermitian). Symmetric cache blocking may mitigate the effects of increasing matrix dimensions and vectors that do not fit in cache, grouping the matrix into large blocks whose sizes are determined by cache size. Optimizations for sparse kernels may also be implemented and evaluated for parallel systems, such as SMPs and MPPs <ref type="bibr" target="#b5">[6]</ref>. Lastly, performance optimized kernels will be distributed to application end-users.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPARSITY: Framework for Optimizing Sparse Matrix-Vector Multiply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic Performance Tuning of Sparse Matrix Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
		<respStmt>
			<orgName>U.C. Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Performance Optimizations and Bounds for Sparse Symmetric Matrix-Multiple Vector Multiply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Lorimier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<idno>UCB/CSD-03-1297</idno>
		<imprint>
			<date type="published" when="2003-11-25" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimization the Performance of Sparse MatrixVector Multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Im</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
		<respStmt>
			<orgName>U.C. Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing sparse matrix vector multiplication on SMPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th SIAM Conf. on Parallel Processing for Sci</title>
		<meeting>of the 9th SIAM Conf. on Parallel essing for Sci</meeting>
		<imprint>
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing sparse matrix-vector multiplication for register reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Science</title>
		<meeting>the International Conference on Computational Science</meeting>
		<imprint>
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing matrix multiply using PHiPAC: a Portable, HighPerformance, ANSI C coding methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovi´casanovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int&apos;l Conf. on Supercomputing</title>
		<meeting>of the Int&apos;l Conf. on Supercomputing<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically tuned linear algebra software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Supercomp</title>
		<meeting>of Supercomp</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Saavedra-Barrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-02" />
		</imprint>
		<respStmt>
			<orgName>U.C. Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling Application Performance by Convolving Machine Signatures with Application Profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carrington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 4th Annual Workshop on Workload Characterization</title>
		<meeting>the IEEE 4th Annual Workshop on Workload Characterization<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Matrix Market: A web resource for test matrix collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Remington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<ptr target="math.nist.gov/MatrixMarket" />
	</analytic>
	<monogr>
		<title level="m">Quality of Numerical Software, Assessment and Enhancement</title>
		<editor>R. F. Boisvert</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="125" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">UF Sparse Matrix Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="www.cise.ufl.edu/research/sparse/matrices" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable cross-platform infrastructure for application performance tuning using hardware counters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eijkhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vorst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>PA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd Edition. SIAM, Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Remington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pozo</surname></persName>
		</author>
		<ptr target="gams.nist.gov/spblas" />
		<title level="m">NIST Sparse BLAS: User&apos;s Guide</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>NIST</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance parallel implicit CFD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characterizing the behavior of sparse algorithms on caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing´92puting´ puting´92</title>
		<meeting>Supercomputing´92puting´ puting´92</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory hierarchy performance prediction for sparse blocked algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Zapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic nonzero structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J C</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A G</forename><surname>Wijshoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1576" to="1587" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generation of efficient code for sparse matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Spheisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Languages and Compilers for Parallel Computing</title>
		<meeting>the 11th Workshop on Languages and Compilers for Parallel Computing</meeting>
		<imprint>
			<date type="published" when="1998-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Relational Approach to the Automatic Generation of Sequential Sparse Matrix Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stodghill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-08" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
