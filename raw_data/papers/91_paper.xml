<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Site-Level Knowledge for Incremental Crawling of Web Forums: A List-wise Strategy *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Ming</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunsong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
							<email>wyma@microsoft.com.chunsong@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">Beijing University of Posts</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Site-Level Knowledge for Incremental Crawling of Web Forums: A List-wise Strategy *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Information Storage and Retrieval]: Information Search and Retrieval -clustering</term>
					<term>information filtering General Terms Algorithms</term>
					<term>Performance</term>
					<term>Experimentation Keywords Web Forum</term>
					<term>Sitemap</term>
					<term>Incremental Crawling</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We study in this paper the problem of incremental crawling of web forums, which is a very fundamental yet challenging step in many web applications. Traditional approaches mainly focus on scheduling the revisiting strategy of each individual page. However, simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites. Instead of treating each individual page independently, we propose a list-wise strategy by taking into account the site-level knowledge. Such site-level knowledge is mined through reconstructing the linking structure, called sitemap, for a given forum site. With the sitemap, posts from the same thread but distributed on various pages can be concatenated according to their times-tamps. After that, for each thread, we employ a regression model to predict the time when the next post arrives. Based on this model, we develop an efficient crawler which is 260% faster than some state-of-the-art methods in terms of fetching new generated content; and meanwhile our crawler also ensure a high coverage ratio. Experimental results show promising performance of Coverage, Bandwidth utilization, and Timeliness of our crawler on 18 various forums.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>To the best of our knowledge, little existing work in literatures has systematically investigated the problem of forum incremental crawling. However, there are still some previous work that should be reviewed, as our approaches were motivated by them.</p><p>Some early work in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> first investigated the dynamic web and treated the information as a depreciating commodity. They first introduced the concepts of lifetime and age of a page which is important for measuring the performance of an incremental crawler. However, they treated every page equally and ignored the importance and change frequency which are also important to an incremental crawler.</p><p>Later work improved the above work and optimized the crawling process by prioritizing the pages. Whether to minimize the age or to maximize the freshness leads to a variety of analytical models by investigating different features. We classify them into three categories:</p><p>(I) How often the content of a page is updated by its owner. Coffman et. al. <ref type="bibr" target="#b6">[7]</ref> analyzed the crawling problem theoretically. Cho et. al. <ref type="bibr" target="#b5">[6]</ref> proposed several methods based on the page update frequency. However, most of these methods are based on the assumption that most web pages change as a Poisson or memoryless process. But the experimental result in <ref type="bibr" target="#b2">[3]</ref> showed that most web pages are modified during the span of US working hours (between 8 AM and 8 PM, Eastern time, Monday to Friday). This phenomenon is even more evident in forum websites because the content are all generated by forum users. Edwards et. al. <ref type="bibr" target="#b9">[10]</ref> tried to use an adaptive approach to maintaining data on actual change rates for the optimization without the above assumption. However, it still treated each page independently and may waste bandwidth. Furthermore, we argue that some other factors, such as time intervals of the latest posts, are more important in web forums. We will show their importance in the experiment part.</p><p>(II) The importance of each web page. Baeza-Yates et. al. <ref type="bibr" target="#b0">[1]</ref> tried to determine the weight of each page based on some strategies similar to PageRank. Wolf et. al. <ref type="bibr" target="#b15">[16]</ref> assigned the weight of each page based on the embarrassment metric of users' search results. In the user-centric crawling strategy <ref type="bibr" target="#b12">[13]</ref>, the targets are mined from user queries to guide the refreshing schedule of a generic search engine; First of all, some pages may have equal importance weight but different update frequency, thus only measuring the importance of each web page is insufficient. Second, both static rank and content importance are useless in web forums. Most pages in web forums are dynamic pages which are generated using some pre-defined templates. It is very hard to compute their PageRank-like static rank since there are medial links among these pages. Furthermore, the content importance measurement is also useless. Once a post is generated, this post always exists unless the author deletes it manually. Before we get these post information, it is very hard to predict their importance. However, once we have their content importance information we usually do not need to revisit it anymore. Some work named focused crawling attempts to only retrieve web pages that are relevant to some pre-defined topics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> or some labeled examples <ref type="bibr" target="#b13">[14]</ref> by assigning pages similar to the target page a higher weight. The target descriptions in focused crawling are quite different in various applications.</p><p>(III) The information longevity of each web page. Olston et. al. <ref type="bibr" target="#b11">[12]</ref> introduced the longevity to determine revisiting frequency of each web page. However, the information longevity in forums is useless since once a post never disappears unless being deleted. This is one of the major differences between general web pages and forum web pages. Moreover, its three generative models are still based on the poisson distribution and some modified forms.</p><p>Realizing the importance of forum data and the challenges in forum crawling, Cai et al. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15</ref>] studied how to reconstruct the sitemap of a target web forum, and how to choose an optimal traversal path to crawl informative pages only. However, this work only addressed the problem of fetching  as much as possible valuable data, but left the problem of refreshing previously downloaded pages <ref type="bibr" target="#b11">[12]</ref> untouched. All the existing methods ignore the tradeoff between discovering new threads and refreshing existing threads. Intuitively, index-of-thread pages should get higher update frequency than post-of-thread pages because any users' update activities will change index-of-thread pages. However, because the number of post-of-thread pages is much larger than index-of-thread pages, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages even if only a small percentage of post-of-thread pages are very active. These post-of-thread pages will occupy most bandwidth, and as a result new discussion threads cannot be downloaded efficiently. Unfortunately, few of existing methods has taken this issue into account. We will show its importance in the experiment part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR SOLUTION</head><p>In this section, we first describe the system overview of the proposed solution, and then introduce the details of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Overview</head><p>The proposed solution consists of two parts, offline mining and online crawling, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>The offline mining is to learn the sitemap of a given forum site with a few pre-sampled pages from the target site. Through analyzing these sampled pages, we can find out how many kinds of pages are there in that site, and how these pages are linked with each others <ref type="bibr" target="#b3">[4]</ref>. This site-level knowledge is then employed in the online crawling process.</p><p>The online crawling consists of three steps: (a) identifying index lists and post lists; (b) predicting the update frequency of a list using a regression model; and (c) balancing bandwidth by adjusting the numbers of various lists in the crawling queue, as shown in the left part of <ref type="figure" target="#fig_2">Fig. 2</ref>. Given a new crawled page, we first identify if it is an index-of-thread page or a post-of-thread page according to its layout information. Since an index list or post list consists of multiple pages, to reconstruct a list, we concatenate corresponding pages following the detected pagination links <ref type="bibr" target="#b14">[15]</ref>. Then, for each reconstructed list, we extract the timestamps of the records (either posts or thread entities) containing in that list. Several statistics are then proposed based on the timestamps to characterize the growth of a list; and a regression model is trained to predict the arrival time of the next record of that list. Finally, given a fixed bandwidth, we balance the numbers of index /post lists, to satisfy the requirements of both completeness and timeliness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">List Reconstruction</head><p>The sitemap knowledge is an organization graph of a give forum site, which is the fundamental component of the proposed crawling method in this paper. The details of its generation algorithm can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, here we just briefly describe it as follows. To estimate the sitemap, we first sample a few pages from the target site. Because the sampling quality is crucial to the whole mining process, to diversity the sampled pages in terms of page layout and to retrieve pages at deep levels, we adopt a combined strategy of breadth-first and depth-first using a double-ended queue. In the implementation, we try to push as many as possible unseen URLs from each crawled page to the queue, and then randomly pop a URL from the front or the end of the queue for a new sampling. In practice, it was found that sampling a few thousands pages is sufficient to reconstruct the sitemap for most forum sites. After that, pages with similar layout structures are further clustered into groups using the single linkage algorithm, as marked with blue nodes in <ref type="figure" target="#fig_3">Fig. 3</ref>. In our approach, we utilize the repetitive regions to characterize the layout of each page. Repetitive regions are very popular in forum pages to present data records stored in a database. Considering that two similar pages usually have different numbers of advertisements, images, and even some complex sub-structure embedded in user posts, the repetitive region-based representation is more robust than the whole DOM tree <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Finally, all possible links among various page groups are established, if in the source group there is a page having an out-link pointing to another page in the target group.</p><p>After that, a classifier is introduced to identify the node of index-of-thread pages or post-of-thread pages in a sitemap. The classifier is built by some simple yet robust features. For example, the node of post-of-thread pages always contains the largest number of pages in sitemap, and the index-ofthread pages should be the parents of post-of-thread pages, as marked with the red rectangle or green rectangle in <ref type="figure" target="#fig_3">Fig.3</ref>. The classifier is site-independent and can be used in different forum sites. We then map each individual page into one of the nodes by evaluate its layout similarity such as sim(p new , N index ) and sim(p new , N post ); and δ is the threshold for decision. We concatenate all the individual pages belonging to the same index list or post list together. The details of this process is described in Algorithm 1. The above process is fully automatic. With the concatenated index lists or post lists, our crawling method becomes a list-wise strat- for all page p i in N index , and OutLinks i is the pagination out-links list of p i do 5:</p><p>If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k . 6:</p><p>Concatenate pnew into the index list of pi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>end for 8: else if sim(p new , N post ) &lt; δ then 9:</p><p>for all page pi in Npost, and OutLinksi is the pagination out-links list of p i do 10:</p><p>If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k . 11:</p><p>Concatenate pnew into the post list of pi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>end for 13: end if egy rather than page-level strategies used by most existing general crawlers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Timestamp Extraction</head><p>To predict the update frequency of a list more accurately, we need to first extract the time information of each record (thread creation time in index lists or post time in post lists). It is difficult to extract the correct time information due to the existence of noisy time records in forum pages, such as users' registration time, last login time and so on. In <ref type="figure" target="#fig_4">Fig.4</ref>, the time contents in orange rectangle are all noisy content while only the purple one is the correct time information.</p><p>There are three steps to extract the time information. We first get the timestamp candidates whose content is a short one and contains digit string such as mm-dd-yyyy or dd/mm/yyyy, or some specific words such as Monday, Tuesday, January, February, etc. Second, we align the html elements containing time information into several groups based for all page p j in L do 4:</p><p>Get all cadidates tsList j in p j whose DOM path is pathi and sort them in their appearance order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for all time candidate tc k in tsList j , 1 ≤ k ≤ M and M is the length of tsj do 6:</p><p>if tc k is earlier than tc k+1 then 7:</p><p>SeqOrd + + 8:</p><p>else if tc k is later than tc k+1 then 9:</p><p>RevOrd </p><note type="other">+ + 10: end if 11: end for 12: end for 13: Set Order</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Model</head><p>The latest replies in each thread can be easily found by revisiting post lists. Similarly, the new discussion threads can be discovered by revisiting index lists. The problem of incremental crawling of web forums is converted to how to estimate the update frequency and predict when the next new record of a list (post list or index list) arrives. Based on the timestamp of each record, we propose several features to help predict the update frequency and describe them in <ref type="table">Table.</ref>1.</p><p>Furthermore, for index lists, we analyze the average thread activity in forum sites. We process all the threads and calculate their active time by checking the time of the first post and the last post in each thread. The result is shown in <ref type="figure">Fig. 5</ref>. The figure represents the percentage of threads with different active time. From the figure we can see the percetage of active thread drops significantly when the active time becomes longer. More than 40% threads keep active no longer than 24 hours and 70% threads are no longer than 3 days. This is one of the major reasons why forum incremental crawling strategy is different from tra-</p><formula xml:id="formula_0">¡ ¢ ¡ £ ¡ ¤ ¡ ¥ ¦ § ¨ © § ¡ ¡ ¢ ¢ £ ¤ ¥ § ! ¦ "</formula><p># $ % <ref type="figure">Figure 5</ref>: The thread activity analysis.</p><p>ditional incremental crawling strategies. Once a thread is created, it usually becomes static after a few days when there is no discussion activity. Thus we introduce a state indicator to avoid bandwidth waste. Suppose there is no discussion activity for ∆t na time since the last post. We compute the standard deviation of time interval ∆t sd by</p><formula xml:id="formula_1">∆t sd = 1/(N − 1) · N −2 i=1 (∆ti − ∆tavg) 2 ,</formula><p>where N is the number of post records. If ∆t na − ∆t avg &gt; α · ∆t sd , we may set ds = 1, otherwise, ds = 0. This factor is for index lists only.</p><p>To combine these factors together, we leverage a linear regression model which is a lightweight computation model and is efficient for online processing.</p><formula xml:id="formula_2">F (x) = w 0 + N i=1 w i · x i<label>(1)</label></formula><p>For each forum site, we train two models F list (x) and F post (x) for index lists and post lists separately. The two models are kept updated during the crawling process for the new crawled pages. In practice, we update the two models every two months. By setting x0 = 1, we can get the corresponding W by Equation 2.</p><formula xml:id="formula_3">W = (X T · X) −1 · X T · Y<label>(2)</label></formula><p>In the crawling process, we predict the new coming records of each list by ∆I = (CT − LT )/F (x), where CT is the current time and LT is the last revisit time (by crawler). We use the predicted ∆I to schedule the crawling process. Though a list may contain multiple pages, we do not need to revisit all of them. We sort the pages in each list based on the timestamps of the records in each page. For an index list, if there are N um list records in each index-of-thread page, and we only need to revisit the top ∆I list /N um list pages, where ∆I list = (CT − LT )/F list (x). For a post list, we need to revisit the last page or the new discovered pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bandwidth Control</head><p>As we have discussed in previous sections, we need to make a tradeoff between discovering new pages and refreshing existing pages. The number of post-of-thread pages is much larger than index-of-thread. Even if only a small percentage of post-of-thread pages are very active, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages, which will occupy most bandwidth. In this case, we need to allocate a dedicated bandwidth for post-of-thread pages since we can only get new discussion threads from them. We introduce a hyper bandwidth control strategy to balance the bandwidth between two kinds of list. The ratio between index lists and post lists can be defined as: </p><formula xml:id="formula_4">ratio ∝ N index N P ost (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>Different from previous work which only considered revisiting existing pages, in this paper, the scenario we have considered is a real case. The crawler is required to crawl a target forum site starting from its portal page. To have a thorough evaluation, a crawling task needs to last for one year or even longer so that we can measure the crawling performance using different metrics at different periods, for example, the warming up stage and the stable stage. This creates the need to build a simulation platform to compare different algorithms because the real-time crawling cannot be repeated for multiple crawling approaches. Before we describe the experimental details, we first introduce the experimental setup and the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>To evaluate the performance of our system on various situations, 18 forums were selected in diverse categories (including bicycle, photography, travel, computer technique, and some general forums) in the experiments, as listed in <ref type="table" target="#tab_1">Table 2</ref>. The average length of service of 18 sites is about 4.08 years.</p><p>As we wish to evaluate different methods in a long time period (about one year) under several different bandwidth situations while we still want the evaluation to be repeated under the same environment to fairly compare different approaches. Because it is impractical to repeat a long lasting crawling process for many times on real sites, we built a simulation platform to facilitate our evaluation. Typically, a forum hosting server organizes forum data using a backend database, and dynamically generates forum pages using page templates. To build the simulation platform for a fo- More precisely, we first mirrored the 18 forum sites using a customized commercial search engine crawler. The crawler was configured to be domain-limited and depth-unlimited. For each site, the crawler started from its portal page and followed any links within that domain, and a unique URL address was followed only once. Consequently, the crawler mirrored all the pages up to June 2008 from 18 forum sites. The mirrored dataset contains 990,476 pages and 5,407,854 individual posts, from March 1999 to June 2008. Using manually wrote data extraction wrappers, we parsed all the pages and stored all the data records in a database.</p><p>The basic behavior of this simulation platform is to response for a URL request associated with a timestamp. We wrote 18 page generators for 18 forum sites to simulate the responses to requests. For any requested URL, since all the corresponding records can be accessed from the database, we can generate a HTML page with the same contents, layout, and related links as the one in the real site. Here we make an assumption that a post will never be deleted after it is generated. Based on this simple yet reasonable assumption, we can easily figure out at any given time whether a record exists based on its post time and how records are organized based on the forum's layout, and therefore be able to restore a snapshot of a forum site to the given time.</p><p>This simulation platform was used in all the following crawling experiments, and provided a fair experimental environment to each crawling approach. Assuming a fixed bandwidth, each crawler was required to crawl the given forum site starting its portal page and from the dummy time period 2006-01-01 to 2007-01-01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods Compared in the Experiments</head><p>To differentiate the advantage of list-wise strategy and the benefit of bandwidth control, we implemented two variants of our method: (1) list-wise strategy (LWS); (2) list-wise strategy + bandwidth control (LWS+BC).</p><p>Since the Curve-Fitting policy and Bound-Based policy in <ref type="bibr" target="#b11">[12]</ref> are the state-of-the-art approaches and are more relevant to our work, we also included them in the experiments. The original Bound-Based policy only crawl existing pages. We have tried our best to adapt the structure-driven-based approach to forum crawling by: 1) giving a new discovered URL the highest weight; and 2) relaxing the interval condition for adjusting refresh period and reference time to accommodate the high frequent update situation in forum sites.</p><p>We also introduced an oracle strategy for comparison. In the oracle strategy, every update activity of each page in the target site is supposed to be known exactly. Given the fixed bandwidth, the oracle policy can choose the pages with more new valuable information to visit. The oracle strategy is an ideal policy and an upper bound for other crawlers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Measurements</head><p>Following pervious work, we assume that the costs of visiting different pages are equal, and we measure the bandwidth cost as the total number of pages which are required to crawl in a given time period <ref type="bibr" target="#b11">[12]</ref>. To evaluate the overall crawling performance for each approach, we measure from the following three aspects:</p><p>• Bandwidth Utilization. If the bandwidth is fixed, bandwidth utilization is an important measure of crawler's capability. This measurement is used to analyze if the crawler can make the best use of a limited bandwidth:</p><formula xml:id="formula_5">B = Inew IB (4)</formula><p>where IB is the bandwidth cost defined as the total number of pages crawled in a time unit and I new is the number of pages containing new information compared to the existing indexed repository.</p><p>• Coverage. A crawler needs to balance between fetching new pages and refreshing existing pages. If a crawler wastes too much bandwidth to refresh previously downloaded pages, it may not be able to crawl all new requested pages, and vice versa. To measure the issue, we define coverage as follows:</p><formula xml:id="formula_6">Cov = I crawl I all<label>(5)</label></formula><p>where I all is the measurement of valuable information existing in the target site and I crawl is the measurement of valuable information having been downloaded by crawler.</p><p>• Timeliness. To measure if we can fetch each post "timely", we introduce timeliness. Suppose there are N elements of valuable information which we have downloaded. The timeliness is defined as:</p><formula xml:id="formula_7">T = 1 N N i=1 ∆ti (6)</formula><p>where ∆t i represents the time period from its updating time to its downloading time. If the element was updated three day ago and we downloaded it one day ago, ∆ti is two day. This value is smaller the better.</p><p>In the forum crawling task, different from general web sites, once a post is submitted, the post will always exist until it is manually deleted by the creator or administrator.</p><p>In this paper, we use the number of unique posts to measure the valuable information in forum sites. In the definition of coverage, I all should be the number of unique posts existing in the target forum site and I crawl should be the number of unique posts having been downloaded. In the definition of timeliness, ∆ti is the time period from a post's creating time to its downloading time. If we download a post one day after the post was created, ∆ti is one day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Warming Up Stage</head><p>To make a fair evaluation for all crawlers, we require all the crawlers to start from the portal page of each site with a fixed bandwidth 3000 pages per day. The crawlers begin to crawl pages from the dummy time 2007-01-01 and last about one year. To illustrate the performance changes of all crawlers in different time periods, we calculate the average performance in everyday in terms of the aforementioned three measurements and present the results in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>From the figure we can see the performance changes apparently in the first 100 days and become stable after about 120 days. This is due to the so called warming up stage during which a crawler needs to first mirror the existing pages and after that it can download new pages. Suppose there are P old posts existing before the crawler starts, ∆P new posts will be generated every day and the bandwidth allows the crawler to crawl B posts per day. At the d th day, there are about P old + ∆P · d posts existing in the target site. At the beginning, since P old ∆P ·d, the crawler is required to download almost all posts belonging to P old . These are all new valuable information compared to the indexed repository. This is why in the first 100 days the bandwidth utilization was approximate to 1, the Coverage increases quickly and the Bandwidth Utilization decreases quickly. We call this stage the warming up stage for the crawler. After about P old /(B − ∆P ) days, the crawler may have finished downloading all old posts and begins to only focus on the posts belonging to ∆P every day and the performance becomes stable.</p><p>Whatever refresh strategy a crawler chooses, if it only assigns new valuable information with the highest weight,  In general, the oracle method always performs the best in all measurements and acts as an ideal method. Beside the oracle method, the LWS+BC performs significantly better than other methods in terms of timeliness. The average coverage per day for all methods becomes to 100% after 100 days. This is because the bandwidth is enough and these methods can archive most historical records. But the performance of these methods on fetching daily new records are different as shown in timeliness results. For the given bandwidth, the timeliness of LWS+BC will decrease after 100 days. This is because LWS+BC can save enough buffer to catch up the update progress for new posts after it finishes crawling all existing posts. The LWS can just keep the timeliness stable because it does not have additional bandwidth to catch up the update progress. The bound-based and curve-fitting policies get very similar performance. The timeliness of them all increases (note that the smaller the better for the timeliness measure) since they cannot fetch new posts timely and thus delay the downloading of new posts. We will analyze them in next section. </p><formula xml:id="formula_8">¡ ¢ £ ¢ ¤ ¥ ¦ § ¨ © § § ! " # $ % &amp; ' " # $ ¢ ( ¢ ¡ ) ¥ ¥ ) ( ( ) 0 0 ) 1 ¦ 2 ¦ 3 4 5 6 &amp; 7 8 9 @ A &amp; B ! @ ' 8 C ! A D E F F E 9 G (a) Bandwidth Utilization ¡ ¢ £ ¢ ¤ ¥ ¦ § ¨ © ¨ ! " # ¢ $ ¢ ¡ % ¥ ¥ % $ $ % &amp; &amp; % ' ¦ ( ) 0 1 " 2 3 4 5 6 " 7 5 # 3 8 6 9 @ A A @ 4 B (b) Coverage ¡ ¡ ¢ ¡ ¡ £ ¤ ¥ ¦ § ¤ ¨ ¨ © ! ! ¡ " ¡ ¡ ¡ # ¡ $ ¡ ¡ $ # ¡ % ¡ ¡ % # ¡ " ¡ ¡ " # ¡ &amp;¦ £ ' ( ) 0</formula><formula xml:id="formula_9">¡ ¢ ¡ ¢ £ ¤ ¥ ¦ § ¨ ¦ ¤ © ¤ ¦ ¤ ! " # $ % &amp; &amp; % ' ( ) 0 ! ' 1 # ) 2 1 3 4 5 ¡ £ ¢ 6 7 8 £ 9 @ ¥ A ¤ B C D E F G E H I P Q C R S T U Q S V E C W X 3 4 5 3 4 5 Y ) (a) Bandwidth Utilization ¡ ¢ ¢ ¡ ¢ ¢ £ ¤ ¥ ¦ § ¨ © § ! " # # " $ % &amp; ' $ ( &amp; ) ( ¡ ¢ 0 ¡ ¢ 0 £ ¤ 1 2 3 £ 4 5 ¥ 6 7 8 9 @ A 9 B C D E 7 F G H I E G P 9 7 Q R S T U S T U V &amp; (b) Coverage ¡ ¡ ¢ ¡ ¡ £ ¤ ¥ ¦ § ¤ ¨ ¨ © ! ! " # $ % " &amp; $ ' &amp; ( ) 0 ¡ 1 ¡ ¡ 2 ¡ ¡ ¡ 1 ¡ ¡ ¡ 3 ¡ ¡ ¡ ¡ ¡ ¡ 4 ¡ ¡ ¡ ¢ ¡ ¡ ¡ 5¦ 6 7 8 9 @ A 9 B C D E 7 F G H I E G P 9 7 Q R ( ) 0 ( ) 0 S $ (c) Timeliness</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with different methods</head><p>A crawler for a commercial search engine is usually not allowed to frequently restart. The performance after its warming-up stage is thus more meaningful. We evaluated all methods under various bandwidth conditions and all crawlers were required to start from the portal page of the given site. The crawlers start from the dummy time 2006-01-01 and last about one year. We only calculate the average performance of different methods for the last 90 days from 2006-10-01 to 2006-12-31 and present the results in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>The curve-fitting policy and bound-based policy perform similarly on timeliness and bandwidth utilization while curvefitting policy performs slightly better on coverage. This is consistent with their original results in <ref type="bibr" target="#b11">[12]</ref>.</p><p>LWS is better than curve-fitting policy and bound-based policy on all measurements, because we can estimate the update frequency more accurately with list-wise information. Furthermore, we can also avoid visiting all the pages in a list and thus save considerable bandwidth.</p><p>LWS+BC is the best policy which further improves the performance apparently compared to LWS. Though the average coverage of different methods seems very close (from 0.98 to 1.0), the actual gap is relatively large when it multiplies 1 million pages or 5 million posts. And the gap may become even larger when we continue to crawl more sites or the bandwidth becomes more limited, as shown in the  <ref type="figure" target="#fig_7">Fig. 7(b)</ref>. Although LWS can estimate the update frequency for index lists and post lists relatively more accurately, it is still very hard to balance these two kinds of pages. In contrast, the bandwidth control policy is more effective to balance between fetching valuable data and refreshing downloaded pages. Such a policy only slightly affects postof-thread pages but benefits the index-of-thread pages a lot. When the bandwidth is set to 3000 pages per day, the average timeliness for LWS+BC is about 65 minutes while the average timeliness for bound-based policy or curve-fitting policy is about 170 minutes and 165 minutes respectively. This shows that LWS+BC is 260% faster than the boundbased policy or curve-fitting policy and thus is capable of downloading new posts timely. Meanwhile, it also achieve a high coverage ratio compared to other methods. To get more insights to this problem, we evaluate these two kinds of pages separately in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Index Lists and Post Lists</head><p>Given a fixed bandwidth of crawling 3000 pages per day, we evaluated the performance for index lists and post lists separately and showed the results in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>From the table, we can see that LWS improves the performance for both two kinds of pages. The timeliness of both index-of-thread pages and post-of-thread pages decreases significantly in <ref type="table" target="#tab_3">Table 3</ref> compared to the curve-fitting policy and bound-based policy. This is because LWS leverages more information and can estimate the update frequency for both two kinds of pages more accurately.</p><p>LWS+BC further improves the performance for index-ofthread pages compared to LWS. The timeliness of indexof-thread pages decreases significantly in <ref type="table" target="#tab_3">Table 3</ref> while the timeliness of post-of-thread pages remains the same. It confirms our previous assumption that bandwidth control can assign the right ratio according to the real update numbers for different kinds of pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Realizing the importance of forum data and the challenges in forum crawling, in this paper, we proposed a list-wise strategy for incremental crawling of web forums. Instead of treating each individual page independently, as did in most existing methods, we have made two improvements. First, we analyze each index list or post list as a whole by concatenating multiple pages belong to this list together. And then we take into account user behavior-related statistics, for example, the number of records and the timestamp of each record. Such information is of great help for developing an efficient recrawl strategy. Second, we balance discovering new pages and refreshing existing pages by introducing a bandwidth control policy for index lists and post lists. To evaluate the proposed crawling strategy, we conducted extensive experiments on 18 forums, compared it with several state-of-the-art methods, and evaluated it under various situations, including different stages through one year's crawling simulation, different bandwidths, and different kinds of pages. Experimental results show that our method outperforms state-of-the-art methods in terms of bandwidth utilization, coverage, and timeliness in all situations. The new strategy is 260% faster than existing methods and meanwhile it also achieves a high coverage ratio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of (a) Index-of-thread pages 3 in sub-board "Getting Started", (b) Postof-thread pages 4 in thread "HOW TO: Using sorting / paging on GridView w/o a DataSourceControl DataSource", and (c) Data growth of an index list.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The flowchart of the proposed solution for incremental forum crawling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of a sitemap. Algorithm 1 The algorithm for constructing index lists and post lists. Suppose sitemap(N, E) is the sitemap knowledge with corresponding nodes N and edges E, N index is the node of index-of-thread pages, Npost is the node of post-of-thread pages. 1: Suppose there is a new page p new . 2: Parse the page pnew and get the pagination out-links list OutLinks new . 3: if sim(pnew, N index ) &lt; δ then 4: for all page p i in N index , and OutLinks i is the pagination out-links list of p i do 5: If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k . 6: Concatenate pnew into the index list of pi. 7: end for 8: else if sim(p new , N post ) &lt; δ then 9: for all page pi in Npost, and OutLinksi is the pagination out-links list of p i do 10: If (1) ∃linkj ∈ OutLinksi and pnew = linkj, or (2) ∃link k ∈ OutLinks new and p i = link k . 11: Concatenate pnew into the post list of pi. 12: end for 13: end if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example timestamps in a page. Algorithm 2 The algorithm of extracting timestamp DOM path from list L. Suppose tsSet is a set of all timestamp candidates for the pages in L and there are N unique DOM paths for these candidates in tsSet. 1: for all DOM path path i , 1 ≤ i ≤ N do 2: Set SeqOrd = 0 and RevOrd = 0. 3: for all page p j in L do 4: Get all cadidates tsList j in p j whose DOM path is pathi and sort them in their appearance order. 5: for all time candidate tc k in tsList j , 1 ≤ k ≤ M and M is the length of tsj do 6: if tc k is earlier than tc k+1 then 7: SeqOrd + + 8: else if tc k is later than tc k+1 then 9: RevOrd + + 10: end if 11: end for 12: end for 13: Set Order i = |SeqOrd−RevOrd|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>i = |SeqOrd−RevOrd| SeqOrd+RevOrd 14: end for 15: Let c = argmax i Order i 16: Return path c on their DOM path since the timestamp in each record should have the same DOM path in HTML document. Fi- nally, since the records are generated in sequence, the times- tamps should satisfy a sequential order. This helps filter noisy time records and extract the correct information. The details of this process is described in Algorithm 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: One year performance of different crawlers in (a) Bandwidth Utilization, (b) Coverage, and (c) Timeliness. the length of the warming up stage will only depend on the number of old posts P old , new post generation speed ∆P and the bandwidth B. Furthermore, if the bandwidth B &lt; ∆P , it means the bandwidth is too small to cover daily generated new posts. In this case, the crawler may not be able to mirror the old posts unless the forum site stops generating new posts. In general, the oracle method always performs the best in all measurements and acts as an ideal method. Beside the oracle method, the LWS+BC performs significantly better than other methods in terms of timeliness. The average coverage per day for all methods becomes to 100% after 100 days. This is because the bandwidth is enough and these methods can archive most historical records. But the performance of these methods on fetching daily new records are different as shown in timeliness results. For the given bandwidth, the timeliness of LWS+BC will decrease after 100 days. This is because LWS+BC can save enough buffer to catch up the update progress for new posts after it finishes crawling all existing posts. The LWS can just keep the timeliness stable because it does not have additional bandwidth to catch up the update progress. The bound-based and curve-fitting policies get very similar performance. The timeliness of them all increases (note that the smaller the better for the timeliness measure) since they cannot fetch new posts timely and thus delay the downloading of new posts. We will analyze them in next section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The performance of different crawlers for different bandwidth in (a) Bandwidth Utilization, (b) Coverage, and (c) Timeliness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : The proposed features in prediction model. Feature Description ∆t1, ∆t2, ∆t3, ∆t4, ∆t5 In contrast to leveraging the timestamp directly, we leverage the time intervals between each two consecutive timestamps which represent the recent update frequency of a list. ∆t avg The average time interval represents the list update history. Because the latest five time intervals may be affected by some accidents, the average time interval of consecutive records in a list helps on smoothing the result and tolerates these accidents1 , cd 2 , cd 3 , . . . , cd 7</head><label>1</label><figDesc>Similar to the feature of current time. We also represent the current day in a week by a vector with 7 dimensions. For example, we represent Wednesday by setting cd3 = 1 and others to zero. ∆t day Since the update frequency of pages is also dependent on working days/weekend days, we split one week into 7 days and leverage the average time interval of the whole forum site in current time span. s1, s2, s3, . . . , s15 Ideally, we can estimate update frequency of current list by checking similar lists. To achieve this goal, we first represent the state of current list via its last five time intervals and then clustering them into 15 clusters based on their Euclidean distances. For each new list, we assign it to one of the states with the smallest Euclidean distance. We represent the 15 states by a vector with 15 dimensions. For example, we represent state 5 by setting s 5 = 1 and others to zero. where given a time period ∆t, N index is the number of new discussion threads within time ∆t and NP ost is the number of new posts arriving within time ∆t. We use the average ratio in history to balance the bandwidth between two lists.</figDesc><table>. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Web Forums in the Experiments 
Id Forum Site 
Description 
1 

www.avsforum.com 
Audio and video 

2 

boards.cruisecritic.com 
Cruise travel message 

3 

www.cybertechhelp.com 
Computer help community 

4 

www.disboards.com 
Disney trip planning 

5 

drupal.org 
Official website of Drupal 

6 

www.esportbike.com 
Sportbike forum 

7 

web2.flyertalk.com 
Frequent flyer community 

8 

www.gpsreview.net 
GPS devices review 

9 

www.kawiforums.com 
Motorcycle forum 

10 www.pctools.com 

Personal computer tools 

11 www.photo.net 

Photography community 

12 photography-on-the.net 

Digital photography 

13 forums.photographyreview.com Photography review 
14 www.phpbuilder.com 

PHP programming 

15 www.pocketgpsworld.com 

GPS help, news and review 

16 www.railpage.com.au 

The Australian rail server 

17 www.storm2k.org 

Weather discussion forum 

18 forum.virtualtourist.com 

Travel and vacation advice 

rum, we need to first mirror the forum site by downloading 
all the forum pages, then parse the forum pages in a re-
verse engineering manner and store the parsed forum posts 
in a database. Thereafter, we can simulate the response to a 
downloading request by regenerating a forum page according 
to the requested URL address and the crawling time. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : The performance differences between index lists and post lists. We use "BU" as "Bandwidth Utilization" here for short.</head><label>3</label><figDesc></figDesc><table>Methods 
Index lists 
Post lists 
BU 
C 
T 
BU 
C 
T 
BB 
0.0013 0.9925 171 0.0173 0.9917 170 
CF 
0.0012 0.9936 167 0.0174 0.9931 166 
LWS 
0.0025 0.9971 73 
0.0276 0.9943 81 
LWS+BC 0.0030 0.9989 28 
0.0329 0.9949 65 
Oracle 
0.0061 1 
2 
0.0415 0.9990 16 

trends of </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crawling a country: Better strategies than breadth-first for web page ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How dynamic is the web? Computer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Keeping up with the changing web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cumputer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">iRobot: An intelligent crawler for Web forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2008-04" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Focused crawling: A new approach to topic-specific web resource discovery. Computer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective page refresh policies for web crawlers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal robot scheduling of web search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of scheduling</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding question-answer pairs from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Focused crawling using content graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Coetzee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB</title>
		<meeting>of VLDB</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An adaptive model for optimizing performance of an incremental web crawler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Mccurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tomlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating topic-driven web crawlers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recrawl scheduling based on information longevity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2008-04" />
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">User-centric web crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-driven crawler generation by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L A</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M B</forename><surname>Cavalcanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring traversal strategy for Web forum crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal crawling strategies for web search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Squillante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ozsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured data extraction from the web based on partial tree alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint optimization of wrapper generation and template detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGKDD</title>
		<meeting>of SIGKDD</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
