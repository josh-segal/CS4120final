<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fault-Tolerant Communication Runtime Support for Data-Centric Programming Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Vishnu</surname></persName>
							<email>abhinav.vishnu@pnl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huub</forename><forename type="middle">Van</forename><surname>Dam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wibe</forename><surname>De Jong</surname></persName>
							<email>wibe.dejong@pnl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiwen</forename><surname>Song</surname></persName>
							<email>shuaiwen.song@cs.vt.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Virginia Polytechnic Institute</orgName>
								<address>
									<postCode>24060</postCode>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">â€¡ Mathematics and Computer Science Division Argonne National Laboratory, Argonne</orgName>
								<orgName type="institution">Pacific Northwest National Laboratory</orgName>
								<address>
									<postCode>99352, 60439</postCode>
									<settlement>Richland</settlement>
									<region>WA, IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fault-Tolerant Communication Runtime Support for Data-Centric Programming Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The largest supercomputers in the world today consist of hundreds of thousands of processing cores and many more other hardware components. At such scales, hardware faults are a commonplace, necessitating fault-resilient software systems. While different fault-resilient models are available, most focus on allowing the computational processes to survive faults. On the other hand, we have recently started investigating fault resilience techniques for data-centric programming models such as the partitioned global address space (PGAS) models. The primary difference in data-centric models is the decoupling of computation and data locality. That is, data placement is decoupled from the executing processes, allowing us to view process failure (a physical node hosting a process is dead) separately from data failure (a physical node hosting data is dead). In this paper, we take a first step toward data-centric fault resilience by designing and implementing a fault0resilient, one-sided communication runtime framework using Global Arrays and its communication system, ARMCI. The framework consists of a fault-resilient process manager; low-overhead and network-assisted remote-node fault detection module; non-data-moving collective communication primitives; and failure semantics and error codes for one-sided communication runtime systems. Our performance evaluation indicates that the framework incurs little overhead compared to state-of-the-art designs and provides a fundamental framework of fault resiliency for PGAS models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The largest systems in the world today already scale to hundreds of thousands of cores. With plans under way for exascale systems to emerge within the next decade, we are likely soon to have systems comprising more than a million processing elements. As researchers work toward architecting these enormous systems, it is becoming increasingly clear that at such scales, resilience to hardware faults is going to be a prominent issue that needs to be addressed. Driven by the needs of large-scale scientific computing applications, a variety of programming models have beenn introduced over the past two decades. While the Message Passing Interface (MPI) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> has become the de facto standard for writing parallel programs, PGAS models have recently gained popularity as well <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Together with these programming models, different communication runtime systems to serve these programming models have also become available <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Fault tolerance in MPI has been an area of significant research <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. Most of this research, however, has focused on allowing the computational processes to survive fault, through either checkpointing or applicationlevel resilience to faults. While a process-driven model for fault tolerance has its benefits, it has the disadvantage that each process manages its data; thus, a failed node implies that the processes residing on those nodes as well as their data are lost and must be recreated or restored. Recently, we have started investigating fault resilience techniques for datacentric programming models such as the partitioned global address space (PGAS) models. The primary difference in data-centric models is the decoupling of computation and data locality. That is, data placement is decoupled from the executing processes, allowing one to view process failure (a physical node hosting a process is dead) separately from data failure (a physical node hosting data is dead).</p><p>However, the first obstruction in providing such data-centric fault resilience is that there is a lack of basic fault resiliency in the underlying communication runtime infrastructure for PGAS models and other associated components such as the process manager. Even for the hard faults, there is no lowoverhead fault detection framework and no support for even a minimal set of fault-resilient collective communication primitives.</p><p>In this paper, we take a first step toward data-centric fault resilience by designing and implementing a fault-resilient, onesided communication runtime framework. Emphasizing the properties of PGAS models for fault resiliency, we present data redundancy models for continued execution during failure and a design for a remote node fault detection module that uses a combination of modern network primitives such as remote direct memory access (RDMA) <ref type="bibr" target="#b15">[15]</ref>, send/receive, and data delivery notification semantics for high-accuracy fault detection. Leveraging this fundamental infrastructure, we design and implement non-data-moving collective communication primitives and provide the foundation for fault-resilient data-moving collectives. We discuss the need for various semantic changes to write-based operations for recovery with data redundancy, and we provide a framework for error notification with one-sided communication primitives. Using Global Arrays (GA) <ref type="bibr" target="#b2">[3]</ref> as an example PGAS model, we implement our design with Aggregate Remote Memory Copy Interface (ARMCI) <ref type="bibr" target="#b6">[7]</ref>, the communication runtime system of Global Arrays <ref type="bibr" target="#b2">[3]</ref>, and refer to our solution as fault-tolerant ARMCI, or FT-ARMCI. Our performance evaluation shows that FT-ARMCI can provide fault resiliency with low overhead. We are currently designing and implementing a fault-resilient, high-order computational chemistry method using Global Arrays. We plan to present the results in the final version of the paper.</p><p>The rest of the article is organized as follows. In Section II, we discuss related work. In Section III, we present the background of our work. In Section IV, we describe the overall design for FT-ARMCI. In Section V, we present a performance evaluation of FT-ARMCI. In section VI, we conclude with a brief summary and discussion of future directions for research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Fault tolerance with high-performance computing has been studied by multiple researchers with various programming models and applications. <ref type="bibr">Fagg and Dongarra et al. introduced FT-MPI [11]</ref>, discussing the process model on occurrence of a failure. Approaches include respawning of MPI processes and patching them with the original communicator or continuing with the holes in the communicator. Our proposed approach is similar to the latter model proposed by FT-MPI; however, fault recovery is done for Global Address space models using Global Arrays <ref type="bibr" target="#b2">[3]</ref>. Gropp and Lusk argued that the statement "MPI is not fault tolerant" is unfounded because fault tolerance is a property of the combination of an MPI program and MPI implementation <ref type="bibr" target="#b8">[9]</ref>. While most MPI implementations choose to abort on a fault, this behavior is not mandated by MPI standard. Gropp Lusk also discussed methods of recovery using dynamic process creation and intercommunicators <ref type="bibr" target="#b8">[9]</ref>. Our proposed approach performs graceful degradation and does not require a process-based fault recovery algorithm at the application level; rather, it requires task-based re-execution.</p><p>With MPI <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, fault tolerance using applicationtransparent/assisted approaches have been discussed widely <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. <ref type="bibr">Gao et al.</ref> observed that user-transparent checkpointing using the Berkeley Lab Checkpoint Restart is beneficial for NAS Parallel Benchmarks and that checkpoint aggregation can reduce the overall time of checkpointing significantly; however, the overhead increases significantly after a small number of processors <ref type="bibr" target="#b9">[10]</ref>. In our approach, we leverage user-assisted data redundancy for fault recovery and perform continued execution with graceful degradation. Bosilca et al. <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr">Bouteiller et al. have</ref> shown that pessimistic message-based logging can be used for recovery on volatile nodes using MPICH-V; however, this approach may not be applicable for one-sided communication libraries such as ARMCI <ref type="bibr" target="#b6">[7]</ref> considered in this work. Another benefit of using the replicated approach is continued execution, whereas message logging requires rollback, although it is useful for applications that are more suited for process-based models.</p><p>Researchers have also focused on providing fault tolerance using virtual machine-based approaches Recent work includes Xen over high-speed networks <ref type="bibr" target="#b11">[12]</ref> in addition to the classical work based on virtual machines using TCP/IP; however, these approaches require either checkpointing for postfailure execution <ref type="bibr" target="#b17">[17]</ref> or proactive fault tolerance <ref type="bibr" target="#b11">[12]</ref>, which depends on the accuracy of fault prediction. Our proposed approach, on the contrary, does not depend on the accuracy of fault prediction, as it is able to perform continued execution upon occurrence of a failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND</head><p>In this section, we present the background of our work. We begin with a description of PGAS models, modern interconnects, and their primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PGAS Models</head><p>Partitioned plobal address space models provide a logical abstraction of global memory space, which is partitioned in remote and local data for leveraging the locality of reference. Many language and library implementations are being designed and implemented under this model, such as X10 <ref type="bibr" target="#b4">[5]</ref>, Chapel <ref type="bibr" target="#b5">[6]</ref>, High Performance Fortran, Unified Parallel C (UPC) <ref type="bibr" target="#b3">[4]</ref>, ZPL, and Titanium. These languages provide mechanisms for accessing data in global space and provide efficient compiler-based implementations to coalesce multiple such data accesses by resolving write dependencies. They also provide mechanisms to allow computation to be executed on a remote node (by using active messages or similar mechanisms). PGAS models provide data-centric abstractions and decouple computation from process-based models. We use this property of PGAS models to provide fault resiliency with FT-ARMCI. Library-based implementations such as Global Arrays <ref type="bibr" target="#b2">[3]</ref> and SHMEM <ref type="bibr" target="#b18">[18]</ref> have also been designed and implemented to serve the purpose of remote memory accesses. The user of Global Arrays <ref type="bibr" target="#b2">[3]</ref> is expected to explicitly request remote/local data pointers and make explicit requests to coalesce data transfers.</p><p>The Global Arrays programming model exposes to the programmer the non-uniform memory access (NUMA) characteristics of high-performance computers. Accesses to a remote portion of the shared data is slower than to the local portion. The locality information for the shared data is available, and a direct access to the local portions of shared data is provided.</p><p>Global Arrays uses ARMCI <ref type="bibr" target="#b6">[7]</ref> as the runtime system for communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modern Interconnects</head><p>In the past decade, high-speed interconnects-both open standard and proprietary-have become available. In the open standard community, InfiniBand <ref type="bibr" target="#b19">[19]</ref> has become popular because of its high performance, including features such as RDMA <ref type="bibr" target="#b15">[15]</ref>, hardware-assisted collective communication primitives, and atomic operations. Similarly, 10 Gigabit Ethernet is becoming popular to support legacy sockets-based applications, while providing primitives for RDMA and zerocopy communication using send/recv primitives <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. In the proprietary interconnects domain, interconnects such as the IBM Blue Gene Torus network and Cray Seastar <ref type="bibr" target="#b22">[22]</ref> have become popular, providing support for RDMA as well as for most features using connectionless transport semantics. High Performance Switch (HPS)IBM's fourth-generation switch, provides support for connectionless RDMA; reliability is implemented by using IBM's LAPI access layer <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>.</p><p>In this paper, we use a combination of RDMA semantics and reliable notification provided by these interconnects to detect remote-node failure(s). The networks discussed above can be classified on the basis of data delivery notification. Networks such as InfiniBand provide "exact once notification" of data delivery with the reliable connection semantics. This results in a guaranteed notification of data delivery, offloaded in the hardware. Other networks such as IBM HPS provide "maximum once notification." Typically, this results in notification when the data delivery is successful; however, data delivery failures are not notified. Keeping these properties in mind, we design and implement the remote fault detection layer presented in Section IV. Our reference implementation over InfiniBand uses "exact once notification" semantics provided by the network for remote-node fault detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Communication Runtime Systems</head><p>Communication runtime systems play an important role in providing efficient, high-performance support to programming models. For PGAS models such as Global Arrays <ref type="bibr" target="#b2">[3]</ref> and Berkeley Unified Parallel C <ref type="bibr" target="#b3">[4]</ref>, the communication runtime systems used are ARMCI <ref type="bibr" target="#b6">[7]</ref> and GASNet, respectively. Each of the communication runtime systems provides generalpurpose, efficient, and widely portable remote memory access (RMA) operations (one-sided communication) optimized for contiguous and noncontiguous (strided, scatter/gather, I/O vector) data transfers. Native network communication interfaces and system resources (such as shared memory) are utilized to achieve the best possible performance of the remote memory access/one-sided communication. Optimized implementations for each of these communication runtime systems are available for Cray Portals, Myrinet (GM and MX) <ref type="bibr" target="#b20">[20]</ref>, Quadrics <ref type="bibr" target="#b21">[21]</ref>, GigaNet (VIA), and InfiniBand (using OpenFabrics and Mellanox Verbs API) <ref type="bibr" target="#b19">[19]</ref>. In addition, they are available for leadership-class machines including Cray XT4/XT5 and Blue Gene/P <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERALL DESIGN</head><p>In this section, we present the overall design of our faultresilient communication runtime system for PGAS models. We begin with a presentation of the properties of PGAS models suited for fault resiliency. We follow this with discussion of our application-level data redundancy model. The overall design is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. Efficient and accurate detection is critical in designing fault-resilient communication runtime systems. In addition, designing a fault-resilient process manager is important, since most process managers abort on occurrence of a node failure. Moreover, a framework for fault-resilient collective communication primitives is needed for data-centric models to maintain the control flow of the application. We have designed a lowoverhead, high-accuracy remote-node fault detection module to achieve this purpose. Using this module, we designed and implemented fault-resilient process manager and fault-resilient non-data-moving collective communication primitives. The details are presented in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Why PGAS Models?</head><p>MPI <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> is the predominantly used programming model for most scientific applications. A broad range of applications, requiring frequent synchronization and precisely described in terms of processes rather than data, naturally fit under this model. However, one class of applications, classified as data centric, follow a task-based execution model and define dependencies in terms of task execution; these applications naturally fit PGAS models.</p><p>An important property of PGAS models is the data-centric nature of the algorithm and independence from the number of processes in the execution model. The nature of the algorithm allows the data requests to be served from arbitrary processes, as long as data consistency is maintained. Thus, the total number of processes can grow and shrink arbitrarily. Assuming a fault model giving a notion of node failure, the data-centric property of the algorithm allows continued execution with minimal changes, compared with the process-centric execution model. Hence, we use PGAS models with Global Arrays as the candidate programming model for implementing our design. However, the generic nature of the design is applicable to runtime systems of other PGAS models such as GASNet <ref type="bibr" target="#b7">[8]</ref>.</p><p>We also use a graceful degradation approach, under which we continue with the available number of processes, rather than respawning the processes of the lost node. The fault recovery with the reincarnation of lost processes becomes expensive, since the new processes may require a synchronous continuum on the failure, which can be prohibitive on exascale systems. The graceful degradation approach also allows to continue execution when the spare nodes are not available-a likelihood on exascale systems with capability-class loads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application-Level Data Redundancy Model</head><p>In this section, we discuss the expected data redundancy model from applications using FT-ARMCI. While the design and development of these applications is on-going, the data redundancy model provides guidelines for designing FT-ARMCI.</p><p>Global Arrays <ref type="bibr" target="#b2">[3]</ref> use the master process on a node <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref> to allocate buffer for global address space; other processes on the node attach to the shared-memory segment. Hence the data redundancy can be achieved at a node level. This can be done by allocating a shadow copy of the global address space on the neighboring node. This is shown in the <ref type="figure">Figure 2</ref> using four nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shadow Copy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node2</head><p>Node3 Node4 Node1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node2</head><p>Node3 Node4</p><p>Primary Copy <ref type="figure">Fig. 2</ref>. Global Address Space Redundancy on Multiple Nodes <ref type="figure">Figure 2</ref> shows that the shadow copy of the local portion of global address space is present on the logical neighboring node. Another assumption in the redundancy model is that each node has enough memory to store the primary and shadow copy completely. This is a limitation of the current model, and we plan to propose space-efficient data redundancy models.</p><p>An important implication of the redundancy is that at least one of the primary or shadow copies should be in a consistent state at the time of recovery. To this end, the communication runtime system must ensure that the same patch of the data is not updated simultaneously in the primary and the shadow copies. We discuss this situation in Section IV-E1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fault Tolerance Management Infrastructure</head><p>The fault tolerance management infrastructure consists of two primary modules: a fault-resilient process manager and a remote-node fault detection module. We present each of these components in detail in the following subsections.</p><p>1) Remote-Node Fault Detection Module: Highly accurate detection of component failure is key to designing a faultresilient communication runtime system. Methods for remotenode fault detection have been proposed in the literature using TCP/IP-based sockets <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>. The kernel involvement and dependency on an entity on a remote node result in greater inaccuracy with this method.</p><p>Modern interconnects provide memory and channel semantics for data transfer with RDMA and send/receive capabilities, respectively. The send/receive semantics bypasses the kernel but requires involvement of remote entity (process/thread on the remote node) to respond to the health checks. The RDMAbased approach does not require involvement of a remote entity. Hence, we use the RDMA-based approach for remotenode fault detection. Using RDMA Read primitives provided by most modern networks and data delivery notification semantics, we conclude that remote node is dead if a failure is received during the read. The details are network specific, and we present the details for our reference implementation with InfiniBand in the following subsection.</p><p>Reference Implementation with InfiniBand: We leverage the "exact once" data delivery notification semantics of reliable connection transport provided by InfiniBand to check the status of the remote node <ref type="bibr" target="#b31">[31]</ref>. A helper thread is created by the master process (only one thread is created per node) during initialization of the communication runtime system. A small buffer is allocated by each thread, which is registered with the InfiniBand hardware; and the associated information is exchanged so that the helper threads can perform RDMA reads on the buffer. The helper thread also performs periodic RDMA reads from nodes to check their health. In addition, itt responds to the health check requests (pongs) from arbitrary nodes. This functionality is also used at multiple execution points explicitly by the ARMCI library to check the status of the remote node, such as fence and collective communication primitives.</p><p>The reference implementation creates an unreliable datagram-based communication channel between all helper threads. As Koop et al. have presented, the unreliable datagram-based approach scales well <ref type="bibr" target="#b32">[32]</ref>. Since reliable connection transport semantics require pairwise connections <ref type="bibr" target="#b27">[27]</ref>, a user-specified topology of reliable connections is also created between the helper threads, with the default being the ring topology. The status of a node can be checked by using RDMA if it is available directly. This is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. For checking the status of an arbitrary node, an unreliable datagram-based virtual connection is used if reliable connection is not available to the node. This is illustrated in the <ref type="figure">Figure 4</ref>. The status of a node can be checked by sending a ping message to that node and sending a ping message to a neighboring node simultaneously. A quorum-based protocol is used to test whether the remote node is healthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node i</head><p>Node i+1  <ref type="bibr" target="#b33">[33]</ref>, most of the PMI implementations are geared to MPI <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. While fault tolerance is not a property of the MPI standard or MPI programs, but rather is a combination of these, state-of-the-art MPI implementations abort on occurrence of a failure. As a result, process management implementations are geared toward aborting on occurrence of a failure. Clearly, this solution does not suffice for the needs of our fault-tolerant communication runtime system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RDMA Read</head><p>To address this issue, we use a process manager distributed with the MVAPICH/MVAPICH2 libraries <ref type="bibr" target="#b34">[34]</ref>, and we enhance the implementation to prevent aborting of the whole job on occurrence of a failure. We plan to integrate the changes required to make the process manager fault resilient with PMI <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fault-Resilient Non-Data-Moving Collective Communication Primitives</head><p>Collective communication primitives are widely used by programming models to provide abstractions for processes in a group to perform an operation. MPI <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> provides a wide variety of data-moving collective communication primitives such as all-to-all broadcast, all-to-all personalized exchange, barrier, reduction, broadcast, and variants of these primitives. However, applications using PGAS models typically use only a small subset of these primitives. Computational chemistry codes such as NWChem <ref type="bibr" target="#b35">[35]</ref> use barrier indirectly by executing a sync operation, which performs active target fence and barrier operatons. Hence, we design and implement a faultresilient barrier primitive as a critical component for our faultresilient one-sided communication library. We will address the topic of fault-tolerant data moving collectives in future work.</p><p>We begin with using the hypercube algorithm for the barrier primitive. The key challenge is to continue the execution of barrier in occurrence of a failur. When a failure occurs, the processes participating with the failed process during the step does not receive a message from the failed process. The participating processes use the remote-node fault detection module presented above to detect the fault. Using this information, these processes communicate with the process, which the failed process would have communicated to in the next step. This is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. As shown in the figure, step 1 finishes successfully for process 2 and process 3, while process 0 does not receive any response from process 1, since process 1 is not alive. Process 0 calculates the destination of process 1 in the next step, process 2, and sends the message to it. Process 2 responds to it in the next step, when it receives the message. This algorithm can esaily be extended for an arbitrary number of failures during execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Semantics for One-Sided Communication Primitives</head><p>In this section, we present the semantics of one-sided communication primitives, which are required to provide fault resiliency to the PGAS models.</p><p>1) Synchronized Write-Based, One-Sided Communication Primitives: One-sided communication primitives provide semantics for buffer reusability with variants for blocking and nonblocking interfaces similar to MPI <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Fence is typically used to ensure the completion of a data transfer operation at a remote node. For Get-based primitives, completion at the initiator side results in completion of the data arrival as well. As presented in Section IV-B, applications using Global Arrays <ref type="bibr" target="#b2">[3]</ref> can leverage a redundant copy of the global address space to ensure that recovery is possible when a node fault occurs.</p><p>A key issue during recovery is that at least one of the copies should be in a consistent state to recover. This requires that any write-based one-sided communication primitive should be "fenced," resulting in each write-based primitive also having to be fenced. This approach guarantees that if a failure occurs during the write, at least the data from one copy is r thatecoverable. However, local completion semantics of Getbased one-sided communication primitives do not require this change. In Section V, we evaluate the impact of this change for microbenchmarks with and without faults.</p><p>2) Error Propagation and Return Codes: Error return codes are a key issue for fault-resilient one-sided communication runtime systems. For our design, we return an error in transmission only during the Get-based primitives, but we do not return error codes during the write-based one-sided communication primitives, put, and accumulate, respectively.</p><p>In addition, changes are required in various protocols of one-sided communication to ensure continued execution during occurrence of a failure. Once a process has failed, we cache the information about the failure, and any further write requests from application are ignored. However, any readbased requests result in a failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PERFORMANCE EVALUATION OF FT-ARMCI</head><p>In this section, we present a performance evaluation of FT-ARMCI. We compare this with the latest release of Global Arrays, version 4.3, which we refer to as "Original" for the rest of the section. We have used Chinook <ref type="bibr" target="#b36">[36]</ref>, an AMD Barcelona-based Supercomputer at Pacific Northwest National Laboratory as the experimental testbed for our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Testbed</head><p>Chinook <ref type="bibr" target="#b36">[36]</ref> is a 160 TFlops system that consists of 2310 HP DL185 nodes with dual socket, 64-bit, Quad-core AMD 2.2 GHz Barcelona processors. Each node has 32 Gbytes of memory and 365 Gbytes of local disk space. Communication between the nodes is performed by using InfiniBand with Voltaire <ref type="bibr" target="#b37">[37]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation without Faults</head><p>In this section, we present the performance evaluation of FT-ARMCI, in the absence of faults. The primary objective is to understand the overhead of FT-ARMCI when no faults occur. We design simple microbenchmarks using one-sided communication primitives and compare the performance of FT-ARMCI with the Original implementation. We use two processes for evaluation, with one process on each node. <ref type="figure">Figures 6 and 7</ref> show the performance evaluation of ARMCI Put contiguous and Put strided one-sided communication primitives, respectively. In the tests process 0 initiates the blocking variant of the communication primitive for a small number of iterations and reports the observed bandwidth. We observe that the peak bandwidth achieved by each of the primitives is similar. However, FT-ARMCI incurs significant overhead for small message latency, as presented in the <ref type="figure">Figures 8 and 9</ref>. For each of these primitives, FT-ARMCI increases the 8-byte message latency to approximately three times. Since each write-based primitive results in an additional exchange of data transfer with the remote node, as presented in Section IV-E1, this overhead is incurred. There are possible performance improvements, such as combining the communication primitive and data transfer for reducing the latency, which we plan to extend in the near term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation with Faults</head><p>In this section, we present a performance comparison of the Original implementation with that of FT-ARMCI in the absence and presence of faults. We evaluate the benchmarks presented in the previous section on a larger number of processes. We modify the benchmarks to report the latency observed at every iteration. At every iteration, each process invokes a one-sided communication primitive in displaced ring communication <ref type="bibr" target="#b39">[39]</ref>, followed by a fence. We compare the Original implementation with no faults, FT-ARMCI No Fault, and FT-ARMCI One Fault. To invoke the faults, we manually kill all the processes on a node at arbitrary points during benchmark execution, in order to emulate a node failure. The point of failure is the middle iteration on the charts. We have chosen this arbitrary communication pattern to show the worst communication pattern. Other patterns reflecting MPIstyle collective communication primitives should incur less overhead. <ref type="figure" target="#fig_0">Figure 10</ref> shows the results for the put communication primitive with 512 processes and 8 bytes. The observed latency is relative to the latency observed in the first iteration of the Original implementation. As explained previously, FT-ARMCI No Fault incurs significant overhead compared to the Original implementation for small messages. At the point of failure, the observed latency is high. A finer-grained analysis of the overheads during the node failure shows that the most time is taken by the timeout before remote-node fault detection module is used. The other overhead is due to the communication protocol of the remote-node fault detection module. While not noticeable in the chart, the overall latency decreases after fault occurrence, because the overall number of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have taken a first step toward data-centric fault resilience by designing and implementing a fault-resilient one-sided communication runtime framework. Emphasizing the properties of PGAS models for fault resiliency, we have presented data redundancy models for continued execution during failure and a design for a remote-node fault detection module that uses a combination of modern network primitives such as remote direct memory access (RDMA) <ref type="bibr" target="#b15">[15]</ref>, send/receive, and data delivery notification semantics for high-accuracy fault detection. Leveraging this fundamental infrastructure, we have designed and implemented non-datamoving collective communication primitives and provided the foundation for fault-resilient data-moving collectives. We have discussed the need for various semantic changes to write-based operations for recovery with data redundancy, and we have provided a framework for error notification with one-sided communication primitives. Using Global Arrays (GA) <ref type="bibr" target="#b2">[3]</ref> as an example PGAS model, we have implemented our design with Aggregate Remote Memory Copy Interface (ARMCI) <ref type="bibr" target="#b6">[7]</ref>, the communication runtime system of Global Arrays <ref type="bibr" target="#b2">[3]</ref>; we refer to our solution as fault-tolerant ARMCI, or FT-ARMCI. Our performance evaluation has shown that FT-ARMCI is able to provide fault resiliency with low overhead. We are currently designing and implementing a fault-resilient, highorder computational chemistry method using Global Arrays, and we plan to present the results in the final version of the paper.</p><p>We will also finish the remaining components for providing fault tolerance. Our immediate goal is to complete the writing of a fault-resilient high-order computational chemistry method such as coupled cluster. In addition, we are working on faulttolerant data-moving collectives with data-centric abstractions, rather than process-centric abstractions such as MPI. As we continue to develop these components, we will conduct largescale evaluations and will make performance improvements to protocols and components.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall Design of FT-ARMCI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Remote-Node Fault Detection When RDMA Is Available Directly</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Execution of Fault-Tolerant Hypercube Algorithm for Barrier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 6. ARMCI Put Unidirectional Bandwidth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>switches and Mellanox [38] adapters. The system runs a version of Linux based on Red Hat Linux Advanced Server. A global 297 Tbyte SFS file system is available to all the nodes.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="789" to="828" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MPI-2: Extending the messagepassing interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huss-Lederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Saphir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Skjellum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euro-Par</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global Arrays: A Nonuniform Memory Access Programming Model for HighPerformance Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Littlefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="189" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Performance Analysis of the Berkeley UPC Compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iancu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">X10: An Object-Oriented Approach to Non-Uniform Cluster Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grothoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Donawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kielstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ebcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von Praun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA &apos;05: Proceedings of the 20th Annual ACM SIGPLAN Conference on ObjectOriented Programming, Systems, Languages, and Applications</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="519" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel Programmability and the Chapel Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="312" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ARMCI: A Portable Remote Memory Copy Library for Distributed Array Libraries and Compiler Run-Time Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="533" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Problems with Using MPI 1.1 and 2.0 as Compilation Targets for Parallel Language Implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bonachea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fault Tolerance in Message Passing Interface Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Application-Transparent Checkpoint/Restart for MPI Programs over InfiniBand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FT-MPI: Fault Tolerant MPI, Supporting Dynamic Applications in a Dynamic World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European PVM/MPI Users&apos; Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface</title>
		<meeting>the 7th European PVM/MPI Users&apos; Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High Performance VMM-Bypass I/O in Virtual Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATEC &apos;06: Proceedings of the annual conference on USENIX &apos;06 Annual Technical Conference. USENIX Association</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MPICH-V: Toward a Scalable Fault Tolerant MPI for Volatile Nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bosilca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouteiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Djilali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fedak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Herault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lemarinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lodygensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Magniette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selikhov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>in Supercomputing &apos;02: Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">ACM/IEEE conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MPICH-V2: A Fault Tolerant MPI for Volatile Nodes based on Pessimistic Sender Based Message Logging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouteiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Herault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krawezik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lemarinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Magniette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;03: Proceedings of the 2003 ACM/IEEE conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">User-Level Network Interface Protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A F</forename><surname>Bhoedjang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>RÃ¼hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Bal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Checkpoint and Restart Service Specification for Open MPI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hursey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Squyres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<idno>TR635</idno>
		<imprint>
			<date type="published" when="2006-07" />
			<pubPlace>Bloomington, Indiana, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Indiana University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable Transparent Checkpoint-Restart of Global Address Space Applications on Virtual Machines over InfiniBand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Brown</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CF &apos;09: Proceedings of the 6th ACM conference on Computing frontiers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A New MPI Implementation for Cray SHMEM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brightwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroPVM/MPI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="122" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">InfiniBand Architecture Specification, Release 1.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Infiniband Trade Association</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Myrinet: A Gigabit-per-second Local Area Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Felderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kulawik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Seizovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1995-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Quadrics Network: High-Performance Clustering Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoisie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SeaStar Interconnect: Balanced Bandwidth for Scalable Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brightwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Pedretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="41" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance and Experience with LAPI -a New High-Performance Communication Library for the IBM RS/6000 SP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dinicola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS/SPDP</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="260" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Architecture and Early Performance of the New IBM HPS Fabric and Adapter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Hochschild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Grice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blackmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goscinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Breaking the Connection: RDMA Deconstructed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sivaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Hochschild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blackmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Interconnects</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Deep Computing Messaging Framework: Generalized Scalable Message Passing on the Blue Gene/P Supercomputer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dozsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Almasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Giampapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blocksome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ratterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Archer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;08: Proceedings of the 22nd annual international conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient On-Demand Connection Management Protocols with PGAS Models over InfiniBand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cluster, Cloud and Grid Computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Hardware-Software Approach to Network Fault Tolerance wwith InfiniBand Cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K P M K</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cluster Computing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CIFTS: A Coordinated Infrastructure for Fault-Tolerant Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hargrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP &apos;09: Proceedings of the 2009 International Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable, Fault Tolerant Membership for MPI Tasks on HPC Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;06: Proceedings of the 20th Annual International Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic Path Migration over InfiniBand: Early Experiences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mamidala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Third International Workshop on System Management Techniques, Processes, and Services, held in conjunction with IPDPS&apos;07</title>
		<meeting>Third International Workshop on System Management Techniques, Processes, and Services, held in conjunction with IPDPS&apos;07</meeting>
		<imprint>
			<date type="published" when="2007-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High performance mpi design using unreliable datagram for ultra-scale infiniband clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and Scalable Startup of MPI Programs in InfiniBand Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="440" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<ptr target="http://mvapich.cse.ohio-state.edu/" />
		<title level="m">MVAPICH/MVAPICH2: MPI-1/MPI-2 for InfiniBand and iWARP with OpenFabrics</title>
		<imprint/>
		<respStmt>
			<orgName>Network-Based Computing Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High Performance Computational Chemistry: An Overview of NWChem, A Distributed Parallel Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>AprÃ </surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Bernholdt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bylaska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Fann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Straatsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Windus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Physics Communications</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="260" to="283" />
			<date type="published" when="2000-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<ptr target="http://emsl.pnl.gov" />
	</analytic>
	<monogr>
		<title level="j">Chinook SuperComputer, Environmental Molecular Science Lab, PNNL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Voltaire Technologies</title>
		<ptr target="http://www.voltaire.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mellanox Technologies</title>
		<ptr target="http://www.mellanox.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hot-Spot Avoidance with Multi-Pathing Over InfiniBand: An MPI Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mamidala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing and Grid</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
