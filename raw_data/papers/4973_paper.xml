<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Parallel Programming with MPI and Unified Parallel C *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>May 17-19, 2010,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Dinan</surname></persName>
							<email>dinan@cse.ohio-state.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewing</forename><surname>Lusk</surname></persName>
							<email>lusk@mcs.anl.gov</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadayappan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Thakur</surname></persName>
							<email>thakur@mcs.anl.gov</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. Comp. Sci. and Eng</orgName>
								<orgName type="department" key="dep2">Dept. Comp. Sci. and Eng</orgName>
								<orgName type="laboratory">Math. and Comp. Sci. Division Argonne National Laboratory Math. and Comp. Sci. Division Argonne National Laboratory</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>2015 Neil Avenue Columbus, 9700 S. Cass Avenue Argonne, 9700 S. Cass Avenue Argonne</addrLine>
									<region>OH U.S.A., IL U.S.A., IL U.S.A</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Math. and Comp. Sci. Division Argonne National Laboratory</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>2015 Neil Avenue Columbus, 9700 S. Cass Avenue Argonne</addrLine>
									<region>OH U.S.A., IL U.S.A</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Parallel Programming with MPI and Unified Parallel C *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">May 17-19, 2010,</date>
						</imprint>
					</monogr>
					<note>-edges that this contribution was authored or co-authored by an employee, contractor or affiliate of the U.S. Government. As such, the Government re-tains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. CF&apos;10,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D13 [Programming Techniques]: Concurrent Programming- Parallel programming; D33 [Programming Languages]: Lan- guage Constructs and Features-Concurrent programming struc- tures General Terms Design</term>
					<term>Languages</term>
					<term>Performance Keywords MPI</term>
					<term>UPC</term>
					<term>PGAS</term>
					<term>Hybrid Parallel Programming</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The Message Passing Interface (MPI) is one of the most widely used programming models for parallel computing. However, the amount of memory available to an MPI process is limited by the amount of local memory within a compute node. Partitioned Global Address Space (PGAS) models such as Unified Parallel C (UPC) are growing in popularity because of their ability to provide a shared global address space that spans the memories of multiple compute nodes. However, taking advantage of UPC can require a large re-coding effort for existing parallel applications. In this paper, we explore a new hybrid parallel programming model that combines MPI and UPC. This model allows MPI programmers incremental access to a greater amount of memory, enabling memory-constrained MPI codes to process larger data sets. In addition, the hybrid model offers UPC programmers an opportunity to create static UPC groups that are connected over MPI. As we demonstrate, the use of such groups can significantly improve the scalability of locality-constrained UPC codes. This paper presents a detailed description of the hybrid model and demonstrates its effectiveness in two applications: a random access benchmark and the Barnes-Hut cosmological simulation. Experimental results indicate that the hybrid model can greatly enhance performance; using hybrid UPC groups that span two cluster nodes, RA performance increases by a factor of 1.33 and using groups that span four cluster nodes, Barnes-Hut experiences a twofold speedup at the expense of a 2% increase in code size.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Message Passing Interface (MPI) is considered to be the de facto standard for parallel programming today <ref type="bibr" target="#b9">[11]</ref>. The flexible, feature-rich interface provided by MPI has successfully allowed many complex scientific applications to be represented and mapped efficiently to large-scale high-end computing systems. However, the amount of memory available to an MPI process is limited by each process's virtual address space; and, for a variety of scientific applications, this space is insufficient to solve emerging problems.</p><p>Many scientific applications today are written in MPI using a one-process-per-core model that partitions memory among the cores. As systems grow, memory per core remains constant or decreases. Shared memory hybrid parallel programming with MPI and OpenMP avoids partitioning of memory and, for some applications, provides access to a large enough amount of memory to simulate increasingly large problems <ref type="bibr" target="#b16">[18]</ref>. For many other applications, however, the memory requirement grows superlinearly with problem size. In particular, the simulation of the phenomena n the nucleus of an atom via the Green's function Monte Carlo (GFMC) method has a per process memory requirement that grows as 2 A · A! in the number of nucleons <ref type="bibr" target="#b15">[17]</ref>. Hybridization of this MPI code with OpenMP has successfully extended it to simulate carbon-12, which requires roughly 0.5 GB memory per node. For larger atoms, however, the per-MPI-process memory requirements quickly exceed the available memory per node. Thus, a new solution is needed.</p><p>Partitioned global address space (PGAS) models such as the Unified Parallel C (UPC) <ref type="bibr" target="#b19">[21]</ref> are relative newcomers to large-scale scientific computing. However, they are gaining popularity because of their ability to provide access to a large, convenient shared global address space. This global address space can span the memory of multiple processes providing access to a large aggregate storage space rather than being restricted by each process's virtual address space. This, together with powerful locality-aware one-sided mechanisms to access the global address space, makes UPC an attractive model for space-constrained scientific codes. However, converting existing parallel programs to UPC to take advantage of its global address space is a daunting task and can require rewriting large amounts of code.</p><p>In this paper, we present a new hybrid parallel programming model that combines MPI and UPC, allowing the programmer to leverage the strengths of both models. The hybrid model provides an incremental pathway to extending existing MPI programs to utilize UPC's partitioned global address space and powerful one-sided communication features. The hybrid model also enables UPC programmers to leverage MPI for process management and to create static UPC groups connected with MPI communication links. Such groups can be applied to static, distributed, shared multidimensional arrays that provide the highest convenience and productivity to UPC programmers. These static groups can eliminate the complexity of manual redistribution of UPC shared data while greatly improving the scalability of UPC codes that lose performance through diminishing locality as the number of processors grows. In addition, interoperability with MPI exposes opportunities for UPC programmers to take advantage of existing parallel libraries and solvers, such as ScaLAPACK <ref type="bibr" target="#b3">[5]</ref>.</p><p>This paper presents a detailed description of the hybrid MPI+UPC parallel programming model and describes the tools and techniques needed to enable hybrid execution and mitigate the complexity of managing two parallel programming models. Using these techniques, we demonstrate the effectiveness of the model in two applications: a random-access benchmark that models the characteristics of the GFMC many-body simulation and the Barnes-Hut cosmological n-body simulation <ref type="bibr" target="#b1">[3]</ref>. Experimental results indicate that for UPC global address spaces that span four cluster nodes, the hybrid model offers up to a twofold speedup for Barnes-Hut and a 25% performance boost for the random access benchmark when compared with a baseline execution on the same number of processor cores. In the case of the Barnes-Hut benchmark, the complexity cost of hybridization was a 2% in the number of lines of code. To our knowledge, this is the first such demonstration of a hybrid MPI+UPC application.</p><p>The rest of the paper is organized as follows. In Section 2 we give an overview of the MPI and UPC parallel programming models and discuss why MPI-2 one-sided messaging extensions fall short of providing the desired global address space programming model. In Section 3 we describe the hybrid MPI+UPC programming model, breaking it into three models that vary the relationship between UPC and MPI: flat, nested-funneled, and nested-multiple. In Section 4 we present an experimental evaluation of the hybrid model in two applications: a randomaccess benchmark and the Barnes-Hut n-body simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW OF MPI AND UPC</head><p>In this section we provide an overview of the MPI and UPC parallel programming models and discuss the strengths and weaknesses of each model in order to identify the advantages of combining them to form a hybrid programming model. Overall, we find that hybridization with UPC compliments MPI codes by adding a large, asynchronous global address space and that hybridization with MPI enhances UPC by providing additional mechanisms for locality control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Message Passing Interface</head><p>The Message Passing Interface (MPI) <ref type="bibr" target="#b9">[11]</ref> is one of the most prevalent and highly tuned parallel programming models and is available on most high-performance computing systems. The MPI-1 standard provides for two-sided messaging where communicating pairs of processes call Send and Recv to transmit a message, as well as a variety of powerful and efficient collective operations. The MPI-2 standard <ref type="bibr" target="#b10">[12]</ref> added support for one-sided messaging that allows processes to perform remote access to exposed regions of memory. In this model, processes collectively expose windows of memory for remote access. The window may then be accessed in either active or passive target mode. Under the active target mode, the host explicitly synchronizes its window between accesses; under the passive target mode, remote processes may access the window without any explicit interaction from the host.</p><p>MPI-2's passive mode provides a model similar to UPC's global address space programming model. However, while the passive mode provides the capability to access remote memory in a onesided manner, it is more restrictive than a global address space in terms of its consistency, coherence, and synchronization characteristics <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b18">20]</ref>. These restrictions have enabled MPI-2 to be supported on a variety of systems, including those that do not provide a cache coherent memory system. However, on systems that do provide memory coherence, these same portability restrictions make it difficult for the programmer to get access to the productivity and performance gains provided by the hardware. In comparison, UPC's model assumes memory coherence and is able to support fine-grained, asynchronous communication and dynamic distributed data structures that present challenges to the MPI-2 model. For these reasons, MPI-2's model is generally considered to provide one-sided messaging rather than a global address space. The restrictions on passive mode communication can be summarized as follows:</p><p>Access Epochs: The remote process must always lock a window before accessing it. The time between lock and unlock operations is referred to as the access epoch. All accesses, including local accesses and accesses that do not overlap, must occur within an access epoch; otherwise the program is in error.</p><p>Ordering: During an access epoch, puts and gets to a window may be reordered by the runtime system. Thus, if a location is written to within an epoch, it cannot be read from or written to again within the same epoch. This restriction makes algorithms that perform read-modify-write operations extremely difficult to implement.</p><p>Concurrency: All accesses to a window, whether local or remote, must occur within a locked access epoch. Hence, concurrency may be greatly impacted if the number of windows is low. Because MPI-2's locks are associated with window objects, the programmer may be compelled to create multiple windows in order to achieve finer-grained locking.</p><p>Window Creation: Window creation is a collective operation. Thus, the creation of multiple windows to increase concurrency must be done collectively. Hence, handling dynamic allocation of elements in distributed data structures (e.g., trees) may require the user to create a system for managing preallocated buffers.</p><p>Shared Pointers: MPI window objects cannot be transferred between processes. Thus, creation of distributed linked data structures requires extra bookkeeping in order to create an out-of-band system for managing portable shared pointers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unified Parallel C</head><p>UPC is a extension of the C programming language that adds support for parallel programming with distributed, shared data. UPC is supported on both shared-and distributed-memory systems and presents the programmer with a logically partitioned global address space that is physically distributed across available memory domains. Under UPC, every shared data element has affinity to a distinct processor, and UPC exposes this data locality information to the programmer so that it can be leveraged to enhance performance. Shared data can be accessed through built-in language-level support as well as through explicit one-sided communication operations. Regardless of location, all data in the global address space can be accessed without explicit help from the data's owner.</p><p>A distinct advantage of UPC over libraries such as MPI is that, because of its language extensions, UPC can offer a tunable approach to performance. The programmer may start with a sequential program and convert it to a simple, shared-memory implementation. From this implementation, the programmer can then incrementally enhance the performance by tuning data locality and array distributions and by relaxing the memory model. For performancecritical sections of the code, the programmer can delve more deeply through explicit memory management and one-sided communication.</p><p>In spite of these advantages, UPC does not provide process groups, and UPC's distributions do not provide the flexibility needed to allocate distributed shared arrays on a subset of processors. Thread teams have been proposed as a UPC extension. However, it is unclear how dynamically created teams can be applied to statically declared distributed shared multidimensional arrays, which offer the highest convenience to UPC programmers. As we discuss in Section 3, the hybrid MPI+UPC model offers a solution that allows the programmer to replicate static shared arrays over hybrid groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MPI + UPC HYBRID MODEL</head><p>The hybrid MPI+UPC programming model combines MPI and UPC in the same program, allowing the programmer to take advantage of MPI's locality control and UPC's global address space. Because UPC is an extension to the C programming language and MPI is a library, a hybrid program is simply a UPC program with calls to the MPI library. This program is compiled with the UPC compiler and linked with the MPI libraries. Many existing scientific applications are written in MPI using Fortran. These programs can still benefit from hybridization by linking with an external UPC library that provides access to data stored in a global address space.</p><p>In the UPC terminology, a single unit of execution is referred to as a UPC thread. Each thread in a UPC execution is given the constants MYTHREAD and THREADS to identify itself in the computation. This terminology is meant to emphasize UPC's goal of providing shared-memory-like programming. In practice, however, most UPC distributions implement UPC threads as operatingsystem-level processes. In this work we focus on this model, but the techniques and hybrid programming model can also be applied to UPC implementations that use threads. Thus, in our discussion we assume that processes in a UPC application all share a common global address space but have distinct private virtual address spaces and instances of the MPI library. For this reason, unlike with hybrid MPI+OpenMP model, sharing a single MPI rank between multiple UPC processes is not easy. Yet such sharing is desirable because we want UPC global address spaces to span multiple cluster nodes and MPI does not support sharing a rank across cluster nodes. We define the hybrid MPI+UPC programming model in terms of submodels that vary the level of nesting and number of instances of both models. A representation of these models is shown in <ref type="figure">Fig- ure</ref> 1. The first model, referred to as the flat model, provides a nonnested common MPI and UPC execution where each process is a part of both the MPI and the UPC execution. Thus, each process has both a unique MPI rank and a UPC thread identifier.</p><p>The second and third models show two nested modes where multiple UPC executions have been launched in the context of a single, outer MPI environment. Each UPC process can communicate via UPC only within its group. Intergroup communication is performed by using MPI. The nested-funneled model provides an operational mode similar to the funneled mode in hybrid MPI+threads. That is, only the master process per group gets an MPI rank and can make MPI calls. The nested-multiple model, on the other hand, provides a mode where every UPC process gets its own MPI rank and can make MPI calls independently.</p><p>When a hybrid MPI+UPC program is executed, the execution is parameterized by a set of ranks and a set of group sizes. The hybrid model inherits the UPC constants THREADS and MYTHREAD, which specify the number of UPC threads (i.e., processes) within a UPC execution and an individual thread's rank. Similarly, each process in the outer MPI execution has a rank within the MPI WORLD communicator and is provided with the size of the MPI execution. In the nested model, where multiple UPC instances are launched as part of the same outer MPI environment, the execution is further parameterized by a group rank and the number of groups.</p><p>Thus, from the point of view of a single computational entity in a hybrid computation, the structure of the hybrid execution can be fully described by the group, UPC THREAD ID, and MPI rank,</p><formula xml:id="formula_0">ID = idgroup, idUP C , idMP I ,</formula><p>and by the number of UPC groups, UPC group size (we assume a model where all UPC groups are the same size), and MPI communicator size,</p><formula xml:id="formula_1">N = ngroup, nUP C , nMP I .</formula><p>These two sets of parameters fully describe all hybrid MPI and UPC combinations. However, this model may be unnecessarily complex for some codes. In the next two subsections, we describe two restrictions on this model, the flat and nested-funneled models, which offer lower complexity when the full model is not needed. We also describe the full nested-multiple model and provide techniques to mitigate complexity and improve programmability. For each model, we give an example hybrid dot product code. This simple code is not intended to motivate the hybrid model because efficient MPI-and UPC-only implementa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Flat Model</head><p>The flat model is the most straightforward hybrid MPI+UPC model. This model, shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, restricts the full model by fixing the number of groups at one. Thus, the model is composed of conjoined UPC and MPI executions where all processes participate in both UPC and MPI communication. In this model, ngroup is 1, and thus the group rank and size can be ignored. Also, nUP C = nMP I , allowing the programmer to eliminate either the UPC or the MPI parameter. Thus, execution under this model can be parameterized by ID = idMP I and N = nMP I .</p><p>An example flat-style hybrid program is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This program computes the dot product of two distributed shared vectors by first finding the partial dot product of all the local elements and then using an MPI reduction to find the sum the partial sums.</p><p>In this program, all UPC threads participate in MPI communication and must initialize MPI. There is no guarantee that MPI will assign ranks equal to the UPC thread IDs because it is unaware of UPC. In order to renumber the MPI space so that MPI and UPC ranks are equal, a new communicator is formed by calling MPI_Comm_split(). In the call, all processes provide the same color to indicate they all wish to join the same communicator and provide their UPC thread ID as the key. The key values are then sorted by MPI, and ranks are distributed in order of the keys provided. Since each UPC thread provides MYTHREAD as the key, its rank in the new MPI communicator will be equal to MYTHREAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Nested Model</head><p>The nested model, shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b) and 1(c), can be viewed in two ways. For MPI programmers, this model reflects an outer MPI execution that contains several UPC executions, each with its own global address space. For UPC programmers, this model can be viewed as multiple instances of their UPC program connected using an outer layer of MPI. We refer to each UPC execution as a UPC group and parameterize the nested execution using the group rank and the number of groups as well as the number of UPC threads per group (we assume groups are of uniform size) and the number of MPI ranks per group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Nested-Funneled Model</head><p>When the number of MPI ranks per group is restricted to 1, we call this the nested-funneled model. In this model, only one process per UPC group can participate in MPI communication. The reason is UPC threads are generally implemented as distinct processes that cannot share an MPI rank. Thus, in the nested-funneled model, the number of MPI ranks must equal the number of groups. This restriction allows the MPI rank of a group to be used as its group ID and allows the total number of MPI ranks to determine the number of groups. Thus, this model is parameterized by the UPC and MPI ranks and process counts: ID = idUP C , idMP I and N = nUP C , nMP I .</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref> we show the dot product example written for the nested-funneled hybrid model. In this model, only one member of each UPC group (MYTHREAD == 0) initializes MPI and is able to make MPI calls. Once this master thread has initialized MPI, it does not need to remap MPI's ranks, as in the flat model, because the ranks will be used as the UPC group IDs. However, the MPI rank and size must be made available to all UPC threads in the group because they also indicate the group rank and number of groups. Therefore, rank and size are stored in shared variables accessible by all threads.</p><p>The nested hybrid model has two explicit levels of parallelism. Hence, the work must be partitioned twice, once across groups and again within groups. The partitioning across groups is done in blocks with block size B. Partitioning within each UPC group is done by using the upc_forall construct, which distributes iterations of the loop according to the affinity expression, &amp;v1 <ref type="bibr">[i]</ref>. UPC will assign each iteration to the thread that has affinity to the corresponding element of v1.</p><p>Once the partial sums are computed for each thread, the partial sum for the group is computed by performing a upc_all_reduceD across each UPC group. This operation has a synchronized entry and exit to ensure that all values are ready when the reduction starts and that all UPC communication is completed before performing the next step. Finally, the master thread performs an MPI reduction to find the global sum. Only the master thread finalizes MPI before the program exits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Nested-Multiple Model</head><p>Nested-multiple is the most powerful because it allows MPI to span all processes in all groups. However, this added flexibility comes with greater complexity. Because the number of UPC and MPI processes and groups may all differ, this model requires the full parameterization: ID = idgroup, idUP C , idMP I and N = ngroup, nUP C , nMP I .</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref> we show the dot product example written for the nested-multiple execution model. In the nested-multiple model multiple UPC groups are launched, and all processes in each group initialize MPI. There is no guarantee that MPI will be able to assign ranks that are contiguous within a UPC group because it is unaware of UPC. Thus, each group has to identify its group ID using a separate mechanism. For example, all MPI processes with UPC thread ID zero can exchange their MPI ranks with each other, sort them, and use this ordering as its UPC group ID. However, if the MPI ranks are renumbered to be contiguous across groups and given that we have restricted groups to be of uniform size, we can find the group ID by dividing the MPI rank of any process by the size of a group idgroup = idMP I /T HREADS. Likewise, since all processes are part of the outer MPI environment, the number of groups can be found by dividing the number of MPI ranks by the size of a group, ngroup = nMP I /nUP C .</p><p>This renumbering of MPI ranks can be accomplished by exchanging r0 the MPI rank of thread 0 in every group and calling MPI_Comm_split with the same color and key = r0 * T HREADS+ M Y T HREAD. The result is a new communicator where the MPI ranks have been assigned contiguously across UPC groups.</p><p>Once renumbering is complete, the work is partitioned into blocks of indices assigned to each group and again into individual iterations assigned the each UPC thread within a group. In contrast to the two-step reduction required by the nested-funneled model, in the nested-multiple model where all threads participate in the outer level of MPI parallelism the partial sums can be reduced to the global sum directly through a collective call to MPI_Reduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Launching Hybrid Computations</head><p>Launching a hybrid MPI+UPC application requires nesting one or more UPC launches within an MPI launch. MPI and UPC both require bootstrap mechanisms that spawn processes either through an existing remote shell service or through specialized servers and pass execution information to the new processes through the environment or command line arguments. This information informs the new process of how to join the computation, often by providing the location of a bootstrap server that can be reached over an existing communication channel such as TCP/IP. In this section we describe tools and techniques that can be used to launch hybrid computations. The complexity involved in launching hybrid jobs can easily be hidden from the user via a wrapper that automates this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Nested-Funneled Launching</head><p>The nested-funneled model requires an MPI launcher that supports MPMD launching in order to provide different parameters to each UPC group. Below is an example launch where each MPMD task provided to mpiexec is separated by a ":".</p><p>$ mpiexec \ -env HOSTS=hosts.1 upcrun -n N myapp \ : -env HOSTS=hosts.2 upcrun -n N myapp \ : ...</p><p>In this example several MPI tasks are launched, each of which executes upcrun. Each instance of upcrun is supplied with a different value for the environment variable HOSTS, which informs upcrun which hosts it should launch the UPC group on. Once UPC has bootstrapped and brought each group online, a single UPC thread from each group will call MPI_Init to start MPI and request an MPI rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Flat and Nested-Multiple Launching</head><p>When flat and nested-multiple hybrid applications are launched, each task (i.e., UPC group) that the MPI launcher starts will request multiple ranks. In contrast, in the nested-funneled case each task requests only a single rank. We have extended the Hydra process manager that is included with the MPICH2 MPI distribution <ref type="bibr">[1]</ref> to allow the user to specify how many ranks will be assigned to each task that is launched. This specification is accomplished through the -ranks-per-proc flag.</p><p>Using the Hydra process manager, a flat hybrid program is launched as follows:</p><p>$ mpiexec --ranks-per-proc=N \ upcrun -n N myapp</p><p>In this case, a single task is launched that will in turn launch an Nprocess UPC execution. Each UPC thread will then initialize MPI, requesting N total ranks from the single task that mpiexec has launched. Thus, -ranks-per-proc informs the MPI process manager of this change in the MPI bootstrapping model. A nested-multiple hybrid program is launched as follows:</p><p>$ mpiexec --ranks-per-proc=N \ -env HOSTS=hosts.1 upcrun -n N myapp \ : -env HOSTS=hosts.2 upcrun -n N myapp \ : ...</p><p>In this case, multiple MPI tasks are launched, and each will spawn an N process UPC execution. The -ranks-per-proc flag is used to inform the MPI process manager that each task it launched will require N ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>We evaluate the hybrid programming model using two benchmarks: a random access benchmark and a hybrid implementation of the Barnes-Hut cosmological n-body simulation. These applications were chosen because their baseline UPC implementations exhibit poor performance as the number of processors is increased, due to a corresponding decrease in locality. In addition, both hybrid MPI+UPC benchmarks demonstrate the use of distributed, shared data structures that are challenging to implement with MPI alone: a distributed shared array that supports fine-grained, one-sided access and a distributed shared tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Random-Access Benchmark</head><p>The random-access benchmark models an application where a large, distributed shared array is accessed by all processors with an irregular access pattern. Replicating the array on all processors is not possible because of memory limitations, and exchanging information via a regular communication pattern is not feasible because of the irregular access pattern. This benchmark is motivated by Green's function Monte Carlo codes where irregular accesses are performed on large shared arrays and information is accumulated locally. Periodically this local information is contributed to the global solution <ref type="bibr" target="#b15">[17]</ref>.</p><p>The random access benchmark uses a static UPC distributed shared array of the form:</p><formula xml:id="formula_2">shared double data[n];</formula><p>The array is cyclically distributed with the default block size of 1 across all threads. Once the array has been initialized, every process performs a fixed number of accesses to random elements in  <ref type="figure">as many elements are local to processor 0 than in (a)</ref>. the array. For each access, they perform 1,000 floating-point operations of computation. When run in full UPC mode, shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a), a single array of n elements is distributed across all UPC threads. Thus n/T HREADS elements are local to each thread. When run in hybrid mode, the benchmark executes in the nested-multiple model, as shown in <ref type="figure" target="#fig_4">Figure 5(b)</ref>. In this model, multiple UPC groups will be launched, and each UPC group will have a full replica of the array that is distributed across only the UPC threads in the group. For groups of size G, therefore, n/G elements will be local to each thread. This benchmark demonstrates replication of shared arrays through hybridization and effectively trades space for locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hybrid N-Body Simulation</head><p>Barnes-Hut is an n-body cosmological simulation algorithm <ref type="bibr" target="#b1">[3]</ref>. This algorithm simulates the pairwise gravitational interactions of n bodies distributed through a 3D region of space. Each body has a position, mass, and velocity associated with it; the algorithm simulates the motion and interaction of the bodies given their initial conditions. Barnes-Hut improves on the naive O(n 2 ) algorithm by summarizing the interaction of distant bodies as an interaction with the center of mass of their region of space. Thus, Barnes-Hut has a computational complexity of O(n log n).</p><p>In Algorithm 1 we present the hybrid MPI+UPC Barnes-Hut algorithm written for the nested-funneled execution model. Barnes-Hut iterates the simulation for tmax time steps. In each time step a new oct-tree is created to represent the decomposition of the 3D region of space under simulation, and the bodies are loaded into the tree. Each node in the oct-tree represents a volume of 3D space. When a node is split, it is split in half along each dimension, resulting in 2 3 = 8 new children.</p><p>Once the tree has been built, a bottom-up traversal is done to summarize the center of mass for each subtree. Next, the bodies are evenly partitioned across all processors using a space-filling curve. When partitioning is complete, each process computes the interactions of each of its bodies to find its state in the next time step. Once all processors finish the force computation, the list of bodies is updated to reflect their new states. Next, a groupwise gather is done to assemble the new bodies into our_bodies. This groupwise gather is characteristic of the nested-funneled model because each group has a single MPI rank making for a multilevel The total number of source lines of code for the original UPC implementation of Barnes-Hut was 2,454 and the number of lines of code after hybridization with MPI was 2,505. Thus, hybridization resulted in 51 additional lines of code for an overall code size increase of 2%. In the nested-funneled example in <ref type="figure" target="#fig_2">Figure 3</ref> we saw that the code required to manage both models is modest. New code in Barnes-Hut comes primarily from enhancing work (body) distribution to be multi-level parallel and from gathering of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Evaluation</head><p>Experiments were conducted an 877-node IBM 1350 cluster located at the Ohio Supercomputing Center. This cluster is configured with two dual-core 2.6G Hz AMD Opteron processors and 8 GB RAM in each node; the interconnect is 10 GBps Infiniband.</p><p>For our evaluation we used the Intrepid GCCUPC v2.3.4.6 compiler with the Berkeley UPC v2.8.0 runtime. The runtime was configured to use the high-performance Infiniband interconnect with the SSH bootstrap. Using the SSH bootstrap instead of the default MPI bootstrap allows us more flexibility in the interaction between UPC and MPI. For our MPI we used MVAPICH-2 v1.2 with the Hydra process manager, which provides MPMD launching support. MPI and the Berkeley UPC runtime were compiled using the Intel C compiler and hybrid applications were compiled using GCCUPC and linked with the MPI and Berkeley UPC libraries.</p><p>For each benchmark we show the performance for a baseline UPC version and hybrid MPI+UPC versions with groups of 4, 8, and 16 cores. Groups of 4 cores span a single node and represent an ideal performance because all data in the UPC global address space is local. On this system, 4-core groups can also be achieved by using the hybrid MPI with OpenMP or threads models. However, larger groups that span multiple nodes are not possible in these models. In practice, the lower bound on group size will be dictated by an application's memory requirements and may require groups that span multiple nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Random-Access Benchmark Performance</head><p>A weak scaling experiment was run with the random-access benchmark to show the effect of data locality on performance. Results in <ref type="figure">Figure 6</ref> show the time required for each process to perform 1,000,000 random accesses to a distributed shared array with 1,000,000 elements. Timings are reported for the baseline UPC implementation and MPI+UPC implementations with group size 4, 8, and 16. The ideal performance for this experiment is a flat line because the work per processor is constant.</p><p>For the baseline UPC implementation, the time rapidly increases, indicating that the average latency of each array access is increasing proportionally to the number of processors. For the hybrid implementations, however, the time remains flat indicating that the average latency remains fixed regardless of the processor count. This trend is explained by the data shown in <ref type="figure">Figure 7</ref>. In this graph, we show the percentage of all accesses to the shared array that access a local element of the array. Because the element accessed is chosen with uniform randomness, this data also shows the percentage of the array that is local to any processor. For the baseline UPC implementation, the percentage of local data decreases proportional to the number of processes as n/T HREADS. In comparison, for a hybrid implementation with group size G the percentage of local data decreases in proportion with the group size as n/G rather than with the number of processors.</p><p>If we express the average latency in terms of the latency l l to access a local element, the latency lr to access a remote element, and P (l) the probability that a randomly selected element will be local, we have lavg = l l ·P (l)+lr·(1−P (l)) = (l l −lr)·P (l)+lr. Thus, as the number of processors is increased and P (l) approaches 0, the average latency approaches lr, which is reflected in the data for the baseline UPC implementation on large processor counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Barnes-Hut Performance</head><p>Strong scaling experiments were conducted with the Barnes-Hut force computation kernel. The results are reported in <ref type="figure">Figure 8</ref> are for a 150,000-body system. We show the speedup for the baseline UPC implementation as well as for hybrid implementations with UPC group sizes of 4, 8, and 16. In <ref type="figure">Figure 9</ref> we show the percentage of node references in the shared oct-tree that were local as the average over all processes.</p><p>From this data, we can see that the baseline UPC implementation scales poorly because the number of nonlocal references is increasing proportional to the number of UPC threads. In comparison, the hybrid schemes offer better performance because the tree has been replicated on each UPC group, giving a substantial increase in the percentage of local data references over the baseline implementation. The hybrid scheme with group size of 4 achieves almost linear scaling because this schemes fixes the group size at the size of an SMP node and all data is local. When two nodes are combined to form a group of size 8 and provide access to twice as much memory, a twofold performance increase over the baseline is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Hybrid parallel programming is the practice of combining multiple parallel programming models within a single application. Hybrid programming with MPI as an outer level of parallelism and OpenMP or threads as an inner, nested level of parallelism is becoming increasingly popular and has been extensively explored <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b16">18]</ref>. In this context, hybridization has been shown to help provide more efficient use of memory and improve the performance of many applications by leveraging shared memory to localize communication and improving load balance through malleable parallelism <ref type="bibr" target="#b7">[9]</ref>.</p><p>The Global Arrays (GA) toolkit for distributed, shared multidimensional arrays <ref type="bibr" target="#b12">[14]</ref> is a successful PGAS model that is compatible with MPI for hybrid programming and provides a flat hybrid model. However, GA focuses specifically on shared array data and does not provide support for arbitrary shared, linked data structures. The Aggregate Remote Memory Copy Interface (ARMCI) <ref type="bibr" target="#b11">[13]</ref> defines a low-level, one-sided communication library that provides the PGAS substrate for GA. ARMCI is fully interoperable with MPI and forms a hybrid MPI+PGAS programming model. However, ARMCI is very low level and does not provide the same convenience, programmability, and completeness (e.g., shared pointers, collectives) as UPC.</p><p>Some work has been done by the Berkeley UPC group to add support for MPI compatibility to UPC <ref type="bibr" target="#b2">[4]</ref>. This mode of hybridization is targeted primarily at supporting the flat hybrid model for interoperability with existing MPI parallel libraries. To our knowledge, however, this paper presents the first exploration of a nested MPI+UPC hybrid programming model. The idea of using groups to improve the locality of PGAS applications has been explored in the context of Global Arrays groups and mirrored arrays <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b14">16]</ref>. In addition, processor teams have been proposed as an extension to the UPC standard. However, these teams would not apply to static shared multidimensional arrays and would require such arrays to be dynamically allocated, complicating indexing. Hybrid MPI+UPC groups offer an alternative approach to dynamic processor teams by replicating static shared structures across groups.</p><p>Chapel <ref type="bibr" target="#b5">[7]</ref>, X10 <ref type="bibr" target="#b6">[8]</ref>, and Fortress <ref type="bibr" target="#b17">[19]</ref> are new, high-productivity parallel programming languages with features that offer a union of benefits from MPI's explicit control over data locality and UPC's convenient global shared view of data. For existing programs, there is a significant barrier to adoption of these new models because of the cost of reimplementation. Hybrid MPI+UPC offers an alternative incremental pathway adopting these features in existing programs. In addition it will offer a testbed for evaluation of the benefits and trade-offs of partitioned-view versus global-view parallel models with respect to performance and productivity through incremental modification of existing applications. Thus, it can serve as a testing ground for developing insights that will better facilitate the development and implementation of new languages offering features for locality control as well as shared-global views.</p><p>The MPI Forum is working toward the next MPI standard, MPI-3, and is actively investigating enhancements to improve the flexibility and usability of MPI's one-sided communication model <ref type="bibr" target="#b18">[20]</ref> as well as improving interoperability for hybrid programming models, including UPC. Hybrid MPI+UPC can offer a different and more convenient model than MPI with one-sided communication because it is able to leverage UPC's high-level programming interface and tunable performance model in comparison with MPI's library approach that requires explicit communication management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUDING REMARKS</head><p>In this paper, we have explored the hybrid programming model formed by combining MPI and UPC. This model offers an incremental pathway that allows existing applications to take advantage of MPI's locality control and UPC's global address space. In addition, it can serve as a testbed for developing new programming models that aim to combine these features. For memoryconstrained MPI codes, the hybrid model enables the processing of larger problems by aggregating the memory of several nodes into a single, shared global address space. For locality-constrained UPC codes, the hybrid model can improve locality through the creation of UPC groups that are connected with MPI.</p><p>We evaluated this new model on two benchmarks, a random access benchmark and the Barnes-Hut n-body simulation. Compared against a baseline execution on 256 cores, we found that, for groups that span two cluster nodes, the hybrid random access benchmark yields a 25% improvement in execution time and hybrid BarnesHut experiences a twofold speedup. In the case of Barnes-Hut the cost of hybridization was a 2% increase in code size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : MPI+UPC hybrid execution models; gray circles rep- resent hybrid MPI+UPC processes, and white circles represent UPC-only processes.</head><label>1</label><figDesc>Figure 1: MPI+UPC hybrid execution models; gray circles represent hybrid MPI+UPC processes, and white circles represent UPC-only processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hybrid dot product example in the flat model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nested-funneled dot product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Nested-multiple dot product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : Random-access benchmark snapshot on 4 processors. In (a) a single copy of the array is distributed across all 4 pro- cessors; in (b) two groups of size two each have a full replica of the array. Elements with affinity to processor 0 are shaded; in (b) twice</head><label>5</label><figDesc>Figure 5: Random-access benchmark snapshot on 4 processors. In (a) a single copy of the array is distributed across all 4 processors; in (b) two groups of size two each have a full replica of the array. Elements with affinity to processor 0 are shaded; in (b) twice as many elements are local to processor 0 than in (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Hybrid nested-funneled Barnes-Hut algorithm. for i = 1 to tmax do our_bodies ← ∅ T ′ ← T T ← Octtree_Create() my_bodies ← partition(T ′ , bodies, group_id, thread_id) for all b ∈ my_bodies do T.insert(b) end for summarize_subtrees(T ) for all b ∈ my_bodies do compute_f orce(b, T ) end for for all b ∈ my_bodies do advance(b) end for our_bodies = our_bodies ∪ my_bodies upc_barrier if M Y T HREAD = 0 then M P I_Allgather(our_bodies, bodies) end if end for parallel programming model. Finally, the masters of each group perform an MPI all gather to gather the updated list of bodies on each UPC group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 : Random-access benchmark execution time for base- line UPC and hybrid implementations.Figure 7 : Random-access benchmark percentage of local refer- ences vs number of processors.</head><label>67</label><figDesc>Figure 6: Random-access benchmark execution time for baseline UPC and hybrid implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : Barnes-Hut force calculation performance for base- line UPC and hybrid implementations.Figure 9 : Barnes-Hut force calculation locality, shown as the percentage of local data accesses.</head><label>89</label><figDesc>Figure 8: Barnes-Hut force calculation performance for baseline UPC and hybrid implementations.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Employing nested OpenMP for the parallelization of multi-zone computational fluid dynamics applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Ayguade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martorell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Jost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="686" to="697" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical o(n log n) force calculation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piet</forename><surname>Hut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="446" to="449" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Berkeley UPC user&apos;s guide version 2.8.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upc</forename><surname>Berkeley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ScaLAPACK user&apos;s guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>D&amp;apos;azeuedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Problems with using MPI 1.1 and 2.0 as compilation targets for parallel language implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bonachea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Duell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Hardware/Software Support for High Performance Scientific and Engineering Computing (SHPSEC)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel programmability and the Chapel language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Zima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. High Performance Computing Applications (IJHPCA)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="312" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">X10: An object-oriented approach to non-uniform cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Grothoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Donawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Kielstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Ebcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Von Praun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA)</title>
		<imprint>
			<publisher>ACM SIGPLAN</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="519" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic load balancing of MPI+OpenMP applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julita</forename><surname>Corbalán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Labarta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Parallel Processing (ICPP)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance characteristics of the multi-zone NAS parallel benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><forename type="middle">F</forename><surname>Van Der Wijngaart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Intl. Parallel and Distributed Processing Symp. (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MPI: A message-passing interface standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mpi</forename><surname>Forum</surname></persName>
		</author>
		<idno>UT-CS-94-230</idno>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Knoxville</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MPI-2: Extensions to the message-passing interface</title>
	</analytic>
	<monogr>
		<title level="m">MPI Forum</title>
		<meeting><address><addrLine>Knoxville</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ARMCI: A portable remote memory copy library for distributed array libraries and compiler run-time systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">1586</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global Arrays: A portable &quot;shared-memory&quot; programming model for distributed memory computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Littlefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing (SC) &apos;94</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Advances, applications and performance of the Global Arrays shared memory programming toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Tipparaju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manojkumar</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Trease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Aprà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perform. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="231" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shared memory mirroring for reducing communication overhead on commodity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarek</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Apra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Cluster Computing</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantum Monte Carlo calculations of light nuclei</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pieper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Nuclear Physics Conference</title>
		<meeting>the 22nd International Nuclear Physics Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">751</biblScope>
			<biblScope unit="page" from="516" to="532" />
		</imprint>
	</monogr>
	<note>Part 1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Development of mixed mode MPI / OpenMP applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorna</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Programming</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel programming and parallel abstractions in fortress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">L</forename><surname>Steele</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Intl. Conf. on Parallel Architecture and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">157</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating high performance RMA interfaces for the MPI-3 standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Tipparaju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ritzdorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 38th Intl. Conf. on Parallel Processing</title>
		<meeting>38th Intl. Conf. on Parallel essing</meeting>
		<imprint>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">UPC language specifications, v1.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upc</forename><surname>Consortium</surname></persName>
		</author>
		<idno>LBNL-59208</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Berkeley National Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
