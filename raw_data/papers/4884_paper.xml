<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Made Easier by Linear Transformations in Perceptrons</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalto University</orgName>
								<orgName type="institution" key="instit2">Aalto University New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalto University</orgName>
								<orgName type="institution" key="instit2">Aalto University New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Aalto University</orgName>
								<orgName type="institution" key="instit2">Aalto University New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning Made Easier by Linear Transformations in Perceptrons</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average , and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlin-ear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning deep neural networks has become a popular topic since the invention of unsupervised pretraining <ref type="bibr" target="#b4">[5]</ref>. Some later works have returned to traditional back-propagation learning and noticed that it can also provide impressive results given either a so- phisticated learning algorithm <ref type="bibr" target="#b11">[11]</ref> or simply enough computational power <ref type="bibr" target="#b2">[3]</ref>. In this work we study backpropagation learning in deep networks with up to five hidden layers.</p><p>In learning multi-layer perceptron (MLP) networks by back-propagation, there are known transformations that speed up learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. For instance, inputs are recommended to be centered to zero mean (or even whitened), and nonlinear functions are proposed to have a range from -1 to 1 rather than 0 to 1 <ref type="bibr" target="#b9">[10]</ref>. Schraudolph <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref> proposed centering all factors in the gradient to have zero mean. This lead to a significant speed-up in learning when using shortcut connections. In this paper, we transform the nonlinearities in the hidden neurons. The effect is very similar to gradient factor centering, but transforming the model instead of the gradient makes it easier to generalize to other contexts such as variational Bayes. We explain the usefulness of these transformations by studying the Fisher information matrix.</p><p>It is well known that second-order optimization methods such as the natural gradient <ref type="bibr" target="#b0">[1]</ref> or Newton's method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with high-dimensional models due to heavy computations with large matrices. In practice, it is possible to use a diagonal or block-diagonal approximation <ref type="bibr" target="#b7">[8]</ref> of the Fisher information matrix. If one has to approximate most of the matrix with zeros anyway, we should use transformations that move these elements as close to zero as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Transformations</head><p>Let us study an MLP-network with a single hidden layer <ref type="bibr" target="#b0">1</ref> and a shortcut mapping, that is, the output column vectors y t for each sample t are modelled as a function of the input column vectors x t with</p><formula xml:id="formula_0">y t = Af (Bx t ) + Cx t + t ,<label>(1)</label></formula><p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and t is the noise which is assumed to be zero mean and Gaussian, that is, p( it ) = N it ; 0, σ 2 i . In order to avoid separate bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p><p>Let us supplement the tanh nonlinearity with auxiliary scalar variables α i and β i for each nonlinearity f i . They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p><formula xml:id="formula_1">f i (b i x t ) = tanh(b i x t ) + α i b i x t + β i ,<label>(2)</label></formula><p>where b i is the ith row vector of matrix B. An example f i can be seen in <ref type="figure">Figure 1</ref>. We will ensure that</p><formula xml:id="formula_2">T t=1 f i (b i x t ) = 0 (3) T t=1 f i (b i x t ) = 0<label>(4)</label></formula><p>by setting α i and β i to</p><formula xml:id="formula_3">α i = − 1 T T t=1 tanh (b i x t )<label>(5)</label></formula><formula xml:id="formula_4">β i = − 1 T T t=1 [tanh(b i x t ) + α i b i x t ]<label>(6)</label></formula><p>as shown in the appendix.</p><p>The effect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p><formula xml:id="formula_5">C new = C old − A(α new − α old )B − A(β new − β old ) [0 0 . . . 1] , (7)</formula><p>where α is a matrix with elements α i on the diagonal and one empty row below for the bias term, and β is a column vector with components β i and one zero below for the bias term.</p><p>We also emphasize making the inputs x k zero mean (and similar in scale) as a preprocessing step (see e.g. <ref type="bibr" target="#b9">[10]</ref>).</p><p>Schraudolph <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref> proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar effect on the gradient. Equation (3) corresponds to Schraudolph's activity centering and Equation <ref type="formula" target="#formula_2">(4)</ref> corresponds to slope centering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Intuitive Justification</head><p>Second-order optimization methods such as the natural gradient <ref type="bibr" target="#b0">[1]</ref> or Newton's method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p><p>The Fisher information matrix contains elements</p><formula xml:id="formula_6">G ij = t ∂ 2 log p(y t | x t , A, B, C) ∂θ i ∂θ j ,<label>(8)</label></formula><p>where ·· is the expectation over the Gaussian distribution of noise t in Equation <ref type="formula" target="#formula_0">(1)</ref>, and vector θ contains all the elements of matrices A, B, and C. Note that y t is a random variable and thus the Fisher information does not depend on the output data.</p><p>These elements are:</p><formula xml:id="formula_7">∂ ∂a ij ∂ ∂a i j log p = 0 i = i − 1 σ 2 i t f j (b j x t )f j (b j x t ) i = i,<label>(9)</label></formula><p>where a ij is the ijth element of matrix A, f j is the jth nonlinearity, and b j is the jth row vector of matrix B. Similarly</p><formula xml:id="formula_8">∂ ∂b jk ∂ ∂b j k log p = − i 1 σ 2 i a ij a ij t f j (b j x t )f j (b j x t )x kt x k t (10)</formula><p>and</p><formula xml:id="formula_9">∂ ∂c ik ∂ ∂c i k log p = 0 i = i − 1 σ 2 i t x kt x k t i = i. (11)</formula><p>The cross terms are Figure 1: As a positive side effect, the nonlinearity in Equation <ref type="formula" target="#formula_1">(2)</ref> does not saturate at all for example with a typical α = −0.5 and β = 0.</p><formula xml:id="formula_10">∂ ∂a ij ∂ ∂b j k log p = − 1 σ 2 i a ij t f j (b j x t )f j (b j x t )x kt (12) ∂ ∂c ik ∂ ∂a i j log p = 0 i = i − 1 σ 2 i t f j (b j x t )x kt i = i (13) ∂ ∂c ik ∂ ∂b jk log p = − 1 σ 2 i a ij t f j (b j x t )x kt x k t .<label>(14)</label></formula><p>Now we can notice that Equations (9-14) contain factors such as f i (·), f i (·), and x it . We argue that by making the factors as close to zero as possible, we help in making nondiagonal elements of the Fisher information closer to zero. For instance,</p><formula xml:id="formula_11">E[f i (·)f j (·)] = E[f i (·)]E[f j (·)]+Cov[f i (·), f j (·)]</formula><p>, so assuming that the hidden units i and j are representing different things, that is, f i (·) and f j (·) are uncorrelated, the nondiagonal element of the Fisher information in Equation (9) becomes exactly zero by using the transformation. When the units are not completely uncorrelated, the element in question will be only approximately zero. The same argument applies to all other elements in Equations <ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b11">(11)</ref><ref type="bibr" target="#b12">(12)</ref><ref type="bibr" target="#b13">(13)</ref><ref type="bibr" target="#b14">(14)</ref>, some of them also highlighting the benefit of making the input data x t zero-mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Positive Side Effect</head><p>Having a non-zero α i has a positive side effect of reducing plateaus in learning. Typical nonlinearities like the tanh function saturate exponentially on positive and negative sides. When the derivative of an activation f i (·) is about zero for most of the data samples, the gradient propagated through it also becomes almost zero, and learning can proceed very slowly or even seem to stop completely. This may explain plateaus in typical learning curves, where the learning proceeds slowly at times. To alleviate the problem, Glorot and Bengio <ref type="bibr" target="#b3">[4]</ref> suggested to use the soft-sign nonlinearity that saturates more slowly, but having a non-zero α i provides a nonlinearity that does not saturate at all. The difference is illustrated in <ref type="figure">Figure 1</ref>. In practice, α i tends to vary from −0.8 to −0.5 as will be seen in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Practical Issues</head><p>There are many practical issues when learning MLP networks and they are addressed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning</head><p>Back-propagation learning is basically a gradient ascent algorithm to maximize the log likelihood log p({y t } T t=1 | {x t } T t=1 , θ) of the parameter vector θ, where the actual back-propagation corresponds to using the chain rule of derivatives and dynamic programming to compute the gradient efficiently.</p><p>The gradient is</p><formula xml:id="formula_12">g i = ∂ log p(y t | x t , θ) ∂θ i ,<label>(15)</label></formula><p>where ·· is the expectation over the data set. The update is</p><formula xml:id="formula_13">θ i ← θ i + γg i ,<label>(16)</label></formula><p>where γ i is a learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Learning</head><p>It is well known (see e.g. <ref type="bibr" target="#b1">[2]</ref>) that looking at all the available data before each update is wasteful, because many samples possess redundant information. We will use a mini-batch learning algorithm, where each update is done based on the 1000 next samples from the randomly shuffled data set. To reduce the effect of noise due to such a small sample, a momentum term is used. The update direction is thus set to</p><formula xml:id="formula_14">g i ← 0.1 ∂ log p(y t | x t , θ) ∂θ i + 0.9g i (17)</formula><p>for all learned parameters, including the shortcut mappings.</p><p>The transformations parameters α i and β i , however are updated after the initialization and after every 1000 iterations thereafter, using the whole data set in Equations (5-6). At the same time, the changes are compensated by updating the shortcut mappings according to Equation <ref type="formula">(7)</ref> and the momentum g i for the gradient updates is reset to zero. Using the transformations only rarely lowers the computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete Outputs</head><p>In classification problems, the output y t is discrete and one can use the soft-max model. The Equation <ref type="formula" target="#formula_0">(1)</ref> is replaced by</p><formula xml:id="formula_15">P (y t = i | x t , θ) = exp [A i f (Bx t ) + C i x t ] j exp [A j f (Bx t ) + C j x t ] ,<label>(18)</label></formula><p>where A i is the ith row of matrix A. Back-propagation is done as before. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple Hidden Layers</head><p>The extension of the proposed approach to multiple hidden layers is straightforward. Equations (2-6) apply to all nonlinear units with b i x t replaced by the appropriate input signal. The shortcut mappings need to be included for skipping any number of layers (see <ref type="figure" target="#fig_2">Figure 2</ref> for an example). The number of weights of course increases quadratically with the number of layers, but this can be avoided by including a layer without transformations as shown in <ref type="figure" target="#fig_5">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>We use the initialization proposed by Glorot and Bengio <ref type="bibr" target="#b3">[4]</ref> for weights between consecutive layers, that is, the weight is drawn from a uniform distribution between ± √ 6/ √ n j + n j+1 where n j is the number of neurons on the jth layer. This normalized initialization is based on the objectives of maintaining activation variances and back-propagated gradient's variance throughout the layers. With unnormalized initialization, the gradient tends to vanish or explode exponentially with the number of layers <ref type="bibr" target="#b3">[4]</ref>. Biases are drawn from a uniform distribution between ±0.5. Weights for all shortcut connections are initialized to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Rate</head><p>We use a hand-set learning rate γ i for each problem. The base learning rate γ is halved for connection weights once for each layer that the connection skips. This is a heuristic to take into account that shortcut connections have a more direct influence to the final output of the network. Also we linearly decrease the learning rate to zero after half of the allocated computational time has been used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization</head><p>Overfitting is a crucial problem in a large network, and regularization is essential to make it perform well with new data. We use three different regularization methods. Firstly, the dimensionality of input data was decreased as a preprocessing step. We used principal component analysis (PCA) followed by a random rotation (see <ref type="figure" target="#fig_3">Figure 3</ref>). The motivation for the random rotation was to make each input approximately equally important. Secondly, we use weight decay (see e.g. <ref type="bibr" target="#b6">[7]</ref>), or equivalently a Gaussian prior on the parameters θ.</p><p>The final update direction becomes</p><formula xml:id="formula_16">g i ← 0.1 ∂ log p(y t | x t , θ) ∂θ i − λθ i + 0.9g i ,<label>(19)</label></formula><p>where λ is the weight decay parameter set by hand. Thirdly, we add randomly generated Gaussian noise to the original input data each time they are presented to the learner, inspired by denoising autoencoders <ref type="bibr" target="#b15">[15]</ref> and also recently used in classification <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare MLP learning with and without proposed transformations in three problems where we can also compare to other state-of-the-art learning algorithms. The two first experiments are image classification tasks, and the last one is an autoregressive model to find a low-dimensional representation of images. Even though all experiments use image data, we do not use or compare to any image-specific processing such as convolutional networks or elastic distortions. Our approach would work exactly the same even if the order of pixels was randomly permutated. All experiments were run on a desktop computer with Intel Core i7 2.93GHz and 8 gigabytes of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MNIST Handwritten Digit Classification</head><p>The MNIST data set <ref type="bibr" target="#b8">[9]</ref> consists of 28 by 28 pixel gray-scale images of handwritten digits with examples depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. There are 60000 training samples and 10000 test samples. The mean activation of each pixel was subtracted from the data, and the dimensionality was dropped from 28 × 28 = 784 to 200 using PCA followed by a random rotation (see <ref type="figure" target="#fig_3">Figure  3)</ref>. A classification network with layer sizes 200-200-200-10 was learned with a normal MLP model (original), one with shortcut mappings included (shortcuts), and the proposed model with shortcut mappings and transformations in the nonlinearities (transformations) (see <ref type="figure" target="#fig_2">Figure 2)</ref>. We also ran a simple 200-10 network (linear) for comparison. Training and test errors were tracked during learning and their computation was not  included in learning time. Learning time was restricted to 15 minutes, weight decay parameter was λ = 0.0001 and the regularization noise was used with standard deviation 0.4 for each input (see <ref type="figure" target="#fig_3">Figure 3)</ref>.</p><p>The results are shown in <ref type="table">Table 1</ref> and <ref type="figure">Figure 4</ref>. With a proper learning rate (1.0), the proposed method gets the test error below 1.5% in six minutes and to 1.1% in fifteen minutes. Thus, the transformations make a simple gradient descent algorithm competitive in speed with complex state-of-the-art learning methods such as TONGA <ref type="bibr" target="#b7">[8]</ref>, which reaches test error 1.7% in around 30 minutes. Deep networks learned with backpropagation have reached 1.64% error in <ref type="bibr" target="#b3">[4]</ref>. Deep belief network <ref type="bibr" target="#b4">[5]</ref>  One can note that the errors drop fast in the latter half of learning when the learning rate is decreased (See middle of <ref type="figure">Figure 4</ref>). This is mostly due to filtering out the noise caused by the stochastic gradient. It might seem that the comparison methods are even catching up at the end. However, when the experiment is repeated with a longer learning time (not shown here), the curves look qualitatively similar with comparison methods almost catching up at the end. Let us also study some of the properties of the transformations. <ref type="figure">Figure 5</ref> shows what the transformed nonlinearities look like in practice. How do they affect the Fisher information matrix? Equation <ref type="formula" target="#formula_7">(9)</ref> measures the covariance of the signals f i (·). The ratio of mean square nondiagonal element of the covariance to mean square diagonal element drops from 0.051 to 0.007 in the first hidden layer and from 0.080 to 0.009 in the second hidden layer, when comparing models learned traditionally or with transformation. The transformations also decrease the norm of the gradient with respect to weights of adjacent layers. The decrease is about 2 to 3-fold in the initial phase of learning. This might also explain why the proposed model performed worse than the others with a too small learning rate (See left part of <ref type="figure">Figure 4</ref>). With a small norm of the gradient and a small learning rate, the optimization simply does not finish in the allocated time.</p><p>To study which regularization method was important, the runs were repeated with several variants. The table below shows test errors evaluated by including regularization methods one by one, using the learning rate γ = 1.0. The final run was repeated with ten times the learning time and γ = 0.5. regularization original shortcuts transform. none 1.02 Adding noise to the inputs turned out to be the most important regularization method, followed by dimensionality reduction by PCA. Using the transformations improved all variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CIFAR-10 Classification</head><p>The CIFAR-10 data set <ref type="bibr" target="#b5">[6]</ref> consists of 32 by 32 pixel color images classified to 10 different classes, with examples depicted in <ref type="figure" target="#fig_3">Figure 3</ref>. There are 50000 training samples and 10000 test samples. Each channel of each pixel was normalized to zero mean unit variance, and the dimensionality was dropped from 32×32×3 = 3072 to 500 using PCA followed by a random rotation. A classification network with layer sizes 500-500-500-10 was used with the same structure and variants as in MNIST classification. Learning time was restricted to 10000 seconds, the base learning rate was set by hand to γ = 0.3, weight decay parameter was λ = 0.001 and the regularization noise was used with standard deviation 0.4 for each input (see <ref type="figure" target="#fig_3">Figure 3)</ref>.</p><p>The results are shown in <ref type="table">Table 1</ref> and <ref type="figure">Figure 4</ref>. Earlier test errors with MLP networks include 48.47% in <ref type="bibr" target="#b5">[6]</ref> and 52.92% in <ref type="bibr" target="#b3">[4]</ref>. The test error of 44.42% obtained here with the original MLP is already much better and it is further improved to 43.70% by using the proposed transformations. It should be noted that even the best back-propagation results are far behind from results obtained with for instance unsupervised pretraining or convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role of Regularization</head><p>The role of regularization was studied by rerunning the CIFAR-10 classification experiment without any regularization. The network size was thus 3072-500-500-10, and we dropped the learning rate to γ = 0.03 to better compare to the initialization. All three methods reached the training error 0.0%, but the test errors increased to 50.7%, 49.1%, and 46.8%. This overfitting was expected since the number of weights in the network is much larger than the number of labels in the training set, which makes the system underdetermined.</p><p>To further study the found solutions, we compute the angles between the 3072-dimensional incoming weight vectors of the neurons in the first hidden layer against the vectors that they were initialized to. The median angle over the 500 units was 39.6 • , 29.8 • , and 22.0 • in the three models. Firstly, the angles are surprisingly small taking into account that high-dimensional vectors easily become rather orthogonal, which indicates that the found solution still retains much of the randomness of the initialization. 2 Secondly, shortcut weights seem to help against overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MNIST Autoencoder</head><p>The third problem uses the same MNIST handwritten digit data set <ref type="bibr" target="#b8">[9]</ref>, but in this case both the inputs and outputs of the network are the images. The network topology is 784-500-250-30-250-500-784 with tanhnonlinearity at each layer except the bottleneck layer in the middle. The output is scaled from -1 to 1 to match the tanh nonlinearity. The goal in this problem is to find a good 30-dimensional representation from which the image can be reconstructed. Naturally the shortcut connections that skip the bottleneck are not used (see left part of <ref type="figure" target="#fig_5">Figure 6</ref>). The labels are not used in this experiment at all. Learning time was restricted to 60000 seconds, the base learning rate was γ = 0.05, weight decay parameter was λ = 0.001 and the regularization noise was used with standard deviation 0.1 for each input. To avoid early divergence in learning, the learning rate was increased from one hundredth to the full rate exponentially during the first one percent of learning time.</p><p>The performance is measured by the average sum of squared reconstruction errors on the test data when it is scaled back to the range from 0 to 1. The final reconstruction error is 2.44 and some reconstructions are visualized in the middle of <ref type="figure" target="#fig_5">Figure 6</ref>. As a comparison, a linear 784-30-784 autoencoder gives an error 7.85. State-of-the-art comparison results are presented by Martens <ref type="bibr" target="#b11">[11]</ref>: Hessian-free optimization gives 2.55 with a larger network. The results in <ref type="bibr" target="#b11">[11]</ref> were further improved to 2.28 by initializing the network with layerwise pretraining, which could be used here, too.</p><p>Typical applications of dimensionality reduction methods include visualization, data denoising, or missing value imputation, but we will show another simple demonstration by comparing them as preprocessing methods for the k nearest neighbor (kNN) classifier. By using the raw pixel data, kNN gives 2.95% test error. Using a linear network to reduce dimensionality to 30 as a preprocessing gives 2.39% error, while using the proposed model gives 1.95% error. It can be concluded that the bottleneck layer has found a representation that also better separates the clusters formed by the different classes, despite the fact that the labels were not used in learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We proposed transformations to nonlinearities that make learning MLP networks much easier. The motivation is to make the nonlinear mapping as separate as possible from the linear mapping which is modelled using shortcut weights. A basic stochastic gradient optimization became faster than state-of-the-art learning algorithms. Those algorithms could also be tested with transformations for further improvements. The theory of the speed-up is based on making the standard gradient more similar to the natural gradient by having the nondiagonal terms of the Fisher information matrix closer to zero. As a side effect, the transformed nonlinearities might also help in avoiding plateaus <ref type="bibr" target="#b9">[10]</ref>. The experiments showed that these simple transformations helps learning deep structures at least up to 5 hidden layers using good-old back-propagation learning.</p><p>The transformations also seemed to help generalization when no regularization was used. We think that this is because the transformations help to separate the simpler and more complex parts of each mapping. Let us think of a network with just one hidden layer. The linear part of the problem (the shortcut connections C) do not suffer much from the overfitting and can be learned more reliably even without regularization. Overfitting the more complex parts A and B might not hurt the performance as much when they do not influence the linear part of the whole mapping. It might be possible to test this hypothesis in the future.</p><p>Another theoretical study that could be done in the future, is to measure the angle between the traditional gradient and the natural gradient with and without transformations. It would require a small network such that computing the natural gradient would be feasible.</p><p>The effect of the transformations could also be studied in other contexts. For variational Bayesian (VB) learning, the proposed transformations make both the signals in the network and the network weights less dependent a posteriori. Often they are assumed to be independent in the posterior approximation of VB anyway. Thus, the transformation makes the effect of the assumption smaller and the approximation more accurate. This should help in avoiding the problem of underfitting (or output weights of too many hidden neurons going to zero). One could also use MCMC methods for sampling network weights. The effectiveness of the sampling process depends heavily on how large jumps can be made in the parameter space. We can make longer jumps for the matrices A and B if we use the proposed transformations to ensure that even large changes in them do not affect the linear part of the input-output mapping. These new contexts would highlight the difference between transforming nonlinearities compared to transforming gradient factors <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref>.</p><p>The term deep learning refers either to networks with many layers (as in this work) but sometimes it is used for unsupervised pretraining which allows for wellperforming deeper networks in practice. The proposed transformations could also be applied to initializations based on unsupervised pretraining. We could also study a multi-task learning problem combining classification and auto-encoding. This simple alternative to layerwise pretraining might provide some useful insights about combining unsupervised and supervised learning.</p><p>A "poor man's variant" of the proposed framework would be to use shortcut connections and fixed nonlinearities such as f (x) = tanh(x) − 0.5x. Dispite its simplicity, this variant might still provide most of the discussed benefits in practice.</p><p>Another future direction is to introduce a third transformation. While currently we aim at making the Fisher information matrix diagonal, we could also make it closer to the unit matrix. This could be done by using a multiplicative transformation in order to normalize the scale of the output and the slope of each nonlinearity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Appearing in Proceedings of the 15 th International Con- ference on Artificial Intelligence and Statistics (AISTATS) 2012, La Palma, Canary Islands. Volume XX of JMLR: W&amp;CP XX. Copyright 2012 by the authors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Traditional structure for a feed-forward multi-layer perceptron network with full connections. Bottom: Network with shortcut connections included, also used in the proposed method with transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Top row shows the PCA filters corresponding to the largest eigenvalues (1-5), second row to the smallest eigenvalues (196-200). The bottom rows show 10 filters after the random rotation in the principal subspace. Middle: Top row shows 5 examples from the MNIST handwritten digit data set. The second row shows reconstructions from the 200 component principal subspace. The third and forth row show reconstructions when including the added noise to the training data. Two instantiations of the noise is shown to remind that the noise is resampled for each epoch. Right: The corresponding images (16 examples) for the CIFAR-10 data set with 500 components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Left: MNIST problem. Classification error rate in percentage as a function of learning rate after 15 minutes of learning. Lower curves in each figure are the training error rates and higher curves are test error rates. The vertical dashed line shows the point at which the learning rate starts to be decreased. Middle: MNIST problem. Error rates against learning time for the best learning rates for each method. Right: CIFAR-10 problem. Error rates against learning time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: MNIST autoencoder. Left: Traditional autoencoder network above, shortcut connections included below. Note that the bottleneck layer does not have a nonlinearity or transformations. Middle: Each triplet shows an example digit from test data, its reconstruction with a deep autoencoder, and a reconstruction with a linear autoencoder as a comparison. Right: Error rate in average sum of squared reconstruction errors plotted against learning time in seconds. Higher curves are test errors, lower curves are training errors. The vertical dashed line shows the point at which the learning rate starts to be decreased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Two instantiations of the noise is shown to remind that the noise is resampled for each epoch. Right: The corresponding images (16 examples) for the CIFAR-10 data set with 500 components.</figDesc><table>linear original shortcuts transformations literature 

MNIST 
classification 

training error 
8.99 
0.063 
0.058 
0.068 
-
test error 
8.58 
1.15 
1.22 
1.10 
1.64 
learning rate 
-
1.0 
0.5 
1.0 
-
# of iterations 
30k 
4717 
3498 
2674 
-

CIFAR-10 
classification 

training error 
58.07 
23.21 
22.46 
4.56 
-
test error 
59.09 
44.42 
44.99 
43.70 
48.47 
# of iterations 
32k 
12k 
8k 
8k 
-

MNIST 
autoencoder 

training error 
8.11 
2.37 
2.11 
1.94 
1.75 
test error 
7.85 
2.76 
2.61 
2.44 
2.55 
# of iterations 
92k 
49k 
38k 
37k 
-

Table 1: Results of the proposed method (transformations) compared against other methods run with the same 
settings and against results in the literature [4, 6, 11]. The number of iterations in the allocated time is reported 
to compare the computational complexities. 

</table></figure>

			<note place="foot" n="1"> The assumption is done for notational simplicity only, the method is applied in the general deep case.</note>

			<note place="foot" n="2"> This also partly explains why initializing with unsupervised pretraining works so well in large networks.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derivations</head><p>Derivation of Equations (5-6):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">First-and second-order methods for learning: Between steepest descent and Newton&apos;s method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="166" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1003.0358</idno>
		<title level="m">Deep big simple neural nets excel on handwritten digit recognition. CoRR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 4 (NIPS 1991)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="950" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topmoumoute online natural gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS*2007)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: tricks of the trade</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adding noise to the input of a model trained with a regularized objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno>1359</idno>
		<imprint>
			<date type="published" when="2011-04" />
			<pubPlace>Montréal (QC), H3C 3J7, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université de Montréal</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Accelerated gradient descent by factor-centering decomposition. Technical Report IDSIA-33-98, Istituto Dalle Molle di Studi sull&apos;Intelligenza Artificiale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Centering neural network gradient factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Genevieve Orr and Klaus-Robert Mller</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="548" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML08)</title>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
