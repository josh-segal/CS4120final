<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
						</author>
						<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Mach Learn</title>
						<imprint>
							<biblScope unit="volume">81</biblScope>
							<biblScope unit="page" from="53" to="67"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s10994-010-5205-8</idno>
					<note type="submission">Received: 30 April 2010 / Accepted: 20 June 2010 / Published online: 22 July 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Entity relation graph · Random walk · Learning to rank · Relational model · Filtering and recommending</keywords>
			</textClass>
			<abstract>
				<p>Scientific literature with rich metadata can be represented as a labeled directed graph. This graph representation enables a number of scientific tasks such as ad hoc retrieval or named entity recognition (NER) to be formulated as typed proximity queries in the graph. One popular proximity measure is called Random Walk with Restart (RWR), and much work has been done on the supervised learning of RWR measures by associating each edge label with a parameter. In this paper, we describe a novel learnable proximity measure which instead uses one weight per edge label sequence: proximity is defined by a weighted combination of simple &quot;path experts&quot;, each corresponding to following a particular sequence of labeled edges. Experiments on eight tasks in two subdomains of biology show that the new learning method significantly outperforms the RWR model (both trained and untrained). We also extend the method to support two additional types of experts to model intrinsic properties of entities: query-independent experts, which generalize the PageRank measure, and popular entity experts which allow rankings to be adjusted for particular entities that are especially important.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most past research on accessing the scientific literature has focused on a small-number of well-defined tasks which represent the scientific literature as a set of documents: such tasks include ad hoc retrieval based on keyword queries, or named entity recognition (NER) and normalization. In fact, scientific literature naturally includes substantial metadata such as author names, citations, and publication venues, as well as derived metadata (such as gene and protein names, in the biomedical literature). An alternative way to represent the scientific literature is as a labeled directed graph, with typed nodes representing documents, terms, and metadata, and labeled edges representing the relationships between them (e.g., "authorOf", "datePublished", etc.). This graph represents not only text, but also implicitly includes social-network information (via co-authorship, and paths between authors and conference venues), expertise information (via paths between an author and entities mentioned in her publications). Domain knowledge can also be easily added to the graph (e.g., adding known relationships between entities, such as protein-protein interaction information).</p><p>Representing the scientific literature as a labeled graph enables a number of scientific tasks to be formulated as typed proximity queries in the graph, in which the user provides as input a set of query nodes and answer type, and receives as output a list of nodes of the desired answer type, ordered by proximity to the query nodes. For instance, traditional keyword-based ranked retrieval of documents can be formulated as a proximity query where the query nodes are term nodes, and the answer type is "document"; also, in past research, future collaborations between scientists have been predicted by proximity queries on a coauthorship graph <ref type="bibr" target="#b8">(Liben-Nowell and Kleinberg 2007)</ref>, document-level gene annotations have been generated using proximity queries on a graph of based on NER-annotated documents and known entity synonyms, and publications involving new gene-protein entities have been predicted using proximity queries on co-authorship graph including documentlevel metadata on entities <ref type="bibr" target="#b1">(Arnold and Cohen 2009)</ref>. In general, the appropriate notion of "proximity" may be task-or user-specific, and hence must be learned or engineered; however, there are also general-purpose graph proximity measures such as random walk with restart (RWR) (also called personalized PageRank) which are fairly successful for many types of tasks.</p><p>In this paper we will consider the use of typed proximity queries to solve four tasks: a "gene recommendation" task considered by <ref type="bibr" target="#b1">Arnold and Cohen (2009)</ref>, and three additional tasks we call venue recommendation, reference recommendation, and expert-finding. As we will argue below, all of these tasks are plausible surrogates for tasks commonly performed by scientists, and data for them is readily obtainable, making them suitable for learning and evaluating task-specific proximity measures for the scientific literature. We evaluate these four tasks, in two subdomains of biology each, and evaluate performance on 2000 test queries for each of these eight tasks.</p><p>The principle contribution of this paper is the development of a new method for learning proximity measures on labeled graphs. In particular, we describe a novel scheme for parameterizing such a measure, in which a proximity measure is defined by a weighted combination of simple "path experts", each of which corresponds to a particular labeled path through the graph. The new learning method outperforms untrained RWR on all eight tasks, achieving an improvement in MAP scores of up to 43%. The new learning method also outperforms a widely-used simpler parameterization in which a weight is associated with each label in the graph, again producing high MAP scores on all eight tasks.</p><p>Another contribution of the paper is extension of the method to support two additional types of experts, which we call query-independent experts and popular entity experts. queryindependent experts provide a rich set of query-independent ranking schemes similar to the PageRank measure. Popular entity experts allow rankings to be adjusted for particular entities that are especially important: for instance, an popular entity expert might assign a higher weight to the specific venue "PKDD" when the query contains the keyword "mining".</p><p>The work of this paper is most closely related to other systems that learn task-specific proximity measures on labeled graphs. Most of these systems have used some variant of the simpler one-weight-per-edge-label parameterization scheme which we use as our baseline (e.g., <ref type="bibr" target="#b5">Diligenti et al. 2005;</ref><ref type="bibr" target="#b4">Chakrabarti and Agarwal 2006;</ref><ref type="bibr" target="#b16">Toutanova et al. 2004</ref>). One line of work that uses a richer "feature set" is described in <ref type="bibr" target="#b11">Minkov et al. (2006)</ref>, which explored using n-grams of edge labels as features for re-ranking results of an RWR-based system, and <ref type="bibr" target="#b10">Minkov and Cohen (2008)</ref>, who proposed a method that upweights RWR-paths which are more likely to reach relevant entities. Our approach can be viewed as principled discriminative version of this algorithm-one important advantage of which is the ability to easily incorporate additional types of information, such as the query-independent and popular entity experts described above.</p><p>There is an interesting connection between the Relational Retrieval (RR) problems considered in this work and Statistical Relational Learning (SRL) problems <ref type="bibr" target="#b7">(Getoor and Taskar 2007)</ref>. RR and SRL have slightly different task definitions: retrieval vs. classification, and their underlying inference methods have different complexities: RWR is generally more efficient than the inference for graphical models (e.g. Markov Logic Networks ( <ref type="bibr" target="#b15">Richardson and Domingos 2006)</ref>). However, RR and SRL are based on the same data model-entity relation graphs, and they share a set of related challenges: efficient parameters estimation, efficient structure learning (or ILP), and hidden concept discovery (or predicate invention).</p><p>In the remainder of the paper, we first describe the tasks and the datasets we will use in our experiments in more detail. We next present baseline results using RWR, a robust general-purpose proximity measure. In the next section, we present the path ranking algorithm, describing first the way in which path experts are enumerated, then the learning algorithm, and finally the two extensions of query-independent experts and popular entity experts. We then describe the experimental results with the new learning methods and conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The dataset and tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tasks</head><p>We consider here three new tasks that are well-suited to solution by typed proximity queries.</p><p>Venue recommendation is the problem of finding a venue to publish a new research paper. Here the query is a heterogeneous set of nodes: the terms in the title of the new paper, the set of entities (genes or proteins) associated with the paper, and the current year. The answer type is "journal", so the answer will be a list of biological journals, ranked by suitability for the new paper.</p><p>Reference recommendation (or citation recommendation) is the problem of finding relevant citations for a new paper. The query is, as in venue recommendation, the title terms and relevant entities for the new paper, the current year, and the answer type is "paper". The desired answer is a list of papers ranked by appropriateness as citations in the new paper. This task is similar to The TREC-CHEM Prior Art Search Task ( <ref type="bibr" target="#b9">Lupu et al. 2009</ref>), and can also be seen as a simplified version of the context-aware citation recommendation task ( <ref type="bibr" target="#b7">He et al. 2010)</ref>.</p><p>Expert finding is the problem of finding a domain expert for a particular topic. The query is again a list of terms and relevant entities, the current year, and the answer type is "person". The desired answer is a list of people with expertise on this topic.</p><p>The first two of these tasks are encountered in preparing a new paper, and the third is encountered in finding reviewers, or new collaborators. To evaluate performance on these tasks, we will compare the ranked list from a query associated with a paper to the actual <ref type="figure">Fig. 1</ref> Schema of the yeast data metadata associated with the paper: specifically, we will compare actual venue to the recommended venues, the actual citations to the recommended citations. Perhaps more speculatively, we will also compare the authors of a paper to the experts recommended by the query based on the title and related-entity set for the paper. In each case the predictions will be made using a graph that does not contain the actual paper in question-see the next subsection for details.</p><p>As a fourth task, we will consider the gene recommendation task considered by Arnold and Cohen (2009)-i.e., predicting, given past publishing history, which genes an author will publish about over the next year. Here the query nodes are an author and a year, and the answer type is "gene". This task is an approximation to predicting future interests.</p><p>Because the fly data is larger than the yeast data, and has an extra entity type protein, we do not use the publication year as part of the query for fly data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head><p>Most previous work on supervised training of RWR-based proximity measure have used a small number of training queries-for instance, Minkov and Cohen used more than 30 queries ( <ref type="bibr" target="#b11">Minkov et al. 2006</ref>)-or else used artificially generated document orderings ( <ref type="bibr" target="#b17">Tsoi et al. 2003;</ref><ref type="bibr" target="#b0">Agarwal et al. 2006</ref>). Using large amounts of realistic data in this study makes it possible to learn more complex models. We created two publication data sets (Yeast and Fly) in the biological domain. Paper content and metadata information are crawled from two resources: PubMed 1 is a free on-line archive of over 18 million biological abstracts for papers published since 1948; PubMed Central (PMC) 2 contains full-text and references to over one million of these papers. <ref type="figure">Figure 1</ref> shows the schema of the yeast corpus. We extracted gene mentions from the Saccharomyces Genome Database(SGD), 3 which is a database of various types of information concerning the yeast organism Saccharomyces cerevisiae, including about 48K papers, each annotated with the genes it mentions. The title words are filtered by a stop word list of size 429. The Authorship relations are further distinguish into three sub-types: any author, first author, and last author. We extracted gene-gene relations from Gene Ontology (GO), <ref type="bibr">4</ref> which is a large ontology describing the properties of and relationships between various biological entities across numerous organisms. <ref type="figure" target="#fig_0">Figure 2</ref> shows the schema of the fly corpus.   papers tagged with genes and proteins. The schema is similar to that of the yeast data, except for a new entity type Protein, <ref type="bibr">6</ref> and several relations among genes. Downstream and Upstream relation connect a gene to its two neighbors on the DNA strand. Each paper can be used to simulate a query and relevance judgments for any of the four above mentioned tasks. However, we need to prevent the system from using information obtained later than the query's date. Therefore, we define a time variant graph in which each edge is tagged with a time stamp (year). When doing random walk for a query generated from a particular paper, we only consider edges that are earlier than the publication date of that paper.</p><p>For each task on any of the two corpora, we randomly hold out 2000 queries for development, and another 2000 queries for testing. We evaluate models by Mean Average Precision (MAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline results</head><p>The RWR based retrieval model is commonly used and studied by today's research community and is serving as our baseline. The two recent extensions with richer "feature set" ( <ref type="bibr" target="#b11">Minkov et al. 2006;</ref><ref type="bibr" target="#b10">Minkov and Cohen 2008)</ref>, however, are not selected as baselines for this work, mainly because we are focusing on large scale problems here. These two extensions are designed for smaller scale problems, therefore not efficient enough to deal with the Fly and Yeast data sets we used in this study. <ref type="table" target="#tab_2">Table 2</ref> shows the result of our two baseline methods: untrained RWR model with all edges set to uniform weight 1.0, and trained RWR model (detail of which will be described in Sect. 3.2). Basically, a random walker can follow any type of edge at each step in a RWR model. While in a trained RWR model, the walker can have preference over different type of edges which is expressed as edge weights. We can see that except on the gene recommendation tasks, supervised training can significantly improve retrieval quality. By comparing four tasks we can see that venue and gene recommendation are relatively easier tasks because they have smaller number of candidate answers. Although the reference recommendation task has large number of candidate entities, the models effectively leverage the citation links among papers to achieve reasonably good retrieval accuracy. Among all four tasks, expert finding is the hardest one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The path ranking algorithm (PRA)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic path experts</head><p>One-parameter-per-edge label RWR proximity measures are limited because the context in which an edge label appears is ignored. For example, in the reference recommendation task, one of the query nodes is a year. There are two ways in which one might use a year y to find candidate papers to cite: (H1) find papers published in year y, or (H2) find papers frequently cited by papers published in year y. Intuitively, the second heuristic seems more plausible than the first; however, a system that insists on a using a single parameter for the "importance" of the edge label PublishedIn cannot easily encode this intuition. To define heuristics of this sort more precisely, let R be a binary relation. We write R(e, e 񮽙 ) if e and e 񮽙 are related by R, and define R(e) ≡ {e 񮽙 : R(e, e 񮽙 )}. We use dom(R) to denote the domain of R, and range(R) for its range. A relation path P is a sequence of relations R 1 . . . R 񮽙 with constraint that ∀i :</p><formula xml:id="formula_0">1 &lt; i &lt; &lt; − 1, range(R i ) = dom(R i+1 ). We define dom(R 1 . . . R 񮽙 ) ≡ dom(R 1 ) and range(R 1 . . . R 񮽙 ) ≡ range(R 񮽙 )</formula><p>, and when we wish to emphasize the types associated with each step in a path, we will write the path P = R 1 . . . R 񮽙 as</p><formula xml:id="formula_1">T 0 R 1 − → · · · R 񮽙 − → · · · T 񮽙</formula><p>where T 0 = dom(R 1 ) = dom(P ), T 1 = range(R 1 ) = dom(R 2 ) and so on. In this notation,the two heuristics suggested above would be written as:  This notation makes it clear that the range of each relation path is paper, the desired type for reference recommendation. We use −1 to denote the inverse of a relation, which is considered as a different relation: for instance, PublishedIn and PublishedIn −1 are considered as different relations.</p><p>For any relation path P = R 1 . . . R 񮽙 and set of query entities E q ⊂ dom(P ), we define a distribution h Eq ,P as follows. If P is the empty path, then define</p><formula xml:id="formula_2">h Eq ,P (e) = 񮽙 1/|E q |, if e ∈ E q 0, otherwise<label>(1)</label></formula><p>If P = R 1 . . . R 񮽙 is nonempty, then let P 񮽙 = R 1 . . . R 񮽙−1 , and define</p><formula xml:id="formula_3">h Eq ,P (e) = 񮽙 e 񮽙 ∈range(P 񮽙 ) h Eq ,P 񮽙 (e 񮽙 ) · I (R 񮽙 (e 񮽙 , e)) |R 񮽙 (e 񮽙 )| ,</formula><p>where I (R(e 񮽙 , e)) is an indicator function that takes value 1 if R(e 񮽙 , e) and 0 otherwise. If we assume that I (R(e 񮽙 , e)) = 0 when e 񮽙 is not in dom(R), then the definition naturally extends to the case where E q is not a subset of dom(P ). Given these definitions, the intuition that "heuristic H1 is less useful than H2" could be formalized as follows: for reference recommendation queries E q , T q , where E q is a set of title words, gene-protein entities, and a year y, entities e 1 with high weight in h Eq ,PublishedIn −1 are not likely to be good citations, where as entities e 2 with high weight in h Eq ,PublishedIn −1 .Cite are likely to be good citations. More generally, given a set of paths P 1 , . . . , P n , one could treat these paths as features for a linear model and rank answers e to the query E q by</p><formula xml:id="formula_4">θ 1 h Eq ,P 1 (e) + θ 2 h Eq ,P 2 (e) + · · · + θ n h Eq ,Pn (e)</formula><p>where the θ i are appropriate weights for the paths.</p><p>In this paper, we consider learning such linear weighting schemes over all relation paths of bounded length 񮽙. For small 񮽙 (e.g., 񮽙 ≤ 4), one can easily generate P(q, l) = {P }, the set of all type-correct relation paths with range T q and length ≤l. The distributions defined by all the relation paths can be summarized as a prefix tree <ref type="figure" target="#fig_2">(Fig. 3)</ref>, where each node corresponds to a distribution h P (e) over the entities. A PRA model ranks e ∈ I (T q ) by the scoring function</p><formula xml:id="formula_5">s(e; θ) = 񮽙 P ∈P(q,l)</formula><p>h Eq ,P (e)θ P</p><p>In matrix form this could be written s = Aθ , where s is a sparse column vector of scores, and θ is a column vector of weights for the corresponding paths P . We will call A the feature matrix, and denote the i-th row of A as A i . We found that, because some of the relations reflect one-to-one mapping, there are paths give exactly the same distribution over the target entities. For example, the following three paths among years are actually equivalent, where Before −1 is the inverse of relation Before: Given the training data, parameter estimation can be formulated as maximizing a regularized objective function</p><formula xml:id="formula_7">O(θ) = 񮽙 m=1...M o (m) (θ ) − λ|θ | 2 /2<label>( 3 )</label></formula><p>where λ is a regularizer, and o (m) (θ ) is a per-instance objective function. In this paper we use binomial log-likelihood (the loss function for logistic regression); however, negative hinge loss (for SVM), negative exponential loss (for boosting), and many other functions could be used instead. Binomial log-likelihood has the advantage of being easy to optimize, and also does not penalize outlier samples too harshly, as exponential loss does. For a training instance (q (m) , r (m) ), let A (m) be its corresponding feature matrix, R (m) be the index set of the relevant entities, and N (m) the index set of the irrelevant entities. In order to balance uneven number of positive and negative entities, we use the average log-likelihood of positive and negative entities as the objective</p><formula xml:id="formula_8">o (m) (θ ) = 񮽙 i∈R (m) ln p (m) i |R (m) | + 񮽙 i∈N (m) ln(1 − p (m) i ) |N (m) | (4) where p (m) i = p(r (m) i = 1; θ) = σ (θ T A (m) i )</formula><p>, σ is the sigmoid function σ (x) = exp(x)/(1 + exp(x)), and the gradient is</p><formula xml:id="formula_9">∂o (m) (θ ) ∂θ = 񮽙 i∈R (m) (1 − p (m) i )A (m) i |R (m) | − 񮽙 i∈N (m) p (m) i A (m) i |N (m) | .<label>(5)</label></formula><p>For most retrieval tasks, there are just a few positive entities but thousands (or millions) of negative ones. Therefore using all of them in the objective function is expensive. Here we used a simple strategy similar to stratified random sampling (Pavlu 2008). First, we sort all the negative entities using PRA model without training (i.e., all feature weights are set to 1.0). Then, entities at the k(k + 1)/2-th positions are selected as negative samples, where k = 0, 1, 2, . . . . This is helpful because, in generally, non-relevant entities highly ranked by some weak ranking function are more important than lower ranked ones: for in-depth comparisons of different selection strategies we refer the reader to <ref type="bibr">Aslam et al.'s work (Aslam et al. 2009</ref>).</p><p>For parameter estimation of the one-weight-per-edge-label RWR model, we use the same log-likelihood objective function and LBFGS optimization procedure as for PRA. Since a RWR can be seen as the combination of all the PCRWs with each path having its weight set to the product of all the edge weights along the path, we can calculate the gradient of edge weights by first calculating the gradient w.r.t. the paths, and then applying the chain rule of derivative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query-independent experts</head><p>The features above describe a entity only in terms of its position in the graph relative to the query entities. However, the relevance of an entity may also depend on query-independent qualities-for instance, its recency of publication, its citation count, or the authoritativeness of the venue in which it was published. To account for these intrinsic properties of entities, we extend every query set E q to include a special entity e * . We then extend the graph so that for each type T , there is a relation AnyT such that AnyT(e * , e) is true for every e ∈ T . For example, the relation AnyPaper maps e * to each paper, and the relation AnyYear maps e * to each year.</p><p>For example, the path e * AnyPaper −−−−→ paper Cite − − → paper defines this random-walk process: start from any paper with equal probability, and then jump to one of its referenced papers. This results in higher probability mass to the papers with high citation count. A path that starts with AnyPaper and then follows two Cite edges assigns weight to papers frequently cited by other highly-cited papers: as path length increases, a combination of this variety of query-independent paths begins to approximate the PageRank for papers on the citation graph.</p><p>These scores can be seen as a rich set of query-independent features, which can be combined with query-dependent path features to rank the target entities. To use them, the scoring function in (2) remains unchanged. However, since these paths are query-independent, we improve performance by computing their values for every entity offline. In particular, using the time-variant-graph described in Sect. 2.1, we calculate, for each year, the h score for all query-independent paths, using only edges earlier than that year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Popular entity experts</head><p>Previous work in information retrieval has shown that entity specific characteristics can be leveraged for retrieval. For the ad hoc retrieval task, some lower ranked document under a query may be interesting to the users and got clicked very often because of features not captured by the ranking function of the system. In this case, promoting these popular documents to higher rank would result in better user experience ( <ref type="bibr" target="#b18">White et al. 2007</ref>). For personalized search ( <ref type="bibr" target="#b6">Dou et al. 2007</ref>), different users may have different information needs under the same query: for instance, the word "mouse" can mean different things for a biologist and a programmer. In this case, modeling the correlation between query entities (users) and target entities (documents) can be useful.</p><p>In this work, we provide a simple yet general way of modeling entity popularities by adding biases and query-conditioned biases to the target entities. For a task with query type T 0 , and target type T q , we introduce a popular entity bias θ pop e for each target entity e ∈ T q . We also introduce a conditional popular entity bias θ e 񮽙 ,e for each query-target entity pair (e 񮽙 , e), where e 񮽙 ∈ T 0 , e ∈ T q . The scoring function in (2) is extended to</p><formula xml:id="formula_10">s(e; θ) = 񮽙 P ∈P(q,l) h Eq ,P (e)θ P + θ pop e + 񮽙 e 񮽙 ∈Eq θ pop e 񮽙 ,e ,<label>(6)</label></formula><p>or in matrix form s = Aθ + θ pop + 񮽙q, where θ pop is an concatenation of all bias parameters, 񮽙 is an matrix of all conditional bias parameters, and q is a binary vector indicating whether each entity is included in the query.</p><p>We can see that the number of parameters is potentially very large. For example, θ pop has the length of the total number of entities of the target type, and 񮽙 is a huge matrix with number of rows and columns equal to the number of entities in the target and query entity type. Since it is impractical to include all of them to the model (consider the task of retrieving documents using words), we use an efficient induction strategy which only add the most important features ( <ref type="bibr" target="#b14">Perkins et al. 2003</ref>). At each LBFGS training iteration, we add to the model the top J popular entity expert parameters which have the largest gradient (in magnitude) w.r.t. the objective function in (3). We call J the batch size. In our experiment, we found J = 20 gives relatively good performance. We also restrict the induction to be applied no more than 20 times during training. In this way, the computation cost is not bounded by the size of θ pop and 񮽙, but the number of non-zero elements in them. In practice, we found that training a PRA model with popular entity experts is not much more expensive than training a regular PRA model, and the details will be given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Biologists currently spend a lot of time in searching for relevant information about specific bioentities (e.g. a set of genes). Here we explore how relational retrieval can help biologists in various tasks. We report empirical results of comparing PRA with unsupervised RWR model, and its supervised version (RWR+train). We also compare to PRA with queryindependent path experts (PRA+qip), and PRA with popular entity experts (PRA+pop).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter tuning on development data</head><p>In this subsection, we show the parameter swiping for reference recommendation task on the yeast data. Other tasks have similar trend, but their plots are not shown here due to space limitation. <ref type="figure" target="#fig_4">Figure 4</ref> shows the relation between path tree depth and model complexity. For PRA model we can see that both model complexity (measured by number of features) and query  execution time are exponential to the path length. The query independent path (qip) extension introduces about twice number of paths than the basic PRA algorithm. However, since these query independent random walks are performed offline, they do not significantly affect query execution time. Although the popular entity experts introduce a large number of features, they are easy to calculate, therefore do not significantly affect query execution time. <ref type="figure" target="#fig_5">Figure 5a</ref> shows the effect of L 2 -regularization and path length on retrieval quality. We can see that a small amount of L 2 -regularization can slightly improve MAP, and longer path lengths give better performances, but only to a certain level. In order to balance between retrieval quality and computational complexity, we fix max path length, for the rest of the experiment, to 4 for the venue recommendation task and 3 for the other three tasks.</p><p>In <ref type="figure" target="#fig_5">Fig. 5b</ref>, we vary the number of training queries to see how training data size affects the quality of the model. we can see that all learning methods benefit from more training data, and it is especially evident when popular entity experts are used. This is due to the fact that   they have a large number of parameters to estimate, and we need at least a thousand training queries to prevent over fitting and to get good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Examples of important path features</head><p>In the TREC-CHEM Prior Art Search Task ( <ref type="bibr" target="#b9">Lupu et al. 2009</ref>), people found that instead of directly searching for patents with the query words, it is much more effective to first find patents with similar topic, then aggregate these patents' citations. The relation path of this strategy can be expressed as "query word  <ref type="table">Table 3</ref>), but also finds several other useful paths. <ref type="table">Table 3</ref> shows a subset of features for a PRA + qip + pop model trained for the reference recommendation task on the yeast data. Feature #1-#8 are regular path features. Among them, feature #6 resembles what most ad-hoc retrieval systems would do to find relevant papers: finding papers with many words overlapping with the query. However, we can see that this feature is not considered the most important by the model. Instead, the model favors the papers that are well cited by on-topic papers (#2), and the papers cited together with the  on-topic papers (#1). Papers cited during the past two years (#7, #8) are also favored. In contrary, general papers published during the past two years (#12, #13) are disfavored.</p><p>Features starting with e * are query-independent path features. We can see that well cited papers are generally favored (#9). Since the number of papers published is increasing every year, feature #14 actually disfavors old papers.</p><p>Features of the form "&gt; XXX" are popular entity biases on specific entities. Features of the form "XXX &gt; XXX" are conditional popular entity biases that associate a query entity with a target entity. We can see that papers about specific genes (e.g. CAL4, CYC1) often cite specific early works (#10, #11). <ref type="table" target="#tab_4">Table 4</ref> shows a subset of features for a PRA+qip+pop model trained for the venue recommendation task on the fly data. We can see that different journals have different preferred topics (#7-#9), and some journals are less likely to accept drosophila related papers (#11-#13). Although journals of old papers are disfavored (#14), journals of popular papers are favored (#6). Interestingly, journals with many on-topic first authors (#2) are more favored than those with just any on-topic authors (#3). <ref type="table" target="#tab_5">Table 5</ref> compares the effectiveness of different ranking algorithms on all four tasks and two copora. We can see that PRA performs significantly better than RWR under most tasks. The query-independent path experts (PRA+qip) manage to improve over basic PRA model in all tasks, and especially in reference recommendation and expert finding tasks. The popular entity experts (PRA+pop) also manage to improve over basic PRA model in all tasks, and the different is very significant on yeast tasks. Compare baseline RWR with PRA and its two extensions: query-independent path experts (+qip) and popular entity experts (+pop). The tasks are Venue Recommendation (Ven), Reference Recommendation (Ref), Expert Finding (Exp), and Gene Recommendation (Gen). Performances are measured by MAP, and the numbers in the brackets are relative improvement (%) over the trained RWR model. Except these † , all improvements are statistically significant at p &lt; 0.05 using paired t -test </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>We proposed a novel method for learning a weighted combination of path-constrained random walkers, which is able to discover and leverage complex path features of relational retrieval data. We also evaluate the impact of using query-independent path features, and popular entity features which can model per entity characteristics. Our experiment on several recommendation and retrieval tasks involving scientific publications shows that the proposed method can significantly outperforms traditional models based on random walk with restarts.</p><p>We are very interested in the generalization from simple relations to hyper-relations which are mappings from possibly more than one source types. For example, there is much incentive to express the AND relation ( <ref type="bibr" target="#b3">Balmin et al. 2004</ref>): e.g. consider the task of finding papers that are both written by certain author and recent. However, model complexity will be a major concern. Efficient structure selection algorithm is very important to make a system practical.</p><p>Furthermore, we are interested in algorithms that introduces new entities and edges to the graph. This can potentially be useful to improving retrieval quality or efficiency. For example, new entities can represent subtopics of research interests, and new links can represent memberships from words, authors or papers to these subtopics. In this way, a model might be able to replace some long paths which we have shown in the experiment with relatively shorter and more effective paths associated with the introduced structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Schema of the fly data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>H</head><label></label><figDesc>1 : year PublishedIn −1 − −−−−−− → paper H 2 : year PublishedIn −1 − −−−−−− → paper Cite − − → paper</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 A 2-level relation tree for a simple schema of paper and author</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>year Before −1 − −−− → year Before − −− → year Before − −− → year year Before − −− → year Before −1 − −−− → year Before − −− → year year Before − −− → year Before − −− → year Before −1 − −−− → year To avoid creating these uninteresting paths, we add constraint to the following relations that they cannot be immediately preceded by its inverse: year Before −1 − −−− → year Before − −− → year year Before − −− → year Before −1 − −−− → year journal PublishedBy −1 − −−−−−− → paper PublishedBy − −−−−− → journal year PublishedIn −1 − −−−−−− → paper PublishedIn −−−−−→ year 3.2 Parameter estimation There have been much previous work in supervised learning of random walk models. Nie et al. (2005) use exhaustive local search over each edge type, which is only applicable when the number of parameters is very small. Diligenti et al. (2005) and its follow up (Minkov et al. 2006) optimize weights on the relations using back-propagation, which has linear con- vergence, therefore requires many iterations to reach convergence. Recent works (Agarwal et al. 2006; Chakrabarti and Agarwal 2006) use more efficient second order optimization procedure like BLMVM for numerical optimization. In this study, we use L-BFGS (Andrew and Gao 2007), one commonly used second order optimization procedure in many machine learning problems, and binomial log-likelihood loss functions. The training data can be represented as D = {(q (m) , r (m) )}, m = 1 . . . M, where r (m) is a binary vector. r (m) e = 1 if entity e is relevant to the query q (m) , and r (m) e = 0 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Model complexity verses maximum path length L for reference recommendation task on the yeast data. Execution time is an average of 2000 test queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 (a) Compare different regularization parameter λ and different path length l. (b) Effect of training data size for reference recommendation task on yeast data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3</head><label>3</label><figDesc>Subset of features from a PRA+qip+pop model trained for the reference recommendation task on the yeast data. In is a shorthand for the PublishedIn relation&gt;Cell. 1979. Sequence of the gene for iso-1-cytochrome c in Saccharomyces cerevisiae . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>ContainedBy − −−−−− → patent Cite − − → patent". In our experiment, the PRA model based on PCRW not only successfully identifies this paths as an important feature in scientific literature domain (path #2 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 Corpus</head><label>1</label><figDesc></figDesc><table>statistics 
Graph size 
No. query 

Paper 
Node 
Edge 
Train 
Dev 
Test 

Yeast 
48K 
164K 
2.8M 
2K 
2K 
2K 

Fly 
127K 
770K 
3.5M 
2K 
2K 
2K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 MAP of the baseline RWR model, and RWR model with learning based on one weight per edge label (see Sect. 3.2 for details). The numbers in parenthesis are relative improvement of MAP(%). Except these † , all improvements are statistically significant at p &lt; 0.0001 using paired t -test</head><label>2</label><figDesc></figDesc><table>Corpus 
Task 
RWR 

Untrained 
Trained 

Yeast 
Venue recommendation 
40.4 
4 4 .2 (+9.4) 

Yeast 
Reference recommendation 
11.8 
1 6 .0 (+35.6) 

Yeast 
Expert finding 
9.9 
1 1 .1 (+12.1) 

Yeast 
Gene recommendation 
14.4 
1 4 .4 (+0.0)  † 

Fly 
Venue recommendation 
45.4 
4 8 .3 (+6.4) 

Fly 
Reference recommendation 
18.8 
2 0 .5 (+9.0) 

Fly 
Expert finding 
5.6 
7 .2 (+28.6) 

Fly 
Gene recommendation 
18.7 
1 9 .2 (+2.7)  † 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 Subset of features from a PRA+qip+pop model trained for the venue recommendation task on the fly data. In is a shorthand for the PublishedIn relation</head><label>4</label><figDesc></figDesc><table>ID 
Weight 
Feature 

1 
2 6 .9 
word 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="6"> In yeast, there is a nearly one-to-one relationship between genes and proteins, as most genes are transcribed to a unique protein; in flies, alternative splicing means that a gene can be transcribed to several different proteins.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable training of l 1 -regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Learning to rank networked entities</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information extraction as link prediction: using curated citation networks to improve gene detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Document selection methodologies for efficient and effective learning-to-rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Objectrank: authority-based keyword search in databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning parameters in entity relationship graphs from ranking preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning web page scores by error back-propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large-scale evaluation and analysis of personalized search strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Context-aware citation recommendation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Overview of the TREC 2009 chemical IR track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<idno>TREC-18</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning graph walk based similarity measures for parsed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contextual search and name disambiguation in email using graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object-level ranking: bringing order to web objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large scale IR evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer and Information Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grafting: fast, incremental feature selection by gradient descent in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning random walk models for inducing word dependency distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive ranking of web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Morini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Studying the use of popular destinations to enhance web search interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
