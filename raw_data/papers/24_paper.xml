<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Identification from Text Using N-gram Based Cumulative Frequency Addition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2004-05-07">May 7th, 2004</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bashir</forename><surname>Ahmed</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Hyuk</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Tappert</surname></persName>
						</author>
						<title level="a" type="main">Language Identification from Text Using N-gram Based Cumulative Frequency Addition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of Student/Faculty Research Day</title>
						<meeting>Student/Faculty Research Day						</meeting>
						<imprint>
							<date type="published" when="2004-05-07">May 7th, 2004</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>language identification</term>
					<term>cumulative frequency addition</term>
					<term>Naïve Bayesian classification</term>
					<term>rank order statistics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper describes the preliminary results of an efficient language classifier using an ad-hoc Cumulative Frequency Addition of N-grams. The new classification technique is simpler than the conventional Naïve Bayesian classification method, but it performs similarly in speed overall and better in accuracy on short input strings. The classifier is also 5-10 times faster than N-gram based rank-order statistical classifiers. Language classification using N-gram based rank-order statistics has been shown to be highly accurate and insensitive to typographical errors, and, as a result, this method has been extensively researched and documented in the language processing literature. However, classification using rank-order statistics is slower than other methods due to the inherent requirement of frequency counting and sorting of N-grams in the test document profile. Accuracy and speed of classification are crucial for a classier to be useful in a high volume categorization environment. Thus, it is important to investigate the performance of the N-gram based classification methods. In particular, if it is possible to eliminate the counting and sorting operations in the rank-order statistics methods, classification speed could be increased substantially. The classifier described here accomplishes that goal by using a new Cumulative Frequency Addition method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Most Natural Language Processing (NLP) problems were once viewed as extremely difficult. In recent years, however, many of these problems have become more tractable due largely to the rapid growth in computing power and the advancements made in language processing algorithms. Research effort in NLP in both academic and commercial settings has increased greatly due to globalization and the need to communicate in an international corporation. The end result of all the effort has resulted in many commercial speech recognition, speech translation, and text-to-speech systems <ref type="bibr" target="#b0">[1]</ref>. This trend is creating awareness of language processing (Speech recognition, speech translation and text-to-speech) as a viable tool, and language identification is the prerequisite of any such system. Corporate America is constantly looking to increase its bottom line by finding ways to increase productivity, and many corporations are trying to leverage speech recognition and text-to-speech technology for customer service. For example, automatic detection of a shift from one language to another in written text can play an important role in the quality of a text-to-speech (TTS) system. The accurate tagging of the words with correct language is critical for the other steps in TTS synthesis to complete successfully <ref type="bibr" target="#b8">[9]</ref>. Thus, when a TTS system is turned on in English mode and it encounters a German word, if the system can detect that the next word is German, it can then automatically switch to German lexica and pronounce the word correctly in German. Authors in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> reported on such polyglot systems and they used heavy linguistic knowledge to resolve dual lingual text to separate English words from German text. This could increase the user acceptance of TTS systems and make commercial application more appealing. To accomplish this goal, we need to have a generic language classifier in front of the TTS module that can detect the language from short character strings.</p><p>The field of linguistic research has existed for a long time. As a result, we find many successful commercial grade language identifiers in the market today. However, not all identifiers are implemented the same way. There are two major approaches to text analysis -using linguistic models and statistical models. Sproat <ref type="bibr" target="#b7">[8]</ref> reported on a textanalysis system based on weighted Finite-State Transducers where transducers were constructed using a lexical toolkit that allowed declarative description of lexicons, morphological rules, and phonological rules etc. While the linguistic models are realistic models, they are complex and require language specific processing rules. Statistical models are generic models and, using machine learning, they utilize different features from training samples to categorize text. Several methods of feature extraction have been used for language classification, including unique 12.2 letter combinations, short word method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, N-gram method, and ASCII vector of character sequences. Among the most reported classifiers are Bayesian Decision Rules <ref type="bibr" target="#b9">[10]</ref>, Rank Order Statistics <ref type="bibr" target="#b10">[11]</ref>, K-Nearest Neighbor, and Vector Space Model. For a language classifier to be useful, especially in applications like shifting from one language to another, it must be fast. This paper describes preliminary research on such a classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Canvar and Trenkle <ref type="bibr" target="#b10">[11]</ref> reported 99.8% correct classification rate on Usenet newsgroup articles written in different languages using rank-order statistics on N-gram profiles. They reported that their system was relatively insensitive to the length of the string to be classified, but the shortest strings that they reported classifying was 300 Bytes (characters). They classified documents by calculating the distances of a test document's N-gram profile from all the training language N-gram profiles and then taking the language corresponding to the minimum distance. In order to perform the distance measurement they had to sort the N-grams in both the training and test profiles.  <ref type="table" target="#tab_0">Table 1</ref>. Distance calculation using rank-order statistics <ref type="bibr" target="#b10">[11]</ref>.</p><p>Their method is simple and works well in identifying the language from text string of 300 bytes or more. The method is also insensitive to typographical errors. However, the authors did not provide any information on the speed of classification rate, only mentioning that their training N-gram profiles were small. In this paper, we focus on the speed of classification rate using N-gram based classification as the underlying method. The two main problems with their classification scheme are the requirements to count the frequency of each N-gram in the test document and to sort the N-grams to perform distance measurement. It is well known that executing a database query with a sort operation is a resource intensive and time consuming operation, and that elimination of sorting from computing algorithms will enhance performance. Here we introduce a new classifier using a similar N-gram profile, but without requiring a sort operation on either the training or testing N-gram profiles. Instead of rank-order statistics, we use Cumulative Frequency Addition (CFA) to classify test documents into different categories. For purposes of comparison, using the same training dataset, we performed classification using the rank-order statistical method and the Naïve Bayesian classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection of Text Samples and Creation of N-gram Profiles</head><p>Text samples were collected in 12 languages. For example, we collected test samples from Danish, English, French, Italian and Spanish online newspapers using a semi-automatic program written in VBA in Microsoft Access. The sample collection program runs by opening Internet Explorer to a given URL, such as a newspaper website, extracting text strings from the active page using Microsoft's dynamic HTML features (MSHTML object library), and storing the content in a text file. To keep track of which file came from which website, the program kept an URL/File mapping table in the database with the URL location and the name of the file created from that URL.</p><p>We created a training database containing N-grams from 240 sample files from 12 Latin-character-based languages, 20 files in each language. The twelve languages were English, Spanish, Italian, Danish, Polish, Swedish, Portuguese, German, French, Romanian, Dutch and Tagalog. The training sample sizes ranged from 65K to 105K as shown in <ref type="table" target="#tab_2">Table 2</ref>. We collected 2, 3, 4, 5, 6 and 7 grams from these language samples and stored them with their counts of occurrence in a database table. After collecting the N-grams, we deleted those that occurred only once, greatly reducing the number of N-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12.3</head><p>Many authors reported preprocessing of training data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, such as getting rid of punctuation marks such as commas, numbers and special characters, before collecting N-gram statistics. Unlike those authors, however, no preprocessing was performed on these training data. Rather, the N-grams were obtained by simply reading each line of text and segmenting the string into different size N-grams, moving forward character by character. Some authors also reported replacing space character with "_" (underscore), but we left space as a valid character. By not preprocessing we had many N-grams in the training database that contained only numbers such as "11", "22", "011" etc. These N-grams were deleted since they are not much help in differentiating among languages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-gram Frequency Calculation</head><p>After eliminating the unnecessary N-grams, we calculated the total N-gram counts for each language as well as the overall N-gram count for the entire training set. We calculated two frequencies for each N-gram -the internal frequency and the overall frequency -as follows:   </p><formula xml:id="formula_0">F I (i, j) = C (i, j) / ∑ i C (i, j) F O (i, j) = C (i, j) / ∑ i,j C (i, j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12.4</head><p>The internal and overall frequencies of each N-gram were normalized by dividing each value by the highest frequency of the entire training database and then adding 1 to each value. Thus, the final value of each N-gram frequency was normalized to a value between 1 and 2. <ref type="table">Table 4</ref> shows sample data from our training database with internal and overall frequencies before normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-gram Rank Ordering</head><p>In the training set, we rank ordered the N-grams in two different ways. First, the internal rank ordering was done for each language by sorting all the N-grams within each language in descending order of frequency and ranking them from 1 to an incrementally higher number. Second, the overall rank ordering was done for the entire training set by sorting the N-grams in descending order of language occurrence, ranking them from 1 to 12 since there are 12 languages in the training database. Note that in the internal rank ordering scheme, an N-gram could be ranked from 1 to the highest ranked position in that language training set. <ref type="table">Table 4</ref> shows sample data on these two rank ordering schemes. Canvar and Trenkle previously described internal rank ordering <ref type="bibr" target="#b10">[11]</ref>.  <ref type="table">Table 4</ref>. Sample data with calculated internal and overall rank orders, and internal and overall frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-gram N-gram</head><p>The data in <ref type="table">Table 4</ref> shows that the internal rank ordering of the N-gram "Minist" was ranked 186th in Portuguese, 192 in Italian, and 220 in Danish. However, this same N-gram when sorted for the entire training set, ranked 1 for Portuguese (count 75), 2 in Italian (count 61), and 3 in Danish (count 49).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Procedures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification by Rank-Order Statistics</head><p>Each test string was tokenized using N-grams of sizes 2, 3, 4, 5, 6 and 7. Just as in the training set, no preprocessing of the string was performed. To classify the string using the rank-order statistical method, while tokenizing, we kept count of each N-gram and incremented the counter if it occurred multiple times. Since we were using a Microsoft Access database, we did this by inserting each N-gram as a record in a test table and updated the N-gram count field of the record on each additional occurrence. We used a Microsoft Access database because we were familiar with it and because the N-grams in both the training and test data were persistent in the tables so we could visually examine each N-gram and check the resulting N-gram list that participated in the classification. After tokenizing and computing N-grams counts, we sorted the N-grams and created the rank ordered lists.</p><p>Once we had the training N-grams and the test N-grams ranked with rank order ids, by issuing a simple SQL and joining the test N-grams and the Training N-grams table, we came up with a candidate N-grams list and used these to perform the distant measurement. A test N-gram that did not have a match in the training database for any language was given a default maximum distance of 1000. Canvar and Trenkle used similar maximum distance for unmatched Ngrams, but did not specify what the maximum distance was. <ref type="table">Table 5</ref> shows the list of candidate Ngrams that would have been used to classify the string "Bon Jour."</p><p>12.5 *** For Ngrams with no match, a default max distance of 1000 was used. <ref type="table">Table 5</ref>. Candidate N-grams from the string "Bon Jour" with their rank-order statistics.</p><p>The rank ordered distances were then summed and sorted from lowest to highest, and the language with the lowest number was selected as the language category. <ref type="table">Table 6</ref> shows the result for 'Bon jour."  <ref type="table">Table 6</ref>. Classification of the string "Bon Jour" as French using the rank-order statistics method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification using Cumulative Frequency Addition</head><p>Similar to the rank-order statistics method, each test string was tokenized using N-grams of sizes 2, 3, 4, 5, 6 and 7. Again, as with the training set, no preprocessing of the string was performed. To classify the string using the cumulative frequency addition method, we simply tokenized the string and built an N-gram list. We did not have to keep count of occurrence for each N-gram and did not have to sort the N-grams. After tokenizing the string, the resulting N-gram list may have contained multiple repetitions of the same N-gram. The following simple SQL statement operates on the training and test N-grams to provide the N-grams participating in the classification (All_Training_N-grams is the name of the database table where the training N-gram profiles are stored).</p><p>Select N-gram, language, internal_frequency, overall_frequency from All_Training_N-grams where N-gram in ("Test N-gram list")</p><p>Again, any test N-gram that did not have a match in the training database for any language was dropped from the calculation. For the string "Bon Jour," <ref type="table">Table 7</ref> shows a list of candidate N-grams with their internal and overall frequencies, and <ref type="table">Table 8</ref> shows the final result.</p><p>12.6  <ref type="table">Table 7</ref>. Candidate N-grams from the string "Bon Jour" with their internal and overall frequency statistics.  <ref type="table">Table 8</ref>. Classification of the string the string "Bon Jour" as French using the cumulative frequency sum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lang</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification using Naïve Bayesian Classifier</head><p>The same set of candidate N-grams from above was used for the NBC method. In this case, instead of addition, we multiplied the normalized frequencies of all candidate N-grams from each language of the training set. The language that produced the highest number was identified as the correct one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A total of 291 files from 5 different languages were used for testing: Danish 52, English 66, French 53, Italian 60 and Spanish 60. <ref type="table">Table 9</ref> shows the number of files tested in each language with the % accuracy obtained in all three methods. <ref type="figure">Figure 1</ref> shows the results in from 50, 100, and 150 byte strings using the three classification methods. For files of 150 Bytes (characters) in length, the rank-order statistics and cumulative frequency addition methods attained an accuracy of 100%, while the Naïve Bayesian classifier attained 99.7%. The cumulative frequency addition and the Naïve Bayesian methods were of comparable speed and 3-10 times faster than the rank-order statistics method <ref type="table" target="#tab_0">(Table 10)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Length of String</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The strength of the cumulative frequency addition and the Naïve Bayesian classification methods are the speed of classification while the strength of the rank-order statistics method is its accuracy of classification. Furthermore, the accuracy of the CFA method described here is comparable to that of the rank-order statistics method in classifying short strings. The speed of classification of the new approach can be particularly useful when classifying mixed lingual texts where the major language is to be identified from long test strings and the minor languages from short strings. The rank-order statistics method is not appropriate for short strings because rank ordering and sorting is slow and requires long strings. Although the Naïve Bayesian classification method is extremely fast, it did not have good accuracy with small strings where it performed poorly compared to the other methods. The reason the CFA methods works well on the language classification problem might be attributable to linguistic properties of this problem. NBC assumes that the probabilities of unique N-grams are independent of other N-grams, and this is not a good assumption, especially when dealing with language. For example, the occurrence of "th" increases the probability that the word may be "the" or 'that." Thus, assuming that "th" is independent of "that" or "the" is not valid. This may explain why frequency addition performs better than NBC with short strings. To more verify these preliminary findings, more testing is needed on larger data sets, and this work is in progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) F I (i, j) = Internal frequency of a N-gram i in language j F O (i, j) = Overall frequency of a N-gram i in language j C (i, j) = Count of the i th N-gram in the j th language ∑ i C (i, j) = Sum of the counts of all the N-grams in language j ∑ i,j C (i, j) = Sum of the counts of all the N-grams in all the languages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 illustrates how they performed their calculation.</head><label>1</label><figDesc></figDesc><table>Language profile 
Test document profile 
Out of place 
Most Frequent 
TH 
TH 
0 
ER 
ING 
3 
ON 
ON 
0 
LE 
ER 
2 
ING 
AND 
1 
Least Frequent 
AND 
ED 
No-match = max distance 

Test Document Distance from Language = Sum of out-of-place values 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Training sample size and the corresponding N-gram statistics.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 shows</head><label>3</label><figDesc></figDesc><table>a sample calculation using a hypothetical Ngram "xyz". 

Language N-gram 
N-gram 
Count 

Total N-grams 
in this Language 

Total N-grams 
in all Languages 

Internal 
Frequency 

Overall 
Frequency 
English 
xyz 
10 
100 
1000 
10/100 
10/1000 
Danish 
xyz 
15 
150 
1000 
15/150 
15/1000 
French 
xyz 
10 
200 
1000 
10/200 
10/1000 
Italian 
xyz 
15 
300 
1000 
15/300 
15/1000 
Tagalog 
xyz 
3 
40 
1000 
3/40 
3/1000 
German 
xyz 
13 
60 
1000 
13/60 
13/1000 
Spanish 
xyz 
28 
150 
1000 
28/150 
28/1000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>A sample of how internal and overall frequencies are calculated.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Tested</head><label></label><figDesc></figDesc><table>Percent Correct 
Rank Order Statistics 
(All languages) 

Percent Correct 
Cumulative Frequency Sum 
(All languages) 

Percent Correct 
Naïve Bayesian Classifier 
(All languages) 

50 
96.56 
97.59 
88.66 

100 
98.97 
98.97 
96.90 

150 
100.00 
100.00 
99.70 

Table 9. Summary of Results. 

12.7 

Freq. Sum vs NBC vs Rank Order Statistics 

82 

84 

86 

88 

90 

92 

94 

96 

98 

100 

102 

50 
100 
150 

Length of Test String 

% Correct 

Naïve Bayes 

Frequency Sum 

Rank Order Statistics 

Figure 1. Percent accuracy of classification of NBC, CFA, and rank-order statistics. 

Language 

String 
Length 

Cumulative 
Frequency Addition 
(seconds) 

Rank-Order 
Statistics 
(seconds) 
Ratio 
French 
236 
0.46 
4.6 
10.00 
French 
206 
0.48 
4.1 
8.54 
French 
160 
0.44 
3.2 
7.27 
French 
134 
0.37 
2.8 
7.57 
French 
101 
0.37 
2.1 
5.68 
French 
75 
0.32 
1.8 
5.62 
French 
51 
0.32 
1.2 
3.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 . Speed of classification for cumulative frequency addition versus rank-order statistics.</head><label>10</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Artificial Speech: Two centuries of tinkering finally produce a sweettalking machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Burdick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-01" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>The Mathematics of</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed Lingual Text Analysis for Polyglot TTS Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Romsdorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-09-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lexical and Syntactic Analysis of Mixed-Lingual Sentences for Text-to-Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wehrli</surname></persName>
		</author>
		<idno>of SNSF Project No 21-59396.99</idno>
	</analytic>
	<monogr>
		<title level="j">Institut TIK</title>
		<imprint>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
<note type="report_type">Final Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From Multilingual to Polyglot Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Traber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurospeech</title>
		<meeting>the Eurospeech</meeting>
		<imprint>
			<date type="published" when="1999-09" />
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual Sentence Categorization According to Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Giguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter</title>
		<meeting>the European Chapter<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Categorization According to Language: A Step Toward Combining Linguistic Knowledge and Statistics Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Giguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International workshop for Parsing Technologies</title>
		<meeting>the International workshop for Parsing Technologies<address><addrLine>Prague-Karlovy Vary, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Stakes of Multilinguality: Multilingual Text Tokenization in Natural Language Diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Giguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4rth International Conference on Artificial Inteligence Workshop</title>
		<meeting>the 4rth International Conference on Artificial Inteligence Workshop<address><addrLine>Cairns, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-08-27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual Text Analysis for Text-To-Speech Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICSLP&apos;96</title>
		<meeting>the ICSLP&apos;96<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High Quality Text-to-Speech Synthesis: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiery</forename><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australia: Special Issues on Speech Recognition and Synthesis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
	<note>Journal of Electrical &amp; Electronics Engineering</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical Identification of Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-03-10" />
		</imprint>
		<respStmt>
			<orgName>Computing Research Laboratory, New Mexico State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">N-gram based Text Categorization, Symposium on Document Analysis and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Canvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Trenkle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="161" to="176" />
			<pubPlace>Las Vegas</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Nevada</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
