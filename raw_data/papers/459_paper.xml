<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMULATION MODEL CALIBRATION WITH CORRELATED KNOWLEDGE-GRADIENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Frazier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research &amp; Financial Engineering</orgName>
								<orgName type="institution">Princeton University Engineering Quadrangle Olden St</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>N.J</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research &amp; Financial Engineering</orgName>
								<orgName type="institution">Princeton University Engineering Quadrangle Olden St</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>N.J</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">P</forename><surname>Simão</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Operations Research &amp; Financial Engineering</orgName>
								<orgName type="institution">Princeton University Engineering Quadrangle Olden St</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Princeton</settlement>
									<region>N.J</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIMULATION MODEL CALIBRATION WITH CORRELATED KNOWLEDGE-GRADIENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address the problem of calibrating an approximate dynamic programming model, where we need to find a vector of parameters to produce the best fit of the model against historical data. The problem requires adaptively choosing the sequence of parameter settings on which to run the model, where each run of the model requires approximately twelve hours of CPU time and produces noisy non-stationary output. We describe an application of the knowledge-gradient algorithm with correlated beliefs to this problem and show that this algorithm finds a good parameter vector out of a population of one thousand with only three runs of the model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>CASTLE Laboratory at Princeton University has undertaken a number of projects for which the models developed were required to match the historical performance of a company. These models use approximate dynamic programming (ADP) to optimize an objective function, but the use of optimization is motivated primarily by the desire to mimic tactical human decision-making for the purposes of strategic planning. In particular, the companies that sponsored the development of models would not accept the results until each model matched a series of performance metrics based on history.</p><p>Calibrating one of these models requires tuning a vector of parameters that often represent bonuses and penalties used to encourage specific behaviors. These are famously known as "soft costs" (or rewards), and they allow us to combine multiple objectives within a single optimization formulation. Thus, while we may wish to maximize profits (revenues minus costs), in reality we must balance short term profits against customer service, equipment productivity and goals for managing people.</p><p>The problem can be posed as an optimization problem. Let ρ be the vector of parameters that we need to tune. Let g k , k ∈ K , be a set of metrics (on-time performance, equipment productivity, cost and revenue metrics), and let G k (ρ) be the value of each metric produced by the model when run with parameter vector ρ, which we can only observe in a noisy way. We may wish to find ρ that solves min ρ F(ρ),</p><formula xml:id="formula_0">F(ρ) = E ∑ k∈K (G k (ρ) − g k ) 2 .<label>(1)</label></formula><p>The problem of tuning this vector of parameters introduces a type of simulation-optimization, where we have to find a set of parameters to minimize a weighted metric that evaluates how well a model matches historical performance. In this setting, our model is basically a black box that can be run to evaluate how well a set of parameters matches history. The challenge is that the models are slow. Optimizing the movements of thousands of drivers, locomotives or jets, despite extensive engineering, is not an easy process. These large-scale models can require hours to run in the best case, and may even require several days. In addition, the outcome of a run is noisy. We use the context of a fleet simulator developed for Schneider National, one of the largest truckload motor carriers in the U.S., as our test environment. The model and its application is described in depth in <ref type="bibr" target="#b25">(Simão, Day, George, Gifford, Nienow, and Powell 2009)</ref>. The model is expensive to run and its output is noisy, and so we view the problem as optimizing the inputs to an expensive and noisy black box simulator. Unlike classical simulation models, for which observations arrive Frazier, Powell and Simão from some distribution fixed in time, ADP requires iterative learning and observations from ADP models undergo a transient initial period while the model learns and the policy stabilizes. Training until the model learns fully is very expensive, and so we would like the ability to perform simulation optimization using only these transient observations. When performing our simulation optimization, we would also like to use our intuition that the ADP model will behave similarly with similar parameter vectors. This paper provides a new method for performing this type of simulation optimization using the concept of the knowledge gradient. The knowledge gradient is a flexible Bayesian-based method that has the ability to learn with transient data, and can exploit correlations in the belief structure around neighboring parameters.</p><p>This problem represents an instance of classical simulation-optimization, for which an extensive literature has evolved. We review this literature in section 2. Section 3 provides an overview of the ADP model developed for Schneider, and a method we developed to project the performance of each metric using fewer ADP iterations. Section 4 describes the calibration process, and section 5 summarizes the knowledge-gradient algorithm and its adaptation to the simulation-optimization problem. Section 6 describes experimental results using the method to calibrate the simulation model for Schneider National, showing that we can find a good calibration vector from a population of a thousand with only six iterations. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LITERATURE REVIEW</head><p>There is an extensive literature that addresses the problem posed by (1). (Spall 2003) provides a thorough review of the stochastic search literature, whose roots can be traced to the seminal papers of <ref type="bibr" target="#b24">(Robbins and Monro 1951)</ref> and <ref type="bibr" target="#b21">(Kiefer and Wolfowitz 1952)</ref>. (Spall 2003) describes a method called the Simultaneous Perturbation Stochastic Approximation method, which uses only two sample realizations to construct a derivative, but the rate of convergence is even slower than when derivatives are available (the value of the technique is much lower cost per iteration). Response surface methods (Myers and Montgomery 2002) have been widely used in stochastic optimization for problems where derivatives are not available.</p><p>The simulation-optimization community has produced a line of research that recognizes that simulations can be expensive and need to be run efficiently (see <ref type="bibr" target="#b27">(Swisher, Jacobson, and Yücesan 2003)</ref> for a survey). An important series of papers in this line of research beginning with <ref type="bibr" target="#b2">(Chen 1995)</ref> and following with <ref type="bibr" target="#b3">(Chen, Dai, and Chen 1996</ref><ref type="bibr" target="#b9">, Chen, Dai, Chen, and Yücesan 1997</ref><ref type="bibr" target="#b6">, Chen, Lin, Yücesan, and Chick 2000</ref><ref type="bibr" target="#b8">, Chen, Chen, and Yucesan 2000</ref><ref type="bibr" target="#b4">, Chen, Donohue, Yücesan, and Lin 2003a</ref> formulates the problem under the name "Optimal Computing Budget Allocation" (OCBA).</p><p>Also considering the problem of making a sequence of measurements in order to maximize the ability to choose the best among a finite set based on the information acquired, ( <ref type="bibr" target="#b19">Gupta and Miescke 1996)</ref> proposes the idea of making a choice that maximizes the value of a single measurement. <ref type="bibr" target="#b11">(Chick and Inoue 2001)</ref> proposes a sequential method called LL(S) (sequential selection with linear loss), which allocates a fixed number of replications among each of a set of competing alternatives at each stage until a budget is exhausted. The allocation is based on an approximation of the value of a measurement at each stage.</p><p>Knowledge-gradient (KG) policies are a general class of methods for making measurements in learning problems. These methods choose the measurement with the largest single-period value, and understand this value as the derivative of the value of knowledge with respect to that measurement. <ref type="bibr" target="#b14">(Frazier, Powell, and Dayanik 2008)</ref> identified the policy used in (Gupta and Miescke 1996) as a knowledge-gradient policy and established its asymptotic optimality. Thus, in that context, the KG policy is the only stationary policy that is both myopically (by construction) and asymptotically optimal. <ref type="bibr">(Frazier, Powell, and Dayanik 2009</ref>) derived the KG policy for a ranking and selection problem with finitely many alternatives when the beliefs about different alternatives are correlated. This is the KG policy most similar to the KG policy we propose here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE ADP MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">THE SCHNEIDER NATIONAL BUSINESS SIMULATOR</head><p>The ADP model we consider assigns drivers to loads over time. The reader is referred to (Simão, Day, George, Gifford, Nienow, and Powell 2009) for a complete description of the model, but we provide a sense of the structure of the system here. The challenge is to manage a fleet of almost 7,000 drivers, each of which is described by a complex vector of attributes that makes it possible to measure statistics such as how often a driver returns home, and the productivity of different driver types such as teams (drivers working in pairs) and solos. The company expected the model to produce performance metrics that closely match historical performance, which was accomplished using a number of tuning parameters in the form of bonuses and penalties to encourage/discourage certain behaviors. The challenge faced in this research is designing a method to tune these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frazier, Powell and Simão</head><p>Let a ∈ A be a vector of attributes of a driver (there were 15 dimensions to this vector for this application), and let R ta be the number of drivers with attribute a. Let d ∈ D be the decision to assign a driver to a load, move a driver empty to another location, or to hold a driver in his current location. Let x tad be the decision (0 or 1) to act on a driver with attribute a using decision d. If R t is the resource state vector (the vector with elements R ta ) and x t is the vector of decisions, we let</p><formula xml:id="formula_1">R t+1 = R M (R t , x t ) + ˆ R t+1</formula><p>be the vector describing the state of drivers in the next time period given that the current resource state vector is R t and we act on these resources using the decision vector x t . ˆ R t+1 captures random (exogenous) changes to the resource state vector, which is used to capture travel delays, equipment failures, and random arrivals to and departures from the system. LetˆDLetˆ LetˆD t be the new demands that were first realized at time t, giving us a state variable of S t = (R t , ˆ D t ). We assign drivers to loads to maximize a contribution function given by</p><formula xml:id="formula_2">C(S t , x t |ρ) = ∑ a ∑ d c tad (ρ)x tad .</formula><p>c tad (ρ) captures the revenue from moving a load, the cost of moving a driver empty to a load, and a series of bonuses and penalties to discourage picking up a load early or late, delivering a load early or late, and putting drivers on loads of an appropriate length, and encouraging the model to get drivers home during targeted intervals (often corresponding to weekends roughly every two to three weeks). Schneider's fleet of drivers are organized into different groups with names such as solo (company employees who drive alone), team (company employees who drive in pairs, allowing one to sleep while the other drives), and independent contractors (IC's), who are not employees and as a result own their own tractors, which raises the operating cost since the driver has to use his wages to cover the cost of his equipment. Other fleet types include regional fleets, where drivers would move shorter loads but stay within a particular region of the country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DESIGNING AN ADP POLICY</head><p>An optimal policy for assigning drivers to loads can be characterized by</p><formula xml:id="formula_3">V t (S t ) = max x t C(S t , x t |ρ) + E [V t+1 (S t+1 )] subject to ∑ d x tad = R ta , a ∈ A , x tad ≥ 0, a ∈ A , d ∈ D.</formula><p>The state S t+1 is a function of the starting state S t , the action x t and the new demandsˆDdemandsˆ demandsˆD t+1 that are called in before time t + 1. R t is a vector, and has a very large number of dimensions because it captures the number of drivers with each attribute vector a. Good policies can be found using ADP, in which we solve</p><formula xml:id="formula_4">x n t = arg max x C(S n t , x|ρ) + ∑ a∈A ¯ v n−1 ta R x ta ,<label>(2)</label></formula><p>where R x ta is an element of R x t = R M (R n t , x), which is the status of all the drivers after we make a decision, but before any new information arrives (in the form ofˆRofˆ ofˆR t+1 ).</p><p>In <ref type="formula" target="#formula_4">(2)</ref>, we are approximating the expectation of the value function using a value function approximation that is linear in the post-decision resource state variable R x t . These are estimated using</p><formula xml:id="formula_5">¯ v n t−1,a = (1 − α n−1 ) ¯ v n−1 t−1,a + α n−1 ˆ v n ta ,</formula><p>where α n−1 is a step-size between 0 and 1. Estimating the slopes ¯ v n t requires stepping forward through time over a horizon (typically a month), where we have to perform roughly 100 iterations to get reasonable estimates for the slopes. At each time period, the optimization problem in equation <ref type="formula" target="#formula_4">(2)</ref> involves assigning several thousand drivers to several thousand loads. Stepping through a month of activities requires several minutes. Repeating this 100 times requires many hours of CPU time on 3GHz Pentium-class processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frazier, Powell and Simão</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE CALIBRATION PROBLEM</head><p>The success of the model was judged in terms of its ability to match a series of metrics. As described in the introduction, the model produces estimates of how many time-at-home (TAH) events a driver would incur during a month, the average length of haul for each fleet type, and other metrics such as equipment productivity and customer service. Getting the model to calibrate against historical performance requires tuning the calibration vector ρ to strike the right balance. For example, we might outperform history for equipment productivity, but find that we were not getting drivers home. To obtain a realistic model, it is important to avoid underperforming the company in any one dimension.</p><p>In our initial model development, the parameter vector ρ was tuned by hand. This was a painstaking process because of the long run times. Furthermore, as the years passed, the data would change as would the performance of the company. As a result, re-tuning the model became an annual exercise that could easily take two weeks of manual experimentation. The goal of this research is to automate the process, while minimizing the time required to find good solutions to the parameter vector.</p><p>As a preliminary to discussion the automation of the calibration process, Sections 4.1 and 4.2 discuss estimation of the quality of calibration for a particular value of ρ with a relatively small number of iterations from the ADP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">OBSERVING MODEL OUTPUT</head><p>In this section we fix ρ and k and discuss how G k (ρ) may be estimated from the output of the ADP model, where we recall that G k (ρ) is defined to be the long-run average of output k when we use input parameter vector ρ. This output is non-stationary, and if we examine its value over the iterations we typically see it moving randomly around a mean level that converges exponentially toward G k (ρ). <ref type="figure" target="#fig_3">Figure 4</ref>.1, which shows 200 iterations of the TAH metric for solo company drivers, is typical.</p><p>A simple but inefficient way to estimate G k (ρ) is to average the output over a very large number of iterations. Including early iterations in the average biases the estimate, either below G k (ρ) as it would for the sample path given in <ref type="figure" target="#fig_3">Figure 4</ref>.1, or above in those cases in which the output trends downward. As the number of iterations becomes large, this bias eventually becomes small, but many iterations are required.</p><p>A slightly better but still inefficient way to calculate G k (ρ) would be to run the ADP model until it appears close to convergence, which might be said to occur in <ref type="figure" target="#fig_3">Figure 4</ref>.1 near iteration 100, and then average only the output obtained afterward. One problem with this method is that the ADP model may not converge perfectly, even after a large number of iterations, instead growing only infinitessimally closer to convergence. This induces a small bias in the estimate. A larger problem is that this method is very time-consuming, since running the model to apparent convergence generally requires two full days of computer time. Furthermore, it does not offer a quick way of obtaining a rough estimate from just a few Frazier, Powell and Simão iterations. The ability to obtain rough estimates early is particularly important to calibration because it allows us to focus subsequent search effort on those input vectors that are likely to calibrate well.</p><p>We have designed a method that provides rough estimates early and at the same time provides later estimates that are at least as accurate as post-apparent-convergence averages. It operates by observing the currently available output, which may be from a relatively small number of iterations, estimates how quickly the output is converging to G k (ρ), and then uses this knowledge of the convergence rate to estimate G k (ρ).</p><p>Our method uses the following statistical model for output k as a function of the iteration n:</p><formula xml:id="formula_6">Y n k (ρ) = B k (ρ) + [G k (ρ) − B k (ρ)] [1 − exp (−nR k (ρ))] + ε n , n &gt; n 0 .<label>(3)</label></formula><p>This model contains a number of newly defined quantities: Y n k (ρ) is the observation from the ADP model of output type k at iteration n when the parameter vector is ρ; G k (ρ) was defined earlier and is the limiting value to which this output converges; R k (ρ) is the rate at which the output converges to its limiting value; n 0 is a threshold usually set to 10 that allows the model to ignore the erratic initial output we see in <ref type="figure" target="#fig_3">Figure 4</ref>.1 and in general; B k (ρ) is a parameter of the model that would be the expected value of the output at the first iteration if not for the erratic initial behavior; and ε n is an independent unbiased normal random variable with variance σ 2 ε (which is also new notation) that encapsulates the randomness in the observation process.</p><p>Using this statistical model, we employ a Bayesian analysis to estimate G k (ρ) from the observations</p><formula xml:id="formula_7">Y 1 k (ρ), . . . ,Y n k (ρ).</formula><p>We place a prior distribution on the parameters G k (ρ), R k (ρ), B k (ρ), and σ 2 ε , and then obtain a posterior distribution conditioned on the data. Conditioned on R k (ρ), the prior is the standard noninformative prior for Bayesian linear regression <ref type="bibr" target="#b17">(Gelman, Carlin, Stern, and Rubin 2004)</ref>, which is uniform on G k (ρ), B k (ρ), log σ ε over R × R × R + . The marginal prior on R k (ρ) is concentrated on a fixed set of points R.</p><p>The marginal prior on R k (ρ) was obtained from a smoothed version of the empirical distribution of R k (ρ) calculated using an older dataset. This dataset resulted from Schneider's operations in 2007 and the empirical distribution was obtained by calculating maximum likelihood estimates of R k (ρ) from data outputted by runs of the ADP model for several different values of k and ρ. 200 iterations were generated by each run of the ADP model used, which is large enough to produce a reasonable expectation of accuracy in the maximum likelihood estimates. We then smoothed this empirical distribution by fitting a normal distribution to it, and chose a finite set of points R = {1/100, . . . , 1/2} that covered the main mass of the fitted distribution. We chose to distribute the inverse of the rates uniformly in R because the sensitivity of the model to the rate is most pronounced when R k (ρ) is close to 0, making the difference between rates 1/100 and 1/99 approximately as important as the difference between rates of 1/2 and 1/4. Although obtaining the empirical distribution required a large amount of computer time to run the ADP model for many iterations with different values of ρ, this one-time investment allows us to calibrate the model under any future dataset, including the one from 2008 calibrated in Section 6.</p><p>Given this prior, the mean and variance of the posterior on G k (ρ) is calculated as follows. For each rate r ∈ R, we first condition on</p><formula xml:id="formula_8">R k (ρ) = r. Define G k (ρ, n, r) = E G k (ρ) | (Y j k (ρ)) n j=0 , R k (ρ) = r to be the conditional posterior mean of G k (ρ), σ G k (ρ, n, r) 2 = Var G k (ρ) | (Y j k (ρ)) n j=0</formula><p>, R k (ρ) = r to be the conditional posterior variance, and L r to be the conditional likelihood of (Y l k (ρ)) n j=0 given R k (ρ) = r. Since the model (3) with R k (ρ) = r is linear in the parameters, these quantities may all be computed using techniques from Bayesian linear regression (see, e.g., <ref type="bibr" target="#b17">(Gelman, Carlin, Stern, and Rubin 2004)</ref>).</p><p>Bayes' rule implies that the posterior probability of R k (ρ) being equal to r is given by p r = L r / ∑ r ∈R L r . Using this, we define notation for and compute the conditional mean and variance of G k (ρ) as</p><formula xml:id="formula_9">G k (ρ, n) = E G k (ρ) | (Y j k (ρ)) n j=0 = ∑ r∈R p r G k (ρ, n, r), σ G k (ρ, n) 2 = Var G k (ρ) | (Y j k (ρ)) n j=0 = ∑ r∈R p r σ G k (ρ, n, r) 2 + G k (ρ, n, r) − G k (ρ, n) 2 .</formula><p>The expression for G k (ρ, n) is derived using the tower property, and the expression for σ G k (ρ, n) 2 using the conditional variance formula. <ref type="figure" target="#fig_1">Figure 2(a)</ref> shows the result of this Bayesian analysis on one particular sequence of 50 iterations of the TAH solo company driver metric with a fixed value of ρ. These iterations are the first 50 from figure 4.1. Both the crosses and circles are observations Y n k from the model, but the red crosses were discarded because they occurred before the threshold n 0 . The solid line is the mean of the posterior, G k (ρ, 50), and the distance from the solid line to the dotted lines above and below is twice the standard deviation of the posterior, 2σ G k (ρ, 50). <ref type="figure" target="#fig_1">Figure 2(b)</ref> shows the three different estimates discussed as a function of how many iterations have been observed. The standard deviation of the posterior is also displayed. We see that the Bayesian method produces rough estimates early whose error is well-quantified by the variance of the posterior, and that even after a large number of iterations its estimate is more accurate than the post-apparent-convergence average. In particular, both averages are biased below G k (ρ).</p><p>Later, we will need to predict the posterior variance of G k (ρ) that will result from observing more iterations. Equivalently, we may predict the posterior precision, since the precision is the inverse of the variance. In <ref type="figure" target="#fig_1">Figure 2</ref>(c) we plot the posterior precision of G k (ρ) as a function of the number of iterations observed. We plot this quantity for several different values of ρ for the TAH metric for both solo company drivers and independent contractors. The relationship between posterior precision and the number of iterations n is roughly linear in n, with the posterior precision approximated by 0.2 (n − n 0 ). Thus, after observing n iterations, we then approximate the posterior variance that would result from observing more iterations by</p><formula xml:id="formula_10">σ G k (ρ, n + ) 2 ≈ σ G k (ρ, n, ) 2 = σ G k (ρ, n) −2 + 0.2 −1 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OBSERVING CALIBRATION QUALITY</head><p>In the previous section, we fixed k and ρ and calculated the posterior distribution on G k (ρ) given observations from the ADP model at parameter vector ρ. We continue with fixed ρ and now consider the posterior distribution on</p><formula xml:id="formula_11">F(ρ) = ∑ k (G k (ρ) − g k ) 2</formula><p>given this same set of observations. We later combine this posterior distribution with posterior distributions on F(ρ ) at values of ρ = ρ to obtain a composite posterior distribution on the overall function F. Given the independent posterior distributions on G k (ρ) for k ∈ K computed in Section 4.1, and the definition (1) of F(ρ), we compute and define notation for the mean and variance of F(ρ) as</p><formula xml:id="formula_12">F(ρ, n) = E F(ρ) | (Y j k (ρ)) n j=0 , k ∈ K = ∑ k∈K G k (ρ, n) − g k 2 + σ G k (ρ, n) 2 ,<label>(5)</label></formula><formula xml:id="formula_13">(σ (ρ, n)) 2 = Var F(ρ) | (Y j k (ρ)) n j=0 , k ∈ K ≈ ( σ (ρ, n)) 2 = ∑ k∈K 4 σ G k (ρ, n) 2 G k (ρ, n) − g k 2 + 2 σ G k (ρ, n) 4 .<label>(6)</label></formula><p>To obtain the approximation ( σ (ρ, n)) 2 of the variance, we approximated the iteration-n posterior distribution on G k (ρ) by a normal distribution with mean G k (ρ, n) and variance σ G k (ρ, n) 2 , and then used the definition (1) of F(ρ) together with the moments of the normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frazier, Powell and Simão</head><p>Later, we will need an estimate of σ (ρ, n + ) after having observed only n observations, where is any strictly positive integer. We approximate (σ (ρ, n + )) 2 by</p><formula xml:id="formula_14">(σ (ρ, n + )) 2 ≈ ( σ (ρ, n, )) 2 = ∑ k∈K 4 σ G k (ρ, n, ) 2 (G k (ρ, n) − g k ) 2 + 2 σ G k (ρ, n, ) 4 ,<label>(7)</label></formula><p>where we have used (6) but substituted G k (ρ, n) for G k (ρ, n + ) and σ G k (ρ, n, ) for σ G k (ρ, n + ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE KNOWLEDGE GRADIENT</head><p>Our strategy uses the KG algorithm to determine how to sequence the testing of different values of ρ. This KG method is similar to the KG method for correlated beliefs introduced in <ref type="bibr">(Frazier, Powell, and Dayanik 2009)</ref>, with the difference residing in the ability of the KG algorithm proposed here to use estimates based on transient behavior. We begin in Section 5.1 by discussing how the posteriors calculated for individual values of ρ in Sections 4.2 may be combined to give a cohesive posterior across all values of ρ. Then, in Section 5, we discuss how the decisions of the KG method are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">THE POSTERIOR DISTRIBUTION ON CALIBRATION QUALITY</head><p>In Section 4.2 we saw how a posterior distribution on F(ρ) for a fixed value of ρ, can be obtained from the observations of the model's output at that value of ρ. In this section, we approximate this posterior as normal and treat it as an observation of F(ρ). The value of this so-called observation will be the mean of the posterior F(ρ, n) and the sampling variance with which it will have been assumed to have been taken will be the approximate variance of the posterior ( σ (ρ, n)) 2 . We begin by letting I be a set of discrete parameter vectors, which we obtained for this study by discretizing the time-at-home bonuses for solo company drivers and independent contractors. We would like to represent functions with domain I by vectors in R |I | . Toward this end, we create a mapping i : I → {1, . . . , |I |} that gives each ρ in I a unique index. We then define a vector F ∈ R |I | by letting F i(ρ) = F(ρ) for each ρ ∈ I . This vector F is a discretized version of the function F. We also let e(ρ) be a column vector of 0's with a 1 in component i(ρ).</p><p>We then take a multivariate normal prior on F with a power exponential covariance matrix <ref type="bibr" target="#b12">(Cressie(1993)</ref>). The parameters of the power exponential covariance matrix were tuned to match the number of modes expected in the function F. We define µ 0 to be the mean vector and Σ 0 to be the covariance matrix of this prior. Using this prior, which is a discrete-domain equivalent of a Gaussian process prior often used within spatial statistics (see, e.g., <ref type="bibr" target="#b12">(Cressie(1993)</ref>)), we can combine these observations to get a cohesive posterior on F across all values ρ ∈ I . We index by m the time spent running the ADP model at all values of ρ. Note that this time is different from the time in the stochastic optimization problem being solved, which is indexed by t. For each ρ ∈ I , let n(ρ, m) be the number of ADP iterations we have observed so far by time m at parameter vector ρ. Using the KG algorithm (described in Section 5.2), we choose from which ρ ∈ I to obtain ADP iterations. We give the name ρ m to the parameter vector chosen. We then obtain additional iterations from the model at ρ m , noting that n(ρ m , m) = 0 if we have not yet measured ρ by time m. We have n(ρ m , m + 1) = n(ρ m , m) + , and n(ρ, m + 1) = n(ρ, m) for ρ = ρ m . In this study, we set to 25 iterations, which takes about 12 hours of computer time, , which is large enough to amortize the cost of computing the KG decision over a reasonably large number of iterations, but small enough to retain the benefits of the algorithm's sequential nature.</p><p>Let I m = {ρ ∈ I : n(ρ, m) &gt; 0} be the set of unique parameter vectors that we have chosen to observe by time m. Then, defining notation F m (ρ) = F(ρ, n(ρ, m)) and σ m (ρ) = σ (ρ, n(ρ, m)), we introduce the following approximation. In this approximation, the mean used is the mean of the actual posterior, while the variance used is an estimate.</p><formula xml:id="formula_15">Approximation 1. The posterior F(ρ) Y j k (ρ), j ≤ n(ρ, m), k ∈ K is normal with mean F m (ρ) and variance ( σ m (ρ)) 2 .</formula><p>Under Approximation 1, from (Gelman, Carlin, Stern, and Rubin 2004), we have that the posterior on F given all of the observations so far is multivariate normal with mean vector µ m and covariance matrix Σ m , where these can be computed as</p><formula xml:id="formula_16">µ m = Σ m (Σ 0 ) −1 µ 0 + ∑ ρ∈I m ( σ m (ρ)) −2 F m (ρ)e(ρ) , Σ m = (Σ 0 ) −1 + ∑ ρ∈I m ( σ m (ρ)) −2 e(ρ)e(ρ) −1 .<label>(8)</label></formula><p>Frazier, Powell and Simão</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">THE KNOWLEDGE GRADIENT POLICY</head><p>The KG policy is a rule for choosing the next input vector ρ m with which to run the ADP model given the observations up to the current time m. This rule is</p><formula xml:id="formula_17">ρ m ∈ arg max ρ∈I ν KG ρ (S m ),</formula><p>where S m = (µ m , Σ m ) is called the current state of knowledge and encapsulates all we need to know from the observations so far, and ν KG ρ (S m ) is the value of collecting more iterations from the ADP model with input parameter vector ρ given the current state of knowledge. ν KG ρ (S m ) is called the knowledge-gradient (KG) factor at ρ, and is defined by</p><formula xml:id="formula_18">ν KG ρ (S m ) = E m max j (−µ m+1 j )|ρ m = ρ − max j (−µ m j ).<label>(9)</label></formula><p>We see from this definition that the KG factor is the expected difference between the value of the best option that we can select given the current state of knowledge, which is max j (−µ m j ), and the value of the best option that we can select given the new knowledge that would result from observing more iterations at ρ, which is max j (−µ m+1 j ).</p><p>To compute or approximate the KG factor, we must must first determine the time-m conditional distribution of µ m+1 . Toward this end, we formalize two approximations and then state a proposition that uses them.</p><formula xml:id="formula_19">Approximation 2. σ m+1 (ρ m ) is equal to its time-m estimate σ m,1 (ρ m ) = σ (ρ m , n(ρ m , m), ). Approximation 3. The time-m conditional distribution of F m+1 (ρ m ) is normal.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1. Under Approximations 1 and 2, the conditional variance Var m µ m+1</head><p>is given by</p><formula xml:id="formula_20">σ m (ρ m ) σ m (ρ m ) , where σ m (ρ m ) = λ m + ( σ m (ρ)) 2 λ m + e(ρ) Σ m e(ρ) [Σ m e(ρ)] , λ m = σ m,1 (ρ m ) 2 −1 − σ m (ρ m ) −2 −1 .</formula><p>If we additionally assume Approximation 3, then the time-m conditional distribution of µ m+1 is multivariate normal with mean vector µ m and covariance matrix σ m (ρ m ) σ m (ρ m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>We begin by noting that (8) implies</p><formula xml:id="formula_21">µ m+1 = Σ m+1   (Σ 0 ) −1 µ 0 + ∑ ρ∈I m+1 ( σ m+1 (ρ)) −2 F m+1 (ρ)e(ρ)   .<label>(10)</label></formula><p>To calculate the conditional variance, we note that F m (ρ) = F m+1 (ρ) and σ m (ρ) = σ m+1 (ρ) for all ρ = ρ m , and thus all the terms except the ρ m term in the sum from (10) do not affect the variance. Thus,</p><formula xml:id="formula_22">Var m µ m+1 = Var m Σ m+1 ( σ m+1 (ρ m )) −2 F m+1 (ρ m )e(ρ m ) .</formula><p>Under Approximation 2, σ m+1 (ρ m ) = σ m,1 (ρ m ) is known at time m, and thus so is Σ m+1 . Thus we can square them and move them outside the variance operator, providing the expression,</p><formula xml:id="formula_23">Var m µ m+1 = Σ m+1 e(ρ m ) Σ m+1 e(ρ m ) ( σ m,1 (ρ m )) −4 Var m F m+1 (ρ m ) .<label>(11)</label></formula><p>Frazier, Powell and Simão</p><p>We first consider the term (</p><formula xml:id="formula_24">σ m,1 (ρ m )) −4 Var m F m+1 (ρ m )</formula><p>. The conditional variance formula and Approximation 2 imply</p><formula xml:id="formula_25">Var m F m+1 = ( σ m (ρ m )) 2 − ( σ m,1 (ρ m )) 2 . Substituting (λ m ) −1 + ( σ m (ρ m )) −2 for ( σ m,1 (ρ m )) −2 and simplifying implies ( σ m,1 (ρ m )) −4 Var m F m+1 (ρ m ) = (λ m ) −1 1 + (λ m ) −1 ( σ m (ρ m )) 2 .</formula><p>We now consider the term Σ m+1 e(ρ m ) in (11). From (8), Approximation 2, and the Sherman-Morrison-Woodbury matrix identity ( <ref type="bibr" target="#b18">Golub and Loan 1996)</ref>, we have,</p><formula xml:id="formula_26">Σ m+1 = (Σ m ) −1 + ( σ m,1 (ρ m )) −2 − ( σ m (ρ m )) −2 e(ρ m )e(ρ m ) −1 = (Σ m ) −1 + (λ m ) −1 e(ρ m )e(ρ m ) −1 = Σ m − λ m + e(ρ m ) Σ m e(ρ m ) −1 [Σ m e(ρ m )] [Σ m e(ρ m )] ,</formula><p>where we understand ( σ m (ρ m )) −2 to be equal to 0 if we have not measured ρ m before time m. Multiplying by e(ρ m ),</p><formula xml:id="formula_27">Σ m+1 e(ρ m ) = [Σ m e(ρ m )] 1 − e(ρ m ) Σ m e(ρ m ) λ m + e(ρ m ) Σ m e(ρ m ) = Σ m e(ρ m ) λ m λ m + e(ρ m ) Σ m e(ρ m ) .</formula><p>Finally, combining these two expressions back into <ref type="formula" target="#formula_0">(11)</ref>, we have</p><formula xml:id="formula_28">Var m µ m+1 = [Σ m e(ρ m )] [Σ m e(ρ m )] λ m + ( σ m (ρ)) 2 (λ m + e(ρ) Σ m e(ρ)) 2 = σ m (ρ m ) σ m (ρ m ) .</formula><p>This shows the statement about Var m µ m+1 . To show the statement about the conditional normality of µ m+1 , we assume Approximation 3 and note that (10) implies µ m+1 is an affine function of a time-m conditionally normal random vector F m+1 , and is thus itself conditionally multivariate normal.</p><p>By Proposition 1, if we let Z m+1 be an independent scalar normal random variable, µ m+1 is approximately equal in distribution to µ m + σ m (ρ m )Z m+1 . Thus the KG factor may be approximated by</p><formula xml:id="formula_29">ν KG ρ (S m ) ≈ E m max ρ∈I (−µ m i(ρ) ) + σ m (ρ m )Z m+1 | ρ m = ρ = h(−µ m , σ m (ρ)),<label>(12)</label></formula><p>where the function h is defined by h(a, b) = E max j a j + b j Z m+1 for generic |I |-dimensional vectors a and b. <ref type="bibr">(Frazier, Powell, and Dayanik 2009)</ref> gives a method for computing h(a, b). We state the method for the special case in which, for each j, there is a strictly positive probability that arg max j a j + b j Z m+1 is uniquely equal to j. The more general computation is performed by first reducing a and b to smaller vectors with this property, and for further details we refer to <ref type="bibr">(Frazier, Powell, and Dayanik 2009)</ref>. To compute h(a, b) in the special case, we first sort the indices of the vectors a and b so that b 1 &lt; b 2 &lt; . . . and the lines defined by z → a j + b j z are sequenced in order of increasing slope. Then, the two lines a j + b j z and a j+1 + b j+1 z intersect when z is equal to c j = a j − a j+1 b j+1 − b j . We then compute (12) using</p><formula xml:id="formula_30">h(a, b) = |I | ∑ j=1 a j Φ(c j ) − Φ(c j−1 ) + b j ϕ(c j−1 ) − ϕ(c j ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>We demonstrate the performance of the KG algorithm by using it to calibrate the ADP model using a dataset recorded from Schneider's operations during 2008. We calibrated two of the more sensitive calibration parameters, which were the bonuses awarded within the model for routing two different types of drivers to their homes. These driver types were solo company drivers and independent contractors. The progress of the algorithm is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, with times m = 3, 4, 5, 6 in the rows from top to bottom, and the columns representing the mean of our posterior belief on fit deviation, µ m i(ρ) (left column), the standard deviation of this belief, Σ i(ρ),i(ρ) m (center column), and the KG factor ν m ρ (right column). Each of the twelve contour plots in this figure</p><p>Frazier, Powell and Simão put the bonus awarded to solo company drivers along the horizontal axis labeled "SOLO bonus", and the bonus awarded to independent contractors along the vertical axis labeled "IC bonus". Previous measurements are marked with a blue circle, while the next measurement to take, which is also the maximum of the KG factors, is marked with a red circle in the KG factor plots. In the mean plots, the point with the smallest mean is marked with a star. We see in these plots that the KG policy strikes a balance between measuring points that have good means, since these are the points about which it is most valuable to have accurate estimates, and measuring points with large standard deviations, since these are the points for which are current estimates are poor. In <ref type="figure" target="#fig_3">Figure 4</ref>, which plots log 10 min i µ m i as a function of m, we see the progress of the algorithm over time. After only three iterations our estimated fit deviation is less than 10 −1.5 , which compares well with the calibration quality that Schneider is able to achieve when calibrating by hand, and would be acceptable according to current practice. After six iterations, estimated fit deviation is close to 10 −2 , and we would likely stop the calibration. These six iterations took 3 days of computer time, compared to the 1 to 2 weeks of computer and employee time that calibration requires when done by hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>The major outcome of this work is the automation of a task that previously used ad hoc judgment and significant amounts of time from an analyst. In the tests performed, the KG method calibrated the model in approximately 1.5 days, compared to 7 − 14 days when calibrated by hand. This automation decreases cost and improves speed while maintaining quality.</p><p>The power of the KG algorithm in this setting arises from the use of the correlation structure in the beliefs about the quality of different calibration vectors. One significant limitation of the procedure is the need to discretize the parameter space. If the parameter space in Section 6 had possessed a larger number of dimensions, say 10 instead of 2, the size of the discretized parameter space would have been so large that computing the KG factors using the techniques described would have been computationally infeasible. We are currently working on a version of the KG policy that can handle continuous parameters without the need to discretize. This approach may alleviate this problem. Despite this limitation, the KG policy promises to improve the efficiency of calibration, and make it possible to easily calibrate expensive ADP models. , and the right column shows the KG factor, ν KG ρ (S m ). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frazier, Powell and Simão</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample path showing of the time-at-home (TAH) metric for solo company drivers over 200 ADP training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Figure 2(a) shows the posterior mean and standard deviation resulting from a Bayesian analysis of the first 50 of the iterations in Figure 4.1. Figure 2(b) displays estimates from the three different estimation methods discussed in the text as a function of how many iterations the method observes. In Figure 2(c), the solid lines show precision of the posterior on G k (ρ) as a function of the number of iterations for several different values of k and ρ. The dotted line shows a linear fit of 0.2 (n − n 0 ), where n 0 = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: From top to bottom, the rows show times m = 3, 4, 5, 6. The left column shows the mean µ m i(ρ) of our belief on F(ρ), the central column shows the standard deviation Σ m i(ρ),i(ρ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Logarithm of estimated fit deviation at the best-fitting ρ discovered so far, min i µ m i , as a function of m.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Design and analysis of experiments for statistical selection, screening and multiple comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bechhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldsman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>J.Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stochastic learning and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An effective approach to smartly allocate computing budget for discrete event simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2598" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A gradient approach for smartly allocating computing budget for discrete event simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation Conference Proceedings</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="398" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal computing budget allocation for Monte Carlo simulation with application to product design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation Modeling Practice and Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="74" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal computing budget allocation for Monte Carlo simulation with application to product design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Donohue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation Modeling Practice and Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="74" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simulation Budget Allocation for Further Enhancing the Efficiency of Ordinal Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Event Dynamic Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="270" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing budget allocation for simulation experiments with different system structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th conference on Winter simulation</title>
		<meeting>the 30th conference on Winter simulation<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computing efforts allocation for ordinal optimization and discrete event simulation. Automatic Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yucesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="960" to="964" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">New development of optimal computing budget allocation for discrete event simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th conference on Winter simulation</title>
		<meeting>the 29th conference on Winter simulation</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="334" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">New myopic sequential sampling procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Submitted to INFORMS J. on Computing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">New two-stage and sequential procedures for selecting the best simulated system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="732" to="743" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A C</forename><surname>Cressie</surname></persName>
		</author>
		<title level="m">Statistics for Spatial Data</title>
		<imprint>
			<publisher>Powell and Simão</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page">605</biblScope>
		</imprint>
	</monogr>
	<note>revised edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The knowledge-gradient policy for correlated normal rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dayanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informs J. on Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A knowledge-gradient policy for sequential information collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dayanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2410" to="2439" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal computing budget allocation under correlated sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th conference on Winter simulation</title>
		<meeting>the 36th conference on Winter simulation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simulation Optimization: A Review, New Developments, and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Simulation Conference</title>
		<imprint>
			<date type="published" when="2005-04" />
			<biblScope unit="page" from="83" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F V</forename><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>John Hopkins University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian look ahead one-stage sampling allocations for selection of the best population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miescke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="244" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Opportunity cost and OCBA selection procedures in ordinal optimization for a fixed number of alternative systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man and Cybernetics Part C-Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="951" to="961" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic estimation of the maximum of a regression function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="462" to="466" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stochastic approximation and recursive algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<editor>Springer. Montgomery, D., and D. Montgomery</editor>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Design and analysis of experiments</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Response surface methodology: Process and product optimization using designed experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Math. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An approximate dynamic programming algorithm for large-scale fleet management: A case application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Simão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nienow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Introduction to stochastic search and optimization: Estimation, simulation and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discrete-event simulation optimization using ranking, selection, and multiple comparison procedures: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yücesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Modeling and Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="154" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>TOMACS)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">AUTHOR BIOGRAPHIES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">He then begins an appointment as Assistant Professor in the department of Operations Research and Information Engineering at Cornell University. His research interest is in the optimal acquisition of information, with applications in simulation, medicine and operations management</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>PETER FRAZIER will receive a Ph.D. in Operations Research &amp; Financial Engineering from Princeton University in</orgName>
		</respStmt>
	</monogr>
	<note>His web address is &lt;www.princeton.edu/˜pfrazier&gt; and his email address is &lt;pfrazier@princeton.edu&gt;</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An Informs Fellow, he has coauthored over 100 refereed publications in stochastic optimization, stochastic resource allocation and related applications. He is the author of Approximate Dynamic Programming, and is currently involved in applications in energy, transportation, finance and homeland security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>POWELL is a professor in the department of Operations Research and Financial Engineering at Princeton University, and director of CASTLE Laboratory</orgName>
		</respStmt>
	</monogr>
	<note>&lt;www.castlelab.princeton.edu&gt;. His email address is &lt;powell@princeton.edu&gt;</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">He has worked on planning systems in the areas of transportation, distribution, logistics, and supply-chain. He has also taught courses on computer programming, basic probability and statistics, and resource and information management at the School of Engineering in Princeton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">P</forename><surname>Sim˜aosim˜</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sim˜ao</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>CASTLE Laboratory, in Princeton University. He holds a Ph.D. in Civil Engineering and Operations Research from Princeton University</orgName>
		</respStmt>
	</monogr>
	<note>His e-mail address is &lt;hpsimao@princeton.edu&gt;</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
