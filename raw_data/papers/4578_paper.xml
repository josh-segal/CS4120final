<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Co-evaluation Framework for Improving Segmentation Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Washington University</orgName>
								<address>
									<addrLine>One Brookings Drive</addrLine>
									<postCode>63130</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">E</forename><surname>Fritts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Washington University</orgName>
								<address>
									<addrLine>One Brookings Drive</addrLine>
									<postCode>63130</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Goldman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Washington University</orgName>
								<address>
									<addrLine>One Brookings Drive</addrLine>
									<postCode>63130</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Co-evaluation Framework for Improving Segmentation Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image segmentation</term>
					<term>effectiveness measure</term>
					<term>machine learning</term>
					<term>segmentation evaluation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Object segmentation is an important preprocessing step for many target recognition applications. Many seg-mentation methods have been studied, but there is still no satisfactory effectiveness measure which makes it hard to compare different segmentation methods, or even different parameterizations of a single method. A good segmentation evaluation method not only would enable different approaches to be compared, but could also be integrated within the target recognition system to adaptively select the appropriate granularity of the segmentation which in turn could improve the recognition accuracy. A few stand-alone effectiveness measures have been proposed, but these measures examine different fundamental criteria of the objects, or examine the same criteria in a different fashion, so they usually work well in some cases, but poorly in the others. We propose a co-evaluation framework , in which different effectiveness measures judge the performance of the segmentation in different ways, and their measures are combined by using a machine learning approach which coalesces the results. Experimental results demonstrate that our method performs better than the existing methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Object segmentation is a fundamental step in many target recognition systems. A good segmentation of the object(s) from an optical or radar image will greatly increase the recognition accuracy. Many segmentation methods have been studied, but there is still no satisfactory effectiveness measure which makes it hard to compare different segmentation methods, or even different parameterizations of a single method. Furthermore, many segmentation methods have a tunable parameter which adjust the granularity of the segmentation and having an effectiveness measure to select which segmentation is best would improve the overall performance of the recognition system.</p><p>Designing a good effectiveness measure of object segmentation is known hard problem. Often the segmentation effectiveness is judged by the overall performance of the recognition system, however such an approach would not enable system to use the segmentation effectiveness measure internally to help choose the best parameterization for the segmentation algorithm. A few stand-alone effectiveness measures have also been proposed. These measures examine different fundamental criteria of the objects or examine the same criteria in a different fashion, so they usually work well in some cases, but poorly in the others. We propose a co-evaluation framework , in which different effectiveness measures judge the performance of the segmentation, and then their measures are combined by a learning algorithm that uses training data to determine how to best coalesce the results from the constituent measures. Our approach can be easily extended to a multi-spectral system if the effectiveness measures are using images in different spectra.</p><p>The remainder of the paper is organized as follows. In Section 2, we provide an overview of the previous research on objective segmentation evaluation. Section 3 describes our co-evaluation method. Experimental results and analysis are presented in Section 4, and Section 5 concludes the paper and discusses future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SEGMENTATION EVALUATION METHODS</head><p>A variety of techniques have been proposed for quantitatively evaluating segmentation methods that can be roughly categorized into three classes 1 : analytic methods, empirical goodness methods, and empirical discrepancy methods.</p><p>Analytic methods assess segmentation algorithms independently of their output, evaluating their effectiveness entirely based on their properties and principles. In general, these methods work only for evaluating certain properties of segmentation algorithms, such as processing strategy (parallel, sequential, iterative, or mixed), processing complexity and efficiency, and segmentation resolution. These properties are usually not decisive for the performance difference between segmentation algorithms. Consequently, analytical methods are generally not good methods for evaluating the object segmentation in target recognition systems.</p><p>Empirical discrepancy methods, also known as relative or supervised evaluation methods, are those methods which evaluate segmentation algorithms by comparing the resulting segmented image against a manuallysegmented reference image, which is often referred to as a gold standard. <ref type="bibr" target="#b1">2</ref> A benefit of empirical discrepancy methods over empirical goodness methods is that the direct comparison between a segmented image and a reference image is believed to provide a finer resolution of evaluation, and as such, discrepancy methods are the most commonly used method of objective evaluation. However, manually generating a reference image is a difficult, subjective, and time-consuming job. And for most images, especially natural images, we generally cannot guarantee that one manually-generated segmentation image is better than another. Furthermore, such an evaluation method cannot be used within a segmentation algorithm to select the best parameters to use within a parameterized segmentation algorithm.</p><p>Empirical goodness methods, also known as stand-alone or unsupervised evaluation methods quantitativally evaluate the results of segmentation algorithms according to some human characterization about the properties of "ideal" segmentation. The benefit of these methods is that they do not require a priori knowledge of the correct segmentation (i.e. they do not need to be assessed versus manually-segmented reference images). The fundamental ideas as to what constitutes these characteristics includes intra-region uniformity, 3-8 inter-region contrast, 4, 9 region shape, 5 and edge evaluation. <ref type="bibr" target="#b9">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Previous Work on Empirical Goodness Methods</head><p>Many of the early methods in this area focused on the the evaluation of foreground-background segmentation methods 3-5, 9 and are unsuitable for multi-region segmented images and so we do not consider them here.</p><p>In describing the evaluation methods considered here, we use the following notation. Let I be the image and let S I be the area (as measured by the number of pixels) in I. We define a segmentation as a division of I into N arbitrarily shaped (and possibly non-contiguous) regions. We use R j to denote the set of pixels in region j, and use S j = |R j | to denote the area of region j. For feature x (e.g. x might be the red, green, or blue value) and pixel p, we use C x (p) to denote the value of feature x for pixel p. We define the average value of feature x in region j by ¯</p><formula xml:id="formula_0">C x (R j ) = p∈Rj C x (p) /S j . The squared color error of region j is defined as e 2 j = x∈{r,g,b} p∈Rj (C x (p) − ¯ C x (R j )) 2 .</formula><p>We use N (a) to denote the number of regions in the segmented image having an area of exactly a, and MaxArea to denote the area of the largest region in the segmented image.</p><p>Liu and Yang 7 proposed the evaluation function</p><formula xml:id="formula_1">F (I) = √ N N j=1 e 2 j √ Sj .</formula><p>Unless the image has very well defined regions with very little variation in luminance and chrominance, the F evaluation function has a very strong bias towards segmentations with very few regions. Only in images where segmentation will result in very uniform regions, will the F evaluation function prefer segmentations with many regions.</p><p>Borsotti et al. <ref type="bibr" target="#b7">8</ref> proposed the following variation upon Liu and Yang's method to address the problems discussed above,</p><formula xml:id="formula_2">F (I) = 1 1000 · S I SQRT M axArea a=1 [N (a)] 1+1/a N j=1 e 2 j 񮽙 S j .</formula><p>Observe that if the segmentation has lots of regions consisting of only one pixel then the multiplicative factor that precedes the summation will be O(N 2 )/S I which is a much larger penalty than the √ N that occurs in F . Thus F will correctly evaluate segmentations with lots of regions as very poor (unless all color errors are 0) whereas F will incorrectly rank them as good segmentations.</p><p>Both F and F reach their minimum value of zero on an image in which each region is its own pixel. This is not a big problem since one should never consider allowing the number of regions in the segmentation to be as large as the area of the image. However, a more serious problem is both F and F highly penalize segmentations with a large number of regions and only when the squared error in all regions gets very small will a segmentation with more than a few regions be evaluated as best. Thus, Borsotti et al. further refine F and F to obtain the evaluation function * ,</p><formula xml:id="formula_3">Q(I) = 1 1000 · S I √ N N j=1 e 2 j 1 + log S j + N (S j ) S j 2</formula><p>. Again √ N is used to penalize segmentations that have a lot of regions. However, the influence that the √ N has is greatly reduced by dividing the squared color error by 1 + log S j which causes the squared color error to have a much bigger influence in Q as compared to its influence in both F and F . As an effect of this change, regions with large area that are not uniform in color are penalized even more in Q than in F and F , and so Q has a very strong bias against regions with large area unless there is very little variation in color. Finally, the second term in the summation in the definition of Q adds a small bias against having lots of regions with the same area. However, this term typically has a very small value as compared to the first term in the summation, and so has negligible effect on the evaluation. The only exception to this fact is when N (S j ) gets large for some region j which can only occur when N , the number of regions, is very large.</p><p>More recently, Zhang, Fritts and Goldman 11 proposed an information theoretic approach to segmentation evaluation function E based on entropy and the minimum description length principle (MDL). Given a segmented image, they define V j as the set of all possible values for the luminance in region j and let L j (m) denote the number of pixels in region j that have luminance of m in the original image. The entropy for region j</p><formula xml:id="formula_4">is defined as H(R j ) = − m∈Vj L j (m) S j log 2 L j (m) S j .</formula><p>They next define the expected region entropy of image I,</p><formula xml:id="formula_5">H r (I) = N j=1 S j S I H(R j )</formula><p>, which is simply the expected entropy across all regions where each regions has weight (or probability) proportional to its area. The expected region entropy serves in a similar capacity to the term involving the squared color error used in F , F , and Q -it is used as a measure of the uniformity within the regions of I. Since an over-segmented image will have a very small expected region entropy, just as done for F , F , and Q, the the expected region entropy must be combined with another term or factor that penalizes segmentations having a large numbers of regions since there would otherwise be a strong bias to over-segment an image. Instead of multiplying the expected region entropy by √ N , they instead introduce the layout entropy</p><formula xml:id="formula_6">H (I) = − N j=1 S j S I log 2 S j S I</formula><p>and define their evaluation measure E = H (I) + H r (I). An alternate view of their evaluation method is obtained by applying the minimum description length (MDL) principle 12 to balance the trade-off between the uniformity of the individual regions with the complexity of the segmentation.</p><p>The final two evaluation measures we consider, V s and V m , are based on the metrics proposed in Correia and Pereira's paper. 13 These metrics are proposed for video segmentation quality measures. We convert these measures to image segmentation quality measures by removing the motion and temporal related portions. For regions in an image, there are per-region metrics and inter-region metrics. Per-region metrics are provided by circularity and elongation (circ elong), and compactness (compact) † . An inter-region metric is provided by</p><formula xml:id="formula_7">contrast = 1 4 · 255 · N b · i,j (2 · max(DY i,j ) + max(DU i,j ) + max(DV i,j ))</formula><p>where N b is the number of border pixels for the region, and DY i,j , DU i,j and DV i,j are the differences between the Y , U and V components of an region's border pixel, respectively, and its four neighbors. For overall segmentation quality evaluation purposes, the relevance of an region must be evaluated taking into account the context where it is found. The contextual relevance metric reflects the importance of an region in terms of the human visual system (HVS), and can be computed by the combination of a set of metrics expressing the features able to capture the viewer's attention. The relevance, Relevance i for region i, is then defined by <ref type="bibr">Correia and Pereira. 14</ref> For the whole segmented image, the per-region and the inter-region metrics for each region are weighted by their relevance. V s and V m are defined as: For all evaluation functions discussed in this paper other than V s and V m , a lower value indicates a better segmentation, whereas a higher V s or V m value means the segmentation is preferred.</p><formula xml:id="formula_8">V k = N i=1 (Relevance i * (W per k * (circ elong i + compact i ) + W inter k * contrast i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CO-EVALUATION FRAMEWORK FOR SEGMENTATION EVALUATION</head><p>Since different stand-alone evaluation methods make their judgments in different ways, giving diverse results on the same segmentation method, we can combine these evaluation methods by applying a learner which determines how to coalesce the results from the constitute evaluation methods. In our approaches, we view the individual evaluation methods as black boxes which enable an evaluation method to be used with little or no modification. These inter-changeable boxes make the whole evaluation system less biased, and less dependent on segmentation algorithms and the content of images. Moreover, since these boxes can work in parallel, the time increase for improved evaluation accuracy is acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different evaluation methods measure the discrepancies of different image features, and give different scores.</head><p>To incorporate these methods into a single system, we need to define a uniform representation into which different evaluators can translate their results. Notice that even a human evaluator cannot tell the absolute goodness of a segmentation result. A human evaluator can say a segmentation result is good or not based on how close the regions is to real-world objects. But how good is the segmentation? It becomes not-so-good when a better segmentation appears. Hence, a natural way of doing evaluation is to compare two segmentation results and indicate which is better.</p><p>An alternate approach would be to convert the scores of each evaluation method to an integer quality score, for example, chosen from 1 to 10. So here instead of comparing two segmented images (based on the original image), a single segmented image is rated relative to the original image. However, to train our co-evaluation method such an approach would require humans to score some training data in the same manner, but human evaluators cannot give a quality score to a segmented image without comparing it with other segmentation results from the same original image, except those results are really good or really bad. After referring to a better segmentation result, a human evaluator tends to give poor score to the current result, vice versa. Narrowing the score range to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> may make training set easier to generate, but the same problem still exists. Taking these into consideration, our system aims to just determine which of two segmentations is best. We can then use this to rank any number of segmented images if that is the desired task.</p><p>We now introduce our Co-Evaluation framework that is shown in <ref type="figure">Figure 1</ref>. In the remainder of this section, we use the term base evaluators to refer to the existing evaluation methods that we will be combining via our co-evaluation strategy. As discussed above, our task is to determine which of two provided segmentations of a given image is best. Thus, this can be viewed as a binary classification task. To train our methods, we provide a set of training data which are pairs of segmentations along with a label as to which one is better. Although a human is used to provide the label for the training data, the resulting co-evaluator strategy does not need any human intervention and thus could be used in an unsupervised manner within an algorithm to select among possible segmentations.</p><p>We consider four different co-evaluation strategies here. Our first strategy, which simply serves as a baseline, does not make use of any learning (i.e. the training data isn't used) but just directly combines the results from <ref type="figure">Figure 1</ref>. Our Co-Evaluation Framework the base evaluators. Our second strategy uses an ensemble based method of learning how to best combine the base evaluators. Finally, our last two strategies uses a standard supervised learning algorithm to learn which prediction to make where the features provided to the supervised learning algorithm are the predictions for each of the base evaluators.</p><note type="other">Evaluator 1 Evaluator 2 Evaluator n Combiner Original Image Segmented Image 1 Segmented Image 2 Human Evaluation Result (For Training Only)</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) The Simple Combiner Strategy</head><p>As a baseline, we consider the very simply strategy of just predicting along with the majority of the base evaluators. We call this strategy the simple combiner strategy. Observe that no learning is used here.</p><p>2) The Weighted Majority (WM) Combiner Strategy Another approach we consider uses an on-line learning approach that focuses on combining the opinions of experts. In particular, we use the weighted majority (WM) algorithm of Littlestone and Warmuth 15 where each base evaluator serves as an expert. In fact since some of the evaluators have accuracy less than 50%, for each base evaluator we introduce two experts, one predicting as the base evaluator, and one predicting opposite of the base evaluator. Associated with each expert is a weight that gives WM's confidence in the accuracy of that expert. Initially, the weight for each expert is 1. During the training phase, the correct answer of which segmentation is better is provided. If the combined prediction was wrong, then every expert that gave the wrong prediction has its weight multiplied by a constant β for 0 ≤ β &lt; 1. Then in the evaluation phase, in order to predict which of two segmentations is best, the weighted majority algorithm predicts according to a weighted vote among the experts. In other words, if the sum of the weights of experts that predict segmentation 1 is better is greater than the weights of experts that predict segmentation 2 is better, then our algorithm selects segmentation 1 as the better segmentation. By selecting β &gt; 0 this algorithm is robust against noise in the data.</p><p>There are many nice features of the WM algorithm including provably bounds on the number of prediction mistakes made under many different circumstances. More recently, several variants of the WM algorithm have been described that also adaptively adjust β to optimize performance. <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> 3) The Bayesian Combiner Strategy An alternate to an ensemble-style method is to use a standard supervised learning algorithm. Our last two strategies use this approach. In the Bayesian combiner strategy we use the Naive Bayes (NB) algorithm to create our final evaluation method. NB makes the assumption that each of the base evaluators are conditionally independent of each other in terms of whether or not their prediction is correct. (We assume that image 1 and image 2 are arbitrarily names and thus it is equally like that each is best). For each evaluator b, the provided training data is used to compute probability image 1 is best given b predicts that image 1 is best and the probability image 1 is best given b predicts that image 2 is best. Once the combiner is trained, these probabilities are then combined using Bayes theorem to produce a probability that image 1 versus image 2 is best and the one with the higher probability is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) The SVM Combiner Strategy</head><p>Based on the structural risk minimization, a Support Vector Machine (SVM) <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> finds a separator that maximizes the separation between the two classes have demonstrated good performances in pattern recognition area, such as handwritten digit recognition, face detection, etc. Because of SVM's excellent classification ability on small dataset, we consider using it as our combiner. In these experiments we used SVM light20, 21 with a linear kernel which creates a linear separator that maximizes the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Methodology</head><p>Different images are involved in different object recognition systems. Especially for military and security purposes where recognition accuracy is very important, various images providing as much information as possible under different conditions are employed. These systems might use totally different mechanisms of imaging. Consequently, those images in recognition system could be optical images, infra-red images, radar images, etc, or the combination of some of them.</p><p>Despite the diversity of images types in object recognition system, in our experiments we can still use optical images in visible light as test images. Those images are representative for several reasons. First of all, for military or security applications, there are a large portion of object recognition systems working on optical images, such as in video surveillance system, or in daytime vehicle identification system. Most importantly, in our proposed co-evaluation framework, only the base evaluators are related to the specific image form. Hence the effectiveness of our method is actually independent of image types. All that is needed are a set of base evaluators for the give image form. In fact, the structure of our framework even enables us to use different types of evaluators to judge images of the same object captured by different imaging technologies. We can evaluate different types of images collaboratively to improve accuracy, which is hard, if not impossible, for many old evaluation methods. By using images in different spectra, it can be easily extended to a multi-spectral system. For example, we can use both infra-red and visible light image to cooperatively identify a tank on a battlefield. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Results and Analysis</head><p>We design two groups of experiments to show the performance of our co-evaluation strategies. They differ in the nature of input segmented images, and consequently yield diverse performances of the base evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Experiment One: human segmentation results vs. machine segmentation results</head><p>In experiment one, the test images are first segmented manually by humans. For each of the segmentation, we generate a machine segmentation with the same number of segments, using the Edge Detection and Image Segmentation (EDISON) System, 22 which is a low-level feature extraction tool that integrates confidence-based edge detection and mean shift-based image segmentation. We make sure for each image, human segmentation looks clearly better than machine segmentation. The test images are partially from the Berkeley Segmentation Dataset, <ref type="bibr" target="#b22">23</ref> and the others are the aircraft images from Military Graphics Collection. <ref type="bibr" target="#b23">24</ref> Each human segmentation is paired with the machine segmentation with the same number of segments. There are 199 pairs of segments in our experiments. We randomly choose 108 pairs as our training set for the combiner using a machine learning method, and use the remaining 91 images as our evaluation set.</p><p>Some exemplar images from our evaluation sets are show in <ref type="figure" target="#fig_3">Figure 2</ref>. The images in the leftmost column are the original images, those in the middle column are human segmentations, and the rightmost column shows machine segmentations. All segmentations have 2 segments (regions). Obviously, the human segmentation is better than machine segmentation in each pair by a human's judgment. If we use " √ " to denote that the evaluation method successfully identifies the human segmentation is the better one in each pair, and use "×" otherwise, the results from the five evaluators can be shown in <ref type="table" target="#tab_0">Table 1</ref>. E works effectively for "Aircraft 1", "Aircraft 2" and "Aircraft 3", V s and V m work for "Aircraft 3" only, and F and Q are wrong for all three cases. The final outputs of the evaluation system, using the simple combiner, the weighted majority (WM) combiner, the Naive Bayesian (NB) combiner and the SVM combiner, respectively, where each machine learning combiner is trained beforehand using 108 pairs of segmentations, are shown in <ref type="table" target="#tab_1">Table 2</ref>. While the simple combiner does not work so well and only correctly identifies the better one for "Aircraft 3", the WM, NB and SVM combiner all evaluate successfully for all three pairs of segmentations. Experimental results (shown in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table">Table 4</ref>) demonstrate that our co-evaluation yields better predictions than any of the base evaluators. For all 91 pairs of segmentations in the evaluation set, the effectiveness of each evaluator is shown in <ref type="table" target="#tab_2">Table 3</ref>. The effectiveness is described by Accuracy, which is defined as the number of times when the evaluating method correctly labeling human segmentation as the better one in the pair, divided by the total number of pairs in evaluation process (i.e. 91 here).</p><formula xml:id="formula_9">E F Q V s V m Aircraft 1 √ × × × × Aircraft 2 √ × × × × Aircraft 3 √ × × √ √</formula><formula xml:id="formula_10">simple WM NB SVM Aircraft 1 × √ √ √ Aircraft 2 × √ √ √ Aircraft 3 √ √ √ √</formula><formula xml:id="formula_11">E F Q V s V m</formula><p>Accuracy 83.52% 13.20% 5.49% 1.10% 1.10%  <ref type="table" target="#tab_2">Table 3</ref> demonstrates that different evaluation methods are not equally effective. While E shows human segmentation is better than machine segmentation in each pair correctly 83.52% of the time, F and Q only work 13.20% and 5.49% of the time, and V s and V m seem to be correct only 1.10% of the time. In other words, if we use one technique as our evaluation method, as most current methods do, it highly depends one which one you choose. It might work relatively well if E is choose, or you could get wrong results almost all the time if V s or V m is used. However, a different set of input images might make E work poorly, where some of the others might work well. Hence, we cannot use E all the time.</p><p>The simple combiner strategy is not a good co-evaluation strategy, since it does not employ any learning. Oftern majority is not correct since for some segmentations many evaluators might perform poorly. If we use a combiner which employs a machine learning technique, we could improve the performance of the evaluation system. The results in <ref type="table">Table 4</ref> reveal that the machine learning strategies do work well. They generally work better than any of the evaluators, thus improving the performance of the whole evaluation system. Both WM and NB learn that predicting opposite of V s and V m is a very good approach that is correct 98.90% of the time.</p><p>simple WM NB SVM Accuracy 4.40% 98.90% 98.90% 92.31% <ref type="table">Table 4</ref>. The evaluation accuracies for co-evaluation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experiment two: machine segmentation results vs. machine segmentation results</head><p>In experiment one, the accuracy of the five evaluators is highly unbalanced for the experimental segmentation pairs, where one accuracy is much higher than the others, and the others are themselves very low. Consequently, we designed another group of experiment that use different segmentation pairs, leading to more similar accuracy among the individual base evaluators.</p><p>In experiment two, the test images are all from the aircraft images in the Military Graphics Collection. <ref type="bibr" target="#b23">24</ref> For each image we create a series of segmentations where the number of segments varies from 2 to 20, using the Improved Hierarchical Segmentation (IHS) algorithm with fast texture feature extraction. <ref type="bibr" target="#b24">25</ref> Then, each human evaluator from an evaluation group picks out the best three segmentations and the worst three segmentations in his/her mind. The intersections of best segmentations and worst segmentations from all evaluators are used as the best set B and the worst set W . For each image, a segmentation in B is paired with a segmentation in W . We use this approach to create 249 pairs of segmentations in which one is clearly better the the other.</p><p>In our experiments, we randomly choose roughly half of the pairs as the training set for machine learning combiner, and use the rest as the test set. We repeat this process 30 times to create 30 different randomly selected training/test sets. Those segmentation pairs are then sent to the co-evaluation methods using different strategies.</p><p>The average performance of each individual base evaluator over the 30 test sets are shown in <ref type="table" target="#tab_3">Table 5</ref>, and the average performance of each co-evaluation strategy over the 30 test sets are shown in <ref type="table">Table 6</ref>. These results are also illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Once again, the results show that the co-evaluation using a machine learning strategy improves the overall evaluation performance.</p><formula xml:id="formula_12">E F Q V s V m</formula><p>Accuracy 32.80% ± 0.99% 47.37% ± 1.06% 75.60% ± 1.34% 55.48% ± 1.29% 62.37% ± 1.27%  <ref type="table">Table 6</ref>. The average evaluation accuracy (mean ± 95% confidence interval) for each co-evaluation strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Image segmentation is a critical early step in many object recognition systems. Its performance greatly affects the recognition accuracy of the whole system. Although many image segmentation methods have been proposed over the last decades, there is still no satisfactory segmentation evaluation method. Among the evaluation methods proposed so far, only the stand-alone objective evaluation methods are suitable for general purpose automatic evaluation process, which is required in most object recognition systems.</p><p>However, these stand-alone methods examine different fundamental criteria of the objects, and rely heavily on the image characteristics they are measuring. So they work well in some cases, or for some groups of images, and poorly for the others. To improve the evaluation accuracy, we propose a co-evaluation method in this paper, in which different evaluators judge the performance of the segmentation in different ways, and their measures are combined by a machine learning algorithm that determines how to coalesce the results from the constitute measures. By determining the circumstances under which the various evaluation metrics perform well versus poorly, it leverages the appropriate evaluators' quality measures to achieve reliable results. Experiment results demonstrate that a machine learning combiner can effectively improve the evaluation accuracy of the whole system.</p><p>Our method has many advantages. First of all, based on these preliminary results, our co-evaluator strategy improves the evaluation accuracy when we use a combiner that employs a machine learning approach. Secondly, most current evaluation techniques can be used directly with little or no modification in our method. In fact, with our method even a subjective evaluator, or an evaluator employing an analytical or empirical discrepancy method can be used as one of the base evaluators enabling evaluation methods from fundamentally different categories to be used together to further improve the evaluation performance. Thirdly, images captured by different imaging technologies can be used collaboratively. This property is invaluable for military and security purposes, because different imaging technologies will provide information from different aspects, which is highly demanded in military/security appliances that must be as reliable as possible under complex working conditions. Also the structure of our system enables each evaluator to compute its prediction in parallel, and thus the overall processing time is determined by the sum of the longest processing time of any evaluator and that of the combiner. All of the combiners we have selected can compute their evaluation very quickly. Most of the costs are associated with the training time which is a pre-processing step for the image recognition system. Comparing our method with previous image segmentation evaluation methods, the major difference is our method applies machine learning techniques to coalesce the results from the base evaluators.</p><p>Future work is still needed. First of all, we plan to try other co-evaluation strategies. We are considering using a variant of WM which employs a self-tuned β. In this paper, all co-evaluation strategies are based only on the evaluation results from the base evaluators, as well as the label indicating which segmentation is indeed better in the training process. These strategies are relatively simple, but the combiner know nothing about the original image or the segmentations themselves. However, by design the performances of these base evaluators are dependent on the content of the original image. If we could use the image features, or meta features which are based upon raw image features, in our training/evaluation processes, the combiner has the possibility of learning what base evaluator is most likely to generate reliable evaluations for each kind of images, when the combiner is actually performing meta-learning. By including the image features or meta-features, our combiner can more effectively coalesce the results from base evaluators, and improve the overall evaluation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Contact information: E-mail: huizhang@wustl.edu, Telephone: 1 314 935-8561, Fax: 1 314 935-7302</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>where</head><label></label><figDesc>k ∈ {s, m}, N is the number of regions in the segmented image, W per s = 0.2239, W inter s = 0.5522, W per m = 0.2979, and W inter m = 0.4043. These weights were determined by extensive experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Five</head><label></label><figDesc>base evaluators are used in the experiments, each employing a current stand-alone evaluation technique. These five evaluation metrics are F proposed by Liu and Yang, 7 Q proposed by Borsotti et al., 8 E proposed by Zhang, Fritts and Goldman, 11 V s and V m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Exemplar segmentation pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The average evaluation accuracy in experiment two. The error bar shown at the top of each bar is the 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The evaluator outputs for exemplar segmentations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The co-evaluation outputs for exemplar segmentations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>The evaluation accuracy for each evaluator.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>The average evaluation accuracy (mean ± 95% confidence interval) for each base evaluator. 

simple 
WM 
NB 
SVM 

Accuracy 52.34% ±1.18% 79.05% ± 2.34% 79.37%± 1.89% 75.87%± 1.65% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank all members from the WUCBIR group for helping create human evaluation results. We also thank Sharath Cholleti for many useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on evaluation methods for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1335" to="1346" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Validation of the interleaved pyramid for the segmentation of 3d vector images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Graaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="467" to="475" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Threshold evaluation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weszka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="622" to="629" />
			<date type="published" when="1978-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic measurement of computer generated image segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Nazif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computer Vision, Graphics, and Image Processing 41</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988-02" />
			<biblScope unit="page" from="233" to="260" />
		</imprint>
	</monogr>
	<note>A survey of thresholding techniques</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image thresholding: some new techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhandari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="139" to="158" />
			<date type="published" when="1993-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-resolution color image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="1994-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of color image segmentation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borsotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="741" to="747" />
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A method for objective edge detection evaluation and detector parameter selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yitzhaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1027" to="1033" />
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An entropy-based objective evaluation method for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE-Storage and Retrieval Methods and Applications for Multimedia</title>
		<meeting>SPIE-Storage and Retrieval Methods and Applications for Multimedia</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A universal prior for integers and estimation by minimum description length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="416" to="431" />
			<date type="published" when="1983-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Objective evaluation of video segmentation quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="186" to="200" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of video object&apos;s relevance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EUSIPCO&apos;2000 -X European Signal Processing Conference</title>
		<meeting>EUSIPCO&apos;2000 -X European Signal Processing Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The weighted majority algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="212" to="261" />
		</imprint>
	</monogr>
	<note>Information and Computation 108</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Averaging expert predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EuroCOLT</title>
		<meeting>of EuroCOLT</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="153" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How to use expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="427" to="485" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to better use expert advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>-Y. Rani Yaroshinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Seiden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="271" to="309" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making large-Scale SVM Learning Practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<ptr target="http://svmlight.joachims.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<ptr target="http://www.caip.rutgers.edu/riul/research/code/EDISON/" />
	</analytic>
	<monogr>
		<title level="j">Edge Detection and Image SegmentatioN System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Military Graphics Collection</title>
		<ptr target="http://www.locked.de/en/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast texture feature extraction method in hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE-Image and Video Communications and Processing</title>
		<meeting>SPIE-Image and Video Communications and essing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
