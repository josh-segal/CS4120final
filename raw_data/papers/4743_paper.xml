<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-tuning the 27-point Stencil for Multicore</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Datta</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California at Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Williams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CRD/NERSC</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Volkov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California at Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Carter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CRD/NERSC</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Oliker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CRD/NERSC</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shalf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CRD/NERSC</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Yelick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CRD/NERSC</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-tuning the 27-point Stencil for Multicore</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study focuses on the key numerical technique of stencil computations, used in many different scientific disciplines, and illustrates how auto-tuning can be used to produce very efficient implementations across a diverse set of current multicore architectures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent transformation from an environment where gains in computational performance came from increasing clock frequency to an environment where gains are realized through ever increasing numbers of modest-performing cores has profoundly changed the landscape of scientific application programming. A major problem facing application programmers is the diversity of multicore architectures that are now emerging. From relatively complex out-of-order CPUs with complex cache structures to relatively simple cores that support hardware multithreading, designing optimal code for these different platforms represents a serious challenge. An emerging solution to this problem is auto-tuning: the automatic generation of many versions of a code kernel that incorporate various tuning strategies, and the benchmarking of these to select the best performing version. Often a key parameter is associated with each tuning strategy (e.g. the amount of loop unrolling or the cache blocking factor), so these parameters must be explored in addition to the layering of the basic strategies themselves.</p><p>In Section 2, we give an overview of the stencil studied, followed by a review of the multicore architectures that form our testbed in Section 3. Then, in Sections 4, 5 and 6, we discuss the characteristics of the 27-point stencil, our applied optimizations, and the parameter search respectively. Finally, we present performance results and conclusions in Sections 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Stencil Overview</head><p>Partial differential equation (PDE) solvers are employed by a large fraction of scientific applications in such diverse areas as heat diffusion, electromagnetics, and fluid dynamics. These applications are often implemented using iterative finite-difference techniques that sweep over a spatial grid, performing nearest neighbor computations called stencils. In a stencil operation, each point in a multidimensional grid is updated with weighted contributions from a subset of its neighbors in both time and space -thereby representing the coefficients of the PDE for that data element. These operations are then used to build solvers that range from simple Jacobi iterations to complex multigrid and adaptive mesh refinement methods <ref type="bibr" target="#b1">[2]</ref>.</p><p>Stencil calculations perform global sweeps through data structures that are typically much larger than the capacity of the available data caches. In addition, the amount of data reuse within a sweep is limited to the number of points in a stencil -often less than 27. As a result, these computations generally achieve a low fraction of processor peak performance as these kernels are typically bandwidth-limited. By no means does this imply there is no potential for optimization. In fact, reorganizing these stencil calculations to take full advantage of memory hierarchies has been the subject of much investigation over the years. These have principally focused on tiling optimizations <ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref> that attempt to exploit locality by performing operations on cache-sized blocks of data before moving on to the next block -a means of eliminating capacity misses. A study of stencil optimization <ref type="bibr" target="#b6">[6]</ref> on (single-core) cache-based platforms found that tiling optimizations were primarily effective when the problem size exceeded the on-chip cache's ability to exploit temporal recurrences. A more recent study of lattice-Boltzmann methods <ref type="bibr" target="#b14">[14]</ref> employed auto-tuners to explore a variety of effective strategies for refactoring lattice-based problems for multicore processing platforms. That study expanded on prior work by utilizing a more compute-intensive stencil, identifying the TLB as the performance bottleneck, developing new optimization techniques and applying them to a broader selection of processing platforms.</p><p>In this paper, we build on our prior work <ref type="bibr" target="#b3">[4]</ref> and explore the optimizations and evaluate the performance of each sweep of a Jacobi (out-of-place) iterative method using a 3D 27-point stencil. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the stencil includes all the points within a 3×3×3 cube surrounding the center grid point. Being symmetric, it only uses four different weights for these points-one each for the center point, the six face points, the twelve edge points, and the eight corner points.</p><p>Since we are performing a Jacobi iteration, we keep two separate doubleprecision (DP) 3D arrays -one that is exclusively read from and a second that is only written to. This means that the stencil computation at each grid point is independent of every other point. As a result, there are no dependencies between these calculations, and they can be computed in any order. We take advantage of this fact in our code.</p><p>In general, Jacobi iterations converge more slowly and require more memory than Gauss-Seidel (in-place) iterations, which only require a single array that is both read from and written to. However, the dependencies involved in GaussSeidel stencil sweeps significantly complicate performance optimization. This topic, while important, will be left as future research.</p><p>Although the simpler 7-point 3D stencil is fairly common, there are many instances where larger stencils with more neighboring points are required. For instance, the NAS Parallel MG (Multigrid) benchmark utilizes a 27-point stencil to calculate the Laplace operator for a finite volume method <ref type="bibr" target="#b0">[1]</ref>. Broadly speaking, the 27-point 3D stencil can be a good proxy for many compute-intensive stencil kernels, which is why it was chosen for our study. For example, consider T. Kim's work on optimizing a fluid simulation code <ref type="bibr" target="#b8">[8]</ref>. By using a Mehrstellen scheme <ref type="bibr" target="#b2">[3]</ref> to generate a 19-point stencil (where δ equals 0) instead of the typical 7-point stencil, he was able to reach the desired error reduction in 34% fewer stencil iterations. For this study, we do not go into any of the numerical properties of stencils; we merely study and optimize their performance across different multicore architectures. As an added benefit, this analysis also helps to expose many interesting features of current multicore architectures. <ref type="table">Table 1</ref> details the core, socket, system and programming of the four cachebased computers used in this work. These include two generations of Intel quad-core superscalar processors (Clovertown and Nehalem) representing similar core architectures but dramatically different integration approaches. Nehalem has replaced Clovertown's front side bus (FSB)-external memory controller hub (MCH) architecture with integrated memory controllers and quick-path for remote socket communication and coherency. At the other end of the spectrum is IBM's Blue Gene/P (BGP) quad-core processor. BGP is a single-socket, dualissue in-order architecture providing substantially lower throughput and bandwidth while dramatically reducing power. Finally, we included Sun's dual-socket, 8-core Niagara architecture (Victoria Falls). Although its peak bandwidth is similar to Clovertown and Nehalem, its peak flop rate is more inline with BGP. Rather than depending on superscalar execution or hardware prefetchers, each of the eight strictly in-order cores supports two groups of four hardware thread contexts (referred to as Chip MultiThreading or CMT) -providing a total of 64 simultaneous hardware threads per socket. The CMT approach is designed to tolerate instruction, cache, and DRAM latency through fine-grained multithreading.  <ref type="table">Table 1</ref>. Architectural summary of evaluated platforms. § All system power is measured with a digital power meter while under a full computational load. ‡ Power running Linpack averaged per blade. (www.top500.org) † out-of-order (ooo).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Testbed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Stencil Characteristics</head><p>The left panel of <ref type="figure">Figure 2</ref> shows the na¨ıvena¨ıve 27-point stencil code after it has been loop unrolled once. The number of reads (from the memory hierarchy) per stencil is 27, but the number of writes is only one. However, when one considers adjacent stencils, we observe substantial reuse. Thus, to attain good performance, a cache (if present) must filter the requests and present only the two compulsory (in 3C's parlance) requests per stencil to DRAM <ref type="bibr" target="#b5">[5]</ref>. There are two compulsory requests   per stencil because every point in the grid must be read once and written once.</p><p>One should be mindful that many caches are write allocate. That is, on a write miss, they first load the target line into the cache. Such an approach implies that writes generate twice the memory traffic as reads even if those addresses are written but never read. The two most common approaches to avoiding this superfluous memory traffic are write through caches or cache bypass stores. <ref type="table" target="#tab_2">Table 2</ref> illustrates the dramatic difference in the per stencil averages for the number of loads and floating-point operations, both for the basic stencil as well as the highly optimized common subexpression elimination (CSE) version of the stencil. Although an ideal cache would distill these loads and stores into 8 bytes of compulsory DRAM read traffic and 8 bytes of compulsory DRAM write traffic, caches are typically not write through, infinite or fully associative, and na¨ıvena¨ıve codes are not cache blocked. As such, we expect an additional 8 bytes of DRAM write allocate traffic, and another 16 bytes of capacity miss traffic (based on the caches found in superscalar processors and the reuse pattern of this stencil) -a 2.5× increase in memory traffic. Auto-tuners for structured grids will actively or passively attempt to elicit better cache behavior and less memory traffic on the belief that reducing memory traffic and exposed latency will improve performance. If the auto-tuner can eliminate all cache misses, we can improve performance by 1.65×, but if the auto-tuner also eliminates all write allocate traffic, then it may improve performance by 2.5×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stencil Optimizations</head><p>Compilers utterly fail to achieve satisfactory stencil code performance because implementations optimal for one microarchitecture may deliver suboptimal performance on another. Moreover, their ability to infer legal domain-specific transformations, given the freedoms of the C language, is limited. To improve upon this, there are a number of optimizations that can be performed at the source level to increase performance, including: NUMA-aware data allocation, array padding, multilevel blocking (shown in <ref type="figure">Figure 3</ref>), loop unrolling and reordering, as well as prefetching for cache-based architectures. These well known optimizations were detailed in our previous work <ref type="bibr" target="#b3">[4]</ref>. Remember, Jacobi's method is both out-of-place and easily parallelized (theoretically, stencils may be executed in any order). This greatly facilitates our parallelization efforts as threads must only synchronize via a barrier after executing all their assigned blocks. In this paper, we detail a new common subexpression elimination optimization.</p><formula xml:id="formula_0">for (k=1; k &lt;= nz; k++) {! for (j=1; j &lt;= ny; j++) {! for (i=1; i &lt; nx; i=i+2) {! next[i,j,k] = ! alpha * ( now[i,j,k] )! +beta * (! now[i,j,k-1] + now[i,j-1,k] +! now[i,j+1,k] + now[i,j,k+1] +! now[i-1,j,k] + now[i+1,j,k]! )! +gamma * (! now[i-1,j,k-1] + now[i-1,j-1,k] +! now[i-1,j+1,k] + now[i-1,j,k+1] +! now[i,j-1,k-1] + now[i,j+1,k-1] +! now[i,j-1,k+1] + now[i,j+1,k+1] +! now[i+1,j,k-1] + now[i+1,j-1,k] +! now[i+1,j+1,k] + now[i+1,j,k+1]! )! +delta * (! now[i-1,j-1,k-1] + now[i-1,j+1,k-1] +! now[i-1,j-1,k+1] + now[i-1,j+1,k+1] +! now[i+1,j-1,k-1] + now[i+1,j+1,k-1] +! now[i+1,j-1,k+1] + now[i+1,j+1,k+1]! );! next[i+1,j,k] = ! alpha * ( now[i+1,j,k] )! +beta * (! now[i+1,j,k-1] + now[i+1,j-1,k] +! now[i+1,j+1,k] + now[i+1,j,k+1] +! now[i,j,k] + now[i+2,j,k]! )! +gamma * (! now[i,j,k-1] + now[i,j-1,k] +! now[i,j+1,k] + now[i+1-1,j,k+1] +! now[i+1,j-1,k-1] + now[i+1,j+1,k-1] +! now[i+1,j-1,k+1] + now[i+1,j+1,k+1] +! now[i+2,j,k-1] + now[i+2,j-1,k] +! now[i+2,j+1,k] + now[i+2,j,k+1]! )! +delta * (! now[i,j-1,k-1] + now[i,j+1,k-1] +! now[i,j-1,k+1] + now[i,j+1,k+1] +! now[i+2,j-1,k-1] + now[i+2,j+1,k-1] +! now[i+2,j-1,k+1] + now[i+2,j+1,k+1]! );! }! }! }! for (k=1; k &lt;= nz; k++) {! for (j=1; j &lt;= ny; j++) {! for (i=1; i &lt; nx; i=i+2) {! sum_edges_0 =! now[i-1,j,k-1] + now[i-1,j-1,k] +! now[i-1,j+1,k] + now[i-1,j,k+1];! sum_edges_1 =! now[i,j,k-1] + now[i,j-1,k] +! now[i,j+1,k] + now[i,j,k+1];! sum_edges_2 =! now[i+1,j,k-1] + now[i+1,j-1,k] +! now[i+1,j+1,k] + now[i+1,j,k+1];! sum_edges_3 =! now[i+2,j,k-1] + now[i+2,j-1,k] +! now[i+2,j+1,k] + now[i+2,j,k+1];! sum_corners_0 =! now[i-1,j-1,k-1] + now[i-1,j+1,k-1] +! now[i-1,j-1,k+1] + now[i-1,j+1,k+1];! sum_corners_1 =! now[i,j-1,k-1] + now[i,j+1,k-1] +! now[i,j-1,k+1] + now[i,j+1,k+1];! sum_corners_2 =! now[i+1,j-1,k-1] + now[i+1,j+1,k-1] +! now[i+1,j-1,k+1] + now[i+1,j+1,k+1];! sum_corners_3 =! now[i+2,j-1,k-1] + now[i+2,j+1,k-1] +! now[i+2,j-1,k+1] + now[i+2,j+1,k+1];! center_plane_1 =! alpha * now[i,j,k] +! beta * sum_edges_1 + gamma * sum_corners_1;! center_plane_2 =! alpha * now[i+1,j,k] +! beta * sum_edges_2 + gamma * sum_corners_2;! side_plane_0 =! beta * now[i-1,j,k] +! gamma * sum_edges_0 + delta * sum_corners_0;! side_plane_1 =! beta * now[i,j,k] +! gamma * sum_edges_1 + delta * sum_corners_1;! side_plane_2 =! beta * now[i+1,j,k] +! gamma * sum_edges_2 + delta * sum_corners_2;! side_plane_3 =! beta * now[i+2,j,k] +! gamma * sum_edges_3 + delta * sum_corners_3;! next[i,j,k] =! side_plane_0 + center_plane_1 + side_plane_2;! next[i+1,j,k] =! side_plane_1 + center_plane_2 + side_plane_3;! }! }! }! No CSE CSE</formula><p>Common subexpression elimination (CSE) involves identifying and eliminating common expressions across several stencils. This type of optimization can be considered to be an algorithmic transformation because of two reasons. First, the flop count is being reduced, and second, the flops actually being performed may be performed in a different order than our original implementation. Due to the non-associativity of floating point operations, this may well produce results that are not bit-wise equivalent to those from the original implementation.  <ref type="table">Table 3</ref>. Attempted optimizations and the associated parameter spaces explored by the auto-tuner for a 256 3 stencil problem (N X, N Y, N Z = 256). All numbers are in terms of doubles.</p><p>Consider <ref type="figure">Figure 4</ref>. If one were to perform the reference stencil for successive points in x, we perform 30 flops per stencil. However, as we loop through x, we may dynamically create several temporaries (unweighted reductions) - <ref type="figure">Figure 4</ref>(b) and (c). For stencils at x and x + 1, there is substantial reuse of these temporaries. On fused multipy-add (FMA)-based architectures, we may implement the stencil by creating these temporaries and performing a linear combination using three temporaries from <ref type="figure">Figure 4</ref>(b), three from <ref type="figure">Figure 4</ref>(c) and the stencil shown in <ref type="figure">Figure 4(d)</ref>. On the x86 architectures, we create a second group of temporaries by weighting the first set, the pseudo-code for which is shown in the right panel of <ref type="figure">Figure 2</ref>. With enough loop unrollings in the inner loop, the CSE code has a lower bound of 18 flops/point. Disappointingly, neither the gcc nor icc compilers were able to apply this optimization automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Auto-Tuning Methodology</head><p>Thus far, we have described our applied optimizations in general terms. In order to take full advantage of the optimizations mentioned in Section 5, we developed an auto-tuning environment <ref type="bibr" target="#b3">[4]</ref> similar to that exemplified by libraries like AT-LAS <ref type="bibr" target="#b13">[13]</ref> and OSKI <ref type="bibr" target="#b12">[12]</ref>. To that end, we first wrote a Perl code generator that produces multithreaded C code variants encompassing our stencil optimizations. This approach allows us to evaluate a large optimization space while preserving performance portability across significantly varying architectural configurations. The parameter space for each optimization individually, shown in <ref type="table">Table 3</ref>, is certainly tractable -but the parameter space generated by combining these optimizations results in a combinatorial explosion. Moreover, these optimizations are not independent of one another; they can often interact in subtle ways that vary from platform to platform. Hence, the second component of our auto-tuner is the search strategy used to find a high-performing parameter configuration.</p><p>To find the best configuration parameters, we employed an iterative "greedy" search. First, we fixed the order of optimizations. Generally, they were ordered by their level of complexity, but there was some expert knowledge employed as well. This ordering is shown in the legend of <ref type="figure" target="#fig_5">Figure 6</ref>; the relevant optimizations were applied in order from bottom to top. Within each individual optimization, we performed an exhaustive search to find the best performing parameter(s). These values were then fixed and used for all later optimizations. We consider this to be an iterative greedy search. If all applied optimizations were independent of one another, this search method would find the global performance maxima. However, due to subtle interactions between certain optimizations, this usually won't be the case. Nonetheless, we expect that it will find a good-performing set of parameters after doing a full sweep through all applicable optimizations.</p><p>In order to judge the quality of the final configuration parameters, two metrics can be used. The more useful metric is the Roofline model <ref type="bibr" target="#b15">[15]</ref>, which provides an upper bound on kernel performance based on bandwidth and computation limits. If our fully tuned implementation approaches this bound, then further tuning will not be productive. However, the Roofline model is outside the scope of this paper. Instead, we can gain some intuition on the quality of our final parameters by doing a second pass through our greedy iterative search. This is represented by the topmost color in the legends of <ref type="figure" target="#fig_5">Figure 6</ref>. If this second pass improves performance substantially, then our initial greedy search obviously wasn't effective. <ref type="figure">Figure 5</ref> visualizes the iterative greedy search algorithm for a domain where there are only two optimizations: A and B. As such, the optimization space is only two dimensional. Given an initial guess at the ideal optimization parameters (point #1), we search over all possible values of optimization A while keeping optimization B fixed. The best performing configuration along this line search then becomes point #2. Starting from point #2, we then search all possible values of optimization B while keeping optimization A fixed. This produces an even better performing configuration -point #3. At this point we've completed one pass through the greedy algorithm. We make the algorithm iterative by repeating the procedure, but starting at point #3. This results in successively better points #4 and #5.</p><p>In reality, we are dealing with many more than just two optimizations, and thus a significantly higher dimensional search space. This is the same parameter search technique that we employed in our earlier work for tuning the 3D 7-point stencil <ref type="bibr" target="#b3">[4]</ref>, however here we employ two passes across the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Architecture Specific Exceptions</head><p>Due to limited potential benefit and architectural characteristics, not all architectures implement all optimizations or explore the same parameter spaces. <ref type="table">Table 3</ref> details the range of values for each optimization parameter by architecture. In this section, we explain the reasoning behind these exceptions to the full auto-tuning methodology. To make the auto-tuning search space tractable, we typically explored parameters in powers of two.</p><p>Both the x86 and BG/P architectures rely on hardware stream prefetching as their primary means for hiding memory latency. As previous work <ref type="bibr" target="#b7">[7]</ref> has shown that short stanza lengths severely impair memory bandwidth, we prohibit core blocking in the unit stride (X) dimension, so CX = N X. Thus, we expect the hardware stream prefetchers to remain engaged and effective.</p><p>Although the standard code generator produces portable C code, compilers often fail to effectively SIMDize the resultant code. As such, we created several instruction set architecture (ISA) specific variants that produce explicitly SIMDized code for x86 and Blue Gene/P using intrinsics. For x86, the option of using a non-temporal store movntpd to bypass the cache was also incorporated.</p><p>Victoria Falls is also a cache-coherent architecture, but its multithreading approach to hiding memory latency is very different than out-of-order execution coupled with hardware prefetching. As such, we allow core blocking in the unit stride dimension. Moreover, we allow each core block to contain either 1 or 8 thread blocks. In essence, this allows us to conceptualize Victoria Falls as either a 128 core machine or a 16 core machine with 8 threads per core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Analysis</head><p>In our experiment, we apply a single out-of-place stencil sweep at a time to a 256 3 grid. The reference stencil code uses only two large flat 3D scalar arrays as data structures, and that is maintained through all subsequent tuning. We do increase the size of these arrays with an array padding optimization, but this does not introduce any new data structures nor change the array ordering. In addition, in order to get accurate measurements, we report the average of   at least 5 timings for each data point, and there was typically little variation among these readings. Below we present and analyze the results from auto-tuning the stencil on each of the four architectures. To ensure fairness, across all architectures we ordered threads to first exploit all the threads on a core, then populate all cores within a socket, and finally use multiple sockets.</p><p>In all our figures, we present performance as GStencil/s (10 9 stencils per second) to allow a meaningful comparison between CSE and non-CSE kernels. In addition, in <ref type="figure" target="#fig_5">Figures 6 and 7</ref>, we stack optimization bars to represent the performance as the auto-tuning algorithm progresses through the greedy search (i.e. subsequent optimizations are built on best configuration of the previous optimizations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Nehalem Performance</head><p>We see in <ref type="figure" target="#fig_5">Figure 6</ref> that the performance of the reference implementation improves by 3.3× when scaling from 1 to 4 cores, but then drops slightly when we use all 8 cores across both sockets. This performance quirk is eliminated when we apply the NUMA-aware optimization.</p><p>There are several indicators that strongly suggest that it is compute-boundcore blocking shows only a small benefit, cache bypass doesn't show any benefit,   performance scales linearly with the number of cores, and the CSE optimization is successful across all core counts. Nonetheless, after full tuning, we now see a 3.6× speedup when using all 8 cores. Moreover, we also see parallel scaling of 8.1× when going from 1 to 8 cores -the ideal multicore scaling.</p><p>It is important to note that the choice of compiler plays a significant role in the effectiveness of certain optimizations as well as the final auto-tuning performance. For instance, <ref type="figure" target="#fig_7">Figure 7</ref> shows the performance for both the gcc and icc compilers when tuning on Nehalem. There are several interesting trends that can be read from this graph. For instance, at every core count, the performance of gcc after applying register blocking is slightly below icc's na¨ıvena¨ıve (or na¨ıvena¨ıve with NUMA-aware using all 8 cores) performance. Therefore, it is likely that the gcc compiler's unrolling facilities are not as good as icc's. In addition, core blocking improves icc performance by at least 18% at the higher core counts, but it does not show any benefit for gcc. As Nehalem is nearly equally memory-and compute-bound, slightly inferior code generation capabilities will hide memory bottlenecks.</p><p>A notable deficiency in both compilers is that neither one was able to eliminate common subexpressions for the stencil. This was confirmed by examining the assembly code generated by both compilers; neither one reduced the flop count from 30 flops/point. As seen in <ref type="figure">Figure 2</ref>, the common subexpressions only arise when examining several adjacent stencil operations. Thus, a compiler must loop unroll before trying to identify common subexpressions. However, even when we explicitly loop unrolled the stencil code, neither compiler was able to exploit CSE. It is unclear whether the compiler lacked sufficient information to effect an algorithmic transformation or simply lacks the functionality. Whatever the reason for this, explicit CSE code improves performance by 25% for either compiler at maximum concurrency.</p><p>In general, icc did at least as well as gcc on the x86 architectures, so only icc results are shown for these platforms. However, the more bandwidth-limited the kernel, the less icc's advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Clovertown Performance</head><p>Unlike the Nehalem chip, the Clovertown is a Uniform Memory Access machine with an older front side bus architecture. This implies that the NUMA-aware optimization will not be useful and the 27-point stencil will likely be bandwidthconstrained.</p><p>As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, memory bandwidth is clearly an issue at the higher core counts. When we run on 1 or 2 cores, cache bypass is not helpful, while the CSE optimization produces speedups of at least 30%, implying that the lower core counts are compute-bound. However, as we scale to 4 and 8 cores, we observe a transition to being memory-bound. The cache bypass instruction improves performance by at least 10%, while the effects of CSE are negligible. All in all, full tuning resulted in a 1.9× improvement using all 8 cores, as well as a 2.7× speedup when scaling from 1 to 8 cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Blue Gene/P Performance</head><p>Unlike the two previous architectures, the IBM Blue Gene/P implements the PowerPC ISA. In addition, the xlc compiler does not generate or support cache bypass at this time. As a result, the best arithmetic intensity we can achieve 1.25. The performance of Blue Gene/P seems to be compute-bound -as seen in <ref type="figure" target="#fig_5">Fig- ure 6</ref>, memory optimizations like padding, core blocking, or software prefetching make no noticeable difference. The only optimizations that help performance are computation-related, like register blocking and CSE. Moreover, after full tuning, the 27-point stencil kernel shows perfect multicore scaling; going from 1 to all 4 cores results in a performance improvement of 3.9×.</p><p>It is important to note that while we did pass xlc the SIMDization flags when compiling the portable C code, we did not use any pragmas or functions like #pragama unroll, #pragama align, or alignx() within the code. Interestingly, when we modified our stencil code generator to explicitly produce SIMD intrinsics, we observed a 10% decrease in performance of the CSE implementation. One should note that unlike x86, Blue Gene does not support an unaligned SIMD load. As such, to load a stream of consecutive elements where the first is not 16-byte aligned, one must perform permutations and asymptotically require two instructions for every two elements. Clearly this is no better than a scalar implementation of one load per element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Victoria Falls Performance</head><p>Like the Blue Gene/P, the Victoria Falls does not exploit cache bypass. Moreover, it is a highly multi-threaded architecture with low-associativity caches. To exploit these characteristics, we introduced the thread blocking optimization specifically for Victoria Falls. In the original implementation of the stencil code, each core block is processed by only one thread. When the code is thread blocked, threads are clustered into groups of 8; these groups work collectively on one core block at a time.</p><p>The reference implementation, shown in <ref type="figure" target="#fig_5">Figure 6</ref>, scales well. Nonetheless, auto-tuning was still able to achieve significantly better results. Many optimizations combined together to improve performance, including array padding, core blocking, common subexpression elimination, and a second sweep of the greedy algorithm. After full tuning, performance improved by 1.8× when using all 16 cores, and we also see parallel scaling of 13.1× when scaling to 16 cores. The fact that we almost achieve linear scaling strongly hints that it is compute-bound.</p><p>The Victoria Falls performance results are even more impressive considering that one must regiment 128 threads to perform one operation; this is 8 times as many as the Nehalem, 16 times more than Clovertown, and 32 times more than the Blue Gene/P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Cross Platform Performance and Power Comparison</head><p>At ultra scale, power has become a severe impediment to increased performance. Thus, in this section not only do we normalize performance comparisons by looking at entire nodes rather than cores, we also normalize performance with power utilization. To that end, we use a power efficiency metric defined as the ratio of sustained performance to sustained system power -GStencil/s/kW. This is essentially the number of stencil operations one can perform per Joule of energy.</p><p>The evolution of x86 multicore chips from the Intel Clovertown, to the Intel Nehalem is an intriguing one. The Clovertown is a uniform memory access architecture that uses an older front-side bus architecture and supports only a single hardware thread per core. In terms of DRAM, it employs FBDIMMs running at a relatively slow 667 MHz. Consequently, it is not surprising to see in <ref type="figure" target="#fig_8">Figure 8</ref> that the Clovertown is the slowest x86 architecture. In addition, due in part to the use of power-hungry FBDIMMs, it is also the least power efficient x86 platform (as evidenced in <ref type="figure" target="#fig_8">Figure 8</ref>). As previously mentioned, Intel's new Nehalem improves on previous x86 architectures in several ways. Notably, Nehalem features an integrated on-chip memory controller, the QuickPath inter-chip network, and simultaneous multithreading (SMT). It also uses three channels of DDR3 DIMMs running at 1066 MHz. On the compute-intensive CSE kernel, we still see a 3.7× improvement over Clovertown.</p><p>The IBM Blue Gene/P was designed for large-scale parallelism, and one consequence is that it is tailored for power efficiency rather than performance. This trend is starkly laid out in <ref type="figure" target="#fig_8">Figure 8</ref>. Despite Blue Gene/P delivering the lowest performance per SMP among all architectures, it still attained the best power efficiency. It should be noted that Blue Gene/P is two process technology generations behind Nehalem (90nm vs. 45nm).</p><p>Victoria Falls' chip multithreading (CMT) mandates one exploit 128-way parallelism. We see that Victoria Falls achieves better performance than either Clovertown or Blue Gene/P. However, in terms of power efficiency, it is second to last, better than only Clovertown. This should come as no surprise given they both use power-inefficient FBDIMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this work, we examined the application of auto-tuning to a 27-point stencil on a wide range of cache-based multicore architectures. The chip multiprocessors examined in our study lie at the extremes of a spectrum of design trade-offs that range from replication of existing core technology (multicore) to employing large numbers of simple multithreaded cores (CMT) to power-optimized designs. Results demonstrate that parallelism discovery is only a small part of the performance challenge. Of equal importance is selecting from various forms of hardware parallelism and enabling memory hierarchy optimizations.</p><p>Our work leverages the use of auto-tuners to enable portable, effective optimization across a broad variety of chip multiprocessor architectures, and successfully achieves the fastest multicore stencil performance to date. Clearly, autotuning was essential in providing substantial speedups regardless of whether the computational balance ultimately became memory or compute-bound; on the other hand, the reference implementation often showed poor (or even negative) scalability. Analysis shows that although every optimization was useful on at least one architecture <ref type="figure" target="#fig_5">(Figure 6</ref>), highlighting the importance of optimization within an auto-tuning framework, the portable C auto-tuner (without SIMDization and cache bypass) often delivered very good performance. This suggests one could forgo optimality for productivity without much loss.</p><p>Results show that Nehalem delivered the best performance of any of the systems, achieving more than a 6× speedup compared the previous generation Intel Clovertown -due, in-part, to the elimination of the front-side bus in favor of on-chip memory controllers. However, the low-power BG/P design offered one of the most attractive power efficiencies in our study, despite its poor single node performance; this highlights the importance of considering these design tradeoffs in an ultrascale, power intensive environment. Due to the complexity of reuse patterns endemic to stencil calculations coupled with relatively small perthread cache capacities, Victoria Falls was perhaps the most difficult machine to optimize -it needed virtually every optimization. Through use of performance models like Roofline <ref type="bibr" target="#b15">[15]</ref>, future work will bound how much further tuning is required to fully exploit each of these architectures.</p><p>Now that power has become the primary impediment to future processor performance improvements, the definition of architectural efficiency is migrating from a notion of "sustained performance" towards a notion of "sustained performance per watt." Furthermore, the shift to multicore design reflects a more general trend in which software is increasingly responsible for performance as hardware becomes more diverse. As a result, architectural comparisons should combine performance, algorithmic variations, productivity (at least measured by code generation and optimization challenges), and power considerations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Visualization of the 27-point stencil used in this work. Note: color represents the weighting factor for each point in the linear combination stencils.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Flops</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 4 .</head><label>24</label><figDesc>Fig. 2. Pseudo-code for one grid sweep using a 27-point stencil. Both panels have code that has been loop unrolled once in the unit-stride (x) dimension. However, the left panel does not exploit common subexpression elimination (CSE), while the right panel does. For instance, the value of the variable sum corners 1 is computed twice in the left panel, but only once in the right panel. The variables sum edges * in the right panel are graphically displayed in Figure 4(b), while the variables sum corners * are shown in Figure 4(c). In this example, the left panel performs 30 flops/point, while the right panel performs 29 flops/point; however, with more loop unrollings, the CSE code will approach 18 flops/point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5. Visualization of the iterative greedy search algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Stencil performance. Before the Common Subexpression Elimination optimization is applied, "GStencil/s" can be converted to "GFlop/s" by multiplying by 30 flops/stencil. Note, explicitly SIMDized BG/P performance was slower than the scalar form both with and without CSE. As such, it is not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A performance comparison between two compilers when auto-tuning the 27-point stencil on Nehalem. At each core count along the x-axis, the left stacked bar shows the performance of the gcc compiler, while the right stacked bar shows that of the icc compiler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A performance comparison for all architectures at maximum concurrency after full tuning. The graph displays performance for the auto-tuned stencil without common subexpression elimination (beige) and with it (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Average stencil characteristics. Arithmetic Intensity is defined as the To-
tal Flops / Total bytes. We assumes 8 bytes each for compulsory read and write, 8 
bytes write-allocate traffic, and 16 bytes for capacity misses. The numbers in parenthe-
ses assume cache bypass. The rightmost column, Potential Benefit from Auto-tuning, is 
computed by dividing the tuned arithmetic intensity by the na¨ıvena¨ıve arithmetic intensity. 
If memory bandwidth is the bottleneck, this is the largest speedup we should expect 
to see. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We would like to express our gratitude to Sun for their machine donations. This work and its authors are supported by the Director, Office of Science, of the U.S. Department of Energy under contract number DE-AC02-05CH11231 and by NSF contract CNS-0325873. This research used resources of the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357. Finally, we express our gratitude to Microsoft, Intel, and U.C. Discovery for providing funding (under Awards #024263, #024894, and #DIG07-10227, respectively) and for the Nehalem computer used in this study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The NAS Parallel Benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fatoohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fineberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weeratunga</surname></persName>
		</author>
		<idno>RNR-94-007</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>NASA Advanced Supercomputing (NAS) Division</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive mesh refinement for hyperbolic partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="484" to="512" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Numerical Treatment of Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Collatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stencil Computation Optimization and Auto-Tuning on State-of-the-art Multicore Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;08: Proceedings of the</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">ACM/IEEE conference on Supercomputing</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating Associativity in CPU Caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1612" to="1630" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit and explicit optimizations for stencil computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Workshop Memory Systems Performance and Correctness</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Impact of modern memory subsystems on cache optimizations for stencil computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual ACM SIGPLAN Workshop on Memory Systems Performance</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hardware-aware analysis and optimization of stable fluids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">I3D &apos;08: Proceedings of the 2008 symposium on Interactive 3D graphics and games</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blocking and array contraction across arbitrarily nested loops using affine partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tiling optimizations for 3D scientific computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC&apos;00</title>
		<meeting>SC&apos;00<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
	<note>Supercomputing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cache-efficient multigrid algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OSKI: A library of automatically tuned sparse matrix kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SciDAC 2005, J. of Physics: Conference Series. Institute of Physics Publishing</title>
		<meeting>of SciDAC 2005, J. of Physics: Conference Series. Institute of Physics Publishing</meeting>
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated Empirical Optimization of Software and the ATLAS project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="35" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lattice Boltzmann simulation optimization on leading multicore platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interational Conference on Parallel and Distributed Computing Systems (IPDPS)</title>
		<meeting><address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Roofline: An insightful visual performance model for floating-point programs and multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Watterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
