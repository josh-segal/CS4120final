<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Memory Snapshot for Concurrent Programming without Synchronization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoong</forename><surname>Chung</surname></persName>
							<email>jaewoong.chung@amd.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Research and Advanced Development Lab AMD Corporation</orgName>
								<orgName type="laboratory" key="lab2">Computer Systems Lab Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woongki</forename><surname>Baek</surname></persName>
							<email>wkbaek@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Research and Advanced Development Lab AMD Corporation</orgName>
								<orgName type="laboratory" key="lab2">Computer Systems Lab Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<email>kozyraki@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Research and Advanced Development Lab AMD Corporation</orgName>
								<orgName type="laboratory" key="lab2">Computer Systems Lab Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Memory Snapshot for Concurrent Programming without Synchronization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Categories and Subject Descriptors B30 [Hardware]: MEMORY STRUCTURES-General General Terms Design Keywords Snapshot, Transactional Memory</keywords>
			</textClass>
			<abstract>
				<p>The industry-wide turn toward chip-multiprocessors (CMPs) provides an increasing amount of parallel resources for commodity systems. However, it is still difficult to harness the available paral-lelism in user applications and system software code. We propose MShot, a hardware-assisted memory snapshot for concurrent programming without synchronization code. It supports atomic multi-word read operations on a large dataset. Since modern processors support atomic access only to a single word, programmers should add synchronization code to process a multi-word dataset concurrently in multithreading environment. With snapshot, programmers read the dataset atomically and process the snapshot image without synchronization code. We implement MShot using hardware resources for transactional memory and reduce the storage overhead from 2.98% to 0.07%. To demonstrate the usefulness of fast snapshot, we use MShot to implement concurrent versions of garbage collection and call-path profiling. Without the need for synchronization code, MShot allows such system services to run in parallel with user applications on spare cores in CMP systems. As a result, the overhead of these services is minimized, approaching that of an ideal implementation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Chip-multiprocessors (CMPs) bring abundant parallelism to commodity systems. However, the lack of concurrency in software prevents programmers from fully exploiting the additional hardware cores. Ideally, sequential code could be easily changed to use Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ICS'09, <ref type="bibr">June 8-12, 2009</ref>, Yorktown Heights, New York, USA. Copyright 2009 ACM 978-1-60558-498-0/09/06 ...$5.00. multiple cores. However, parallelization is not trivial in practice because programmers must deal with the complications of concurrency management.</p><p>To alleviate this problem, it is important to develop architectural tools that help programmers to exploit parallelism. In search of such tools, we find memory snapshot particularly useful for improving the concurrency of software systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">22]</ref>. The key benefit of snapshot is to support atomic multi-word read operations that produce a consistent view on a large dataset. Since modern processors support atomic memory access only to a single word, programmers have to deal with complex synchronization issues to process a multiple-word dataset concurrently in multithreading environment. With snapshot, concurrent programs read a large dataset atomically and work with a consistent snapshot image of the dataset without synchronization code. The basic concept of snapshots has been used widely in databases, file systems, and reliable storage <ref type="bibr" target="#b11">[11,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b28">33]</ref>. While several algorithms for memory snapshot have been proposed, their applicability for performance optimizations is limited due to high runtime overhead since they rely on pure software techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. More sophisticated software implementations allow for additional gains at the cost of algorithmic complexity for applications <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">22]</ref>. This paper proposes MShot, hardware-assisted memory snapshot for concurrent programming without synchronization code. MShot provides a fast memory snapshot with hardware acceleration. Programmers use a snapshot to read a multi-word dataset atomically and to process it concurrently in multithreading environment. The snapshot image is isolated from further memory updates, shared by multiple threads, and accessed with normal load-/store instructions. MShot supports multiple memory snapshots of arbitrary lifetime that consist of multiple disjoint memory regions. A fast memory snapshot is beneficial for the software systems in need of atomic multi-word read operations such as fast concurrent backup, checkpointing, debugging parallel programs, concurrent garbage collection, dynamic profilers, fast copy-on-write, and inmemory databases.</p><p>We implement MShot using hardware resources available in transactional memory (TM). TM executes a group of instructions in an atomic and isolated manner <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">24]</ref>. The opportunity for sharing resources between MShot and hardware TM is understood intuitively since both systems support some sort of atomic execution. The key idea is to use the cache as a buffer for snapshot data and additional bits per cache line for snapshot metadata like a cache-based hardware TM system does for transactional data and metadata. Due to resource sharing with hardware TM, the storage overhead of MShot is reduced from 2.98% to 0.07%.</p><p>To evaluate MShot, we prototyped two snapshot-based systems: garbage collection (GC) and dynamic profiling. Taking advantage of the fact that "once garbage, always garbage," the snapshot-based GC takes a memory snapshot and performs collection without interfering with the application (mutator) threads running in parallel. The snapshot-based profiler takes a snapshot of the stack for callpath profiling. Reading data structures atomically and efficiently without synchronization code, these systems run the tasks concurrently with applications and add only a negligible runtime overhead to the applications.</p><p>The rest of the paper is organized as follows. Section 2 presents the motivation for this work. Section 3 explains the definition, programming interface, and hardware implementation of fast memory snapshot. Section 4 explains MShot implementation with TM hardware resources. Section 5 presents the quantitative evaluation. Section 6 discusses related work, and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivating Example</head><p>Modern processors support atomic read and write operations on single-word data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, if multiple words need to be read atomically while some of the words are written concurrently, systems do not provide a consistent view of the words for the read operation. Let's consider an example that mimics dynamic memory profiling where a profiling thread traverses an object reference graph and an application thread changes the graph concurrently. In <ref type="figure" target="#fig_0">Figure 1</ref>(a), the profiling thread has visited node A, determined that node B was the only child node, and reads node B while the application thread has detached node D from node C and is attaching it to node A. Since node A has already been visited, the profiling thread fails to visit node D, a new child node of node A, unobserved before. The problem is that the profiling thread cannot read the whole graph atomically and may miss visiting some live objects due to concurrent mutation by the application thread.</p><p>There are several solutions to address the problem. The profiling thread can grab a global lock for the graph, but in doing so it loses concurrency. The two-phase locking in database literature grabs per-node locks gradually and releases all locks when the graph traversal completes. It allows for more parallelism, but introduces lock acquisition overhead and may cause deadlocks when the profiling thread and the application thread traverse the same link in the opposite directions as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Transactional memory (TM) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">24]</ref> guarantees the atomic and isolated execution of the instructions in a transaction. With TM, the profiling thread can read the whole graph atomically by enclosing all graph access instructions within a transaction. However, the transaction is likely to be much longer than usually expected <ref type="bibr" target="#b9">[10]</ref> and may suffer from frequent restarts due to constant conflicts with the application thread as shown in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. Virtual memory protection <ref type="bibr" target="#b4">[5]</ref> and Dynamic Binary Translation(DBT) <ref type="bibr" target="#b30">[37]</ref> can be used to save old values of the graph for the profiling thread. However, virtual memory protection suffers from the page fault exception overhead and DBT from the instruction instrumentation overhead.</p><p>There have been developed algorithms specific to concurrent graph traversal such as tricolor marking <ref type="bibr" target="#b18">[18]</ref> where three colors (i.e., white, gray, and black) are used to present the traversal status of each node. This scheme solves the problem by changing the color of node A from black (i.e., traversal done) to gray (i.e., traversal ongoing) when node D is attached to node A as shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>. Since it takes advantage of knowledge on the data structure such as pointers to child nodes and the number of child nodes, this scheme is not easily generalized for atomic multi-word read operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hardware-assisted Memory Snapshot</head><p>Memory snapshot provides support for atomic multi-word read operations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. A "snapshot" of m memory elements is created to provide a consistent view for p processors. Then, the processors are allowed to execute two operations: update to write a memory element in the snapshot and scan to read memory elements. The scan operation is an atomic read operation on the memory elements and produces a consistent copy of them at the moment the scan is executed. Memory snapshot solves the race problem easily without synchronization as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(e). A memory snapshot is taken (i.e., scan) on all nodes of the graph. The profiling thread reads the pointer from node C to node D in the snapshot image while the application thread modifies the pointer in the up-to-date image. Unfortunately, despite the great potential as a programming primitive for concurrent programming, memory snapshots have been implemented in software and have performance issues such as O(mp) update time <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or O(m) scan time <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">22]</ref>, which prevent them from being adopted in a wide range of applications.</p><p>We propose fast memory snapshot with hardware assistance for easy concurrent programming without synchronization code. It accelerates snapshot operations with hardware resources to provide O(1) update (as fast as a single memory write operation) and O(p) scan in O(m) space. The O(1) update time enables application threads to write to the up-to-date image without performance degradation. The O(p) scan time allows snapshots to be used for large datasets in performance-oriented programs. We use the cache as a buffer for snapshot data and additional bits per cache line for snapshot metadata. A consistent snapshot image is taken with interprocess communication and maintained with cache coherence protocol support. Using hardware acceleration, memory snapshot can be applied to performance-oriented software system, including the following:</p><p>• Fast Concurrent Checkpoint: Process state is checkpointed for backward recovery with checkpoint in fault tolerance mechanisms <ref type="bibr" target="#b4">[5]</ref> or for guest OS migration in virtual machines <ref type="bibr" target="#b29">[36]</ref>. The fast memory snapshot is used to create the backup memory image concurrently without slowing down the applications (due to O(1) update time). Logging threads run in parallel with application threads to log the checkpointed image into disks.</p><p>• Concurrent Garbage Collection: Concurrent garbage collection typically incorporates sophisticated algorithms to deal with races between mutators and collectors and increases code management cost <ref type="bibr" target="#b12">[12]</ref>. With fast memory snapshots, concurrent garbage collection can be as simple as a stop-theworld garbage collector by taking a snapshot of part of the heap and collecting garbage from the snapshot image concurrently.</p><p>• Concurrent Memory Profiling: As shown in the example in the previous section, concurrent memory profilers <ref type="bibr">[27,</ref><ref type="bibr">28]</ref> benefit from fast memory snapshot by traversing a consistent object reference graph in the snapshot image.</p><p>• Concurrent Call-Path Profiling: Just-In-Time (JIT) compilers in Java virtual machines and the C# runtime system optimize the application code by finding hot execution paths with call-path profiling <ref type="bibr" target="#b13">[13]</ref>. With fast memory snapshot, compilers take a snapshot of a thread stack periodically and analyze the stack in parallel with application threads.</p><p>• Fast Copy-On-Write (COW): COW is used for shared virtualto-physical page mappings in fast fork() <ref type="bibr" target="#b27">[32]</ref> and for disk block sharing between virtual machines <ref type="bibr">[34]</ref>. The performance problem with COW is that the thread modifying shared data is stalled until a safe copy of the data is made. With fast memory snapshot, the thread takes a snapshot of the data and modifies the data without being stalled for the copy to be made. The copy is made in parallel from the snapshot image.</p><p>• In-memory Database: Unlike traditional database systems, in-memory database systems achieve good performance by loading the whole data schema into main memory <ref type="bibr" target="#b14">[14]</ref>. The snapshot image on the in-memory data schema can be used to generate reports on data usage, to replicate the data in a database cluster, and to support the isolation-level necessary for database transactions.</p><p>In summary, fast memory snapshot is useful for applications that need atomic multi-word read operations to obtain a recent and consistent view of various data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FAST MEMORY SNAPSHOT DESIGN</head><p>We propose MShot, a hardware-assisted memory snapshot for atomic multi-word read operations on arbitrary datasets. We first present the definition and programming interface for fast memory snapshot and then explain the hardware mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition and Interface</head><p>MShot provides a snapshot of the memory image at a certain point in time. Multiple disjoint address regions can be part of a snapshot. Once a snapshot is taken on these regions (i.e., equivalent to a scan operation of software snapshots), MShot maintains two memory images: the master image with the up-to-date data and the snapshot image. To read from the snapshot image, threads first join the snapshot. Any number of threads can join the snapshot. Until the user threads leave the snapshot, normal read operations to the snapshot regions return the snapshot data. The separation of taking a snapshot from joining the snapshot makes it possible to take a snapshot in a performance critical section and analyze the image later using additional cores. A snapshot is destroyed after all user threads have left the snapshot. A snapshot image in MShot is read-only like software snapshots <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>. MShot also captures the associated core register values at the moment the snapshot is taken. This is useful for analyzing the image later on. There can be multiple snapshots active at any moment. To simplify the hardware requirements, MShot does not allow overlapping snapshot regions and only supports cache-line granularity in specifying regions. <ref type="table" target="#tab_1">Table 1</ref> shows the programming interface of MShot to take, join, leave, and destroy a snapshot. A simple data structure called snapshot info shown in <ref type="figure">Figure 2</ref> is used to set up a snapshot. A unique snapshot ID (SID) is assigned to a newly taken snapshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation</head><p>There are three key software and hardware components in MShot implementation as shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The Snapshot Information <ref type="table">Table (</ref>SIT) shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a) is a doubly-indexed hash table for managing all snapshot information. This software structure is indexed either by SID or by virtual address. SIT entries, called Snapshot Information Blocks (SIB), are similar to the snapshot info structures passed to take snapshot(). join list is a linked list of the threads that joined a snapshot. MShot increments a simple counter atomically to generate a unique SID.</p><p>The Snapshot information Look-aside Buffer (SLB) shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b) is a small 64-entry hardware cache for accelerating the retrieval of snapshot information from the SIT. Each entry encodes information about a snapshot region. The SLB is accessed in parallel with the TLB. A matching SLB entry returns two fields: the SID to which the snapshot region belongs and the J (join) bit indicating if the current thread has joined the snapshot. The SLB maintains an OV(overflow) bit to indicate that there is a SLB entry evicted due to capacity issues. If the bit is not set, an SLB miss is ignored since there is no snapshot region associated with the memory address. If the bit is set, the miss is handled by a software refill handler that accesses the SIT. To support the software handler, two SLB instructions are added: SLBI to invalidate SLB entries and SLBLD to load them. SLBI and SLBLD are similar to TLBI and TLDLD in PowerPC <ref type="bibr" target="#b26">[30]</ref>.</p><p>Two Snapshot Metadata bits and SID bits are added per cache line for data versioning. MS (Modified since Snapshot) bit is set when a cache line in a snapshot region has been modified since the snapshot is taken. RS (Read from Snapshot) bit is set when a cache line in a snapshot region is refilled with old data from the snapshot image. If both bits are not set for a cache line, it implies that the master image and the snapshot image have the same data for the line. A 6-bit SID is set when either the MS bit or RS bit is set to indicate the snapshot for which the bits are set. The SID bits are set to 0 for the cache lines unrelated to a snapshot.</p><p>Take snapshot() initiates the process of taking a snapshot. An SIB is created for the snapshot by copying snapshot info to the block and inserting it into the SIT. MShot ensures that all cores are aware of the snapshot information before the call returns. This guarantees any write to the snapshot after the snapshot is taken triggers data versioning to build the snapshot image gradually, avoiding O(m) scan time. The snapshot information is exchanged by using the three-way handshake shown in <ref type="figure" target="#fig_3">Figure 4</ref>. First, snapshot request messages are sent via inter-core signals to the other threads, invoking snapshot signal handlers. The handlers copy the saved register values of the threads to a pre-allocated data structure pointed to by the saved regs field of the snapshot info argument, and load the snapshot information into the SLB. Next, the handlers send re-  // optional. Thread id to isolate selectively } <ref type="figure">Figure 2</ref>: snapshot info data structure used for take snapshot().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><note type="other">Method Function Snapshot take snapshot (snapshot info*) Take a snapshot on the address regions specified by snapshot info.snapshot regions.</note><p>sponse messages back to the thread that initiated the snapshot and wait for resume messages. On receiving the response messages from all the handlers, the initiating thread sends a resume message to terminate the handlers and resume application threads. This process has O(p) complexity.</p><p>Join snapshot() updates the SIB to remember that the thread is a new user of the snapshot. The corresponding SLB entry is reloaded to set the J bit for the thread.</p><p>Cache operations with the snapshot metadata bits are summarized in <ref type="table" target="#tab_2">Table 2</ref>. If a user thread writes to a snapshot, an exception is triggered since the user thread is only allowed to read from the snapshot. The SLB detects this case. A read from the snapshot is a hit if there is a cache line with a matching address tag and its MS bit is 0. This indicates that the line has not been modified since the snapshot was taken. If it is a miss, the SID bit and J bit are piggybacked to the refill request to indicate that the refill should come from the snapshot image. On receiving the refill request, other caches search for a cache line with matching address tag whose MS bit is 0. If the MS bit of the cache line with matching address is 1, the MS bit is attached to the response message to notify the requester that the master image has deviated from the snapshot for that address. Finding no matching cache line in other caches, the request is sent to main memory and the data is refilled. When refilling the cache line, we set the RS bit if the MS bit is piggy-backed. The RS bit is used later when destroying the snapshot to see if the line is to be invalidated.</p><p>A read from or a write to the snapshot by a thread not using the snapshot (i.e., J bit is 0) is a hit if there is a cache line with a matching address tag and the RS bit is 0. This indicates that the line does not contain the old data from a snapshot. If it is a write hit, the MS bit is set to indicate that the line no longer belongs to the snapshot image. This indication is accomplished systemwide by simply setting the MS bit since the line is exclusive to that processor. This makes updates O(1). If the write misses, the MS bit is set after the line is refilled. A cache miss is handled similar to the case of a thread that has joined the snapshot. The SID and J bit are piggy-backed on cache line refill requests. The bits are used by the other caches to find a cache line whose address tag matches and whose RS bit is 0.</p><p>Since the cache capacity is limited, a long-lived snapshot may cause cache overflow. Snapshot virtualization mechanism is explained in conjunction with transactional memory in the next section.</p><p>Leave snapshot() is called to stop using the snapshot. The ID of the thread is removed from the SIT and the SLB entries are reloaded to clear the corresponding J bits.</p><p>Destroy snapshot() destroys the snapshot. It starts with invalidating the SLB entries. Cache lines with the SID of the snapshot are invalidated if the RS bit is set. All metadata bits of the snapshot are gang-cleared. It completes by removing the SIB of the snapshot from the SIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESOURCE SHARING WITH TRANSACTIONAL MEMORY</head><p>Some astute readers would have already noticed the similarity between the MShot implementation and a typical cache-based hardware TM system (HTM). The similarity comes from the fact that they use essentially the same mechanism for data versioning where they both use the cache as a buffer for multiple data versions and add metadata bits per cache line for version information. Expecting that HTMs will be adopted widely in the near future, we present the base hardware TM system assumed in this paper and explain how MShot uses the hardware resources for HTM. At the end of this section, we calculate the hardware cost shared by MShot and HTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Hardware TM System</head><p>Hardware TMs use hardware resources to accelerate transactional execution of a group of instructions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. The baseline hardware TM assumed in this paper records transactional loads and stores with two TM metadata bits per cache line: R bit for loads and W bit for stores <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. They are used to distinguish transactional data and non-transactional data. Transactional data are buffered in the cache <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. It supports fast context switching of transactional code using transaction IDs per cache line <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. HTM systems are limited by the capacity of hardware caches. The baseline HTM system uses Page-based TM (PTM) <ref type="bibr" target="#b8">[9]</ref> to deal with the cache overflow. PTM supports the features necessary for MShot such as paging of overflowed pages and has a reasonable incremental hardware cost as will be shown in Section 4.3. PTM maintains a software shadow page table with the hardware table access logic located next to the memory controller <ref type="bibr" target="#b8">[9]</ref>. When a cacheline with transactional data or metadata is evicted, PTM allocates a new page (shadow page) to store the last committed version of the data and uses the old page (home page) to store the overflowed data. The shadow page table maintains the proper mapping information.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MShot using TM Hardware Resources</head><p>The key idea to support memory snapshot with HTM resources is to map snapshot read/write operations to transactional read/write operations properly. We map a memory snapshot to a read-only transaction that never aborts until the end of the transaction. The values read from the snapshot image are included in the read-set of the read-only transaction. Write operations to the snapshot regions are handled as transactional stores. The conflicts against the readonly transaction are ignored since the snapshot excludes the new updates. Moreover, regardless of the conflicts, the cache lines with the snapshot data are invalidated eventually by aborting the readonly transaction when the snapshot is destroyed.</p><p>To realize this mapping, MShot uses the two TM metadata bits for the two snapshot metadata bits, the transaction ID field for the SID, the shadow page table for overflowed snapshot metadata, and the shadow pages for overflowed snapshot data. <ref type="table" target="#tab_3">Table 3</ref> summarizes the resource mapping between the base HTM and MShot. MShot maps the MS bit to the W bit and uses it to distinguish the master image from the snapshot image. Similarly, the RS bit is mapped to the R bit to mark old data from the snapshot image that is invalidated when the snapshot is destroyed. Additional combinational logic is used for the bit control as explained in Section 3.2. SID is mapped to transaction ID to allow multiple snapshots to share the cache. MShot uses the gang-clear logic to reset the metadata bits when a snapshot is destroyed.</p><p>The shadow page table is used to deal with cache overflow of snapshot data. MShot uses the home pages to store the master image and the shadow pages to buffer the snapshot image. When a cache line with an MS bit (i.e., up-to-date data) is evicted, the MS bit and SID of the line are piggy-backed. The shadow page table perceives the SID as the transaction ID and the MS bit as the W bit. The snapshot data of the cache line is copied to the shadow page and the evicted up-to-date data of the master image goes to the home page. As in PTM, shadow pages are from a pool of preallocated pages. MShot evicts a cache line with the RS bit (i.e., snapshot data) silently as it is part of the snapshot image in the main memory. To reload an overflowed cache line, the hardware attaches the J bit and SID bits with the refill request. The shadow page table uses the J bit to select if the home page or the shadow page is read. If the bit is 1, the shadow page is read. If not, the home page is read. When a snapshot is destroyed, MShot sends out the transaction commit message with the SID of the snapshot telling the shadow page table that it is the end of the read-only transaction. Then, the shadow page table releases the shadow pages that contain the snapshot image. All in all, <ref type="figure" target="#fig_4">Figure 5</ref> shows the hardware resources shared with HTM in dark gray and the MShot-specific hardware components in light gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Write to snapshot</head><p>Read from snapshot Snapshot Trigger exception. Hit if address tag matches and MS bit is not set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Threads</head><p>Set RS bit and SID when refilled with MS bit piggy-backed. The Rest Hit if address tag matches and RS bit is not set. Hit if address tag matches and RS bit is not set. Set MS bit and SID when hit or refilled.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware Cost Saving</head><p>The hardware cost for the MShot implementation consists of the combinational logic for control and the storage overhead for data and metadata. Since the complexity of the combinational logic is hard to quantify, we focus on the storage overhead in this paper. As a baseline system, we assume a 16-core CMP with 64KB 4-way private L1 cache, 1MB 8-way private L2 cache with 32B line size, and 16GB main memory. This configuration is used for evaluation in Section 5 as well. The storage overhead of the additional bits in cache is about 34 KBytes per core, from (2b for snapshot metadata + 6b for SID) * ((64KB + 1MB)/32B). The storage requirements for SLB per core is about 1 KBytes, from (64b for start address + 64b for end address + 6b for SID + 1b for J bit) * 64 entries. Then, the storage overhead of the SLB and additional bits in the cache for 16 cores is about 560 KBytes. The majority of the PTM hardware cost for MShot is from the <ref type="table" target="#tab_2">2048-entry shadow page table cache  in the hardware table access logic. The shadow page table entry</ref> consists of physical page numbers for a home page and a shadow page, a valid bit, and read/write summary vectors. As the physical address length is 34 bits for 16GB main memory, the total storage overhead of the table is about 75 KBytes, from (2*22b for home-/shadow physical page number + 1b for valid bit + 2*(4K/32B)b for read/write summary vectors) * 2048 entries. The storage size of the cache structure in the baseline system is about 20.8 MBytes including bits for physical tags (19 bits for L1 and 16 bits for L2), coherence (3 bits), and ECC (4 Bytes).</p><p>Overall, the total storage overhead of the hardware resources used for MShot is 2.98%, from (560KB + 75KB)/20.8MB. However, given the baseline HTM, the additional storage overhead is only 0.07%, from (16 * 1KB SLB)/20.8MB. This shows clearly the benefits of resource sharing between MShot and HTM. If the baseline HTM does not have a PTM, the MShot implementation adds the shadow page table for itself. However, the total storage overhead goes up only to 0.43% from (16KB + 75KB)/20.8MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">System Issues</head><p>Like the TLB, the SLB must be reloaded on context-switches because it is indexed by virtual addresses. The SLB entries do not need to be saved since the SIT is always more up-to-date than the SLB. On rescheduling, the SLB can be loaded either eagerly or lazily. We use the eager approach in our evaluation.</p><p>On  shadow page table and swaps the shadow page out, too. The two pages are paged in together when the page is accessed again.</p><p>Since our baseline TM system has only two TM metadata bits, user transactions are not supported once the bits are used up by MShot. In order to support user transactions together with snapshots, the baseline TM system needs to support nested transactions with at least two pairs of TM metadata bits per cache line <ref type="bibr" target="#b22">[23]</ref> to dedicate a pair to user transactions and the other pair to MShot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System and Applications</head><p>We implemented MShot on a cycle-accurate simulator and evaluated it with the parameters in <ref type="table" target="#tab_5">Table 4</ref>. We used nine applications and one micro benchmark. W3M is a client-side web browser <ref type="bibr">[35]</ref>. Pypy is a python interpreter <ref type="bibr">[31]</ref>. Gzip is a compression tool <ref type="bibr">[16]</ref>. Mpeg2 is a MPEG-2 decoder <ref type="bibr" target="#b2">[3]</ref>. Cfrac performs continuous fraction factorization for integers <ref type="bibr" target="#b5">[6]</ref>. Nullhttpd is an HTTPD web server <ref type="bibr">[29]</ref> . Vacation mimics an e-commerce system <ref type="bibr" target="#b6">[7]</ref>. Mp3d and Radix are from SPLASH2 <ref type="bibr" target="#b31">[38]</ref>. RBtree is a micro benchmark that adds, searches, and deletes objects into and from a red-black tree in transactions.</p><p>We evaluate MShot with garbage collection and call-path profiling. Snapshot GC takes advantage of the fact that "once garbage, always garbage." If a garbage object is found in the snapshot image, it is garbage in the master image as well. It takes a snapshot on the memory regions to collect and has collectors find garbage from the snapshot image while application threads modify the master image. Snapshot call-path profiling periodically takes a snapshot of the thread stack and walks the stack to obtain information on the call graph of functions in the application code. We compare snapshot GC with parallel GC that stops the world and collects garbage with multiple collectors in parallel <ref type="bibr" target="#b32">[39]</ref>  is compared with single-threaded call-path profiler that stops the world and analyzes the stacks <ref type="bibr" target="#b13">[13]</ref>. The runtime overheads from garbage collection and profiling are normalized to the application execution time without running GC and profiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Snapshot GC</head><p>We used seven applications and a micro-benchmark for the GC tests. To observe meaningful behavior within a reasonable simulation time, a 32MB heap is used. While this is smaller than what a real environment would use, the applications still exhibit reasonable ratios of GC time over the total execution time (from 1% for Cfrac to 18% for Gzip). We also show the result from running Mpeg2 with a 128MB heap. <ref type="figure">Figure 6</ref> presents the runtime overheads added to the application execution time with and without snapshot. They are normalized to the execution time when running without GC by using a very large heap. Lower bars are better. In each bar, Stop is the time spent for stopping the system to start GC, Mark the time for the mark phase, Reclaim for the reclaim phase, and Snapshot the time to initiate a snapshot. App time is the time spent in the application itself. The MShot bar represents snapshot-based GC while the Para bar represents the parallel GC (non-concurrent).</p><p>For all applications except Nullhttpd and RBtree, snapshot GC eliminates most of the runtime overhead experienced with parallel GC. Nullhttpd and RBtree show an increase of the application execution time itself due to the increased memory contention with GC. On average, snapshot GC reduces the overhead down to 1.5%, which makes garbage collection essentially cost-free. Snapshot GC scales well with a 128MB heap for Mpeg2. <ref type="table" target="#tab_9">Table 5</ref> shows the memory requirements to maintain the overflowed data for the snapshot-based GC. PTM uses additional physical pages for shadow pages even if a single cache line overflows within each page. In the table, Snapshot Data is for the actual overflowed snapshot data at cache line granularity and Shadow Page is for the number of shadow pages times the 4-KByte page size. For all applications except Nullhttpd and Mpeg2, the memory requirements for snapshot data is under 1 MByte. The worst case (Nullhttpd) is still under 7 MBytes for the 16-core CMP. Moreover, the page mappings for the shadow pages fit completely into the 2048-entry shadow page table. <ref type="figure" target="#fig_6">Figure 7</ref> shows the runtime overhead, due to call-path profiling, which is normalized to the application execution time without profiling. Prof is the time for running the profiler, App for the execution of the application itself, and Snap the time to take a snapshot. The profiler is triggered 50K cycles after the previous profiling has completed. Gzip, Pypy, and W3M experience the largest performance improvements with snapshot profiling ranging from 68% (Pypy) to 75% (Gzip) due to the deep call graph (average depth of 14). This result is important for programs written in object-oriented language since they typically have deeper call depth. On the contrary, Mp3d, Radix, and Cfrac have an average depth of 4 to 6. Regardless of the applications' call depth, the snapshot call-path profiler adds negligible overhead to applications (less than 3%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Snapshot Call-path Profiler</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>We have discussed general alternatives to hardware-assisted memory snapshot in Section 2.1. Beyond that, there are more applicationspecific solutions for concurrent programs. Several software implementation schemes have been proposed recently for mostly concurrent and parallel garbage collectors <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b32">39]</ref>. While they are competitive to MShot in terms of low runtime overhead, MShot allows for algorithmic simplicity and easy code management as well. There are recent advances in dynamic profiling such as SuperPin and Shadow Profiling <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">37]</ref>. However, the SuperPin paper reports overheads of 100% because of heavy-weight cloning, forking, and DBT overheads. Shadow Profiling shows lower overheads but does much simpler profiling. (e.g. count basic blocks, rather than actually scanning through heap and stack).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We propose MShot, hardware-assisted memory snapshot. MShot </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Application Sequential Application</head><p>Figure 6: The graph shows the runtime overhead caused by parallel GC (Para) and snapshot GC (MShot). The overhead is normalized to the application execution time without GC. Parallel GC stops the applications, and runs with 10 cores. Snapshot GC concurrently runs with 2 cores. Sequential applications run with 1 core. Parallel GC stops the applications, and runs with 2 cores. Snapshot GC runs concurrency with 1 core.  allows programmers to read a large dataset atomically and process the snapshot image concurrently without synchronization code in multithreading environment. MShot is implemented in a costeffective manner by using TM hardware resources. The evaluation result shows that MShot allows garbage collectors and call-path profilers to run in parallel with user applications on spare cores with negligible interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>We appreciate Karin Strauss and Nick George for their insightful comments on this work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Concurrency Issues and Synchronization Schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MShot hardware and software structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three-way handshaking for snapshot initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 shows</head><label>5</label><figDesc>Figure 5 shows the hardware resources of the base HTM system in dark gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Resource sharing between HTM and MShot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The runtime overhead caused by sequential call-path profiler (Seq) and snapshot call-path profiler (MShot). It is normalized to application execution time without profiling. For the sequential call path profiler, we use 1 core for both application and profiling. For the snapshot call-path profiler, we concurrently use 1 core for the application and 1 core for profiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : MShot software interface.</head><label>1</label><figDesc></figDesc><table>snapshot_info { 
short SID; 
// unique snapshot id 
long[][2] * snapshot_regions; 
// array of (start address, end address) pairs 
void * saved_regs; 
// saved register values 
short TID; 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Memory operations with snapshot.</head><label>2</label><figDesc></figDesc><table>HTM resource 
Usage in MShot 
Transactional metadata bits per cache line 
Snapshot metadata bits per cache line 
Transaction ID (TID) per cache line 
Snapshot ID (SID) per cache line 
Transactional metadata bit gang clear logic 
Snapshot metadata bit gang clear logic 
Shadow page table in SW, table access logic in HW 
Providing access to master/snapshot images 
Home/Shadow pages 
Containing Master/Snapshot images separately 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Hardware resource mapping between HTM and MShot.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>paging, MShot flushes out the page from the cache to main memory intentionally. The flush causes PTM to generate the shadow page if needed. After paging out the home page normally, the OS checks if there is an associated shadow page by referencing the</figDesc><table>Feature 
Description 
CPU 
2GHz, single-issue, in-order x86 core 
SLB 
64 entries 
L1 Cache 
64 KB, 4-way, 32B line, MESI, write-back 
1 cycle hit time, private 
L2 Cache 
1 MB, 8-way, 32B block, MESI, write back 
10 cycle latency, private, 8 banks 
bit vector of sharers 
Shadow PT 
2048 entries 
Memory 
4 GB, 100 cycle latency 
Interconnect Tiled network, 32B links, 3 cycles per hop 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Parameters for the simulated CMP system.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>. Snapshot call-path profiler</head><label></label><figDesc></figDesc><table>Computing Core 

TLB 

Snapshot 
metadata 
control 
logic 

Load/Store 
data 

Virtual 
address 

SID, J 

Physical 
address 

Piggy-backed MS 

Coherence 
request 
data 
request/response 

Shared by 
HTM and 
Mshot 

Unique to 
Mshot 

SLB 

... 

Data Cache 

TID 
W1 R1 
Data 
V D E Addr_Tag Wn Rn 

On-chip Interconnect 

. . 

Memory Controller 
Shadow 
Page Table 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Memory requirement to manage overflowed snapshot data.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Atomic snapshots of shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Afek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Attiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="873" to="890" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Composite registers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODC &apos;90: Proc. of the 9th ACM symp. on Principles of distributed computing</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time parallel mpeg-2 decoding in software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Intl. Parallel Processing Symp</title>
		<meeting>11th Intl. Parallel essing Symp</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making the fast case common and the uncommon case simple in unbounded transactional memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Processor and memory-based checkpoint and rollback recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recent progress and prospects for integer factorisation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COCOON &apos;00: Proc. of the 6th Intl. Conf. on Computing and Combinatorics</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Effective Hybrid Transactional Memory System with Strong Isolation Guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trautmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 34th Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bulk Disambiguation of Speculative Threads in Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 33rd Intl. Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbounded Page-Based Transactional Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 12th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Common Case Transactional Behavior of Multithreaded Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
		<title level="m">on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lazy database replication with snapshot isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daudjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;06: Proc. of the 32nd intl. conf. on Very large data bases</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Garbage-first garbage collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detlefs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Flood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM &apos;04: Proc. of the 4nd intl. symp. on Memory management</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-overhead call path profiling of unmodified, optimized code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Froyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;05: Proc. of the 19th intl. conf. on Supercomputing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Main memory database systems: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable algorithms for global snapshots in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS &apos;06: Proc. of the 20th intl. conf. on Supercomputing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transactional Memory Coherence and Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 31st Intl</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Symp. on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The incremental garbage collection of processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1977 Symp. on Artificial Intelligence and Programming Languages</title>
		<meeting>of the 1977 Symp. on Artificial Intelligence and Programming Languages</meeting>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transactional Memory: Architectural Support for Lock-Free Data Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 20th Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="1993-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sapphire: copying gc without stopping the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JGI &apos;01: Proc. of the 2001 joint ACM-ISCOPE conf. on Java Grande</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An optimal multi-writer snapshot algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jayanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;05: Proc. of the 37th ACM symp. on Theory of computing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Architectural Semantics for Practical Transactional Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 33rd Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Log-Based Transactional Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bobba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 12th Intl. Conf. on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supporting Nested Transactional Memory in LogTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Moravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bobba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proc. of the 12th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shadow profiling: Hiding instrumentation costs with parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO &apos;07: Proceedings of the Intl. Symp. on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">PowerPC Assembler Reference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effects of copy-on-write memory management on the response time of UNIX fork operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metat-data snapshotting: A simple mechanism for file system consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SNAPI &apos;03: Proc. of Intl. Workshot on Storage Network Architectur and Parallel I/O</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Memory Resource Management in VMware ESX server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">SI</biblScope>
			<biblScope unit="page" from="181" to="194" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Superpin: Parallelizing dynamic instrumentation for real-time performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO &apos;07: Proceedings of the Intl. Symp. on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The SPLASH2 Programs: Characterization and Methodological Considerations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Intl. Symp. on Computer Architecture</title>
		<meeting>of the 22nd Intl. Symp. on Computer Architecture<address><addrLine>Santa Margherita, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Task-pushing: a scalable parallel gc marking algorithm without synchronization operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS &apos;07: Proc. of intl. Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
