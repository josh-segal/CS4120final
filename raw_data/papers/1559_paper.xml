<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantification of Integrity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
							<email>clarkson@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantification of Integrity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Integrity</term>
					<term>quantitative information flow</term>
					<term>informa- tion theory</term>
					<term>database privacy</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Two kinds of integrity measures-contamination and suppression-are introduced. Contamination measures how much untrusted information reaches trusted outputs; it is the dual of information-flow confidentiality. Suppression measures how much information is lost from outputs; it does not have a confidentiality dual. Two forms of suppression are considered: programs and channels. Program suppression measures how much information about the correct output of a program is lost because of attacker influence and implementation errors. Channel suppression measures how much information about inputs to a noisy channel is missing from channel outputs. The relationship between quantitative integrity, confidentiality, and database privacy is examined.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many integrity requirements for computer systems are qualitative, but quantitative requirements can also be valuable. For example, a system might combine data from trusted and untrusted sensors if the untrusted sensors cannot corrupt the result too much. As another example, we might add noise to a database, thereby protecting privacy, if the resulting anonymized database still contains enough uncorrupted information to be useful for statistical analysis. Yet methods for quantification of corruption-that is, damage to integrity-have received little attention to date, whereas quantification of information leakage has been a topic of research for over twenty years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>To quantify corruption, a formal definition of "integrity" is required. We know of no such widely accepted definition, although the widely accepted informal definition seems to be "prevention of unauthorized modification of information" <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b7">[8]</ref>. So we take two distinct notions of information modification as points of departure: taint analysis and program correctness. These lead to two distinct measures of corruption that we name "contamination" and "suppression."</p><p>Contamination generalizes taint analysis <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b13">[13]</ref>, which tracks information flow from tainted inputs to outputs that are supposed to be untainted-or, using alternative terminology, from untrusted inputs to outputs that are supposed to be trusted. This flow results in contamination of the trusted outputs. Trusted outputs are not supposed to be influenced by untrusted information, so contamination corrupts integrity. We might be willing to deem a program secure if it allows only limited contamination, even though taint analysis would deem the same program to be insecure. So quantification of contamination would be useful.</p><p>Flow between untrusted and trusted objects was first studied by Biba <ref type="bibr" target="#b14">[14]</ref>, who identified a duality between models of integrity and confidentiality. The confidentiality dual to contamination is leakage, which is information flow from secret inputs to public outputs. Previous work has developed measures of leakage based on information theory <ref type="bibr" target="#b15">[15]</ref> and on beliefs <ref type="bibr" target="#b16">[16]</ref>. This paper adapts those measures to contamination. <ref type="bibr" target="#b0">1</ref> Through the Biba duality, we obtain a measure for corruption from a measure for leakage.</p><p>Suppression, our other measure for corruption, is derived from program correctness. For a given input, a correct implementation should produce an output o permitted by a specification. The output might be permitted to differ from o provided the output conveys the same information as o. An implementation might, for example, produce all the bits in the binary representation of o but in reverse order. Or the implementation might produce o xor k, where k is a known constant. Any knowledgeable user of these implementations could recover o from the implementation's output.</p><p>Conversely, the output of an incorrect implementation would fail to convey all the information about o. For example, a (somewhat) incorrect implementation might output only the first few bits of o; or it might output o with probability p and output garbage with probability 1−p; or it might output o xor u, where u is an untrusted input. In each case, we say that program suppression of information about the correct output has occurred. Implementation outputs are not supposed to omit information about correct outputs, so program suppression corrupts integrity. Yet we might be willing to use an implementation that produces sufficient information about the correct output, hence exhibits little program suppression, even though a traditional verification methodology (e.g., Hoare logic) would deem the implementation to be incorrect. So quantification of program suppression would be useful.</p><p>The echo specification "o := t" gives rise to an important form of program suppression that we call channel suppression. The echo specification stipulates that a correct output o is the value of input t, similar to the Unix echo command. For the echo specification, our model of program suppression simplifies to the information-theoretic model of communication channels <ref type="bibr" target="#b18">[18]</ref>, in which a message is sent through a noisy channel. The receiver cannot observe the sender's inputs or the noise but must attempt to determine what message was sent. Sometimes the receiver cannot recover the message or recovers an incorrect message. For example, a noisy channel could be modeled by implementation "o := t xor u", in which noise supplied by the attacker in untrusted input u causes information about correct output t to be lost. The channel thus damages the integrity of information.</p><p>With programs and channels, suppression occurs when information is lost. This paper shows how to use information theory to quantify suppression, including how to quantify the attacker's influence on suppression. We start with channel suppression. Then we generalize to program suppression, giving both information-theoretic and belief-based definitions. Applying the Biba duality to these definitions yields no meaningful confidentiality dual. So the classical duality of confidentiality and integrity was, in retrospect, incomplete.</p><p>We might suspect that contamination generalizes suppression, or vice versa, but this is not the case. Consider the following three program statements, which take t as trusted input and u as untrusted input, and produce o as trusted output. Suppose that these statements are potential implementations of echo specification "o := t":</p><p>• o := (t, u), where (t, u) denotes the pair whose components are t and u. This program exhibits (only) contamination, because trusted output contains information derived from untrusted input. A user of this program's output might filter out and ignore contaminant u, but that's irrelevant: the contaminant damages integrity just by its presence, as in taint analysis. Contamination is concerned only with measuring the amount of contaminant in the output-not what the user does with the output. This program does not exhibit program suppression, because its output contains all the information about the value of t.</p><p>• o := t xor n, where n is randomly generated noise.</p><p>This program exhibits (only) program suppression, because information about the correct output is lost. Suppression concerns that loss; suppression is not concerned with the presence of contaminant. In fact, this program cannot exhibit contamination, because it has no untrusted inputs.</p><p>• o := t xor u. This program exhibits contamination, because untrusted information affects trusted output. This program also exhibits program suppression, because the noise of u causes information about the correct output to be lost.</p><p>So although contamination and suppression both are kinds of corruption, they are distinct phenomena. In addition to introducing measures for corruption, this paper examines the relationship between the informationtheoretic and belief-based approaches to quantifying information flow. We show that, for individual executions of a program, the belief-based definition is equivalent to an information-theoretic definition. And we show that, in expectation over all executions, the belief-based definition is a natural generalization of an information-theoretic definition.</p><p>Finally, we revisit work on database privacy. Databases that contain information about individuals are sometimes anonymized and published to enable statistical analysis. The goal is to protect the privacy of individuals, while still providing useful data for analysis. Mechanisms for anonymization suppress information-that is, integrity is sacrificed for confidentiality. Using our measure for channel suppression along with a measure for leakage, we are able to make this intuition precise and to analyze database privacy conditions from the literature.</p><p>We proceed as follows. Models for quantifying contamination and suppression are given in §II and §III. Belief-based definitions and their relationship to information-theoretic definitions are examined in §IV. Database privacy is analyzed in §V. Related work is discussed in §VI, and §VII concludes. All proofs appear in the accompanying technical report <ref type="bibr" target="#b19">[19]</ref>.</p><p>Basic notions from information theory (e.g., entropy and mutual information) are used throughout the paper. Their definitions can be found in the accompanying technical report <ref type="bibr" target="#b19">[19]</ref> or in any introductory text <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. QUANTIFICATION OF CONTAMINATION</head><p>Three agents are involved in our model of program execution: a system, a user, and an attacker. <ref type="bibr" target="#b1">2</ref> The system executes the program, which has variables categorized as input, output, or internal. Input variables may only be read by the program, output variables may only be written by the program, and internal variables may be read and written but may never be observed by any agent except the system itself. The user and the attacker supply inputs by writing the initial values of input variables. These agents receive outputs by reading the final values of output variables. The attacker is untrusted, whereas the user is trusted.</p><p>Our goal is to quantify the information about untrusted inputs that contaminates trusted outputs. This goal generalizes taint analysis, which just determines whether any information about untrusted inputs contaminates trusted outputs. We accomplish our goal by quantifying the information the user learns about untrusted inputs by observing trusted inputs and outputs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacker Attacker</head><p>User User u in u out t in t out <ref type="figure">Figure 1</ref>. Contamination model Definition: Contamination is the amount of information a user learns about untrusted inputs by observing trusted inputs and outputs.</p><p>Our use of terms "learning" and "observation" might suggest quantification of confidentiality. The resemblance is deliberate, because we seek a definition of integrity that is dual to confidentiality. In particular, our approach is dual to the technique of Clark et al. <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref> for quantifying leakage. <ref type="bibr" target="#b2">3</ref> The definition of contamination engenders two restrictions on the user's access to variables. First, the user may not directly read untrusted inputs. Otherwise, we would be quantifying something trivial-the amount of information the user learns about untrusted inputs by observing untrusted inputs. Second, the user may not read untrusted outputs, because we are interested only in the information the user learns from trusted outputs. In addition to these restrictions, we do not allow the user to write untrusted inputs. So the user may access only the trusted variables. Similarly, the attacker may access only the untrusted variables. <ref type="bibr" target="#b3">4</ref> These access restrictions agree with the Biba integrity model <ref type="bibr" target="#b14">[14]</ref>: they prohibit reading up (the user cannot read untrusted information) and writing down (the attacker cannot write trusted information). The resulting communication model for contamination is depicted in <ref type="figure">figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contamination in Single Executions</head><p>Information theory explains the behavior of channels. A channel, like a program, accepts inputs and produces outputs. So information flow can be quantified by modeling a program as a channel and using information theory to derive the amount of information transmitted over the channel.</p><p>A channel's inputs are characterized by a probability distribution of individual input events. Channels might be noisy and introduce randomness into output events, so a channel's outputs are also characterized by a probability distribution. Let t in , u in , and t out denote trusted input, untrusted input, and trusted output events. (Each event may <ref type="bibr" target="#b2">3</ref> Hence, readers familiar with that work will be unsurprised by our final definition of expected contamination in equation (6), and perhaps unsurprised by the development leading up to it. But we present the full development because it illuminates each step through the lens of integrity (rather than confidentiality), thus increasing confidence in our definitions. It also makes this paper self-contained. <ref type="bibr" target="#b3">4</ref> Flows from trusted to untrusted need not be prohibited. So the attacker could be allowed to read trusted inputs or outputs, and the user could be allowed to write untrusted inputs. But for simplicity, we don't consider those flows in this paper.</p><p>comprise the values of several input or output variables.) Also let T in , U in , and T out denote probability distributions on trusted inputs, untrusted inputs, and trusted outputs. <ref type="bibr" target="#b4">5</ref> Mutual information characterizes the quantity of information that can be learned about channel inputs by observing outputs. Let I(u in , t out ) denote the mutual information between events u in and t out -that is, the amount of information either event conveys about the other. <ref type="bibr" target="#b5">6</ref> Note that I(·, ·) is mutual information between single events, not the more familiar mutual information between distributions of events. Let I(u in , t out | t in ) denote the mutual information between events u in and t out , conditioned on the occurrence of event t in .</p><p>The quantity C 1 of contamination of trusted outputs by untrusted inputs in a single execution, given the trusted inputs, is defined as follows:</p><formula xml:id="formula_0">C 1 I(u in , t out | t in ).<label>(1)</label></formula><p>(The subscript 1 is a mnemonic for "single.") Consider the following program:</p><formula xml:id="formula_1">o T := i U xor j T<label>(2)</label></formula><p>Suppose that variables o T , i U , and j T are one-bit trusted output, untrusted input, and trusted input, respectively, and that the values of i U and j T are chosen uniformly at random. Intuitively, the user should be able to infer the value of i U by observing j T and o T , hence there is 1 bit of contamination. And according to definition (1) of C 1 , the quantity of contamination caused by program (2) is indeed 1 bit. For example, the calculation of I(i U = 0, o T = 1 | j T = 1) proceeds as follows: </p><formula xml:id="formula_2">I(i U = 0, o T = 1 | j T = 1) = − log Pr(i U = 0 | j T = 1)Pr(o T = 1 | j T = 1) Pr(i U = 0, o T = 1 | j T = 1) = − log ( 1 / 2 )( 1 / 2 ) 1 / 2 = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contamination in Sequences of Executions</head><p>Given C 1 , which provides a means to quantify contamination for single executions, we can quantify the contamination over a sequence of single executions. As an example, consider the following program, where operator &amp; denotes bitwise AND:</p><formula xml:id="formula_3">o T := i U &amp; j T<label>(3)</label></formula><p>Suppose that the attacker chooses a value for untrusted input i U and that the user is allowed to execute the program multiple times. The user chooses a potentially new value for trusted input j T in each execution, but the single value for i U is used throughout. Also, suppose that all variables are k bits and that i U is chosen uniformly at random. Intuitively, the contamination from this program in a single execution is the number of bits of j T that are set to 1. Thus, a user that supplies 0x0001 for j T learns 7 the least significant bit of i U (so there is 1 bit of contamination); 0x0011 yields the two least significant bits (2 bits of contamination), etc. But if a user executes the program twice, supplying first 0x0001 then 0x0011, the user learns a total of only 2 bits, not 3 (= 1 + 2). Directly summing C 1 for each execution provides only an inexact upper bound on the contamination. To calculate the exact amount of contamination for a sequence of executions, note the following. The untrusted input is chosen randomly at the beginning of the sequence. Each successive execution enables the user to refine knowledge of that untrusted input. So each successive calculation of contamination should use an updated distribution of untrusted inputs, embodying the user's refined knowledge about the particular untrusted input chosen at the beginning of the sequence. 8 Let U j be a random variable representing the user's accumulated knowledge in execution j about the untrusted input event, and let t j out and t j in be the trusted input and output events in that execution. The distribution of U j+1 is defined in terms of the distribution of U j :</p><formula xml:id="formula_4">Pr(U j+1 = u in ) = Pr(U j = u in | t j out , t j in ).<label>(4)</label></formula><p>So the updated distribution is obtained simply by conditioning on the trusted input and output. This conditioning is repeated after each execution. We thus obtain the following formula for the total contamination C in a sequence of executions:</p><formula xml:id="formula_5">C = j I(u j in , t j out | t j in ),</formula><p>where u j in is the untrusted input event in execution j, and mutual information I(·) is calculated according to distribution U j on untrusted inputs.</p><p>Returning to program (3), initial distribution U 1 on i U is uniform. But distribution U 2 , obtained by supplying 0x0001 as the first input, is uniform over i U that have the same least significant bit as j T . Thus, the user learns only 1 bit by supplying 0x0011 in the second execution. The total contamination according to C is exactly 2 bits for the sequence-which is what our intuition suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Contamination in Expectation</head><p>C 1 quantifies contamination in a single execution. It could be used at runtime by an execution monitor <ref type="bibr" target="#b24">[24]</ref> to constrain how much contamination occurs during program execution. We might, however, be interested in how much contamination occurs on average over all executions of a program-a quantity that might be conservatively bounded by a static analysis. We now turn our attention to that quantity.</p><p>The expected quantity C of contamination of trusted outputs by untrusted inputs, given the trusted inputs, is the expected value of C 1 :</p><formula xml:id="formula_6">C = E[C 1 ].<label>(5)</label></formula><p>The right-hand side of equation <ref type="formula" target="#formula_6">(5)</ref> can be rewritten as the mutual information I(U in , T out | T in ) between distributions U in and T out , conditioned on observation of T in . That yields our definition of expected contamination:</p><formula xml:id="formula_7">C I(U in , T out | T in ).<label>(6)</label></formula><p>Definition (6) of C yields an operational interpretation of contamination. In information theory, the capacity of a channel is the maximum quantity of information, over all distributions of inputs, that the channel can transmit. Shannon <ref type="bibr" target="#b18">[18]</ref> proved that channel capacity enjoys an operational interpretation in terms of coding theory: a channel's capacity is the highest rate, in bits per channel use, at which information can be sent over the channel with arbitrarily low probability of error. Therefore, the maximum quantity of contamination should also be the highest rate at which the attacker can contaminate the user. We leave investigation of this interpretation as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Leakage</head><p>Clark et al. <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b22">[22]</ref> define quantity L of leakage from secret inputs S in to public outputs P out , given public inputs P in , as follows:</p><formula xml:id="formula_8">L I(S in , P out | P in ).<label>(7)</label></formula><p>Replacing "untrusted" with "secret" and "trusted" with "public" in equation (6) yields equation <ref type="formula" target="#formula_8">(7)</ref>. Contamination and leakage are therefore information-flow duals: their definitions are the same, except the ordering of security levels is reversed. For example, the definition of C conditions on T in , which represents inputs provided by a user with a high security level (because the user is cleared to provide trusted inputs); whereas the definition of leakage conditions on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program</head><p>Attacker Attacker</p><p>Sender Receiver <ref type="figure" target="#fig_2">Figure 2</ref>. Channel suppression model P in , which represents inputs provided by a user with a low security level (because the user is not cleared to read secret inputs). So Biba's qualitative duality for confidentiality and integrity <ref type="bibr" target="#b14">[14]</ref> extends to these quantitative models.</p><formula xml:id="formula_9">u in u out t in t out</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. QUANTIFICATION OF SUPPRESSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Channel Suppression</head><p>To quantify channel suppression, we refine our model of program execution by replacing the user with two agents, a sender and receiver. The receiver, by observing the program's outputs, attempts to determine the inputs provided by the sender. The program models a channel in which inputs are messages, and the receiver attempts to determine what messages were sent. For example, the sender might be a database, and the program might construct a web page using queries to the database; the receiver attempts to reconstruct information in the database from the incomplete information in the web page. Information that cannot be reconstructed has been suppressed.</p><p>Definition: Channel suppression is the amount of information a receiver fails to learn about trusted inputs by observing trusted outputs.</p><p>As with contamination, the program receives trusted inputs as the initial values of variables and produces trusted outputs as the final values of variables. But now the sender writes the initial values of trusted inputs, and the receiver reads the final values of trusted outputs. These are the only ways that the sender and receiver may access variables. We continue to model an attacker, who attempts to interfere with trusted outputs by writing the initial values of untrusted inputs. For simplicity, the attacker is not allowed to read trusted inputs or outputs. This communication model for channel suppression is depicted in <ref type="figure" target="#fig_2">figure 2</ref>.</p><p>We first define channel suppression for single executions. As in our model of contamination, let t in and t out be trusted input and trusted output events. Since I(t in , t out ) is the quantity of information obtained about trusted inputs by observing trusted outputs, I(t in , t out ) is the quantity CT 1 of channel transmission from the sender to the receiver in a single execution:</p><formula xml:id="formula_10">CT 1 I(t in , t out ).<label>(8)</label></formula><p>Let I(t in |t out ) denote the information conveyed by the occurrence of event t in , conditioned on observation of the occurrence of t out . We can rewrite the right-hand side of equation <ref type="formula" target="#formula_10">(8)</ref>:</p><formula xml:id="formula_11">CT 1 = I(t in ) − I(t in |t out ).<label>(9)</label></formula><p>Quantity I(t in ) is the total information that the receiver could learn about the trusted input, and I(t in |t out ) is what remains to be learned after the receiver observes the trusted output. So I(t in |t out ) is the quantity of information that failed to be transmitted. <ref type="bibr" target="#b8">9</ref> Therefore, I(t in |t out ) is the quantity CS 1 of channel suppression in a single execution:</p><formula xml:id="formula_12">CS 1 I(t in |t out ).<label>(10)</label></formula><p>Although untrusted input u in does not directly appear in equations <ref type="formula" target="#formula_10">(8)</ref> or <ref type="formula" target="#formula_0">(10)</ref>, they do not ignore the attacker's influence on channel suppression: trusted output t out , which does appear, can depend on u in . Also, recall that the definition (1) of contamination C 1 conditions on u in ; equations <ref type="formula" target="#formula_10">(8)</ref> and <ref type="formula" target="#formula_0">(10)</ref> do not because the receiver cannot directly observe untrusted input-unlike the user, who could in the contamination model.</p><p>We next define channel suppression in expectation. Let I(T in , T out ) denote the mutual information between distributions T in and T out , and let H(T in |T out ) denote the entropy of distribution T in , conditioned on observation of T out . (As before, T in and T out are probability distributions on trusted inputs and trusted outputs.) By taking the expectation of CT 1 and CS 1 , we obtain the expected quantities of channel transmission CT and channel suppression CS: <ref type="bibr" target="#b10">10</ref> CT I(T in , T out ),</p><formula xml:id="formula_13">CS H(T in |T out ).<label>(11)</label></formula><p>These definitions account for the attacker's influence on channel transmission and channel suppression, because distribution T out depends on the attacker's distribution U in on untrusted inputs. Also, these definitions should yield an operational interpretation in terms of coding theory; we leave that interpretation as future work. <ref type="bibr" target="#b11">11</ref> As an example, consider the following program:</p><formula xml:id="formula_15">o T := i T xor rnd(1)<label>(13)</label></formula><p>Variables i T and o T are one-bit trusted input and output variables. Program expression rnd(x) returns x uniformly random bits. Suppose that trusted input distribution T in is uniform on {0, 1}. Then channel transmission CT is 0 bits <ref type="bibr" target="#b8">9</ref> Alternatively, the right-hand side of equation <ref type="formula" target="#formula_10">(8)</ref> could be rewritten as I(tout ) − I(tout |t in ). Perhaps this formula could also yield a measure for integrity, were we interested in backwards execution of programs-that is, computing inputs from outputs. <ref type="bibr" target="#b10">10</ref> Note that expected channel suppression CS is defined using entropy H, not using mutual information I, even though channel suppression CS 1 is defined using self-information I. This notational quirk is inherited from information theory and occurs because entropy-not mutual informationis the expectation of self-information. <ref type="bibr" target="#b11">11</ref> The basis of that interpretation would be the capacity of the channel from trusted inputs to trusted outputs (cf. §II-C). and channel suppression CS is 1 bit. These quantities are intuitively sensible: because of the bit of random noise added by the program, the receiver cannot learn anything about i T by observing o T .</p><p>Similarly, consider the following program:</p><formula xml:id="formula_16">o T := i T xor j U<label>(14)</label></formula><p>Variable j U is a one-bit untrusted input. Suppose that untrusted input distribution U in is uniform. Then program <ref type="formula" target="#formula_0">(14)</ref> exhibits the same behavior as program <ref type="formula" target="#formula_0">(13)</ref>: 0 bits of channel transmission and 1 bit of channel suppression. Because of the bit of random noise added by the attacker, the receiver cannot learn anything about i T by observing o T .</p><p>Programs <ref type="formula" target="#formula_0">(13)</ref> and <ref type="formula" target="#formula_0">(14)</ref> both cause 1 bit of channel suppression, but the source of that channel suppression is different. For program (13), the source is program randomness; for program <ref type="formula" target="#formula_0">(14)</ref>, it is the attacker. We develop definitions that distinguish between these sources, next.</p><p>1) Attacker-controlled channel suppression: Let CS P denote the quantity of channel suppression attributable solely to the program-that is, the quantity that would occur if the attacker's input were known to the receiver:</p><formula xml:id="formula_17">CS P H(T in | T out , U in ).<label>(15)</label></formula><p>This definition differs from definition (12) of channel suppression CS only by the additional conditioning on U in , which has the effect of accounting for the attacker's untrusted inputs. Any remaining channel suppression must come solely from the program. Define the quantity of channel suppression CS A under the attacker's control as the difference between the maximum amount of channel suppression caused by the attacker's choice of U in and the minimum (which need not be 0 because of channel suppression attributable solely to the program):</p><formula xml:id="formula_18">CS A max Uin (CS) − min Uin (CS).<label>(16)</label></formula><p>(CS is a function of T out , which is a function of U in , so quantifying over U in is sensible.) For program <ref type="formula" target="#formula_0">(13)</ref>, quantity CS P of program-controlled channel suppression is 1 bit, and quantity CS A of attackercontrolled channel suppression is 0 bits. The converse holds for program <ref type="bibr" target="#b14">(14)</ref>, which exhibits 0 bits of programcontrolled channel suppression and 1 bit of attackercontrolled channel suppression.</p><p>The following program exhibits both attacker-and program-controlled channel suppression:</p><formula xml:id="formula_19">o2 T := i2 T xor i2 U xor rnd(1)<label>(17)</label></formula><p>All variables in program (17) are two-bit. One bit of program-controlled channel suppression CS P is caused by the xor with rnd(1). But the attacker controls the rest of the channel suppression. If the attacker chooses i2 U uniformly at random, the channel suppression is maximized and equal to 2 bits; whereas if the attacker makes i2 U a constant (e.g., always "00"), the channel suppression is the minimal 1 bit caused by rnd(1). Calculating CS A yields 1 (= 2 − 1) bit of attacker-controlled channel suppression.</p><p>2) Error-correcting codes: An error-correcting code adds redundant information to a message so that information loss can be detected and corrected. One of the simplest errorcorrecting codes is the repetition code R n <ref type="bibr" target="#b25">[25]</ref>, which adds redundancy by repeating a message n times to form a codeword. For example, R 3 would encode message 1 as codeword 111. The code-word is sent over a noisy channel, which might corrupt the code-word; the receiver reads this possibly corrupted word from the channel. For example, the sender might send code-word 111, yet the receiver could receive word 101. To decode the received word, the receiver can employ nearest-neighbor decoding: the nearest neighbor of a word w is a code-word c that is closest to w by the Hamming distance. (The nearest neighbor is not necessarily unique for some codes, in which case an arbitrary nearest neighbor is chosen.) Treating words as vectors, Hamming distance d(w, x) between words w and x is the number of positions i at which w i = x i . For the repetition code, nearest-neighbor decoding means that a word is decoded to the symbol that occurs most frequently in the word. For example, word 101 would be decoded to code-word 111, thus to message 1; but word 001 would be decoded to message 0.</p><p>Consider the following program BSC, which models the binary symmetric channel studied in information theory:</p><formula xml:id="formula_20">BSC : w := m xor rnd p (n)</formula><p>Variable m, which contains a message, is an n-bit trusted input, and variable w, which contains a word, is an n-bit trusted output. Expression rnd p (x), in which p is a constant, returns x independent, random bits. Each bit is distributed such that 0 occurs with probability p and 1 occurs with probability 1 − p. (So rnd(x), used in program (13), abbreviates rnd 0.5 (x).) Thus, each bit of input m has probability 1 − p of being flipped in output w.</p><p>Suppose that n = 1 and that the distribution of trusted input m is uniform. Then the probability that BSC outputs w such that w = m holds is p. So quantity CS 1 of channel suppression is − log p. Next, suppose that the sender and receiver employ repetition code R 3 with program BSC. The sender encodes a one-bit input m into three bits and provides those as input to BSC (so now n = 3). The receiver gets a three-bit output and decodes it to bit w. Denote this composed program as R 3 (BSC). The probability that w = m holds is now p 3 + 3p 2 (1 − p), which can be derived by a simple argument. (See the accompanying technical report <ref type="bibr" target="#b19">[19]</ref>.) The amount of channel suppression CS 1 is thus − log(p 3 + 3p 2 (1 − p)), which is less than − log p for any p &gt; 1 2 . So for any p &gt; 1 2 (i.e., for any channel at least slightly biased toward correct transmission) the channel suppression from R 3 (BSC) is less than the channel suppression from BSC. We conclude that repetition code R 3 improves channel transmission. Although this conclusion is unsurprising, it illustrates that our theory of channel suppression suffices to re-derive a well-known fact from coding theory.</p><p>3) Channel suppression vs. contamination: Recall program (14), restated here:</p><formula xml:id="formula_21">o T := i T xor j U</formula><p>This program is essentially the same as program "o := t xor u" from §I. We previously analyzed program <ref type="bibr" target="#b14">(14)</ref> and determined that it exhibits 1 bit of channel suppression if T in and U in are uniform distributions on {0, 1}. We can also analyze the program for contamination: j T is supplied by a user, and o T is observed by that same user. Calculating C yields a contamination of 1 bit, indicating that the user learns all the (untrusted) information in i U . So this program exhibits both contamination and channel suppression, as we argued in §I.</p><p>You might wonder how a program with a one-bit output can exhibit both 1 bit of contamination and 1 bit of channel suppression. The answer is that contamination concerns injection of information (here 1 bit of untrusted information is injected), whereas suppression concerns loss of information (here 1 bit of trusted information is lost).</p><p>Also, recall program (13), restated here:</p><formula xml:id="formula_22">o T := i T xor rnd(1)</formula><p>This program is essentially the same as program "o := t xor n" from §I. We previously determined that program (13) exhibits 1 bit of channel suppression. Because there are no untrusted inputs, quantity C of contamination is 0. So this program exhibits only channel suppression, as we argued in §I. 12 4) Channel suppression and leakage: Recall that leakage <ref type="formula" target="#formula_8">(7)</ref> is the quantity L of information flow from secret inputs to public outputs. Leakage can be prevented by employing channel suppression. Consider a declassifier that accepts trusted, secret inputs and produces trusted, public outputs. The declassifier's task is to selectively release some secret information and suppress the rest. Whatever information is not leaked by the declassifier ought to have been suppressed.</p><p>That intuition is made formal by the following proposition. Let s denote a secret input event and let p denote a public output event. Let I(s) denote the self-information of event s. Let L 1 denote the leakage in a single execution of the declassifier and be defined as I(s, p); this definition follows from equation <ref type="formula" target="#formula_8">(7)</ref> by removing the conditioning on P in (since the declassifier has no public inputs) and <ref type="bibr" target="#b12">12</ref> These arguments implicitly assume that random number generator rnd(·) is trusted. Untrusted generators could also be modeled, but we don't pursue that here.  <ref type="figure">Figure 3</ref>. Program suppression model by removing the expectation over all executions from the definition of mutual information I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specification</head><formula xml:id="formula_23">Proposition 1. L 1 + CS 1 = I(s).</formula><p>So for a given probability distribution of high inputs, leakage plus channel suppression is a constant. Confidentiality is obtained by eroding integrity, and vice versa. Any security condition for declassifiers-we discuss some in §V-that requires a minimum amount of confidentiality thereby restricts the maximum amount of integrity. And any utility condition that requires a minimum amount of integrity thereby restricts the maximum amount of confidentiality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Program Suppression</head><p>We now generalize the idea of suppression from communication channels to program correctness. Consider a specification program, depicted in <ref type="figure">figure 3</ref>: the specification receives a trusted input t in from the sender and produces a correct, trusted output t spec for the receiver. This idealized program does not interact with the attacker. But in the real world, an implementation program that does interact with the attacker would be used to realize the specification. The implementation receives trusted input t in from the sender and untrusted input u in from the attacker; the implementation then produces untrusted output u out for the attacker and trusted output t impl for the receiver. A correct implementation would always produce the correct t specthat is, t impl would equal t spec . Incorrect implementations thus produce incorrect outputs, in part because they enable the attacker to influence the output.</p><p>In this model, the receiver observes t impl but is interested in t spec . So the extent to which t impl informs the receiver about t spec determines how much integrity the implementation has with respect to the specification. We can quantify this extent with information theory: program transmission is the amount of information that can be learned about t spec by observing t impl . Likewise, program suppression is the amount of information that t impl fails to convey about t spec .</p><p>Definition: Program suppression is the amount of information a receiver fails to learn about the specification's trusted output by observing the implementation's trusted output.</p><p>Let T spec be the distribution on the specification's trusted outputs, and let T impl be the distribution on the implementation's trusted outputs. These output distributions depend on trusted input distribution T in , untrusted input distribution U in (only for T impl ), and on the programs' semantics. Moreover, T spec and T impl are based on the same underlying trusted input-that is, the specification and the implementation must be executed with the same trusted input. We require T spec to be a (deterministic) function of its input:</p><formula xml:id="formula_24">H(T spec |T in ) = 0.<label>(18)</label></formula><p>The definitions of program transmission and program suppression in single executions (PT 1 and PS 1 ) and in expectation (PT and PS) are then as follows:</p><formula xml:id="formula_25">PT 1 I(t spec , t impl ),<label>(19)</label></formula><formula xml:id="formula_26">PS 1 I(t spec |t impl ),<label>(20)</label></formula><p>PT I(T spec , T impl ),</p><formula xml:id="formula_27">PS H(T spec |T impl ).<label>(21)</label></formula><p>The rationale for these definitions remains unchanged from our development of channel transmission and suppression. Note that the attacker's influence is accounted for because T impl can depend on U in . Channel transmission and suppression can now be seen as instances of program transmission and suppression for the echo specification, which stipulates that t spec equal t in . (This specification is deterministic and therefore satisfies equation (18).) In §III-A, the output of the channel is called t out , hence t impl equals t out . Given these equalities, we have that T spec = T in and T impl = T out . Making these substitutions in the above definitions yields the definitions of channel transmission and channel suppression in single executions (CT 1 and CS 1 ) and in expectation (CT and CS).</p><p>Before turning to more compelling examples, we consider the following specification as a corner case:</p><formula xml:id="formula_29">o T := 42</formula><p>This specification represents a constant function: T spec is the distribution assigning probability 1 to output 42. So quantity PS of program suppression is 0 bits, because the entropy of T spec is 0 regardless of whether it is conditioned on T impl , hence regardless of the implementation. Therefore no implementation of a constant function exhibits program suppression.</p><p>1) Examples of program suppression: Consider the following specification SumSpec for computing the sum of array a, which contains m elements indexed from 0 to m − 1:  <ref type="formula" target="#formula_1">(24)</ref> and <ref type="formula" target="#formula_1">(25)</ref> below, appears in the accompanying technical report <ref type="bibr" target="#b19">[19]</ref>.) So if m = 10, n = 1, and p = 0.5, then PS US is 1 bit. This quantity is intuitively sensible: the implementation omits array element a <ref type="bibr">[0]</ref>, which is distributed according to Bin(1, 0.5), and the entropy of that distribution is 1 bit (because it assigns probability 0.5 to each of two values, 0 and 1). Moreover, this analysis suggests that UnderSum always exhibits program suppression equal to the entropy of the distribution on a <ref type="bibr">[0]</ref>:</p><formula xml:id="formula_30">PS = H(Bin(n, p)).<label>(24)</label></formula><p>Indeed, it is straightforward to reduce equation (23) to equation (24). Hence, UnderSum suppresses exactly the information about the omitted array element.</p><p>OverSum exhibits a different quantity PS OS of program suppression:</p><formula xml:id="formula_31">PS OS = s∈Bin(mn,p), i ∈Unif (0,2 j −1) 2 −j Pr(s) log 2 −j Pr(s) Pr(s + i ) .<label>(25)</label></formula><p>Now if m = 10, n = 1, p = 0.5, and j = 1, then PS OS is about 0.93 bits. The 1 bit of randomness added by the attacker through a <ref type="bibr">[m]</ref>, which is uniformly distributed on {0, 1}, suppresses nearly 1 bit of information from the sum. The program suppression is not fully 1 bit because there are corner-case values that completely determine what the summands are-for example, if the sum is 0, then all array elements are 0 and the attacker's input is 0. As m increases, PS OS approaches 1, because such corner cases occur with decreasing probability. So in the limit, the attacker can exploit memory location a[m] to suppress a single array element. <ref type="bibr" target="#b14">14</ref> 2) Probabilistic specifications: Equation <ref type="formula" target="#formula_0">(18)</ref> requires specifications to be deterministic. Consider eliminating that requirement and allowing probabilistic specifications-for example, "o T := rnd(1)". This specification stipulates that the output must be 0 or 1, and that each output must occur with probability 1 2 . There is no correct output according to this specification; instead, there is a correct distribution on outputs. And program suppression should be the amount of information the receiver fails to learn about that correct distribution-rather than about a correct output-by observing the implementation. When quantifying suppression of correct outputs, we needed a probability distribution on outputs to model the receiver's uncertainty. To quantify suppression of correct distributions, we would need an extra level of distributions: a probability distribution on a probability distribution on outputs. So far, we have modeled only discrete probability distributions, which have finite support. But there are infinitely many probability distributions on outputs, so it seems we would need to upgrade our model with continuous probability distributions and differential entropy (the continuous analogue of entropy). We leave this mathematical upgrade as future work.</p><p>3) Duality: Program suppression is the amount of information the implementation's trusted output fails to reveal about the trusted output that is correct according to the specification. Applying the Biba duality, the confidentiality dual of program suppression would be the amount of information that the implementation's public output fails to reveal about the public output that is correct according to the specification. For confidentiality, this flow is uninteresting: <ref type="bibr" target="#b14">14</ref> This kind of analysis might be used to provide a mathematical explanation of why failure-oblivious computing (FOC) <ref type="bibr" target="#b26">[26]</ref> is successful at increasing software robustness. FOC rewrites out-of-bounds array reads to return strategically-chosen values that enable software to survive memory errors. Perhaps the choice of values could be understood as minimizing program suppression; we leave further investigation as future work. the amount of information that flows-or fails to flow-to public outputs does not characterize how a program leaks or hides secret information. So there does not seem to be a dual to suppression.</p><p>Other notions of integrity also lack obvious confidentiality duals-for example, the Clark-Wilson <ref type="bibr" target="#b3">[4]</ref> integrity policy for commercial organizations, based on well-formed transactions and verification procedures. Apparently, the Biba duality goes only so far.</p><p>4) Suppression vs. availability: If a program (or channel) suppresses all its input, the receiver gains no information. It might at first seem as though the program has made the correct output unavailable, so we might be tempted to conclude that suppression measures availability rather than integrity. However, availability is usually concerned with timely response-not with quality of informationwhereas suppression is concerned with quality, not timeliness. Furthermore, techniques typically employed to prevent suppression differ from those for improving availability. For example, error-correcting codes defend against (channel) suppression, but they do not improve availability-if a channel goes down (e.g., a wire is cut), a code cannot restore communication. And replication improves availability but potentially introduces (program) suppression, because different replicas might provide different responses and combining those responses might yield incorrect output. Hashes and digital signatures are therefore used in conjunction with replication to increase integrity by ensuring correct output.</p><p>Complete absence of information could be viewed as complete confidentiality, complete loss of integrity, or complete unavailability. Thus some quantitative relaxation of "complete absence" could yield quantitative characterizations of confidentiality, integrity, or availability. So there might be some interesting relationships-perhaps even new dualities-still to be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTEGRITY AND BELIEFS</head><p>In our models of contamination and suppression, inputs are chosen according to probability distributions. For example, the user assumes that untrusted inputs are chosen according to distribution U in in our model of contamination. But the user could be wrong-the inputs could be chosen according to a different distribution; a calculation of information flow would then need to incorporate both distributions.</p><p>Clarkson et al. <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b27">[27]</ref> show how to quantify leakage from secret inputs to public outputs when attackers have (possibly incorrect) beliefs about the inputs. And since leakage is dual to contamination, that belief-based approach ought to work for quantifying contamination. We show that it does, next, as well as adapt it to suppression. For both contamination and suppression, the belief-based approach turns out to generalize the information-theoretic approach used so far in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contamination and Beliefs</head><p>Define a belief to be a probability distribution of untrusted inputs. The user has a prebelief U in about untrusted input event u in . Recall that u in is unobservable by the user. The user instead observes the trusted input and output, enabling the user to refine U in to a postbelief U in about u in . Unless the user's prebelief assigns probability 1 to u in , the prebelief is inaccurate. To quantify inaccuracy, we stipulate a function D such that D(X Y ) is the inaccuracy of belief X about reality Y, where Y is also a distribution. Intuitively, D(X Y ) is the distance from the belief to reality. In previous work <ref type="bibr" target="#b16">[16]</ref>, we showed that relative entropy can successfully instantiate D:</p><formula xml:id="formula_32">D(X Y ) x Pr(Y = x) log Pr(Y = x) Pr(X = x) .<label>(26)</label></formula><p>The right-hand side of this definition is the relative entropy 15 between Y and X.</p><p>Since reality is, in our model of contamination, always a distribution that assigns probability 1 to event u in , we can simplify our notation and definition. Let D(X x) be the inaccuracy of belief X about event x:</p><formula xml:id="formula_33">D(X x) − log Pr(X = x).<label>(27)</label></formula><p>Equation <ref type="formula" target="#formula_1">(27)</ref> follows from (26) by setting Y to be a distribution that assigns probability 1 to event x. This simplified definition is equivalent to self-information-that is,</p><formula xml:id="formula_34">D(X x) = I(x),<label>(28)</label></formula><p>where the probability of x in the calculation of selfinformation I(x) is specified by X. Quantity C B of contamination of beliefs is the improvement in accuracy of the user's belief, because the more accurate the belief becomes, the more untrusted information the user has learned:</p><formula xml:id="formula_35">C B D(U in u in ) − D(U in u in ).<label>(29)</label></formula><p>In previous work <ref type="bibr" target="#b16">[16]</ref>, we defined an experiment protocol for calculating a postbelief from a prebelief and a probabilistic program semantics. That protocol turns out to be equivalent to calculating U in according to equation <ref type="formula" target="#formula_4">(4)</ref>: U in equals U in conditioned on t in and t out . Furthermore, the quantity of contamination according to C B equals the quantity of contamination according to C 1 (1).</p><formula xml:id="formula_36">Theorem 1. C B = C 1 .</formula><p>Thus belief-based quantification is equivalent to mutual information-based quantification on single executions. <ref type="bibr" target="#b15">15</ref> The traditional notation for the relative entropy between Y and X is D(Y X), but we use notation D(X Y ) to emphasize the asymmetry between the two distributions. Also, we abuse notation by treating distributions as random variables in the probability terms.</p><p>Moreover, define belief U in to be correct if the attacker chooses u in by sampling user prebelief U in -that is, if the user is correct about how untrusted inputs are chosen. Then applying the expectation operator to both sides of theorem 1, we have that the expected quantity of contamination of beliefs equals the expected quantity of contamination according to C (6).</p><formula xml:id="formula_37">Corollary 1. U in is correct implies E[C B ] = C.</formula><p>Thus belief-based quantification generalizes mutual information-based quantification.</p><p>Corollary 1 can also be understood in terms of leakage by applying the duality of contamination C and leakage L (7). If the attacker's distribution S in on secret inputs is correct, the expected quantity of leakage according to the beliefbased approach equals the quantity of leakage according to the mutual information-based approach. So corollary 1 also establishes how belief-based and mutual informationbased measures for confidentiality are related: the mutual information measure is a special case of the belief measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Suppression and Beliefs</head><p>In our model of contamination, the user holds beliefs about untrusted inputs. To model channel suppression, we replaced the user with a sender and a receiver. So to model channel suppression with beliefs, we now regard the receiver as the agent who holds beliefs. The receiver's joint prebelief (T in , U in ) characterizes the receiver's uncertainty about trusted input t in supplied by the sender and untrusted input u in supplied by the attacker. And the receiver's postbelief T in characterizes the receiver's uncertainty about the untrusted input after observing the trusted output, so T in equals T in conditioned on t out . The improvement in the accuracy of the receiver's belief is the quantity CT B of belief-based channel transmission:</p><formula xml:id="formula_38">CT B D(T in t in ) − D(T in t in ).<label>(30)</label></formula><p>Term D(T in t in ) characterizes the remaining error in the receiver's postbelief, hence the quantity of information that the receiver did not learn about t in . So D(T in t in ) is the quantity CS B of belief-based channel suppression:</p><formula xml:id="formula_39">CS B D(T in t in ).<label>(31)</label></formula><p>Unsurprisingly, the following results-corresponding to those we obtained for contamination-hold. For the corollary, we extend the definition of correct prebelief to mean that (T in , U in ) is correct if inputs t in and u in are chosen by the sender and attacker by sampling distributions T in and U in , respectively. Thus the belief-based definition of channel suppression generalizes the mutual information-based definition. Likewise, we can generalize belief-based channel suppression and transmission to program suppression and transmission. Let T spec = T spec |t impl . The following definitions of belief-based program transmission PT B and belief-based program suppression PS B are straightforward generalizations of equations <ref type="formula" target="#formula_3">(30)</ref> and <ref type="formula" target="#formula_0">(31)</ref>:</p><formula xml:id="formula_40">PT B D(T spec t spec ) − D(T spec t spec ),<label>(32)</label></formula><formula xml:id="formula_41">PS B D(T spec t spec ).<label>(33)</label></formula><p>We obtain the obvious result:</p><formula xml:id="formula_42">Corollary 3. PT B = PT 1 and PS B = PS 1 . Further, (T in , U in ) is correct implies E[PT B ] = PT and E[PS B ] = PS.</formula><p>So belief-based definitions again generalize mutual information-based definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CASE STUDY: DATABASE PRIVACY</head><p>Databases that contain information about individuals sometimes must respond to queries in a way that protects the privacy of those individuals. Such databases often will employ an anonymizer to suppress information about individuals. We model the anonymizer as a program that receives two inputs, as depicted in <ref type="figure">figure 4</ref>. The first input is the user's query. The second input is a response computed by the database with the user's query. Both inputs are trusted by the anonymizer and by the user. The response contains information from the database-perhaps even its entire contents-so the response is secret. The query, however, is public because it contains no sensitive information about individuals. The anonymizer produces an anonymized response as output. <ref type="bibr" target="#b16">16</ref> The anonymized response is trusted by the user and is public, because it (presumably) has been anonymized. Although the query and responses might involve statistics (e.g., sums or averages) computed from individuals' information, we do not restrict our consideration to any particular statistics. Our model is agnostic about the domains of queries and data.</p><p>The user attempts to learn secret information about individuals through queries. The anonymizer should leak some information to the user; otherwise, interacting with it would be pointless. And the anonymizer acts as a noisy communication channel, where the database is the sender and the user is the receiver. <ref type="bibr" target="#b17">17</ref> The anonymizer suppresses some information from this channel's outputs to protect privacy. By proposition 1, the amount of leakage plus the amount of channel suppression is a constant that depends on the distribution of database content. This is sensiblewhatever the anonymizer doesn't suppress, it leaks.</p><p>The quantitative frameworks we have developed for integrity and confidentiality yield a nuanced characterization of database privacy. We demonstrate this by analyzing two popular security conditions, k-anonymity <ref type="bibr" target="#b28">[28]</ref> anddiversity <ref type="bibr" target="#b29">[29]</ref>. For each, we are able to offer an informationtheoretic characterization of the security condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. k-anonymity</head><p>Sweeney <ref type="bibr" target="#b28">[28]</ref> proposes k-anonymity, a security condition for anonymizers, which requires every individual to be anonymous within some set (of individuals) of size at least k. For example, if Alice was born Nov. 26, 1865, and if gender and birth date are both published, then at least k − 1 other females born that day must be included in the published data. If the original database does not contain at least that many individuals, the data must be changed in some way to satisfy k-anonymity. Sweeney proposes generalization, which hierarchically replaces attributes values with less specific values. For example, Alice's birth date might be replaced by Nov. 1865, by 1865, or even by 18**. Generalization improves confidentiality by obscuring identities, but it diminishes the information conveyedthat is, generalization corrupts integrity. That tradeoff is unsurprising in light of proposition 1. Sweeney quantifies the integrity of generalized data with a precision metric that is based on the generalization hierarchy and the domains used in it.</p><p>Adapting Sweeney's insight to information flow, we could imagine requiring that the public output of a program corresponds to at least k possible secret inputs. This requirement would make any particular input be anonymous within a set of size k. We have the tools to analyze how generalization affects our notions of leakage and channel suppression. <ref type="bibr" target="#b18">18</ref> As an example, consider generalization of birth dates. Assume that birth dates are uniformly distributed within a given year-for example, 1865. <ref type="bibr" target="#b19">19</ref> Then, according to our definitions, a program that outputs the entire input date leaks about 8.5 bits and suppresses 0 bits; a program that outputs just the month and year leaks about 3.6 bits and suppresses about 4.9 bits; and a program that outputs just the year leaks 0 bits and suppresses about 8.5 bits.</p><p>Moreover, leakage and channel suppression enable an information-theoretic understanding of generalization. Channel suppression quantifies how much information is lost because of generalization, whereas Sweeney's precision metric has no obvious information-theoretic interpretation. And leakage quantifies how much information is released despite generalization, whereas k-anonymity makes no guarantees on how much information might be leaked. For example, suppose that published data includes a medical diagnosis and a favorite pet. If it is known that Alice's favorite pet is a cat and that the rest of the individuals in the population are highly unlikely to have a cat as a favorite pet, then Alice's medical diagnosis could be inferred with high probability. Thus information about Alice would be leaked despite kanonymity. As another example, if a program's output could have been caused by any one of k possible inputs, but one of those inputs is much more probable than the rest, then information about the input would be leaked despite k-anonymity. These kinds of leakage-made possible by the attacker's background knowledge-were discovered by Machanavajjhala et al. <ref type="bibr" target="#b29">[29]</ref>, who invented a new criterion, -diversity. We turn to that, next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. -diversity</head><p>The principle of -diversity <ref type="bibr" target="#b29">[29]</ref> is that published data should not only make every individual's sensitive information appear to have at least possible values, but that each of those values should have roughly equal probability. This principle blunts background knowledge attacks, which depend on some sensitive values having significantly higher probability than the rest.</p><p>Machanavajjhala et al. <ref type="bibr" target="#b29">[29]</ref> give an instantiation of the -diversity principle based on entropy, as follows. Define a block to be a set of tuples in which each tuple corresponds to an individual and in which every individual has the same values for non-sensitive attributes. For example, a block might contain all the tuples corresponding to individuals whose birth date is 18** and whose favorite pet is a cat. However, individuals in the block may (indeed, should) have different values for their sensitive attributes. We can construct an empirical probability distribution of sensitive attributes in the block by taking their relative frequenciesfor example, given the following block, the distribution would assign probability 0.5 to cancer and 0.25 to both heart disease and influenza: For each such probability distribution B constructed from a block of published data, entropy -diversity requires that H(B) ≥ log holds, where H(B) denotes the entropy of distribution B. 20 Applying this definition, we have that the block above is 1.5-diverse. Notice that it is not 2-diverse because the two most frequent sensitive values (either cancer and heart disease, or cancer and influenza) do not occur with roughly equal probability-cancer is twice as likely as the other diagnoses. More generally, consider any block with distribution B that satisfies entropy -diversity. The entropy of a uniform distribution of events is log . So if H(B) ≥ log , we have that B is at least as uncertain as a distribution of sensitive information in which the information has at least possible values, all of which are equally likely. Hence entropydiversity is an instantiation of the -diversity principle.</p><p>We now recast entropy -diversity in terms of information flow. In the example above, B is the distribution on the diagnosis of an arbitrary patient that results from observing the block. More generally, B is the distribution on trusted (secret) inputs that results from observing a block, which is a trusted (public) output, under the assumption that the observer's initial distribution T in on inputs is uniform. (Were it not uniform, B would be a function of the block's empirical distribution and the observer's initial distribution.) Hence, B = T in |t out . And since H(B) ≥ log , we have that H(T in |t out ) ≥ log for any t out produced by the anonymizer. We can use this fact to obtain a bound on the anonymizer's channel suppression:</p><formula xml:id="formula_43">CS = equation (12) H(T in |T out ) = H(X|Y ) = E y∈Y [H(X|y)] E tout ∈Tout [H(T in |t out )] ≥ fact above E tout ∈Tout [log ]</formula><p>= expectation of constant log .</p><p>So we have that CS ≥ log . As a straightforward consequence of its definition, entropy -diversity therefore enforces a bound on channel suppression.</p><p>Interpreting that bound, suppose that an individual is in the block from which B was constructed, and suppose that T in is uniform-meaning that the individual is equally likely to have any value for his sensitive attributes. Then B yields the probability distribution on that individual's sensitive attributes that results from observing the published block. Entropy -diversity requires at least log bits of uncertainty in that distribution. So at least log bits of information are suppressed about the individual's sensitive attributes.</p><p>However, entropy -diversity does not directly place a bound on the amount of information that may be transmitted; beyond the log bits that are suppressed, there might be many bits that are transmitted about an individual. For example, there might be • a lot of information about the individual (e.g., an entire DNA sequence) already present in the input, little of which is suppressed; or • a lot of background knowledge about the individual already possessed by the user, enabling inference of a lot of information from the output. To measure the utility of published data-that is, how useful the data is for studying the characteristics of a population-Machanavajjhala et al. <ref type="bibr" target="#b29">[29]</ref> and <ref type="bibr">Kifer and Gehrke [32]</ref> use an information-theoretic metric called Kullback-Leibler divergence. This metric is another name for relative entropy D (26). Let B be an empirical probability distribution of sensitive attributes, as constructed above from anonymized data. <ref type="bibr" target="#b21">21</ref> And let R be an empirical distribution similarly constructed from the original (non-anonymous) data. Their utility measure is the relative entropy of B to R-that is, D(B R). Notice that the best possible utility is 0, meaning that B and R are the same distribution, and that the higher the utility is, the less the distributions are alike. So we call this metric anti-utility.</p><p>Again recasting in terms of information flow, note that anti-utility is the distance between two distributions: an empirical distribution of trusted inputs, after observing trusted outputs; and an empirical distribution of trusted inputs. Were we to ignore the "empirical" part of that characterization, we could say that anti-utility is D(T in |t out T in ), which is the expectation of D(T in |t out t in ) with respect to t in . That latter quantity is CS B (31), because T in = T in |t out . And by corollary 2, expected belief-based channel suppression E[CS B ] is equal to information-theoretic channel suppression CS. So anti-utility would be the quantity of channel suppression if we used real, instead of empirical, distributions. <ref type="bibr" target="#b22">22</ref> This equivalence is sensible, because the less suppression data suffers, the more useful it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Research on quantification of information flow began with analysis of covert channels, and progress has been made from theoretical definitions to automated analyses <ref type="bibr" target="#b33">[33]</ref>- <ref type="bibr" target="#b37">[37]</ref>. Quantification of integrity and corruption is a relatively new line of research.</p><p>Newsome, McCamant, and Song <ref type="bibr" target="#b17">[17]</ref> implement a dynamic analysis that automatically quantifies attacker influence in real-world programs. They quantify the influence an attacker can exert over the execution of a program as the logarithm of the size of the set of possible outputs. This <ref type="bibr" target="#b21">21</ref> We simplify their definition here. They define B as the maximum entropy distribution with respect to empirical distributions calculated from several published data sets. <ref type="bibr" target="#b22">22</ref> Definitions of anti-utility <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b32">[32]</ref> use empirical distributions because they deal with concrete databases and anonymizations. quantity is the same as our contamination C 1 in a single execution, assuming that programs are deterministic and that all inputs are either under the control of the attacker or are fixed constants. But our definition of C 1 allows probabilistic programs, trusted inputs that are not under the control of the attacker, and arbitrary distributions on inputs and outputs.</p><p>Heusser and Malacaria <ref type="bibr" target="#b38">[38]</ref> quantify the information leaked by a database query. They model database queries as programs, which enables application of their general purpose, automated, static analysis of leakage for C programs. Their work does not address integrity or relate information flow to existing database-privacy security conditions. Biba <ref type="bibr" target="#b14">[14]</ref> defines the integrity problem as the formulation of "policies and mechanisms that provide a subsystem with the isolation necessary for protection from subversion." He formulates several such policies, one of which (termed the "strict integrity policy") is dual to the Bell-LaPadula confidentiality policy <ref type="bibr" target="#b39">[39]</ref>. But since Biba's motivating concern was guaranteeing that systems perform as their designers intended, correctness is also a critical piece of the integrity puzzle. Our program suppression measure PS addresses correctness; perhaps other quantitative notions of correctness, such as software testing metrics, could also be understood as addressing quantitative integrity.</p><p>Information-flow integrity policies have sometimes received less attention than their confidentiality counterparts. For example, early versions of Jif <ref type="bibr" target="#b40">[40]</ref> (then called JFlow) did not include integrity policies, and Flow Caml <ref type="bibr" target="#b41">[41]</ref> does not distinguish confidentiality from integrity but instead uses an arbitrary lattice of security levels. But work on securing information flows in distributed systems programmed in Jif led to an appreciation for the role of informationflow integrity policies, because they were needed to "protect security-critical information from damage by subverted hosts" <ref type="bibr" target="#b42">[42]</ref>-an instance of Biba's integrity problem. Securing information flows in the presence of declassification (when, e.g., secret information is reclassified as public) also turned out to require integrity policies, so that attackers could not gain control over what information is declassified <ref type="bibr" target="#b43">[43]</ref>. So integrity cannot be easily dismissed, even when confidentiality is the primary concern.</p><p>Several recent systems use integrity policies in interesting ways. Jif-derived languages and systems <ref type="bibr" target="#b44">[44]</ref>- <ref type="bibr" target="#b46">[46]</ref> for building secure distributed applications incorporate integrity policies, enabling principals to specify fine-grained requirements on how their information may be affected by other principals. These policies drive automated partitioning of applications, in which computations can be assigned to principals who are sufficiently trusted to perform the computations. When no such principals exist, computations can be replicated and their results validated against each other to boost integrity. Flume [47]-a system that integrates information flow with operating system abstractions such as processes, pipes, and sockets-also incorporates u in t in u out t out <ref type="figure">Figure 5</ref>. Information-flow integrity in a program integrity policies, preventing (e.g.) untrusted dynamicallyloaded code from affecting information in the process that loads it. Airavat <ref type="bibr" target="#b48">[48]</ref> integrates information flow with MapReduce <ref type="bibr" target="#b49">[49]</ref> and differential privacy <ref type="bibr" target="#b50">[50]</ref>, providing confidentiality and integrity for MapReduce computations and automatically declassifying computation results if they do not violate differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUDING REMARKS</head><p>When we began this work, we thought we could simply apply Biba's confidentiality-integrity duality to obtain a quantitative model of integrity from previous work on quantitative confidentiality. We soon discovered that the resulting model, which we named contamination, was not same as the classical information-theoretic model of quantitative integrity, which we named channel suppression. We later discovered that channel suppression could be generalized to characterize program correctness, yielding another kind of quantitative integrity.</p><p>Are there other kinds of (quantitative) integrity waiting to be discovered? We suspect so. We have not dealt, for example, with the Clark-Wilson <ref type="bibr" target="#b3">[4]</ref> integrity policy, which stipulates the use of trusted procedures to modify data. Nor have we dealt with database integrity constraints, which stipulate conditions that database records must satisfy.</p><p>We cannot even attempt to prove that contamination and suppression are sufficient to express all integrity properties, because we lack a formal definition of integrity. But we can gain some insight by reviewing the information-flow model we have used in this paper, depicted in <ref type="figure">figure 5</ref>. The solid arrows in this figure represent two kinds of integrity that we identified, contamination (flow from u in to t out ) and channel suppression (attenuation of flow from t in to t out ). The dashed arrows represent flows that are uninteresting from our security perspective: it does not matter how much trusted or untrusted information flows to untrusted outputs. Since these four arrows represent all possible flows, we conclude that contamination and channel suppression are the only interesting integrity properties in this information-flow model. Other kinds of integrity must exist outside it.</p><p>Finally, our work exemplifies how measurement can drive research, even in computer security. In an effort to measure integrity, we came to disentangle suppression from contamination. We also bridged a gap between database privacy and quantitative information-flow security. Lord Kelvin had it right:</p><p>When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of a meagre <ref type="bibr">[sic]</ref> and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely in your thoughts advanced to the state of Science.</p><p>-William Thomson, 1st Baron Kelvin 23</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>And calculating I(i U = a, o T = b | j T = c) for any a, b, and c such that b = a xor c would yield the same contamination of 1 bit. If b = a xor c, then the calculation would yield an undefined quantity because of division by zero. This result is sensible, because such a relationship among a, b, and c is impossible with program (2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SumSpec</head><label></label><figDesc>: for (i = 0; i &lt; m; i++) { s := s+a[i]; } (Assume throughout that s is initially 0.) Programmers frequently introduce off-by-one errors into loop guards. Implementation UnderSum exhibits such an error by omitting array element a[0]: UnderSum : for (i = 1; i &lt; m; i++) { s := s+a[i]; } Conversely, implementation OverSum adds a[m], which is not an element of a: OverSum : for (i = 0; i &lt;= m; i++) { s := s+a[i]; } Suppose that each array element a[i] is identically, independently distributed according to a binomial distribu- tion with parameters n and p. Let Bin(n, p) denote this distribution. 13 Also suppose that the value found in a[m] is uniformly distributed on integer interval [0, 2 j − 1]; let Unif (0, 2 j − 1) denote this distribution. We consider ele- ments a[0]..a[m-1] to be properly initialized according to their binomial distribution and therefore to be trusted. But a[m] is not an element of the array, so it might have been initialized by the attacker; we therefore consider a[m] to be untrusted. UnderSum exhibits the following quantity PS US of pro- gram suppression: PS US = s ∈Bin(n,p), i∈Bin(n(m−1),p) Pr(s )Pr(i) log Pr(s ). (23) (The full calculation of PS US , as well as the calculations for equations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 .</head><label>2</label><figDesc>Figure 4. Anonymizer model</figDesc></figure>

			<note place="foot" n="2"> Although we phrase our theory in terms of programs, other notions of computation could be used. All we require is that there are inputs, outputs, and a way to derive output distributions from input distributions.</note>

			<note place="foot" n="5"> Distribution Tout could be defined in terms of T in , U in , and some representation of the channel-for example, if the channel is represented as a probabilistic program, the denotational semantics of that program describes how to calculate Tout [23]. 6 Some readers might be more familiar with I(· ; ·) as a notation for mutual information. We use a comma, rather than a semi-colon, to emphasize that the notation is symmetric.</note>

			<note place="foot" n="7"> Recall that contamination is the amount of information a user learns about untrusted input by observing trusted input and output. 8 Readers familiar with the use of beliefs in quantification of information flow will recognize this distribution as representing a belief; we discuss this matter further in §IV.</note>

			<note place="foot" n="13"> A binomial distribution models the probability of the number of successes obtained in a series of n experiments, each of which succeeds with probability p. We choose this distribution because it enjoys a convenient summation property-if X ∼ Bin(nx, p) and Y ∼ Bin(ny, p), then X + Y ∼ Bin(nx + ny, p), where Z ∼ D denotes that random variable Z is distributed according to distribution D-and because it illustrates that our theory is not limited to uniform distributions.</note>

			<note place="foot" n="16"> The anonymizer might also produce some output about the anonymization it just performed, and this output might be stored in the database and used during future anonymizations.</note>

			<note place="foot" n="17"> Alternatively, we could use program suppression to model anonymizers instead of channel suppression. The anonymizer would be an implementation program; the specification would be the query evaluator. Channel suppression is simpler-it does not require modeling the query evaluator. 18 Sweeney defines &quot;suppression&quot; differently than we do; she uses it to mean the complete removal of an individual&apos;s information from the output. 19 Birth dates are, in reality, probably not uniformly distributed [30].</note>

			<note place="foot" n="20"> The definition of entropy -diversity originates with Øhrn and OhnoMachado [31].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Supported in part by ONR grant N00014-09-1-0652, AFOSR grant F9550-06-0019, National Science Foundation grants 0430161 and CCF-0424422 (TRUST), and a gift from Microsoft Corporation.</p><p>We thank Steve Chong and Andrew Myers for discussions about this work. We also thank Aslan Askarov, Steve Chong, Johannes Gehrke, Boris Köpf, Andrew Myers, and the anonymous CSF reviewers for comments on a draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cryptography</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Data</forename><surname>Security</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Covert channel capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Millen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Security and Privacy</title>
		<meeting>IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="1987-04" />
			<biblScope unit="page" from="60" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Security mechanisms in highlevel network protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Voydock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1983-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of commercial and military computer security policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Security and Privacy</title>
		<meeting>IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="1987-04" />
			<biblScope unit="page" from="184" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Information processing systems: Open systems interconnection-basic reference model</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>International Organization for Standardization. Part 2: Security architecture, ISO 7498-2</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information Technology Security Evaluation Criteria: Provisional harmonised criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecsc</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eec</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eaec</forename><forename type="middle">)</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Commission of the European Communities</title>
		<imprint>
			<date type="published" when="1991-06" />
		</imprint>
	</monogr>
	<note>document COM(90) 314, Version 1.2</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Computers at Risk: Safe Computing in the Information Age</title>
		<imprint>
			<date type="published" when="1991" />
			<publisher>National Academy Press</publisher>
			<pubPlace>Washington, D.C.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Research Council</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Common Criteria for Information Technology Security Evaluation: Part 1: Introduction and general model</title>
		<idno>cCMB-2006-09-001</idno>
		<imprint>
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
	<note>Version 3.1, Revision 1. Available from www.commoncriteriaportal.org</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Programming Perl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Sebastopol, California: O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Electrical Units of Measurement&quot;, a lecture delivered at the Institution of Civil Engineers in London on May 3, 1883 and published in Popular Lectures and Addresses</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">73</biblScope>
			<pubPlace>Macmillan, London, 1889</pubPlace>
		</imprint>
	</monogr>
	<note>Quoted in [51</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Secure program execution via dynamic information flow tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devedas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Architectural Support for Programming Languages and Systems</title>
		<meeting>ACM Conference on Architectural Support for Programming Languages and Systems</meeting>
		<imprint>
			<date type="published" when="2004-10" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding security vulnerabilities in Java applications with static analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Security Symposium</title>
		<meeting>USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="page" from="271" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic taint analysis for automatic detection, analysis and signature generation of exploits on commodity software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<ptr target="http://www.isoc.org/isoc/conferences/ndss/05/proceedings/papers/taintcheck.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. Symposium on Network and Distributed System Security</title>
		<meeting>Symposium on Network and Distributed System Security</meeting>
		<imprint>
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taint-enhanced policy enforcement: A practical approach to defeat a wide range of attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Security Symposium</title>
		<meeting>USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrity considerations for secure computer systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Biba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MITRE Corporation</title>
		<imprint>
			<date type="published" when="1977-04" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep. MTR-3153</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantitative information flow, relations and polymorphic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="199" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantifying information flow with beliefs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="655" to="701" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring channel capacity to distinguish undue influence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="doi">10.1145/1554339.1554349</idno>
		<ptr target="http://doi.acm.org/10.1145/1554339.1554349" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Workshop on Programming Languages and Analysis for Security</title>
		<meeting>ACM Workshop on Programming Languages and Analysis for Security</meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantification of integrity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/1813/14470" />
	</analytic>
	<monogr>
		<title level="j">Cornell University Computing and Information Science Technical Report</title>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
		<title level="m">Elementary Information Theory</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A static analysis for quantifying information flow in a simple imperative language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantics of probabilistic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kozen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="328" to="350" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enforceable security policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foundations of Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adámek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Enhancing server availability and security through failure-oblivious computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dumitran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Beebee</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Operating System Design and Implementation</title>
		<meeting>USENIX Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Belief in information flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Security Foundations Workshop</title>
		<meeting>IEEE Computer Security Foundations Workshop</meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="31" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Achieving k-anonymity privacy protection using generalization and suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Uncertainty, Fuzziness and Knowledge-based Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="571" to="588" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venkitasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy beyond k-anonymity</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An analysis of the distribution of birthdays in a calendar year</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.panix.com/∼murphy/bday.html" />
		<imprint>
			<date type="published" when="2009-12-29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using Boolean reasoning to anonymize databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Øhrn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ohno-Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="254" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Injecting utility into anonymized datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Management of Data</title>
		<meeting>ACM Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward a mathematical foundation for information flow security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Security and Privacy</title>
		<meeting>IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="1991-05" />
			<biblScope unit="page" from="21" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantifying information flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Security Foundations Workshop</title>
		<meeting>IEEE Computer Security Foundations Workshop</meeting>
		<imprint>
			<date type="published" when="2002-06" />
			<biblScope unit="page" from="18" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quantified interference for a while language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Notes in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="149" to="166" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantitative information flow as network capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Programming Language Design and Implementation</title>
		<meeting>ACM Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automated discovery and quantification of information leaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rybalchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Security and Privacy</title>
		<meeting>IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2009-05" />
			<biblScope unit="page" from="141" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Applied quantitative information flow and statistical databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malacaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Formal Aspects in Security and Trust</title>
		<imprint>
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Secure computer systems: Mathematical foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Lapadula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MITRE Corporation</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<date type="published" when="1973-03" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep. 2547</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">JFlow: Practical mostly-static information flow control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Principles of Programming Languages</title>
		<meeting>ACM Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="228" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Information flow inference for ML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pottier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Simonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="158" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Untrusted hosts and confidentiality: Secure program partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdancewic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Operating Systems Principles</title>
		<meeting>ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2001-10" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust declassification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdancewic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Security Foundations Workshop</title>
		<meeting>IEEE Computer Security Foundations Workshop</meeting>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SIF: Enforcing confidentiality and integrity in web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Security Symposium</title>
		<meeting>USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2007-08" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Secure web applications via automatic partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Operating Systems Principles</title>
		<meeting>ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fabric: A platform for secure distributed computation and storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Waye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Operating Systems Principles</title>
		<meeting>ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Information flow control for standard OS abstractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brodsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cliffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Operating Systems Principles</title>
		<meeting>ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Airavat: Security and privacy for MapReduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T V</forename><surname>Setty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2010-04" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Operating System Design and Implementation</title>
		<meeting>USENIX Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Colloquium on Automata, Languages and Programming</title>
		<meeting>International Colloquium on Automata, Languages and Programming</meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The need of psychological training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Scripture</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/1766918" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">474</biblScope>
			<biblScope unit="page" from="127" to="128" />
			<date type="published" when="1892-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
