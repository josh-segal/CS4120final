<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking the Performance of Storage Systems that expose SPARQL Endpoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
							<email>christian.bizer@fu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Web-based Systems Group</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
								<address>
									<addrLine>Garystr. 21</addrLine>
									<postCode>14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Schultz</surname></persName>
							<email>aschultz@mi.fu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Web-based Systems Group</orgName>
								<orgName type="institution">Freie Universität Berlin</orgName>
								<address>
									<addrLine>Garystr. 21</addrLine>
									<postCode>14195</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking the Performance of Storage Systems that expose SPARQL Endpoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Benchmark</term>
					<term>Scalability</term>
					<term>Semantic Web</term>
					<term>SPARQL</term>
					<term>RDF</term>
					<term>relational database to RDF mapping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The SPARQL Query Language for RDF and the SPARQL Protocol for RDF are implemented by a growing number of storage systems and are used within enterprise and open web settings. As SPARQL is taken up by the community there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints via the SPARQL protocol. Such systems include native RDF stores, systems that map relational databases into RDF, and SPARQL wrappers around other kinds of data sources. This paper introduces the Berlin SPARQL Benchmark (BSBM) for comparing the performance of these systems across architectures. The benchmark is built around an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about products. The benchmark query mix illustrates the search and navigation pattern of a consumer looking for a product. After giving an overview about the design of the benchmark, the paper presents the results of an experiment comparing the performance of D2R Server, a relational database to RDF wrapper, with the performance of Sesame, Virtuoso, and Jena SDB, three popular RDF stores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Semantic Web is increasingly populated with larger amounts of RDF data. Within the last year, the W3C Linking Open Data 1 community effort has stimulated the publication and interlinkage of datasets amounting to over 2 billion RDF triples. Semantic Web technologies are also increasingly used within enterprise settings 2 . A publicly accessible example of this trend is the Health Care and Life Science demonstration held at the 16th International World Wide Web Conference which involved datasets amounting all together to around 450 million RDF triples <ref type="bibr">3</ref> .</p><p>A growing number of storage systems have started to support the SPARQL Query Language for RDF <ref type="bibr" target="#b0">[1]</ref> and the SPARQL Protocol for RDF <ref type="bibr" target="#b1">[2]</ref>. These systems include native RDF triple stores as well as systems that map relational databases into RDF 4 . In the future, these systems may also include SPARQL wrappers around other kinds of data sources such as XML stores or object-oriented databases.</p><p>As SPARQL is taken up by the community and as the amount of data that is accessible via SPARQL endpoints increases, there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints.</p><p>The Berlin SPARQL Benchmark (BSBM) is a benchmark for comparing the performance of systems that expose SPARQL endpoints across architectures. The benchmark is built around an e-commerce use case, where a set of products is offered by different vendors and consumers have posted reviews about products.</p><p>The benchmark consists of a data generator and a test driver. The data generator supports the creation of arbitrarily large datasets using the number of products as scale factor. In order to be able to compare the performance of RDF triple stores with the performance of RDF-mapped relational databases or XML stores, the data generator can output RDF, XML and relational representations of the benchmark data.</p><p>The test driver executes sequences of parameterized SPARQL queries over the SPARQL protocol against the system under test. In order to simulate a realistic workload, the benchmark query mix emulates the search and navigation pattern of a consumer looking for a product.</p><p>The rest of the paper is structured as follows: Section 2 gives an overview about existing benchmarks for Semantic Web technologies. Section 3 describes the design of the Berlin SPARQL benchmark. Section 3.1 gives an overview about the benchmark dataset. Section 3.2 motivates the benchmark query mix and defines the benchmark queries. Section 4 presents the results of an experiment comparing the performance of D2R Server, a relational database to RDF wrapper, with the performance of Sesame, Virtuoso, and Jena SDB, three popular RDF stores. Section 5 discusses the contributions of the paper and outlines our next steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A benchmark is only a good tool for evaluating a system if the benchmark dataset and the workload are similar to the ones expected in the target use case <ref type="bibr" target="#b2">[4]</ref>. Thus a variety of benchmarks for Semantic Web technologies have been developed.</p><p>A widely used benchmark for comparing the performance, completeness and soundness of OWL reasoning engines is the Lehigh University Benchmark (LUBM) <ref type="bibr" target="#b4">[6]</ref>. The LUBM benchmark has been extended in <ref type="bibr" target="#b5">[7]</ref> to the University Ontology Benchmark (UOBM) by adding axioms that make use of all OWL Lite and OWL DL constructs. As both benchmarks predate the SPARQL query language, they do not support benchmarking specific SPARQL features such as OPTIONAL filters or DESCRIBE and UNION operators.</p><p>An early performance benchmark for SPARQL is the DBpedia Benchmark <ref type="bibr" target="#b6">[8]</ref>. The benchmark measures the execution time of 5 queries that are relevant in the context of DBpedia Mobile <ref type="bibr" target="#b7">[9]</ref> against parts of the DBpedia dataset.</p><p>A recent SPARQL benchmark is SP 2 Bench <ref type="bibr" target="#b8">[10]</ref>. SP 2 Bench uses a scalable dataset that reflects the structure of the DBLP Computer Science Bibliography. The benchmark queries are designed for the comparison of different RDF store layouts and RDF data management approaches.</p><p>A first benchmark for comparing the performance of relational database to RDF mapping tools with the performance of native RDF stores is presented in <ref type="bibr" target="#b9">[11]</ref>. The benchmark focuses on the production of RDF graphs from relational databases and thus only tests SPARQL CONSTRUCT queries.</p><p>Further information about RDF benchmarks and current benchmark results are found on the ESW RDF Store Benchmarking wiki page 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Design of the Berlin Benchmark</head><p>This section introduces the design goals of the Berlin SPARQL Benchmark and gives an overview about the benchmark dataset, the query mix and the individual benchmark queries.</p><p>The Berlin SPARQL Benchmark was designed along three goals: First, the benchmark should allow the comparison of different storage systems that expose SPARQL endpoints across architectures. Testing storage systems with realistic workloads of use case motivated queries is a well established benchmarking technique in the database field and is for instance implemented by the TPC H benchmark <ref type="bibr" target="#b10">[12]</ref>. The Berlin SPARQL Benchmark should apply this technique to systems that expose SPARQL endpoints. As an increasing number of Semantic Web applications do not rely on heavyweight reasoning but focus on the integration and visualization of large amounts of data from autonomous data sources on the Web, the Berlin SPARQL Benchmark should not be designed to require complex reasoning but to measure the performance of queries against large amounts of RDF data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Benchmark Dataset</head><p>The BSBM dataset is settled in an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about these products on various review sites.</p><p>The benchmark dataset can be scaled to arbitrary sizes by using the number of products as scale factor. The data generation is deterministic in order to make it possible to create different representations of exactly the same dataset. Section 2 of the BSBM specification 6 contains the complete schema of the benchmark dataset, all data production rules and the probability distributions that are used for data generation. Due to the restricted space of this paper, we will only outline the schema and the production rules below.</p><p>The benchmark dataset consists of instances of the following classes: Product, ProductType, ProductFeature, Producer, Vendor, Offer, Review, Reviewer and ReviewingSite.</p><p>Products are described by an rdfs:label and an rdfs:comment. Products have between 3 and 5 textual properties. The values of these properties consist of 5 to 15 words that are randomly chosen from a dictionary. Products have 3 to 5 numeric properties with property values ranging from 1-2000 with a normal distribution. Products have a type that is part of a type hierarchy. The depth and width of this subsumption hierarchy depends on the chosen scale factor. In order to run the benchmark against stores that do not support RDFS inference, the data generator can forward chain the product hierarchy and add all resulting rdf:type statements to the dataset. Products have a variable number of product features which depend on the position of a product in the product hierarchy. Each product type and product feature is described by an rdfs:label and an rdfs:comment.</p><p>Products are offered by vendors. Vendors are described by a label, a comment, a homepage URL and a country URI. Offers are valid for a specific period and contain the price and the number of days it takes to deliver the product. The number of offers for a product and the number of offers by each vendor follow a normal distribution.</p><p>Reviews are published by reviewers. Reviewers are described by their name, mailbox checksum and the country the reviewer live in. Reviews consist of a title and a review text between 50-300 words. Reviews have up to four ratings with a random integer value between 1 and 10. Each rating is missing with a probability of 0.3. The number of reviews for a product and the number of reviews published by each reviewer follow a normal distribution. <ref type="table" target="#tab_0">Table 1</ref> summarizes the number of instances of each class in BSMB datasets of different sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Benchmark Query Mix</head><p>In order to simulate a realistic workload on the system under test, the Berlin SPARQL Benchmark executes sequences of parameterized queries that are motivated by the use case. The benchmark queries are designed to emulate the search and navigation pattern of a consumer looking for a product. In a real world setting, such a query sequence could for instance be executed by a shopping portal which is used by consumers to find products and sales offers. First, the consumer searches for products that have a specific type and match a generic set of product features. After looking at basic information about some matching products, the consumer gets a better idea of what he actually wants and searches again with a more specific set of features. After going for a second time through the search results, he searches for products matching two alternative sets of features and products that are similar to a product that he likes. After narrowing down the set of potential candidates, the consumer starts to look at offers and recent reviews for the products that fulfill his requirements. In order to check the trustworthiness of the reviews, he retrieves background information about the reviewers. He then decides which product to buy and starts to search for the best prize for this product offered by a vendor that is located in his country and is able to deliver within three days. Tables 2 shows the BSBM query mix resulting from this search and navigation path.   ?product bsbm:productFeature %ProductFeature1% . ?product bsbm:productFeature %ProductFeature3% . ?product bsbm:productPropertyNumeric2 ?p2 . FILTER ( ?p2&gt; %y% ) }} ORDER BY ?label LIMIT 10 OFFSET 10</p><p>Query 5: Find products that are similar to a given product SELECT DISTINCT ?product ?productLabel WHERE { ?product rdfs:label ?productLabel . %ProductXYZ% rdf:type ?prodtype. ?product rdf:type ?prodtype . FILTER (%ProductXYZ% != ?product) %ProductXYZ% bsbm:productFeature ?prodFeature . ?product bsbm:productFeature ?prodFeature . %ProductXYZ% bsbm:productPropertyNumeric1 ?origProperty1 . ?product bsbm:productPropertyNumeric1 ?simProperty1 . FILTER (?simProperty1 &lt; (?origProperty1 + 150) &amp;&amp; ?simProperty1 &gt; (?origProperty1 -150)) %ProductXYZ% bsbm:productPropertyNumeric2 ?origProperty2 . ?product bsbm:productPropertyNumeric2 ?simProperty2 . FILTER (?simProperty2 &lt; (?origProperty2 + 220) &amp;&amp; ?simProperty2 &gt; (?origProperty2 -220)) } ORDER BY ?productLabel LIMIT 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query 6: Find products having a label that contains specific words</head><p>SELECT ?product ?label WHERE { ?product rdfs:label ?label . ?product rdf:type bsbm:Product . FILTER regex(?label, "%word1%|%word2%|%word3%")}  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query 7: Retrieve in-depth information about a product including offers and reviews</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmark Experiment</head><p>Today, most enterprise data is stored in relational databases. In order to prevent synchonization problems, it is preferable in many situations to have direct SPARQL access to this data without having to replicate it into RDF. We were thus interested in comparing the performance of relational database to RDF wrappers with the performance of native RDF stores. In order to get an idea which architecture performs better for which types of queries and for which dataset sizes, we ran the Berlin SPARQL Benchmark against D2R Server 7 , a database to RDF wrapper which rewrites SPARQL queries into SQL queries against an application-specific relational schemata based on a mapping, as well as against Sesame 8 , Virtuoso 9 , and Jena SDB 10 , three popular RDF stores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The experiment was conducted on a DELL workstation (processor: Intel Core 2 Quad Q9450 2.66GHz; memory: 8GB DDR2 667; hard disks: 160GB (10,000 rpm) SATA2, 750GB (7,200 rpm) SATA2) running Ubuntu 8.04 64-bit as operating system. We used the following releases of the systems under test: as underlying RDMS. The load performance of the systems was measured by loading the N-Triple representation of the BSBM datasets into the triple stores and by loading the relational representation in the form of MySQL dumps into the RDMS behind D2R Server. The loaded datasets were forward chained and contained all rdf:type statements for product types. Thus the systems under test did not have to do any inferencing.</p><p>The query performance of the systems was measured by running 50 BSBM query mixes (altogether 1250 queries) against the systems over the SPARQL protocol. The test driver and the system under test (SUT) were running on the same machine in order to reduce the influence of network latency. In order to enable the SUTs to load parts of the working set into main memory and to take advantage of query result and query execution plan caching, 10 BSBM query mixes (altogether 250 queries) were executed for warm-up before the actual times were measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Interpretation</head><p>This section summarizes the results of the experiment and gives a first interpretation. The complete run logs of the experiment and the exact configuration of the SUTs are found on the BSBM website. <ref type="table" target="#tab_7">Table 5</ref> summarizes the time it took to load the N-Triple files and the MySQL dumps into the different stores. For all dataset sizes, loading the MySQL dump was significantly faster than loading the N-Triple representation of the benchmark data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Load Times</head><p>When comparing the load performance of the triple stores, it turns out that Sesame and Virtuoso are faster for small datasets while Jena SDB is faster for larger datasets.  <ref type="table" target="#tab_8">Table 6</ref> contains the overall times for running 50 query mixes against the stores. It turns out that Sesame is the fastest RDF store for small dataset sizes. Virtuoso is the fastest RDF store for big datasets, but is rather slow for small ones. According to feedback from the Virtuoso team, this is due to the time spent on compiling the query and deciding on the query execution plan being independent of the dataset size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Overall Run Time</head><p>Compared to the RDF stores, D2R Server showed an inferior overall performance. When running the query mix against the 25M dataset, query 5 did hid the 30 seconds time out and was excluded from the run. Thus there is no overall run time figure for D2R Server at this dataset size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Average Run Time Per Query</head><p>The picture drawn by the overall run times radically changed when we examined the run times of specific queries. It turned out that no store is superior for all queries. Sesame is the fastest store for queries 1 to 4 at all dataset sizes. Sesame shows a bad performance for queries 5 and 9 against large datasets which prevents the store from having the best overall performance for all dataset sizes. D2R Server, which showed an inferior overall runtime, is the fastest store for queries 6 to 10 against the 25M dataset. Jena SDB turns out to be the fastest store for query 9, while Virtuoso is good at queries 5 and 6. The tables below contain the arithmetic means of the query execution times over all 50 runs per query (in seconds). The best performance figure for each dataset size is set bold in the tables.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper introduced the Berlin SPARQL Benchmark for comparing the performance of different types of storage systems that expose SPARQL endpoints and presented the results of an initial benchmark experiment. The paper provides the following contributions to the field of SPARQL benchmarking:</p><p>1. The W3C Data Access Working Group has developed the SPARQL query language together with the SPARQL protocol in order to enable data access over the Web. The Berlin SPARQL Benchmark is the first benchmark that measures the performance of systems that use both technologies together to facilitate data access over the Web. 2. The Berlin Benchmark provides an RDF triple, an XML and a relational representation of the benchmark dataset. The Benchmark can thus be used to compare the performance of storage systems that use different internal data models. 3. Testing storage systems with realistic workloads of use case motivated queries is a well established technique in the database field and is for instance used by the widely accepted TPC H benchmark <ref type="bibr" target="#b10">[12]</ref>. The Berlin SPARQL Benchmark is the first benchmark that applies this technique to systems that expose SPARQL endpoints.</p><p>The benchmark experiment has shown that no store is dominant for all queries. Concerning the performance of relational database to RDF wrappers, D2R Server has shown a very good performance for specific queries against large datasets but was very slow at others. For us as the development team behind D2R Server it will be very interesting to further analyze these results and to improve D2R Server's SPARQL to SQL rewriting algorithm accordingly. As next steps for the benchmark, we plan to:</p><p>1. Run the benchmark with 100M, 250M and 1B triple datasets against all stores in order to find out at which dataset size the query times become unacceptable. 2. Run the benchmark against other relational database to RDF wrappers in order to make a contribution to the current work of the W3C RDB2RDF Incubator Group 11 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Run the benchmark against further RDF stores and test different Sesame and</head><p>Jena SDB storage layouts and setups. 4. Extend the test driver with multi-threading features in order to simulate multiple clients simultaneously working against a server. 5. Extend the benchmark to the Named Graphs data model in order to compare RDF triple stores and Named Graph stores.</p><p>The complete benchmark specification, current benchmark results, benchmark datasets, detailed run logs as well as the source code of the data generator and test driver (GPL license) are available from the Berlin SPARQL Benchmark website at http://www4.wiwiss.fu-berlin.de/bizer/BerlinSPARQLBenchmark/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )</head><label>1</label><figDesc>Jena SDB Version 1.1 (Hash Storage Layout) with MySQL Version 5.0.51a-3 as underlying RDMS and Joseki Version 3.2 (CVS) as HTTP interface. 2) Sesame Version 2.2-Beta2 (Native Storage) with Tomcat Version 5.5.25.5 as HTTP interface. 3) Virtuoso Open-Source Edition v5.0.6 (Native Storage) 4) D2R Server Version 0.4 (Standalone Setup) with MySQL Version 5.0.51a-3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 . Number of instances in BSBM datasets of different sizes.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The BSBM query mix. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 contains</head><label>3</label><figDesc>the SPARQL representation of the benchmark queries. Parameters within the queries are enclosed with % chars. During a test run, these parameters are replaced with values from the benchmark dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>The BSBM benchmark queries 

Query 1: Find products for a given set of generic features 

SELECT DISTINCT ?product ?label 
WHERE { 
?product rdfs:label ?label . 
?product rdf:type %ProductType% . 
?product bsbm:productFeature %ProductFeature1% . 
?product bsbm:productFeature %ProductFeature2% . 
?product bsbm:productPropertyNumeric1 ?value1 . 
FILTER (?value1 &gt; %x%)} 
ORDER BY ?label 
LIMIT 10 

Query 2: Retrieve basic information about a specific product for display purposes 

SELECT ?label ?comment ?producer ?productFeature ?propertyTextual1 
?propertyTextual2 ?propertyTextual3 ?propertyNumeric1 
?propertyNumeric2 ?propertyTextual4 ?propertyTextual5 
?propertyNumeric4 
WHERE { 
%ProductXYZ% rdfs:label ?label . 
%ProductXYZ% rdfs:comment ?comment . 
%ProductXYZ% bsbm:producer ?p . 
?p rdfs:label ?producer . 
%ProductXYZ% dc:publisher ?p . 
%ProductXYZ% bsbm:productFeature ?f . 
?f rdfs:label ?productFeature . 
%ProductXYZ% bsbm:productPropertyTextual1 ?propertyTextual1 . 
%ProductXYZ% bsbm:productPropertyTextual2 ?propertyTextual2 . 
%ProductXYZ% bsbm:productPropertyTextual3 ?propertyTextual3 . 
%ProductXYZ% bsbm:productPropertyNumeric1 ?propertyNumeric1 . 
%ProductXYZ% bsbm:productPropertyNumeric2 ?propertyNumeric2 . 
OPTIONAL { %ProductXYZ% bsbm:productPropertyTextual4 ?propertyTextual4 } 
OPTIONAL { %ProductXYZ% bsbm:productPropertyTextual5 ?propertyTextual5 } 
OPTIONAL { %ProductXYZ% bsbm:productPropertyNumeric4 ?propertyNumeric4 }} 

Query 3: Find products having some specific features and not having one feature 

SELECT ?product ?label 
WHERE { 
?product rdfs:label ?label . 
?product rdf:type %ProductType% . 
?product bsbm:productFeature %ProductFeature1% . 
?product bsbm:productPropertyNumeric1 ?p1 . 
FILTER ( ?p1 &gt; %x% ) 
?product bsbm:productPropertyNumeric3 ?p3 . 
FILTER (?p3 &lt; %y% ) 
OPTIONAL { 
?product bsbm:productFeature %ProductFeature2% . 
?product rdfs:label ?testVar } 
FILTER (!bound(?testVar)) } 
ORDER BY ?label 
LIMIT 10 

Query 4: Find products matching two different sets of features 

SELECT ?product ?label 
WHERE { 
{ ?product rdfs:label ?label . 
?product rdf:type %ProductType% . 
?product bsbm:productFeature %ProductFeature1% . 
?product bsbm:productFeature %ProductFeature2% . 
?product bsbm:productPropertyNumeric1 ?p1 . 
FILTER ( ?p1 &gt; %x% ) 
} UNION { 
?product rdfs:label ?label . 
?product rdf:type %ProductType% . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 . Characteristics of the BSBM Benchmark Queries.</head><label>4</label><figDesc></figDesc><table>Characteristic 
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 
Q10 
Simple Filters 
√ 
√ 
√ 
√ 
√ 
√ 
√ 
√ 
Complex Filters 
√ 
More than 9 triple patterns 
√ 
√ 
√ 
√ 
OPTIONAL operator 
√ 
√ 
√ 
√ 
√ 
LIMIT modifier 
√ 
√ 
√ 
√ 
√ 
√ 
ORDER BY modifier 
√ 
√ 
√ 
√ 
√ 
√ 
DISTINCT modifier 
√ 
√ 
√ 
REGEX operator 
√ 
UNION operator 
√ 
DESCRIBE operator 
√ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Load times for different stores and dataset sizes (in seconds). 

50K 
250K 
1M 
5M 
25M 
Jena SDB 
5 
24 
116 
1053 
13306 
Sesame 
3 
18 
132 
1988 
27674 
Virtuoso 
2 
33 
87 
609 
49096 
D2R Server 
0.4 
2 
6 
38 
202 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Overall runtime for executing 50 query mixes (in seconds). 

Sesame 
Virtuoso 
Jena SDB 
D2R Server 
50 K 
9.4 
162.0 
23.4 
66.4 
250 K 
28.5 
162.8 
72.9 
153.4 
1 M 
115.2 
201.4 
268.0 
484.4 
5 M 
569.1 
476.8 
1406.6 
2188.1 
25 M 
3038.205 
2089.118 
7623.958 
not applicable 

</table></figure>

			<note place="foot" n="1"> http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData 2 http://www.w3.org/2001/sw/sweo/public/UseCases/ 3 http://esw.w3.org/topic/HCLS/Banff2007Demo</note>

			<note place="foot" n="5"> http://esw.w3.org/topic/RdfStoreBenchmarking 6 http://www4.wiwiss.fu-berlin.de/bizer/BerlinSPARQLBenchmark/spec/</note>

			<note place="foot" n="7"> http://www4.wiwiss.fu-berlin.de/bizer/d2r-server/</note>

			<note place="foot" n="8"> http://www.openrdf.org/about.jsp 9 http://virtuoso.openlinksw.com/dataspace/dav/wiki/Main/ 10 http://jena.hpl.hp.com/wiki/SDB</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prud&amp;apos;hommeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seaborne</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/rdf-sparql-query/" />
		<title level="m">SPARQL Query Language for RDF. W3C Recommendation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">; S</forename><surname>Torres</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/xquery/" />
		<title level="m">XQuery 1.0: An XML Query Language. W3C Recommendation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>SPARQL Protocol for RDF. W3C Recommendation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Requirements Driven Framework for Benchmarking Semantic Web Knowledge Base Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbo</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering, issue</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="309" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s Wrong with OWL Benchmarks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weithöner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Scalable Semantic Web Knowledge Base Systems</title>
		<meeting>the 2nd International Workshop on Scalable Semantic Web Knowledge Base Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="101" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LUBM: A Benchmark for OWL Knowledge Base Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics, issue</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="182" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards a Complete OWL Ontology Benchmark (UOBM)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: Research and Applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4011</biblScope>
			<biblScope unit="page" from="125" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<ptr target="http://www4.wiwiss.fu-berlin.de/benchmarks-200801/" />
		<title level="m">RDF Store Benchmarks with DBpedia comparing Virtuoso, SDB and Sesame</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DBpedia Mobile: A Location-Enabled Linked Data Browser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop about Linked Data on the Web (LDOW2008)</title>
		<meeting>the 1st Workshop about Linked Data on the Web (LDOW2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Experimental Comparison of RDF Data Management Approaches in a SPARQL Benchmark Scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Küchlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pinkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Semantic Web Conference</title>
		<meeting>the International Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benchmarking RDF Production Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Svihala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Database and Expert Systems Applications</title>
		<meeting>the 18th International Conference on Database and Expert Systems Applications</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transaction Processing Performance Council: TPC Benchmark H, Standard Specification Revision 2</title>
		<ptr target="http://www.tpc.org/tpch/spec/tpch2.7.0.pdf" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
