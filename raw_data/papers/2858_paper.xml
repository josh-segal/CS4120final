<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototype-Driven Learning for Sequence Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
							<email>aria42@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution" key="instit1">University of California Berkeley</orgName>
								<orgName type="institution" key="instit2">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution" key="instit1">University of California Berkeley</orgName>
								<orgName type="institution" key="instit2">University of California Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototype-Driven Learning for Sequence Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distri-butional similarity features in a log-linear gener-ative model. On part-of-speech induction in En-glish and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system&apos;s error trends.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning, broadly taken, involves choosing a good model from a large space of possible models. In supervised learning, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources. In unsupervised learning, model behavior is largely determined by the structure of the model. Designing models to exhibit a certain target behavior requires another, rare kind of expertise and effort. Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort. We therefore consider here how to learn models with the least effort. In particular, we argue for a certain kind of semi-supervised learning, which we call prototype-driven learning.</p><p>In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences. For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see <ref type="figure">figure 4</ref> for a concrete prototype list for this task). This manner of specifying prior knowledge about the task has several advantages. First, is it certainly compact (though it remains to be proven that it is effective). Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task). Indeed, prototype lists have been used pedagogically to summarize tagsets to students <ref type="bibr">(Man- ning and Schütze, 1999</ref>). Finally, natural language does exhibit proform and prototype effects <ref type="bibr" target="#b10">(Radford, 1988)</ref>, which suggests that learning by analogy to prototypes may be effective for language tasks.</p><p>In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task. Our general approach is to use distributional similarity to link any given word to similar prototypes. For example, the word reported may be linked to said, which is in turn a prototype for the part-of-speech VBD. We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1). Distributional prototype features provide substantial error rate reductions on all three tasks. For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Related Work: Tagging</head><p>For our part-of-speech tagging experiments, we used data from the English and Chinese Penn treebanks <ref type="bibr" target="#b7">(Marcus et al., 1994;</ref><ref type="bibr" target="#b3">Ircs, 2002</ref>). Example sentences</p><p>The proposed changes also would allow executives to report exercises of options later and less often . <ref type="table">(b)   NR  AD VV AS  PU  NN  VV  DER VV  PU  PN  AD VV DER VV PU DEC  NN  VV  PU   !"# $ % &amp; '  ()  *  +  ,  -./ 0 *  +  , 1  2  34 56 7   (c)   FEAT  FEAT  FEAT  FEAT  NBRHD NBRHD NBRHD  NBRHD  NBRHD  SIZE  SIZE  SIZE  SIZE   Vine  covered cottage  ,  near  Contra Costa  Hills  .  2  bedroom house  ,   FEAT  FEAT  FEAT  FEAT  FEAT  RESTR  RESTR  RESTR  RESTR  RENT  RENT  RENT  RENT   modern kitchen  and  dishwasher  .  No  pets  allowed  .  1050  /</ref>   <ref type="figure" target="#fig_0">figure 1</ref>(a) and (b). A great deal of research has investigated the unsupervised and semisupervised induction of part-of-speech models, especially in English, and there is unfortunately only space to mention some highly related work here.</p><p>One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework. For example, <ref type="bibr" target="#b8">Merialdo (1991)</ref> presents experiments learning HMMs using EM. Merialdo's results most famously show that re-estimation degrades accuracy unless almost no examples are labeled. Less famously, his results also demonstrate that reestimation can improve tagging accuracies to some degree in the fully unsupervised case.</p><p>One recent and much more successful approach to part-of-speech learning is contrastive estimation, presented in <ref type="bibr" target="#b11">Smith and Eisner (2005)</ref>. They utilize task-specific comparison neighborhoods for part-ofspeech tagging to alter their objective function.</p><p>Both of these works require specification of the legal tags for each word. Such dictionaries are large and embody a great deal of lexical knowledge. A prototype list, in contrast, is extremely compact. <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see <ref type="figure" target="#fig_0">fig- ure 1(c)</ref>). Labels in this domain tend to be "sticky" in that the correct annotation tends to consist of multi-element fields of the same label. The overall approach of <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on. Given enough refine-  <ref type="figure">Figure 2</ref>: Prototype list derived from the development set of the CLASSIFIEDS data. The BOUND-ARY field is not present in the original annotation, but added to model boundaries (see Section 5.3). The starred tokens are the results of collapsing of basic entities during pre-processing as is done in ( <ref type="bibr" target="#b2">Grenager et al., 2005)</ref> ments the model learns to segment with a reasonable match to the target structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks and Related Work: Extraction</head><p>In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rather centers on feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>In the present work, we consider the problem of learning sequence models over text. For each document x = [x i ], we would like to predict a sequence of labels y = [y i ], where x i ∈ X and y i ∈ Y. We construct a generative model, p(x, y|θ), where θ are the model's parameters, and choose parameters to maximize the log-likelihood of our observed data D:</p><formula xml:id="formula_0">L(θ; D) = x∈D log p(x|θ) = x∈D log y p(x, y|θ) ess yi 񮽙NN, VBD񮽙 yi−1 񮽙DT, NN񮽙 φ(yi−1, yi) φ(xi, yi) xi 񮽙DT, NN񮽙 񮽙NN, VBD񮽙 φ(xi, yi) =      word = reported suffix-2 = ed proto = said      ∧ VBD φ(yi−1, yi) = DT ∧ NN ∧ VBD 1 witness yi 񮽙NN, VBD񮽙 yi−1 񮽙DT, NN񮽙 φ(yi−1, yi) φ(xi, yi) xi 񮽙DT, NN񮽙 񮽙NN, VBD񮽙 φ(xi, yi) =      word = reported suffix-2 = ed proto = said      ∧ VBD φ(yi−1, yi) = DT ∧ NN ∧ VBD 1 witness yi 񮽙NN, VBD񮽙 yi−1 񮽙DT, NN񮽙 xi reported φ(yi−1, yi) φ(xi, yi) xi 񮽙DT, NN񮽙 񮽙NN, VBD񮽙 φ(xi, yi) =      word = reported suffix-2 = ed proto = said      ∧ VBD φ(yi−1, yi) = DT ∧ NN ∧ VBD 1 ess yi 񮽙NN, VBD񮽙 yi−1 񮽙DT, NN񮽙 xi reported xi−1 witness φ(yi−1, yi) φ(xi, yi) xi 񮽙DT, NN񮽙 񮽙NN, VBD񮽙 φ(xi, yi) =      word = reported suffix-2 = ed proto = said      ∧ VBD φ(yi−1, yi) = DT ∧ NN ∧ VBD 1 x i reported x i−1 witness f (x i , y i ) =          word = reported suffix-2 = ed proto = said proto = had          ∧ VBD f (y i−1 , y i ) = DT ∧ NN ∧ VBD 1 f (x i , y i ) =          word = reported suffix-2 = ed proto = said proto = had          ∧ VBD f (y i−1 , y i ) = DT ∧ NN ∧ VBD 1</formula><p>Figure 3: Graphical model representation of trigram tagger for English POS domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Markov Random Fields</head><p>We take our model family to be chain-structured Markov random fields (MRFs), the undirected equivalent of HMMs. Our joint probability model over (x, y) is given by</p><formula xml:id="formula_1">p(x, y|θ) = 1 Z(θ) n i=1 φ(x i , y i )φ(y i−1 , y i )</formula><p>where φ(c) is a potential over a clique c, taking the form exp θ T f (c) , and f (c) is the vector of features active over c. In our sequence models, the cliques are over the edges/transitions (y i−1 , y i ) and nodes/emissions (x i , y i ). See <ref type="figure">figure 3</ref> for an example from the English POS tagging domain.</p><p>Note that the only way an MRF differs from a conditional random field (CRF) ( <ref type="bibr" target="#b4">Lafferty et al., 2001</ref>) is that the partition function is no longer observation dependent; we are modeling the joint probability of x and y instead of y given x. As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prototype-Driven Learning</head><p>We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y ∈ Y, a set of prototypes words, p y ∈ P y . See figures 2 and 4 for examples of prototype lists. 1</p><p>Broadly, we would like to learn sequence models which both explain the observed data and meet our prior expectations about target structure. A straightforward way to implement this is to constrain each prototype word to take only its given label(s) at training time. As we show in section 5, this does not work well in practice because this constraint on the model is very sparse.</p><p>In providing a prototype, however, we generally mean something stronger than a constraint on that word. In particular, we may intend that words which are in some sense similar to a prototype generally be given the same label(s) as that prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distributional Similarity</head><p>In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words <ref type="bibr">(Schütze, 1995;</ref><ref type="bibr" target="#b0">Clark, 2001</ref>). The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness <ref type="bibr" target="#b10">(Radford, 1988)</ref>. We present more details in section 5, but for now assume that a similarity function over word types is given.</p><p>Suppose further that for each non-prototype word type w, we have a subset of prototypes, S w , which are known to be distributionally similar to w (above some threshold). We would like our model to relate the tags of w to those of S w .</p><p>One approach to enforcing the distributional assumption in a sequence model is by supplementing the training objective (here, data likelihood) with a penalty term that encourages parameters for which each w's posterior distribution over tags is compatible with it's prototypes S w . For example, we might maximize,</p><formula xml:id="formula_2">x∈D log p(x|θ) − w z∈Sw KL( t|z || t|w)</formula><p>where t|w is the model's distribution of tags for word w. The disadvantage of a penalty-based approach is that it is difficult to construct the penalty term in a way which produces exactly the desired behavior.</p><p>Instead, we introduce distributional prototypes into the learning process as features in our log-linear model. Concretely, for each prototype z, we introduce a predicate PROTO = z which becomes active at each w for which z ∈ S w (see <ref type="figure">figure 3)</ref>. One advantage of this approach is that it allows the strength of the distributional constraint to be calibrated along with any other features; it was also more successful in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Estimation</head><p>So far we have ignored the issue of how we learn model parameters θ which maximize L(θ; D). If our model family were HMMs, we could use the EM algorithm to perform a local search. Since we have a log-linear formulation, we instead use a gradientbased search. In particular, we use L-BFGS ( <ref type="bibr" target="#b5">Liu and Nocedal, 1989)</ref>, a standard numerical optimization technique, which requires the ability to evaluate L(θ; D) and its gradient at a given θ.</p><p>The density p(x|θ) is easily calculated up to the global constant Z(θ) using the forward-backward algorithm <ref type="bibr" target="#b9">(Rabiner, 1989)</ref>. The partition function is given by</p><formula xml:id="formula_3">Z(θ) = x y n i=1 φ(x i , y i )φ(y i−1 , y i ) = x y score(x, y)</formula><p>Z(θ) can be computed exactly under certain assumptions about the clique potentials, but can in all cases be bounded byˆZ</p><formula xml:id="formula_4">byˆ byˆZ(θ) = K =1ˆZ =1ˆ =1ˆZ (θ) = K =1 x:|x|= score(x, y)</formula><p>Where K is a suitably chosen large constant. We can efficiently computê Z (θ) for fixed using a generalization of the forward-backward algorithm to the lattice of all observations x of length (see <ref type="bibr" target="#b11">Smith and Eisner (2005)</ref> for an exposition).</p><p>Similar to supervised maximum entropy problems, the partial derivative of L(θ; D) with respect to each parameter θ j (associated with feature f j ) is given by a difference in feature expectations: </p><formula xml:id="formula_5">∂L(θ; D) ∂θ j = x∈D E y|x,θ f j − E x,</formula><formula xml:id="formula_6">E x,y|θ f j = K =1 p(|x| = )E x,y|,θ f j</formula><p>For fixed , we can calculate E x,y|,θ f j using the lattice of all inputs of length . The quantity p(|x| = ) is simplyˆZsimplyˆ simplyˆZ (θ)/ ˆ Z(θ). As regularization, we use a diagonal Gaussian prior with variance σ 2 = 0.5, which gave relatively good performance on all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experimented with prototype-driven learning in three domains: English and Chinese part-of-speech tagging and classified advertisement field segmentation. At inference time, we used maximum posterior decoding, 2 which we found to be uniformly but slightly superior to Viterbi decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English POS Tagging</head><p>For our English part-of-speech tagging experiments, we used the WSJ portion of the English Penn treebank ( <ref type="bibr" target="#b7">Marcus et al., 1994)</ref>. We took our data to be either the first 48K tokens (2000 sentences) or 193K tokens (8000 sentences) starting from section 2. We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in <ref type="bibr" target="#b11">Smith and Eisner (2005)</ref>: exact word type, With just these features (our baseline BASE) the problem is symmetric in the 45 model labels. In order to break initial symmetry we initialized our potentials to be near one, with some random noise. To evaluate in this setting, model labels must be mapped to target labels. We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset. The results of BASE, reported in table 1, depend upon random initialization; averaging over 10 runs gave an average per-position accuracy of 41.3% on the larger training set.</p><p>We automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often. This resulted in 116 prototypes for the 193K token setting. <ref type="bibr">3</ref> For comparison, there are 18,423 word types occurring in this data.</p><p>Incorporating the prototype list in the simplest possible way, we fixed prototype occurrences in the data to their respective annotation labels. In this case, the model is no longer symmetric, and we no longer require random initialization or post-hoc mapping of labels. Adding prototypes in this way gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over BASE. It appears as though the prototype information is not spreading to non-prototype words.</p><p>In order to remedy this, we incorporated distributional similarity features. Similar to <ref type="bibr">(Schütze, 1995)</ref>, we collect for each word type a context vector of the counts of the most frequent 500 words, conjoined with a direction and distance (e.g +1,-2). We then performed an SVD on the matrix to obtain a reduced rank approximation. We used the dot product between left singular vectors as a measure of distributional similarity. For each word w, we find the set of prototype words with similarity exceeding a fixed threshold of 0.35. For each of these prototypes z, we add a predicate PROTO = z to each occurrence of w. For example, we might add PROTO = said to each token of reported (as in <ref type="figure">figure 3</ref>). <ref type="bibr">4</ref> Each prototype word is also its own prototype (since a word has maximum similarity to itself), so when we lock the prototype to a label, we are also pushing all the words distributionally similar to that prototype towards that label. <ref type="bibr">5</ref> This setting, PROTO+SIM, brings the all-tokens accuracy up to 80.5%, which is a 37.5% error reduction over PROTO. For non-prototypes, the accuracy increases to 67.8%, an error reduction of 38.4% over PROTO. The overall error reduction from BASE to PROTO+SIM on all-token accuracy is 66.7%. <ref type="table">Table 5</ref> lists the most common confusions for PROTO+SIM. The second, third, and fourth most common confusions are characteristic of fully supervised taggers (though greater in number here) and are difficult. For instance, both JJs and NNs tend to occur after determiners and before nouns. The CD and DT confusion is a result of our prototype list not containing a contains-digit prototype for CD, so the predicate fails to be linked to CDs. Of course in a realistic, iterative design setting, we could have altered the prototype list to include a contains-digit prototype for CD and corrected this confusion. <ref type="figure">Figure 6</ref> shows the marginal posterior distribution over label pairs (roughly, the bigram transition matrix) according to the treebank labels and the PROTO+SIM model run over the training set (using a collapsed tag set for space). Note that the broad structure is recovered to a reasonable degree.</p><p>It is difficult to compare our results to other systems which utilize a full or partial tagging dictionary, since the amount of provided knowledge is substantially different. The best comparison is to <ref type="bibr" target="#b11">Smith and Eisner (2005)</ref> who use a partial tagging dictionary. In order to compare with their results, we projected the tagset to the coarser set of 17 that they used in their experiments. On 24K tokens, our PROTO+SIM model scored 82.2%. When Smith and Eisner (2005) limit their tagging dictionary to words which occur at least twice, their best performing neighborhood model achieves 79.5%. While these numbers seem close, for comparison, their tagging dictionary contained information about the allowable tags for 2,125 word types (out of 5,406 types) and the their system must only choose, on average, between 4.4 tags for a word. Our prototype list, however, contains information about only 116 word types and our tagger must on average choose between 16.9 tags, a much harder task. When Smith and Eisner (2005) include tagging dictionary entries for all words in the first half of their 24K tokens, giving tagging knowledge for 3,362 word types, they do achieve a higher accuracy of 88.1%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Chinese POS Tagging</head><p>We also tested our POS induction system on the Chinese POS data in the Chinese Treebank <ref type="bibr" target="#b3">(Ircs, 2002</ref>). The model is wholly unmodified from the English version except that the suffix features are removed since, in Chinese, suffixes are not a reliable indicator of part-of-speech as in English ( <ref type="bibr" target="#b12">Tseng et al., 2005</ref>). Since we did not have access to a large auxiliary unlabeled corpus that was segmented, our distributional model was built only from the treebank text, and the distributional similarities are presumably degraded relative to the English. On 60K word tokens, BASE gave an accuracy of 34.4, PROTO gave 39.0, and PROTO+SIM gave 57.4, similar in order if not magnitude to the English case.</p><p>We believe the performance for Chinese POS tagging is not as high as English for two reasons: the general difficulty of Chinese POS tagging ( <ref type="bibr" target="#b12">Tseng et al., 2005</ref>) and the lack of a larger segmented corpus from which to build distributional models. Nonetheless, the addition of distributional similarity features does reduce the error rate by 35% from BASE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Information Field Segmentation</head><p>We tested our framework on the CLASSIFIEDS data described in <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> under conditions similar to POS tagging. An important characteristic of this domain (see <ref type="figure" target="#fig_0">figure 1(a)</ref>) is that the hidden labels tend to be "sticky," in that fields tend to consist of runs of the same label, as in figure 1(c), in contrast with part-of-speech tagging, where we rarely see adjacent tokens given the same label. <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> report that in order to learn this "sticky" structure, they had to alter the structure of their HMM so that a fixed mass is placed on each diagonal transition. In this work, we learned this structure automatically though prototype similarity features without manually constraining the model (see On the test set of ( <ref type="bibr" target="#b2">Grenager et al., 2005</ref>), BASE scored an accuracy of 46.4%, comparable to <ref type="bibr" target="#b2">Grenager et al. (2005)</ref>'s unsupervised HMM baseline. Adding the prototype list (see <ref type="figure">figure 2</ref>) without distributional features yielded a slightly improved accuracy of 53.7%. For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content. Therefore, when we collect context vectors for word types in this domain, we make no distinction by direction or distance and collect counts from a wider window. This notion of distributional similarity is more similar to latent semantic indexing <ref type="bibr" target="#b1">(Deerwester et al., 1990)</ref>. A natural consequence of this definition of distributional similarity is that many neighboring words will share the same prototypes. Therefore distributional prototype features will encourage labels to persist, naturally giving the "sticky" effect of the domain. Adding distributional similarity features to our model (PROTO+SIM) improves accuracy substantially, yielding 71.5%, a 38.4% error reduction over BASE. <ref type="bibr">6</ref> Another feature of this domain that <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one. <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> experiment with manually adding boundary states and biasing transitions from these states to not self-loop. We capture this "boundary" effect by simply adding a line to our protoype-list, adding a new BOUNDARY state (see <ref type="figure">figure 2</ref>) with a few (hand-chosen) prototypes. Since we utilize a trigram tagger, we are able to naturally capture the effect that the BOUNDARY tokens typically indicate transitions between the fields before and after the boundary token. As a post-processing step, when a token is tagged as a BOUNDARY Correct Tag Predicted Tag % of Errors token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate. Adding the BOUNDARY label yields significant improvements, as indicated by the PROTO+SIM+BOUND setting in <ref type="table">Table 5</ref>.3, surpassing the best unsupervised result of <ref type="bibr" target="#b2">Grenager et al. (2005)</ref> which is 72.4%. Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in <ref type="bibr" target="#b2">Grenager et al. (2005)</ref>.</p><p>We also compared our method to the most basic semi-supervised setting, where fully labeled documents are provided along with unlabeled ones. Roughly 25% of the data had to be labeled in order to achieve an accuracy equal to our PROTO+SIM+BOUND model, suggesting that the use of prior knowledge in the prototype system is particularly efficient.</p><p>In table 5.3, we provide the top confusions made by our PROTO+SIM+BOUND model. As can be seen, many of our confusions involve the FEATURE field, which serves as a general purpose background state, which often differs subtly from other fields such as SIZE. For instance, the parenthical comment: ( master has walk -in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field. NEIGHBORHOOD and AD-DRESS is another natural confusion resulting from the fact that the two fields share much of the same vocabulary (e.g [ ADDRESS 2525 Telegraph Ave.] vs.</p><p>[ NBRHD near Telegraph]).</p><p>Acknowledgments We would like to thank the anonymous reviewers for their comments. This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way. These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity. Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sequence tasks: (a) English POS, (b) Chinese POS, and (c) Classified ad segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: English POS prototype list</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Results on test set for ads data in (Grenager et al., 2005).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>Figure 6: English coarse POS tag structure: a) corresponds to "correct" transition structure from labeled data, b) corresponds to PROTO+SIM on 24K tokens</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Most common classified ads confusions</figDesc></figure>

			<note place="foot" n="1"> Note that this setting differs from the standard semisupervised learning setup, where a small number of fully labeled examples are given and used in conjunction with a larger amount of unlabeled data. In our prototype-driven approach, we never provide a single fully labeled example sequence. See section 5.3 for further comparison of this setting to semi-supervised learning.</note>

			<note place="foot" n="2"> At each position choosing the label which has the highest posterior probability, obtained from the forward-backward algorithm.</note>

			<note place="foot" n="3"> To be clear: this method of constructing a prototype list required statistics from the labeled data. However, we believe it to be a fair and necessary approach for several reasons. First, we wanted our results to be repeatable. Second, we did not want to overly tune this list, though experiments below suggest that tuning could greatly reduce the error rate. Finally, it allowed us to run on Chinese, where the authors have no expertise. 4 Details of distributional similarity features: To extract context vectors, we used a window of size 2 in either direction and use the first 250 singular vectors. We collected counts from all the WSJ portion of the Penn Treebank as well as the entire BLIPP corpus. We limited each word to have similarity features for its top 5 most similar prototypes. 5 Note that the presence of a prototype feature does not ensure every instance of that word type will be given its prototype&apos;s label; pressure from &quot;edge&quot; features or other prototype features can cause occurrences of a word type to be given different labels. However, rare words with a single prototype feature are almost always given that prototype&apos;s label.</note>

			<note place="foot" n="6"> Distributional similarity details: We collect for each word a context vector consisting of the counts for words occurring within three token occurrences of a word. We perform a SVD onto the first 50 singular vectors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The unsupervised induction of stochastic context-free grammars using distributional clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of field segmentation models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Meeting of the ACL</title>
		<meeting>the 43rd Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Building a large-scale annotated chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nianwen Xue Ircs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Mathematical Programming</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tagging english text with a probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="809" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformational Grammar. Cambridge University Press, Cambridge. Hinrich Schütze. 1995. Distributional part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Meeting of the ACL</title>
		<meeting>the 43rd Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Morphological features help pos tagging of unknown words across language varieties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
