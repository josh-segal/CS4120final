<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-CSAIL</orgName>
								<orgName type="institution" key="instit1">MIT-BCS</orgName>
								<orgName type="institution" key="instit2">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-CSAIL</orgName>
								<orgName type="institution" key="instit1">MIT-BCS</orgName>
								<orgName type="institution" key="instit2">University of Glasgow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">G</forename><surname>Schyns</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT-CSAIL</orgName>
								<orgName type="institution" key="instit1">MIT-BCS</orgName>
								<orgName type="institution" key="instit2">University of Glasgow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>0730-0301/06/0700-$5.00 0527</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hybrid images</term>
					<term>human perception</term>
					<term>scale space</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: A hybrid image is a picture that combines the low-spatial frequencies of one picture with the high spatial frequencies of another picture producing an image with an interpretation that changes with viewing distance. In this figure, the people may appear sad, up close, but step back a few meters and look at the expressions again. Abstract We present hybrid images, a technique that produces static images with two interpretations, which change as a function of viewing distance. Hybrid images are based on the multiscale processing of images by the human visual system and are motivated by masking studies in visual perception. These images can be used to create compelling displays in which the image appears to change as the viewing distance changes. We show that by taking into account perceptual grouping mechanisms it is possible to build compelling hybrid images with stable percepts at each distance. We show examples in which hybrid images are used to create textures that become visible only when seen up-close, to generate facial expressions whose interpretation changes with viewing distance, and to visualize changes over time within a single picture.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Here we exploit the multiscale perceptual mechanisms of human vision to create visual illusions (hybrid images) where two different interpretations of a picture can be perceived by changing the viewing distance or the presentation time. We use and extend the method originally proposed by <ref type="bibr" target="#b12">Schyns and Oliva [1994;</ref>. <ref type="figure">Fig. 1</ref> shows an example of a hybrid image assembled from two images * e-mail: oliva@mit.edu † e-mail:torralba@csail.mit.edu ‡ e-mail: p.schyns@psy.gla.ac.uk in which the faces displayed different emotions. High spatial frequencies correspond to faces with "sad" expressions. Low spatial frequencies correspond to the same faces with "happy" and "surprise" emotions (i.e., the emotions are, from left to right: happy, surprise, happy and happy). To switch from one interpretation to the other one can step away a few meters from the picture.</p><p>Artists have effectively employed low spatial frequency manipulation to elicit a percept that changes when relying on peripheral vision (e.g., <ref type="bibr" target="#b4">[Livingstone 2000;</ref><ref type="bibr" target="#b1">Dali 1996]</ref>). Inspired by this work, <ref type="bibr" target="#b14">Setlur and Gooch [2004]</ref> propose a technique that creates facial images with conflicting emotional states at different spatial frequencies. The images produce subtle expression variations with gaze changes. In this paper, we demonstrate the effectiveness of hybrid images in creating images with two very different possible interpretations.</p><p>Hybrid images are generated by superimposing two images at two different spatial scales: the low-spatial scale is obtained by filtering one image with a low-pass filter; the high spatial scale is obtained by filtering a second image with a high-pass filter. The final image is composed by adding these two filtered images. Note that hybrid images are a different technique than picture mosaics <ref type="bibr">[Sil- vers 1997]</ref>. Picture mosaics have two interpretations: a local one (which is given by the content of each of the pictures that compose the mosaic) and a global one (which is best seen at some predefined distance). Hybrid images, however, contain two coherent global image interpretations, one of which is of the low spatial frequencies, the other of high spatial frequencies.</p><p>We illustrate this technique with several proof-of-concept examples. We show how this technique can be applied to create face pictures that change expression with viewing distance, to display two configurations of a scene in a single picture, and to present textures that disappear when viewed at a distance. tered with a high-pass filter (1 − G 2 ):</p><formula xml:id="formula_0">H = I 1 · G 1 + I 2 · (1 − G 2 ),</formula><p>the operations are defined in the Fourier domain. Hybrid images are defined by two parameters: the frequency cut of the low resolution image (the one to be seen at a far distance), and the frequency cut of the high resolution image (the one to be seen up close). An additional parameter can be added by introducing a different gain for each frequency channel. For the hybrids shown in this paper we have set the gain to 1 for both spatial channels. We use gaussian filters (G 1 and G 2 ) for the low-pass and the high-pass filters. We define the cut-off frequency of each filter as the frequency for with the amplitude gain of the filter is 1/2. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the process used to create one hybrid image. The distance at which each component of a hybrid image is best seen and the distance at which the hybrid percept alternates can be fully determined as a function of the image size and the cutoff frequencies of the filters (expressed in cycles/image 1 ). When viewing the images in this paper, switch between interpretations by stepping a few meters away from the picture. Note that the larger you display the images, the farther you will have to go in order to see the alternative image interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The perception of hybrid images</head><p>In the following section we describe the motivation behind hybrid images, as they relate to studies in human perception. We will provide the framework for understanding the mechanisms involved in perception of double image percepts.</p><p>Visual psychophysics research has shown that human observers are able to comprehend the meaning of a novel image within a short glance (100 msec <ref type="bibr" target="#b11">[Potter 1975]</ref>). This phenomenal performance of rapid image understanding can be experienced while watching fast scene edits in an action movie or in a music video. Research in human perception has suggested that image understanding efficiency is based on a multi-scale, global to local analysis of the visual input <ref type="bibr" target="#b0">[Burt and Adelson 1983;</ref><ref type="bibr" target="#b5">Majaj et al. 2002]</ref>: an initial analysis of the global structure and the spatial relationships between components guides the analysis of local details <ref type="bibr" target="#b12">[Schyns and Oliva 1994;</ref><ref type="bibr" target="#b18">Watt 1987]</ref>. The global precedence hypothesis of image analysis ("seeing the forest before the trees", <ref type="bibr" target="#b6">[Navon 1977]</ref>) implies a coarse-to-fine frequency analysis of an image, where the low spatial frequency components, which are contrasted and carried by the fast magnocellular pathway, dominate early visual processing <ref type="bibr" target="#b3">Lindeberg 1993;</ref><ref type="bibr" target="#b8">Parker et al. 1992;</ref><ref type="bibr" target="#b12">Schyns and Oliva 1994;</ref><ref type="bibr" target="#b17">Sugase et al. 1999]</ref>.</p><p>Using hybrid stimuli, <ref type="bibr" target="#b12">Schyns and Oliva [1994]</ref> tested the role that spatial frequency bands play for the interpretation of natural images. When the task required identifying a scene image quickly, human observers interpreted the low spatial frequency band (at a frequency cutoff of 8 cycles/image) before the high spatial frequency band (from 24 cycles/image): when showed hybrid images for 30 ms only, observers identified the low spatial scale (e.g., they would answer "cheetah" when presented with the image from <ref type="figure" target="#fig_1">Fig. 3</ref>) whereas for 150 ms duration, they identified the high spatial scale first (e.g., tiger in <ref type="figure" target="#fig_1">Fig. 3</ref>). Interestingly, participants were unaware that the visual stimuli had two interpretations. Additional experiments suggested that the spatial frequency band preferentially selected for interpreting an image depends on the task the viewer must solve. Using hybrid faces similar to the one in <ref type="figure" target="#fig_3">Fig. 5</ref>.b, Schyns and Oliva <ref type="bibr">[1999]</ref> showed that when participants were asked to determine the emotion of an hybrid face image displayed for only 50 ms (happy, angry or neutral), they selected the low spatial frequency face (angry in <ref type="figure" target="#fig_3">Fig. 5</ref>.b), but when they had to determine the gender of the same image, they used the low spatial frequency components of the hybrid as often as the high. Again, participants did not report noticing presence of two emotions or two genders in these images. These results demonstrated that the selection of frequency bands for fast image recognition is a flexible mechanism: The image analysis might still unfold according to a low to high spatial scale processing, but human observers are able to quickly select the frequency band, low or high, that conveyed the most information to solve a given task and interpret the image. Importantly, when selecting a spatial frequency, observers were not conscious of the information in the other spatial scale.</p><p>In the study of human perception, hybrid images allow characterizing the role of different frequency channels for image recognition, and evaluate the time course of spatial frequency processing. Hybrid images provide a new paradigm in which images interpretation can be modulated by playing with viewing distance or presenta-a) b) c)  tion time. For a given distance of viewing, or a given temporal frequency a particular band of spatial frequency dominates visual processing. Visual analysis of the hybrid image still unfolds from global to local perception, but within the selected frequency band, for a given viewing distance, the observer will perceive the global structure of the hybrid first (that the image in <ref type="figure" target="#fig_1">Fig. 3</ref> represents a head), and take an additional hundred milliseconds to organize the local information into a coherent percept (organization of blobs if the image is viewed at a far distance, or organization of edges for close viewing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rules of perceptual grouping and hybrid images</head><p>In theory, one can combine any two images to create a hybrid picture. In practice, aesthetically pleasing hybrid images require following some rules that we describe in this section. In successful</p><p>Hybrid images, when one percept dominates, consciously switching to the alternative interpretation becomes almost impossible.</p><p>Only when the viewing distance changes can we switch to the alternative interpretation. In a hybrid image it is important that the alternative image is perceived as noise (lacking internal organization) or that it blends with the dominant subband.</p><p>Rules of perceptual grouping modulate the effectiveness of hybrid images. Low spatial frequencies (blobs) lack a precise definition of object shapes and region boundaries, which require the visual system to group the blobs together to form a meaningful interpretation of the coarse scale. When observers are presented with ambiguous forms they interpret the elements in the simplest way. Observers prefer an arrangement having fewer rather than more elements, having a symmetrical rather than an asymmetrical composition and generally respecting other Gestalt rules of perception.</p><p>Symmetry and repetitiveness of a pattern in the low spatial frequencies are bad: they form a strong percept that it is difficult to eliminate perceptually. If the image in the high spatial frequencies lacks the same strong grouping cues, the image interpretation corresponding to the low spatial frequencies will always be available, even when viewing from a short distance. By introducing accidental alignments it is possible to reduce the influence of one spatial channel over the other. For instance, in <ref type="figure" target="#fig_0">Fig. 2</ref> the top of the elephant (low spatial frequencies) is aligned with the horizon line (both low and high spatial frequencies). Therefore, when seeing the image up close, the top edge of the elephant can be explained by some of the fine edges. This reduces the saliency of the elephant. <ref type="figure" target="#fig_1">Fig. 3</ref> shows several examples of hybrid images with different degrees of agreement between the low and high spatial frequencies.</p><p>Color provides a very strong grouping cue that can be used to create more compelling illusions. For instance, in <ref type="figure" target="#fig_2">Fig. 4</ref> color is used only in the high spatial frequencies to enhance the bicycle and to reinforce the interpretation of the motorcycle as shadows when the image is viewed up close.</p><p>The importance of correctly choosing the cut-off frequencies for the filters is illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. In <ref type="figure" target="#fig_3">Fig. 5</ref>.a, both filters have a strong overlap, and consequently, there is not a clean transition between the two faces. For the hybrid image on <ref type="figure" target="#fig_3">Fig. 5</ref>.b, the two filters have little overlap. The result is a cleaner image that produces an unambiguous interpretation (it looks like a woman from up close and as a man from far away). This is especially important when the images are not perfectly aligned.</p><p>One interesting observation is that when the images are properly constructed, the observer seems to perceive the masked image a noise. Hybrid images break one important statistical property of real-world natural images <ref type="figure" target="#fig_5">(Fig. 6)</ref>, i.e., the correlations between outputs of pass-band filters at consecutive spatial scales. <ref type="figure" target="#fig_5">Fig. 6</ref>.a shows the cross-correlation matrix obtained between the different levels of a Laplacian pyramid for a natural image. The edges found at one scale are correlated with the edges found in the scales below and above. The same thing is obtained when two images are superimposed (additive transparency). In this case there is not a simple filter to separate both images (and the percept of the two images is mixed independently of the distance at which we observe the image). <ref type="figure" target="#fig_5">Fig. 6</ref>.c shows the correlation matrix obtained when an image is blurred (with a cutoff frequency of 16c/i) and then corrupted with additive white noise. The correlation matrix reveals which scales are dominated by the noise, as they do not have the cross-scale correlations we'd expect from a natural image. In the case of a hybrid image, the correlation matrix ( <ref type="figure" target="#fig_5">Fig. 6.d</ref>) reveals the existence of two groups. <ref type="figure" target="#fig_6">Fig. 7</ref> shows the output of a Laplacian pyramid applied to the hybrid image from <ref type="figure" target="#fig_3">Fig. 5</ref>.b. Low frequency channels and high frequency channels see different images. Note that each subband is also an hybrid image itself. If you move away from the page, you will see that, one by one, the subbands take the identity of the low-scales. At reading distance, the four images on the top row are interpreted as an angry man; the bottom, a stern woman. As you step back from the images, you will see that the angry man's face begins to appear in more subbands. The finer the scale of each subband, the farther you have to go in order to see the switch of images.</p><p>In summary, two primary mechanisms can be exploited to create compelling hybrid images. The first is maximizing the correlation   between edges in the two scales so that they blend. The second resides in the fact that the remaining edges that do not correlate with other edges across scales can be perceived as noise. This is the case in <ref type="figure" target="#fig_3">Fig. 5</ref>.b, for which there is a very compelling blending of edges across scales, but, when viewing the image up close, there seems to be some low-spatial frequency noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Capacity of scale space</head><p>Up to now, hybrid images have been obtained by mixing two images, but could it be possible to combine more than two images and still have a coherent percept that transitions as we change viewing distance? In a study about text masking, Majaj et al. <ref type="bibr">[2002]</ref> created a stimulus superimposing 4 letters, each containing energy at different spatial scales. As the observer moves away from the stimuli, they report the image switching from one letter to another. The results are interesting, but the lack of good grouping cues between the multiple scales creates an image that looks distorted. Also, multiple letters are visible at any given time. Superposition of multiple images remains an open issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications</head><p>In this section we discuss some applications (see video complementing the paper for additional examples).</p><p>Private font: We can use the hybrid images to display text that is not visible for people standing at some distance from the  screen. Commercial products for user privacy generally rely on head mounted displays or on polarized screens for which visibility decreases with viewing angle. Hybrid fonts comprises two components: the high spatial frequencies (which will contain the text) and the low spatial frequencies (which will contain the masking image).</p><p>For the high pass filter we use a gaussian filter with a width (σ ) adjusted so that σ &lt; n p , where n p is the thickness of a letter's stroke measured in pixels. The low-frequency channel (masking signal) contains a text-like texture [Portilla and <ref type="bibr" target="#b10">Simoncelli 2000]</ref>. <ref type="bibr" target="#b16">Solomon and Pelli [1994]</ref> have shown that letters are more effectively masked by a noise in the frequency band of 3 cycles per letter. Therefore we adjust the cut-off frequency of the low-pass filter to be 3 * n with n being the number of letters in a text line. The goal is to reduce the interference of the noise with the text when we viewing up close, while having an effective masking noise when looking from further away. In the example shown in <ref type="figure" target="#fig_7">Fig. 8</ref> the text is only readable from a distance below one meter. From a distance of about two meters, the text is unreadable. Masking of the low spatial frequencies is very important in producing this effect <ref type="figure" target="#fig_7">(Fig. 8)</ref>. The text in the bottom has only been high-pass filtered, and there is no masking at low spatial frequencies, therefore it remains easy to read at relatively long distances.</p><p>Hybrid textures: We can create textures that disappear with viewing distance. An example of this idea is shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. This figure shows an example of a woman's face that turns into a cat when looking close. Note that this effect can not be obtained by superimposing the woman's face and the cat's face using transparency. Using transparency (additive superposition) creates a face that will not change with distance.</p><p>Changing faces: Hybrid images are especially powerful to create images of faces that change expressions, identity, or pose as we vary the viewing distance. <ref type="figure">Fig. 1</ref> shows a compelling example of changes of facial expression. The edges at multiple scales blend producing images that look natural at all distances. In the case of face images, correct alignment between facial features is important in order to create pictures that seem unaltered. In case of misalignment, the best is to apply a distortion (affine warping) to the face that will be in the low spatial frequencies.</p><p>Time changes: <ref type="figure" target="#fig_8">Fig. 9</ref> shows an example of using an hybrid image to show two states of a house by combining two picture taken at two different instants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have described the technique, hybrid images, which permits creating images with two interpretations that change as a function of viewing distance. Despite the simplicity of the technique, the images produce very compelling surprise effects on naive observers. They also provide an interesting new visualization tool to morph two complementary images into one. Creating compelling hybrid images is an open and challenging problem, as it relies on perceptual grouping mechanisms that interact across different spatial scales. Left) The house under construction. When you view the image at a short distance, the house is seen under construction, but if you step away from the picture you will see its final state.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: hybrid images are generated by superimposing two images at two different spatial scales: the low-spatial scale is obtained by filtering one image with a low-pass filter, and the high spatial scale is obtained by filtering a second image with a high-pass filter. The final hybrid image is composed by adding these two filtered images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perceptual grouping between edges and blobs. The three images are perceived as a tiger when seen up-close and as a cheetah from far away. The differences among the three images is the degrees of alignment between the edges and blobs. Image a) contains two images superimposed without alignment. In image b), the eyes are aligned. And in image c), the head pose and the locations of eyes and mouth are aligned. Under proper alignment, the residual frequency band does not manage to build a percept. When seen up-close, it is difficult to see the cheetah's face, which is perfectly masked by the tiger's face. From far away, the tiger's edges are assimilated to the cheetah's face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Color at high spatial frequencies is used to enhance the bicycle up-close. From a distance, one sees a motorcycle. The shape of the motorcycle is interpreted as shadows up-close.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An angry man or a thoughtful woman? Both hybrid images are produced by combining the faces of an angry man (low spatial frequencies) and a stern woman (high spatial frequencies). You can switch the percept by watching the picture from a few meters. a) Bad hybrid image. The image looks ambiguous from up close due to the filter overlap. b) Good Hybrid image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Correlations across levels of a Laplacian pyramid for images following several manipulations. a) Natural image, b) two images added, c) blurry image with additive white noise, and d) hybrid image ( f 1 = 16 cycles/image, f 2 = 48 cycles/image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Output of a Laplacian pyramid revealing the components of the hybrid image of Fig. 5.b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The hybrid font becomes invisible at few meters. The bottom text remains easy to read at relatively long distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: right) Cat woman: the texture corresponding to the cat's face disappears when the image is viewed from a few meters. Left) The house under construction. When you view the image at a short distance, the house is seen under construction, but if you step away from the picture you will see its final state.</figDesc></figure>

			<note place="foot" n="2"> The design of hybrid images A hybrid image (H) is obtained by combining two images (I 1 and I 2 ), one filtered with a low-pass filter (G 1 ) and the second one fil-</note>

			<note place="foot" n="1"> We use the units cycle/image for spatial frequencies as they are independent of the image resolution. The output of a gaussian filter with a cutoff frequency of 16 cycles/image will be the same independently of the resolution of the original image. The units cycle/degree of visual angle are used to describe the resolution observed when the image has a fixed size and is seen from a fixed distance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adelson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Salvador Dali Museum Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dali</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Bulfinch Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global precedence, spatial frequency channels, and the statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Nozawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kitterle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="197" to="230" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting salient blob-like images structures and their spatial scales with a scale-space primal sketch: a method for focus of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindeberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="283" to="318" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is it warm? is it real? or just low spatial frequency?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livingstone</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">1299</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of spatial frequency channels in letter identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majaj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kurshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palomares</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1165" to="1184" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Forest before trees: the precedence of global features in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="353" to="383" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coarse blobs or fine edges? evidence that information diagnosticity changes the perception of complex visual stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliva</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schyns</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="72" to="107" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal integration of spatially filtered visual images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Role of coarse and fine information in face and object processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1448" to="1466" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Portilla</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simoncelli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="49" to="71" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meaning in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potter</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="965" to="966" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From blobs to boundary edges: Evidence for time-and spatial-scale-dependent scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schyns</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliva</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dr. angry and mr. smile: when categorization flexibly modifies the perception of faces in rapid visual presentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schyns</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliva</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="243" to="265" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is that a smile?: gaze dependent facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Setlur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gooch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NPAR &apos;04: Proceedings of the 3rd international symposium on Non-photorealistic animation and rendering</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="79" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Photomosaics. Henry Holt and Company, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visual filter mediating letter identification recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pelli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="395" to="397" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global and fine information coded by single neurons in the temporal visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugase</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawano</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="869" to="873" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scanning from coarse to fine spatial scales in the human visual system after onset of a stimulus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Watt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt.Soc.Am: A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
