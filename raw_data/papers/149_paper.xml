<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Hypothesis Testing under Stochastic Deadlines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">I Frazier</forename><surname>Orfe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Princeton University Princeton</orgName>
								<orgName type="institution" key="instit2">CSBMB Princeton University</orgName>
								<address>
									<postCode>08544, 08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ, NJ</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
							<email>ajyu@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Princeton University Princeton</orgName>
								<orgName type="institution" key="instit2">CSBMB Princeton University</orgName>
								<address>
									<postCode>08544, 08544</postCode>
									<settlement>Princeton</settlement>
									<region>NJ, NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Hypothesis Testing under Stochastic Deadlines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most models of decision-making in neuroscience assume an infinite horizon, which yields an optimal solution that integrates evidence up to a fixed decision threshold; however, under most experimental as well as naturalistic behavioral settings, the decision has to be made before some finite deadline, which is often experienced as a stochastic quantity, either due to variable external constraints or internal timing uncertainty. In this work, we formulate this problem as sequential hypothesis testing under a stochastic horizon. We use dynamic programming tools to show that, for a large class of deadline distributions, the Bayes-optimal solution requires integrating evidence up to a threshold that declines monotonically over time. We use numerical simulations to illustrate the optimal policy in the special cases of a fixed deadline and one that is drawn from a gamma distribution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Major strides have been made in understanding the detailed dynamics of decision making in simple two-alternative forced choice (2AFC) tasks, at both the behavioral and neural levels. Using a combination of probabilistic and dynamic programming tools, it has been shown that when the decision horizon is infinite (i.e. no deadline), the optimal policy is to accumulate sensory evidence for one alternative versus the other until a fixed threshold, and report the corresponding hypothesis <ref type="bibr" target="#b0">[1]</ref>. Under similar experimental conditions, it appears that humans and animals accumulate information and make perceptual decisions in a manner close to this optimal strategy <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and that neurons in the posterior parietal cortex exhibit response dynamics similar to that prescribed by the optimal algorithm <ref type="bibr" target="#b4">[5]</ref>. However, in most 2AFC experiments, as well as in more natural behavior, the decision has to be made before some finite deadline. This corresponds to a finite-horizon sequential decision problem. Moreover, there is variability associated with that deadline either due to external variability associated with the deadline imposition itself, or due to internal timing uncertainty about how much total time is allowed and how much time has already elapsed. In either case, with respect to the observer's internal timer, the deadline can be viewed as a stochastic quantity.</p><p>In this work, we seek to understand the optimal strategy and its dynamics for decision-making under the pressure of a stochastic deadline. We will show through analytical and numerical analysis that the optimal policy is a monotonically declining decision threshold over time. Declining decision thresholds have been used previously in <ref type="bibr" target="#b5">[6]</ref> to model the speed vs. accuracy tradeoff, and also in the context of sequential hypothesis testing (see <ref type="bibr" target="#b6">[7]</ref> for a survey). In the following, we first present a formal model of the problem, as well as the main theoretical results (Sec. 2). We then use numerical simulations to examine the optimal policy in some specific examples (Sec. 3).</p><p>1 − p 0 , they are generated from an alternate probability density f 0 . Let θ be index of the generating distribution. The objective is to decide whether θ is 0 or 1 quickly and accurately, while also under the pressure of a stochastic decision deadline.</p><p>We define x t (x 1 , x 2 , . . . , x t ) to be the vector of observations made by time t. This vector of observations gives information about the generating density θ. Defining p t P{θ = 1 | x t }, we observe that p t+1 may be obtained iteratively from p t via Bayes' rule,</p><formula xml:id="formula_0">p t+1 = P{θ = 1 | x t+1 } = p t f 1 (x t+1 ) p t f 1 (x t+1 ) + (1 − p t )f 0 (x t+1 ) .<label>(1)</label></formula><p>Let D be a deadline drawn from a known distribution that is independent of the observations x t . We will assume that the deadline D is observed immediately and effectively terminates the trial. Let c &gt; 0 be the cost associated with each unit time of decision delay, and d ≥ .5 be the cost associated with exceeding the deadline, where both c and d are normalized against the (unit) cost of making an incorrect decision. We choose d ≥ .5 so that d is never smaller than the expected penalty for guessing at θ. This avoids situations in which we prefer to exceed the deadline.</p><p>A decision-policy π is a sequence of mappings, one for each time t, from the observations so far to the set of possible actions: stop and choose θ = 0; stop and choose θ = 1; or continue sampling. We define τ π to be the time when the decision is made to stop sampling under decision-policy π, and δ π to be the hypothesis chosen at this time -both are random variables dependent on the sequence of observations. More formally, π π 0 , π 1 , . . ., where π t (x t ) → {0, 1, continue}, and τ π min(D, inf{t ∈ N : π t (x t ) ∈ {0, 1}}), δ π π τπ (x τπ ). We may also define σ π inf{t ∈ N : π t (x t ) ∈ {0, 1}} to be the time when the policy would choose to stop sampling if the deadline were to fail to occur. Then</p><formula xml:id="formula_1">τ π = min(D, σ π ).</formula><p>Our loss function is defined to be l(τ, δ; θ, D) = 1 {δ񮽙 =θ} 1 {τ &lt;D} + cτ + d1 {τ ≥D} . The goal is to find a decision-policy π which minimizes the total expected loss</p><formula xml:id="formula_2">L π l(τ π , δ π ; θ, D) θ,D,x = P(δ π 񮽙 = θ, τ π &lt; D) + cτ π + d P(D ≤ τ π ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dynamic Programming</head><p>A decision policy is characterized by how τ and δ are generated as a function of the data observed so far. Thus, finding the optimal decision-policy is equivalent to finding the random variables τ and δ that minimize l(τ, δ; θ, D). The optimal policy decides whether or not to stop based on whether p t is inside a set C t ⊆ [0, 1] or not. Our goal is to show that C t is a continuous interval, that C t+1 ⊆ C t , and that for large enough t, C t is empty. That is, the optimal policy is to iteratively compute p t based on incoming data, and to decide for the respective hypothesis as soon as it hits either a high (δ = 1) or low (δ = 0) threshold. Furthermore, the two thresholds decay toward each other over time and eventually meet.</p><p>We will use tools from dynamic programming to analyze this problem. Our approach is illustrated in <ref type="figure" target="#fig_2">Fig. 2.1</ref>. The red line denotes the cost of stopping at time t as a function of the current belief p t = p. The blue line denotes the cost of continuing at least one more time step, as a function of p t . The black line denotes the cost of continuing at least two more time steps, as a function of p t . Because the cost of continuing is concave in p t (Lemma 1), and larger than stopping for p t ∈ {0, 1} (Lemma 4), the continuation region is an interval delimited by where the costs of continuing and stopping intersect (blue dashed lines). Moreover, because the cost of continuing two more timesteps is always larger than that of continuing one more for a given amount of belief (Lemmas 2 and 3), that "window" of continuation narrows over time <ref type="bibr">(Main Theorem)</ref>. This method of proof parallels that of optimality for the classic sequential probability ratio test in <ref type="bibr" target="#b8">[9]</ref>.</p><p>Before proving the lemmas and the theorem, we first introduce some additional definitions. The value function V : N × [0, 1] → R + specifies the minimal cost (incurred by the optimal policy) at time t, given that the deadline has not yet occurred, that x t have been observed, and that the current cumulative evidence for θ = 1 is p t :</p><formula xml:id="formula_3">V (t, p t ) inf τ ≥t,δ l(τ, δ; θ, D) | D &gt; t, p t θ,D,x .</formula><p>The cost associated with continuing at time t, known as the Q-factor for continuing and denoted by Q, takes the form </p><formula xml:id="formula_4">Q(t, p t ) inf τ ≥t+1,δ l(τ, δ; θ, D) | D &gt; t, p t θ,D,x .<label>(3)</label></formula><formula xml:id="formula_5">Q(t + 1, p) − c Q(t, p) ¯ Q(t, p)</formula><p>Continuing vs. Stopping Cost p t = p <ref type="figure">Figure 1</ref>: Comparison of the cost Q(t, p) of stopping at time t (red); the cost Q(t, p) of continuing at time t (blue solid line); and Q(t + 1, p) − c (black solid line), which is the cost of continuing at time t + 1 minus an adjustment Q(t + 1, p) − Q(t, p) = c. The continuation region C t is the interval between the intersections of the solid blue and red lines, marked by the blue dotted lines, and the continuation region C t+1 is the interval between the intersections of the solid black and red lines, marked by the black dotted lines. Note that</p><formula xml:id="formula_6">Q(t + 1, p) − c ≥ Q(t, p), so C t contains C t+1 .</formula><p>Note that, in general, both V (t, p t ) and Q(t, p t ) may be difficult to compute due to the need to optimize over infinitely many decision policies. Conversely, the cost associated with stopping at time t, known as the Q-factor for stopping and denoted by Q, is easily computed as</p><formula xml:id="formula_7">Q(t, p t ) = inf δ=0,1 l(t, δ; θ, D) | D &gt; t, p t θ,D,x = min{p t , 1 − p t } + ct,<label>(4)</label></formula><p>where the infimum is obtained by choosing δ = 0 if p t ≤ .5 and choosing δ = 1 otherwise.</p><p>An optimal stopping rule is to stop the first time the expected cost of continuing exceeds that of stopping, and to choose δ = 0 or δ = 1 to minimize the probability of error given the accumulated evidence (see <ref type="bibr" target="#b8">[9]</ref>). That is, τ * = inf{t ≥ 0 : Q(t, p t ) ≤ Q(t, p t )} and δ * = 1 {p τ * ≥1/2} . We define the continuation region at time t by</p><formula xml:id="formula_8">C t p t ∈ [0, 1] : Q(t, p t ) &gt; Q(t, p t ) so that τ * = inf{t ≥ 0 : p t / ∈ C t }.</formula><p>Although we have obtained an expression for the optimal policy in terms of Q(t, p) and Q(t, p), computing Q(t, p) is difficult in general. Lemma 1. The function p → Q(t, p t ) is concave with respect to p t for each t ∈ N. Proof. We may restrict the infimum in Eq. 3 to be over only those τ and δ depeding on D and the future observations x t+1 {x t+1 , x t+2 , . . .}. This is due to two facts. First, the expectation is conditioned on p t , which contains all the information about θ available in the past observations x t , and makes it unnecessary for the optimal policy to depend on x t except through p t . Second, dependence on p t in the optimal policy may be made implicit by allowing the infimum to be attained by different τ and δ for different values of p t but removing explicit dependence on p t from the individual policies over which the infimum is taken. With τ and δ chosen from this restricted set of policies, we note that the distribution of the future observations x t+1 is entirely determined by θ and so we have l(τ,</p><formula xml:id="formula_9">δ; θ, D) | θ, p t D,xt+1 = l(τ, δ; θ, D) | θ D,xt+1</formula><p>. Summing over the possible values of θ, we may then write:</p><formula xml:id="formula_10">l(τ, δ; θ, D) | p t θ,D,xt+1 = k∈{0,1} l(τ, δ; θ, D) | θ = k D,xt+1 P{θ = k | p t } = l(τ, δ; θ, D) | θ = 0 D,xt+1 (1 − p t ) + l(τ, δ; θ, D) | θ = 1 D,xt+1 p t .</formula><p>Eq. (3) can then be rewritten as:</p><formula xml:id="formula_11">Q(t, p t ) = inf τ ≥t+1,δ l(τ, δ; θ, D) | θ = 0 D,xt+1 (1 − p t ) + l(τ, δ; θ, D) | θ = 1 D,xt+1 p t ,</formula><p>where this infimum is again understood to be taken over this set of policies depending only upon observations after time t. Since neither l(τ, δ; θ, D) | θ = 0 nor l(τ, δ; θ, D) | θ = 1 depend on p t , this is the infimum of a collection of linear functions in p t , and hence is concave in p t ( <ref type="bibr" target="#b7">[8]</ref>).</p><p>We now need a lemma describing how expected cost depends on the distribution of the deadline. Let D ′ be a deadline whose distribution is different than that of D. Let π * be the policy that is optimal given that the deadline has distribution D, and denote σ π * by σ * . Then define</p><formula xml:id="formula_12">V ′ (t, p t ) min(p σ * , 1 − p σ * )1 {σ * &lt;D ′ } + c min(σ * , D ′ ) + d1 {σ * ≥D ′ } | p t , D ′ &gt; t θ,D,x</formula><p>so that V ′ gives the expected cost of taking the stopping time σ * which is optimal for deadline D and applying it to the situation with deadline D ′ . Similarly, let Q ′ (t, p t ) and Q ′ (t, p t ) denote the corresponding expected costs under σ * and D ′ given that we continue or stop, respectively, at time t given p t and D ′ &gt; t. Note that Q ′ (t, p t ) = Q(t, p t ) = min(p t , 1 − p t ) + ct. These definitions are the basis for the following lemma, which essentially shows that replacing the deadline D which a less urgent deadline D ′ lowers cost. This lemma is needed for Lemma 3 below.</p><formula xml:id="formula_13">Lemma 2 If D ′ is such that P{D ′ &gt; t + 1 | D ′ &gt; t} ≥ P{D &gt; t + 1 | D &gt; t} for all t, then V ′ (t, p) ≤ V (t, p) and Q ′ (t, p) ≤ Q(t, p)</formula><p>for all t and p.</p><p>Proof. First let us show that if we have V ′ (t + 1, p ′ ) ≤ V (t + 1, p ′ ) for some fixed t and all p ′ , then we also have Q ′ (t, p) ≤ Q(t, p) for that same t and all p. This is the case because, if we fix t, then</p><formula xml:id="formula_14">Q(t, p t ) = (d + c(t + 1)) P{D = t+1 | D &gt; t} + V (t + 1, p t+1 ) | p t x t+1 P{D &gt; t+1 | D &gt; t} = d + c(t + 1) + V (t + 1, p t+1 ) − (d + c(t + 1)) | p t x t+1 P{D &gt; t+1 | D &gt; t} ≥ d + c(t + 1) + V (t + 1, p t+1 ) − (d + c(t + 1)) | p t x t+1 P{D ′ &gt; t+1 | D ′ &gt; t} ≥ d + c(t + 1) + V ′ (t + 1, p t+1 ) − (d + c(t + 1)) | p t x t+1 P{D ′ &gt; t+1 | D ′ &gt; t} = Q ′ (t, p).</formula><p>In the first inequality we have used two facts: that</p><formula xml:id="formula_15">V (t + 1, p t+1 ) ≤ Q(t + 1, p t+1 ) = min(p t+1 , 1 − p t+1 ) + c(t + 1) ≤ d + c(t + 1) (which is true because d ≥ .5); and that P{D &gt; t + 1 | D &gt; t} ≤ P{D ′ &gt; t + 1 | D ′ &gt; t}.</formula><p>In the second inequality we have used our assumption that</p><formula xml:id="formula_16">V ′ (t + 1, p ′ ) ≤ V (t + 1, p ′ ) for all p ′ .</formula><p>Now consider a finite horizon version of the problem where σ * is only optimal among stopping times bounded above by a finite integer T . We will show the lemma for this case, and the lemma for the infinite horizon version of the problem follows by taking the limit as T → ∞.</p><p>We induct backwards on t. Since σ * is required to stop at T , we have</p><formula xml:id="formula_17">V (T, p T ) = Q(T, p T ) = Q ′ (T, p T ) = V ′ (T, p T )</formula><p>. Now for the induction step. Fix p and t &lt; T . If σ * chooses to stop at t when p t = p, then V (t, p) = Q(t, p) = Q ′ (t, p) = V ′ (t, p). If σ * continues instead, then <ref type="figure">p)</ref> by the induction hypothesis.</p><formula xml:id="formula_18">V (t, p) = Q(t, p) ≥ Q ′ (t, p) = V ′ (t,</formula><p>Note the requirement that d ≥ 1/2 in the previous lemma. If this requirement is not met, then if p t is such that d &lt; min(p t , 1 − p t ) then we may prefer to get timed out rather than choose δ = 0 or δ = 1 and suffer the expected penalty of min(p t , 1 − p t ) for choosing incorrectly. In this situation, since the conditional probability P{D = t + 1 | D &gt; t} that we will time out in the next time period grows as time moves forward, the continuation region may expand with time rather than contract. Under most circumstances, however, it seems reasonable to assume the deadline cost to be at least as large as that of making an error.</p><p>We now state Lemma 3, which shows that the cost of delaying by one time period is as least as large as the continuation cost c, but may be larger because the delay causes the deadline to approach more rapidly.</p><p>Lemma 3. For each t ∈ N and p ∈ (0, 1),</p><formula xml:id="formula_19">Q(t − 1, p t−1 = p) ≤ Q(t, p t = p) − c. Proof. Fix t. Let σ * inf{s ≥ t + 1 : p s / ∈ C s } so that min(σ * , D) attains the infimum for Q(t, p t ). Also define σ ′ inf{s ≥ t : p s / ∈ C s+1 } and τ ′ min(D, σ ′ ).</formula><p>Since τ ′ is within the set over which the infimum defining Q(t − 1, p) is taken,</p><formula xml:id="formula_20">Q(t − 1, p) ≤ min(p τ ′ , 1 − p τ ′ )1 {τ ′ &lt;D} + cτ ′ + d1 {τ ′ ≥D} | D &gt; t − 1, p t−1 = p D,xt = min(p σ ′ , 1 − p σ ′ )1 {σ ′ &lt;D} + c min(D, σ ′ ) + d1 {σ ′ ≥D} | D &gt; t − 1, p t−1 = p D,xt = min(p σ * , 1−p σ * )1 {σ * −1&lt;D} + c min(D, σ * −1) + d1 {σ * −1≥D} | D &gt; t−1, p t = p D,xt+1 ,</formula><p>where the last step is justified by the stationarity of the observation process, which implies that the joint distribution of (p s ) s≥t , p σ * , and σ * conditioned on p t = p is the same as the joint distribution of (p s−1 ) s≥t , p σ ′ , and σ ′ + 1 conditioned on p t−1 = p. Let D ′ = D + 1 and we have</p><formula xml:id="formula_21">Q ′ (t, p) = min(p σ * , 1−p σ * )1 {σ * &lt;D ′ } + c min(D ′ , σ * ) + d1 {σ * ≥D ′ } | D ′ &gt; t, p t = p D ′ ,xt+1 , so Q(t − 1, p) ≤ Q ′ (t, p) − c. Finally, as D ′ satisfies the requirements of Lemma 2, Q ′ (t, p) ≤ Q(t, p).</formula><p>Lemma 4. For t ∈ N, Q(t, 0) = Q(t, 1) = c(t + 1) + dP{D = t + 1 | D &gt; t}. Proof. On the event p t = 0, we have that P{θ = 0} = 1 and the policy attaining the infimum in (3) is τ * = t+1, δ * = 0. Thus, Q(t, 0) becomes</p><formula xml:id="formula_22">Q(t, 0) = l(τ * , δ * ; θ, D) | D &gt; t, p t = 0 D,xt+1 = l(τ * , δ * ; θ, D) | D &gt; t, θ = 0 D,xt+1 = d1 {t+1≥D} + c(t + 1) | D &gt; t, θ = 0 D,xt+1 = c(t+1) + dP{D = t+1 | D &gt; t}.</formula><p>Similarly, on the event p t = 1, we have that P{θ = 1} = 1 and the policy attaining the infimum in <ref type="formula" target="#formula_4">(3)</ref> is τ * = t+1, δ * = 1. Thus, Q(t, 1) = c(t+1) + dP{D ≤ t + 1 | D &gt; t}.</p><p>We are now ready for the main theorem, which shows that C t is either empty or an interval, and that C t+1 ⊆ C t . To illustrate our proof technique, we plot Q(t, p), Q(t, p), and Q(t + 1, p) − c as functions of p in <ref type="figure" target="#fig_2">Figure 2.</ref>1. As noted, the continuation region C t is the set of p such that Q(t, p) ≤ Q(t, p), To show that C t is either empty or an interval, we note that Q(t, p) is a concave function in p (Lemma 1) whose value at the endpoints p = 0, 1 are greater than the corresponding values of Q(t, p) (Lemma 4). Such a concave function may only intersect Q(t, p), which is a constant plus min(p, 1 − p), either twice or not at all. When it intersects twice, we have the situation pictured in <ref type="figure" target="#fig_2">Figure 2</ref>.1, in which C t is a non-empty interval, and when it does not intersect C t is empty.</p><p>To show that C t+1 ⊆ C t we note that the difference between Q(t + 1, p) and Q(t, p) is the constant c. Thus, to show that C t , the set where Q(t, p) contains Q(t, p), is larger than C t+1 , the set where Q(t + 1, p) is larger than Q(t + 1, p), it is enough to show that the difference between Q(t + 1, p) and Q(t, p) is at least as large as the adjustment c, which we have done in Lemma 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem.</head><p>At each time t ∈ N, the optimal continuation region C t is either empty or a closed interval, and C t+1 ⊆ C t . Proof. Fix t ∈ N. We begin by showing that C t+1 ⊆ C t . If C t+1 is empty then the statement follows trivially, so consider the case when C t+1 񮽙 = ∅. Choose p ∈ C t+1 . Then</p><formula xml:id="formula_23">Q(t, p) ≤ Q(t + 1, p) − c ≤ Q(t + 1, p) − c = min{p, 1 − p} + ct = Q(t, p). Thus, p ∈ C t , implying C t+1 ⊆ C t .</formula><p>Now suppose that C t is non-empty and we will show it must be a closed interval. Let a t inf C t and b t sup C t . Since C t is a non-empty subset of <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>, we have a t , b t ∈ <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Furthermore, a t &gt; 0 because Q(t, p) ≥ c(t + 1) + dP{D = t + 1 | D &gt; t} &gt; ct = Q(t, 0) for all p, and the continuity of Q(t, ·) implies that Q(t, p) &gt; Q(t, p) &gt; 0 for p in some open interval around 0. Similarly, b t &lt; 1. Thus, a t , b t ∈ (0, 1).</p><p>We will show first that [a t , 1/2] ⊆ C t . If a t &gt; 1/2 then this is trivially true, so consider the case that a t ≤ 1/2. Since Q(t, ·) is concave on the open interval (0, 1), it must also be continuous there. This and the continuity of Q imply that Q(t, a t ) = Q(t, a t ). Also, Q(t, 0) &gt; Q(t, 0) by Lemma 4. Thus a t &gt; 0 and we may take a left-derivative at a t . For any ε ∈ (0, a t ), a t − ε / ∈ C t so Q(a t − ε) &gt; Q(a t − ε). This implies together with Q(t, a t ) = Q(t, a t ) that</p><formula xml:id="formula_24">∂ − ∂p Q(t, a t ) = lim ε→0 + Q(t, a t ) − Q(t, a t − ε) ε ≤ lim ε→0 + Q(t, a t ) − Q(t, a t − ε) ε = ∂ − ∂p Q(t, a t ).</formula><p>Since Q(t, ·) is concave by Lemma 1 and Q(t, ·) is linear on [0, 1/2], we have for any p ′ ∈ [a t , 1/2],</p><formula xml:id="formula_25">∂ − ∂p Q(t, p ′ ) ≤ ∂ − ∂p Q(t, a t ) ≤ ∂ − ∂p Q(t, a t ) = ∂ − ∂p Q(t, p ′ ).</formula><p>Since Q(t, ·) is concave, it is differentiable except at countably many points, so for any p ∈ [a t , 1/2],</p><formula xml:id="formula_26">Q(t, p) = Q(t, a t ) + p a t ∂ − ∂p Q(t, p ′ ) dp ′ ≤ Q(t, a t ) + p a t ∂ − ∂p Q(t, p ′ ) dp ′ = Q(t, p).</formula><p>Therefore p ∈ C t , and, more generally,</p><formula xml:id="formula_27">[a t , 1/2] ⊆ C t . By a similar argument, [1/2, b t ] ⊆ C t . Finally, C t ⊆ [a t , b t ] ⊆ [a t , 1/2] ∪ [1/2, b t ] ⊆ C t and we must have C t = [a t , b t ].</formula><p>We also include the following proposition, which shows that if D is finite with probability 1 then the continuation region must eventually narrow to nothing.</p><p>Proposition. If P{D &lt; ∞} = 1 then there exists a T &lt; ∞ such that C T = ∅. Proof. First consider the case when D is bounded, so P{D ≤ T + 1} = 1 for some time T &lt; ∞.</p><formula xml:id="formula_28">Then, Q(T, p T ) = d + c(T + 1), while Q(T, p T ) = cT + min(p T , 1 − p T ) ≤ cT + 1/2. Thus Q(T, p T ) − Q(T, p T ) ≥ d + c − 1/2 &gt; 0, and C T = ∅.</formula><p>Now consider the case when P{D &gt; t} &gt; 0 for every t. By neglecting the error probability and including only continuation and deadline costs, we obtain Q(t, p t )</p><formula xml:id="formula_29">≥ d P{D = t+1 | D &gt; t}+c(t+1).</formula><p>Bounding the error probability by 1/2 we obtain Q(t, p t ) ≤ ct + 1/2. Thus, </p><formula xml:id="formula_30">Q(t, p t ) − Q(t, p t ) ≥ c + d P{D = t + 1 | D &gt; t} − 1/2. Since P{D &lt; ∞} = 1, lim t→∞ c + d P{D = t+ 1 | D &gt; t} − 1/2 = c + d − 1/2 &gt; 0,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Computational simulations</head><p>We conducted a series of simulations in which we computed the continuation region and distributions of response time and accuracy for the optimal policy for several choices of the parameters c and d, and for the distribution of the deadline D. We chose the observation x t to be a Bernoulli random variable under both f 0 and f 1 for every t = 1, 2, . . . with different values for q θ P{x i = 1 | θ}. In our simulations we chose q 0 = .45 and q 1 = .55.</p><p>We computed optimal policies for two different forms of deadline distribution: first for a deterministic deadline fixed to some known constant; and second for a gamma distributed deadline. The gamma distribution with parameters k &gt; 0 and β &gt; 0 has density</p><formula xml:id="formula_31">(β k /Γ(k))x k−1 e −βx for x &gt; 0,</formula><p>where Γ(·) is the gamma function. The parameters k and β, called the shape and rate parameters respectively, are completely determined by choosing the mean and the standard deviation of the distribution since the gamma distribution has mean k/β and variance k/β 2 . A fixed deadline T may actually be seen as a limiting case of a gamma-distributed deadline by taking both k and β to infinity such that k/β = T is fixed.</p><p>We used the table-look-up form of the backward dynamic programming algorithm (see, e.g., <ref type="bibr" target="#b9">[10]</ref>) to compute the optimal Q-factors. We obtained approximations of the value function and Q-factors at a finite set of equally spaced discrete points {0, 1/N, . . . , (N − 1)/N, 1} in the interval <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. In our simulations we chose N = 999. We establish a final time T that is large enough that P{D ≤ T } is nearly 1, and thus P{τ * ≤ T } is also nearly 1. In our simulations we chose T = 60. We approximated the value function V (T, p T ) at this final time by Q(T, p T ). Then we calculated value functions and Q-factors for previous times recursively according to Bellman's equation:</p><formula xml:id="formula_32">Q(t, p) = V (t + 1, p t+1 ) | p t = p p t+1 ; V (t, p) = min(Q(t, p), Q(t, p)).</formula><p>This expectation relating Q(t, ·) to V (t + 1, ·) may be written explicitly using our hypotheses and Eq. 1 to define a function g so that p t+1 = g(p t , x t+1 ). In our case this function is defined by</p><formula xml:id="formula_33">g(p t , 1) (p t q 1 )/(p t q 1 + (1 − p t )q 0 ) and g(p t , 0) (p t (1 − q 1 ))/(p t (1 − q 1 ) + (1 − p t )(1 − q 0 )). Then we note that P{x t+1 = 1 | p t } = P{x t+1 = 1 | θ = 1}p t + P{x t+1 = 1 | θ = 0}(1 − p t ) = p t q 1 + (1 − p t )q 0 , and similarly P{x t+1 = 0 | p t } = p t (1 − q 1 ) + (1 − p t )(1 − q 0 ). Then Q(t, p t ) = (c(t+1)+d)P{D ≤ t+1 | D &gt; t} + P{D &gt; t+1 | D &gt; t} [ V t+1, g(p t , 1) p t q 1 +(1 − p t )q 0 +V t+1, g(p t , 0) p t (1 − q 1 )+(1 − p t )(1 − q 0 ) .</formula><p>We computed continuation regions C t from these Q-factors, and then used Monte Carlo simulation with 10 6 samples for each problem setting to estimate P{δ = θ | τ = t} and P{τ = t} as functions of t. The results of these computational simulations are shown in <ref type="figure" target="#fig_2">Figure 2</ref>. We see in <ref type="figure" target="#fig_2">Fig. 2A</ref> that the decision boundaries for a fixed deadline (solid blue) are smoothly narrowing toward the midline. Clearly, at the last opportunity for responding before the deadline, the optimal policy would always generate a response (and therefore the thresholds merge), since we assumed that the cost of penalty is greater than the expected cost of making an error: d ≥ .5 (since the optimal policy is to choose the hypothesis with probability ≥ .5, the expected probability of error is always ≤ .5). At the time step before, the optimal policy would only continue if one more data point is going to improve the belief state enough to offset the extra time cost c. Therefore, the optimal policy only continues for a small "window" around .5 even though it has the opportunity to observe one more data point. At earlier times, the window "widens" following similar logic. When uncertainty about the deadline increases (larger std(D); shown in dashed and dash-dotted blue lines), the optimal thresholds are squeezed toward each other and to the left, the intuition being that the threat of encountering the deadline spreads earlier and earlier into the trial. The red lines denote the average accuracy for different stopping times obtained from a million Monte Carlo simulations of the observation-decision process. They closely follow the decision thresholds (since the threshold is on the posterior probability p τ ), but are slightly larger, because p τ must exceed the threshold, and p t moves in discrete increments due to the discrete Bernoulli process.</p><p>The effect of decreasing the mean deadline is to shift the decision boundaries left-ward, as shown in <ref type="figure" target="#fig_2">Fig. 2B</ref>. The effect of increasing the cost of time c is to squeeze the boundaries toward the midline <ref type="figure" target="#fig_2">(Fig. 2C</ref> -this result is analogous to that seen in the classical sequential probability ratio test for the infinite-horizon case. The effect of increasing d is to squeeze the thresholds to the left <ref type="figure" target="#fig_2">(Fig. 2D)</ref>, and the rate of shifting is on the order of log(d) because the tail of the gamma distribution is falling off nearly exponentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this work, we formalized the problem of sequential hypothesis testing (of two alternatives) under the pressure of a stochastically sampled deadline, and characterized the optimal policy. For a large class of deadline distributions (including gamma, normal, exponential, delta), we showed that the optimal policy is to report a hypothesis as soon as the posterior belief hits one of a pair of monotonically declining thresholds (toward the midline). This generalizes the classical infinite horizon case in the limit when the deadline goes to infinity, and the optimal policy reverts to a pair of fixed thresholds as in the sequential probability ratio test <ref type="bibr" target="#b0">[1]</ref>. We showed that the decision policy becomes more conservative (thresholds pushed outward and to the right) when there's less uncertainty about the deadline, when the mean of the deadline is larger, when the linear temporal cost is larger, and when the deadline cost is smaller.</p><p>In the theoretical analysis, we assumed that D has the property that P{D &gt; t+u | D &gt; t} is nonincreasing in t for each u ≥ 0 over the set of t such that P{D &gt; t} &gt; 0. This assumption implies that, if the deadline has not occurred already, then the likelihood that it will happen soon grows larger and larger, as time passes. The assumption is violated by multi-modal distributions, for which there is a large probability the deadline will occur at some early point in time, but if the deadline does not occur by that point in time then will not occur until some much later time. This assumption is met by a fixed deadline (std(D)→ 0), and also includes the classical infinite-horizon case (D → ∞) as a special case (and the optimal policy reverts to the sequential probability ratio test). This assumption is also met by any distribution with a log-concave density because log P{D &gt; t + u | D &gt; t} = log P{D &gt; t+u} − log P{D &gt; t} = F (t+u) − F (t), where F (t) log P{D &gt; t}. If the density of D is log-concave, then F is concave ( <ref type="bibr" target="#b7">[8]</ref>), and the increment F (t+u)−F (t) is non-increasing in t.</p><p>Many common distributions have log-concave densities, including the exponential distribution, the gamma distribution, the normal distribution, and the uniform distribution on an interval.</p><p>We used gamma distributions for the deadline in the numerical stimulations. There are several empirical properties about timing uncertainty in humans and animals that make the gamma distribution particularly suitable. First, realizations from the gamma distribution are always non-negative, which is consistent with the assumption that a subject never thinks a deadline has passed before the experiment has started. Second, if we fix the rate parameter β and vary the shape k, then we obtain a collection of deadline distributions with different means whose variance and mean are in a fixed ratio, which is consistent with experimental observations <ref type="bibr" target="#b10">[11]</ref>. Third, for large values of k the gamma distribution is approximately normal, which is also consistent with experimental observations <ref type="bibr" target="#b10">[11]</ref>. Finally, a gamma distributed random variable with mean µ may be written as the sum of k = µβ independent exponential random variables with mean 1/β, so if the brain were able to construct an exponential-distributed timer whose mean 1/β were on the order of milliseconds, then it could construct a very accurate gamma-distributed timer for intervals of several seconds by resetting this exponential timer k times and responding after the kth alarm. This has interesting ramifications for how sophisticated timers for relatively long intervals can be constructed from neurons that exhibit dynamics on the order of milliseconds.</p><p>This work makes several interesting empirical predictions. Subjects who have more internal uncertainty, and therefore larger variance in their perceived deadline stochasticity, should respond to stimuli earlier and with lower accuracy. Similarly, the model makes quantitative predictions about the subject's performance when the experimenter explicitly manipulates the mean deadline, and the relative costs of error, time, and deadline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and there exists a T such that c + d P{D = t+1 | D &gt; t} − 1/2 &gt; 0 for every t ≥ T . This implies that, for t ≥ T and p t ∈ [0, 1], Q(t, p t ) − Q(t, p t ) &gt; 0 and C t = ∅.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plots of the continuation region C t (blue), and the probability of a correct response P{δ = θ | τ = t} (red). The default settings were c = .001, d = 2, mean(D) = 40, std(D) = 1, and q 0 = 1−q 1 = .45. In each plot we varied one of them while keeping the others fixed. In (A) we varied the standard deviation of D, in (B) the mean of D, in (C) the value of c, and in (D) the value of d.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jonathan Cohen, Savas Dayanik, Philip Holmes, and Warren Powell for helpful discussions. The first author was supported in part by the Air Force Office of Scientific Research under grant AFOSR-FA9550-05-1-0121.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolfowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statisti</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="326" to="365" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Response Times: Their Role in Inferring Elementary Mental Org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Oxford Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R &amp;amp;</forename><surname>Ratcliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="347" to="56" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pyschol. Rev</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="700" to="65" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J I &amp;amp;</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shadlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mozer</surname></persName>
		</author>
		<title level="m">Proc. Twenty Sixth Annual Conference of the Cognitive Science Society</title>
		<meeting>Twenty Sixth Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="981" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sequential Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Siegmund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S &amp;amp;</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An Introduction to Signal Detection and Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the curses of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rakitin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Anim. Behav. Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
