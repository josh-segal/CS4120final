<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cluster-Based Delta Compression of a Collection of Files</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Memon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Suel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitre</forename><surname>Trendafilov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cluster-Based Delta Compression of a Collection of Files</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Delta compression techniques are commonly used to succinctly represent an updated version of a file with respect to an earlier one. In this paper, we study the use of delta compression in a somewhat different scenario, where we wish to compress a large collection of (more or less) related files by performing a sequence of pairwise delta compressions. The problem of finding an optimal delta encoding for a collection of files by taking pairwise deltas can be reduced to the problem of computing a branching of maximum weight in a weighted directed graph, but this solution is inefficient and thus does not scale to larger file collections. This motivates us to propose a framework for cluster-based delta compression that uses text clustering techniques to prune the graph of possible pairwise delta encodings. To demonstrate the efficacy of our approach, we present experimental results on collections of web pages. Our experiments show that cluster-based delta compression of collections provides significant improvements in compression ratio as compared to individually compressing each file or using tar+gzip, at a moderate cost in efficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Delta compressors are software tools for compactly encoding the differences between two files or strings in order to reduce communication or storage costs. Examples of such tools are the diff and bdiff utilities for computing edit sequences between two files, and the more recent xdelta <ref type="bibr" target="#b15">[16]</ref>, vdelta <ref type="bibr" target="#b11">[12]</ref>, vcdiff <ref type="bibr" target="#b14">[15]</ref>, and zdelta <ref type="bibr" target="#b25">[26]</ref> tools that compute highly compressed representations of file differences. These tools have a number of applications in various networking and storage scenarios; see <ref type="bibr" target="#b20">[21]</ref> ¡ This project was supported by a grant from Intel Corporation, and by the Wireless Internet Center for Advanced Technology (WICAT) at Polytechnic University. Torsten Suel was also supported by NSF CAREER Award NSF CCR-0093400.</p><p>for a more detailed discussion. In a communication scenario, they typically exploit the fact that the sender and receiver both possess a reference file that is similar to the transmitted file; thus transmitting only the difference (or delta) between the two files requires a significantly smaller number of bits. In storage applications such as version control systems, deltas are often orders of magnitude smaller than the compressed target file.</p><p>Delta compression techniques have also been studied in detail in the context of the World Wide Web, where consecutive versions of a web page often differ only slightly <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> and pages on the same site share a lot of common HTML structure <ref type="bibr" target="#b4">[5]</ref>. In particular, work in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref> considers possible improvements to HTTP caching based on sending a delta with respect to a previous version of the page, or another similar page, that is already located in a client or proxy cache.</p><p>In this paper, we study the use of delta compression in a slightly different scenario. While in most other applications, delta compression is performed with respect to a previous version of the same file, or some other easy to identify reference file, we are interested in using delta compression to better compress large collections of files where it is not obvious at all how to efficiently identify appropriate reference and target files. Our approach is based on a reduction to the optimum branching problem in graph theory and the use of recently proposed clustering techniques for finding similar files.</p><p>We focus on collections of web pages from several sites. Applications that we have in mind are efficient downloading and storage of collection of web pages for off-line browsing, and improved archiving of massive terabyte web collections such as the Internet Archive (see http://archive.org). However, the techniques we study are applicable to other scenarios as well, and might lead to new general-purpose tools for exchanging collections of files that improve over the currently used zip and tar/gzip tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions of this Paper</head><p>In this paper, we study the problem of compressing collections of files, with focus on collections of web pages, with varying degrees of similarity among the files. Our approach is based on using an efficient delta compressor, in particular the zdelta compressor <ref type="bibr" target="#b25">[26]</ref>, to achieve significantly better compression than that obtained by compressing each file individually or by using tools such as tar and gzip on the collection. Our main contributions are:</p><p>The problem of obtaining optimal compression of a collection of ¡ files, given a specific delta compressor, can be solved by finding an optimal branching on a directed graph with ¡ nodes and ¡ ¢ edges. We implement this algorithm and show that it can achieve significantly better compression than current tools. On the other hand, the algorithm quickly becomes inefficient as the collection size grows beyond a few hundred files, due to its quadratic complexity.</p><p>We present a general framework, called cluster-based delta compression, for efficiently computing nearoptimal delta encoding schemes on large collections of files. The framework combines the branching approach with two recently proposed hash-based techniques for clustering files by similarity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Within this framework, we evaluate a number of different algorithms and heuristics in terms of compression and running time. Our results show that compression very close to that achieved by the optimal branching algorithm can be achieved in time that is within a small multiplicative factor of the time needed by tools such as gzip.</p><p>We also note three limitations of our study: First, our results are still preliminary and we expect additional improvements in running time and compression over the results in this paper. In particular, we believe we can narrow the gap between the speed of gzip and our best algorithms. Secondly, we restrict ourselves to the case where each target file is compressed with respect to a single reference file. Additional significant improvements in compression might be achievable by using more than one reference file, at the cost of additional algorithmic complexity. Finally, we only consider the problem of compressing and uncompressing an entire collection, and do not allow individual files to be added to or retrieved from the collection.</p><p>The rest of this paper is organized as follows. The next subsection lists related work. In Section 2 we discuss the problem of compressing a collection of files using delta compression, and describe an optimal algorithm based on computing a maximum weight branching in a directed graph. Section 3 provides our framework called clusterbased delta compression and outlines several approaches under this framework. In Section 4, we present our experimental results. Finally, Section 5 provides some open questions and concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>For an overview of delta compression techniques and applications, see <ref type="bibr" target="#b20">[21]</ref>. Delta compression techniques were originally introduced in the context of version control systems; see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> for a discussion. Among the main delta compression algorithms in use today are diff and vdelta <ref type="bibr" target="#b11">[12]</ref>. Using diff to find the difference between two files and then applying gzip to compress the difference is a simple and widely used way to perform delta compression, but it does not provide good compression on files that are only slightly similar. vdelta, on the other hand, is a relatively new technique that integrates both data compression and data differencing. It is a refinement of Tichy's block-move algorithm <ref type="bibr" target="#b23">[24]</ref> that generalizes the well known Lempel-Ziv technique <ref type="bibr" target="#b26">[27]</ref> to delta compression. In our work, we use the zdelta compressor, which was shown to achieve good compression and running time in <ref type="bibr" target="#b25">[26]</ref>.</p><p>The issue of appropriate distance measures between files and strings has been studied extensively, and many different measures have been proposed. We note that diff is related to the symmetric edit distance measure, while vdelta and other recent Lempel-Ziv type delta compressors such as xdelta <ref type="bibr" target="#b15">[16]</ref>, vcdiff <ref type="bibr" target="#b14">[15]</ref>, and zdelta <ref type="bibr" target="#b25">[26]</ref> are related to the copy distance between two files. Recent work in [6] studies a measure called LZ distance that is closely related to the performance of Lempel-Ziv type compressing schemes. We also refer to <ref type="bibr" target="#b5">[6]</ref> and the references therein for work on protocols for estimating file similarities over a communication link.</p><p>Fast algorithms for the optimum branching problem are described in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. While we are not aware of previous work that uses optimum branchings to compress collections of files, there are two previous applications that are quite similar. In particular, Tate <ref type="bibr" target="#b22">[23]</ref> uses optimum branchings to find an optimal scheme for compressing multispectral images, while Adler and Mitzenmacher <ref type="bibr" target="#b0">[1]</ref> use it to compress the graph structure of the World Wide Web. Adler and Mitzenmacher <ref type="bibr" target="#b0">[1]</ref> also show that a natural extension of the branching problem to hypergraphs that can be used to model delta compression with two or more reference files is NP Complete, indicating that an efficient optimal solution is unlikely.</p><p>We use two types of hash-based clustering techniques in our work, a technique with quadratic complexity called min-wise independent hashing proposed by Broder in <ref type="bibr" target="#b2">[3]</ref> (see also Manber and Wu <ref type="bibr" target="#b16">[17]</ref> for a similar technique), and a very recent nearly linear time technique called localitysensitive hashing proposed by Indyk and Motwani in <ref type="bibr" target="#b13">[14]</ref> and applied to web documents in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Delta Compression Based on Optimum Branchings</head><p>Delta compressors such as vcdiff or zdelta provide an efficient way to encode the difference between two similar files. However, given a collection of files, we are faced with the problem of succinctly representing the entire collection through appropriate delta encodings between target and reference files. We observe that the problem of finding an optimal encoding scheme for a collection of files through pairwise deltas can be reduced to that of computing an optimum branching of an appropriately constructed weighted directed graph ¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Reduction</head><p>Formally, a branching of a directed graph ¡ is defined as a set of edges such that <ref type="formula">(1)</ref> contains at most one incoming edge for each node, and <ref type="formula">(2)</ref> does not contain a cycle. Given a weighted directed graph, a maximum branching is a branching of maximum edge weight. Given a collection of includes an extra null node corresponding to the empty file that is used to model the compression savings if a file is compressed by itself (using, e.g., zlib, or zdelta with an empty reference file).</p><p>Given the above formulation it is not difficult to see that a maximum branching of the graph ¡ gives us an optimal delta encoding scheme for a collection of files. Condition (1) in the definition of a branching expresses the constraint that each file is compressed with respect to only one other file. The second condition ensures that there are no cyclical dependencies that would prevent us from decompressing the collection. Finally, given the manner in which the weights have been assigned, a maximum branching results in a compression scheme with optimal benefit over the uncompressed case.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Results</head><p>We implemented delta compression based on the optimal branching algorithm described in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>, which for dense graphs takes time proportional to the number of edges. <ref type="table" target="#tab_6">Ta- ble 1</ref> shows compression results and times on several collections of web pages that we collected by crawling a limited number of pages from each site using a breadth-first crawler.</p><p>The results indicate that the optimum branching approach can give significant improvements in compression over using cat or tar followed by gzip, outperforming them by a factor of ' to ( . However, the major problem with the optimum branching approach is that it becomes very inefficient as soon as the number of files grows beyond a few dozens. For the cbc.ca data set with ( 6 % pages, it took more than an hour ()</p><formula xml:id="formula_0">&amp; 3 ( 6 ( ¢ ¡</formula><p>) to perform the computation, while multiple hours were needed for the set with all sites. <ref type="figure" target="#fig_2">Figure 2</ref> plots the running time in seconds of the optimal branching algorithm for different numbers of files, using a set of files from the gcc software distribution also used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. Time is plotted on a logarithmic scale to accomodate two curves: the time spent on computing the edge weights (upper curve), and the time spent on the actual branching computation after the weights of the graph have been determined using calls to zdelta (lower curve). While both curves grow quadratically, the vast majority of the time is spent on computing appropriate edge weights for the graph ¡ , and only a tiny amount is spent on the actual branching computation afterwards. Thus, in order to compress larger collections of pages, we need to find techniques that avoid computing the exact weights of all edges in the complete graph ¡ . In the next sections, we study such techniques based on clustering of pages and pruning and approximation of edges. We note that another limitation of the branching approach is that it does not support the efficient retrieval of individual files from a compressed collection, or the addition of new files to the collection. This is a problem in some applications that require interactive access, and we do not address it in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cluster-Based Delta Compression</head><p>As shown in the previous section, delta compression techniques have the potential for significantly improved compression of collections of files. However, the optimal algorithm based on maximum branching quickly becomes a bottleneck as we increase the collection size ¡ , mainly due to the quadratic number of pairwise delta compression computations that have to be performed. In this section, we describe a basic framework, called Cluster-Based Delta Compression, for efficiently computing near-optimal delta compression schemes on larger collections of files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Framework</head><p>We first describe the general approach, which leads to several different algorithms that we implemented. In a nutshell, the basic idea is to prune the complete graph , and then find the best delta encoding scheme within this subgraph. More precisely, we have the following general steps: The assignment of weights in the second step can be done either precisely, by performing a delta compression across each remaining edge, or approximately, e.g., by using estimates for file similarity produced during the document analysis in the first step. Note that if the weights are computed precisely by a delta compressor and the resulting compressed files are saved, then the actual delta compression after the last step consists of simply removing files corresponding to unused edges (assuming sufficient disk space).</p><p>The primary challenge is Step (1), where we need to efficiently identify a small subset of file pairs that give good delta compression. We will solve this problem by using two sets of known techniques for document clustering, one set proposed by Broder <ref type="bibr" target="#b2">[3]</ref> and Manber and Wu <ref type="bibr" target="#b16">[17]</ref>, and one set proposed by Indyk and Motwani <ref type="bibr" target="#b13">[14]</ref> and applied to document clustering by Haveliwala, Gionis, and Indyk <ref type="bibr" target="#b9">[10]</ref>. These techniques were developed in the context of identifying near-duplicate web pages and finding closely related pages on the web. While these problems are clearly closely related to our scenario, there are also a number of differences that make it nontrivial to apply the techniques to delta compression, and in the following we discuss these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">File Similarity Measures</head><p>The compression performance of a delta compressor on a pair of files depends on many details, such as the precise locations and lengths of the matches, the internal compressibility of the target file, the windowing mechanism, and the performance of the internal Huffman coder. A number of formal measures of file similarity, such as edit distance (with or without block moves), copy distance, or LZ distance <ref type="bibr" target="#b5">[6]</ref> have been proposed that provide reasonable approximations; see <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref> for a discussion. However, even these simplified measures are not easy to compute with, and thus the clustering techniques in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10]</ref>  </p><formula xml:id="formula_1">¤ ¨ £ ¢ ¤ ¦ ! § " © # $ ¦ ! § " © ¤ ¤ ¦ ¨ § © ¤ . (Note that shingle containment is not symmet- ric.)</formula><p>Thus, both of these measures assign higher similarity scores to files that share a lot of short substrings, and intuitively we should expect a correlation between the delta compressibility of two files and these similarity measures. In fact, the following relationship between shingle intersection and the edit distance measure can be easily derived: We refer to <ref type="bibr" target="#b8">[9]</ref> for a proof and a similar result for the case of edit distance with block moves. A similar relationship can also be derived between shingle containment and copy distances. Thus, shingle intersection and shingle containment are related to the edit distance and copy distance measures, which have been used as models for the corresponding classes of edit-based and copy-based delta compression schemes. While the above discussion supports the use of the shingle-based similarity measures in our scenario, in practice the relationship between these measures and the achieved delta compression ratio is quite noisy. Moreover, for efficiency reasons we will only approximate these measures, introducing additional potential for error.</p><formula xml:id="formula_2">Given</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clustering Using Min-Wise Independent Hashing</head><p>We now describe the first set of techniques, called minwise independent hashing, that was proposed by Broder in <ref type="bibr" target="#b2">[3]</ref>. (A similar technique is described by Manber and Wu in <ref type="bibr" target="#b16">[17]</ref>.) The simple idea in this technique is to approximate the shingle similarity measures by sampling a small subset of shingles from each file. However, in order to obtain a good estimate, the samples are not drawn independently from each file, but they are obtained in a coordinated fashion using a common set of random hash functions that map shingles of length ¡ t o integer values. We then select in each file the smallest hash values obtained this way.</p><p>We refer the reader to <ref type="bibr" target="#b2">[3]</ref> for a detailed analysis. Note that there are a number of different choices that can be made in implementing these schemes:</p><p>Choice of hash functions: We used a class of simple linear hash functions analyzed by Indyk in <ref type="bibr" target="#b12">[13]</ref> and also used in <ref type="bibr" target="#b9">[10]</ref>.  . This results in a significant speedup over the optimal algorithm in practice, although the algorithm will eventually become inefficient due to the quadratic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Clustering Using Locality-Sensitive Hashing</head><p>The second set of techniques, proposed by Indyk and Motwani <ref type="bibr" target="#b13">[14]</ref> and applied to document clustering by <ref type="bibr">Haveliwala, Gionis, and Indyk [10]</ref>, is an extension of the first set that results in an almost linear running time. In particular, these techniques avoid the pairwise comparison between all ¡ files by performing a number of sorting steps on specially designed hash signatures that can directly identify similar files.</p><p>The first step of the technique is identical to that of the min-wise independent hashing technique for fixed sample size. -bit signature). If two files agree on their signature, then this is strong evidence that their intersection is above some threshold. It can be formally shown that by repeating this process a number of times that depends on and the chosen threshold, we will find most pairs of files with shingle intersection above the threshold, while avoiding most of the pairs below the threshold. For a more formal description of this technique we refer to <ref type="bibr" target="#b9">[10]</ref>.</p><p>The resulting algorithm consists of the following steps:</p><p>( We note two limitations. First, the above implementation only identifies the pairs that are above a given fixed similarity threshold. Thus, it does not allow us to determine the U best neighbors for each node, and it does not provide a good estimate of the precise similarity of a pair (i.e., whether it is significantly or only slightly above the threshold). Second, the method is based on shingle intersection, and not shingle containment. Addressing these limitations is an issue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>In this section, we perform an experimental evaluation of several cluster-based compression schemes that we implemented based on the framework from the previous section. We first introduce the algorithm and the experimental setup. In Subsection 4.2 we show that naive methods based on thresholds to do not give good results. The next three subsections look at different techniques that resolve this problem, and finally Subsection 4.6 presents results for our best two algorithms on a larger data set. Due to space constraints and the large number of options, we can only give a selection of our results. We refer the reader to <ref type="bibr" target="#b19">[20]</ref> for a more complete evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithms</head><p>We implemented a number of different algorithms and variants. In particular, we have the following options: Sample size: fixed size vs. fixed rate.</p><p>Similarity measure: intersection vs. containment.</p><p>Edge pruning rule: threshold vs. best neighbors vs. heuristics.</p><p>Edge weight: exact vs. estimated.</p><p>We note that not every combination of these choices make sense. For example, our LSH implementations do not support containment or best neighbors, and require a fixed sample size. On the other hand, we did not observe any benefit in using multiple hash functions in the MH scheme, and thus assume a single hash function for this case. We note that in our implementations, all samples were treated as sets, rather than multi-sets, so a frequently occurring string is presented at most once. <ref type="bibr" target="#b1">2</ref> All algorithms were implemented in C and compiled using gcc 2.95.2 under Solaris 7. Experiments were run on a E450 Sun Enterprise server, with two UltraSparc . The pages were crawled in a breadthfirst crawl that attempted to fetch all pages reachable from the www.poly.edu homepage, subject to certain pruning rules to avoid dynamically generated content and cgi scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Threshold-Based Methods</head><p>The first experiments that we present look at the performance of MH and LSH techniques that try to identify and retain all edges that are above a certain similarity threshold.</p><p>In <ref type="table" target="#tab_8">Table 2</ref> we look at the optimum branching method and at three different algorithms that use a fixed threshold to select edges that are considered similar, for different thresholds. For each method, we show the number of similar edges, the number of edges included in the final branching, and the total improvement obtained by the method as compared to compressing each file individually using zlib. The results demonstrate a fundamental problem that arises in these threshold-based methods: for high thresholds, the vast majority of edges is eliminated, but the resulting branching is of poor quality compared to the optimal one. For low thresholds, we obtain compression close to the optimal, but the number of similar edges is very high; this is a problem since the number of edges included in ¡ £ 2 Intuitively, this seems appropriate given our goal of modeling delta compression performance.  determines the cost of the subsequent computation. 3 Unfortunately, these numbers indicate that there is no real "sweet spot" for the threshold that gives both a small number of similar edges and good compression on this data set. We note that this result is not due to the precision of the sampling-based methods, and it also holds for thresholdbased LSH algorithms. A simplified explanation for this is that data sets contain different clusters of various similarity, and a low threshold will keep these clusters intact as dense graphs with many edges, while a high threshold will disconnect too many of these clusters, resulting in inferior compression. This leads us to study several techniques for overcoming this problem:  In summary, using a fixed threshold followed by an optimal branching on the remaining edges does not result in a very good trade-off between compression and running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Using Best Neighbors</head><p>We now look at the case where we limit the number of remaining edges in the MH algorithm by keeping only the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Estimated Weights</head><p>By using the containment measure values computed by the MH clustering as the weights of the remaining edges in ¡ £</p><p>, we can further decrease the running time, as shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">LSH Pruning Heuristic</head><p>For LSH algorithms, we experimented with a simple heuristic for reducing the number of remaining edges where, after the sorting of the file signatures, we only keep a subset of the edges in the case where more than ' files have identical signatures. In particular, instead of building a complete graph on these files, we connect these files by a simple linear chain of directed edges. This somewhat arbitrary heuristic (which actually started out as a bug) results in significantly decreased running time at only a slight cost in compression, as shown in <ref type="table" target="#tab_14">Table 5</ref>. We are currently looking at other more principled approaches to thinning out tightly connected clusters of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Best Results for Large Data Set</head><p>Finally, we present the results of the best schemes identified above on the large data set of ' 6 % 2 &amp; R 6 % pages from the poly.edu domain. We note that our results are still somewhat preliminary and can probably be significantly improved by some optimizations. We were unable to compute the optimum branching on this set due to its size.</p><p>The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>In this paper, we have investigated the problem of using delta compression to obtain a compact representation of a cluster of files. As described, the problem of optimally encoding a collection using delta compression based on a single file can be reduced to the problem of computing a maximum weight branching. However, while providing superior compression, this algorithm does not scale to larger collections, motivating us to propose a faster cluster-based delta compression framework. We studied several file clustering heuristics and performed extensive experimental comparisons. Our preliminary results show that significant compression improvements can be obtained over tar+gzip at moderate additional computational costs.</p><p>Many open questions remain. First, some additional optimizations are possible that should lead to improvements in compression and running time, including faster sampling and better pruning heuristics for LSH methods. Second, the cluster-based framework we have proposed uses only pairwise deltas, that is, each file is compressed with respect to only a single reference file. It has been shown <ref type="bibr" target="#b4">[5]</ref> that multiple reference files can result in significant improvements in compression, and in fact this is already partially exploited by tar+gzip with its ( 7 ' ¢ ¡ window on small files. As discussed, a polynomial-time optimal solution for multiple reference files is unlikely, and even finding schemes that work well in practice is challenging. Our final goal is to create general purpose tools for distributing file collections that improve significantly over tar+gzip.</p><p>In related work, we are also studying how to apply delta compression techniques to a large web repository 4 that can</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>reduction (in bytes) obtained by delta-compressing file with respect to file . In addition to these ¡ nodes, the graph ¡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 showsFigure 1 . Example of a directed and weighted complete graph. The optimal branching for the graph consists of</head><label>11</label><figDesc>Figure 1. Example of a directed and weighted complete graph. The optimal branching for the graph consists of the edges ¤ 1 % 2 ¨ 3 &amp; 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Running time of the optimal branching algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>Collection Analysis: Perform a clustering computa- tion that identifies pairs of files that are very similar and thus good candidates for delta compression. Build a sparse directed subgraph ¡ ¤ £ containing only edges between these similar pairs. (2) Assigning Weights: Compute or estimate appropriate edge weights for ¡ ¤ £ . (3) Maximum Branching: Perform a maximum branch- ing computation on ¡ ¤ £ to determine a good delta en- coding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Basic scheme: MH vs. LSH. Number of hash function: single hash vs. multiple hash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>We note that the large amount of memory and fast disk minimize the impact of I/O on the running times. We used two data sets: The medium data set consists of the union of the six web page collections from Section 2, with &amp;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>U most similar edges into each node, as proposed above. Clearly, this limits the total number of edges in ¡ £ to U¡ , thus reducing the cost of the subsequent computations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>and shingle containment. Formally, for a file</head><label></label><figDesc>that we use are based on two even simpler similarity measures, which we refer to as shingle intersection</figDesc><table>and an integer 

¡ 

, we define the 
shingle set (or 

¡ 

-gram set) 

¢¤ 


of 

as the multiset of 
substrings of length 

¡ 

(called shingles) that occur in 

. 
Given two files 

and 

£ 
, we define the shingle intersec-
tion of 

and 

£ 
as 

£¤¨£ 


£¤¨ 

£¤¨£ 

¢ 

¥ ¤ 
¦ 
¨  § 
© 


¦ 
¨  § 
© 


¤ 
¤ 
¦ 
¨  § 
© 


¦ 
¨  § 
© 


¤ 

. We define the 
shingle containment of 

with respect to 

£ 
as 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>total running time for the clustering step using min- wise independent hashing is thus roughly 1 ¤¡ ¢ ¡ A ¡ ¢ H¡ where ¡ is the number of files, ¡ the (average) size of each file, and ¡ the (average) size of each sample. The main ad- vantage over the optimal algorithm is that for each edge</head><label></label><figDesc>, resulting in sample sizes that are proportional to file sizes. One or several hash functions: One way to select ¡ samples from a file is to use ¡ hash functions, and in- clude the minimum value under each hash function in the sample. Alternatively, we could select one random hash function, and select the ¡ smallest values under this hash function. We selected the second method as it is significantly more efficient, requiring only one hash function computation for each shingle.After selecting the sample, we estimate the shingle inter- section or shingle containment measures by intersecting the samples of every pair of files. Thus, this phase takes time quadratic in the number of files. Finally, we decide which edges to include in the sparse graph ¡ £ . There are two in- dependent choices to be made here: Similarity measure: We can use either intersection or containment as our measure.A detailed discussion of the various implementation choices outlined here and their impact on running time and com- pression is given in the experimental section.</figDesc><table>Sample Sizes: One option is to use a fixed number of 
samples, say 

&amp; 
3 % 
7 % 

or 

&amp;% 
6 % 
7 % 

, from each file, independent 
of file size. Alternatively, we could sample at a con-
stant rate, say 

&amp; 

9 P 
7 Q 

6 ) 

or 

&amp; 

7 P 

B &amp;' 

S R 

Shingle size: We used a shingle size of 

¡ 

¢ 
) 

bytes in 
the results reported here. (We also experimented with 

¡ 

¢ 

T R 

b 
ut achieved slightly worse results.) 

Threshold versus 

U 

best neighbors: We could keep 
all edges above a certain similarity threshold, say 


% 

W V 

, 
in the graph. Or, for each file, we could keep the 

U 

m ost 
promising incoming edges, for some constant 

U 

, i.e., 
the edges coming from the 

U 

nearest neighbors w.r.t. 
the estimated similarity measure. 

The , 
instead of performing a delta compression step between 
two files of size 

¡ 

(several kilobyte), we perform a sim-
pler computation between two samples of some small size 
¡ 
(say, 
¡ 

¢ 

% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>)</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>, is to now use these hash values to con- struct file signatures that consist of the concatenation of</head><label></label><figDesc></figDesc><table>That is, we select from each file a fixed number of 
min-wise independent hash values, using 
¡ 
different random 
hash functions. For a file 

, let 

£ 

A ¤ 

b e the value selected 
by the 

t h hash function. The main idea, called locality-
sensitive hashinghash values (e.g., for 


¢ 
) 

we concatenate four 

( 
7 ' 

-bit hash 
values into one 

&amp; 
4 'R 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>1 ) Sampling: Extract a fixed number</head><label>1</label><figDesc></figDesc><table>¡ 
of hash values 

£ 

@ 
A ¤ 

from each file 

in the collection, using 
¡ 
dif-
ferent hash functions. 

(2) Locality-sensitive hashing: Repeat the following 

¤ 

times: 

(a) Randomly select 

indexes 

¦ ¥ 

tö 

tö  § 
© 


from 

% 
2 ¨ 





¨¡ 

F D 

&amp; 

. 

1 If 

different hash functions are used, then an additional factor of 

has to be added to the first term. 

(b) For each file 

construct a signature by concate-
nating hash values 

£ 
! 

¤ 

t o 

£ 

@ 

# " 
% $ 
' &amp; 

¤ 

. 

(c) Sort all resulting signatures, and scan the sorted 
list to find all pairs of files whose signature is 
identical. 

(d) For each such pair, add edges in both directions 
to 

¡ 

£ 
. 

Thus, the running time of this method is given by 

¤¡ 

¡ 

¡ 

A 

¤ 

# 

¡ 

H 

) ( 
! 0 

¤ 
# 

¡ 


A 

, where 
¡ 
, 

¤ 

, and 

are constants in the range 
from 

' 

to at most 

&amp;% 
6 % 

depending on the choice of parame-
ters. We discuss parameter settings and their consequences 
in detail in the experimental section. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Number of remaining edges, number 
of edges in the final branching, and compres-
sion benefit for threshold-based clustering 
schemes for different sampling techniques 
and threshold values (column 3). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Running time and compression ben-
efit for 

U 

-neighbor schemes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 3 shows</head><label>3</label><figDesc></figDesc><table>the running times of the various phases 
and the compression benefit as a function of the number 
of neighbors 

U 

and the sampling rate. The clustering time 
of course depends heavily on the sampling rate; thus one 
should use the smallest sampling rate that gives reasonable 
compression, and we do not observe any significant impact 
on compression up to a rate of 

&amp; 

7 P 

B &amp; 
4 'R 

for the file sizes we 
have. The time for computing the weights of the graph 

¡ 

£ 
grows (approximately) linear with 

U 

. The compression rate 
grows with 

U 

, but even for very small 

U 

, such as 

U 

¢ 
' 

, we 
get results that are within 


V 

of the maximum benefit. As in 
all our results, the time for the actual branching computation 
on 

¡ 

£ 
is negligible. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 4 .</head><label>4</label><figDesc>The time for building the weighted graph is now essentially reduced to zero. However, we have an extra step at the end where we perform the actual compression across edges, which is independent of</figDesc><table>U 

and has the same 
cost as computing the exact weights for 

U 

¢ 
&amp; 

. Looking 
at the achieved benefit we see that for 

U 

¢ 

) R 

w e are within 
about 

¥ 

V 

of the optimum, at a total cost of less than 

% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Running time and compression bene-
fit for 

U 

-neighbor schemes with sampling rate 

&amp; 

9 P 

B &amp; 
4 'R 

and estimated edge weights. 

threshold 
edges branching 
benefit 
size over zlib 
20% 28,866 
1640 6689872 
40% 
8,421 
1612 6242688 
60% 
6,316 
1538 5426000 
80% 
2,527 
1483 4945364 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>Number of remaining edges and com-
pression benefit for LSH scheme with pruning 
heuristic. 

seconds (versus about 

&amp; 

# Q 

seconds for standard zlib and 
several hours for the optimum branching). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Comparison of best MH and LSH 
schemes to zlib and cat+gzip. 

% 

W V 

and the pruning heuristic from the previous subsec-
tion. For MH, about 

¥ 

V 

of the running time is spent on 
the clustering, which scales as 

¤¡ 

¢ 

and thus eventually 
becomes a bottleneck, and 

' 

V 

on the final compression 
step. For LSH, more than 

¥ 

V 

is spent on computing the 
exact weights of remaining edges, while the rest is spent on 
the clustering. 

</table></figure>

			<note place="foot" n="3"> For example, if we compute the exact weight of each edge above a  threshold, then we have to perform over   calls to zdelta at a cost of about   each.</note>

			<note place="foot" n="4"> Similar to the Internet Archive at http://www.archive.org. store billions of pages on a network of workstations. Note that in this scenario, fast insertions and lookups are crucial, and significant changes in the approach are necessary. An early prototype of the system is currently being evaluated.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards compressing web graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Data Compression Conference (DCC)</title>
		<meeting>of the IEEE Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimistic deltas for WWW latency reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 USENIX Annual Technical Conference</title>
		<meeting><address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-01" />
			<biblScope unit="page" from="289" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compression and Complexity of Sequences (SEQUENCES&apos;97)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Camerini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fratta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maffioli</surname></persName>
		</author>
		<title level="m">A note on finding optimum branchings. Networks</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cache-based compaction: A new technique for optimizing web transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INFOCOM&apos;99</title>
		<meeting>of INFOCOM&apos;99</meeting>
		<imprint>
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communication complexity of document exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sahinalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Vishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM-SIAM Symp. on Discrete Algorithms</title>
		<meeting>of the ACM-SIAM Symp. on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">xProxy: A transparent caching and delta transfer system for web objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ionescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
	<note>unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rate of change and other metrics: a live study of the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Symp. on Internet Technologies and Systems (ITS-97)</title>
		<meeting>of the USENIX Symp. on Internet Technologies and Systems (ITS-97)<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="page" from="147" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using q-grams in a DBMS for approximate string processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pietarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable techniques for clustering the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Haveliwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the WebDB Workshop</title>
		<meeting>of the WebDB Workshop<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WebExpress: A system for optimizaing web browsing in a wireless environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Housel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindquist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd ACM Conf. on Mobile Computing and Networking</title>
		<meeting>of the 2nd ACM Conf. on Mobile Computing and Networking</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delta algorithms: An empirical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A small approximately min-wise independent family of hash functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Symp. on Discrete Algorithms</title>
		<meeting>of the 10th Symp. on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1999-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: Towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 30th ACM Symp. on Theory of Computing</title>
		<meeting>of the 30th ACM Symp. on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998-05" />
			<biblScope unit="page" from="604" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Engineering a differencing and compression data format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usenix Annual Technical Conference</title>
		<meeting>the Usenix Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2002-06" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">File system support for delta compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">MS Thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GLIMPSE: A tool to search through entire file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1994 Winter USENIX Conference</title>
		<meeting>of the 1994 Winter USENIX Conference</meeting>
		<imprint>
			<date type="published" when="1994-01" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Potential benefits of delta-encoding and data compression for HTTP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIG-COMM Conference</title>
		<meeting>of the ACM SIG-COMM Conference</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delta encoding of related web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Data Compression Conference (DCC)</title>
		<meeting>of the IEEE Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cluster-based delta compression of a collection of files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trendafilov</surname></persName>
		</author>
		<idno>TR-CIS-2002-05</idno>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
		<respStmt>
			<orgName>Polytechnic University, CIS Department</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Algorithms for delta compression and remote file synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lossless Compression Handbook</title>
		<editor>Khalid Sayood</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
		<title level="m">Finding optimum branchings. Networks</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Band ordering in lossless compression of multispectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="211" to="320" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The string-to-string correction problem with block moves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="321" />
			<date type="published" when="1984-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">RCS: A system for version control. Software -Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tichy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985-07" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">zdelta: a simple delta compression tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<idno>TR-CIS- 2002-02</idno>
		<imprint>
			<date type="published" when="2002-06" />
		</imprint>
		<respStmt>
			<orgName>Polytechnic University, CIS Department</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A universal algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
