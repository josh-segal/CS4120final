<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Communication Analysis of Parallel 3D FFT for Flat Cartesian Meshes on Large Blue Gene Systems ⋆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
							<email>chan@mcs.anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">ASCI FLASH Center</orgName>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Math. and Comp. Sci. Division, Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
							<email>wgropp@illinois.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
							<email>thakur@mcs.anl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Math. and Comp. Sci. Division, Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Communication Analysis of Parallel 3D FFT for Flat Cartesian Meshes on Large Blue Gene Systems ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Parallel 3D FFT is a commonly used numerical method in scientific computing. P3DFFT is a recently proposed implementation of parallel 3D FFT that is designed to allow scalability to massively large systems such as Blue Gene. While there has been recent work that demonstrates such scalability on regular cartesian meshes (equal length in each dimension), its performance and scalability for flat carte-sian meshes (much smaller length in one dimension) is still a concern. In this paper, we perform studies on a 16-rack (16384-node) Blue Gene/L system that demonstrates that a combination of the network topology and the communication pattern of P3DFFT can result in early network saturation and consequently performance loss. We also show that remap-ping processes on nodes and rotating the mesh by taking the communication properties of P3DFFT into consideration, can help alleviate this problem and improve performance by up to 48% in some special cases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fast Fourier Transform (FFT) has been one of the most popular and widely used numerical methods in many areas of scientific computing, including digital speech and signal processing, solving partial differential equations, molecular dynamics <ref type="bibr">[3]</ref>, many-body simulations and monte carlo simulations <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b6">14]</ref>. Given its importance, there have been a large number of libraries that provide different implementations of FFT (both sequential and parallel) aimed at achieving high-performance in various environments. FFTW <ref type="bibr" target="#b7">[15]</ref>, IBM PESSL <ref type="bibr" target="#b5">[13]</ref>, and the Intel Math Kernel Library (MKL) <ref type="bibr" target="#b1">[9]</ref> are a few examples of such implementations. P3DFFT <ref type="bibr">[6]</ref> is a recently proposed parallel implementation of 3D FFT that is designed to allow scalability for large problem sizes on massively large systems such as Blue Gene (BG) <ref type="bibr" target="#b8">[16]</ref>. It aims at achieving such scalability by limiting communication to processes in small local sub-communicators instead of communicating with all processes in the system.</p><p>While there has been previous work that demonstrates the scalability of P3DFFT for regular 3D cartesian meshes, where all dimensions of the mesh are of equal length <ref type="bibr">[7]</ref>, its inability to achieve similar scalability for flat 3D cartesian meshes, where one dimension is much smaller than the other two, is a known problem <ref type="bibr" target="#b10">[18]</ref>. Flat 3D cartesian meshes are a good tool in studying quasi-2D systems that occur during the transition of 3D systems to 2D systems (e.g., in superconducting condensate <ref type="bibr" target="#b9">[17]</ref>, Quantum-Hall effect <ref type="bibr" target="#b12">[20]</ref> and turbulence theory in geophysical studies <ref type="bibr" target="#b11">[19]</ref>). Thus, such loss of scalability can be a serious problem that needs to be addressed.</p><p>In this paper, we analyze the performance of P3DFFT for flat 3D cartesian meshes on a large 16-rack (16384-node) Blue Gene/L (BG/L) system. Specifically, we perform detailed characterization of the communication pattern used by P3DFFT and its behavior on the BG network topology. We observe that a combination of the network topology and the communication pattern of P3DFFT can result in parts of the communication to over-saturate the network, while other parts under-utilize the network. This causes overall loss of performance on large-scale systems. We also show that carefully remapping processes on nodes and rotating the mesh by taking the communication properties of P3DFFT into consideration can help alleviate this problem. Our experimental results demonstrate up to 48% improvement in performance in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of Parallel 3D FFT Techniques</head><p>FFT <ref type="bibr" target="#b0">[8]</ref> is an efficient algorithm to compute the Discrete Fourier Transform (DFT) and its inverse. Fourier transform consists of a forward and a backward transform. The forward operation transforms a function f(x) in real space X to a function F(k) in Fourier space K. The backward transform does the reverse operation that transforms F(k) in Fourier space K to f(x) in real space X. In this section, we will mainly discuss the forward fourier transform, but the backward fourier transform can be similarly performed by reversing the steps in the forward transform.</p><p>A typical 3D forward fourier transform for a real-space function f(x,y,z) can be expressed as follows:</p><formula xml:id="formula_0">f (k x , k y , k z ) = z y x f (x, y, z) · e ikxx e iky y e ikz z<label>(1)</label></formula><p>The goal here is to perform a 1D fourier transform on each of the three dimensions of the 3D data mesh, distributed over P processes. There are two basic approaches for doing this <ref type="bibr" target="#b3">[11]</ref>, distributed FFT and transpose-based FFT. Distributed FFT relies on a parallel implementation of 1D-FFT, with each process communicating the necessary data with other processes. Transpose-based FFT, on the other hand, relies on a sequential version of 1D-FFT that performs the transform on one dimension at a time, and transposing the data grid when needed. There are two different transpose-based FFT strategies for 3D meshes, which differ in their data decomposition pattern. Let us consider a data grid of size: n x × n y × n z .</p><p>-1D Decomposition: In the 1D data decomposition technique, the data grid is divided across P processes such that each process gets a 2D slab of the grid (size of n x ·n y ·n z /P ). Each process carries out a typical sequential 2D-FFT on its local slab, and thus does not require any communication during this operation. Once the 2D-FFT has completed, it transposes the mesh using an MPI Alltoallv() operation. This allows it to receive data corresponding to the third dimension, on which a 1D-FFT is applied. Thus, only one global transpose is used in this technique. However, the drawback is that it only scales max(n x ,n y ,n z ) number of processes. </p><formula xml:id="formula_1">n x × (n y /P row ) × (n z /P col ).</formula><p>Each process first performs a 1D-FFT along the length of the column (say x-axis). Then it does a transpose on the remaining two axes (y-and z-axis) and performs 1D-FFT on the y-axis. Finally, it performs a transpose on the y-and z-axes and performs a 1D-FFT on the z-axis. Two transposes are performed altogether. P3DFFT uses this strategy as it can theoretically scale up to n x ·n y ·n z / min(n x ,n y ,n z ) processes. Neither of the transpose based FFT techniques allows for easy overlap of communication and computation as the transpose where the communication takes places has to be finished before the local 1D-FFT can be carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>A number of implementations of Parallel 3D FFT exist. FFTW <ref type="bibr">[4]</ref> has been a popular implementation of parallel 3D FFT. While there has been prior literature <ref type="bibr" target="#b2">[10]</ref> that identified issues with its performance and improved its scalability to some extent, FFTW itself relies on 1D decomposition (described in Section 2) which allows it to only scale up to a theoretical limit of max(n x ,n y ,n z ) number of processes. That is, with a problem size of 4096 3 , FFTW cannot use more than 4096 processors. Thus, it is not ideal to use on massively parallel systems such as BG which support hundreds of thousands of processors. P3DFFT has recently been proposed to deal with such scalability issues and allow 3D FFT to be effectively used on such systems.</p><p>Like P3DFFT, IBM recently proposed an alternate implementation of Parallel 3D FFT, specifically for their BG system, known as BGL3DFFT <ref type="bibr" target="#b4">[12]</ref>. However, BGL3DFFT has several limitations. First, it is a closed source implementation that restricts much utility for open research. Second, owing to its lack of Fortran support, it has not gained too much popularity in mainstream scientific computing. Third, while there is published literature that shows its scalability for small 3D grids (up to 128×128×128) <ref type="bibr" target="#b4">[12]</ref>, there is no evidence of its scalability for larger problem sizes. Keeping the drawbacks of BGL3DFFT aside, we believe that the problems in handling flat cartesian problems exist in the BGL3DFFT implementation as well, and that our observations are relevant there too.</p><p>There is also previous literature that shows that P3DFFT scales reasonably well with large regular cubical 3D meshes <ref type="bibr">[7]</ref>. However, recently, Joerg Schumacher pointed out the importance of 3D-FFT on flat cartesian meshes where n x = n y &gt; n z in his crossover study from 3D to quasi-2D turbulence systems <ref type="bibr" target="#b10">[18]</ref> and found that 3D-FFT on flat cartesian meshes does not scale as well as regular cartesian meshes. Our paper uses Joerg's study as a motivation to understand the scalability issues of P3DFFT for flat cartesian meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Communication Overheads in P3DFFT</head><p>In this section, we first present relevant details about the BG network in Section 4.1. We next present the communication characteristics of P3DFFT in Section 4.3 and an analysis of network saturation caused by such communication in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BG/L Network Overview</head><p>BG/L has five different networks <ref type="bibr">[5]</ref>. Two of them (1G Ethernet and 100M Ethernet with JTAG interface) are used for file I/O and system management while the other three (3-D Torus Network, Global Collective Network and Global Interrupt Network) are used for MPI communication. The 3-D torus network is used for point-to-point MPI and multicast operations and connects all compute nodes to form a 3-D torus; thus, each node has six neighbors. Each link provides a bandwidth of 175 MB/s per direction for a total of 1.05 GB/s bidirectional bandwidth per node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyzing Network Saturation Behavior in P3DFFT on BG/L</head><p>As described earlier, unlike regular clusters that use switched network fabrics, the Blue Gene family of supercomputers relies on a torus network for interconnectivity. Thus, each node is directly connected with only six other nodes. To reach any other node, the message has to traverse more than one link; this leads to network link sharing by multiple messages, leading to network saturation.</p><p>Since P3DFFT does not directly perform communication with all processes in the system, but rather communicates only with processes in its row and column sub-communicators, the network saturation behavior is tricky. In <ref type="figure" target="#fig_3">Figure 2</ref>(a), we show the mapping of the processes in the row and column sub-communicators to the physical torus on BG/L. This example considers a system size of 512 processes, with the row sub-communicator containing 32 processes and the column sub-communicator containing 16 processes, i.e., a 32×16 process grid. Thus, the first row would have processes 1 to 32, the second row would have processes 33 to 64 and so on.   Note that the size of different dimensions in the BG/L torus is fixed based on the available allocation. In this case, we pick a torus topology of 8 × 8 × 8. Therefore, the first row of processes in the process grid (1 to 32) map to the first four physical rows on the BG/L torus (shown as red circles in <ref type="figure" target="#fig_3">Figure 2</ref>(a)). Similarly, the second row of processes in the process grid (33 to 64) map to the next four physical rows (shown as pink rectangles). It is to be noted that all processes in the row communicator are always allocated adjacent to each other. That is, any communication within the row sub-communicator will not require the message to go outside these four rows.</p><p>The mapping of the processes corresponding to the column communicator is, unfortunately, more complicated than the row communicator. Processes corresponding to the first column are 1, 33, 65, 97, etc. These processes are not all topologically adjacent. In other words, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>, messages traversing the non-adjacent portions of the column communicator have to pass through more links, oftentimes contending with messages from other communicators, and can thus saturate the network significantly faster as compared to the row communicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Communication Characterization of P3DFFT</head><p>Consider a 3D data grid of size N = n x ×n y ×n z which needs to be solved on a system with P processes. P3DFFT decomposes the 3D grid into a 2D processor mesh of size P row ×P col , where P row ×P col = P . It splits the 2D processor mesh into two orthogonal sub-communicators-one in each dimension. Thus, each process will be a part of a row and a column sub-communicator. As shown in <ref type="figure" target="#fig_3">Figure 2(b)</ref>, the first global transpose of the forward 3D-FFT consists of n z /P col iterations of MPI Alltoallv over the row sub-communicator (the short red states), with the message count per process-pair being m row defined in Equation 2. The total message count per process for the first transpose becomes n x ·n y ·n z /(P row ·P col ).</p><formula xml:id="formula_2">m row = n x · n y P 2 row = N n z · P 2 row (2)</formula><p>The second global transpose consists of one single iteration of MPI Alltoallv over the column communicator (the long red states in <ref type="figure" target="#fig_3">Figure 2</ref>(b)), with message count per process being n x · n y · n z /(P row · P col ), which is the same as the first transpose. The corresponding message count per process-pair is m col , where</p><formula xml:id="formula_3">m col = n x · n y · n z P row · P col · P col = N · P row P 2<label>(3)</label></formula><p>The total communication cost for the two global transposes becomes:</p><formula xml:id="formula_4">T (n z , P row ) = n z P col · T row (m row ) + T col (m col ) = n z · P row P · T row N n z · P 2 row + T col N · P row P 2<label>(4)</label></formula><p>where T row () and T col () are functions of communication latency for the row and column communicators. The 2D processor decomposition and the symmetry requirement of the real-to-complex 3D-FFT together demands the following conditions:</p><formula xml:id="formula_5">n z P col ≥ 1, n y P row ≥ 1, and n x P row ≥ 2<label>(5)</label></formula><p>P row and n z are chosen as independent variables that affect the total communication time. P row can take different values depending on how the processors are arranged as a 2D processor mesh, while satisfying the validity conditions presented in Equation 5. As P row decreases, P col could become bigger than n z and violates the first condition in Equation 5. However, by rotating this grid, the values of n x and n z can be interchanged to maintain the inequality as P row decreases further. We will study this possibility in our experiments later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Understanding the Trends in P3DFFT Performance</head><p>The total communication time in P3DFFT is impacted by three sets of variables: (i) message size, (ii) communicator size and (iii) congestion in the communicator topology. The first two variables (message size and communicator size) are directly related to the P row parameter described in Equation 4. The third parameter, however, depends on the physical topology of the processes present in the communicator and their communication pattern, as these conditions determine how many messages share the same link on the torus network.</p><p>P3DFFT internally uses MPI Alltoallv to transpose the data grid. For most implementations of MPI, including the one on Blue Gene, this is implemented as a series of point-to-point communication operations, with each process sending and receiving data to/from every other process in the communicator. For this communication pattern, even in a communicator where all the processes are topologically adjacent (row communicator), the number of messages that need to traverse the same network link in the torus network can increase quadratically.  <ref type="figure" target="#fig_4">Figure 3</ref>, it can be shown that the number of messages traversing the busiest link in each direction increases quadratically with increasing communicator size. The exception to this rule is when one dimension of the torus completes. For example, for a communicator with 8 processes, the first dimension in the torus is fully utilized; thus, since BG/L uses a 3-D torus, this would mean that these processes can use an extra wrap-around link along this dimension. In this case, the maximum number of messages on the busiest link would be half the value it would have been without this wrap-around link.</p><p>In summary, if the first dimension of the torus has a processors, for communicator sizes of 1, 2, 4, ..., a/2, a, the number of messages on the busiest link would increase as 1, 4, 16, ..., (a/4) 2 , (a/2) 2 × 2, i.e., a quadratic increase in congestion with increasing communicator size. This trend continues for the second and third dimensions as well. Using this analysis, we can observe that a small system that has a torus configuration of 8 × 8 × 8 would have a much smaller amount of congestion as compared to a large system that has a torus configuration of 8 × 32 × 16.</p><p>The top 4 graphs in <ref type="figure" target="#fig_7">Figure 5</ref> illustrate the total bandwidth per process achieved by MPI Alltoallv for different message sizes on a small system (P = 512). The diamonds and triangles marked on the figures show the different message sizes (and corresponding bandwidths) that are used within P3DFFT for data grid configurations of 512×512×128 and 128×512×512. We notice that as long as the message size is larger than about 1 KB, both the row and column communicator achieve the peak bandwidth; thus, for best performance, it is preferred that a large message size be used. However, as illustrated in Equation 4, when P row becomes large, the message size used by the row communicator drops quadratically. This causes it to use a very small message size for large P row values resulting in the network not being saturated, and consequently performance loss. Thus, a small P row value is preferred. For large systems (P = 4096), the large impact of congestion, as described above, can be observed in the bottom 4 graphs in <ref type="figure" target="#fig_7">Figure 5</ref>. The congestion causes a two-fold difference in the MPI Alltoallv bandwidths achieved by the row and column communicators.</p><p>In the network saturation region, the time taken by MPI Alltoallv can be approximated as a linear function, i.e. T sub (m sub ) ≈ α sub · (r · m sub ) + β sub where sub is the sub-communicator label for either row or col, and r is the precision of the datatype that r · m sub is the message size in byte. In order to use the asymptotic function meaningfully, we investigated how the α and β change with their corresponding sub-communicator size. The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. Notice that the Y-axes of both pictures are divided by the size of the sub communicator. This is necessary to scale out the effect of the communicator size. Four system sizes, P = 512, 1024, 2048, 4096 are plotted in the figures. They all overlap nicely to some universal functions. The scaled slope and intercept functions will be called S(P sub ) and I(P sub ) respectively. They are defined as</p><formula xml:id="formula_6">S (P sub ) = α sub (P sub ) P sub and I (P sub ) = β sub (P sub ) P sub<label>(6)</label></formula><p>For small systems P = 512, 1024, S(P sub ) increases linearly in P sub = 1, 2, 4 and then becomes a constant afterward. But for system P = 1024 which is similar to P = 512, except a step jump appears from P sub = 16 to P sub = 32. For P = 2048, S(P sub ) increases linearly in P sub = 1, 2, 4, 8, and then stays as a constant afterward. For P = 4096 which is similar to P = 2048, except with a step jump from P sub = 32 to P sub = 64. We believe the step jumps are due to the sudden increase of contention as P sub 's topology changes as explained earlier in this section. With Equations 6, 2 and 3, Equation 4 can be simplified to</p><formula xml:id="formula_7">T (n z , P row ) ≈ n z P col (α row (r · m row ) + β row ) + (α col (r · m col ) + β col ) = n z P col α row P row rN n z P row + β row + α col P col rN P + β col = r S (P row ) + S P P row N P + I (P row ) n z P 2 row P + I P P row P P row (7)</formula><p>The T (n z , P row ) is linear in n z but its dependence on P row is rather complicated. Since the behaviors of S() and I() in P sub are known from <ref type="figure" target="#fig_5">Figure 4</ref>, we can reasonably describe how the total transpose time changes with P row . Based on <ref type="figure" target="#fig_5">Figure 4</ref>, S() is always positive and monotonic in P sub . For large systems (P = 4096), S() &lt; 0.035. For small systems (P = 512), S() &lt; 0.007. I() is more of less a positive constant of order O(10) except the big negative spike occurs at P sub = 64. For the system parameters being considered here, we are mainly interested in P row &lt; √ P , each term in Equation 7 can be estimated as follows:</p><formula xml:id="formula_8">1st term ∝ N P ≫ P, 2nd term ∝ n z P 2 row P &lt; n z &lt; P, 3rd term ∝ P col &lt; P</formula><p>However, all the terms in Equation 7 are made equally important by S() ≪ I().</p><p>For simplicity, let's ignore any terms that is O(P 2 row ) or higher and consider the small P row limit, where S(P row ) → s 0 · P row , S(P/P row ) → S ∞ , and I(P/P row ) → I ∞ . Equation 7 can be approximated as:</p><formula xml:id="formula_9">T (n z , P row ) ≈ r [s 0 · P row + S ∞ ] N P + I ∞ P P row (8) =⇒ P min row = P I ∞ rs 0 N and T (n z , P min row ) ≈ rS ∞ N P + 2 rs 0 N I ∞<label>(9)</label></formula><p>P min row is where the minimum of T (n z , P row ) occurs. For N = 512×512×128 and P = 512, r = 4, s 0 ∼ 0.002, S ∞ ∼ 0.007, I ∞ ∼ 3, then P min row ∼ √ 3 ∼ 1.73 and T (n z , P min row ) ∼ 3.5 msec. For N = 2048×2048×512 and P = 4096, r = 4, s 0 ∼ 0.002, S ∞ ∼ 0.035, I ∞ ∼ 7, then P min row ∼ 2 √ 7 ∼ 5.3 and T (n z , P min row ) ∼ 93 msec. Both predicted T (n z , P min row ) values are within few percents of the actual measured experimental values.</p><p>For more accurate estimation, O(P 2 row ) terms and the full features of S() and I() are all needed. I() has a negative spike of O(10 2 ) at P row = 64 = √ P as in <ref type="figure" target="#fig_5">Figure 4</ref>. The negative spike will certainly produce a local minimum of total transpose time for N = 4096. If the flat cartesian grid is rotated to increase n z to avoid violating the validity conditions in Equation 5, P row can get a lot closer to 1. Also, the discrete jumps seen in S() in <ref type="figure" target="#fig_5">Figure 4</ref> could be reflected in the observed total trasnpose time as sudden jump seen in the corresponding S().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation and Analysis</head><p>In this section, we experimentally evaluate the P3DFFT library using a fortran physics program 4 that uses a flat cartesian mesh. Specifically, in this program, some of the variables only have x-and y-components, but no z-component. This means that the physical system emulated by this program is a quasi-2D system with preferential treatment in the z-axis. Therefore, our analysis of total communication time with respect to change in P row in Equation 7 is applicable to this program, but not the communication analysis with respect to change in n z . This is because the variation in P row and P col affects only the MPI Alltoallv used in the two global transposes employed by the 3D-FFT which is being applied uniformly to all variables. In order words, the variation of the fortran program with respect to P row is equivalent to the variation of 3D-FFT algorithm. But the  variation of the fortran program with respect to n z includes both the variation of the 3D-FFT algorithm and the special asymmetric treatment of the z-axis in the phyical problem. In Section 5.1, we first observe the communication behavior for a small half-rack (512-node) system and verify our analysis. Then, in Section 5.2, we utilize this understanding to evaluate and optimize the performance of P3DFFT for a large-scale system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Communication Analysis on a Small-scale System</head><p>In this section, a small BG/L system of 512 nodes is used to study the behavior of P3DFFT. These 512 nodes form a regular torus of 8×8×8 dimensions. We ran our fortran program that uses the P3DFFT library with various data grid configurations on different processor mesh arrangements. <ref type="table" target="#tab_1">Table 1</ref> presents the timing data from this run. Four data grid configurations <ref type="bibr">(256 × 256 × 64, 64 × 25 × 256, 512 × 512 × 128 and 128×512×512)</ref>, and four different processor mesh decompositions (8×64, 16×32, 32×16 and 64×8), were attempted on the 512-node system. In <ref type="table" target="#tab_1">Table 1</ref>, we can see that the best timing for each data grid configuration occurs at the processor-mesh with the shortest row dimension, i.e., shortest P row . Also, we see that the fortran program is taking longer to finish as P row increases. Both features are consistent with our findings with Equation 8 in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on a Large-scale System</head><p>In this section, we evaluate the performance of P3DFFT on a large-scale (16-rack or 16384-node) BG/L system. Specifically, we evaluated the performance of a 2048×2048×512 data grid, with different processor-mesh configurations, on 4 racks (4096 nodes), 8 racks (8192 nodes) and 16 racks (16384 nodes). For the 4-rack system, we also tried out two different torus topologies (16×32×16 and 16×16×16). Further, we also study the impact of rotating the data grid. <ref type="table" target="#tab_3">Tables 7, 6, 4 and 3</ref> show the performance results for the different system sizes and configurations with our fortran test program. All these results indicate that the best performance occurs at the smallest P row and largest n z , i.e. rotated data grid, shown in the tables, when the number of processors P and the problem size N (FFT data grid size) are fixed. The small P row giving the best performance is consistent with our findings of Equation 8. The best performance occuring at largest n z for fixed problem size N is more a feature of the physics problem being solved in the fortran program and not a feature of P3DFFT as explained in the beginning of section 5.    <ref type="table" target="#tab_3">Tables 4 and 3</ref> show the performance numbers of the fortran with the same problem sizes (2048×2048×512 and 512×2048×2048) and the same number of nodes (4096 nodes). The only difference between these two tables is that the different torus configurations are used. <ref type="table" target="#tab_4">Table 4</ref> is evaluated on a 8×32×16 torus, while <ref type="table" target="#tab_3">Table 3</ref> is evaluated on a 16×16×16 torus. The fastest performance at 64×64 can be explained by the big negative spike of I() seen in <ref type="figure" target="#fig_5">Figure 4</ref> and Equation 7. We notice that for the processor-mesh, 64×64, P3DFFT is 10% faster in the 16×16×16 torus as compared to the 8 × 32 × 16 torus. The reason for this behavior is the layout of the column communicator as described in Section 4.2. Specifically, in the 8×32×16 torus configuration, each row (64 processes) takes up eight physical rows on the torus. Thus, the processes in the column communicator can be up to 8 rows apart. On the other hand, in the 16×16×16 torus configuration, each row takes up only four rows, thus reducing the distance between the processes in the column communicator and consequently improving their performance. <ref type="table">Table 5</ref>. Timing from fortran P3DFFT program (in second) with two different processor sizes, 8192 and 16384. P: Processor mesh configuration. N: FFT data grid configuration.  Next, let us consider <ref type="table" target="#tab_5">Table 6</ref> that shows the performance for 8192 processors. If we notice the 2048×2048×512 FFT data grid configuration, we see that in this case, the smallest value of P row (16×512 configuration) does not provide the best performance. Instead, 64×128 provides a better performance. This again can be explained by the big negative spike of I() seen in <ref type="figure" target="#fig_5">Figure 4</ref> and Equation 7. This suggests 8192 processor configuration is similar to the non-cubical torus 4096 processor configuration discussed earlier in Equation 8 with the existence of at least two optimal configurations. Comparing the performance impact of the data grid rotation from the 2048× 2048 × 512 configuration to the 512× 2048 × 2048 configuration, we notice that the performance improves by about 26%. Not all the improvement is from the communication time.</p><p>The final result we present is for the large 16-rack (16384-node) system. We notice that this case is a little different from the 4-rack (4096-node) and the 8-rack (8192-node) results where two optimal configurations can be obtained. However, the overall performance is still consistent with the other results. That is, performance improves with decreasing P row and with increasing n z . Specifically, reducing P row can improve performance by about 15% as compared to the default 128×128 processor mesh configuration. Increasing n z , on the other hand, can improve performance by close to 48%.</p><p>Based on all the experimental results, we notice that there could be multiple optimal system configurations, two possible ones are 1) small P row in rotated data grid with larger n z . 2) P row ≃ √ P in the regular data grid with smaller n z . The later optimal configuration may not exist in all system sizes and configurations. But the first optimal configuration seems to always exist. Rotating the FFT data grid furthers the path of performance improvements that have been stopped by Equation 5. This indicates that as we move to even larger problem sizes, the lessons learnt in this paper will have increasingly higher importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks and Future Work</head><p>P3DFFT is a recently proposed implementation of parallel 3D FFT for largescale systems such as IBM Blue Gene. While there have been a lot of studies that demonstrate the scalability of P3DFFT on regular cartesian meshes (where all dimensions are equal in length), there seems to be no previous work that studies its scalability for flat cartesian meshes (where the length of one dimension is much smaller than the rest). In this paper we studied the performance and scalability of P3DFFT for flat cartesian meshes on a 16-rack (16384-node) Blue Gene system and demonstrated that a combination of the network topology and the communication pattern of P3DFFT can result in parts of the communication to over-saturate the network, while other parts under-utilize the network. This can cause overall loss of performance on large-scale systems. We further showed that remapping processes on nodes and rotating the FFT data grid by taking the communication properties of P3DFFT into consideration, can help alleviate this problem and improve performance by up to 48% in some cases.</p><p>While our work alleviates the issue of network saturation, it does not completely avoid it. For future work, we would like to further the study of alleviation of network contention by rotating the torus configuration through environment variable BG MAPPING which allows user to rearrange process layout in the torus, and we would also like to study the impact of split-collectives to hide communication time that can be aggravated due to such saturation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc>2D Decomposition: In the 2D data decomposition technique (shown in Fig- ure 1), one face (2D) of the mesh is divided over P = P row × P col pro- cesses, so each process contains a column (pencil) of the data mesh of size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. 2D Decomposition: 1D FFT in each dimension followed by a transpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Mapping the row and column communicator processes in a 2D process grid to a 3D torus; (b) Jumpshot's communicator view on P3DFFT's communication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MPI Alltoallv Congestion Behavior Figure 3 illustrates this behavior. Let us consider a torus of 8 × 8 × 8 processes. For a row communicator with only 2 processes, the two processes just exchange data between each other. Thus, there is only one message per link in each direction (BG/L links are bidirectional). For a row communicator with 4 processes, the exchange is more complicated with the busiest link serving up to 4 messages in each direction. Though not represented in Figure 3, it can be shown that the number of messages traversing the busiest link in each direction increases quadratically with increasing communicator size. The exception to this rule is when one dimension of the torus completes. For example, for a communicator with 8 processes, the first dimension in the torus is fully utilized; thus, since BG/L uses a 3-D torus, this would mean that these processes can use an extra wrap-around link along this dimension. In this case, the maximum number of messages on the busiest link would be half the value it would have been without this wrap-around link. In summary, if the first dimension of the torus has a processors, for communicator sizes of 1, 2, 4, ..., a/2, a, the number of messages on the busiest link would increase as 1, 4, 16, ..., (a/4) 2 , (a/2) 2 × 2, i.e., a quadratic increase in congestion with increasing communicator size. This trend continues for the second and third dimensions as well. Using this analysis, we can observe that a small system that has a torus configuration of 8 × 8 × 8 would have a much smaller amount of congestion as compared to a large system that has a torus configuration of 8 × 32 × 16. The top 4 graphs in Figure 5 illustrate the total bandwidth per process achieved by MPI Alltoallv for different message sizes on a small system (P = 512). The diamonds and triangles marked on the figures show the different message sizes (and corresponding bandwidths) that are used within P3DFFT for data grid configurations of 512×512×128 and 128×512×512. We notice that as long as the message size is larger than about 1 KB, both the row and column communicator achieve the peak bandwidth; thus, for best performance, it is preferred that a large message size be used. However, as illustrated in Equation 4, when P row becomes large, the message size used by the row communicator drops quadratically. This causes it to use a very small message size for large P row values resulting in the network not being saturated, and consequently performance loss. Thus, a small P row value is preferred. For large systems (P = 4096), the large impact of congestion, as described above, can be observed in the bottom 4 graphs in Figure 5. The congestion causes a two-fold difference in the MPI Alltoallv bandwidths achieved by the row and column communicators. In the network saturation region, the time taken by MPI Alltoallv can be approximated as a linear function, i.e. T sub (m sub ) ≈ α sub · (r · m sub ) + β sub where sub is the sub-communicator label for either row or col, and r is the precision of the datatype that r · m sub is the message size in byte. In order</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Plots of scaled slope, S(), and intercept, I(), of asymptotic fit of the latency of the MPI Alltoallv at P=512,1024,2048,4096. The graphs show that the slope and intercept scales with the subcommunicator size in a meaningful way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Total bandwidth per process vs message size of small (P = 512) and large systems (P = 4096).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Timing of the fortran P3DFFT program (in second) with P = 512 (8×8×8 
torus). P: Processor mesh configuration. N: FFT data grid configuration. 

P \ N 256×256×64 64×256×256 512×512×128 128×512×512 
8×64 
1.294 
1.37 
9.08 
9.98 
16×32 
1.276 
1.65 
9.08 
10.73 
32×16 
1.41 
2.34 
9.62 
11.86 
64×8 
1.74 
10.66 
15.01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Timing from fortran P3DFFT program (in second) with one processor size 
4096 but different torus configurations. P: Processor mesh configuration. N: FFT data 
grid configuration. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>P = 4096 (16×16×16 torus) 
P \ N 2048×2048×512 512×2048×2048 
16×256 
185.9 
151.2 
64×64 
179.2 
256×16 
194.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>P = 4096 (8×32×16 torus) 
P \ N 2048×2048×512 512×2048×2048 
8×512 
215.2 
181.36 
32×128 
218.4 
190.4 
64×64 
201.7 
179.3 
128×32 
198.2 
194.4 
512×8 
239.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>P = 8192 (16×32×16 torus) 
P \ N 2048×2048×512 512×2048×2048 
16×512 
146.5 
113.1 
64×128 
142.5 
115.3 
128×64 
144.3 
125.7 
512×16 
165.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table>P = 16384 (32×32×16 torus) 
P \ N 2048×2048×512 512×2048×2048 
16×1024 
79.6 
32×512 
117.9 
84.4 
128×128 
118.1 
91.1 
512×32 
128.2 
1024×16 
160.1 

</table></figure>

			<note place="foot" n="4"> The program, provided by Joerg Schumacher, has been modified for our benchmarking purpose.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Algorithm for the Machine Calculation of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Fourier Series. Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://www.intel.com/cd/software/products/asmo-na/eng/307757.htm" />
		<title level="m">Intel Corporation. Intel Math Kernel Library (MKL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Development and Integration of a Distributed 3D FFT for a Cluster of Workstations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Board</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual Linux Showcase and Conference</title>
		<meeting>the 4th Annual Linux Showcase and Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Redistribution strategies for portable parallel FFT: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Tessera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable framework for 3D FFTs on the Blue Gene/L supercomputer: implementation and early performance measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eleftheriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rayshubskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J C</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Germain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="457" to="464" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The IBM Parallel Engineering and Scientific Subroutine Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Filippone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Applied Parallel Computing, Computations in Physics</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blue Matter: Scaling of N-body simulations to one atom per node</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rayshubskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eleftheriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J C</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Giampapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pitera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Swope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Germain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The design and implementation of FFTW3. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="216" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the Blue Gene/L system architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Blumrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L.-T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coteus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Giampapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Haring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heidelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoenicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Kopcsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Liebsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ohmacht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Steinmacher-Burow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vranas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="195" to="212" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Static and Dynamic Coupling Transitions of Vortex Lattices in Disordered Anisotropic Superconductors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Zimnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Kolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grnbech-Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">5416</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Turbulence in Laterally Extended Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Schumacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Putz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Computing: Architectures, Algorithms and Applications</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instability of 2D Flows to Hydrostatic 3D Perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Straub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Atmos. Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="79" to="102" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">New Method for High-Accuracy Determination of the Fine-Structure Constant Based on Quantized Hall Resistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Klitzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pepper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="494" to="497" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
