<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Factorized Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomi</forename><surname>Silander</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Helsinki Institute for Information Technology HIIT</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teemu</forename><surname>Roos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Helsinki Institute for Information Technology HIIT</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petri</forename><surname>Kontkanen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Helsinki Institute for Information Technology HIIT</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petri</forename><surname>Myllymäki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Helsinki Institute for Information Technology HIIT</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Factorized Normalized Maximum Likelihood Criterion for Learning Bayesian Network Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a new scoring criterion, factorized normalized maximum likelihood, for learning Bayesian network structures. The proposed scoring criterion requires no parameter tuning, and it is decomposable and asymptotically consistent. We compare the new scoring criterion to other scoring criteria and describe its practical implementation. Empirical tests confirm its good performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The popular Bayesian criterion, BDeu <ref type="bibr" target="#b1">(Buntine, 1991)</ref>, for learning Bayesian network structures has recently been reported to be very sensitive to the choice of prior hyperparameters <ref type="bibr" target="#b19">(Silander et al., 2007</ref>). On the other hand, general model selection criteria, such as AIC <ref type="bibr" target="#b0">(Akaike, 1973)</ref> and BIC <ref type="bibr" target="#b16">(Schwarz, 1978)</ref>, are derived through asymptotics and their behavior is suboptimal for small sample sizes. The study of different scoring criteria is further complicated by the fact that learning the network structure is NPhard for all popular scoring criteria <ref type="bibr" target="#b2">(Chickering, 1996)</ref>, even if these criteria have a convenient characteristic of decomposability, which allows incremental scoring in heuristic local search <ref type="bibr" target="#b9">(Heckerman et al., 1995)</ref>. Due to recent advances in exact structure learning ( <ref type="bibr" target="#b11">Koivisto and Sood, 2004;</ref><ref type="bibr" target="#b18">Silander and Myllymäki, 2006</ref>) it is feasible to find the optimal network for decomposable scores when the number of variables is less than about 30. This makes it possible to study the behavior of different scoring criteria without the uncertainty stemming from the heuristic search.</p><p>In this paper we introduce a new decomposable scoring criterion for learning Bayesian network structures, the factorized normalized maximum likelihood (fNML). This score features no tunable parameters, thus avoiding the sensitivity problems of Bayesian scores. We show that the new criterion is asymptotically consistent.</p><p>Unlike AIC and BIC, it is derived based on optimality criterion for finite sample sizes, and it has a probabilistic interpretation.</p><p>The rest of the paper is structured as follows. In Section 2, we will first introduce Bayesian networks and the notation needed later. In Section 3, we review the most popular decomposable scores, after which in Section 4, we are ready to introduce the fNML criterion. We then briefly discuss the implementation of this new score in Section 5. Section 6 presents the empirical experiments, and the conclusions are summarized in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bayesian Networks</head><p>We assume that reader is familiar with Bayesian networks (for tutorial, see <ref type="bibr" target="#b10">(Heckerman, 1996)</ref>), and only introduce the notation needed later in this paper.</p><p>A Bayesian network defines a joint probability distribution for an m-dimensional multivariate data vector X = (X 1 , . . . , X m ). We assume that all variables are discrete, so that variable X i may have r i different values {1, . . . , r i }.</p><p>A Bayesian network consists of a directed acyclic graph G and a set of conditional probability distributions. We specify the DAG with a vector G = (G 1 , . . . , G m ) of parent sets so that G i ⊂ {X 1 , . . . , X m } denotes the parents of variable X i , i.e., the variables from which there is an arc to X i . Each parent set G i has q i (q i = Xp∈G i r p ) possible values that are the possible value combinations of the variables belonging to G i . We assume a non-ambiguous enumeration of these values and denote the fact that G i holds the j th value combination simply by G i = j.</p><p>The local Markov property for Bayesian networks states that each variable is independent of its non-descendants given its parents. Functionally this is equivalent to the following factorization of the joint distribution</p><formula xml:id="formula_0">P (x | G) = m i=1 P (x i | G i ).<label>(1)</label></formula><p>The conditional probability distributions P (X i | G i ) are determined by a set of parameters, Θ, via the equation</p><formula xml:id="formula_1">P (X i = k | G i = j, Θ) = θ ijk ,</formula><p>where k is a value of X i , and j is a value configuration of the parent set G i . We denote the set of parameters associated with variable X i by Θ i .</p><p>For Since the rows of D are assumed to be i.i.d, the probability of a data matrix can be calculated just by taking the product of the row probabilities. Combining equal terms yields</p><formula xml:id="formula_2">P (D | G, Θ) = m i=1 q i j=1 r i k=1 θ N ijk ijk ,<label>(2)</label></formula><p>where N ijk denotes number of rows in</p><formula xml:id="formula_3">D X i =k,G i =j .</formula><p>For a given structure G, we use notationˆP notationˆ notationˆP (D | G) = sup θ P (D | G, θ). The maximizing parameters are simply the relative frequencies found in data:</p><formula xml:id="formula_4">ˆ θ ijk = N ijk N ij</formula><p>, where N ij denotes the number of rows in D G i =j , or 1.0 if N ij = 0. We often drop the dependency on G when it is clear from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decomposable scores</head><p>In general, a scoring function Score(G, D) for learning a Bayesian network structure is called decomposable, if it can be expressed as a sum of local scores</p><formula xml:id="formula_5">Score(G, D) = m i=1 S(D i , D G i ).<label>(3)</label></formula><p>Many popular scoring functions avoid overfitting by balancing the fit to the data with the complexity of the model. A common form of this idea can be expressed as</p><formula xml:id="formula_6">Score(G, D) = logˆPlogˆ logˆP (D | G) − ∆(D, G), (4)</formula><p>where ∆(D, G) is a complexity penalty.</p><p>The maximized likelihoodˆPlikelihoodˆ likelihoodˆP (D | G) decomposes by the network structure, and for the decomposable scores handled in this paper, the complexity penalty decomposes too. Hence, we can write the penalized scores in the decomposed form (3), with the local scores given by</p><formula xml:id="formula_7">S(D i , D G i ) = logˆPlogˆ logˆP (D i | D G i ) + ∆ i (D i , D G i ). (5) Different scores differ in how the local penalty ∆ i (D i , D G i ) is determined.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AIC and BIC</head><p>Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are two popular decomposable scores for learning Bayesian network structures. These scores do not have any additional parameters so in this sense they are similar to the proposed fNML score. The penalty terms for these scores are</p><formula xml:id="formula_8">∆ BIC i = q i (r i −1) 2</formula><p>ln N , and ∆ AIC i = q i (r i − 1). Both of these complexities are independent of the data, and only depend on the arities r i of random variables and the structure of the Bayesian network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bayesian Dirichlet scores</head><p>Bayesian Dirichlet (BD) scores assume that the parameter vectors Θ ij are independent of each other and distributed by Dirichlet distributions with hyper-parameter vector α ij . Given a vector of hyper-parameters α, the local score can be written as</p><formula xml:id="formula_9">S BD (D i , D G i , α) = log P (D i | D G i , α) = q i j=1 log P (D G i =j i | D G i =j G i , α ij ) = q i j=1 log B( α ij + N ij ) B( α ij ) ,</formula><p>where B is a multinomial Beta function</p><formula xml:id="formula_10">B(α 1 , . . . , α K ) = K k=1 Γ(α k ) Γ( K i=1 α k )</formula><p>.</p><p>With all α ijk = 1 we get a K2-score <ref type="bibr" target="#b4">(Cooper and Herskovits, 1992)</ref>, and with α ijk = α q i r i we get a family of BDeu scores popular for giving equal scores to different Bayesian network structures that encode the same independence assumptions. BDeu scores depend only on a single parameter, the equivalent sample size α. Recent studies on the role of this parameter show that network learning under BDeu is very sensitive to this parameter ( <ref type="bibr" target="#b19">Silander et al., 2007)</ref>.</p><p>For comparison, we can write the BD-score as a penalized maximized likelihood with penalty</p><formula xml:id="formula_11">∆ BD i (D i , D G i ) = (6) q i i=i logˆP logˆ logˆP (D G i =j i | D G i =j G i ) P (D G i =j i | D G i =j G i , α ij ) .</formula><p>We immediately notice that this penalty is always positive. The complexity is datadependent and it is controlled by the hyperparameters α ijk . The asymptotic behavior of this Bayesian regret is well studied <ref type="bibr" target="#b7">(Grünwald, 2007)</ref>. However, when learning Bayesian networks, the data parts D G i =j i are often very small, which makes asymptotic result less informative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">fNML</head><p>The factorized normalized maximum likelihood (fNML) score is based on the normalized maximum likelihood (NML) distribution <ref type="bibr" target="#b17">(Shtarkov, 1987;</ref><ref type="bibr" target="#b15">Rissanen, 1996)</ref>. The NML distribution for the model class M (which may or may not be a Bayesian network) is the unique distribution solving the minimax problem</p><formula xml:id="formula_12">min Q max D ′ ˆ P (D ′ | M) Q(D ′ | M) ,<label>(7)</label></formula><p>where Q ranges over all distributions. As originally shown by Shtarkov (1987) the solution of the above minimax problem is given by</p><formula xml:id="formula_13">P NML (D | M) = ˆ P (D | M) D ′ ˆ P (D ′ | M) ,<label>(8)</label></formula><p>where the normalization is over all data sets D ′ of a fixed size N . The log of the normalizing factor is called parametric complexity or regret. Evaluation of the normalizing sum is often hard due to exponential number of terms in the sum. Currently, there are tractable formulas for only a handful of models; for examples, see <ref type="bibr" target="#b7">(Grünwald, 2007)</ref>. In the case of a single r-ary multinomial variable and the sample size n the normalizing sum is given by</p><formula xml:id="formula_14">C r n = k 1 +k 2 +...+kr=n n! k 1 ! k 2 ! · · · k r ! r j=1 k j n k j ,<label>(9)</label></formula><p>where the sum goes over all non-negative integer vectors (k j ) r j=1 that sum to n. A linear-time algorithm for the computation of C r n was introduced recently by .</p><p>Given a data set D, the NML model selection criterion proposes to choose the model M for which the P NML (D | M) is largest. After taking the logarithm the score is in a form of penalized log likelihood with complexity penalty describing how well the model can fit any equal size dataset D ′ .</p><p>Because of the score equivalence of the maximum likelihood score, the NML score is score equivalent as well. However, it is not decomposable, and the parent assignment problem is known to be NP-hard <ref type="bibr" target="#b12">(Koivisto, 2006</ref>). Sacrificing the score equivalence we propose a decomposable version of this score, which penalizes the complexity locally similarly to the other decomposable scores. Specifically, we propose the local score</p><formula xml:id="formula_15">S fNML (D i , D G i ) = log P NML (D i | D G i ) (10) = logˆP logˆ logˆP (D i | D G i ) D ′ i ˆ P (D ′ i | D G i ) ,</formula><p>where the normalizing sum goes over all the possible D i -column vectors of length N , i.e.,</p><formula xml:id="formula_16">D ′ i ∈ {1, . . . , r i } N .</formula><p>Since equation (10) defines a (log) conditional distribution for the data column D i , adding these local scores together yields a total score that defines a distribution for the whole data. In this sense fNML can be seen as an alternative way to define the marginal likelihood for the data</p><formula xml:id="formula_17">log P fNML (D | G) = m i=1 log P NML (D i | D G i ).</formula><p>At the same time, combining the local scores yields an enumerator that equals the decomposition of the maximum likelihood, thus the whole score can be seen as a penalized maximum log-likelihood with local (data-dependent) penalties</p><formula xml:id="formula_18">∆ fNML i (D G i ) = log D ′ i ˆ P (D ′ i | D G i ).<label>(11)</label></formula><p>The following observation follows from the factorization of the maximum likelihood by the parent configurations, and it is crucial for efficient calculation of the local penalty term. Theorem 1. The local penalty of fNML can be expressed in terms of multinomial normalizing constants</p><formula xml:id="formula_19">∆ fNML i (D G i ) = q i j=1 log C r i N ij ,</formula><p>where</p><formula xml:id="formula_20">C r i N ij</formula><p>is the normalizing constant of NML for an r i -ary multinomial model with sample size N ij .</p><p>The theorem follows by noting that the maximized likelihoodˆPlikelihoodˆ likelihoodˆP (D i | D G i ) factorizes into independent parts according to the values of D G i .</p><p>To conclude this section we show that asymptotically, and under mild regularity conditions, the fNML score belongs to the (large) class of BIC-like scores that are consistent. Other scores in this class include most Bayesian and MDL criteria. The regularity conditions required for BIC-like behavior typically exempt a measure zero set of generating parameters, such as the boundaries of the parameter simplex. The following theorem gives sufficient conditions on the penalty term that guarantee consistency for exponential family models.</p><p>Theorem 2 (Remark 1.2 in <ref type="bibr" target="#b8">(Haughton, 1988)</ref>). For (curved) exponential families, if data is generated by an i.i.d. distribution p, and the penalty term is given by <ref type="bibr">1</ref> 2 k a N , where k is the number of parameters and a N is a sequence of positive real numbers, satisfying a N /N → 0, and a N → ∞, as N → ∞, then, symptotically, the model containing p that has the least number of parameters will be chosen.</p><p>Since Bayesian networks are curved exponential families <ref type="bibr" target="#b6">(Geiger et al., 2001;</ref><ref type="bibr" target="#b3">Chickering, 2002)</ref>, it now remains to prove that the penalty term of fNML satisfies this property.</p><p>Theorem 3 (Asymptotically fNML behaves like BIC). Assuming that the maximum likelihood parameters are asymptotically bounded away from the boundaries of the parameter simplex, the local penalty of fNML behaves as</p><formula xml:id="formula_21">∆ fNML i (D G i ) = q i (r i − 1) 2 log N + O(1),</formula><p>almost surely, where the O(1) term is bounded by a constant wrt. N .</p><p>Proof. By Thm. 1, the local penalty is a sum of logarithms of multinomial normalizing constants. The latter is known to grow as log C r i N ij = r i −1 2 log N ij +O(1), <ref type="bibr" target="#b15">(Rissanen, 1996)</ref>. Under the assumption that the maximum likelihood parameters are bounded away from the boundaries, the counts N ij grow linearly in the total sample size N almost surely, which implies that we have log N ij = log([η + o(1)]N ) = log N + O(1) with some 0 &lt; η &lt; 1. Adding together the q i terms yields the result.</p><p>Since q i (r i − 1) is the number of parameters (associated with the ith variable), the property of Thm. 2 holds for the fNML penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>We now provide information for practical implementation of the fNML score for Bayesian networks. Due to the decomposability of the score the only new implementational issue for the fNML is to calculate the terms C r n of the Thm. 1. For reasonable N and R (R = max r i ) these values can be stored in an N × R table, which can be done before structure learning. Moreover, this table does not depend on data or any parameters, so it can be done just once.</p><p>The calculation of the C-table with N rows and R columns proceeds as follows. First of all, C r 0 = 1 for all r, and C 1 n = 1 for all n. For r = 2 we can use the formula (9), which yields</p><formula xml:id="formula_22">C 2 n = n h=0 n h h n h n − h N n−h ,<label>(12)</label></formula><p>and for r &gt; 2 we can use the recursion <ref type="bibr">(Kontka- nen and Myllymäki, 2007</ref>)</p><formula xml:id="formula_23">C r n = C r−1 n + n r − 2 C r−2 n .<label>(13)</label></formula><p>Calculating the column C 2 * using the formula (12) takes time O(N 2 ), and the calculation of the rest of the table using the formula (13) takes just O(N K). For very large N , the complexity of calculating the column C 2 * may be prohibitive. In this case a very accurate Szpankowski approximation ( <ref type="bibr" target="#b14">Kontkanen et al., 2003</ref>)</p><formula xml:id="formula_24">C 2 n = nπ 2 e q 8 9nπ + 3π−16 36nπ (14)</formula><p>can be used.</p><p>If the space for storing the table is critical, one may just store 1000 first entries of column C 2 * , use Szpankowski approximation for the rest of the column, and use formula (13) for calculating the values for r &gt; 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>It is not obvious how to compare different criteria for learning Bayesian network structures. If the data is generated from a Bayesian network, one might call for selecting the data generating network, but if the generating network is complex, and the sample size is small, it may be rational to pick a simpler model. This simplicity requirement is often backed up by arguments about the generalization capability of the model. However, it is not always clear how the network structure should be used for prediction.</p><p>A softer version of discovering the generating model is to compute a structural distance measure between the selected and the generating network structures. A common choice is to calculate an editing distance with operations such as arc additions, deletions and reversals. Even if we take the generating structure as a golden standard, this approach is problematic, since these editing operations are not independent. For example, fixing a certain arc can lead to several other changes to the network structure if the selection by a score is made only among the structures having the fixed arc present.</p><p>Despite of these problems in the empirical testing, we conducted a golden standard experiment. We first generated data from different networks with five nodes, and then studied how the generating network structures were ranked among all the possible networks by different scoring criteria.</p><p>For BDeu and fNML scores that both calculate the probability P (D | G), we also compared the scores for the real data sets. This experiment can be seen as the result of a sequential prediction competition, since by the chain rule we can write</p><formula xml:id="formula_25">P (D | G) = N i=1 P (d i | G, d i−1 ),<label>(15)</label></formula><p>where d i is the ith data vector, and d i−1 = {d 1 , . . . , d i−1 } denotes the first i−1 vectors. The idea follows the principle of prequential model selection <ref type="bibr" target="#b5">(Dawid, 1984)</ref>. We will now explain the experiments in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Artificial data</head><p>We first compared the ability of different scoring criteria to discover the data generating structure. For this purpose we generated 100 different 5-node Bayesian network structures with 4 edges and another 100 structures with 7 edges. The variables were randomly assigned to have 2 -4 values (r i ∈ {2, 3, 4}). For each network, we generated parameters by two different schemes. The first scheme exactly matched the assumptions of the BDeu score with α = 1, i.e., the parameters were distributed by θ ij ∼ Dir( 1</p><formula xml:id="formula_26">r i q j , . . . , 1 r i q j</formula><p>). The other scheme was to generate the parameters independently from a Dirichlet distribution θ ij ∼ Dir(1/2, . . . , 1/2). This distribution was selected instead of the uniform distribution in order to make the generating structure more identifiable.</p><p>For each network (structure + parameters), we generated 100 data sets of 1000 data vectors, and studied how different scoring criteria ranked the structure of the generating network among all the 5-node networks as a function of (sub)sample size.</p><p>Not surprisingly, the results indicate that when parameter generation mechanism matches the assumptions of the BDeu-score, the BDeu usually also ranks the generating structure higher than the other scores <ref type="figure" target="#fig_0">(Figure 1(a)</ref>). However, fNML usually behaves very similarly to BDeu. The density of the network (4 vs. 7 edges) is not a very significant factor. If anything, the similar behavior of fNML and BDeu is more pronounced in networks with 7 edges. For the parameter-free scores, AIC and BIC, the underfitting tendency of BIC can be clearly detected whereas AIC tends to rank the generating network higher. Qualitatively these two scores seem to behave similarly to each other.</p><p>Switching the parameter generation scheme to independent Dirichlets with α ijk = 0.5 usually also switches the ranking ability of fNML and BDeu, while the behavior of AIC and BIC stays mostly unaffected. For example, <ref type="figure" target="#fig_0">Fig- ure 1(b)</ref> was generated using the same network structure as for <ref type="figure" target="#fig_0">Figure 1(a)</ref>. Only the parameter generation scheme was changed from BDeu to Dir. For dense networks fNML often appears as a clear winner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Real data</head><p>Learning the structure with AIC or BIC does not readily suggest any particular way to use the learned structure for prediction, but the prequential interpretation of the BDeu score and the fNML allows comparison. However, the BDeu score is known to be very sensitive to the For predictive comparison we selected 20 UCI data sets 1 for which the score maximizing hyperparameter α has been reported <ref type="bibr" target="#b19">(Silander et al., 2007)</ref>, and we compared the maximum fNML scores to the maximum scores obtained with BDeu1 (BDeu with α = 1.0) and BDeu* (BDeu with score maximizing α). In reality, we do not know the score maximizing α's, and searching structures with many α is usually computationally too hard. Optimal structures were obtained by the exact structure learning algorithm described in <ref type="bibr">(Silander and Myl- lymäki, 2006</ref>). <ref type="table" target="#tab_1">Table 1</ref> lists for each data set the number of data vectors N , the number of variables m, the average number of values per variable #vals, the BDeu maximizing equivalent sample size parameter α * (with integer precision), and the ac-1 http://www.ics.uci.edu/ ∼ mlearn/MLRepository.html tual scores obtained with three different scoring criteria. The score obtained with fNML is the best of the three 14 times out of 20, and only once BDeu1 yields higher score than fNML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have introduced a new probabilistic scoring criterion, the factorized normalized maximum likelihood, for learning Bayesian network structures from complete discrete data. The score aims at being an efficient and parameter-free criterion for finite sample sizes. The score is also decomposable, which makes it possible to use it with existing search heuristics and exact structure learning algorithms.</p><p>Initial empirical tests are promising. We are particularly pleased with fNML's ability to learn network structures with good predictive capabilities. While lot more empirical work has to be done, the current experiments already show a great promise for a good and care free scoring criterion for learning Bayesian network structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The median curves for different scoring criteria as a function of sample size when the parameters for a 5-node, 7-edge network were generated by the BDeu and Dir(1/2,. . . ,1/2) schemes. Errorbars indicate upper and lower quartiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Summary of the prediction experiment.</head><label>1</label><figDesc></figDesc><table>Data 
N m #vals α  *  
BDeu1 
BDeu* 
fNML 

balance 
625 
5 
4.6 48 
-4549.06 
-4445.64 
-4478.36 
iris 
150 
5 
3.0 
2 
-452.21 
-449.71 
-450.90 
thyroid 
215 
6 
3.0 
2 
-577.52 
-575.55 
-572.42 
liver 
345 
7 
2.9 
4 
-1309.67 
-1299.83 
-1299.38 
ecoli 
336 
8 
3.4 
8 
-1715.92 
-1661.34 
-1643.64 
abalone 
4177 
9 
3.0 
6 
-15946.58 
-15891.25 
-15847.33 
diabetes 
768 
9 
2.9 
4 
-3678.57 
-3662.31 
-3654.02 
post operative 
90 
9 
2.9 
3 
-647.35 
-642.98 
-639.94 
yeast 
1484 
9 
3.7 
6 
-7938.60 
-7873.21 
-7848.98 
breast cancer 
286 10 
4.3 
8 
-2781.62 
-2737.20 
-2739.34 
shuttle 
58000 10 
3.0 
3 -97635.72 -97620.78 
-97714.22 
tic tac toe 
958 10 
2.9 51 
-9423.07 
-9126.78 
-9162.39 
bc wisconsin 
699 11 
2.9 
8 
-3315.51 
-3262.33 
-3239.56 
glass 
214 11 
3.3 
6 
-1288.93 
-1255.73 
-1233.18 
page blocks 
5473 11 
3.2 
3 
-12455.60 
-12438.01 
-12410.69 
heart cleveland 
303 14 
3.1 13 
-3450.07 
-3356.78 
-3352.32 
heart hungarian 
294 14 
2.6 
5 
-2376.53 
-2348.23 
-2343.65 
heart statlog 
270 14 
2.9 10 
-2867.54 
-2819.37 
-2814.28 
wine 
178 14 
3.0 
8 
-1866.41 
-1821.28 
-1808.66 
adult 
32561 15 
7.9 50 -329373.73 -326803.91 -326486.85 

equivalent sample size parameter, which creates 
an extra complication. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the anonymous referees for valuable comments. This work was supported in part by the Academy of Finland (Project Civi), by the Finnish Funding Agency for <ref type="bibr">Technology and Innovation (Kukot, PMMA)</ref>, and the IST Programme of the European Community, under the PASCAL Network of Excellence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Information Theory</title>
		<editor>B.N. Petrox and F. Caski</editor>
		<meeting>the Second International Symposium on Information Theory<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<publisher>Akademiai Kiado</publisher>
			<date type="published" when="1973" />
			<biblScope unit="page" from="267" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theory refinement on Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<editor>B. D&apos;Ambrosio, P. Smets, and P. Bonissone</editor>
		<meeting>the Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks is NP-Complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from Data: Artificial Intelligence and Statistics V</title>
		<editor>D. Fisher and H. Lenz</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical theory: The prequential approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society A</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="278" to="292" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stratified exponential families: graphical models and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="505" to="529" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Minimum Description Length Principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grünwald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the choice of a model to fit data from an exponential family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M A</forename><surname>Haughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="355" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A tutorial on learning with Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<idno>MSR-TR- 95-06</idno>
	</analytic>
	<monogr>
		<title level="j">One Microsoft Way</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>Advanced Technology Division</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exact Bayesian structure discovery in Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="549" to="573" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parent assignment is hard for the MDL, AIC, and NML costs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koivisto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual Conference on Learning Theory (COLT-06)</title>
		<meeting>the 19th Annual Conference on Learning Theory (COLT-06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="289" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A lineartime algorithm for computing the multinomial stochastic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient computation of stochastic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Intelligence and Statistics</title>
		<editor>C. Bishop and B. Frey</editor>
		<meeting>the Ninth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fisher information and stochastic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="1996-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Universal sequential coding of single messages. Problems of Information Transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shtarkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple approach for finding the globally optimal Bayesian network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence</title>
		<editor>R. Dechter and T. Richardson</editor>
		<meeting>the 22nd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="445" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On sensitivity of the MAP Bayesian network structure to the equivalent sample size parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence</title>
		<editor>R. Parr and L. van der Gaag</editor>
		<meeting>the 23rd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="360" to="367" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
