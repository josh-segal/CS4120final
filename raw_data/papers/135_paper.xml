<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetry of Information: A Closer Look</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><forename type="middle">Zimand</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Towson University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetry of Information: A Closer Look</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Symmetry of information establishes a relation between the information that x has about y (denoted I(x : y)) and the information that y has about x (denoted I(y : x)). In classical information theory, the two are exactly equal, but in algorithmical information theory, there is a small excess quantity of information that differentiates the two terms, caused by the necessity of packaging information in a way that makes it accessible to algorithms. It was shown in [Zim11] that in the case of strings with simple complexity (that is the Kolmogorov complexity of their Kolmogorov complexity is small), the relevant information can be packed in a very economical way, which leads to a tighter relation between I(x : y) and I(y : x) than the one provided in the classical symmetry-of-information theorem of Kolmogorov and Levin. We give here a simpler proof of this result. This result implies a van Lambalgen-type theorem for finite strings and plain complexity: If x is c-random and y is c-random relative to x, then xy is O(c)-random. We show that a similar result holds for prefix-free complexity and weak-K-randomness.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In classical information theory, the information that X has about Y is equal to the information that Y has about X, i.e., I(X : Y ) = I(Y : X). <ref type="bibr">1</ref> In algorithmical information theory, there exists a similar relation, but it is less perfect:</p><p>I(x : y) ≈ I(y : x).</p><p>In this paper we take a closer look at the level of approximation indicated by "≈." In the line above, x and y are binary strings of lengths n x and, respectively, n y , and I(x : y), the information in x about y, is defined by I(x : y) = C(y | n y ) − C(y | x, n y ), where C(·) is the plain Kolmogorov complexity. The precise form of the equation is given by the classical Kolmogorov-Levin theorem <ref type="bibr">[ZL70]</ref>, which states that |I(x : y) − I(y : x)| = O(log n x + log n y ).</p><p>This is certainly an important result, but in some situations, it is not saying much. Suppose, for example, that x has very little information about y, say, I(x : y) is a constant (not depending on x and y). What does this tell about I(y : x)? Just from the definition and ignoring I(x : y), I(y : x) can be as large as n x − O(1), and, somewhat surprisingly, given that I(x : y) = O(1), I(y : x) can still have the same order of magnitude. As an example, consider y a c-random string (meaning C(y | n y ) ≥ n y − c), whose length n y is also c-random. Let x be the binary encoding of n y . Then I(x :</p><formula xml:id="formula_1">y) = C(y | n y ) − C(y | x, n y ) = O(1), but I(y : x) = C(x | n x ) − C(x | y, n x ) = n x − O(1)</formula><p>. This example shows that even though, for some strings, the relation (1) is trivial, it is also tight. In many applications, the excess term O(log n x + log n y ) does not hurt too much. But sometimes it does. For example, it is the reason why the KolmogorovLevin Theorem does not answer the following basic "direct product" question: If x and y are c-random n-bit strings and I(x : y) ≤ c, does it follow that xy is</p><formula xml:id="formula_2">O(c)-random?</formula><p>The answer is positive (providing the finite analog of the van Lambalgen theorem). It follows from a recent result of the author <ref type="bibr">[Zim11]</ref>, which establishes a more precise symmetry-of-information relation for strings with simple complexity, i.e., for strings x such that C (2) (x | n x ) is small, say, bounded by a constant. <ref type="bibr">2</ref> Note that all random strings have simple complexity, and that there are other types of strings with simple complexity as well. The main result in <ref type="bibr">[Zim11]</ref> implies that if x and y are strings of equal length that have simple complexity bounded by c and I(x : y) ≤ c, then I(y : x) ≤ O(c).</p><p>The proof method in <ref type="bibr">[Zim11]</ref> is based on randomness extraction. Alexander Shen (personal communications) has observed that in fact a proof of the above result in <ref type="bibr">[Zim11]</ref> can be obtained via the standard method used in the proof of Kolmogorov-Levin Theorem (see <ref type="bibr">[DH10,LV08]</ref>). We present here such a proof.</p><p>We slightly extend the symmetry-of-information relation from <ref type="bibr">[Zim11]</ref> to strings x and y that may have different lengths. We prove the following (with the convention that log 0 = 0):</p><p>Theorem 1 (Main Theorem). For all strings x and y,</p><formula xml:id="formula_3">I(y : x) ≤ I(x : y) + O(log I(x : y)) + O(log |n x − n y |) + δ(x, y), where δ(x, y) = O(C (2) (x | n x ) + C (2) (y | n y )).</formula><p>Thus, for strings x and y with simple complexity, I(y : x) ≤ I(x : y) + O(log I(x : y)) + O(log |n x − n y |).</p><p>As mentioned, Theorem 1 implies an analog of the van Lambalgen Theorem for finite strings and plain Kolmogorov complexity. Recall that van Lambalgen Theorem <ref type="bibr">[vL87]</ref> states that for infinite sequences x and y the following two statements are equivalent:</p><p>(1) x is random and y is random relative by x, (2) x ⊕ y is random.</p><p>(Here, random means Martin-Löf random, and x ⊕ y is the infinite sequence obtained by alternating the bits of x and y). Theorem 1 implies immediately the following.</p><formula xml:id="formula_4">Theorem 2 ([Zim11])</formula><p>. Let x and y be two strings of length n such that x is c-random and y is c-random relative to x. Then xy is O(c)-random.</p><p>Formally</p><formula xml:id="formula_5">, if C(x | n) ≥ n − c, C(y | x) ≥ n − c, then C(xy | 2n) ≥ 2n − O(c).</formula><p>(The constant in the O(·) notation depends only on the universal Turing machine.)</p><p>Is there a van Lambalgen Theorem analog for prefix-free complexity? To adress this question, we note that there are two notions of randomness for the prefix-free complexity K, weak K-randomness, and strong K-randomness (see <ref type="bibr">[DH10]</ref>). An n-bit string x is (1) weakly c-(</p><formula xml:id="formula_6">K-random) if K(x | n) ≥ n − c, and (2) strongly c-(K-random) if K(x | n) ≥ n + K(n) − c.</formula><p>For weak K-randomness, we prove in Section 4 the following analog of the van Lambalgen Theorem. For strong K-randomness, the question remains open.</p><p>Theorem 3. Let x and y be two strings of length n such that x is weakly c-(K-random) and y is weakly c-(K-random) relative to x. Then xy is weakly O(c)-(K-random).</p><p>Formally</p><formula xml:id="formula_7">, if K(x | n) ≥ n − c, K(y | x) ≥ n − c, then K(xy | 2n) ≥ 2n − O(c).</formula><p>(The constant in the O(·) notation depends only on the universal Turing machine.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Proof techniques</head><p>The proofs of Symmetry of Information Theorems have a combinatorial nature. To fix some ideas, let us first sketch the proof of the classical KolmogorovLevin Theorem which establishes relation (1). The setting of the theorem, as well as for the rest of our discussion, is as follows: x and y are binary strings of lengths n x and, respectively, n y . An easy observation (see Section 2.2), shows that a relation between I(y : x) and I(x : y) can be deduced from a "chain rule:" C(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − (small term). In our case, to obtain (1), it is enough to show that</p><formula xml:id="formula_8">C(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − O(log n x + log n y ).<label>(2)</label></formula><p>Let us consider a 2 nx × 2 ny table with rows indexed by u ∈ {0, 1} nx and columns indexed by v ∈ {0, 1} ny . Let t = C(xy | n x , n y ). We assign boolean values to the cells of the table as follows. The (u, v)-cell in the table is 1 if C(uv | n x , n y ) ≤ t and it is 0 otherwise. The number of cells equal to 1 is less than 2 t+1 , because there are only 2 t+1 programs of length ≤ t. Let m be such that the number of 1's in the x row is in the interval (2 m−1 , 2 m ]. Note that, given x, n y and t, we can enumerate the 1-cells in the x-row and one of them is the (x, y) cell. It follows that C(y | x, n y ) ≤ m + O(log t).</p><p>Now consider the set of rows that have at least 2 m−1 1s. The number of such rows is at most 2 t+1 /2 m−1 = 2 t−m+2 . We can effectively enumerate these rows if we are given n x , n y , m and t. Since the x row is one of these rows, it follows that C(x | n x ) ≤ t − m + O(log n y + log m + log t).</p><p>Adding equations <ref type="formula" target="#formula_9">(3)</ref> and <ref type="formula" target="#formula_10">(4)</ref> and keeping into account that log m ≤ log n y and log t ≤ O(log n x + log n y ), the relation (2) follows. A careful inspection of the proof reveals that the excess term O(log n x + log n y ) is caused by the fact that the enumerations involved in describing y and x need to know m (which is bounded by C(y | n y )) and t = C(xy | n x , n y ). In case x and y are c-random, it is more economical to use randomness deficiencies. In particular instead of using t = C(xy | n x , n y ), we can do a similar argument based on the randomness deficiency of xy, i.e., w = (n x + n y ) − C(xy | n x , n y ). This is an observation of Chang, Lyuu, Ti and Shen <ref type="bibr">[CLTS09]</ref>, who attribute it to folklore. For strings with simple complexity, it is advantageous to express t as t = C(x | n x ) + C(y | x) − w because the first two terms have a short description. This yields a proof of the Main Theorem which we present in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and background on Kolmogorov complexity</head><p>The Kolmogorov complexity of a string x is the length of the shortest effective description of x. There are several versions of this notion. We use here mainly the plain complexity, denoted C(x), and the conditional plain complexity of a string x given a string y, denoted C(x | y), which is the length of the shortest effective description of x given y. The formal definitions are as follows. We work over the binary alphabet {0, 1}. A string is an element of {0, 1} * . If x is a string, n x denotes its length. Let M be a Turing machine that takes two input strings and outputs one string. For any strings x and y, define the Kolmogorov complexity of x conditioned by y with respect to M , as</p><formula xml:id="formula_11">C M (x | y) = min{|p| | M (p, y) = x}.</formula><p>There is a universal Turing machine U with the following property: For every machine M there is a constant c M such that for all x, C U (x | y) ≤ C M (x | y) + c M . We fix such a universal machine U and dropping the subscript, we write C(x | y) instead of C U (x | y). We also write C(x) instead of C(x | λ) (where λ is the empty string). If n is a natural number, C(n) denotes the Kolmogorov complexity of the binary representation of n. We use C (2) (x|n x ) as a shorthand for C(C(x | n x ) | n x ). For two strings x and y, the information in x about y is denoted I(x : y) and is defined as I(x : y) = C(y | n y ) − C(y | x, n y ).</p><p>Prefix-free complexity K is defined in a similar way, the difference being that the universal machine is required to be prefix-free.</p><p>In this paper, the constant hidden in the O(·) notation only depends on the universal Turing machine.</p><p>In this paper, by convention log 0 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Symmetry of Information and the Chain Rule</head><p>All the forms of the Symmetry of Information that we are aware of have been derived from the chain rule and this paper is no exception. The chain rule states that C(xy | n x , n y ) ≈ C(x | n x ) + C(y | x, n y ). For us it is of interest to have an accurate estimation of the "≈" relation. It is immediate to see that</p><formula xml:id="formula_12">C(xy | n x , n y ) ≤ C(y | n y ) + C(x | y, n x ) + 2C (2) (y | n y ) + O(1).</formula><p>If we show a form of the converse inequality</p><formula xml:id="formula_13">C(xy | n x , n y ) ≥ C(x | n x ) + C(y | x, n y ) − (small term),<label>(5)</label></formula><p>then we deduce that</p><formula xml:id="formula_14">C(x | n x ) − C(x | y, n x ) ≤ C(y | n y ) − C(y | x, n y ) + 2C (2) (y | n y ) + (small term), i.e., I(y : x) ≤ I(x : y) + 2C (2) (y | n y ) + (small term).</formula><p>Thus our efforts will be directed to establishing forms of the relation (5) in which (small term) is indeed small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proof of the Main Theorem</head><p>We demonstrate Theorem 1, using the standard proof method of the KolmogorovLevin Theorem.</p><p>Let t x = C(x | n x ), t y = C(y | x, n y ) and t = C(xy | n x , n y ). We take w = t x + t y − t, which is the term called (small term) in equation <ref type="formula" target="#formula_13">(5)</ref>, and plays the role of randomness deficiency. Our goal is to show that w = O(log I(x : y)) + O(log |n x − n y |) + δ(x, y), from which the Main Theorem follows (using the discussion in Section 2.2).</p><p>We assume that w &gt; 0, otherwise there is nothing to prove. We will need to handle information (|n x − n y |, t x , t y , w, b)), where b is a bit that indicates if n x &gt; n y or not. Let Λ be a string encoding in a self-delimiting way this information, and let λ be the length of Λ. Note that λ ≤ 2 log w + O(C (2) (x | n x ) + C (2) (y | n y ) + log I(x : y) + log |n x − n y |).</p><p>We build a 2 nx × 2 ny boolean table, with rows and columns indexed by the strings in {0, 1} nx and, respectively, {0, 1} ny , and we set the value of cell (u, v) to be 1 if C(uv | n x , n y ) ≤ t, and to be 0 otherwise. Let S be the set of 1-cells, and S u be the set of 1-cells in row u. Note that |S| ≤ 2 t+1 .</p><p>Let m be defined as 2 m−1 &lt; |S x | ≤ 2 m . We take F to be the set of "fat" rows, i.e., the set of rows having more than 2 m−1 1-cells. We have |F | &lt; |S|/2 m−1 ≤ 2 t−m+2 . Note that x is in F , and that the elements of F can be effectively enumerated given the information Λ and m. It follows that, given Λ and m, the string x can be described by its index in the enumeration of F . This index can be written on exactly t − m + 2 bits, so that knowing t which can be deduced from Λ, we can reconstruct m. It follows that</p><formula xml:id="formula_15">C(x | n x , Λ) ≤ t − m + 2 + O(1). (6)</formula><p>Next we note that y is in S x and the elements of S x can be enumerated given x and Λ. It follows that y can be described by its index in the enumeration of S x . We obtain C(y | x, Λ) ≤ m + O(1).</p><p>From Equations <ref type="formula">(6)</ref> and <ref type="formula" target="#formula_16">(7)</ref>, we conclude that</p><formula xml:id="formula_17">C(x|n x , Λ) + C(y|x, Λ) ≤ t + O(1) = t x + t y − w + O(1)</formula><p>.</p><formula xml:id="formula_18">Note that t x = C(x | n x ) ≤ C(x | n x , Λ) + O(λ) and t y = C(y | x, n y ) ≤ C(y | x, Λ) + O(λ)</formula><p>. We obtain t x + t y ≤ t x + t y − w + O(λ). Taking into account the estimation for λ, we have w −O(log w) = O(C (2) (x | n x )+C (2) (y | n y )+log I(x : y) + log |n x − n y |), from which, w = O(C (2) (x | n x ) + C (2) (y | n y ) + log I(x : y) + log |n x − n y |), as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A van Lambalgen Theorem for prefix-free complexity</head><p>In this section we prove Theorem 3. We introduce first the following notation: d is a self delimited encoding of the integer d. We can have |d| ≤ 2 log d + 4.</p><p>For a string u, let A u (d) = {v ∈ {0, 1} n | K(uv | n) ≤ 2n − d}, and let</p><formula xml:id="formula_19">F (d) = {u ∈ {0, 1} n | |A u (d)| ≥ 2 n−d }.</formula><p>Consider the following program p 1 , which has x of length n on the oracle tape (as conditional information):</p><p>On input the pair of positive integers (d, i), encoded as d bin(i), check if i is written on n − d bits. If not, diverge. If yes, enumerate strings u of length n such that K(xu | n) ≤ 2n − 2d. Output the i-th string in the enumeration.</p><p>Note that the domain of p 1 is prefix-free. Let c 1 be the length of program p 1 .</p><p>Let p 2 be the following program, which has n on the oracle tape (as conditional information):</p><p>On input the pair of positive integers (d, i), encoded as d bin(i), check if i is written on n − d bits. If not, diverge. If yes, enumerate the elements of F (d) and output the i-th one.</p><p>Program p 2 is prefix-free. Let c 2 be its length.</p><formula xml:id="formula_20">Let d 0 = max{2(c + 4 + c 1 ), 2(c + 4 + c 2 )}.</formula><p>If there is some i such that</p><formula xml:id="formula_21">p 1 (d 0 , i) outputs u, then K(u | x) ≤ n − d 0 + 2 log d 0 + 4 + c 1 &lt; n − c. So, there is no i such that p 1 (d 0 , i) outputs y. This can happen only if either (a) K(xy | n) &gt; 2n − 2d 0 , or (b) K(xy | n) ≤ 2n − 2d 0 but there are 2 n−d0 strings u which are enumerated before y.</formula><p>If (a), we are done. We show that (b) is impossible. Indeed, if (b) holds, then |A x (d 0 )| ≥ 2 n−d0 and thus x is in F (d 0 ). Also |F (d 0 )| ≤ 2 2n−2d0 /2 n−d0 = 2 n−d0 . It follows that for some i, program p 2 on input (d 0 , i) outputs x. But then K(x | n) &lt; n − d 0 + 2 log d 0 + 4 + c 2 &lt; n − c, which is a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Final remarks</head><p>For symmetry of information, the argument from [Zim11] based on randomness extractors, a concept introduced and studied intensively in computational complexity, can be replaced by the simpler and more direct argument suggested by Sasha Shen and presented here. Nevertheless, we still think that extractors play a role in Kolmogorov complexity theory. They have been used in Kolmogorov complexity extraction (see <ref type="bibr">[FHP + 11,HPV11,Mus12,Zim10d,Zim09]</ref>, and the survey paper <ref type="bibr">[Zim10a]</ref>) and, of course, this is not surprising given the analogy between Kolmogorov complexity extraction and randomness extraction. But they have also been used for obtaining results that are not immediately connected to randomness extraction, such as counting results regarding dependent and independent strings <ref type="bibr">[Zim10b]</ref>, and independence amplification <ref type="bibr">[Zim10c]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>I am grateful to Alexander Shen who has noticed that the main theorem can be obtained with the method used in the standard proof of the Kolmogorov-Levin Theorem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The author is supported in part by NSF grant CCF 1016158. Most of the results have been presented at International Workshop on Theoretical Computer Science, Feb 21- 24, 2012, Auckland, New Zealand. URL: http://triton.towson.edu/~mzimand 1 X and Y are random variables and I(X : Y ) = H(Y ) − H(Y | X), where H is Shannon entropy.</figDesc></figure>

			<note place="foot" n="2"> The notation C (2) (x | nx) is a shorthand for C(C(x | nx) | nx).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sets of k-independent sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Algorithmic randomness and complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hirschfeldt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting Kolmogorov complexity with applications to dimension zero-one laws</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">L</forename><surname>Fhp + 11</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Fortnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hitchcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vinodchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="627" to="636" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kolmogorov complexity in randomness extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hitchcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Vinodchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCT</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An introduction to Kolmogorov complexity and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vitanyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
	<note>3rd edition. 1st edition in</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Space-bounded Kolmogorov extractors. CoRR, abs/1203</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Musatov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3674</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Random sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Lambalgen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Amsterdam</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. dissertation. Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting the Kolmogorov complexity of strings and sequences from sources with limited independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 26th STACS</title>
		<meeting>26th STACS<address><addrLine>Freiburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Possibilities and impossibilities in Kolmogorov complexity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="74" to="94" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Counting dependent and independent strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MFCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6281</biblScope>
			<biblScope unit="page" from="689" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Impossibility of independence amplification in Kolmogorov complexity theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MFCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6281</biblScope>
			<biblScope unit="page" from="701" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two sources are better than one for increasing the Kolmogorov complexity of infinite sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="707" to="722" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Symmetry of information and bounds on nonuniform randomness extraction via Kolmogorov complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 26th IEEE Conference on Computational Complexity</title>
		<meeting>26th IEEE Conference on Computational Complexity</meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zvonkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Russian Mathematical Surveys</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="83" to="124" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
