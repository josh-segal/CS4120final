<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speeding Up HMM Decoding and Training by Exploiting Sequence Repetitions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Mozes</surname></persName>
							<email>shaymozes@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Weimann</surname></persName>
							<email>oweimann@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Computer Science and Artificial Intelligence Laboratory</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Ziv-Ukelson</surname></persName>
							<email>michaluz@post.tau.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<postCode>69978</postCode>
									<settlement>Tel-Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speeding Up HMM Decoding and Training by Exploiting Sequence Repetitions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>HMM</term>
					<term>Viterbi</term>
					<term>dynamic programming</term>
					<term>compression</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a method to speed up the dynamic program algorithms used for solving the HMM decoding and training problems for discrete time-independent HMMs. We discuss the application of our method to Viterbi&apos;s decoding and training algorithms [21], as well as to the forward-backward and Baum-Welch [4] algorithms. Our approach is based on identifying repeated substrings in the observed input sequence. We describe three algorithms based alternatively on byte pair encoding (BPE) [19], run length encoding (RLE) and Lempel-Ziv (LZ78) parsing [22]. Compared to Viterbi&apos;s algorithm, we achieve a speedup of Ω(r) using BPE, a speedup of Ω(r log r) using RLE, and a speedup of Ω(log n k) using LZ78, where k is the number of hidden states, n is the length of the observed sequence and r is its compression ratio (under each compression scheme). Our experimental results demonstrate that our new algorithms are indeed faster in practice. Furthermore, unlike Viterbi&apos;s algorithm, our algorithms are highly parallelizable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last few decades, Hidden Markov Models (HMMs) proved to be an extremely useful framework for modeling processes in diverse areas such as errorcorrection in communication links <ref type="bibr" target="#b20">[21]</ref>, speech recognition <ref type="bibr" target="#b5">[6]</ref>, optical character recognition <ref type="bibr" target="#b1">[2]</ref>, computational linguistics <ref type="bibr" target="#b16">[17]</ref>, and bioinformatics <ref type="bibr" target="#b11">[12]</ref>.</p><p>The core HMM-based applications fall in the domain of classification methods and are technically divided into two stages: a training stage and a decoding stage. During the training stage, the emission and transition probabilities of an HMM are estimated, based on an input set of observed sequences. This stage is usually executed once as a preprocessing stage and the generated ("trained") models are stored in a database. Then, a decoding stage is run, again and again, in order to classify input sequences. The objective of this stage is to find the most probable sequence of states to have generated each input sequence given each model, as illustrated in <ref type="figure">Fig. 1</ref>. The HMM on the observed sequence X = x 1 , x 2 , . . . , x n and states 1, 2, . . . , k. The highlighted path is a possible path of states that generate the observed sequence. VA finds the path with highest probability.</p><p>Obviously, the training problem is more difficult to solve than the decoding problem. However, the techniques used for decoding serve as basic ingredients in solving the training problem. The Viterbi algorithm (VA) <ref type="bibr" target="#b20">[21]</ref> is the best known tool for solving the decoding problem. Following its invention in 1967, several other algorithms have been devised for the decoding and training problems, such as the forward-backward and Baum-Welch <ref type="bibr" target="#b3">[4]</ref> algorithms. These algorithms are all based on dynamic programs whose running times depend linearly on the length of the observed sequence. The challenge of speeding up VA by utilizing HMM topology was posed in 1997 by Buchsbaum and Giancarlo <ref type="bibr" target="#b5">[6]</ref> as a major open problem. In this contribution, we address this open problem by using text compression and present the first provable speedup of these algorithms.</p><p>The traditional aim of text compression is the efficient use of resources such as storage and bandwidth. Here, however, we compress the observed sequences in order to speed up HMM algorithms. This approach, denoted "acceleration by text-compression", was previously applied to some classical problems on strings. Various compression schemes, such as LZ77, LZW-LZ78, Huffman coding, Byte Pair Encoding (BPE) and Run Length Encoding (RLE), were employed to accelerate exact and approximate pattern matching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> and sequence alignment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. In light of the practical importance of HMM-based classification methods in state-of-the-art research, and in view of the fact that such techniques are also based on dynamic programming, we set out to answer the following question: can "acceleration by text compression" be applied to HMM decoding and training algorithms?</p><p>Our results. Let X denote the input sequence and let n denote its length. Let k denote the number of states in the HMM. For any given compression scheme, let n denote the number of parsed blocks in X and let r = n/n denote the compression ratio. Our results are as follows.</p><p>1. BPE is used to accelerate decoding by a factor of Ω(r). 2. RLE is used to accelerate decoding by a factor of Ω( r logr ).</p><p>3. Using LZ78, we accelerate decoding by a factor of Ω( log n k ). Our algorithm guarantees no degradation in efficiency even when k &gt; log n and is experimentally more than five times faster than VA. 4. The same speedup factors apply to the Viterbi training algorithm. 5. For the Baum-Welch training algorithm, we show how to preprocess a repeated substring of size once in O(k 4 ) time so that we may replace the usual O(k 2 ) processing work for each occurrence of this substring with an alternative O(k 4 ) computation. This is beneficial for any repeat with λ nonoverlapping occurrences, such that λ &gt; k 2 −k 2 . 6. As opposed to VA, our algorithms are highly parallelizable. This is discussed in the full version of this paper.</p><p>Roadmap. The rest of the paper is organized as follows. In section 2 we give a unified presentation of the HMM dynamic programs. We then show in section 3 how these algorithms can be improved by identifying repeated substrings. Two compressed decoding algorithms are given in sections 4 and 5. In section 6 we show how to adapt the algorithms to the training problem. Finally, experimental results are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let Σ denote a finite alphabet and let X ∈ Σ n , X = x 1 , x 2 , . . . , x n be a sequence of observed letters. A Markov model is a set of k states, along with emission probabilities e k (σ) -the probability to observe σ ∈ Σ given that the state is k, and transition probabilities P i,j -the probability to make a transition to state i from state j.</p><p>The Viterbi Algorithm. The Viterbi algorithm (VA) finds the most probable sequence of hidden states given the model and the observed sequence. i.e., the sequence of states s 1 , s 2 , . . . , s n which maximize</p><formula xml:id="formula_0">n i=1 e s i (x i )P s i ,s i−1<label>(1)</label></formula><p>The dynamic program of VA calculates a vector v t <ref type="bibr">[i]</ref> which is the probability of the most probable sequence of states emitting x 1 , . . . , x t and ending with the state i at time t. v 0 is usually taken to be the vector of uniform probabilities</p><formula xml:id="formula_1">(i.e., v 0 [i] = 1 k ). v t+1 is calculated from v t according to v t+1 [i] = e i (x t+1 ) · max j {P i,j · v t [j]} (2)</formula><p>Definition 1 (Viterbi Step). We call the computation of v t+1 from v t a Viterbi step.</p><p>Clearly, each Viterbi step requires O(k 2 ) time. Therefore, the total runtime required to compute the vector v n is O(nk 2 ). The probability of the most likely sequence of states is the maximal element in v n . The actual sequence of states can be then reconstructed in linear time.</p><p>It is useful for our purposes to rewrite VA in a slightly different way. Let M σ be a k × k matrix with elements M σ i,j = e i (σ) · P i,j . We can now express v n as:  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref> can be used to reduce the k 3 term. However, for small values of k this is not profitable.</p><formula xml:id="formula_2">v n = M xn M xn−1 · · · M x2 M x1 v 0 (3) where (A B) i,j = max k {A i,k · B k,j } is</formula><p>The Forward-Backward Algorithms. The forward-backward algorithms are closely related to VA and are based on very similar dynamic programs. In contrast to VA, these algorithms apply standard matrix multiplication instead of max-times multiplication. The forward algorithm calculates f t [i], the probability to observe the sequence x 1 , x 2 , . . . , x t requiring that s t = i as follows:</p><formula xml:id="formula_3">f t = M xt · M xt−1 · · · · · M x2 · M x1 · f 0 (4)</formula><p>The backward algorithm calculates b t [i], the probability to observe the sequence x t+1 , x t+2 , . . . , x n given that s t = i as follows:</p><formula xml:id="formula_4">b t = b n · M x n · M x n−1 · · · · · M x t+2 · M x t+1 (5)</formula><p>Another algorithm which is used in the training stage and employs the forwardbackward algorithm as a subroutine, is the Baum-Welch algorithm, to be further discussed in Section 6.</p><p>A motivating example. We briefly describe one concrete example from computational biology to which our algorithms naturally apply. CpG islands <ref type="bibr" target="#b4">[5]</ref> are regions of DNA with a large concentration of the nucleotide pair CG. These regions are typically a few hundred to a few thousand nucleotides long, located around the promoters of many genes. As such, they are useful landmarks for the identification of genes. The observed sequence (X) is a long DNA sequence composed of four possible nucleotides (Σ = {A, C, G, T }). The length of this sequence is typically a few millions nucleotides (n 2 25 ). A well-studied classification problem is that of parsing a given DNA sequence into CpG islands and non CpG regions. Previous work on CpG island classification used Markov models with either 8 or 2 states (k = 8 or k = 2) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Exploiting Repeated Substrings in the Decoding Stage</head><p>Consider a substring W = w 1 , w 2 , . . . , w of X, and define</p><formula xml:id="formula_5">M (W ) = M w M w −1 · · · M w 2 M w 1 (6)</formula><p>Intuitively, M i,j (W ) is the probability of the most likely path starting with state j, making a transition into some other state, emitting w 1 , then making a transition into yet another state and emitting w 2 and so on until making a final transition into state i and emitting w . In the core of our method stands the following observation, which is immediate from the associative nature of matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 1. We may replace any occurrence of</head><formula xml:id="formula_6">M w M w −1 · · · M w1 in eq. (3) with M (W ).</formula><p>The application of observation 1 to the computation of equation <ref type="formula">(3)</ref> saves − 1 Viterbi steps each time W appears in X, but incurs the additional cost of computing M (W ) once.</p><p>An intuitive exercise. Let λ denote the number of times a given word W appears, in non-overlapping occurrences, in the input string X. Suppose we na¨ıvelyna¨ıvely compute M (W ) using (|W | − 1) max-times matrix multiplications, and then apply observation 1 to all occurrences of W before running VA. We gain some speedup in doing so if</p><formula xml:id="formula_7">(|W | − 1)k 3 + λk 2 &lt; λ|W |k 2 λ &gt; k<label>(7)</label></formula><p>Hence, if there are at least k non-overlapping occurrences of W in the input sequence, then it is worthwhile to na¨ıvelyna¨ıvely precompute M (W ), regardless of it's size |W |.</p><p>Definition 2 (Good Substring). We call a substring W good if we decide to compute M (W ).</p><p>We can now give a general four-step framework of our method:</p><formula xml:id="formula_8">(I) Dictionary Selection: choose the set D = {W i } of good substrings. (II) Encoding: precompute the matrices M (W i ) for every W i ∈ D. (III) Parsing: partition the input sequence X into consecutive good sub- strings X = W i 1 W i 2 · · · W i n and let X denote the compressed repre- sentation of this parsing of X, such that X = i 1 i 2 · · · i n . (IV) Propagation: run VA on X , using the matrices M (W i ).</formula><p>The above framework introduces the challenge of how to select the set of good substrings (step I) and how to efficiently compute their matrices (step II). In the next two sections we show how the RLE and LZ78 compression schemes can be applied to address this challenge. The utilization of the BPE compression scheme is discussed in the full version of this paper. Another challenge is how to parse the sequence X (step III) in order to maximize acceleration. We show that, surprisingly, this optimal parsing may differ from the initial parsing induced by the selected compression scheme. To our knowledge, this feature was not applied by previous "acceleration by compression" algorithms.</p><p>Throughout this paper we focus on computing path probabilities rather than the paths themselves. The actual paths can be reconstructed in linear time as described in the full version of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Acceleration via Run-length Encoding</head><p>In this section we obtain an Ω( r logr ) speedup for decoding an observed sequence with run-length compression ratio r. A string S is run-length encoded if it is described as an ordered sequence of pairs (σ, i), often denoted "σ i ". Each pair corresponds to a run in S, consisting of i consecutive occurrences of the character σ. For example, the string aaabbcccccc is encoded as a 3 b 2 c 6 . Run-length encoding serves as a popular image compression technique, since many classes of images (e.g., binary images in facsimile transmission or for use in optical character recognition) typically contain large patches of identically-valued pixels. The fourstep framework described in section 3 is applied as follows.</p><p>(I) Dictionary Selection: for every σ ∈ Σ and every i = 1, 2, . . . , log n we choose σ 2 i as a good substring.</p><formula xml:id="formula_9">(II) Encoding: since M (σ 2 i ) = M (σ 2 i−1 ) M (σ 2 i−1 )</formula><p>, we can compute the matrices using repeated squaring. (III) Parsing: Let W 1 W 2 · · · W n be the RLE of X, where each W i is a run of some σ ∈ Σ. X is obtained by further parsing each W i into at most log |W i | good substrings of the form σ 2 j . (IV) Propagation: run VA on X , as described in Section 3.</p><p>Time and Space Complexity Analysis. The offline preprocessing stage consists of steps I and II. The time complexity of step II is O(|Σ|k 3 log n) by applying maxtimes repeated squaring in O(k 3 ) time per multiplication. The space complexity is O(|Σ|k 2 log n). This work is done offline once, during the training stage, in advance for all sequences to come. Furthermore, for typical applications, the O(|Σ|k 3 log n) term is much smaller than the O(nk 2 ) term of VA.</p><p>Steps III and IV both apply one operation per occurrence of a good substring in X : step III computes, in constant time, the index of the next parsing-comma, and step IV applies a single Viterbi step in k 2 time.</p><formula xml:id="formula_10">Since |X | = n i=1 log|W i |, the complexity is n i=1 k 2 log|W i | = k 2 log(|W 1 | · |W 2 | · · · |W n |) ≤ k 2 log((n/n ) n ) = O(n k 2 log n n ).</formula><p>Thus, the speedup compared to the O(nk 2 ) time of VA is Ω( n n log n n ) = Ω( r logr ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acceleration via LZ78 Parsing</head><p>In this section we obtain an Ω( log n k ) speedup for decoding, and a constant speedup in the case where k &gt; log n. We show how to use the LZ78 <ref type="bibr" target="#b21">[22]</ref> (henceforth LZ) parsing to find good substrings and how to use the incremental nature of the LZ parse to compute M (W ) for a good substring W in O(k 3 ) time.</p><p>LZ parses the string X into substrings (LZ-words) in a single pass over X. Each LZ-word is composed of the longest LZ-word previously seen plus a single letter. More formally, LZ begins with an empty dictionary and parses according to the following rule: when parsing location i, look for the longest LZ-word W starting at position i which already appears in the dictionary. Read one more letter σ and insert W σ into the dictionary. Continue parsing from position i + |W | + 1. For example, the string "AACGACG" is parsed into four words: A, AC, G, ACG. Asymptotically, LZ parses a string of length n into O(hn/ log n) words <ref type="bibr" target="#b21">[22]</ref>, where 0 ≤ h ≤ 1 is the entropy of the string. The LZ parse is performed in linear time by maintaining the dictionary in a trie. Each node in the trie corresponds to an LZ-word. The four-step framework described in section 3 is applied as follows. Time and Space Complexity Analysis. Steps I and III were already conducted offline during the pre-processing compression of the input sequences (in any case LZ parsing is linear). In step II, computing M (W σ) = M (W ) M σ , takes O(k 3 ) time since M (W ) was already computed for the good substring W . Since there are O(n/ log n) LZ-words, calculating the matrices M (W ) for all W s takes O(k 3 n/ log n). Running VA on X (step IV) takes just O(k 2 n/ log n) time. Therefore, the overall runtime is dominated by O(k 3 n/ log n). The space complexity is O(k 2 n/ log n).</p><p>The above algorithm is useful in many applications, such as CpG island classification, where k &lt; log n. However, in those applications where k &gt; log n such an algorithm may actually slow down VA.</p><p>We next show an adaptive variant that is guaranteed to speed up VA, regardless of the values of n and k. This graceful degradation retains the asymptotic Ω( log n k ) acceleration when k &lt; log n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">An improved algorithm</head><p>Recall that given M (W ) for a good substring W , it takes k 3 time to calculate M (W σ). This calculation saves k 2 operations each time W σ occurs in X in comparison to the situation where only M (W ) is computed. Therefore, in step I we should include in D, as good substrings, only words that appear as a prefix of at least k LZ-words. Finding these words can be done in a single traversal of the trie. The following observation is immediate from the prefix monotonicity of occurrence tries.</p><p>Observation 2. Words that appear as a prefix of at least k LZ-words are represented by trie nodes whose subtrees contain at least k nodes.</p><p>In the previous case it was straightforward to transform X into X , since each phrase p in the parsed sequence corresponded to a good substring. Now, however, X does not divide into just good substrings and it is unclear what is the optimal way to construct X (in step III). Our approach for constructing X is to first parse X into all LZ-words and then apply the following greedy parsing to each LZ-word W : using the trie, find the longest good substring w ∈ D that is a prefix of W , place a parsing comma immediately after w and repeat the process for the remainder of W .</p><p>Time and Space Complexity Analysis. The improved algorithm utilizes substrings that guarantee acceleration (with respect to VA) so it is therefore faster than VA even when k = Ω(log n). In addition, in spite of the fact that this algorithm re-parses the original LZ partition, the algorithm still guarantees an Ω( log n k ) speedup over VA as shown by the following lemma. Lemma 1. The running time of the above algorithm is bounded by O(k 3 n/log n).</p><p>Proof. The running time of step II is at most O(k 3 n/ log n). This is because the size of the entire LZ-trie is O(n/ log n) and we construct the matrices, in O(k 3 ) time each, for just a subset of the trie nodes. The running time of step IV depends on the number of new phrases (commas) that result from the re-parsing of each LZ-word W . We next prove that this number is at most k for each word.</p><p>Consider the first iteration of the greedy procedure on some LZ-word W . Let w be the longest prefix of W that is represented by a trie node with at least k descendants. Assume, contrary to fact, that |W | − |w | &gt; k. This means that w , the child of w , satisfies |W | − |w | ≥ k, in contradiction to the definition of w . We have established that |W | − |w | ≤ k and therefore the number of re-parsed words is bounded by k + 1. The propagation step IV thus takes O(k 3 ) time for each one of the O(n/ log n) LZ-words. So the total time complexity remains O(k 3 n/ log n).</p><p>Based on Lemma 1, and assuming that steps I and III are pre-computed offline, the running time of the above algorithm is O(nk 2 /e) where e = Ω(max(1, log n k )). The space complexity is O(k 2 n/logn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Training Problem</head><p>In the training problem we are given as input the number of states in the HMM and an observed training sequence X. The aim is to find a set of model parameters θ (i.e., the emission and transition probabilities) that maximize the likelihood to observe the given sequence P (X| θ). The most commonly used training algorithms for HMMs are based on the concept of Expectation Maximization. This is an iterative process in which each iteration is composed of two steps. The first step solves the decoding problem given the current model parameters. The second step uses the results of the decoding process to update the model parameters. These iterative processes are guaranteed to converge to a local maximum. It is important to note that since the dictionary selection step (I) and the parsing step (III) of our algorithm are independent of the model parameters, we only need run them once, and repeat just the encoding step (II) and the propagation step (IV) when the decoding process is performed in each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Viterbi training</head><p>The first step of Viterbi training <ref type="bibr" target="#b11">[12]</ref> uses VA to find the most likely sequence of states given the current set of parameters (i.e., decoding). Let A ij denote the number of times the state i follows the state j in the most likely sequence of states. Similarly, let E i (σ) denote the number of times the letter σ is emitted by the state i in the most likely sequence. The updated parameters are given by:</p><formula xml:id="formula_11">P ij = A ij i A i j and e i (σ) = E i (σ) σ E i (σ ) (8)</formula><p>Note that the Viterbi training algorithm does not converge to the set of parameters that maximizes the likelihood to observe the given sequence P (X| θ) , but rather the set of parameters that locally maximizes the contribution to the likelihood from the most probable sequence of states <ref type="bibr" target="#b11">[12]</ref>. It is easy to see that the time complexity of each Viterbi training iteration is O(k 2 n + n) = O(k 2 n) so it is dominated by the running time of VA. Therefore, we can immediately apply our compressed decoding algorithms from sections 4 and 5 to obtain a better running time per iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baum-Welch training</head><p>The Baum-Welch training algorithm <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> converges to a set of parameters that locally maximize the likelihood to observe the given sequence P (X| θ), and is the most commonly used method for model training. We give here a brief explanation of the algorithm and of our acceleration approach. The complete details appear in the full version of this paper.</p><p>Recall the forward-backward matrices: f t [i] is the probability to observe the sequence x 1 , x 2 , . . . , x t requiring that the t'th state is i and that b t <ref type="bibr">[i]</ref> is the probability to observe the sequence x t+1 , x t+2 , . . . , x n given that the t'th state is i. The first step of Baum-Welch calculates f t [i] and b t [i] for every 1 ≤ t ≤ n and every 1 ≤ i ≤ k. This is achieved by applying the forward and backward algorithms to the input data in O(nk 2 ) time (see eqs. <ref type="formula">(4)</ref> and <ref type="formula">(5)</ref>). The second step recalculates A and E according to</p><formula xml:id="formula_12">A i,j = t P (s t = j, s t+1 = i|X, θ) E i (σ) = t|x t =σ P (s t = i|X, θ)<label>(9)</label></formula><p>where P (s t = j, s t+1 = i|X, θ) is the probability that a transition from state j to state i occurred in position t in the sequence X, and P (s t = i|X, θ) is the probability for the t'th state to be i in the sequence X. These quantities are given by:</p><formula xml:id="formula_13">P (s t = j, s t+1 = i|X, θ) = f t [j] · P i,j · e i (x t+1 ) · b t+1 [i] i f n [i]<label>(10)</label></formula><p>and</p><formula xml:id="formula_14">P (s t = i|X, θ) = f t [i] · b t [i] i f n [i] .<label>(11)</label></formula><p>Finally, after the matrices A and E are recalculated, Baum-Welch updates the model parameters according to equation <ref type="formula">(8)</ref>.</p><p>We next describe how to accelerate the Baum-Welch algorithm. Note that in the first step of Baum-Welch, our algorithms to accelerate VA (Sections 4 and 5) can be used to accelerate the forward-backward algorithms by replacing the max-times matrix multiplication with regular matrix multiplication. However, the accelerated algorithms only compute f t and b t on the boundaries of good substrings. In order to solve this problem and speed up the second step of Baum-Welch as well, we observe that when accumulating the contribution of some appearance of a good substring W of length |W | = to A, Baum-Welch performs O(k 2 ) operations, but updates at most k 2 entries (the size of A). Hence, it is possible to obtain a speedup by precalculating the contribution of each good substring to A and E. For brevity the details are omitted here and will appear in the full version of this paper. To summarize the results, preprocessing a good substring W requires O(k 4 ) time and O(k 4 ) space. Using the preprocessed information and the values of f t and b t on the boundaries of good substrings, we can update A and E in O(k 4 ) time per good substring (instead of k 2 ). To get a speedup we need λ, the number of times the good substring W appears in X to satisfy:</p><formula xml:id="formula_15">k 4 + λk 4 &lt; λλk 2 λ &gt; k 2 − k 2<label>(12)</label></formula><p>This is reasonable if k is small. If = 2k 2 , for example, then we need λ to be greater than 2k 2 . In the CpG islands problem, if k = 2 then any substrings of length eight is good if it appears more than eight times in the text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>We implemented both a variant of our improved LZ-compressed algorithm from subsection 5.1 and classical VA in C++ and compared their execution times on a sequence of approximately 22,000,000 nucleotides from the human Y chromosome and on a sequence of approximately 1,500,000 nucleotides from chromosome 4 of S. Cerevisiae obtained from the UCSC genome database. The benchmarks were performed on a single processor of a SunFire V880 server with 8 UltraSPARC-IV processors and 16GB main memory. The implementation is just for calculating the probability of the most likely sequence of states, and does not traceback the optimal sequence itself. As we have seen, this is the time consuming part of the algorithm. We measured the running times for different values of k. As we explained in the previous sections we are only interested in the running time of the encoding and the propagation steps (II and IV) since the combined parsing/dictionary-selections steps (I and III) may be performed in advance and are not repeated by the training and decoding algorithms. The results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Our algorithm performs faster than VA even for surprisingly large values of k. For example, for k = 60 our algorithm is roughly three times faster than VA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. The HMM on the observed sequence X = x 1 , x 2 , . . . , x n and states 1, 2, . . . , k. The highlighted path is a possible path of states that generate the observed sequence. VA finds the path with highest probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>I) Dictionary Selection: the good substrings are all the LZ-words in the LZ-parse of X. (II) Encoding: construct the matrices incrementally, according to their or- der in the LZ-trie, M (W σ) = M (W ) M σ . (III) Parsing: X is the LZ-parsing of X. (IV) Propagation: run VA on X , as described in section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of the cumulative running time of steps II and IV of our algorithm (marked x) with the running time of VA (marked o), for different values of k. Time is shown in arbitrary units on a logarithmic scale. Runs on the 1.5Mbp chromosome 4 of S. cerevisiae are in solid lines. Runs on the 22Mbp human Y-chromosome are in dotted lines. The roughly uniform difference between corresponding pairs of curves reflects a speedup factor of more than five.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Let sleeping files lie: Pattern matching in Z-compressed files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comp. and Sys. Sciences</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="307" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HMM based optical character recognition in the presence of deterministic transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Agazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1813" to="1826" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching for run length encoded strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Apostolico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inequalities</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cpg-rich islands as gene markers in the vertebrate nucleus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Genetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="342" to="347" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithmic aspects in speech recognition: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Buchsbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giancarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal of Experimental Algorithms</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An improved algorithm for computing the edit distance of run length coded strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Csirik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="93" to="96" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">All-pairs shortest paths with real weights in O(n 3 /logn) time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Workshop on Algorithms and Data Structures</title>
		<meeting>9th Workshop on Algorithms and Data Structures</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="318" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hidden Markov chains and the analysis of genome structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Churchill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers Chem</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix multiplication via arithmetical progressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="251" to="280" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A sub-quadratic sequence alignment algorithm for unrestricted cost matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crochemore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ziv-Ukelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Annual ACMSIAM Symposium on Discrete Algorithms</title>
		<meeting>13th Annual ACMSIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Biological Sequence Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mitcheson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate string matching over Ziv-Lempel compressed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karkkainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Annual Symposium On Combinatorial Pattern Matching (CPM)</title>
		<meeting>11th Annual Symposium On Combinatorial Pattern Matching (CPM)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1848</biblScope>
			<biblScope unit="page" from="195" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lempel-Ziv parsing and sublinear-size index structures for string matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karkkainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third South American Workshop on String Processing (WSP)</title>
		<meeting>Third South American Workshop on String essing (WSP)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="141" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximate matching of run-length compressed strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Makinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Annual Symposium On Combinatorial Pattern Matching (CPM)</title>
		<meeting>12th Annual Symposium On Combinatorial Pattern Matching (CPM)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1645</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A text compression scheme that allows fast searching directly in the compressed file</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Annual Symposium On Combinatorial Pattern Matching (CPM)</title>
		<meeting>5th Annual Symposium On Combinatorial Pattern Matching (CPM)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2089</biblScope>
			<biblScope unit="page" from="31" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster approximate string matching over compressed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conference (DCC)</title>
		<meeting>Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speeding up pattern matching by text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1767</biblScope>
			<biblScope unit="page" from="306" to="315" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian elimination is not optimal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimal decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory, IT</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the complexity of finite sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="81" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
