<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Kernel Regression with Order Preferences *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin</orgName>
								<address>
									<postCode>53706</postCode>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Kernel Regression with Order Preferences *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel kernel regression algorithm which takes into account order preferences on unlabeled data. Such preferences have the form that point x1 has a larger target value than that of x2, although the target values for x1, x2 are unknown. The order preferences can be viewed as side information or a form of weak labels, and our algorithm can be related to semi-supervised learning. Learning consists of formulating the order preferences as additional regularization in a risk minimization framework. We define a linear program to effectively solve the optimization problem. Experiments on benchmark datasets, sentiment analysis , and housing price problems show that the proposed algorithm outperforms standard regression, even when the order preferences are noisy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We propose a novel algorithm for kernel regression. The proposed regression algorithm is able to incorporate domain knowledge about the relative order of target values on unlabeled examples. As a motivating example, consider the task of predicting real estate prices. The price of a house varies significantly depending on its location and many other factors. However, a domain expert may determine that everything being "roughly equal," the feature number of bedrooms determines the order of house prices. For instance, a 4-bedroom house is more expensive than a 3-bedroom one.</p><p>At first glance, it may appear that such knowledge can be enforced by a positive correlation between the feature and the target. However, modeling such knowledge as positive correlation can be difficult in non-linear kernel regression, because of the non-linear feature mapping. Besides, in general a correlation may only hold for part of the range of the feature value, and it would be inappropriate to force the correlation across the range. We would like a more general approach to capture such knowledge.</p><p>We propose to encode such domain knowledge with order preferences on unlabeled examples. That is, for all pairs of unlabeled examples x i , x j satisfying the "roughly equal" condition, such domain knowledge specifies the order between their target values f (x i ) and f (x j ), even though their actual target values are unknown. Respecting the domain knowledge then amounts to incorporating the order preferences into a kernel regression framework. When labeled data is scarce, these order preferences should improve our regression model.</p><p>Another practical application of our approach is in predicting Internet file transfer rates based on network properties like round trip time, available bandwidth, queuing delay, package loss rate, and so on ( <ref type="bibr" target="#b15">Mizra et al. 2007</ref>). The features have intuitive impact on transfer rate, but the exact relation is highly non-linear and unknown. We can, however, easily create order preferences on unlabeled data using domain knowledge. In general, order preferences can encode certain complex domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression with Order Preferences</head><p>Let us formally define our regression problem. In addition to a labeled training set {(x 1 , y 1 ), . . . , (x l , y l )}, we assume that we are given p order preferences between pairs of unlabeled examples. An order preference is defined by a tuple <ref type="bibr">(i, j, d, w)</ref>, with the interpretation that we would like f (x i ) − f (x j ) ≥ d. As discussed below, we encode it as a soft preference rather than a hard constraint. The scalar w ≥ 0 is the weight (confidence) for the preference. Obviously knowing the order preferences is much weaker than knowing the labels of the unlabeled examples. In this sense the preferences are a form of weakly labeled data or side information. We would like to use them to improve regression.</p><p>It is possible to represent the order preferences as directed edges in a graph <ref type="bibr" target="#b7">(Dekel, Manning, &amp; Singer 2004)</ref>, where the edges represent asymmetric order information. However, it is worth noting that order preferences can also encode similarity. For example, the two preferences (i, j, 0, w), (j, i, 0, w) encode f (x i ) = f (x j ). More generally, the two preferences (i, j, −ǫ, w), (j, i, −ǫ, w) encode closeness:</p><formula xml:id="formula_0">|f (x i ) − f (x j )| ≤ ǫ. It is also easy to encode a ≤ f (x i ) − f (x j ) ≤ b.</formula><p>As special cases of order preference, one can also encode unary preferences f (</p><formula xml:id="formula_1">x i ) ≤ g(x i ), f (x i ) = g(x i ), or f (x i ) ≥ g(x i ),</formula><p>where g is some given function. The unary preferences are closely related to the work of <ref type="bibr" target="#b13">Mangasarian et al. (2004)</ref>, which adds them to kernel machines.</p><p>Our approach to add order preferences to kernel regression is to treat them as regularization. Recall the standard risk minimization framework for kernel regression is</p><formula xml:id="formula_2">min f ∈H l i=1 c(x i , y i , f (x i )) + λΩ(f H ), (1)</formula><p>where H is the Reproducing Kernel Hilbert Space (RKHS) induced by some kernel, c() is a loss function for regression, λ is a weight parameter on the regularizer, and Ω() is a monotonic increasing function.</p><p>With the order preferences we now define an additional regularization term r(x, f ). Intuitively if the function f satisfies all order preferences, r should be zero; if f violates some, r increases. A natural choice is to use a shifted hinge function: for order preference (i, j, d, w), the regularization term for this single preference is</p><formula xml:id="formula_3">w max(d − (f (x i ) − f (x j )), 0)</formula><p>. That is, it is zero if the preference is satisfied; otherwise it is the amount the preference is violated, weighted by w. As a side note, we point out that if we have two preferences (i, j, −ǫ, w), (j, i, −ǫ, w), this would form the ǫ-insensitive loss <ref type="bibr" target="#b19">(Smola &amp; Schölkopf 2004</ref>).</p><p>We define the regularization term r(x, f ) as the sum of shifted hinge function on all order preferences:</p><formula xml:id="formula_4">r(x, f ) = p q=1 w q max(d q − (f (x iq ) − f (x jq )), 0). (2)</formula><p>We note that order preferences have been used in ranking problems <ref type="bibr" target="#b8">(Herbrich, Obermayer, &amp; Graepel 2000;</ref><ref type="bibr">Burges et al. 2005;</ref><ref type="bibr" target="#b20">Yu et al. 2006</ref>; Chu &amp; Ghahramani 2005); in particular (Joachims 2002) employed a similar shifted hinge function for ranking. However they have not been used in regression before. Our problem is</p><formula xml:id="formula_5">min f ∈H l i=1 c(x i , y i , f (x i )) + λ 1 Ω(f H ) + λ 2 r(x, f ).(3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Linear Program Formulation</head><p>To fully specify the above problem, we choose to use the ǫ-insensitive loss c(x, y, f ) = |y − f | ǫ in support vector regression:</p><formula xml:id="formula_6">|y − f | ǫ = 0 if |y − f | ≤ ǫ |y − f | − ǫ otherwise.<label>(4)</label></formula><p>We further choose Ω(f H ) to be a linear function, in this case the 1-norm of the dual parameters discussed below, resulting in 1-norm support vector machines <ref type="bibr">(Bradley &amp; Man- gasarian 1998;</ref><ref type="bibr" target="#b1">Bi et al. 2003;</ref><ref type="bibr" target="#b21">Zhu et al. 2004</ref>). The formulation originates from generalized support vector machines <ref type="bibr" target="#b14">(Mangasarian 2000)</ref>. Such 1-norm support vector machines are comparable in performance to the standard 2-norm support vector machines, but with the advantage that they can be solved as linear programs, which tends to be more efficient.</p><p>The solution can be characterized by a representer theorem <ref type="bibr" target="#b12">(Kimeldorf &amp; Wahba 1971;</ref><ref type="bibr" target="#b17">Schölkopf, Herbrich, &amp; Smola 2001)</ref>: The minimizer f * ∈ H admits the form</p><formula xml:id="formula_7">f * (x) = l+2p i=1 α i K(x i , x)</formula><p>, where x i ranges from the labeled examples to the unlabeled examples involved in the p order preferences. The proof uses the standard orthogonality argument, and is omitted for space consideration.</p><p>Let K(x, x 1:l ) denote the row vector of kernel values between a point x and the labeled data x 1:l . We represent our function f in dual form by</p><formula xml:id="formula_8">f (x) = K(x, x l:l )α + α 0 (5)</formula><p>where α is a column vector of dual parameters, one for each labeled point; α 0 is a bias scalar. This amounts to approximating the representer theorem by setting dual parameters not on the labeled data to zero for a sparse representation. Our linear-program regression problem is</p><formula xml:id="formula_9">min α,α0 1 l l i=1 |y i − f (x i )| ǫ + λ 1 α 1 + λ 2 1 p p q=1 w q max(d q − (f (x iq ) − f (x jq )), 0), (6)</formula><p>where</p><formula xml:id="formula_10">α 1 = l i=1 |α i | is the 1-norm of α. The bias α 0 is not regularized.</formula><p>We transform (6) into a standard linear program by introducing auxiliary variables for the three terms respectively. Let 1 be the all-one vector, ξ an l-vector of slack variables, η an l-vector, ν a p-vector, d the difference vector, w the weight vector, K(x i 1:p , x 1:l ) the p × l kernel matrix between the first points in the order constraints and the labeled data, and K(x j 1:p , x 1:l ) the same sized kernel matrix between the second points in the order constraints and the labeled data. Vector inequalities are element-wise. With standard transform techniques, our linear program for kernel regression with order preferences can be written as:</p><formula xml:id="formula_11">min α,α0,ξ,η,ν 1 l 1 ⊤ ξ + λ 1 1 ⊤ η + λ2 p w ⊤ ν s.t. − ξ − ǫ1 ≤ y 1:l − K(x 1:l , x 1:l )α − α 0 1 ≤ ξ + ǫ1 ξ ≥ 0 − η ≤ α ≤ η (K(x i 1:p , x 1:l ) − K(x j 1:p , x 1:l ))α ≥ d − ν ν ≥ 0.</formula><p>(7) This is a linear program with 3l + p + 1 variables and 5l + 2p constraints. The global optimal solution can be found efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections to Semi-Supervised Learning</head><p>It is instructive to note that several semi-supervised learning approaches can be expressed in a similar form as (3). In particular, manifold regularization <ref type="bibr">(Belkin, Niyogi, &amp; Sind- hwani 2004</ref>) uses</p><formula xml:id="formula_12">r(x, f ) = i,j∈U w ij (f (x i ) − f (x j )) 2 ,</formula><p>where U is the unlabeled data, and w ij represents the similarity between x i , x j based on domain knowledge. <ref type="bibr">S3VMs (Collobert et al. 2006</ref>; Joachims 1999b) uses</p><formula xml:id="formula_13">r(x, f ) = i∈U max(1 − |f (x i )|, 0)</formula><p>to attempt to push unlabeled examples out of the margin. Co-train style multiview learning <ref type="bibr" target="#b3">(Brefeld et al. 2006;</ref><ref type="bibr" target="#b18">Sindhwani, Niyogi, &amp; Belkin 2005</ref>) uses</p><formula xml:id="formula_14">r(x, f ) = M u,v=1 i∈U (f u (x i ) − f v (x i )) 2</formula><p>for the M different views, to encourage them to make the same prediction on the same unlabeled example. These methods and our order preferences all encode some domain knowledge other than labels. One might establish many order preferences on unlabeled data. For example, higher bandwidth, shorter delay and fewer package loss leads to higher file transfer rates. They can all be viewed as unlabeled-data-dependent regularizers r(x, f ). Our order preferences may contain slightly stronger information, and we view them as filling in the continuum between supervised learning and semi-supervised learning. It is possible to combine order preferences with existing semi-supervised learning methods by adding the respective r(x, f ) terms together (with appropriate weights) to form a new regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We demonstrate the benefit of order preferences with four groups of experiments. We implemented our linear program (7) using CPLEX. All experiments ran quickly. Solving the LP for each trial takes 0.2 to 0.5 seconds depending on the number of order preferences and unlabeled data size. In all experiments, ǫ in the ǫ-insensitive loss (4) was set to 0, and preference weights w were set to 1. We use the acronym SSL for <ref type="formula">(7)</ref>, and SVR for the corresponding standard 1-norm support vector regression (i.e., λ 2 = 0). We also experimented with standard 2-norm support vector regression using SVM light (Joachims 1999a), and the results were comparable to SVR and not reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Toy Example</head><p>First we use a toy example to illustrate order preferences. We constructed a polynomial function of degree 3 as our target (the dotted line in <ref type="figure" target="#fig_0">Figure 1(a)</ref>). We randomly sampled three points (the open circles) from the target function as training data and gave them to SVR. For this experiment we set λ 1 = 0. Since there were not enough training data points, SVR produced a fit (the dashed line) through the training points but very different from the target.</p><p>We then randomly selected a pair of unlabeled points −0.15, 0.30. Note they did not coincide with the training points. Without revealing the actual target values at these points, we constructed an order preference using their true order: (0.30, −0.15, 0, 1), or equivalently f (0.30) − f (−0.15) ≥ 0. Note we set d = 0 so that the order preference specified their order but not the true difference; hence it was weaker. We set w = 1. In <ref type="figure" target="#fig_0">Figure 1</ref>(a) the order preference is shown at the lower left as a line linking the two unlabeled points (black dots). The point with the larger value has a larger dot. SVR happened to violate the order preference. With the three training points and this order preference, SSL produced a better fit (the solid line).</p><p>In <ref type="figure" target="#fig_0">Figure 1(b)</ref> we added more order preferences, generated similarly from random unlabeled point pairs and their true order. Note some preferences were already satisfied by SVR. The SSL function was further improved. We consistently observed such behavior in repeated random trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark Datasets</head><p>We experimented with five regression benchmark datasets (Boston, Abalone, Computer, California, Census; Available at http://www.niaad.liacc.up.pt/ ∼ ltorgo/ Regression/DataSets.html), and report results on all of them. One difficulty in working with such standard datasets is creating sensible order preferences on unlabeled data. Ideally the order preferences would be prepared by experts with domain knowledge on the tasks. Lacking such experts, we had to create simulated order preferences from the relation of true values on unlabeled points (more details later; Note, however, we never give out the true values themselves). Therefore our results on benchmark datasets should be viewed as "oracle experiments." Nonetheless they are useful indications of how well our regression would perform given such domain knowledge.</p><p>For each benchmark dataset, we normalized its input features to zero mean, unit variance. For categorical features with k distinct values, we mapped them into indicator vectors of length k. We used Radial Basis Function (RBF) kernels k(x, x ′ ) = exp(−σx − x ′ 2 ) for all datasets. We used 5-fold cross validation to find the optimal RBF bandwidth σ, and SVR 1-norm weight λ 1 . The parameters were tuned for SVR on a 9 × 9 logarithmic grid in 10 −4 ≤ σ ≤ 10 4 and 10 −4 ≤ λ 1 ≤ 10 4 . We simply fixed λ 2 at 1. This is partly justified by the fact that in (6), the 'shifted hinge function' is on a similar scale to the ǫ-insensitive loss; both incur a linear penalty when violated. Tuning λ 2 might produce better results than reported here, but with limited labeled data (which has been used to tune λ 1 and σ for SVR already) it is hard to do.</p><p>All experiments were repeated for 20 random trials. Different algorithms shared the same random trials so we could perform paired statistical tests. In each trial we split the data into three parts: l labeled points, u unlabeled points that were used to generate order preferences, and test points that were the rest of the dataset (see <ref type="table" target="#tab_0">Table 1</ref> Partition). Test points were unseen by either algorithm during training. All results we report are test-set mean-absolute-error over the 20 trials. Let t be the test set size. Test-set mean-absoluteerror is defined as i∈test |y i − f (x i )|/t. We address the following questions:</p><p>Can order preferences improve regression? We randomly sampled with replacement p = 1000 pairs (x i , x j ) from the u unlabeled points. For each sampled pair, we generated an order preference from the true target values y i , y j . Without loss of generality let y i ≥ y j . Our simulated order preference was</p><formula xml:id="formula_15">f (x i ) − f (x j ) ≥ 0.5(y i − y j ).<label>(8)</label></formula><p>Let us explain our order preferences. We could have created the 'perfect' order preferences with the pair: encode f (x i ) − f (x j ) = y i − y j . But in real tasks it might be difficult to know the exact difference y i − y j , so we did not do that. On the other hand, with inequality preferences we could have set f (x i ) − f (x j ) ≥ 0. It would only encode order, without any information on the actual difference. But in real tasks one might have some rough estimate of the difference, and (8) was meant to simulate this estimate. <ref type="table" target="#tab_0">Table 1</ref> compares the test-set mean-absolute-error of SVR and SSL. The differences on all datasets are significant with a paired t-test at the 0.01 level. We conclude that, with the order preferences (8), SSL significantly improves regression performance over SVR.</p><formula xml:id="formula_16">f (x i ) − f (x j ) ≥ y i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What if we change the number of order preferences p?</head><p>One expects a larger gain with more order preferences p. We systematically varied p from 10 to 5000, keeping everything else the same as in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure" target="#fig_4">Figure 2(a)</ref> shows that it was indeed the case. A very small p sometimes hurts SSL, making it worse than SVR. But as p grows larger SSL rapidly improves, and levels off at around p = 100. This indicates that one needs only a moderate amount of order preferences to enjoy the benefit.</p><p>What if we change the labeled data size l? The benefit of order preferences is expected to diminish with more labeled data. We fixed the number of order preferences p = 1000, and systematically varied l. As expected, <ref type="figure" target="#fig_4">Fig- ure 2(b)</ref> shows that SSL is most useful when l is small, and the benefit reduces as l grows.</p><p>How precise do the order preferences need to be? <ref type="bibr">Ex- tending (8)</ref>, one can define order preferences as f (x i ) − f (x j ) ≥ β(y i − y j ) where β controls how precise they are. As mentioned earlier, β = 0 only supplies order information, and a larger β estimates the differences. We varied β from 0 to 2 (over-estimate) for the experiments in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure" target="#fig_4">Figure 2(c)</ref> shows that with only the order (β = 0) SSL already outperformed SVR. With a conservative estimate of the differences (0 &lt; β &lt; 1) SSL was even better. However larger β seems to be inferior. This might be advantageous in practice, since one does not need to know the precise differences, and can err on the safe side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis in Movie Reviews</head><p>We next experimented with the real-world problem of sentiment analysis in movie reviews. Given a movie review text document x, we would like to predict f (x), the rating (e.g., '4 stars') given to the movie by the reviewer. We assume that by looking at the wording of unlabeled reviews, one can determine that some movies will likely be rated higher than others (even though we do not know their actual ratings). These are incorporated as order preferences. We worked on the "scale dataset v1.0" with continuous ratings, available at http://www.cs.cornell.edu/ people/pabo/movie-review-data/ and first used in <ref type="bibr" target="#b16">(Pang &amp; Lee 2005)</ref>. It contains four authors with 1770, 902, 1307, 1027 reviews respectively. For each author, we varied l ∈ {30, 60, 120}, and let u = 500, p = 500. The remaining reviews were test examples. Each experiment was repeated for 20 random trials. All reported results are testset mean-absolute-error. Each review document was represented as a word-presence vector, normalized to sum to 1. We used a linear kernel, set λ 1 = 10 −7 and λ 2 = 1.</p><p>As a proxy for expert knowledge, we used a completely separate "snippet dataset" also located at the above URL. The snippet dataset is very different from the scale dataset: it contains single punch line sentences (snippets) instead of full reviews; the snippets have binary positive/negative labels instead of continuous ratings; it comes from different authors on different movies. We trained a standard binary, linear-kernel SVM classifier g on the snippet data using SVM light . We then applied g on random pairs of unlabeled movie reviews x i , x j in the scale dataset. The order of the continuous margin output g(x i ), g(x j ) serves as our proxy for expert knowledge <ref type="bibr">1</ref> . Since this is a very crude and noisy estimate, we created an order preference (i, j, 0, 1) only if g(x i ) − g(x j ) &gt; 0.25, where 0.25 is an arbitrary threshold. Note we set d = 0 since we do not know the      <ref type="table" target="#tab_1">Table 2</ref> presents the results of our sentiment analysis experiments. As expected, SSL is most useful when l is small, and the gain over SVR gradually diminishes with larger l. SSL leads to improvements in all cases, and the differences are significant (*) with paired t-tests at the 0.05 level in about half of the cases 2 . We expect better order preferences from advanced natural language processing (e.g., parsing) to bring larger improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting Housing Prices Using Heuristic Order Preferences</head><p>As a final real-world experiment, we played the role of real estate experts to carry out the scenario introduced in the beginning of the paper. We used the same California dataset in <ref type="table" target="#tab_0">Table 1</ref>, but this time with order preferences derived from <ref type="bibr">2</ref> As a sanity check, we also experimented with wrong order preferences by intentionally flipping all preferences (i, j, 0, 1) into (j, i, 0, 1). As expected, SSL with wrong orders became worse than SVR by 1% -13% for different authors at l = 120. domain knowledge instead of oracles. The task is to predict the median house value for 20640 groups of houses throughout the state. With other factors being roughly equal, we believe the value is largely determined by the number of bedrooms. We decided that two groups are "roughly equal" if they are located within 25 miles of each other (i.e., they are in the same community), their median house ages differ by at most 10 years, and they are inhabited by residents whose median income level differs by at most $1000. We repeated the experimental setup in the benchmark section, and for each random trial, we created approximately 1200 order preferences. Specifically, for all pairs of housing groups in the labeled and unlabeled data that satisfy the "roughly equal" criteria, we created a preference that the group with more bedrooms has a higher target value. We omitted preferences between two labeled groups, since they are either redundant or incorrect. We set w = 1 and d = 0, and used the same λ parameters as in the benchmark section. Note that the order preferences are created without any knowledge of the actual target values, and that the relations we constructed are highly non-linear. We found that the heuristic preferences led to a 6% reduction in test-set mean-absolute-error in SSL (54664 ± 2521) compared to SVR (58268 ± 4435). The difference is statistically significant with a paired t-test at the 0.01 level. This experiment demonstrates that order preferences with some noise can still be beneficial. In fact, a postexperimental analysis of the created order preferences revealed that only 70% were actually accurate (i.e., 30% of "roughly equal" housing group pairs do not have the predicted relation based on bedrooms). We expect our method to extend well to new tasks (e.g., predicting Internet file transfer rates) where large numbers of reasonably accurate order preferences can be generated automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We presented a novel kernel regression algorithm with order preferences, formulated as a linear program. We showed that even with noisy, heuristic order preferences, the regression performance is improved. Our algorithm can be easily extended beyond regression. For example, one future direction is to apply order preferences to ordinal classification <ref type="bibr" target="#b5">(Chu &amp; Keerthi 2005</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example comparing SVR and SSL, showing the benefit of order preferences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The effect of the number of order preferences p (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The effect of labeled data size l (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The effect of the difference scaling factor β (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of various parameters on SSL on the Benchmark data. y-axis is test-set mean-absolute-error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Benchmark data. All improvements are statistically significant.</head><label>1</label><figDesc></figDesc><table>Dataset 
Partition 
Mean absolute error 
Improvement 
dim 
l/u/test 
SVR 
SSL 
Boston 
13 
20/200/286 
4.780 ± 1.351 3.511 ± 0.376 
27% 
Abalone 
8 
30/1000/3147 
1.856 ± 0.180 1.685 ± 0.102 
9% 
Computer 
21 
30/1000/7162 
7.373 ± 3.445 5.364 ± 0.998 
27% 
California 
8 60/1000/19580 58268 ± 4435 52120 ± 1843 
11% 
Census 
16 60/1000/21724 24992 ± 1377 
23241 ± 901 
7% 

Boston 
Abalone 
Computer 
California 
Census 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Movie review sentiment analysis mean-absolute-error for each author.</head><label>2</label><figDesc></figDesc><table>Dataset 
l/u/test 
SVR 
SSL 
Improvement 

Author (a) 

30/500/1240 
0.1383 ± 0.0072 0.1362 ± 0.0028 
1.5% 
60/500/1210 
0.1323 ± 0.0042 0.1311 ± 0.0025 
0.9% 
120/500/1150 0.1224 ± 0.0042 0.1219 ± 0.0024 
0.4% 

Author (b) 

30/500/372 
0.1645 ± 0.0146 0.1540 ± 0.0046 
* 6.4% 
60/500/342 
0.1514 ± 0.0063 0.1496 ± 0.0046 
* 1.2% 
120/500/282 
0.1431 ± 0.0063 0.1416 ± 0.0062 
* 1.0% 

Author (c) 

30/500/777 
0.1405 ± 0.0163 0.1357 ± 0.0070 
3.4% 
60/500/747 
0.1268 ± 0.0072 0.1258 ± 0.0038 
0.8% 
120/500/687 
0.1150 ± 0.0048 0.1138 ± 0.0047 
1.0% 

Author (d) 

30/500/497 
0.1433 ± 0.0151 0.1350 ± 0.0052 
* 5.8% 
60/500/467 
0.1366 ± 0.0104 0.1293 ± 0.0037 
* 5.3% 
120/500/407 
0.1256 ± 0.0092 0.1226 ± 0.0038 
2.4% 

</table></figure>

			<note place="foot" n="1"> Our use of g simulates a layman (not an expert) reading two reviews and saying &quot;the author liked this one more than that one.&quot; This layman does not have enough experience to predict the actual star ratings, but is able to tell that one sounds more positive than the other.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Manifold regularization: A geometric framework for learning from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<idno>TR-2004-06</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Chicago</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dimensionality reduction via sparse support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Embrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Breneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1229" to="1243" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Feature selection via concave minimization and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient co-regularized least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; C</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML06. Burges</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>ICML05</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian processes for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1019" to="1041" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">New approaches to support vector ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">05</biblScope>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large scale transductive SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1687" to="1712" />
			<date type="published" when="2006-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Loglinear models for label-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 16</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<editor>Smola, A. J.</editor>
		<editor>Bartlett, P.</editor>
		<editor>Schölkopf, B.</editor>
		<editor>and Schuurmans, D.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="115" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making large-scale svm learning practical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>Schölkopf, B.</editor>
		<editor>Burges, C.</editor>
		<editor>and Smola, A.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="200" to="209" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD02</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some results on Tchebychean spline functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge-based kernel approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1127" to="1141" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<editor>Smola, A. J.</editor>
		<editor>Bartlett, P.</editor>
		<editor>Schölkopf, B.</editor>
		<editor>and Schuurmans, D.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A machine learning approach to TCP throughput prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mizra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMETRICS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A generalized representer theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A co-regularized approach to semi-supervised learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd ICML Workshop on Learning with Multiple Views</title>
		<meeting>of the 22nd ICML Workshop on Learning with Multiple Views</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Collaborative ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">06</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">1-norm support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 16</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
