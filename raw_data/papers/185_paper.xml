<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Optimization and Learning for Renewal Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20101">2010 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Proc</forename><surname>Asilomar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>On</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Systems</forename><surname>Signals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Computers</forename><surname>And</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nov</forename></persName>
						</author>
						<title level="a" type="main">Dynamic Optimization and Learning for Renewal Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="20101">2010 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of optimizing time averages in systems with independent and identically distributed behavior over renewal frames. This includes scheduling and task processing to maximize utility in stochastic networks with variable length scheduling modes. Every frame, a new policy is implemented that affects the frame size and that creates a vector of attributes. An algorithm is developed for choosing policies on each frame in order to maximize a concave function of the time average attribute vector, subject to additional time average constraints. The algorithm is based on Lyapunov optimization concepts and involves minimizing a &quot;drift-plus-penalty&quot; ratio over each frame. The algorithm can learn efficient behavior without a-priori statistical knowledge by sampling from the past. Our framework is applicable to a large class of problems, including Markov decision problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Consider a stochastic system that regularly experiences times when the system state is refreshed, called renewal times. The goal is to develop a control algorithm that maximizes the time average of a reward process associated with the system, subject to time average constraints on a collection of penalty processes. The renewal-reward theorem is a simple and elegant technique for computing time averages in such systems (see, for example, <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>). However, the renewal-reward theorem requires random events to be independent and identically distributed (i.i.d.) over each renewal frame. While this i.i.d. assumption may hold if a single control law is implemented repeatedly, it is often difficult to choose in advance a single control law that optimizes the system subject to the desired constraints. This paper investigates the situation where the control policies used may differ from frame to frame, and are designed to dynamically solve the problem of interest.</p><p>This renewal problem arises in many different applications. One application of interest is a task processing network. For example, consider a network of wireless devices that repeatedly collaborate to accomplish tasks (such as reporting sensor data to a destination, or performing distributed computation on data). Tasks are performed one after the other, and for each task we must decide what modes of operation and communication to use, possibly allowing some nodes of the network to remain idle to save power. It is then important to make decisions that maximize the time average utility associated with task processing, subject to time average power constraints at each node. Alternatively, one may want to minimize time average power, subject to constraints on utility and on the Michael J. Neely is with the Electrical Engineering department at the University of Southern California, Los Angeles, CA. This material is supported in part by one or more of the following: the DARPA IT-MANET program grant W911NF-07-0028, the NSF Career grant CCF-0747525, and continuing through participation in the Network Science Collaborative Technology Alliance sponsored by the U.S. Army Research Laboratory.</p><p>"left-over" communication rates available for data that is not associated with the task processing.</p><p>This paper develops a general framework for solving such problems. To do so, we extend the theory of Lyapunov optimization from <ref type="bibr" target="#b2">[3]</ref>. Specifically, work in <ref type="bibr" target="#b2">[3]</ref> considers discrete time queueing networks and develops a simple drift-pluspenalty rule for making optimal decisions. These decisions are made in a greedy manner every slot based only on the observed traffic and channel conditions for that slot, without requiring a-priori knowledge of the underlying probability distribution. However, the work in <ref type="bibr" target="#b2">[3]</ref> assumes all slots have fixed length, the random network condition is observed at the beginning of each slot and does not change over the slot, and this condition is not influenced by control actions. The general renewal problem treated in the current paper is more complex because each frame may have a different length and may contain a sequence of random events. The frame length and the random event sequence may depend on the control decisions made over the course of the frame. Rather than making a single decision every slot, every frame we must specify a policy, being a contingency plan for making decisions over the course of the frame in reaction to the resulting system events.</p><p>This paper solves the general problem with a conceptually simple technique that chooses a policy to minimize a driftplus-penalty ratio every frame. We first develop algorithms for minimizing the time average of a penalty process subject to a collection of time average constraints. We then consider maximization of a concave function of a vector of time average attributes subject to similar constraints. This utility maximization problem is challenging because of the variable frame length. We overcome this challenge with a novel transformation together with a variation of Jensen's inequality.</p><p>While this paper focuses on task processing applications, we note that our renewal framework can also handle Markov decision problems. Specifically, suppose the system operates according to either a continuous or discrete time Markov chain with control-dependent transition probabilities. If the chain has a recurrent state, then renewals can be defined as re-visitations to this state, and the same drift-plus-penalty ratio technique can be applied. However, the drift-plus-penalty ratio may be difficult to optimize for Markov decision problems with high dimension (see also <ref type="bibr" target="#b3">[4]</ref>).</p><p>Prior work on learning algorithms for Markov decision problems is in <ref type="bibr" target="#b4">[5]</ref>, and related work in <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref> considers learning for optimization of energy and delay in queueing systems. The works <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[9]</ref> use stochastic approximation theory and two-timescale convergence analysis. The Lagrange multiplier updates in <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[9]</ref> are analogous to the virtual queue updates we use in this paper. However, the Lyapunov optimization framework we use is different and does not require a two- timescale approach. It also provides more explicit bounds on convergence times and deviations from optimality, and allows a broader class of problems such as task processing problems.</p><formula xml:id="formula_0">t[0]=0 t[1] t[2] t[3] t[4] T[0] T[1] T[2] T[3]</formula><p>The Lyapunov optimization technique that we use in this paper is based on our previous work in <ref type="bibr" target="#b2">[3]</ref>[10] <ref type="bibr" target="#b10">[11]</ref>[12] that develops the drift-plus-penalty method for stochastic network optimization, including opportunistic scheduling for throughpututility maximization <ref type="bibr" target="#b2">[3]</ref>[10] <ref type="bibr" target="#b11">[12]</ref> and average power minimization <ref type="bibr" target="#b10">[11]</ref> (see also <ref type="bibr" target="#b12">[13]</ref>). Alternative "fluid-based" stochastic optimization techniques for queueing networks are developed in <ref type="bibr" target="#b13">[14]</ref>[15] <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref>, and dual and primal-dual algorithms for systems without queues, based on tracking a corresponding static optimization problem, are in <ref type="bibr" target="#b17">[18]</ref>[19] <ref type="bibr" target="#b19">[20]</ref>. Our current paper considers the more complex renewal problem, and leverages ideas in <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b20">[21]</ref>, where <ref type="bibr" target="#b3">[4]</ref> considers a frame-based Lyapunov framework for Markov decision problems involving network delay, and <ref type="bibr" target="#b20">[21]</ref> develops a ratio rule for utility optimization in wireless systems with variable length frames and time-correlated channels.</p><p>Recent work in <ref type="bibr" target="#b21">[22]</ref> considers a task processing system where multiple wireless "reporting nodes" select data formats (e.g., "voice" or "video") in which to deliver sensed information. The work <ref type="bibr" target="#b21">[22]</ref> also uses a renewal structure. However, it assumes a single random event occurs at the beginning of each renewal frame, and the event and frame size are not influenced by control actions. More general problems can be treated using the theory developed in the current paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RENEWAL SYSTEM MODEL</head><p>Consider a system that operates over renewal frames. Specifically, consider the timeline of non-negative real times t ≥ 0, and suppose this timeline is segmented into successive frames of duration </p><formula xml:id="formula_1">{T [0], T [1], T [2], . . .}, as shown in</formula><formula xml:id="formula_2">t[r] = r−1 i=0 T [i]</formula><p>The interval of all times t such that t[r] ≤ t &lt; t[r + 1] is defined as the rth renewal frame, defined for each r ∈ {0, 1, 2, . . .}.</p><p>At the beginning of each renewal frame r, the controller selects a policy π <ref type="bibr">[r]</ref> from an abstract policy space P, and implements the policy over the duration of the frame. There may be random events that arise over the renewal frame (with distributions that are possibly dependent on the policy), and the policy specifies a contingency plan for reacting to these events. The policy incurs a vector of penal- </p><formula xml:id="formula_3">ties y[r] = (y 0 [r], y 1 [r], . . . , y L [r]) and attributes x[r] = (x 1 [r], . . . , x M [r]) for some integers L ≥ 0, M ≥ 0 (where</formula><formula xml:id="formula_4">T [r] = ˆ T (π[r]) (1) y l [r] = ˆ y l (π[r]) ∀l ∈ {0, 1, . . . , L} (2) x m [r] = ˆ x m (π[r]) ∀m ∈ {1, . . . , M }<label>(3)</label></formula><p>We assume the values of</p><formula xml:id="formula_5">[ ˆ T (π[r]), (ˆ y l (π[r])), (ˆ x m (π[r]</formula><p>))] for frame r are conditionally independent of events in previous frames given the particular policy π = π <ref type="bibr">[r]</ref>, and are identically distributed over all frames that use the same policy π.</p><p>Consider now a particular control algorithm that chooses policies π[r] ∈ P every frame r according to some well defined (possibly probabilistic) rule, and define the following frame-average expectations, defined for integers R &gt; 0:</p><formula xml:id="formula_6">T [R] = 1 R R−1 r=0 E {T [r]} , y l [R] = 1 R R−1 r=0 E {y l [r]}<label>(4)</label></formula><p>where we recall that</p><formula xml:id="formula_7">T [r], y l [r], x m [r] depend on the policy π[r] by (1)-(3). Define x m [R]</formula><p>similarly, and define the infinite horizon frame-average expectations T , y l , x m by:</p><formula xml:id="formula_8">(T , y l , x m ) = lim R→∞ (T [R], y l [R], x m [R])</formula><p>where we temporarily assume the limits are well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Optimization Objective</head><p>The first type of problem we consider uses only penalties y[r]: We must choose a policy π[r] ∈ P every frame r to minimize the ratio y 0 /T subject to constraints on y l /T :</p><formula xml:id="formula_9">Minimize: y 0 /T (5) Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} (6) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}<label>(7)</label></formula><p>where c l for l ∈ {1, . . . , L} are a given collection of realvalued (possibly negative) constants. The motivation for looking at the ratio y l /T is that it defines the time average penalty associated with the y l [r] process. To see this, suppose the following limits converge to constants y av l and T av with probability 1:</p><formula xml:id="formula_10">lim R→∞ 1 R R−1 r=0 y l [r] = y av l , lim R→∞ 1 R R−1 r=0 T [r] = T av (w.p.1)</formula><p>Under very mild conditions, the existence of the limits y av l and T av implies the frame-average expectations also have well defined limits, with y l = y av l and T = T av . This holds, for example, whenever y l [r] and T [r] are deterministically bounded by finite constants, or when more general conditions hold that allow the Lebesgue dominated convergence theorem to be applied <ref type="bibr" target="#b22">[23]</ref>. Then the time average penalty per unit time associated with y l <ref type="bibr">[r]</ref> (sampled only at renewal times for simplicity) satisfies with probability 1:</p><formula xml:id="formula_11">lim R→∞ R−1 r=0 y l [r] R−1 r=0 T [r] = lim R→∞ 1 R R−1 r=0 y l [r] 1 R R−1 r=0 T [r] = y l T</formula><p>Therefore, the value y l /T indeed represents the limiting penalty per unit time associated with the process y l <ref type="bibr">[r]</ref>.</p><p>The problem (5)- <ref type="formula" target="#formula_9">(7)</ref> seeks only to minimize a time average subject to time average constraints. The second problem we consider, more general than the first, seeks to maximize a concave and entrywise non-decreasing function φ(γ) of the time average attribute vector ratio x/T , where x = (x 1 , . . . , x M ):</p><formula xml:id="formula_12">Maximize: φ(x/T ) (8) Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} (9) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .} (10)</formula><p>where φ(γ) is a given concave and entrywise non-decreasing utility function defined over</p><formula xml:id="formula_13">γ = (γ 1 , . . . , γ M ) ∈ R M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Boundedness Assumptions</head><p>We assume <ref type="table">x</ref>  such that for all π[r] ∈ P and all m ∈ {1, . . . , M } we have: </p><formula xml:id="formula_14">y min 0 ≤ E {ˆy{ˆy 0 (π[r])|π[r]} ≤ y max 0 0 &lt; T min ≤ E ˆ T (π[r])|π[r] ≤ T max x min m ≤ E {ˆx{ˆx m (π[r])|π[r]} ≤ x max</formula><formula xml:id="formula_15">γ min m = min[x min m /T min , x min m /T max ] γ max m = max[x max m /T max , x max m /T max ]</formula><p>Define the hyper-rectangle R by:</p><formula xml:id="formula_16">R = {γ ∈ R M |γ min m ≤ γ m ≤ γ max m ∀m ∈ {1, . . . , M }} (11)</formula><p>Then for any algorithm that chooses policies π[r] ∈ P for all frames r, it is not difficult to show that </p><formula xml:id="formula_17">x m [R]/T [R] ∈ R for all R ∈ {1, 2, 3, . . .}, where T [R], x m [R], T [R]</formula><formula xml:id="formula_18">E ˆ T (π[r]) 2 |π[r] ≤ σ 1 E ˆ y l (π[r]) 2 |π[r] ≤ σ 1 ∀l ∈ {1, . . . , L} E ˆ x m (π[r]) 2 |π[r] ≤ σ 1 ∀m ∈ {1, . . . , M } C. Optimality of i.i.d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Algorithms</head><p>We now state the problem (5)-(7) more precisely, using lim sups which do not require existence of a well defined limit:</p><formula xml:id="formula_19">Minimize: lim sup R→∞ y 0 [R] T [R]<label>(12)</label></formula><p>Subject to: lim sup R→∞</p><formula xml:id="formula_20">y l [R] T [R] ≤ c l ∀l ∈ {1, . . . , L} (13) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}<label>(14)</label></formula><p>Assume that the constraints <ref type="formula" target="#formula_4">(13)</ref>- <ref type="formula" target="#formula_6">(14)</ref> are feasible, and define ratio opt as the infimum ratio in (12) over all algorithms that satisfy these constraints.</p><p>Define an i.i.d. algorithm as one that, at the beginning of each new frame r ∈ {0, 1, 2, . . .}, chooses a policy π <ref type="bibr">[r]</ref> by independently and probabilistically selecting π ∈ P according to some distribution that is the same for all frames r. </p><formula xml:id="formula_21">E {ˆy{ˆy 0 (π * [r])} ≤ E ˆ T (π * [r]) (ratio opt + δ) (15) E {ˆy{ˆy l (π * [r])} ≤ E ˆ T (π * [r]) (c l + δ) ∀l ∈ {1, . . . , L} (16) Proof:</formula><p>The proof is similar to results in <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b12">[13]</ref>, and is omitted for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OPTIMIZING TIME AVERAGES</head><p>Here we develop an algorithm to treat the problem (5)- <ref type="formula" target="#formula_9">(7)</ref>. To treat the constraints y l /T ≤ c l , which are equivalent to the constraints y l ≤ c l T , we define virtual queues Z l [r] for l ∈ {1, . . . , L}, with finite initial condition and with update equation:</p><formula xml:id="formula_22">Z l [r+1] = max[Z l [r]+y l [r]−c l T [r], 0]∀l ∈ {1, . . . , L} (17)</formula><p>The intuition is that if we can stabilize the queue Z l <ref type="bibr">[r]</ref>, then the time average of the "service process" c l T [r] is greater than or equal to the time average of the "arrival process" y l [r] (see also <ref type="bibr" target="#b10">[11]</ref> for application to virtual power queues for meeting time average power constraints).</p><p>Let</p><formula xml:id="formula_23">Z[r] = (Z 1 [r], . . . , Z L [r]</formula><p>) be the vector of virtual queues, and define the following quadratic Lyapunov function</p><formula xml:id="formula_24">L(Z[r]): L(Z[r]) = 1 2 L l=1 Z l [r] 2</formula><p>The value L(Z <ref type="bibr">[r]</ref>) is a scalar measure of the size of the queue backlogs. The intuition is that if we can take actions that consistently push this value down, then queues can be stabilized. Define the frame-based conditional Lyapunov drift ∆(Z <ref type="bibr">[r]</ref>) by:</p><formula xml:id="formula_25">∆(Z[r]) = E {L(Z[r + 1]) − L(Z[r])|Z[r]}</formula><p>Lemma 2: Under any control decision for choosing π[r] ∈ P, we have for all r and all possible Z[r]:</p><formula xml:id="formula_26">∆(Z[r]) ≤ B + L l=1 Z l [r]E {y l [r] − c l T [r]|Z[r]}<label>(18)</label></formula><p>where B is a constant that satisfies for all r and all possible Z[r]:</p><formula xml:id="formula_27">B ≥ 1 2 L l=1 E (y l [r] − c l T [r]) 2 |Z[r]<label>(19)</label></formula><p>Such a constant B exists by the boundedness assumptions in Section II-B.</p><p>Proof: Squaring (17) yields:</p><formula xml:id="formula_28">Z l [r + 1] 2 ≤ (Z l [r] + y l [r] − c l T [r]) 2 = Z l [r] 2 + (y l [r] − c l T [r]) 2 +2Z l [r](y l [r] − c l T [r])</formula><p>Taking conditional expectations, dividing by 2, and summing over l ∈ {1, . . . , L} yields the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Drift-Plus-Penalty Ratio Algorithm</head><p>Our Drift-Plus-Penalty Ratio Algorithm is designed to minimize a sum of the variables on the right-hand-side of the drift bound (18) and a penalty term, divided by an expected frame size, as in <ref type="bibr" target="#b20">[21]</ref>. The penalty term uses a non-negative constant V that will be shown to affect a performance tradeoff:</p><p>• (Policy Selection) Every frame r ∈ {0, 1, 2, . . .}, observe the virtual queues Z[r] and choose a policy π[r] ∈ P to minimize the following expression:</p><formula xml:id="formula_29">E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r]<label>(20)</label></formula><p>• <ref type="formula">(</ref> </p><formula xml:id="formula_30">E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r] ≤ C + inf π∈P   E V ˆ y 0 (π) + L l=1 Z l [r]ˆ y l (π)|Z[r] E ˆ T (π)|Z[r]  </formula><p>In Section V-B it is shown that the infimum of (20) over π ∈ P is the same as the infimum over the extended class of probabilistically mixed strategies that choose a random π ∈ P according to some distribution (exactly what i.i.d. policies do every frame). Thus, if policy π[r] is a C-additive approximation, then: </p><formula xml:id="formula_31">E V ˆ y 0 (π[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] ≤ E ˆ T (π[r])|Z[r] C + E{Vˆy0E{Vˆ E{Vˆy0(π * [r])+ P L l=1 Z l [r]ˆ y l (π * [r])} E{ˆTE{ˆ E{ˆT (π * [r])}<label>(21)</label></formula><note type="other">a) For all l ∈ {1, . . . , L} we have:</note><formula xml:id="formula_32">lim sup R→∞ y l [R]/T [R] ≤ c l ∀l ∈ {1, . . . , L}<label>(22)</label></formula><p>lim sup</p><formula xml:id="formula_33">R→∞ R−1 r=0 y l [r] R−1 r=0 T [r] ≤ c l (w.p.1)<label>(23)</label></formula><p>where "w.p.1" stands for "with probability 1." b) For all integers R &gt; 0 we have:</p><formula xml:id="formula_34">y 0 [R] T [R] ≤ ratio opt + (B/T [R] + C) V + E {L(Z[0])} V RT [R]<label>(24)</label></formula><p>and hence:</p><formula xml:id="formula_35">lim sup R→∞ y 0 [R]/T [R] ≤ ratio opt + (B/T min + C)/V (25)</formula><p>where B is defined in <ref type="bibr" target="#b18">(19)</ref>, and ratio opt is the optimal solution to (12)- <ref type="formula" target="#formula_6">(14)</ref>. Thus, the algorithm satisfies all constraints, and the value of V can be chosen appropriately large to make (B/T min + C)/V arbitrarily small, ensuring that the time average penalty is arbitrarily close to its optimal value ratio opt . The tradeoff in choosing a large value of V comes in the size of the Z l <ref type="bibr">[r]</ref> queues and the number of frames required for E {Z l [R]} /R to approach zero (which affects convergence time of the algorithm, see (33) in the proof). In particular, it can be shown from (30) that there are constants F 1 , F 2 such that (see <ref type="bibr" target="#b23">[24]</ref>):</p><formula xml:id="formula_36">E {Z l [R]} R ≤ F 1 + V F 2 R + L l=1 E {Z l [0] 2 } R 2</formula><p>Proof: (Theorem 1) Consider any frame r ∈ {0, 1, 2, . . .}. Combining (18) and (21) yields:  <ref type="formula" target="#formula_37">(26)</ref>, and letting δ → 0 yields:</p><formula xml:id="formula_37">∆(Z[r]) + V E {ˆy{ˆy 0 (π[r])|Z[r]} ≤ B + E ˆ T (π[r])|Z[r] C + E{Vˆy0E{Vˆ E{Vˆy0(π * [r])+ P L l=1 Z l [r]ˆ y l (π * [r])} E{ˆTE{ˆ E{ˆT (π * [r])} − L l=1 Z l [r]c l E ˆ T (π[r])|Z[r]<label>(26)</label></formula><formula xml:id="formula_38">∆(Z[r]) + V E {ˆy{ˆy 0 (π[r])|Z[r]} ≤ B +EˆT +Eˆ +EˆT (π[r])|Z[r] [C + V ratio opt ]<label>(27)</label></formula><p>Taking expectations of the above yields:</p><formula xml:id="formula_39">E {L(Z[r + 1])} − E {L(Z[r])} + V E {ˆy{ˆy 0 (π[r])} ≤ B + E ˆ T (π[r]) [C + V ratio opt ]<label>(28)</label></formula><p>Summing the above over r ∈ {0, . . . , R − 1} for some integer R &gt; 0 and dividing by R yields:</p><formula xml:id="formula_40">E {L(Z[R])} − E {L(Z[0])} R + V y 0 [R] ≤ B + T [R][C + V ratio opt ]<label>(29)</label></formula><p>Rearranging terms in the above and using the fact that E {L(Z[R])} ≥ 0 yields the result of part (b).</p><p>To prove part (a), from <ref type="bibr" target="#b26">(27)</ref> there is a constant F such that:</p><formula xml:id="formula_41">∆(Z[r]) ≤ F<label>(30)</label></formula><p>Thus, the drift of a quadratic Lyapunov function is bounded by a constant. Further, the second moments of per-frame changes in Z l <ref type="bibr">[r]</ref> are bounded because of the second moment assumptions on y l [r] and T <ref type="bibr">[r]</ref>. It follows that (see <ref type="bibr" target="#b23">[24]</ref>):</p><formula xml:id="formula_42">lim R→∞ E {Z l [R]} /R = 0<label>(31)</label></formula><formula xml:id="formula_43">lim R→∞ Z l [R]/R = 0 (w.p.1)<label>(32)</label></formula><p>Now from the queue update (17) we have for any frame r:</p><formula xml:id="formula_44">Z l [r + 1] ≥ Z l [r] + y l [r] − c l T [r]</formula><p>Summing the above over r ∈ {0, . . . , R − 1} for some integer R &gt; 0 yields:</p><formula xml:id="formula_45">Z l [R] − Z l [0] ≥ R−1 r=0 [y l [r] − c l T [r]</formula><p>] Taking expectations, dividing by R, and using E {Z l [0]} ≥ 0 yields for all integers R &gt; 0:</p><formula xml:id="formula_46">E {Z l [R]} R ≥ y l [R] − c l T [R]</formula><p>Thus:</p><formula xml:id="formula_47">y l [R] T [R] ≤ c l + E {Z l [R]} RT [R] ≤ c l + E {Z l [R]} RT min<label>(33)</label></formula><p>Taking limits of the above and using (31) proves <ref type="bibr" target="#b21">(22)</ref>. A similar argument uses (32) to prove (23). Under a mild "Slater-type" assumption that ensures the constraints (13) are achievable with "-slackness," the queues Z l <ref type="bibr">[R]</ref> can be shown to be strongly stable, in the sense that the time average expectation is bounded by O(V ). If further mild fourth moment boundedness assumptions hold for y l [r] and T [r] then the same bound (25) can be shown to hold for pure time averages with probability 1 <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. UTILITY OPTIMIZATION</head><p>Consider now the problem (8)-(10), which seeks to maximize φ(x/T ) subject to y l /T ≤ c l for all l ∈ {1, . . . , L}. We transform this problem of maximizing a function of a time average ratio into a problem of the type (5)-(7). The following variation on Jensen's inequality is crucial in this transformation:</p><p>Lemma 3: (Variation on Jensen's Inequality) Let φ(γ) be any continuous and concave function defined over γ ∈ R for some closed and bounded hyper-rectangle R. Let (T [r], γ[r]) be a sequence of arbitrarily correlated random vectors for r ∈ {0, 1, 2, . . .}. Assume that T [r] &gt; 0, γ[r] ∈ R for all r, and:</p><formula xml:id="formula_48">0 &lt; T min ≤ E {T [r]} ≤ T max &lt; ∞ ∀r ∈ {0, 1, 2, . . .}</formula><p>Then for any R &gt; 0:</p><formula xml:id="formula_49">1 R R−1 r=0 E {T [r]φ(γ[r])} 1 R R−1 r=0 E {T [r]} ≤ φ 1 R R−1 r=0 E {T [r]γ[r]} 1 R R−1 r=0 E {T [r]}</formula><p>Furthermore, assuming that the limits T φ(γ) and T γ defined below exist, we have:</p><formula xml:id="formula_50">T φ(γ)/T ≤ φ(T γ/T )<label>(34)</label></formula><p>where:</p><formula xml:id="formula_51">T φ(γ) = lim R→∞ 1 R R−1 r=0 E {T [r]φ(γ[r])} T γ = lim R→∞ 1 R R−1 r=0 E {T [r]γ[r]}</formula><p>Proof: See <ref type="bibr" target="#b12">[13]</ref>. Now define an auxiliary vector γ[r] = <ref type="figure" target="#fig_0">(γ 1 [r]</ref>, . . . , γ M [r]), to be chosen in the set R defined in (11) on every frame r.</p><p>Lemma 4: (Equivalent Transformation) The problem (8)-(10) is equivalent to the following transformed problem:</p><formula xml:id="formula_52">Maximize: T φ(γ)/T (35) Subject to: x m ≥ T γ m ∀m ∈ {1, . . . , M } (36) y l /T ≤ c l ∀l ∈ {1, . . . , L} (37) γ[r] ∈ R ∀r ∈ {0, 1, 2, . . .} (38) π[r] ∈ P ∀r ∈ {0, 1, 2, . . .} (39) Proof: We briefly sketch the proof: Let π * [r], γ * [r]</formula><p>be a policy that optimally solves the above transformed problem, and assume for simplicity it yields well defined time averages</p><formula xml:id="formula_53">T * , y * l , x * m , T * φ(γ * ), T * γ * ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and optimal utility</head><formula xml:id="formula_54">util * = T * φ(γ * )/T * .</formula><p>Then the policy π * [r] also satisfies all constraints of problem <ref type="formula">(8)</ref>- <ref type="formula">(10)</ref>, and yields:</p><formula xml:id="formula_55">φ(x * /T * ) ≥ φ(T * γ * /T * ) ≥ T * φ(γ * )/T * = util *</formula><p>where the first inequality above holds by (36) and the entrywise non-decreasing property of φ(γ), and the second holds by <ref type="bibr">(34)</ref>. Thus, the optimal utility of problem (8)-(10) is greater than or equal to that of the transformed problem. A similar argument shows it is also less than or equal to the optimal utility of the transformed problem. The transformed problem (35)-(39) has the structure of the problem (5)- <ref type="formula" target="#formula_9">(7)</ref>  </p><formula xml:id="formula_56">G m [r + 1] = max[G m [r] + T [r]γ m [r] − x m [r], 0] (40) Define G[r] = (G 1 [r], . . . , G M [r])</formula><p>. The drift-plus-penalty ratio to minimize every frame r is then:</p><formula xml:id="formula_57">E −V ˆ T (π[r])φ(γ[r]) + L l=1 Z l [r]ˆ y l (π[r])|Z[r] E ˆ T (π[r])|Z[r] + E M m=1 G m [r][ ˆ T (π[r])γ m [r] − ˆ x m (π[r])]|Z[r] E ˆ T (π[r])|Z[r]</formula><p>It is easy to see that the above can be minimized by separately choosing γ[r] ∈ R and π[r] ∈ P to minimize their respective terms, and thatˆTthatˆ thatˆT (π <ref type="bibr">[r]</ref>) cancels out of the auxiliary variable decisions. The resulting algorithm is thus to observe Z[r] and G[r] every frame r ∈ {0, 1, 2, . . .} and perform the following:</p><p>• (Auxiliary Variables) Choose γ[r] ∈ R to maximize:</p><formula xml:id="formula_58">V φ(γ[r]) − M m=1 G m [r]γ m [r]</formula><p>• (Policy Selection) Choose π[r] ∈ P to minimize:</p><formula xml:id="formula_59">E L l=1 Z l [r]ˆ y l (π[r]) − M m=1 G m [r]ˆ x m (π[r])|Z[r] E ˆ T (π[r])|Z[r]</formula><p>• (Virtual Queue Update) Update Z <ref type="bibr">[r]</ref> by <ref type="formula" target="#formula_9">(17)</ref> and G <ref type="bibr">[r]</ref> by (40). The auxiliary variable update is a simple deterministic maximization of a concave function over a hyper-rectangle, and can be separated into M optimizations of single-variable concave functions over an interval if the utility function has the form</p><formula xml:id="formula_60">φ(γ) = M m=1 φ m (γ m ).</formula><p>The policy selection step is again an optimization of a ratio of expectations and can be done as described in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OPTIMIZING THE RATIO OF EXPECTATIONS</head><p>Here we show how to minimize the ratio of expectations given in (20) (and also in the policy selection stage of the previous section). These problems can be written more generally as choosing a policy π[r] ∈ P to minimize the ratio: </p><formula xml:id="formula_61">E {a(π)} E {b(π)}</formula><formula xml:id="formula_62">θ * = inf π∈P E {a(π)} E {b(π)}</formula><p>If the expectation E {b(π)} is the same for all π ∈ P (such as when the frame size is independent of the policy), then θ * is obtained by infimizing the numerator E {a(π)}. This is typically easier (often involving learning for stochastic shortest path computations [25] <ref type="bibr" target="#b3">[4]</ref>). Otherwise, the following simple lemma is useful. Lemma 5: We have:</p><formula xml:id="formula_63">inf π∈P E {a(π) − θ * b(π)} = 0<label>(41)</label></formula><p>Further, for any real number θ, we have:</p><formula xml:id="formula_64">inf π∈P E {a(π) − θb(π)} &lt; 0 if θ &gt; θ * (42) inf π∈P E {a(π) − θb(π)} &gt; 0 if θ &lt; θ * (43) Proof: See [13].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Bisection Algorithm</head><p>Lemma 5 immediately leads to the following simple bisection algorithm: Suppose we have upper and lower bounds θ min and θ max , so that we know θ min ≤ θ * ≤ θ max . Then we can define θ = (θ min + θ max )/2, and compute the value of inf π∈P E {a(π) − θb(π)}. If the result is 0, then θ = θ * .</p><p>If positive, then θ &lt; θ * , and otherwise θ &gt; θ * . We can then refine our upper and lower bounds. This leads to a simple iterative algorithm where the distance between the upper and lower bounds decreases by a factor of 2 on each iteration. It thus approaches the optimal θ * value exponentially fast. Each step of the iteration involves minimizing an expectation, rather than a ratio of expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimizing over Pure Policies</head><p>Note that for any set of policies S, Lemma 5 implies that inf π∈S E {a(π) − θb(π)} = 0 if and only if θ = inf π∈S E {a(π)} /E {b(π)}. Now suppose we have a set of policies P pure that we call pure policies, and that the policy space P consists of all pure policies as well as all "mixtures" (or convex combinations) of pure policies, being policies that choose a pure policy in P pure with some particular probability distribution. More generally, define Ω as the set of all vectors (E {a(π)} , E {b(π)}) achievable over π ∈ P pure , and suppose the set of all (E {a(π)} , E {b(π)}) achievable over π ∈ P is equal to the convex hull of Ω. Recall that θ * is the infimum ratio of E {a(π)} /E {b(π)} over π ∈ P. Then:</p><formula xml:id="formula_65">0 = inf π∈P E {a(π) − θ * b(π)} = inf (a,b)∈Conv(Ω) [a − θ * b] = inf (a,b)∈Ω [a − θ * b] = inf π∈P pure E {a(π) − θ * b(π)}</formula><p>where the third inequality holds because the infimum of a linear function over the convex hull of a set is equal to the infimum over the set itself. It follows that θ * is also the infimum ratio of E {a(π)} /E {b(π)} over π ∈ P pure . This means that to achieve the infimum ratio over policies π ∈ P, it suffices to restrict our search to pure policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimizing with Initial Information</head><p>Suppose at the beginning of each frame, we observe a vector η[r] of initial information that can affect the penalties and frame size. Suppose that {η[r]} ∞ r=0 is i.i.d. over frames. Each policy π ∈ P first observes η <ref type="bibr">[r]</ref> and then chooses a sub-policy π ∈ P η <ref type="bibr">[r]</ref> , where P η <ref type="bibr">[r]</ref> is a space that possibly depends on η <ref type="bibr">[r]</ref>. To minimize E {a(π)}, it suffices to observe η <ref type="bibr">[r]</ref> and choose π ∈ P η <ref type="bibr">[r]</ref> to minimize the conditional expectation E {a(π )|η[r]}. However, this is not necessarily true for minimizing the ratio E {a(π)} /E {b(π)}.</p><p>A correct approach is the following: If θ * is known, we can simply choose π ∈ P η[r] to minimize:</p><formula xml:id="formula_66">E {a(π ) − θ * b(π )|η[r]}</formula><p>If θ * is unknown, we can carry out the bisection routine. Let θ be the midpoint in the current iteration. We must compute:</p><formula xml:id="formula_67">inf π∈P E {a(π) − θb(π)} = E inf π ∈P η[r] E {a(π ) − θb(π )|η[r]}</formula><p>(44) The infimizing decision π can be made by observing η <ref type="bibr">[r]</ref>, without requiring knowledge of its probability distribution. However, the value in (44) cannot be computed without knowledge of this distribution. Instead, suppose we have W</p><formula xml:id="formula_68">i.i.d. samples {η w } W w=1</formula><p>. We can then approximate the value in (44) by the function val(θ) defined below:</p><formula xml:id="formula_69">val(θ) = 1 W W w=1 inf π ∈Pη w E {a(π ) − θb(π )|η w } (45)</formula><p>By the law of large numbers, val(θ) approaches the exact value of <ref type="formula" target="#formula_6">(44)</ref> with a large choice of W . The bisection routine can be carried out using the val(θ) approximation, being sure to use the same samples at each step of the iteration (but different samples on each frame r). Note that val(θ) is nonincreasing in θ, so the bisection will converge provided that it is initialized so that val(θ min ) ≥ 0 and val(θ max ) ≤ 0. If we cannot independently generate W samples, we use the W past observed values of η[r] from previous frames. There is a subtle issue here, as these past values have influenced system performance and are thus correlated with the current a(π) and b(π) functions. However, a delayed queue argument similar to that given in <ref type="bibr" target="#b25">[26]</ref> shows these past values can still be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Alternative Formulation</head><p>Note that constraints of the form y l ≤ 0 are equivalent to y l /T ≤ c l in the special case c l = 0, and thus can be handled using the framework of this paper. Now consider the following problem structure:</p><formula xml:id="formula_70">Minimize: y 0 Subject to: y l /T ≤ c l ∀l ∈ {1, . . . , L} π[r] ∈ P ∀r ∈ {0, 1, 2, . . .}</formula><p>Such a problem has a different structure than the problem (5)-(7), and is easier to solve as it does not require a ratio of expectations. It can be solved using the same virtual queues Z l <ref type="bibr">[r]</ref> in <ref type="formula" target="#formula_9">(17)</ref>, but every frame r observing Z[r] and selecting a policy π[r] ∈ P to minimize the following expression:</p><formula xml:id="formula_71">E{VˆyE{Vˆ E{Vˆy 0 (π[r]) + L l=1 Z l [r][ˆ y l (π[r]) − c l ˆ T (π[r])]|Z[r]}</formula><p>Analysis is omitted for brevity (see Exercise 7.3 in <ref type="bibr" target="#b12">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Alternative Algorithm</head><p>The following is an alternative algorithm for the original problem (5)-(7) that does not require a ratio minimization (and hence does not require a bisection step): Use the same virtual queues Z l <ref type="bibr">[r]</ref> in (17). Define θ[0] = 0, and define θ[R] for R ∈ {1, 2, 3, . . .} by:</p><formula xml:id="formula_72">θ[R] = R−1 r=0 y 0 [r]/ R−1 r=0 T [r]<label>(46)</label></formula><p>Every frame r, observe Z[r] and θ <ref type="bibr">[r]</ref> and select a policy π[r] ∈ P to minimize the following expression:</p><formula xml:id="formula_73">E{V [ˆ y 0 (π[r]) − θ[r] ˆ T (π[r])]|Z[r], θ[r]}<label>(47)</label></formula><formula xml:id="formula_74">+E{ L l=1 Z l [r][ˆ y l (π[r]) − c l ˆ T (π[r])]|Z[r], θ[r]}</formula><p>It is shown in Exercise 7.5 of <ref type="bibr" target="#b12">[13]</ref> that all constraints are met, and that if θ[r] converges to a constant with probability 1, then with probability 1:</p><formula xml:id="formula_75">lim R→∞ R−1 r=0 y 0 [r]/ R−1 r=0 T [r] ≤ ratio opt + O(1/V )</formula><p>The disadvantage is that the convergence time is not as clear as that given in part (b) of Theorem 1. Further, use of the time average (46) makes it difficult to adapt to changes in system parameters, so that it may be better to approximate (46) with a moving average or an exponentially decaying average. Here we provide a simple task processing example. An infinite sequence of tasks must be processed one at a time with the help of a network of 5 wireless devices. This applies, for example, in scenarios similar to <ref type="bibr" target="#b21">[22]</ref> where each new task represents an event that is sensed by the wireless devices (each at different sensing qualities <ref type="bibr" target="#b26">[27]</ref>), and we must select which device reports the event information. The renewal structure is shown in <ref type="figure" target="#fig_11">Fig. 2</ref>. At the beginning of each new task r, a period of 0.5 time units is expended to communicate control information about the task. Each of the 5 devices expends 0.5 units of energy in this control phase. At the end of this phase, the network controller obtains a vector η[r] of parameters for task r. The vector η[r] has the form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SIMULATIONS FOR A TASK PROCESSING NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Control</head><formula xml:id="formula_76">η[r] = [(qual 1 [r], T tran 1 [r]), . . . , (qual 5 [r], T tran 5 [r])]</formula><p>where for each l ∈ {1, . . . , 5}, qual l [r] is a real number representing the information quality if device l is chosen to process task r, and T tran l [r] is the transmission time required for device l to transmit the corresponding information to a receiving station. The controller must choose one of the 5 devices to process the task, and must also choose the amount of idle time at the end of the frame (chosen within the interval <ref type="bibr">[0,</ref><ref type="bibr" target="#b4">5]</ref>), so that the policy decision π[r] has the form:</p><formula xml:id="formula_77">π[r] = (l[r], Idle[r]) ∈ {1, 2, 3, 4, 5} × {I ∈ R|0 ≤ I ≤ 5}</formula><p>Define P tran as the power expenditure associated with wireless transmission. The chosen device l[r] expends P tran × T tran l <ref type="bibr">[r]</ref> units of energy in the transmit phase, while all other devices l = l[r] expend no energy in this phase. None of the devices expend energy in the idle phase, which helps to limit the average power expenditure in the system.</p><p>The goal is to maximize the quality of information (q.o.i) per unit time subject to an average power constraint of 0.25 at each device. Definê y 0 (π[r]) as −1 times the q.o.i. obtained for task r, ˆ y l (π <ref type="bibr">[r]</ref>) as the energy expended by device l on task r, andˆTandˆ andˆT (π <ref type="bibr">[r]</ref>) as the frame duration for task r:  We simulate the drift-plus-penalty ratio algorithm for 10 6 frames, using the bisection method with W past samples of η <ref type="bibr">[r]</ref> as in <ref type="formula" target="#formula_6">(45)</ref>   <ref type="bibr">[0, l]</ref> for l ∈ {1, 2, 3, 4, 5} (so that device 5 tends to have the highest quality, while device 1 tends to have the lowest). We initialize θ min = −5V , θ max</p><formula xml:id="formula_78">= 5 l=1 Z l [r]</formula><p>3. Each step of the bisection computes val(θ) according to a simple deterministic optimization, and the bisection routine is run for each frame until θ max −θ min &lt; 0.001. Using V = 100, the resulting q.o.i per unit time is plotted in <ref type="figure" target="#fig_13">Fig. 3</ref>. This increases to its optimal value as W is increased. However, in this example, W does not need to be very large for accurate results: Even W = 1 produces a value that is near optimal (note that the y-axis in <ref type="figure" target="#fig_13">Fig. 3</ref> distinguishes utility only in the 3rd significant digit).</p><p>All It can be seen that devices {2, . . . , 5} are utilized to their maximum power constraints because these tend to give the highest quality, while average power for device 1 is slack.</p><p>The alternative algorithm of Section V-E, which does not require a bisection routine and amounts to a simple deterministic optimization for (47) every frame, achieves similar time average power expenditures to the above. It also achieves utility as shown in <ref type="figure" target="#fig_13">Fig. 3</ref>, being the constant that does not depend on W (as no sampling from the past is needed). Its utility is slightly larger than that of the bisection algorithm, and is approached by the bisection algorithm as W increases. It appears that this algorithm is simpler and yields "automatic learning" by using the time average value θ[r], but it might have trouble adapting if system parameters change.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A timeline illustrating renewal frames for the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Define t[0] = 0, and for each positive integer r define t[r] as the rth renewal time:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>= 0 corresponds to problems without y[r] penalties, and M = 0 corresponds to problems without x[r] attributes). The policy may also affect the renewal frame duration T [r]. Formally, the values T [r], y l [r], x m [r] are determined by random functionsˆTfunctionsˆ functionsˆT (·), ˆ y l (·), ˆ x m (·) of the policy π[r]:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>m [r], T [r], and y 0 [r] have bounded conditional expectations, regardless of the policy. That is, there are finite constants x min m , x max m , T min , T max , y min 0 , y max 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are frame average expectations over the first R frames, as defined by (4). Finally, we assume the conditional second moments of T [r], x m [r], and y l [r] (for l = 0) are finite, regardless of the policy. That is, there is a finite constant σ 1 such that for all π[r] ∈ P:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Queue Update) Observe the resulting y[r] and T [r] values, and update virtual queues Z l [r] by (17). Details on minimizing (20) are given in Section V. Rather than assuming we achieve the exact infimum of (20) over all policies π[r] ∈ P, it is useful to allow our decisions to come within an additive constant C of the infimum. Definition 1: A policy π[r] is a C-additive approximation for the problem (20) if for a given constant C ≥ 0 we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>where π * [r] is any i.i.d. algorithm. Note that conditional expectations given Z[r] are the same as unconditional ex- pectations under i.i.d. algorithms, because their decisions are independent of system history. Theorem 1: (Algorithm Performance) Assume the con- straints of problem (12)-(14) are feasible. Fix constants C ≥ 0, V ≥ 0, and assume the above algorithm is implemented using any C-additive approximation every frame r for the minimiza- tion in (20). Assume initial conditions satisfy E {L(Z[0])} &lt; ∞. Then:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>In the above inequality, π[r] represents the C-additive approx- imate decision actually made, and π * [r] is from any alternative i.i.d. algorithm. Fixing any δ &gt; 0, plugging the i.i.d. algorithm π * [r] from (15)-(16) into the right-hand-side of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>if we define y 0 [r] = − T [r]φ(γ[r]), write the constraints (36) as T γ m − x m ≤ 0, and define policy decision π [r] = (π[r], γ[r]) ∈ P × R. The resulting algorithm is thus the same as that given in Section III-A, and for this context it is given as follows: For the constraints (37), use the same virtual queues Z l [r] defined in (17). For the constraints (36), define virtual queues G m [r] for m ∈ {1, . . . , M } by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>where a(π), b(π) are random functions of π ∈ P, and b(π) is strictly positive with T max ≥ E {b(π)|π} ≥ T min &gt; 0 for all π ∈ P. The function b(π) is equal tô T (π). The function a(π) depends on Z[r], and the above expectations are implicitly conditioned on Z[r], although we suppress this notation for simplicity. Define θ * as the optimal ratio:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of the 3 phases of a renewal frame r ∈ {0, 1, 2, . . .}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>ˆ</head><label></label><figDesc>y 0 (π[r]) = −qual l[r] [r] ˆ y l (π[r]) = 0.5 + P tran T tran l [r]1 {l[r]=l} ∀l ∈ {1, . . . , 5}ˆT 5}ˆ 5}ˆT (π[r]) = 0.5 + T tran l[r] [r] + Idle[r] where 1 {l[r]=l} is an indicator function that is 1 if l[r] = l and 0 else. The problem is then to minimize y 0 /T subject to y l /T ≤ 0.25 for all l ∈ {1, . . . , 5}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Utility for the drift-plus-penalty ratio algorithm (with bisection) and the time-averaged alternative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>of Section V-C. We use P tran = 1.0. The vectors {η[r]} ∞ r=0 are assumed to be i.i.d. with independently chosen components, where T tran l [r] is uniformly distributed in [0.5, 2.5] for all l, and qual l [r] is uniformly distributed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>average power constraints are met in all simulations (for each W ). Results for W = 10 are: q.o.i/T = 0.852950, T = 3.180275, Idle = 1.421260, y 0 = −2.712615, and: y 1 /T = 0.182335 ≤ 0.25 y 2 /T = 0.249547 ≤ 0.25 , y 3 /T = 0.250018 ≤ 0.25 y 4 /T = 0.250032 ≤ 0.25 , y 5 /T = 0.250046 ≤ 0.25</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Discrete Stochastic Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to Probability Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-12" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>8th edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resource allocation and cross-layer control in wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Georgiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Networking</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="149" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic optimization for markov modulated networks with application to delay constrained wireless scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Decision and Control (CDC)</title>
		<meeting>IEEE Conf. on Decision and Control (CDC)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Policy gradient stochastic approximation algorithms for adaptive control of constrained time varying markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Vázquez Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Decision and Control</title>
		<meeting>IEEE Conf. on Decision and Control</meeting>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">q-learning algorithms for constrained markov decision processes with randomized monotone policies: Application to mimo transmission control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Djonin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2170" to="2181" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An on-line learning algorithm for energy efficient delay constrained scheduling over a fading channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Salodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhorkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="732" to="742" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic framework for dynamically optimizing multi-user video transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="308" to="320" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decomposition principles and online learning in cross-layer optimization for delay-sensitive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1401" to="1415" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dynamic Power Allocation and Routing for Satellite and Wireless Networks with Time Varying Channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology, LIDS</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Energy optimal control for time varying wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2915" to="2934" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairness and optimal stochastic control for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Modiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2005-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stochastic Network Optimization with Application to Communication and Queueing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Morgan &amp; Claypool</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fair resource allocation in wireless networks using queue-length-based scheduling and congestion control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eryilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1344" />
			<date type="published" when="2007-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximizing queueing network utility subject to stability: Greedy primal-dual algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queueing Systems</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="457" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Greedy primal-dual algorithm for dynamic resource allocation in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queueing Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="220" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scheduling in wireless networks under uncertainties: A greedy primal-dual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negi</surname></persName>
		</author>
		<idno>arXiv:1001:2050v2</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv Technical Report</title>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint rate control and scheduling in multihop wireless networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 43rd IEEE Conf. on Decision and Control</title>
		<meeting>of 43rd IEEE Conf. on Decision and Control<address><addrLine>Paradise Island, Bahamas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimality of certain channel aware scheduling policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Annual Allerton Conference on Communication , Control, and Computing</title>
		<meeting>40th Annual Allerton Conference on Communication , Control, and Computing<address><addrLine>Monticello, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymptotic properties of proportionalfair sharing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whiting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 40th Annual Allerton Conf. on Communication, Control, and Computing</title>
		<meeting>of 40th Annual Allerton Conf. on Communication, Control, and Computing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Network utility maximization over partially observable markovian channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1008.3421v1</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv Technical Report</title>
		<imprint>
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing information credibility in social swarming applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Terlecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar-Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<idno>arXiv:1009:6006</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv technical report</title>
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Probability with Martingales. Cambridge Mathematical Textbooks</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Queue stability and probability 1 convergence via lyapunov optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv Technical Report</title>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neuro-Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Max weight learning algorithms with application to scheduling in unknown environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.0630v1</idno>
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Building principles for a quality of information specification for sensor information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bisdikian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Thornley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Int&apos;l Conf. on Information Fusion (Fusion &apos;09)</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
