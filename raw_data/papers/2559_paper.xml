<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOME NEW DIRECTIONS IN GRAPH-BASED SEMI-SUPERVISED LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
							<email>goldberg@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
							<email>tushar@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison Madison</orgName>
								<address>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SOME NEW DIRECTIONS IN GRAPH-BASED SEMI-SUPERVISED LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-semi-supervised learning</term>
					<term>multi-manifold</term>
					<term>online learning</term>
					<term>compressive sensing</term>
					<term>graph</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this position paper, we first review the state-of-the-art in graph-based semi-supervised learning, and point out three limitations that are particularly relevant to multimedia analysis: (1) rich data is restricted to live on a single manifold; (2) learning must happen in batch mode; and (3) the target label is assumed smooth on the manifold. We then discuss new directions in semi-supervised learning research that can potentially overcome these limitations: (i) modeling data as a mixture of multiple manifolds that may intersect or overlap; (ii) online semi-supervised learning that learns incrementally with low computation and memory needs; and (iii) learning spectrally sparse but non-smooth labels with compressive sensing. We give concrete examples in each new direction. We hope this article will inspire new research that makes semi-supervised learning an even more valuable tool for multimedia analysis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">STATE-OF-THE-ART IN GRAPH-BASED SEMI-SUPERVISED LEARNING AND LIMITATIONS</head><p>Semi-supervised learning encompasses many different model assumptions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Graph-based semi-supervised learning is an important family of methods that make the following common assumption. Let x i , x j ∈ X be two input items, and y i , y j ∈ Y be their labels. Usually, X ⊆ R D and Y = {−1, 1} for binary classification, but multiclass classification and regression are common, too. Let d(·, ·) be an appropriate distance measure on X . The graph-based assumption states that if d(x i , x j ) is small, then y i ≈ y j . This assumption applies regardless of whether x i , x j are labeled. If x 1 is labeled, and there is a sequence of unlabeled items x 2 , . . . , x k such that d(x i , x i+1 ) is small for i = 1 . . . k − 1, then the label y 1 will propagate along the sequence.</p><p>Formally, a graph is formed with nodes being labeled data {(x i , y i )} n i=1 and unlabeled data {x i } n+m i=n+1 . The undirected</p><p>We would like to thank the Wisconsin Alumni Research Foundation. AG is supported in part by a Yahoo! Key Technical Challenges Grant.</p><p>edges reflect similarity between nodes: the edge weight w ij between nodes x i , x j is large if d(x i , x j ) is small. A common choice of edges is to connect each node to its kNNs with weight 1, and disconnect it from all other nodes (weight 0). Let W be the (n + m) × (n + m) weight matrix, and D the diagonal degree matrix with </p><formula xml:id="formula_0">D ii = j w ij . Let L = D − W be</formula><formula xml:id="formula_1">f Lf = 1/2 i,j (f (x i ) − f (x j )) 2 w ij .</formula><p>This assumption is behind graph-based semi-supervised methods such as Mincut <ref type="bibr" target="#b2">[3]</ref>, graph random walk <ref type="bibr" target="#b3">[4]</ref>, Gaussian Random Fields <ref type="bibr" target="#b4">[5]</ref>, local and global consistency <ref type="bibr" target="#b5">[6]</ref>, spectral graph transducer <ref type="bibr" target="#b6">[7]</ref>, manifold regularization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and many other variants. In particular, Belkin et al. generalize graph-based learning to the manifold setting, where X is assumed to be a low dimensional manifold in R D , the labels y change smoothly on the manifold, and the graph constructed on labeled and unlabeled training data is a random realization of the manifold. This provides an elegant conceptual model. The graph-based assumption has been extended to directed edges like links between Web pages <ref type="bibr" target="#b9">[10]</ref> and dissimilarity edges <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Applications of graph-based semi-supervised learning abound.</p><p>Despite their success, we point out three major limitations of graph-based methods:</p><p>(1) Current methods assume that X is a single manifold, or multiple well-separated manifolds. Therefore, it makes sense to create the graph W using kNN edges, or Gaussian weighted edges w ij = exp −λd(x i , x j ) 2 , where d(·, ·) is based on Euclidean distance. In both cases, nearby nodes are strongly connected and are assumed to have similar labels. However, in multimedia data, the distribution of objects might form multiple manifolds that intersect or partially overlap with each other. For example, in motion segmentation from video images, the tracked feature points on different objects form multiple intersecting and overlapping manifolds <ref type="bibr" target="#b12">[13]</ref>. Even though each individual manifold obeys the label smoothness assumption, nearby items on different manifolds may not satisfy this assumption. Straightforward application of existing graph-based semi-supervised learning will not achieve optimal performance.</p><p>(2) Current methods learn in batch mode. That is, they require the training set to be available all at once. However, consider a robot with a camera that continuously takes video of its surroundings, and learns the names of various objects. A human annotator provides object names (labels) only occasionally on selected video frames. This is therefore semisupervised learning. But the robot cannot afford to store the massive amount of mostly unlabeled video before learning. It requires an "anytime classifier" that is ready at all times, while continuously improving itself. And training must be cheap and quick. What we need is semi-supervised learning that operates in online mode.</p><p>(3) Current methods assume label smoothness on the graph. As dissimilarity edges show, this may not always be the case <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. In general, the relation between the label and the underlying graph can be studied from the perspective of harmonic analysis. It is well-known that the traditional smoothness assumption is equivalent to favoring low frequency components of the graph spectrum <ref type="bibr" target="#b13">[14]</ref>. Recent advances in compressive sensing (see, e.g., <ref type="bibr" target="#b14">[15]</ref>) allow learning from an arbitrary combination of low and high frequency components, as long as the number of components is small. We present, to our knowledge, the first connection between compressive sensing and graph-based transduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTI-MANIFOLD LEARNING</head><p>We recently introduced a novel graph as a first step in addressing data containing a mixture of manifolds <ref type="bibr" target="#b15">[16]</ref>. The idea is to assign edge weights based on differences in local geometry around each item x. Our intuition is that items on different manifolds, or in regions with different density, should be considered dissimilar and lead to low edge weights. Computationally, we compare local regions using Hellinger distance, which is sensitive to local manifold structures. We start by estimating the local sample covariance matrix Σ x around a randomly selected set of anchor items x. Then, the squared Hellinger distance between two anchor points</p><formula xml:id="formula_2">x i , x j is H 2 (p, q) = 1 2 p(x) − q(x) 2</formula><p>dx, where p = N (x; 0, Σ xi ) and q = N (x; 0, Σ xj ) are zero mean Gaussians with those local sample covariance matrices. The Hellinger distance H is symmetric, in <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>, small when the local geometry is similar, and large when there is significant difference in density, manifold dimensionality or orientation (see <ref type="figure" target="#fig_1">Figure 2</ref> in <ref type="bibr" target="#b15">[16]</ref>). Finally, we build a sparse kNN graph over the labeled and anchor unlabeled items as follows: Each such x is connected by a weighted, undirected edge to its k nearest Mahalanobis neighbors. Note that, since Σ x captures the local geometry around x, we "follow the manifold" by using the Mahalanobis distance as the local distance metric at </p><formula xml:id="formula_3">x: d 2 M (x, x ) = (x − x ) Σ −1 x (x − x ).</formula><p>The neighborhood size k is set to grow with dataset size. The graph edges are weighted using the standard RBF scheme, but with Hellinger distance: w ij = exp −λH 2 (p, q) . See <ref type="figure">Figure 3</ref> in <ref type="bibr" target="#b15">[16]</ref> for an example graph using this weighting scheme. In short, the graph combines locality and geometry: an edge has large weight when the two nodes are close in Mahalanobis distance, and have similar covariance structure. Importantly, it effectively separates intersecting and overlapping manifolds into individual pieces.</p><p>We demonstrate the effectiveness of this Hellinger graph with manifold regularization <ref type="bibr" target="#b7">[8]</ref> on two synthetic datasets. Dollar sign is a regression dataset containing two intersecting manifolds with target values varying greatly across intersection points <ref type="figure" target="#fig_0">(Figure 1(a)</ref>, color indicates y). Surfacehelix is a classification dataset with a 1D toroidal helix intersecting a surface-each manifold is a separate class <ref type="figure" target="#fig_0">(Fig- ure 1(b)</ref>). <ref type="figure" target="#fig_0">Figure 1</ref> compares three learners on these datasets: <ref type="bibr">[Supervised]</ref>: supervised learner (kernel regression or SVM) trained on labeled data only, ignoring unlabeled data.</p><p>[MR]: standard manifold regularization (LapRLS or LapSVM) using a Euclidean-based 3NN graph <ref type="bibr" target="#b7">[8]</ref>. [Hellinger-MR]: manifold regularization (LapRLS or LapSVM) using this novel Hellinger graph. See <ref type="bibr" target="#b15">[16]</ref> for details about the parameters governing the Hellinger graph. All other parameters were tuned using 5-fold cross validation. All datasets start with M = 20, 000 unlabeled items, from which we select m ∼ O(M/log(M )) anchor items. <ref type="figure" target="#fig_0">Figure 1</ref> shows performance on a separate test set of 20,000 items, averaged over 10 trials. For the dollar sign data, standard MR performs only as well as supervised learning, while Hellinger-MR achieves statistically significantly better MSE in all four n conditions (based on paired t-tests). For the surface-helix data, the three methods are all statistically significantly different for all n, with Hellinger-MR making the best use of unlabeled data.</p><p>Open issues surrounding our Hellinger graph remain, including how to select anchors, and how to optimize parameters. Furthermore, some other metrics over matrices or probability distributions may be more appropriate than Hellinger distance for this purpose. Finally, it could be useful to exploit the labeled data for detecting and validating the presence of multiple manifolds with differing target values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ONLINE SEMI-SUPERVISED LEARNING</head><p>We argue that the following is an important and practical setting, especially for real-time multimedia applications:</p><p>1. At time t an adversary picks (x t , y t ) and shows x t . 2. The learner predicts f t (x t ).</p><p>3. With (small) probability p the adversary reveals y t .</p><p>Otherwise x t remains unlabeled. 4. The learner updates its predictor to f t+1 , even when y t is not given. Repeat with t = t + 1.</p><p>This is clearly online learning. It differs from the standard online setting in that learning happens even on unlabeled data. The goal is to update f t in such a way that there is no regret, i.e., the wrong predictions the online procedure makes over time are comparable to a batch learner, which has access to the same input simultaneously but has to use the single best fixed predictor. A good online semi-supervised learning algorithm should achieve zero regret with sublinear space and time complexity. As a concrete example, our online semi-supervised algorithm in <ref type="bibr" target="#b16">[17]</ref> employs online convex programming with an asymptotic zero-regret guarantee. At the heart of the algorithm is a gradient step in kernel space</p><formula xml:id="formula_4">f t+1 = f t − η t ∂Jt(f ) ∂f ft</formula><p>, where η t is a stepsize that decays as</p><formula xml:id="formula_5">O(1/ √ t).</formula><p>The term J t (f ) is the instantaneous risk functional. When summing over time, we recover the standard batch manifold regularization risk J(f ) = 1 n+m n+m t=1 J t (f ). The definition of J t (f ) can be found in <ref type="bibr" target="#b16">[17]</ref>; suffice it to say that the instantaneous graph energy is</p><formula xml:id="formula_6">t−1 i=1 (f (x i ) − f (x t )) 2 w it .</formula><p>That is, it involves the edges from x t to all previous nodes in the graph. However, its complexity grows linearly with t. One approximation is to use a buffer of fixed size τ :</p><formula xml:id="formula_7">t τ t−1 i=t−τ (f (x i ) − f (x t )) 2 w it .</formula><p>That is, old nodes from τ steps ago are discarded. <ref type="figure" target="#fig_1">Figure 2</ref> compares batch vs. online semi-supervised learning (manifold regularization)'s running time and test error on MNIST digit recognition 1 vs. 2. The online algorithm achieves a desirable constant learning complexity at each step, and has comparable accuracy as batch mode.</p><p>Keeping a fixed buffer of recent input is not optimal ultimately. The dynamic graph constructed on items in the buffer only reflects a random and noisy snapshot of the underlying manifold structure. Given the same space constraint, it is better to form a summary of all the input so far. One possibility is some form of online clustering that forms a mixture model (e.g., Gaussian mixtures) on the input, using a fixed number of mixing components, as in <ref type="figure" target="#fig_1">Figure 2(right)</ref>. A "hyper-graph" can then be maintained on the mixture model, with nodes being the components. Graph-based learning proceeds on the hyper-graph, which is a fixed-size summary of the manifold seen so far. This is the idea behind the use of Random Projection Trees in <ref type="bibr" target="#b16">[17]</ref>; see also <ref type="bibr" target="#b17">[18]</ref>.</p><p>Looking forward, we need more efficient online semisupervised algorithms with theoretical guarantees. The combination of online semi-supervised learning and online active learning also deserves attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LEARNING NON-SMOOTH LABELS ON GRAPHS</head><p>The spectrum of the graph is the set of eigenvalue, eigenvector pairs</p><formula xml:id="formula_8">{(λ i , ψ i )} n+m i=1</formula><p>, where the Laplacian L = i λ i ψ i ψ i . If we sort λ from small to large, then λ 1 = . . . = λ k = 0 if and only if the graph has k disconnected components. The eigenvectors Ψ = {ψ i } form an orthonormal basis. Any target label function on the graph can be decomposed into f = i α i ψ i , where ψ 1 corresponds to the lowest frequency component, and ψ n+m the highest frequency component. The function's energy can be shown to be f Lf = i λ i α 2 i . Existing semi-supervised learning algorithms assume that f is smooth with respect to the graph. This is equivalent to assuming large (non-zero) α's for small i, and small (zero) α's for large i. In the future, one may wish to allow non-smooth labels f to model richer data, which must still be learnable from a small number of labeled points. Compressive sensing offers such guarantee if f is spectrally sparse, i.e., if only S n + m of the α's are non-zero. These S non-zero components can occupy any frequency, thus allowing non-smooth f and generalizing the graph smoothness assumption.</p><p>Our key insight is that transductive learning on graphs corresponds to compressive sensing using the (n + m) × (n + m) canonical basis Φ = I. The n × (n + m) sensing matrix consists of n random rows selected from Φ. The sensing matrix simply reads out the label values at n nodes; these correspond to the n labeled data items. Importantly, when the graph is "nice," i.e., without small (nearly) disconnected components, the graph spectrum basis Ψ is incoherent with the canonical basis Φ. This allows the exact recovery of the whole f from the n observations when n ≥ Cµ 2 (Φ, Ψ)S log(n + m), where µ(Φ, Ψ) = √ n + m max i,j |φ i ψ j | is the coherence (the lower the better).</p><p>We now give a concrete example. Consider a closed chain graph (i.e., a ring) with n+m = 1024 nodes and edge weights 1. The graph spectrum Ψ is the discrete Fourier basis, whose coherence with the canonical basis is √ 2. Let S = 3, and f = −ψ 5 − 1.3ψ 8 + ψ 63 , which is spectrally sparse yet nonsmooth as shown in <ref type="figure">Figure 3(left)</ref>. We take n measurements  <ref type="figure">Fig. 3</ref>. Compressive sensing on a closed-chain graph from the canonical basis (i.e., select labeled items); thus we have labels y = f on those n random nodes. We then solve the standard 1 minimization problem to recoverˆαrecoverˆ recoverˆα (thusˆfthusˆ thusˆf on the whole graph). We vary n from 1 to 60. For each n we run 100 trials; each takes n random rows from Φ to form the sensing matrix. For each trial, we compute the recovery error f − ˆ f 2 /f 2 . Each trial is a dot in <ref type="figure">Figure 3</ref>(right). It seems exact recovery happens when n &gt; 35 for this graph.</p><p>Much work remains in improving this novel way of performing transduction on a graph. Potential research directions include: identifying real-world problems with spectrally sparse labels on graphs, finding bases that are more localized than the Laplacian spectrum yet still incoherent with the canonical sensing basis, and studying label acquisition mechanisms when the sensing basis is not canonical (e.g., random matrices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented three new research directions for graphbased semi-supervised learning and our initial approaches at solving these novel problems. We hope this article will inspire new research, making semi-supervised learning an even more valuable tool for multimedia analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multi-manifold learning with Hellinger-graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Online semi-supervised learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>the unnormalized graph Laplacian matrix (the nor- malized Laplacian D −1/2 LD −1/2 can be used, too). Let f be a function on the graph. Then the graph assumption is equiv- alent to having a small energy</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<editor>Olivier Chapelle, Alexander Zien, and Bernhard Schölkopf</editor>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Sciences, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchi</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th International Conf. on Machine Learning</title>
		<meeting>18th International Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Partially labeled classification with Markov random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schïkopfschïkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System 16</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML-03, 20th International Conference on Machine Learning</title>
		<meeting>ICML-03, 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond the point cloud: from transductive to semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML05, 22nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dissimilarity in graph-based semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by mixed label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-D motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>American Mathematical Society</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to compressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-manifold semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online manifold regularization: A new learning setting and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
