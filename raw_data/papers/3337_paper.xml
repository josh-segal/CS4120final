<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Sampling for Entity Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Yahoo! Research</orgName>
								<orgName type="department" key="dep2">Yahoo! Research</orgName>
								<orgName type="department" key="dep3">Yahoo! Research</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Iyengar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Yahoo! Research</orgName>
								<orgName type="department" key="dep2">Yahoo! Research</orgName>
								<orgName type="department" key="dep3">Yahoo! Research</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Parameswaran</surname></persName>
							<email>adityagp@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Yahoo! Research</orgName>
								<orgName type="department" key="dep2">Yahoo! Research</orgName>
								<orgName type="department" key="dep3">Yahoo! Research</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhor</forename><surname>Rastogi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Yahoo! Research</orgName>
								<orgName type="department" key="dep2">Yahoo! Research</orgName>
								<orgName type="department" key="dep3">Yahoo! Research</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active Sampling for Entity Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or non-duplicates is the one of selecting informative examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0-1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more non-duplicate pairs than duplicate pairs). To address this, a recent work [1] proposes to maximize recall of the classi-fier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst-case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier under precision constraint with provably sub-linear label complexity (under certain distributional assumptions). Our algorithm uses as a black-box any active learning approach that minimizes 0-1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black-box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Entity Matching (EM) is the problem of determining if two entities in a data set refer to the same real-world object. Entity matching is a complex and ubiquitous problem that appears in numerous application domains (including image processing, information extraction and integration, and natural language processing), often under different terminology (e.g., coreference resolution, record linkage, and deduplication <ref type="bibr" target="#b9">[10]</ref>).</p><p>Machine learning approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> for entity matching often learn a classifier over pairs of entities labeling them as duplicate or non-duplicate. The classifier is built over mod-els such as SVM or logistic regression, using features like string similarity. The models are trained over labeled example pairs, which are usually costly to obtain. Active learning techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> are thus used that attempt to carefully select the examples to label while learning a good classifier -one that minimizes the 0-1 loss. These algorithms provide label complexity guarantees, i.e., guarantees on the number of labeled examples required to learn the classifier.</p><p>In entity matching, the number of duplicate pairs is typically small, o(m), for a dataset of m entities. On the other hand, the total number of pairs including non-duplicates can be Θ(m 2 ). While blocking techniques <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16]</ref> are used to reduce the number of pairs considered for entity matching from Θ(m 2 ) to a more manageable number, denoted n, the non-duplicates still vastly outnumber the duplicate pairs, often by a ratio of 100 to 1, as evident in many benchmark entity matching datasets. In such cases, minimizing 0-1 loss is insufficient: a classifier that labels all pairs as non-duplicates has a small 0-1 loss of less than 1%. Thus even this extremely poor classifier is considered good under 0-1 loss.</p><p>To address this, a recent work <ref type="bibr" target="#b0">[1]</ref> considers precision and recall 1 of the classifier in the objective function. Precision in entity matching is the fraction of pairs labeled as duplicates that are true duplicates, and recall is the fraction of true duplicates labeled as duplicates. So the classifier that labels all pairs as non-duplicates, labels no true duplicates as duplicates, and has a recall of 0. Thus the classifier is correctly considered poor under the recall metric.</p><p>[1] gives an active learning algorithm to maximize recall under a constraint that precision should be greater than a specified threshold. The algorithm makes a monotonicity assumption on the precision and recall of the classifier over its parameter space. The algorithm then searches for the optimal classifier essentially through a binary search over the classifiers in the high-dimensional parameter space. Due to this high-dimensional binary search, the algorithm has high label complexity (i.e., many labeled example requested) and computational complexity, and in the worst-case requires the labels of all n input pairs. Furthermore, the monotonicity assumption does not usually hold in practice, and while the algorithm still returns a feasible classifier satisfying the precision constraint, its recall is often poor.</p><p>In this paper, we propose an active learning algorithm for optimizing recall under the precision constraint, using as a black-box any active learning approach that minimizes 0-1 loss. We first use a Langrange multiplier to cast the precision constrained recall optimization problem into an unconstrained one with an objective that is a linear combination of precision and recall. For a fixed value of the Langrange multipier, the linear objective can be optimized using the given black-box. Then we search for the right value of the Langrange multiplier. For this we embed all classifiers in a two dimensional space, and perform a search along the convex hull <ref type="bibr" target="#b17">[18]</ref> of the embedded classifiers.</p><p>A major challenge in our work is to ensure that the search over the embedded classifiers is efficient, and we show that via discretization techniques, we are able to achieve a rather low number of worst-case O(log 2 n) calls to the black-box. We also show that additional calls would not help: our output classifier is guaranteed to be the best one that can be found using any number of calls to the black-box. By comparison, <ref type="bibr" target="#b0">[1]</ref> has exponential (in the number of dimensions) worst-case computational complexity.</p><p>While the classifier output by our algorithm need not be optimal in terms of its recall, we show that it is paretooptimal (no other classifier dominates it in both recall and precision). We also provide additional guarantees on how close our output classifier is to the optimal by showing that they lie on the same edge of the convex hull of the embedded classifiers, and coincide in case the optimal is a vertex point. Contributions and Outline:</p><p>• Our main result is an active learning algorithm that approximately maximizes recall of the classifier under the precision constraint using as a black-box any active learning approach that minimizes 0-1 loss. We show that label complexity of our algorithm is at most O(log 2 n) times the label complexity of the black-box. We also show that the classifier learnt by our algorithm is a pareto-optimal, and close to the true optimal in terms of recall.</p><p>• We use the IWAL active learning algorithm <ref type="bibr" target="#b3">[4]</ref> as the black-box for 0-1 loss minimization. IWAL has been shown to have a sublinear label complexity under certain low noise assumptions. Since our algorithm has a label complexity of at most O(log 2 n) times than that of IWAL, it also has sublinear label complexity under the same low noise assumptions. To our knowledge, this is the first active learning algorithm with sublinear label complexity for optimizing the precision-constrained recall objective.</p><p>• We provide an empirical evaluation of our algorithm on several real-world matching data sets and compare it with the technique in <ref type="bibr" target="#b0">[1]</ref>. We show that our algorithm achieves better quality over these datasets while significantly reducing label and computational complexity. We present background on active learning in Sec. 2, followed by our approach in Sec. 3. We then discuss experimental evaluation in Sec. 5 and conclude in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>In this section, we define the notation and setup the problem formally. We also briefly review the important active learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>Let E be the set of all entities for the matching task, and m be the size of E. We assume that the set of all candidate entity pairs to be matched from E has been generated (say by some blocking technique) and denote it as C. We denote n = |C| the number of candidate pairs. For a pair of entities (e1, e2), we assume that a d-dimensional feature vector x = (x1, x2, . . . , x d ) represents the similarity between entity e1 and e2, and let X be the set of all feature vectors corresponding to pairs in C. We say that the label of x ∈ X, denoted y(x), is 1 if x corresponds to a duplicate pair, and −1 otherwise. In this paper, we consider linear classifiers defined as follows. We let H be the set of all linear classifiers. For any classifier h ∈ H, we define number of false positives, false negatives, true positives as the following:</p><formula xml:id="formula_0">f p(h) = x∈X 1(h(x) = 1 ∧ y(x) = −1) f n(h) = x∈X 1(h(x) = −1 ∧ y(x) = 1) tp(h) = x∈X 1(h(x) = 1 ∧ y(x) = 1)</formula><p>The precision and recall of a classifier are then defined as</p><formula xml:id="formula_1">precison(h) = tp(h) tp(h)+f p(h) and recall(h) = tp(h) tp(h)+f n(h) .</formula><p>Problem. We wish to maximize recall under the precision constraint. Formally, this is stated as below.</p><formula xml:id="formula_2">Problem 1 (Recall). Given τ ∈ [0, 1], find h ∈ H to maximize recall(h) subject to precison(h) ≥ τ</formula><p>The Recall problem is difficult to solve because recall(h) and precison(h) are complicated functions of h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Active Learning</head><p>Now we briefly review the main problems and techniques in active learning. Problems. In active learning, the focus has primarily been to solve the problem of minimizing 0-1-loss. We call this problem as the 01-Loss problem. The goal of the problem 2 is to minimize the total number of false negatives and false positives. We state it below.</p><formula xml:id="formula_3">Problem 2 (01-Loss). Find h ∈ H to minimize f n(h) + f p(h) n</formula><p>A slight generalization of the problem considers a weighted loss function in which false negatives and false positives have different penalties. We denote the problem as 01-LossWeighted and define it below.</p><formula xml:id="formula_4">Problem 3 (01-LossWeighted). Given α ∈ [0, 1], find h ∈ H to minimize αf n(h) + (1 − α)f p(h) n</formula><p>Techniques. One of the first active learning algorithms was given in <ref type="bibr" target="#b6">[7]</ref>. The algorithm solves the 01-Loss problem for separable datasets -datasets for which a classifier exists having 0 empirical 0-1 loss. The paper showed that a good classifier can be learnt for separable datasets using only O(log n) labeled examples. The algorithm was based on the idea of selective sampling: each example point is queried with a probability, computed based on the point and previously labeled examples.</p><p>Since <ref type="bibr" target="#b6">[7]</ref>, several approaches have been proposed to handle non-separable datasets. Most recently, IWAL <ref type="bibr" target="#b3">[4]</ref> proposed an efficient active learning algorithm having sublinear label complexity under some distributional assumptions. The algorithm is again based on selective sampling, and assigns a probability of querying an example based on the disagreement between two classifiers, each learnt on all previously labeled examples, but differing in the labels for the example in interest. The algorithm guarantees the following properties.</p><p>Theorem 2.2 (IWAL <ref type="bibr" target="#b3">[4]</ref>). IWAL approximately minimizes 01-loss using O(error(h * )nθ + √ n log nθ) labeled examples, where error(h * ) is the 01-loss of the optimal classifier, and θ, the disagreement coefficient <ref type="bibr" target="#b11">[12]</ref>, is a parameter depending on the source distribution and the hypothesis class, but independent of the number of points n.</p><p>Under certain distributional assumption <ref type="bibr" target="#b3">[4]</ref>, θ is a small constant and error(h * ) = o(1). Thus the label complexity is provably sublinear under those assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OUR LEARNING ALGORITHMS</head><p>In this section, we describe our approach to solve the Recall problem given a black-box to solve the 01-Loss problem. We use two algorithms: (i) ConvexHull algorithm that approximately solves Recall using a solution for the 01-LossWeighted problem, and (ii) RejectionSampling algorithm that reduces an instance of 01-LossWeighted problem into that of the 01-Loss problem. We describe each of the algorithms below. But we first begin by slightly generalizing the Recall problem. Problem Generalization: The Recall problem maximizes recall under the precision constraint. Maximizing recall(h) of a classifier h is equivalent to minimizing the fraction of false negatives, f n(h)/n. Instead of just minimizing this fraction, we consider here a more general problem that minimizes a linear combination of false negatives and false positives, αf n(h)+(1−α)f p(h) n , under the precision constraint. Further, the constraint on precision can be transformed into a simpler one. Denoting = τ /(1 − τ ), we can write.</p><formula xml:id="formula_5">tp(h) tp(h) + f p(h) ≥ τ ≡ · tp(h) − f p(h) ≥ 0</formula><p>This gives us a generalized version of the Recall problem that we call as RecallWeighted.</p><formula xml:id="formula_6">Problem 4 (RecallWeighted). Given α ∈ [0, 1] and ∈ [0, ∞), find h ∈ H to minimize αf n(h)+(1−α)f p(h) n subject to · tp(h) − f p(h) ≥ 0</formula><p>Note that for α = 0, the solution for RecallWeighted problem is same as that of Recall problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The ConvexHull Algorithm</head><p>Now we describe the ConvexHull algorithm that approximately solves the RecallWeighted problem by repeatedly solving the 01-LossWeighted problem. Note that the objective of both problems is the same, but RecallWeighted has an additional precision constraint. The ConvexHull algorithm essentially removes the constraint using a trick similar to Lagrange multipliers. We describe this below.</p><p>Embedding. We embed a classifier h as a point in two dimensional space with the first coordinate equal to negative of the objective and the second to the slack of the precision constraint, as shown below.</p><formula xml:id="formula_7">α(h) = −(αf n(h) + (1 − α)f p(h)) n β(h) = · tp(h) − f p(h) n</formula><p>Thus, the RecallWeighted problem is equivalent to finding a classifier h that has the highest α(h) under the constraint that β(h) ≥ 0. The 01-LossWeighted problem can be shown to be equivalent to finding a classifier h that maximizes α(h) + λβ(h) for a given λ.</p><p>Let P = {(α(h), β(h)) : h ∈ H} be the set of all twodimensional embeddings of all possible linear classifiers. While P need not be a convex set, we denote C to be the convex hull polytope of points in P and say a classifier h lies on C if (α(h), β(h)) lies on an edge of C. Any classifier h lying on C is pareto-optimal: no other classifier h exists that has both −α(h ) (i.e. objective) as well as β(h ) (i.e. precision) better than h. We will show that ConvexHull algorithm returns classifiers on the convex hull. (Hence the name.)</p><p>Even though the set of all linear classifiers H is exponentially large in the number of dimensions, the size of P is bounded by O(n 3 ), since many classifiers embed to the same point in P . To see this, note that embedded coordinates α(h) and β(h) are functions of f n(h), f p(h), and tp(h), each of which can vary from 0 to n.</p><p>Since H is exponentially large, and P much smaller, we perform our search for optimal classifier in the embedded space. For the search, we define S the set of all possible slopes of lines joining any two points in P , and Λ a sorted array of all possible values λ where −1/λ is a slope in S. Any slope is a ratio of two differences, each with at most O(n) values, and hence both S and Λ have at most O(n 2 ) values. The ConvexHull Algorithm. For the algorithm, we assume a black-box B for the 01-LossWeighted problem that takes a λ and returns a h maximizing α(h) + λβ(h). Theorem 3.1 (Complexity of ConvexHull). The ConvexHull algorithm terminates with a feasible classifier (i.e. classifier satisfying the precision constraint) with label and computation complexity at most 2 log n times that of blackbox B used in the algorithm.</p><p>Proof. Since size of Λ is bounded by O(n 2 ), the the binary search procedure finishes in at most 2 log n iterations. This immediately gives the required label and computational complexity bounds for the algorithm. The feasibility of the solution is guaranteed as a result of the binary search as long as one classifier h exists with β(h) ≥ 0, i.e satisfies the precision constraint. This is always true, for a linear classifier labeling all points as negative.</p><p>On the optimality of the solution: In general, the solution returned by the ConvexHull algorithm need not be optimal. Recall the set C, the convex hull polytope of embedded points. Each classifier on C is pareto-optimal: i.e. no other classifier dominates it on both the value of objective and precision. The optimal classifer is guaranteed to lie on the convex hull, while the algorithm can return some other point on the same edge. Many times the two coincide, and the algorithm does return the optimal classifier. We state this result formally below. Proof. First note that any pareto-optimal classifier lies on the convex hull. If not, then extending the line β = α from the point towards the convex hull results in a classifier dominating it in both objective value as well as precision. Since optimal is also pareto-optimal, it has to lie on the convex hull.</p><p>Next we show that the black-box B(λ) also returns a h on the convex hull C. Assume the contrary, and suppose it returns a classifier h not on the convex hull. Then for h, there is a point on an edge AB of the convex hull that strictly dominates it in terms of both objective value and precision. 1: Input: A set of n points X, A black-box B for 01-Loss problem 2: Output: A solution for 01-LossWeighted problem 3: Let ¯ x = (), ¯ y = () be empty sequences. 4: for each point x in X do 5:</p><p>If B(¯ x, ¯ y, x) = FALSE, continue to next point. 6:</p><p>Otherwise, query the point x. Let y be its label 7: Toss a coin with success probability α if y = 1 and 1 − α if y = −1 8:</p><p>If the coin returns failure, continue to next point 9:</p><p>Otherwise, add x and y to ¯ x and ¯ y respectively. 10: end for 11: return B h (¯ x, ¯ y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: RejectionSampling Algorithm</head><p>Then one of the two vertices, either A or B, has as good or better α + λβ value, contradicting the assumption. Furthermore, we show that h = B(λ) lies on an edge AB, then its slope has to be −1/λ. This follows directly from linearity of the objective function α + λβ. Thus our black box function gives us, for each λ, either a vertex of the convex hull, or a point on the line of the convex hull that has slope −1/λ. Let λ0 be the smallest λ for which h = B(λ) is feasible, i.e. β(h) ≥ 0. If −1/λ0 is a slope between two existing edges in the convex hull, then the binary search of the ConvexHull algorithm would give the vertex corresponding to λ0. Furthermore, that vertex will be optimal. If −1/λ0 is the slope of one of the edges (say AB) of the convex hull, then the binary search find λ0 and B(λ0) will return a point on AB. Also the optimal will be on the same edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The RejectionSampling Algorithm</head><p>In this section we describe the RejectionSampling algorithm that reduces the 01-LossWeighted problem into an instance of 01-Loss problem. Recall that the difference in the two problems is that while the former minimizes any linear combination, αf n(h)+(1−α)f p(h) n , of the false negatives and false positives, the latter only minimizes their sum. Both the problems are however unconstrained. Our algorithm is very similar to idea of rejection sampling used for transforming cost-sensitive binary classification into the standard setting <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref>. Our analysis is slightly different as we use empirical loss functions rather than distribution ones. Black-box We assume a black-box B for solving the 01-Loss problem. We further assume that B reads all input points one by one, maintaining some internal state as it is reading the points, which it uses to determine whether or not to ask the label for the next point. We model this behavior as follows: B accepts as input a sequence of points ¯ x = x1, . . . , x k along with their labels ¯ y = y1, . . . , y k , and a new point x, and returns true or false indicating whether or not to query the label for this point. Once all points have been considered, a function B h accepts all the points ¯ x for which labels were asked, along with all their labels ¯ y, and returns the classifier h minimizing 01-loss. Note that the label complexity of the black-box is exactly the size of sequence ¯ x, since those are points that the black-box decided to query.</p><p>Since the 01-LossWeighted problem has a penalty α for false negative and 1 − α for false positive, one can use B for solving the 01-LossWeighted problem if the distribution of false negatives and false positives is changed appropriately. This can be done simply by rejecting a positive point with probability 1 − α and a negative point with probability α. However, given just a point we do not know its label, and thus can not decide its rejection probability. To overcome this we query B to check whether or not query the point's label, and if it returns true, we use the label to set the rejection probability. This procedure works, but since we are rejecting even labeled points, the label complexity takes a hit. The algorithm formally corresponding to the above intuition is defined in in Algorithm 2. It begins with empty sequences ¯ x, ¯ y for points and their labels. It then repeatedly picks a new point, and uses the black-box to determine whether or not to query the point's label. While doing this it gives the black-box all points ¯ x and their labels ¯ y. If the black-box decides not to query the point, it is ignored and the algorithm moves to the next point. Otherwise, the points label is determined, and used to determine the rejection probability. If the point is rejected, then it is ignored even though its label has already been queried. Otherwise, the point is added to the sequence of labeled points ¯ x. Finally, the black-box is fed all the labeled points in ¯ x and their labels, and the returned classifier is output.</p><p>Since RejectionSampling algorithm is randomized, we can only show probability bounds on its optimality and label complexity. The following theorem shows that w.h.p the objective value of the returned classifier is within O(1/ √ n) away from the optimal. Additionally, it shows that label complexity is bounded in expectation. The same label complexity bound can be shown to hold w.h.p.</p><p>Theorem 3.3. Let hrs be solution returned by the RejectionSampling algorithm. Then with probability at least 1 − δ, the difference in the objective value of hrs and the optimal is at most O( log δ/n). Furthermore, the expected label complexity of the algorithm is at most max( α Proof. Denote F N (h), F P (h) the set of false negatives and false positives for a classifier h. Since some false positives and false negatives are rejected in a random run of the RejectionSampling algorithm, denote for a point x, r(x) the random variable equal to 1 if x is selected in RejectionSampling, and 0 if it is rejected.</p><p>The black-box B essentially minimizes the objective Obj(h) equal to</p><p>x:F N (h) r(x)/n + x :F P (h) r(x )/n. Also expectation E(h) of Obj(h) over the coin tosses of the algorithm is simply αf n(h)+(1−α)f p(h) n , which is the right objective for the 01-LossWeighted problem. Using Hoeffding, we can say that w.p. 1 − δ, Obj(h) deviates from its expectation E(h), by at most O( log δ/n). This shows that If h * is an optimal classifier for the 01-LossWeighted problem, w.p. 1 − δ, |Obj(h * ) − E(Obj(h * )| is bounded by O( log δ/n). Similarly, w.p. 1−δ, |Obj(hrs)− E(Obj(hrs)| is bounded by O( log δ/n). Now we know that since the black-box returned hrs, Obj(hrs) ≤ Obj(h * ). Since h * is optimal for the 01-LossWeighted problem, and E(h * ) the objective for the same, we know E(h * ) ≤ E(hrs). This shows that difference in Obj(hrs) and E(h * ) is no more than O( log δ/n). Hence proved.</p><p>The proof of expected label complexity follows directly from the expected number of labeled examples rejected by the RejectionSampling algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Approach</head><p>Now we describe our overall approach. We do not introduce new techniques here, but explain and analyze how the various algorithms are run together. We begin by running the ConvexHull algorithm for solving the Recall problem. During its run, ConvexHull might make calls to its black-box for solving the 01-LossWeighted problem. Whenever a call is made, we invoke a run of the RejectionSampling algorithm. When that in turn makes a call to its black-box for solving the 01-Loss problem, we invoke a run of the IWAL algorithm <ref type="bibr" target="#b3">[4]</ref>. We describe this process in detail below. We also compute the label complexity of the overall process. Run of ConvexHull algorithm: We use ConvexHull algorithm to solve an instance of the RecallWeighted problem with α = 1 − 1/ log n. Note that the objective that we are optimizing is then (1−1/ log n)f n(h)+1/ log nf p(h) n , instead of f n(n) n that we need to do for solving the Recall problem. However, since α = 1 − 1/ log n is very close to 1, the two objectives can be shown to be at most 1/ log n away from each other. Thus an approximate solution for RecallWeighted for this α is a good approximate solution for the Recall problem. Here we solved RecallWeighted instead of Recall to ensure bounded labeled complexity when running the RejectionSampling algorithm, which we discuss next. Run of RejectionSampling algorithm: During the above run of ConvexHull algorithm, whenever a call to the blackbox for solving the 01-LossWeighted problem is made, we invoke the RejectionSampling algorithm. Note that the objective for the 01-LossWeighted problem is given by α(h) + λβ(h), which expands to (1 − 1/ log n + λ)f nh + (1/ log n + λ)f p(h) n Thus RejectionSampling algorithm solves an instance of 01-LossWeighted problem with</p><formula xml:id="formula_8">α = 1 − 1/ log n + λ 1 + (1 + )λ</formula><p>This α determines the rejection probabilities and the label complexity of the RejectionSampling algorithm. Finally, when the run is complete, we return the output classifier to the ConvexHull algorithm Run of IWAL algorithm <ref type="bibr" target="#b3">[4]</ref>: During the above run of RejectionSampling algorithm, whenever a call to the blackbox for solving the 01-Loss problem is made, we invoke the IWAL algorithm, which is described in detail in <ref type="bibr" target="#b3">[4]</ref>. We only use it as a black-box here, and return its output to the RejectionSampling algorithm. Now we reason about the total number of examples that are queried in the above process.</p><p>Theorem 3.4 (Overall label complexity). The label complexity of our overall approach is at most O(log 2 n) times that of the IWAL algorithm. Since IWAL algorithm has a label complexity of O(error(h * )nθ + √ n log nθ), the overall label complexity is O(2 log 2 n error(h * )nθ + √ n log nθ ).</p><p>Under certain distributional assumptions <ref type="bibr" target="#b3">[4]</ref>, this label complexity is sublinear.</p><p>Proof. From theorem 3.1, the label complexity of ConvexHull is 2 log n times that of the black-box RejectionSampling used in the algorithm. Further, since α = 1−1/ log n+λ 1+(1+)λ , and 1 − α = 1/ log n+λ 1+(1+)λ , we can show that max(1/α , 1/(1 − α ) ≤ max(log n,</p><formula xml:id="formula_9">1 + , 1 1 + )</formula><p>Since is a constant independent of n, max(1/α , 1/(1 − α ) is O(log n). Thus from Theorem 3.3, the label complexity of RejectionSampling is at most O(log n) times the label complexity of the black-box IWAL used. Finally, since the IWAL complexity is O(error(h * )nθ + √ n log nθ) as shown in Theorem 2.2, we get the required result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>The work related to us can be placed under three categories. We describe each of them in turn. Active learning for entity matching. The previous work most similar to ours is <ref type="bibr" target="#b0">[1]</ref>, which also provides an active learning algorithm to maximize recall under a constraint that precision should be greater than a specified threshold. This objective function is especially suitable for the class imbalanced entity matching problem. However, their algorithm makes a monotonicity assumption on the precision and recall of the classifier over its parameter space. The algorithm then searches for the optimal classifier essentially through a binary search along each dimension in the highdimensional parameter space. Due to this high-dimensional search, the algorithm has high label and computational complexity, and in the worst-case requires the labels of all n input pairs, as well as exponential in dimensions computational complexity. Furthermore, the monotonicity assumption does not usually hold in practice, and while the algorithm still returns a feasible classifier satisfying the precision constraint, its recall is often poor. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> also study the problem of using active learning for deduplication. However, the approaches proposed does not provide any guarantees in terms of precision or recall, which is undesirable for highly imbalanced datasets. <ref type="bibr" target="#b20">[21]</ref> considers the problem of classification when the people providing the labels may be noisy and then studies the tradeoff between requesting the same label again versus requesting new labels. This work is not applicable to our setting since we use inhouse experts to provide labels for each pair requested. Active learning for general classification. Almost all related work on active learning for the general binary classification problem focuses on 0-1 loss minimization. One of the first active learning algorithms was given in <ref type="bibr" target="#b6">[7]</ref>. The algorithm assumes separability of the datasets, i.e. it assumes that a classifier exists having 0 empirical 0-1 loss. The paper showed that a good classifier can be learnt for separable problems using only O(log n) labeled examples. The algorithm was based on the idea of selective sampling: each example point is queried with a probability, computed based on the point, and also on previously labeled examples.</p><p>Since <ref type="bibr" target="#b6">[7]</ref>, several recent approaches have been proposed to handle non-separable datasets. Most recently, IWAL <ref type="bibr" target="#b3">[4]</ref> proposed an efficient active learning algorithm having sublinear label complexity under some distributional assumptions. The algorithm is again based on selective sampling, and assigns a probability of querying an example based on the disagreement between two classifiers, each learnt on all previously labeled examples, but differing in the labels for the example in interest.</p><p>To our knowledge, <ref type="bibr" target="#b2">[3]</ref> is the only active learning algorithm that minimizes general loss functions in addition to 0-1 loss. However, the algorithm can not perform constrained optimization required for ensuring our precision constraint. Furthermore, no direct efficient implementation of algorithm is known for the class of linear classifiers. Previous approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref> also required maintaining a candidate set of hypotheses (called a version space), which is computationally infeasible.</p><p>For a good overview of active learning, see <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr" target="#b8">[9]</ref> provides a good summary of recent work in the area. Entity Matching. Many techniques have been proposed for the entity matching problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>. The ones most relevant to us are learning-based techniques that train a classifier over labeled pairs of examples. These include naive bayes <ref type="bibr" target="#b23">[24]</ref>, decision trees <ref type="bibr" target="#b5">[6]</ref>, SVMs <ref type="bibr" target="#b4">[5]</ref>. All of these techniques are fully supervised, i.e., they do not try to reduce the label complexity by choosing the pairs whose labels to request. To reduce the number of labeled examples, <ref type="bibr" target="#b13">[14]</ref> describes an algorithm that uses a heuristic string similarity function and then samples pairs having varied similarity scores. The algorithm can not directly be applied for active learning as it is only for training data selection, but can in fact be used in conjunction with our active learning algorithm as a preprocessing step to select the pool of candidate pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we describe our experimental setup (Section 5.1) and present results comparing our approach against a previous state-of-the-art algorithm <ref type="bibr" target="#b0">[1]</ref> on several real-world datasets (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Data Sets</head><p>A brief description of the four real-world datasets used in our experiments is shown in <ref type="table">Table 1</ref>. These include:</p><p>• Business: This is a dataset of local business listings used in the production system at Yahoo!. Each listing contain attributes like name, street, phone, etc. of the business. A set of labeled pairs is obtained as follows: we tokenize the name and street attributes into sets of k-grams. We then do blocking using a combination of prefix-filtering and min-hash to select pairs of entities that have a Jaccard overlap of more than 0.2 on the set of k-grams for either name or street attributes. Then using a heuristic matching function that gives a score to each pair, we select pairs having varied values for the scores, similar to the technique in <ref type="bibr" target="#b13">[14]</ref>. Finally, the selected pairs are judged by human editors to generate a dataset of 3958 labeled pairs.</p><p>• Person: This is a record linkage dataset from the UCI machine learning repository <ref type="bibr" target="#b10">[11]</ref>. It contains 574913 pairs for which features have been computed and labels have been provided.</p><p>• DBLP-ACM <ref type="bibr" target="#b13">[14]</ref>: This is a large bibliography record link-  <ref type="table">Table 1</ref>: Description of datasets used in our experiments. The blocking functions used to reduce the number of pairwise comparisons during matching are Jaccard similarity (Jacc), string equality (EQ), phonetic equality (SoundEQ) and trigram Jaccard similarity (Char3Jacc). The number of similarity features is shown in the last column.</p><p>age task in which 494437 matching records in dblp and acm have been manually labeled as either duplicates or non-duplicates.</p><p>• Scholar-DBLP <ref type="bibr" target="#b13">[14]</ref>: This is a dataset of 589326 pairs similar to dblp-acm. It however includes automatically extracted records from google scholar, which cause data quality problems and make matching harder. The last three datasets are highly imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Algorithms</head><p>We implement three algorithms for active learning.</p><p>• monotone: This is our implementation of the algorithm described in <ref type="bibr" target="#b0">[1]</ref>. The precision evaluations required for the algorithm is done over random samples of size 100.</p><p>To ensure same examples be sampled for each precision evaluation whenever possible, a random permutation is chosen and fixed. Then each random sample is generated by selecting the first 100 applicable examples from this permutation.</p><p>• vw: This is an implementation of the IWAL algorithm <ref type="bibr" target="#b3">[4]</ref> obtained from the open source Vowpal Wabbit <ref type="bibr" target="#b12">[13]</ref>. The algorithm uses stochastic gradient descent to learn a linear classifier. The 01-loss is approximated as squaredloss for efficient implementation.</p><p>• cvhull: This is an implementation of both ConvexHull and RejectionSampling algorithms on top of IWAL as described in Sec. 3.3. Whenever ConvexHull calls its black-box, an implementation of RejectionSampling is invoked. When RejectionSampling calls for its black-box, vw is invoked. The vw algorithm reads examples one at a time, which we feed to the algorithm in a randomly chosen order kept fixed across its different invokation. The precision constraint check required in ConvexHull is implemented by a precision evaluation of over random samples of size 100, generated in the same manner as in monotone. Note that all of our datasets consist of labeled examples. The active learning algorithms are run on the datasets as follows: as the active learning algorithm reads the examples the labels are kept hidden. Whenever the algorithm requires a label for a specific example, its label is read from the dataset and given to the algorithm. This cuts out the need for a human involvement of labeling during our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In <ref type="figure">Fig. 1</ref>, we report the F-1 achieved by monotone and cvhull on all datasets, as the threshold used for the precision constraint is varied. We do not report the F-1 for vw, since it returns classifier that often violates the precision constraint. The graphs show than cvhull has consistently higher F-1 that monotone over all datasets and precision thresholds. The difference in F-1 between the two algorithms is as big as 0.15 in some cases, but becomes smaller (around 0.05 on average) for highest precision threshold of 0.9. While this difference is still significant, it does indicate that the gains at high precision thresholds are limited, possibly owing to limited choices of classifiers satisfying the precision constraint.</p><p>In <ref type="figure">Fig. 2</ref>, we report the number of queries required by monotone and cvhull on all datasets, as the threshold used for the precision constraint is varied. We again do not report the number for vw, since it returns classifier that often violates the precision constraint. The graphs show that cvhull requires significantly lower number of queries than monotone on all but 2 cases. The difference in number of examples is particularly stark for Person and Scholar-DBLP datasets, sometimes more than 3000 examples. Particularly attractive property of cvhull is that it learns the classifier in around 500 points for all but one datasets and all threshold values.</p><p>In <ref type="figure" target="#fig_5">Fig. 3</ref>, we compare the computation time for the two algorithms monotone and cvhull as the number of dimensions is varied for the Person dataset. The figure clearly demonstrates that monotone has an exponential complexity w.r.t number of dimensions, while cvhull has an almost a constant dependence.</p><p>In <ref type="figure">Fig. 4</ref>, we show that that vw often fails to produce a feasible classifier, i.e. one that satisfies the precision constraint. The graph plots the success rate, i.e. the fraction of times, over 10 random runs, the algorithm outputs a feasible classifier. Obviously as the precision threshold is increased, number of feasible classifiers decrease, and so does the success rate. This graph demonstrates the need for the ConvexHull and RejectionSampling algorithms used in cvhull on top of vw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we proposed an active learning algorithm for the entity matching problem. The algorithm tries to learn a classifier with maximum recall under a constraint that its precision should be greater than threshold. The algorithm uses any of the existing active learning technique for minimizing 01-loss as a black-box. We showed that the algorithm outputs a classifier having recall close to the optimal, and has good label and computation complexity. We also compared the algorithm against the state-of-the-art active learning algorithm for entity matching, and show that we outperform it in terms of metrics such as F1 of the trained classifier, number of labeled examples required, and computation time on several real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 1 .</head><label>21</label><figDesc>A linear classifier h is represented by a d-dimensional vector w = (w1, . . . , w d ). h classifies a feature vector x as positive, i.e. h(x) = 1 if w.x ≥ 0 and negative, i.e. h(x) = −1, otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The algorithm, shown in Algorithm 1, essentially does a binary search over values in Λ. Starting from the two extreme ele- ments Λ[min], Λ[max], the algorithm repeatedly picks their midpoint λ = Λ[(max + min)/2], and then computes a clas- sifier h maximizing α(h) + λβ(h) using the black-box B. Depending on whether β(h) ≥ 0 or not, the intervals are 1: Input: A set of n pairs X, Black-box B(λ) returning h with max α(h) + λβ(h) 2: Output: Approximate solution for RecallW eighted 3: Compute sorted list Λ of all values λ, s.t. −1/λ is a line slope in the embedded space. 4: Set min = 0, max = |Λ|. 5: while min &lt; max do 6: Set mid = (min + max)≥ 0 set max = mid, else set min = mid 10: end while 11: return h Algorithm 1: ConvexHull Algorithm updated to ensure that we always have at least one extreme point that satisfies feasibility requirement β(h) ≥ 0. Finally the algorithm terminates with a feasible solution, i.e. a clas- sifier h with β(h) ≥ 0. We can show following additional properties of the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 . 2 .</head><label>32</label><figDesc>The optimal solution lies on the convex hull C. The ConvexHull algorithm returns a classifier on the same edge of the convex hull. If the optimal is a vertex point of C, then the algorithm returns the optimal solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>times the label complexity of the black-box B for the 01-Loss problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The computational complexity of the two algorithms as the dimension is varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :Figure 2 :Figure 4 :</head><label>124</label><figDesc>Figure 1: The effect of varying the precision threshold on the F-measure for different datasets.</figDesc></figure>

			<note place="foot" n="1"> That work actually considers an approximation of recall metric, but we ignore this distinction for now.</note>

			<note place="foot" n="2"> Learning literature considers a more difficult version of 01-Loss problem when the examples come from an unknown distribution, here we consider the simpler problem using the empirical distribution, for which the same techniques apply.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On active learning of record matching packages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Götz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Kaushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Importance weighted active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Agnostic active learning without constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive duplicate detection using learnable string similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Example-driven design of efficient record matching queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surajit</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee-Chung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Kaushik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international conference on Very large data bases, VLDB &apos;07</title>
		<meeting>the 33rd international conference on Very large data bases, VLDB &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="327" to="338" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Generalization with Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general agnostic active learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tutorial summary: Active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">178</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Duplicate record detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><forename type="middle">S</forename><surname>Verykios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online importance weight aware updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training selection for tuning entity matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Köpcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">QDB/MUD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Frameworks for entity matching: A comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Köpcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient clustering of high-dimensional data sets with application to reference matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;00</title>
		<meeting>the sixth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cost-Sensitive Binary Classification and Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computational geometry: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Preparata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shamos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive deduplication using active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Computer Sciences Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Get another label? improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning object identification rules for information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="607" to="633" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entity resolution with iterative blocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">Euijong</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Menestrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Koutrika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th SIGMOD international conference on Management of data, SIGMOD &apos;09</title>
		<meeting>the 35th SIGMOD international conference on Management of data, SIGMOD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="219" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The state of record linkage and current research problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>U.S. Census Bureau</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Statistical Research Division</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cost-Sensitive Learning by Cost-Proportionate Example Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
