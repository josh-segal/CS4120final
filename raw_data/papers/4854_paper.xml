<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The ZCache: Decoupling Ways and Associativity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
							<email>sanchezd@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<email>kozyraki@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The ZCache: Decoupling Ways and Associativity</title>
					</analytic>
					<monogr>
						<title level="m">Appears in the Proceedings of the 43rd Annual IEEE/ACM Symposium on Microarchitecture (MICRO-43)</title>
						<imprint>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale CMP with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As Moore's law enables chip-multiprocessors (CMPs) with tens and hundreds of cores <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref>, the limited bandwidth, high latency, and high energy of main memory accesses become an important limitation to scalability. To mitigate this bottleneck, CMPs rely on complex memory hierarchies with large and highly associative caches, which commonly take more than 50% of chip area and contribute significantly to static and dynamic power consumption <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>The goal of this work is to improve the efficiency of cache associativity. Higher associativity provides more flexibility in block (re)placement and allows us to utilize the limited cache capacity in the best possible manner. Last-level caches in existing CMPs are already highly associative and the trend is to increase the number of ways with core count. Moreover, several architectural proposals rely on highly associative caches. For example, many designs for transactional memory and thread-level speculation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, deterministic replay <ref type="bibr" target="#b41">[42]</ref>, event monitoring and user-level interrupts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>, and even memory consistency implementations <ref type="bibr" target="#b11">[12]</ref> use caches to buffer or pin specific blocks. Low associativity makes it difficult to buffer large sets of blocks, limiting the applicability of these schemes or requiring expensive fall-back mechanisms.</p><p>Conventional caches improve associativity by increasing the number of physical ways. Unfortunately, this also increases the latency and energy cost of cache hits, placing a stringent trade-off on cache design. For example, this trade-off limits the associativity of first-level caches in most chips to two or four ways. For last-level caches, a 32-way set-associative cache has up to 3.3× the energy per hit and is 32% slower than a 4-way design. Most alternative approaches to improve associativity rely on increasing the number of locations where a block can be placed (with e.g. multiple locations per way <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37]</ref>, victim caches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> or extra levels of indirection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>). Increasing the number of possible locations of a block ultimately increases the energy and latency of cache hits, and many of these schemes are more complex than conventional cache arrays (requiring e.g. heaps <ref type="bibr" target="#b2">[3]</ref>, hashtable-like arrays <ref type="bibr" target="#b17">[18]</ref> or predictors <ref type="bibr" target="#b9">[10]</ref>). Alternatively, hashing can be used to index the cache, spreading out accesses and avoiding worst-case access patterns <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>. While hashingbased schemes improve performance, they are still limited by the number of locations that a block can be in.</p><p>In this paper, we propose a novel cache design that achieves arbitrarily high associativity with a small number of physical ways, breaking the trade-off between associativity and access latency or energy. The design is motivated by the observation that associativity is the ability of a cache to select a good block to evict on a replacement. For instance, assuming an access pattern with high temporal locality, the best block to evict is the least recently used one in the entire cache. For a transactional memory system, the best block to evict is one that does not store transactional metadata. A cache that provides a higher quality stream of evicted blocks essentially has higher associativity, regardless of the number of ways it uses and the number of locations each block can be placed in.</p><p>Our three main contributions are: 1) We propose zcache, a cache design that improves associativity while keeping the number of possible locations (i.e. ways) of each block small. The zcache's design is based on the insight that associativity is not determined by the number of locations that a block can reside in, but by the number of replacement candidates on an eviction. Like a skew-associative cache <ref type="bibr" target="#b38">[39]</ref>, a zcache accesses each way using a different hash function. A block can be in only one location per way, so hits, the common case, require only a single lookup. On a replacement, the zcache exploits that with different hash functions, a block that conflicts with the incoming block can be moved to a non-conflicting location in another way instead of being evicted to accommodate the new block. This is similar to cuckoo hashing <ref type="bibr" target="#b34">[35]</ref>, a technique to build space-efficient hash tables. On a miss, the zcache walks the tag array to obtain additional replacement candidates, evicts the best one, and performs a series of relocations to accommodate the incoming block. This happens off the critical path, concurrently with the miss and other lookups, so it has no effect on access latency. 2) We develop a novel analysis framework to understand associativity and compare the associativities of different cache designs independently of the replacement policy. We define associativity as a probability distribution and show that, under a set of conditions, which are met by zcaches, associativity depends only on the number of replacement candidates. Therefore, we prove that the zcache decouples associativity from the number of ways (or locations that a block can be in). 3) We evaluate a first use of zcaches at the last-level cache of the CMP's memory hierarchy. Using the analytical framework we show that, for the same number of ways, zcaches provide higher associativity than set-associative caches for most workloads. We also simulate a variety of multithreaded and multiprogrammed workloads on a large-scale CMP, and show that zcaches achieve the benefits of highly-associative caches without increasing access latency or energy. For example, over a set of 10 miss-intensive workloads, a 4-way zcache provides 7% higher IPC and 10% better energy efficiency than a 32-way set-associative cache. The rest of the paper is organized as follows. Section II gives the necessary background on approaches to increase cache associativity. Section III presents the zcache design. Section IV develops the theoretical framework to understand and analyze associativity. Section V discusses our evaluation methodology, and Section VI presents the evaluation of the zcache as a lastlevel cache. Section VII discusses additional related work, and Section VIII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND ON CACHE ASSOCIATIVITY</head><p>Apart from simply increasing the number of ways in a cache and checking them in parallel, there is abundant prior work on alternative schemes to improve associativity. They mainly rely on either using hash functions to spread out cache accesses, or increasing the number of locations that a block can be in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hashing-based Approaches</head><p>Hash block address: Instead of using a subset of the block address bits as the cache index, we can use a better hash function on the address to compute the index. Hashing spreads out access patterns that are otherwise pathological, such as strided accesses that always map to the same set. Hashing slightly increases access latency as well as area and power overheads due to this additional circuitry. It also adds tag store overheads, since the full block address needs to be stored in the tag. Simple hash functions have been shown to perform well <ref type="bibr" target="#b25">[26]</ref>, and some commercial processors implement this technique in their last-level cache <ref type="bibr" target="#b40">[41]</ref>. Skew-associative caches: Skew-associative caches <ref type="bibr" target="#b38">[39]</ref> index each way with a different hash function. A specific block address conflicts with a fixed set of blocks, but those blocks conflict with other addresses on other ways, further spreading out conflicts. Skew-associative caches typically exhibit lower conflict misses and higher utilization than a set-associative cache with the same number of ways <ref type="bibr" target="#b6">[7]</ref>. However, they break the concept of a set, so they cannot use replacement policy implementations that rely on set ordering (e.g. using pseudo-LRU to approximate LRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Approaches that Increase the Number of Locations</head><p>Allow multiple locations per way: Column-associative caches <ref type="bibr" target="#b0">[1]</ref> extend direct-mapped caches to allow a block to reside in two locations based on two (primary and secondary) hash functions. Lookups check the second location if the first is a miss and a rehash bit indicates that a block in the set is in its secondary location. To improve access latency, a hit in a secondary location causes the primary and secondary locations to be swapped. This scheme has been extended with better ways to predict which location to probe first <ref type="bibr" target="#b9">[10]</ref>, higher associativities <ref type="bibr" target="#b44">[45]</ref>, and schemes that explicitly identify the less used sets and use them to store the more used ones <ref type="bibr" target="#b36">[37]</ref>. The drawbacks of allowing multiple locations per way are the variable hit latency and reduced cache bandwidth due to multiple lookups, and the additional energy required to do swaps on hits. Use a victim cache: A victim cache is a highly or fullyassociative small cache that stores blocks evicted from the main cache until they are either evicted or re-referenced <ref type="bibr" target="#b24">[25]</ref>. It avoids conflict misses that are re-referenced after a short period, but works poorly with a sizable amount of conflict misses in several hot ways <ref type="bibr" target="#b8">[9]</ref>. Scavenger <ref type="bibr" target="#b2">[3]</ref> divides cache space into two equally large parts, a conventional set-associative cache and a fully-associative victim cache organized as a heap. Victim cache designs work well as long as misses in the main cache are rare. On a miss in the main cache, they introduce additional latency and energy consumption to check the victim cache, regardless of whether the victim cache holds the block. Use indirection in the tag array: An alternative strategy is to implement tag and data arrays separately, making the tag array highly associative, and having it contain pointers to a non-associative data array. The Indirect Index Cache (IIC) <ref type="bibr" target="#b17">[18]</ref> implements the tag array as a hash table using openchained hashing for high associativity. The V-Way cache <ref type="bibr" target="#b35">[36]</ref> implements a conventional set-associative tag array, but makes it larger than the data array to make conflict misses rare. Tag indirection schemes suffer from extra hit latency, as they are forced to serialize accesses to the tag and data arrays. Both the IIC and the V-Way cache have tag array overheads of around 2×, and the IIC has a variable hit latency.</p><p>Several of these designs both increase cache associativity and propose a new replacement policy, sometimes tailored to the proposed design <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. This makes it difficult to elucidate how much improvement is due to the higher associativity and how much depends on the better replacement policy. In this work we consider that associativity and replacement policy are separate issues, and focus on associativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE ZCACHE DESIGN</head><p>Structurally, the zcache shares many common elements with the skew-associative cache. Each way is indexed by a different hash function, and a cache block can only reside in a single position on each way. That position is given by the hash value of the block's address. Hits happen exactly as in the skew-associative cache, requiring a single lookup to a small number of ways. On a miss, however, the zcache exploits the fact that two blocks that conflict on a way often do not conflict on the other ways to increase the number of replacement candidates. The zcache performs a replacement over multiple steps. First, it walks the tag array to identify the set of replacement candidates. It then picks the candidate preferred by the replacement policy (e.g. least recently used block for LRU), and evicts it. Finally, it performs a series of relocations to be able to accommodate the incoming block at the right location.</p><p>The multi-step replacement process happens while fetching the incoming block from the memory hierarchy, and does not affect the time required to serve the miss. In non-blocking caches, simultaneous lookups happen concurrently with this process. The downside is that the replacement process requires extra bandwidth, especially on the tag array, and needs extra energy. However, should bandwidth or energy become an issue, the replacement process can be stopped early, simply resulting in a worse replacement candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Operation</head><p>We explain the operation of the replacement process in detail using the example in <ref type="figure" target="#fig_1">Fig. 1</ref>. The example uses a small 3-way cache with 8 lines per way. Letters A-Z denote cache blocks, and numbers denote hash values. <ref type="figure" target="#fig_1">Fig. 1g</ref> shows the timeline of reads and writes to the tag and data arrays, and the memory bus. Throughout <ref type="figure" target="#fig_1">Fig. 1, addresses</ref> and hash values obtained in the same access share the same color. Walk: <ref type="figure" target="#fig_1">Fig. 1a</ref> shows the initial contents of the cache and the miss for address Y that triggers the process. Initially, the addresses returned by the tag lookup for Y are our only replacement candidates for the incoming block (addresses A, D and M). These are the first-level candidates. A skewassociative cache would only consider these candidates. In a zcache, the controller starts the walk to expand the number of candidates by computing the hash values of these addresses, shown in <ref type="figure" target="#fig_1">Fig. 1b</ref>. One of the hash values always matches the hash value of the incoming block. The others denote the positions in the array where we could move each of our current replacement candidates to accommodate the incoming block. For example, as column A in <ref type="figure" target="#fig_1">Fig. 1b</ref> shows, we could move block A to line 2 in way 1 (evicting K) or line 1 in way 2 (evicting X) and write incoming block Y in line 5 of way 0.</p><p>We take the six non-matching hash values in <ref type="figure" target="#fig_1">Fig. 1b</ref> and perform two accesses, giving us an additional set of six second-level replacement candidates, as shown in <ref type="figure" target="#fig_1">Fig. 1c</ref> (addresses B, K, X, P, Z, and S). We can repeat this process (which, at its core, is a breadth-first graph walk) indefinitely, getting more and more replacement candidates. In practice, we eventually need to stop the walk and select the best candidate found so far. In this example, we expand up to a third level, reaching 21 (3+6+12) replacement candidates. In general, it is not necessary to obtain full levels. <ref type="figure" target="#fig_1">Fig. 1d</ref> shows a tree with the three levels of candidates. Note how, in expanding the second level, some hash values are repeated and lead to the same address. These repeats are bound to happen in this small example, but are very rare in larger caches with hundreds of blocks per way. Relocations: Once the walk finishes, the replacement policy chooses the best replacement candidate. We discuss the implementation of replacement policies in Section III-E. In our example, block N is the best candidate, as shown in <ref type="figure" target="#fig_1">Fig. 1d</ref>. To accommodate the incoming block Y, the zcache evicts N and relocates its ancestors in the tree (both data and tags), as shown in <ref type="figure" target="#fig_1">Fig. 1e</ref>. This involves reading and writing the tags and data to their new locations, as the timeline in <ref type="figure" target="#fig_1">Fig. 1g</ref> indicates. <ref type="figure" target="#fig_1">Fig. 1f</ref> shows the contents of the cache after the replacement process is finished, with N evicted and Y in the cache. Note how N and Y both used way 0, but completely different locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General figures of merit</head><p>A zcache with W ways where the walk is limited to L levels has the following figures of merit:</p><p>• Replacement candidates (R): Assuming no repeats when expanding the tree,</p><formula xml:id="formula_0">R = W 񮽙 L−1 l=0 (W − 1) l .</formula><p>• Replacement process energy (E miss ): If the energies to read/write tag or data in a single way are denoted E rt , E wt , E rd and E wd , then</p><formula xml:id="formula_1">E miss = E walk + E relocs = R × E rt + m × (E rt + E rd + E wt + E wd )</formula><p>, where m ∈ {0, .., L − 1} is the number of relocations. Note that reads and writes to the data array, which consume most of the energy, grow with L, i.e. logarithmically with R.</p><p>• Replacement process latency: Because accesses in a walk can be pipelined, the latency of a walk grows with the number of levels, unless there are so many accesses on each level that they fully cover the latency of a tag array read:</p><formula xml:id="formula_2">T walk = 񮽙 L−1 l=0 max(T tag , (W − 1) l )</formula><p>. This means that, for W &gt; 2, we can get tens of candidates in a small amount of delay. For example, <ref type="figure" target="#fig_1">Fig. 1g</ref> assumes a tag read delay of 4 cycles, and shows how the walk process for 21 candidates (3 levels) completes in 4×3 = 12 cycles. The whole process finishes in 20 cycles, much earlier than the 100 cycles used to retrieve the incoming block from main memory.   </p><formula xml:id="formula_3">A B P L N G F N A X Y D K Z T E E K M X S X M Q R X A A N A X Y D M X A Y N Y 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation</head><p>To implement the replacement process, the cache controller needs some modifications involving hash functions, some additional state and, for non-blocking caches, scheduling of concurrent operations. Hash functions: We need one hash function per way. Hash functions range from extremely simple (e.g. bit selection) to exceedingly complex (e.g. cryptographic hash functions like SHA-1). In this study, we use H 3 hash functions <ref type="bibr" target="#b10">[11]</ref>, a family of low-cost, universal, pairwise-independent hash functions that require a few XOR gates per hash bit <ref type="bibr" target="#b37">[38]</ref>. State: The controller needs to remember the positions of the replacement candidates visited during the walk and the position of the best eviction candidate. Tracking only the most desirable replacement candidate is not sufficient, because relocations need to know about all blocks in the path. However, a single-ported SRAM or small register file suffices. Note that we do not have to remember full tags, just hash values. Also, no back-pointers need to be stored, because for a certain position in the SRAM, the parent's position is always the same. In the example shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the controller needs 63 bits of state to track candidates (21 hash values × 3 bits/value). If the cache was larger, e.g. 3MB, with 1MB per way and 64-byte lines (requiring 14 bits/hash value), it would need 294 bits. Additionally, the controller must buffer the tags and data of the L lines it reads and writes on a relocation. Since the number of levels is typically small (2 or 3 in our experiments), this also entails a small overhead. Concurrent operations for non-blocking caches: To avoid increasing cache latency, the replacement process should be able to run concurrently with all other operations (tag/data reads and writes due to hits, write-backs, invalidations, etc.). The walk process can run concurrently without interference. This may lead to benign races where, for example, the walk identifies the best eviction candidate to be a block that was accessed (e.g. with a hit) in the interim. This is exceedingly rare in large caches, so we simply evict the block anyway. In smaller caches (e.g. highly-associative but small TLBs or first-level caches), we could keep track of the best two or three eviction candidates and discard them if they are accessed while the walk process is running.</p><p>In the second part of the replacement, the relocations, the controller must block intervening operations to at most L positions while blocks in these positions are being relocated. We note that the controller already has logic to deal with these cases (e.g. with MSHRs <ref type="bibr" target="#b27">[28]</ref>).</p><p>While it is feasible to run multiple replacement processes concurrently, it would complicate the cache controller, and since replacements are not in the critical path, they can simply queue up. Concurrent replacements would only make sense to increase bandwidth utilization when the cache is close to bandwidth saturation. As shown in Section VI, we do not see the need for such mechanism in our experiments.</p><p>In conclusion, the zcache imposes minor state and logic overheads to traditional cache controllers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extensions</head><p>We now discuss additional implementation options to enhance zcaches. Avoiding repeats: In small first-level caches or TLBs, repeats can be common due to walking a significant portion of the cache. Moreover, a repeat at a low level can trigger the expansion of many repeated candidates. Repeats can be avoided by inserting the addresses visited during the walk in a Bloom filter <ref type="bibr" target="#b5">[6]</ref>, and not continuing the walk through addresses that are already represented in the filter. Repeats are rare in our experiments, so we do not see any performance benefit from this. Alternative walk strategies: The current walk performs a breadth-first search for candidates, fully expanding all levels. Alternatively, we could perform a depth-first search (DFS), always moving towards higher levels of replacement candidates. Cuckoo hashing <ref type="bibr" target="#b34">[35]</ref> follows this strategy. DFS allows us to remove the walk table and interleave walk with relocations, reducing state. However, it increases the number of relocations for a given number of replacement candidates (since L = R/W ), which in turn increases both the energy required per replacement (as relocations read and write to the much wider data array) and replacement latency (as accesses in the walk cannot be pipelined). BFS is a much better match to a hardware implementation as the extra required state for BFS is a few hundred bits at most. Nevertheless, a controller can implement a hybrid BFS+DFS strategy to increase associativity cheaply. For instance, in our example in <ref type="figure" target="#fig_1">Fig. 1</ref>, the controller could perform a second phase of BFS, trying to re-insert N rather than evicting it, to double the number of candidates without increasing the state needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Replacement Policy</head><p>So far, we have purposely ignored how the replacement policy is implemented. In this section, we cover how to implement or approximate LRU. While set-associative caches can cheaply maintain an order of the blocks in each set (e.g. using LRU or pseudo-LRU), since the concept of a set does not exist in a zcache, policies that rely on this ordering need to be implemented differently. However, several processor designs already find it too expensive to implement set ordering and resort to policies that do not require it <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>. Additionally, some of the latest, highest-performing policies do not rely on set ordering <ref type="bibr" target="#b23">[24]</ref>. While designing a replacement policy specifically tailored to zcaches is an interesting endeavor, we defer it to future work. Full LRU: We use a global timestamp counter, and add a timestamp field to each block in the cache. On each access, the timestamp counter is incremented, and the timestamp field is updated to the current counter value. On a replacement, the controller selects the replacement candidate with the lowest timestamp (in mod 2 n arithmetic). This design requires very simple logic, but timestamps have to be large (e.g. 32 bits) to make wrap-arounds rare, thus having high area overhead.</p><p>Bucketed LRU: To decrease space overheads, timestamps are made smaller, and the controller increases the timestamp counter once every k accesses. For example, with k = 5% of the cache size and n = 8 bits per timestamp, it is rare for a block to survive a wrap-around without being either accessed or evicted. We use this LRU policy in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. AN ANALYTICAL FRAMEWORK FOR ASSOCIATIVITY</head><p>Quantifying and comparing associativity across different cache designs is hard. In set-associative caches, more ways implicitly mean higher associativity. However, when comparing different designs (e.g. a set-associative cache and a zcache), the number of ways becomes a useless proxy for associativity.</p><p>The most commonly used approach to quantify associativity is by the number of conflict misses <ref type="bibr" target="#b20">[21]</ref>. Conflict misses for a cache are calculated by subtracting the number of misses incurred by a fully-associative cache of the same size from the total number of misses. Using conflict misses as a proxy for associativity has the advantage of being an end-to-end metric, directly linking associativity to performance. However, it is subject to three problems. First, it is highly dependent on the replacement policy; for example, by using an LRU replacement policy in a workload with an anti-LRU access pattern, we can get higher conflict misses when increasing the number of ways. Second, in CMPs with multilevel memory hierarchies, changing the associativity can alter the reference stream at higher cache levels, and comparing the number of conflict misses when the total number of accesses differs is meaningless. Finally, conflict misses are workload-dependent, so they cannot be used as a general proxy for associativity.</p><p>In this section, we develop a framework to address these issues, with the objectives of 1) being able to compare associativity between different cache organizations, and 2) determining how various design aspects (e.g. ways, number of replacement candidates, etc) influence cache associativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Associativity Distribution</head><p>Model: We divide a cache into the following components:</p><p>• Cache array: Holds tags and data, implements associative lookups by block address, and, on a replacement, gives a list of replacement candidates that can be evicted.</p><p>• Replacement policy: Maintains a global rank of which cache blocks to replace. This model assumes very little about the underlying cache implementation. The array could be set-associative, a zcache, or any of the schemes mentioned in Section II. The only requirement that we impose on the replacement policy is to define a global ordering of blocks, which most policies inherently do. For example, in LRU blocks are ranked by the time of their last reference, in LFU they are ordered by access frequency, and in OPT <ref type="bibr" target="#b3">[4]</ref> they are ranked by the time to their next reference. This does not mean that the implementation actually maintains this global rank. In a set-associative cache, LRU only needs to remember the order of elements in each set, and in a zcache this can be achieved with timestamps, as explained in Section III-E. By convention, blocks with a higher preference to be evicted are given a higher rank r. In a cache with B blocks, r ∈ [0, ..., B − 1]. To make the rest of the analysis independent of cache size, we define a block's eviction priority to be its rank normalized to <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>, i.e. e = r/(B − 1). Associativity distribution: We define the associativity distribution as the probability distribution of the eviction priorities of evicted blocks. In a fully-associative cache, we would always evict the block with e = 1.0. However, most cache designs examine only a small subset of the blocks in an eviction, so they select blocks with lower eviction priorities. In general, the more skewed the distribution is towards e = 1.0, the higher the associativity is. The associativity distribution characterizes the quality of the replacement decisions made by the cache in a way that is independent of the replacement policy. Note that this decouples how the array performs from ill-effects from the replacement policy. For example, a highly associative cache may always find replacement candidates with high eviction priorities, but if the replacement policy does a poor job in ranking the blocks, this may actually hurt performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Linking Associativity and Replacement Candidates</head><p>Defining associativity as a probability distribution lets us evaluate the quality of the replacement candidates, but is still dependent on workload and replacement policy. However, under certain general conditions this distribution can be characterized by a single number, the number of replacement candidates. This is the figure of merit optimized by zcaches. Uniformity assumption: If the cache array always returns n replacement candidates, and we treat the eviction priorities of these blocks as random variables E i , assuming that they are 1) uniformly distributed in [0,1] and 2) statistically independent from each other, we can derive the associativity distribution. Since E 1 , ..., E n ∼ U [0, 1], i.i.d, the cumulative distribution function (CDF) of each eviction priority is</p><formula xml:id="formula_4">F Ei (x) = P rob(E i ≤ x) = x, x ∈ [0, 1] 1 .</formula><p>The associativity is the random variable A = max {E 1 , ..., E n }, and its CDF is:</p><formula xml:id="formula_5">F A (x) = P rob(A ≤ x) = P rob(E 1 ≤ x ∧ ... ∧ E n ≤ x) = P rob(E i ≤ x) n = x n , x ∈ [0, 1]</formula><p>Therefore, under this uniformity assumption, the associativity distribution only depends on n, the number of replacement candidates. <ref type="figure">Fig. 2</ref> shows example CDFs of the associativity distribution, in linear and semi-log scales, with each line representing a different number of replacement candidates. The higher the number of replacement candidates, the more skewed towards 1.0 the associativity distribution becomes. Also, evictions of blocks with a low eviction priority quickly become very rare. For example, for 16 replacement candidates, the probability of evicting a block with e &lt; 0.4 is 10 −6 . Random candidates cache: The uniformity assumption makes it simple to characterize associativity, but it is not met in general by real cache designs. However, a cache array that returns n randomly selected replacement candidates (with repetition) from all the blocks in the cache always achieves these associativity curves perfectly. Each E i is uniformly distributed because it is an unbiased random sampling of one of the B possible values of a rank, and since different selections are done independently, the E i are independent as well. We simulated this cache design with tens of real workloads, under several configurations and replacement policies, and obtained associativity distributions as shown in <ref type="figure">Fig. 2</ref>, experimentally validating the previous derivation. Although this random candidates cache design is unrealistic, it reveals a sufficient condition to achieve the uniformity assumption: the more randomized the replacement candidates, the better a cache will match the uniformity assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Associativity Measurements of Real Caches</head><p>Our analytical framework implies that the number of replacement candidates is the key metric in determining associativity. We now evaluate whether this is the case using real cache designs. Set-associative caches: <ref type="figure">Fig. 3a</ref> shows the associativity distributions for 8MB L2 set-associative caches of 4 and 16 ways, using an LRU replacement policy. The details on system configuration and methodology can be found in Section V. Each of the 6 solid lines represents a different benchmark, Cores 32 cores, x86-64 ISA, in-order, IPC=1 except on memory accesses, 2 GHz L1 caches 32 KB, 4-way set associative, split D/I, 1-cycle latency L2 cache 8 MB NUCA , 8 banks, 1 MB bank, shared, inclusive, MESI directory coherence, 4-cycle average L1-to-L2-bank latency, 6-11-cycle L2 bank latency MCU 4 memory controllers, 200 cycles zero-load latency, 64 GB/s peak memory BW from a representative selection of PARSEC and SPECOMP applications. The single dotted line per graph plots the associativity distribution under the uniformity assumption, which is independent of the workload. We see that the distributions differ significantly from the uniformity assumption. Two workloads (wupwise and apsi) do significantly worse, with the CDF rapidly climbing towards 1.0. For example, in wupwise, 60% of the evictions happen to blocks with ≤ 20% eviction priority. Others (mgrid, canneal and fluidanimate) have sensibly worse associativity, and only one benchmark (blackscholes) outperforms the uniformity assumption. These differences are not surprising: replacement candidates all come from the same small set, thwarting independence, and locality of reference will skew eviction priorities towards lower values, breaking the assumption of an uniform distribution.</p><p>We can improve associativity with hashing. <ref type="figure">Fig. 3b</ref> shows the associativity distributions of set-associative caches indexed by an H 3 hash of the block address. Associativity distributions generally improve, but some hot-spots remain, and all workloads now perform sensibly worse than the uniformity assumption case. Skew-associative caches and zcaches: <ref type="figure">Fig. 3c</ref> shows the associativity distributions of 4 and 16-way skew-associative caches. As we can see, skew-associative caches closely match the uniformity assumption on all workloads. These results provide an analytical foundation to the previous empirical observations that skew-associative caches "improve performance predictability" <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Fig. 3d</ref> shows the associativity of 4-way zcaches with 2 and 3 levels of replacement candidates. We also observe a close match to the uniformity assumption. This is expected, since replacement candidates are even more randomized: n thlevel candidates depend on the addresses of the (n−1) th -level candidates, making the set of positions checked varying with cache contents.</p><p>In conclusion, both skew-associative caches and zcaches match the uniformity assumption in practice. Hence, their associativity is directly linked to the number of candidates examined on replacement. Although the graphs only show a small set of applications for clarity, results with other workloads and replacement policies are essentially identical. The small differences observed between applications decrease by either increasing the number of ways (and hash functions) or improving the quality of hash functions (the same experiments using more complex SHA-1 hash functions instead of H 3 yield distributions identical to the uniformity assumption).</p><p>Overall, our analysis framework reveals two main results:</p><p>1) In a zcache, associativity is determined by the number of replacement candidates, and not the number of ways, essentially decoupling ways and associativity. 2) When using an equal number of replacement candidates, zcaches empirically show better associativity than setassociative caches for most applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL METHODOLOGY</head><p>Infrastructure: We perform microarchitectural, executiondriven simulation using an x86-64 simulator based on Pin <ref type="bibr" target="#b30">[31]</ref>. We use McPAT <ref type="bibr" target="#b29">[30]</ref> to obtain comprehensive timing, area and energy estimations for the CMPs we model, and use CACTI 6.5 <ref type="bibr" target="#b32">[33]</ref> for more detailed cache area, power and timing models. We use 32nm ITRS models, with a high-performance process for all the components of the chip except the L2 cache, which uses a low-leakage process. System: We model a 32-core CMP, with in-order x86 cores modeled after the Atom processor <ref type="bibr" target="#b16">[17]</ref>. The system has a 2-level cache hierarchy, with a fully shared L2 cache. <ref type="table" target="#tab_1">Table I</ref> shows the details of the system. On 32nm, this CMP requires about 220mm 2 and has a TDP of around 90W at 2GHz, both reasonable budgets. Workloads: We use a variety of multithreaded and multiprogrammed benchmarks: 6 PARSEC <ref type="bibr" target="#b4">[5]</ref> applications (blackscholes, canneal, fluidanimate, freqmine, streamcluster and swaptions), 10 SPECOMP benchmarks (all except galgel, which gcc cannot compile) and 26 SPECCPU2006 programs (all except dealII, tonto and wrf, which we could not compile).</p><p>For multiprogrammed runs, we run different instances of the same single-threaded CPU2006 application on each core, plus 30 random CPU2006 workload combinations (choosing 32 workloads each time, with repetitions allowed). These make a total of 72 workloads. All applications are run with their reference (maximum size) input sets. For multithreaded workloads, we fast-forward into the parallel region and run the first 10 billion instructions. Since synchronization can skew IPC results for multithreaded workloads <ref type="bibr" target="#b1">[2]</ref>, we do not count instructions in synchronization routines (locks, barriers, etc.) to determine when to stop execution, but we do include them in energy calculations. For multiprogrammed workloads, we follow standard methodology from prior work <ref type="bibr" target="#b23">[24]</ref>: we fastforward 20 billion instructions for each process, simulate until all the threads have executed at least 256 million instructions, and only take the first 256 million instructions of each thread into account for IPC computations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION OF ZCACHE AS A LAST-LEVEL CACHE</head><p>The zcache can be used with any design that requires high associativity at low overheads in terms of area, hit time, and hit energy. In this paper, we evaluate zcache as a lastlevel cache in a 32-node CMP. We defer other use cases, such as first-level caches or TLBs, to future work. We first quantify the area, energy and latency advantages of zcaches versus set-associative caches with similar associativity, then compare the performance and system-wide energy over our set of workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cache Costs</head><p>Table II shows the timing, area and power requirements of both set-associative caches and zcaches with varying associativities. We use CACTI's models to obtain these numbers. Tag and data arrays are designed separately by doing a full design space exploration and choosing the design that minimizes area×delay×power. Arrays are sub-banked, and both the address and data ports are implemented using H-trees. We show results for both serial and parallel-lookup caches. In serial caches, tag and data arrays are accessed sequentially, saving energy at the expense of delay. In parallel caches, both tag and data accesses are initiated in parallel. When the tag read resolves the appropriate way, it propagates a wayselect signal to the data array, which selects and propagates the correct output. This parallelizes most of the tag and data accesses while avoiding an exceedingly wide data array port. For zcaches, we explore designs with two and threelevel walks. We denote zcaches with "W /R", indicating the number of ways and replacement candidates, respectively. For example, a 4/16 zcache has 4 ways and 16 replacement candidates per eviction (obtained from a two-level walk). <ref type="table" target="#tab_1">Table II</ref> shows that increasing the number of ways beyond 8 starts imposing significant area, latency and energy overheads. For example, a 32-way cache with serial lookups has 1.22× the area, 1.23× the hit latency and 2× the hit energy of a 4-way cache (for parallel lookups, hit latency is 1.32× and hit energy is 3.3×). This is logical, since a 32-way cache reads 4× more tag bits than data bits per lookup, the tag array has a much wider port, and the critical path is longer (slower tag array, more comparators). For zcaches, however, area, hit latency and hit energy grow with the number of ways, but not with the number of replacement candidates. This comes at the expense of increasing energy per miss, which, however, is still similar to set-associative caches with the same associativity. For example, a serial-lookup zcache 4/52 has almost twice the associativity of a 32-way set-associative cache at 1.3× higher energy per miss, but retains the 2× lower hit energy and 1.23× lower access latency of a 4-way cache. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the improvements in both L2 misses per thousand instructions (MPKI) and IPC for all workloads, using both OPT and LRU replacement policies. Each line represents the improvement of a different cache design over a baseline 4-way set-associative cache with H 3 hashing. Caches without hashing perform significantly worse (even at high associativities), so we do not consider them here. Serial-lookup caches are used in all cases. For each line, workloads (in the x-axis) are sorted according to the improvement achieved, so each line is monotonically increasing. Fractional improvements are given (e.g. a L2 MPKI reduction of 1.2 means 1.2× lower MPKI than the baseline). OPT: <ref type="figure" target="#fig_3">Fig. 4a</ref> shows the effects of using OPT replacement (i.e. evicting the candidate reused furthest). OPT simulations are run in trace-driven mode. Although OPT is unrealistic, it removes ill-effects from the replacement policy (where e.g. increasing associativity degrades performance), allowing us to decouple replacement policy issues from associativity effects 2 . Note that these numbers do not necessarily show maximum improvements from increasing associativity, as other replacement policies may be more sensitive to associativity changes. In terms of misses, higher associativities always improve MPKI, and designs with the same associativity have practically the same improvements (e.g. 16-way set-associative vs Z4/16). However, for set-associative caches, these improvements in MPKI do not always translate to IPC, due to the additional access latency (1 extra cycle for 16-way, 2 cycles for 32-way). For example, the 32-way set-associative design performs worse than the 4-way design on 15 workloads (which have a large number of L1 misses, but few L2 misses), and performs worse than the 16-way design on half of the workloads (36). In contrast, zcaches do not suffer from increased access latency, sensibly improving IPC with associativity for all workloads (e.g. a Z4/52 improves IPC by up to 16% over the baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance</head><p>LRU: <ref type="figure" target="#fig_3">Fig. 4b</ref> compares cache designs when using LRU. Associativity improves MPKI for all but 3 workloads, and both MPKI and IPC improvements are significant (e.g. a Z4/52 reduces L2 misses by up to 2.1× and improves performance by up to 25% over a 4-way set-associative cache). With LRU, the difference between Z4/16 and Z4/52 designs is lower than with OPT, however they significantly outperform both the baseline and the Z4/4 (skew-associative) design. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the performance and system-wide energy efficiency when using serial and parallel-lookup caches, under both OPT and LRU replacement policies. Results are normalized to a serial-lookup, 4-way set-associative cache with H 3 hashing. Each graph shows improvements on five representative applications, as well as the geometric means of both all 72 workloads and the 10 workloads with the highest L2 MPKI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Serial vs Parallel-Lookup Caches</head><p>We can distinguish three types of applications: a few benchmarks, like blackscholes or freqmine, have low L1 miss rates, and are insensitive to the L2's organization. Other applications, like ammp and gamess, have frequent L2 hits but infrequent L2 misses. These workloads are sensitive to hit latency, so parallel-lookup caches provide higher performance gains than increasing associativity (e.g. a 3% IPC improvement on gamess vs serial-lookup caches). In fact, increasing associativity in set-associative caches reduces performance due to higher hit latencies, while highly-associative zcaches do not degrade performance. Finally, workloads like cpu2K6rand0, canneal, and cactusADM have frequent L2 misses. These applications are often sensitive to associativity, and a highly-associative cache improves performance (by reducing L2 MPKI) more than reducing access time (e.g. in cactusADM with LRU, going from Z4/4 to Z4/52 improves IPC by 9%, while going from serial to parallel-lookup improves IPC by 3%).</p><p>In terms of energy efficiency, set-associative caches and zcaches show different behaviors when increasing associativity. Because hit energy increases steeply with the number of ways in parallel-lookup caches, 16 and 32-way set-associative caches often achieve lower energy efficiency than seriallookup caches (e.g. up to 8% lower BIPS/W in cactusADM). In contrast, serial and parallel-lookup zcaches achieve practically the same energy efficiency on most workloads, due to their similarly low access and miss energies. In conclusion, zcaches enable highly-associative, energy-efficient parallellookup caches.</p><p>Overall, zcaches offer both the best performance and energy efficiency. For example, under LRU, when considering all 72 workloads, a parallel-lookup zcache 4/52 improves IPC by 7% and BIPS/W by 3% over the 4-way baseline. Over the subset of the 10 most L2 miss-intensive workloads, a zcache 4/52 improves IPC by 18% and energy efficiency by 13% over the 4-way baseline, and obtains 7% higher performance and 10% better energy efficiency than a 32-way set-associative cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Array Bandwidth</head><p>Since zcaches perform multiple tag lookups on a miss, it is worth examining whether these additional lookups can saturate bandwidth. Of the 72 workloads, the maximum average load per bank is 15.2% (i.e. 0.152 core accesses/cycle/L2 bank). However, as L2 misses increase, average load decreases: at 0.005 misses/cycle/bank, average load is 0.035 accesses/cycle/bank, and total load on the tag array for a Z4/52 cache is 0.092 tag accesses/cycle/bank. In other words, as L2 misses increase, bandwidth pressure on the L2 decreases; the system is self-throttling. ZCaches use this spare tag bandwidth to improve associativity. Ultimately, even for high-MLP architectures, the load on the tag arrays is limited by main memory bandwidth, which is more than an order of magnitude smaller than the maximum L2 tag bandwidth and much harder to scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>The zcache is inspired by cuckoo hashing, a technique to build space-efficient hash tables proposed by Pagh and Rodler <ref type="bibr" target="#b34">[35]</ref>. The original design uses two hash functions to index the hash table, so each lookup needs to check two locations. On an insertion, if both possible locations are occupied, the incoming item replaces one of them at random, and the replaced block is reinserted. This is repeated until either an empty location is found or, if a limit number of retries is reached, elements are rehashed into a larger array. Though cuckoo hashing has been mostly studied as a technique for software hash tables, hardware variants have been proposed to implement lookup tables in IP routers <ref type="bibr" target="#b15">[16]</ref>. For additional references, Mitzenmacher has a survey on recent research in cuckoo hashing <ref type="bibr" target="#b31">[32]</ref>.</p><p>Both high associativity and a good replacement policy are necessary to improve cache performance. The growing importance of cache performance has sparked research into alternative policies that outperform LRU <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>. The increasing importance of on-chip wire delay has also motivated research in non-uniform cache architectures (NUCA) <ref type="bibr" target="#b26">[27]</ref>. Some NUCA designs such as NuRAPID <ref type="bibr" target="#b14">[15]</ref> use indirection to enhance the flexibility of NUCA placement and reduce access latency instead of increasing associativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS AND FUTURE WORK</head><p>We have presented the zcache, a cache design that enables high associativity with a small number of ways. The zcache uses a different hash function per way to enable an arbitrarily large number of replacement candidates on a miss. To evaluate the zcache's associativity, we have developed a novel analytical framework to characterize and compare associativity. We use this framework to show that, for zcaches, associativity is determined by the number of replacement candidates, not the number of ways, hence decoupling ways and associativity. An evaluation using zcaches as the last-level cache in a CMP shows that they provide high associativity with low overheads in terms of area, hit time, and hit energy. ZCaches outperform traditional set-associative caches in both performance and energy efficiency, with a 4-way zcache achieving both 18% higher performance and 13% higher performance/watt than 4-way set-associative counterpart over a set of 10 L2 missintensive workloads, and 7% higher performance and 10% better energy efficiency than a 32-way set-associative cache.</p><p>There are several opportunities for further research, such as using zcaches to build highly associative first-level caches and TLBs for multithreaded cores. Additionally, replacement policies that are specifically suited to the zcache could be designed. Finally, since the zcache makes it trivial to increase or reduce associativity with the same hardware design, it would be interesting to explore adaptive replacement schemes that use the high associativity only when it improves performance, saving cache bandwidth and energy when high associativity is not needed, or even making associativity a software-controlled property.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Replacement process in a zcache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Associativity CDFs under the uniformity assumption (F A (x) = x n , x ∈ [0, 1]) for n = 4, 8, 16, 64 candidates, in linear and logarithmic scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. L2 MPKI and IPC improvements for all workloads, over a 4-way set-associative with hashing baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. IPC and energy efficiency (BIPS/W) improvements for serial and parallel-lookup caches, over a serial-lookup 4-way set-associative with hashing baseline. Each graph shows improvements for 5 representative workloads, plus the geometric mean over both all 72 workloads and the 10 workloads with the highest L2 MPKI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TABLE I MAIN CHARACTERISTICS OF THE SIMULATED CMPS. THE LATENCIES ASSUME A 32 NM PROCESS AT 2GHZ.</head><label>I</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Note that we are treating E i as continuous random variables, even though they are discrete (normalized ranks with one of B equally probable values in [0, 1]). We do this to achieve results that are independent of cache size B. Results are the same for the discretized version of these equations.</note>

			<note place="foot" n="2"> In caches with interference across sets, like skew-associative and zcaches, OPT is not actually optimal, but it is a good heuristic.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We sincerely thank John Brunhaver, Christina Delimitrou, David Lo, George Michelogiannakis and the anonymous reviewers for their useful feedback on earlier versions of this manuscript. Daniel Sanchez was supported by a HewlettPackard Stanford School of Engineering Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Column-associative caches: a technique for reducing the miss rate of direct-mapped caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pudar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th annual Intl. Symp. on Computer architecture</title>
		<meeting>of the 20th annual Intl. Symp. on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">IPC considered harmful for multiprocessor workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scavenger: A new last level cache architecture with global block priority</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th annual IEEE/ACM Intl Symp. on Microarchitecture</title>
		<meeting>of the 40th annual IEEE/ACM Intl Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of replacement algorithms for a virtualstorage computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Belady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Syst. J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The PARSEC benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Intl. Conf. on Parallel Architectures and Compilation Techniques</title>
		<meeting>of the 17th Intl. Conf. on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skewed associativity enhances performance predictability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 22nd annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disintermediated active communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the mathematics of caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Brehob</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Michigan State University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictive sequential associative cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd IEEE Symp. on HighPerformance Computer Architecture</title>
		<meeting>of the 2nd IEEE Symp. on HighPerformance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal classes of hash functions (extended abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th annual ACM Symposium on Theory of Computing</title>
		<meeting>of the 9th annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BulkSC: bulk enforcement of sequential consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 34th annual Intl. Symp. on Computer architecture</title>
		<meeting>of the 34th annual Intl. Symp. on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bulk disambiguation of speculative threads in multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cascaval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 33rd annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-LIFO: the foundation of a new family of replacement policies for last-level caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distance associativity for high-performance energy-efficient non-uniform cache architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 36th Annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 36th Annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient hardware-based multi-hash scheme for high speed IP lookup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Demetriades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Melhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th IEEE Symp. on High Performance Interconnects</title>
		<meeting>of the 16th IEEE Symp. on High Performance Interconnects</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sub-1W to 2W low-power IA processor for mobile internet devices and ultra-mobile PCs in 45nm hi-K metal gate CMOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fully associative softwaremanaged cache design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 27th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transactional memory coherence and consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Carlstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hertzberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 31st annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inside the Intel Itanium 2 processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hewlett-Packard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating associativity in cpu caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 48-core IA-32 message-passing processor with DVFS in 45nm CMOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive insertion policies for managing shared caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sebot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th intl. conf. on Parallel Architectures and Compilation Techniques</title>
		<meeting>of the 17th intl. conf. on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High performance cache replacement using re-reference interval prediction (RRIP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C S</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 37th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 17th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using prime numbers for cache indexing to eliminate conflict misses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Intl. Symp. on High Performance Computer Architecture</title>
		<meeting>of the 10th Intl. Symp. on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An adaptive, nonuniform cache structure for wire-delay dominated on-chip caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th intl. conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>of the 10th intl. conf. on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 8th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Westmere: A family of 32nm IA processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kurd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">McPAT: an integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pin: building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGPLAN conf. on Programming Language Design and Implementation</title>
		<meeting>of the ACM SIGPLAN conf. on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Some open questions related to cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th annual European Symp. on Algorithms</title>
		<meeting>of the 17th annual European Symp. on Algorithms</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 40th annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ECMon: exposing cache events for monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 36th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 36th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th annual European Symp. on Algorithms</title>
		<meeting>of the 9th annual European Symp. on Algorithms</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The v-way cache: Demand based associativity via global replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 32nd annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 32nd annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive line placement with the set balancing cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 42nd annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Implementing signatures for transactional memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 40th annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>of the 40th annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A case for two-way skewed-associative caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 20th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A 40nm 16-core 128-thread CMT SPARC SoC processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UltraSPARC T2 supplement to the Ultra-SPARC architecture</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Sun Microsystems</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The bulk multicore architecture for improved programmability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cascaval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The implementation of POWER7: A highly parallel and scalable multi-core high-end server processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl. Solid-State Circuits Conf</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PIPP: promotion/insertion pseudopartitioning of multi-core shared caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 36th annual Intl. Symp. on Computer Architecture</title>
		<meeting>of the 36th annual Intl. Symp. on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two fast and highassociativity cache schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
