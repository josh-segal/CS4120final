<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON SEQUENTIALLY NORMALIZED MAXIMUM LIKELIHOOD MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teemu</forename><surname>Roos</surname></persName>
							<email>teemu.roos@cs.helsinki.fijorma.rissanen@mdl-research.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Helsinki Institute for Information Technology HIIT</orgName>
								<orgName type="institution" key="instit2">University of Helsinki</orgName>
								<address>
									<postBox>P.O.Box 68</postBox>
									<postCode>FI-00014</postCode>
									<country key="FI">FINLAND</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Rissanen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Helsinki Institute for Information Technology HIIT</orgName>
								<orgName type="institution" key="instit2">University of Helsinki</orgName>
								<address>
									<postBox>P.O.Box 68</postBox>
									<postCode>FI-00014</postCode>
									<country key="FI">FINLAND</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ON SEQUENTIALLY NORMALIZED MAXIMUM LIKELIHOOD MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The important normalized maximum likelihood (NML) distribution is obtained via a normalization over all sequences of given length. It has two short-comings: the resulting model is usually not a random process, and in many cases, the normalizing integral or sum is hard to compute. In contrast, the recently proposed sequentially normalized maximum likelihood (SNML) models always comprise a random process and are often much easier to compute. We present some results on SNML type models in the Markovian and linear-Gaussian model classes.</p><p>In the linear-Gaussian case, the resulting sequentially normalized least squares (SNLS) model is particularly interesting. The associated sequentially minimized squared deviations are smaller than both the usual least squares and the squared prediction errors used in the so called predictive least squares (PLS) criterion. The SNLS model is asymptotically optimal within the given class of distributions by reaching the lower bound on the logarithmic prediction errors, given by the stochastic complexity, up to lower-order terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SOME MINMAX PROBLEMS</head><p>Consider the model class M k = {f (x n ; θ)}, θ = θ 1 , . . . , θ k , and data sequences x n = x 1 , . . . , n, for n = 1, 2, . . . . Let m be a large enough integer such that the ML estimatê θ t = ˆ θ(x t ) can be computed for t &gt; m. The number log 1/f (x n ; ˆ θ n )</p><p>has been considered as the ideal target for the code length obtainable with the model class, <ref type="bibr" target="#b0">[1]</ref>, which, however, is not attainable, because f (x n ; ˆ θ n ) is not a probability distribution. This leads to the minmax problem min q max x n log f (x n ; ˆ θ n ) q(x n ) , with the solution due to Shtarkov, known as the normalized maximum likelihood (NML) universal model, <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_0">ˆ f NML (x n ; M γ ) = f (x n ; ˆ θ(x n )) C n (1) C n = 񮽙 f (y n ; ˆ θ(y n ))dy n .</formula><p>However, the normalizing coefficient can be evaluated easily only for restricted model classes, and the model does not define a random process. This means that it cannot be used for prediction and its evaluation for data compression is difficult. Now, consider for all t &gt; m, the problem</p><formula xml:id="formula_1">min q(x|x t−1 ) max x log f (x t−1 , x; ˆ θ(x t−1 , x)) q(x | x t−1 )</formula><p>.</p><p>The solution is given by the conditional NML modeî</p><formula xml:id="formula_3">modeî f (x t | x t−1 ) = f (x t ; ˆ θ(x t )) K t (x t−1 )<label>(3)</label></formula><formula xml:id="formula_4">K t (x t−1 ) = 񮽙 f (x t−1 , x; ˆ θ(x t−1 , x))dx.</formula><p>This is proved the same way as the solution to Shtarkov's problem: First, replacing the numerator by the density function (3) does not change the solution, and the maximized ratio of the two density functions (3) and q(x | x t−1 ), which is not smaller than unity, is made unity when the latter is selected equal to the former. It is clear that the normalizing coefficient K t (x t−1 ), which in general is a function of x t−1 , is easier to calculate, at least numerically, than the normalizing coefficient in the NML universal model. For another type of normalization, where the numerator f (x t ; ˆ θ(x t )) is replaced by the conditional density f (x t | x t−1 ; ˆ θ(x t )), see <ref type="bibr" target="#b2">[3]</ref>. Putting together the conditional NML densities gives the sequentially normalized maximum likelihood (sNML) model:</p><formula xml:id="formula_5">f SNML (x n ) = f m (x m ) n t=m+1ˆf t=m+1ˆ t=m+1ˆf (x t | x t−1 ),<label>(4)</label></formula><p>where f m (x m ) is a suitably chosen initial distribution. The result is, by construction, a random process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BERNOULLI MODEL</head><p>We begin with an example involving the Bernoulli class B = {P (x; p)}, where the parameter p = P (1). The ML estimate is given byˆpbyˆ byˆp(x n ) = n 1 /n, where n 1 = t x t is the number of 1's in x n . If n 0 = n − n 1 the maximized likelihood is</p><formula xml:id="formula_6">P (x n ; n 1 /n) = n 1 n n1 n 0 n n0 .</formula><p>The conditional NML predictive probability can be written asˆP</p><formula xml:id="formula_7">asˆ asˆP (1 | x n ) = (n 1 + 1) e(n 1 ) (n 0 + 1) e(n 0 ) + (n 1 + 1) e(n 1 ) ,<label>(5)</label></formula><p>where e(n 0 ) = (1 + 1/n 0 ) n0 and e(n 1 ) = (1 + 1/n 1 ) n1 ; take e(k) = 1 for k = 0. For instance, in the problem considered by Laplace, given a sequence of '1's, the successive probabilities of yet another '1' are </p><formula xml:id="formula_8">P Lap (1 | x n ) = n 1 + 1 n + 2 ,</formula><p>which gives the same sequence as 1 2 , 2 3 , 3 4 , 4 5 , . . ., i.e., the certainty of 80 %, which is achieved by the Laplace probability on the fourth step, is achieved by conditional NML already on the second step.</p><p>The same conditional probability functionˆPfunctionˆ functionˆP (1 | x n ) was found in <ref type="bibr" target="#b1">[2]</ref>, where it was shown to behave similarly to the Krichevski-Trofimov predictive probability</p><formula xml:id="formula_9">P KT (1 | x n ) = n 1 + 1/2 n + 1 .</formula><p>It was also found later in <ref type="bibr" target="#b3">[4]</ref>, in effect, as the solution to the following minmax problem</p><formula xml:id="formula_10">min θ max x log f (x t−1 , x; ˆ θ(x t−1 , x)) f (x | x t−1 ; θ) .<label>(6)</label></formula><p>Neither Krichevski-Trofimov predictive probability nor the related Laplace probability has been shown to have any particular optimality property, except asymptotically. Takimoto and Warmuth <ref type="bibr" target="#b3">[4]</ref> showed that for the Bernoulli models, the regret of the sNML model (4) satisfies for all sequences the inequality</p><formula xml:id="formula_11">R(f SNML , x n ) := ln 1/f SNML (x n ) − ln 1/f (x n ; ˆ θ(x n )) ≤ 1 2 ln(n + 1) + 1 2 .<label>(7)</label></formula><p>We conclude this section by noting that in the Bernoulli case, the alternative version of normalization mentioned in Sec. 2, where the numerator of (3) is replaced by f (x t | x t−1 ; ˆ θ(x t )), agrees with the Laplace probability, see <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LINEAR-QUADRATIC MODELS</head><p>In the rest of the paper, we are concerned with deriving a model selection criterion for a class of normal models</p><formula xml:id="formula_12">f (y n | X n ; σ 2 , b) = (2πσ 2 ) −n/2 exp − 1 2σ 2 n 1 (y t − b ′ ¯ x t ) 2 ,</formula><p>induced by the regression equations</p><formula xml:id="formula_13">y t = b ′ ¯ x t + ǫ t ,<label>(8)</label></formula><p>where the prime indicates transposition,</p><formula xml:id="formula_14">b ′ = (b(1), . . . , b(k)), with k ∈ N. The deviations (ǫ t ) n t=1</formula><p>are taken as an i.i.d. sequence generated by a normal distribution of zero-mean and variance σ 2 . The columns ¯ x t = (x t,1 , . . . , x t,k ) ′ of real valued elements, defining the regressor matrices X t , are either non-random, or ¯ x t = (y t−1 , . . . , y t−k ) ′ as in AR models.</p><p>For each t = 1, 2, . . . n, let k(t) be the largest integer such that the least squares estimate b t = (b t,1 , . . . , b t,k(t) ) ′ can be uniquely solved. Hence, typically k(t) = min{t, k} except for AR models, where k(t) = min{t − 1, k}. We let m be the smallest integer t such that k(t) = k.</p><p>Central to this work are the following three representations of data for t = 1, 2, . . . n, and k ≥ k(t):</p><formula xml:id="formula_15">y t = b ′ t−1 ¯ x t + e t = k(t) i=1 b t−1,i x t,i + e t ,<label>(9)</label></formula><formula xml:id="formula_16">y t = b ′ n ¯ x t + ˆ ǫ t (n) = k(t) i=1 b n,i x t,i + ˆ ǫ t (n),<label>(10)</label></formula><formula xml:id="formula_17">y t = b ′ t ¯ x t + ˆ e t = k(t) i=1 b t,i x t,i + ˆ e t .<label>(11)</label></formula><p>The predictor b ′ t−1 ¯ x t of y t in the first case is called the 'plug-in' predictor, in which the parameters are calculated from the data available up to t − 1. The plug-in model defines a conditional normal density function for t &gt; m,</p><formula xml:id="formula_18">f (y t | y t−1 , X t ; b t−1 , ˆ σ 2 t−1 ) = 1 񮽙 2πˆσ2πˆσ 2 t−1 exp − e 2 t 2ˆσ2ˆσ 2 t−1 , wherê σ 2 t−1 = 1 t−1 t−1 i=1ˆǫi=1ˆ i=1ˆǫ 2 i (t − 1)</formula><p>, and y t−1 = y 1 , . . . , y t−1 . The resulting joint density function obtained by multiplying the conditional densities of y m+1 , . . . , y n , and ignoring constant terms, defines (by its negative logarithm) the so-called predictive minimum description length (PMDL) criterion, studied in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and <ref type="bibr" target="#b7">[8]</ref>. Its special case for constant variancê</p><formula xml:id="formula_19">σ 2 t−1 = σ 2 is the predictive least squares (PLS) criterion, PLS(n, k) = n t=m+1 (y t − b ′ t−1 ¯ x t ) 2</formula><p>, studied in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b7">[8]</ref>.</p><p>The second representation (10) is traditional, and it, too, has associated model selection criteria, including AIC <ref type="bibr" target="#b9">[10]</ref>, and BIC <ref type="bibr" target="#b10">[11]</ref>,</p><formula xml:id="formula_20">BIC(n, k) = n 2 logˆσlogˆ logˆσ 2 n + k + 1 2 log n,</formula><p>where k + 1 is the number of parameters (including the variance). The BIC criterion is obtained by an approximation of a joint density function of the data where the negative logarithm of the maximized likelihood f (y n | X n ; b n , ˆ σ 2 n ) determines the first term. In the AIC criterion the second term is k + 1, the number of parameters. Both criteria are often multiplied by 2/n, so that the first term is simply the logarithm of the residual sum of squares.</p><p>Also involving the second representation, the normalized maximum likelihood (NML) criterion is obtained directly as the normalized version of the maximized likelihood, where the normalizing term is given by C n,k = <ref type="bibr" target="#b11">[12]</ref>. In order to make the integral finite, the range of integration Y has to be restricted, which requires hyper-parameters. A solution which eliminates the effect of the hyper-parameters to model selection by a second normalization is presented in <ref type="bibr" target="#b12">[13]</ref>, see also <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. The corresponding parameterfree criterion is</p><formula xml:id="formula_21">y n ∈Y f (y n | X n ; b n , ˆ σ 2 n ) dy n [1], [2],</formula><formula xml:id="formula_22">NML(n, k) = n − k 2 logˆσ logˆ logˆσ 2 n n − k + k 2 logˆR logˆ logˆR k + 1 2 log(k(n − k)), wherê R = b ′ n X n X ′ n b n /n.</formula><p>The third representation, which we are interested in, is new. The sum of squared deviationsêdeviationsˆdeviationsê 2 t is smaller than either the sum of the traditional least squaresˆǫsquaresˆ squaresˆǫ 2 t (n), or the sum of the squared prediction errors e 2 t . However, since the parameters of the corresponding conditional density function f (y t | y t−1 , X t ; b t , ˆ σ 2 t ) involve at each step t &gt; m the response variable y t , it too needs to be normalized in order to obtain a proper density function. We study the asymptotic behavior of the resulting sequentially normalized least squares criterion for both fixed designs and random ones appearing in AR models. The criterion involves no approximations and is free of any hyperparameters which tend to affect the outcome especially for small samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SEQUENTIALLY NORMALIZED LEAST SQUARES</head><p>In order to obtain a meaningful model selection criterion with a capability to find a balance between goodness of fit and complexity, we convert the squared deviations into a density model. Consider first the simple case where the variance σ 2 is fixed. The non-normalized conditionals</p><formula xml:id="formula_23">f (y t | y t−1 , X t ; σ 2 , b t ) = 1 √ 2πσ 2 exp − (y t − ˆ y t ) 2 2σ 2 ,<label>(12)</label></formula><p>are obtained by replacing the parameter vector b in the conditional normal density function f (y t | y t−1 , X t ; σ 2 , b) by the least squares estimate b t .</p><p>For each fixed k, for t &gt; m, where m is the smallest value for t for which k(t) = k, the well known recursions exist, see for instance <ref type="bibr" target="#b15">[16]</ref>,</p><formula xml:id="formula_24">b t = V t t j=1 ¯ x j y j = b t−1 + V t−1 1 + c t ¯ x t (y t − ¯ x ′ t b t−1 ) (13) V t = (X t X ′ t ) −1 = V t−1 − V t−1 ¯ x t ¯ x ′ t V t−1 /(1 + c t ) (14) c t = ¯ x ′ t V t−1 ¯ x t d t = ¯ x ′ t V t ¯ x t 1 − d t = 1/(1 + c t ).<label>(15)</label></formula><p>The last equality was shown in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref> with the interpretation that the quantity 1 − d t is the ratio of the (Fisher) information in the first t−1 observations relative to all the t observations, <ref type="bibr" target="#b7">[8]</ref>. This also implies that 0 ≤ d t ≤ 1. By (13) we obtainˆy</p><formula xml:id="formula_25">obtainˆ obtainˆy t = ¯ x ′ t [V t−1 ¯ x t (y t − ¯ x ′ t b t−1 )/(1 + c t ) + b t−1 ] = c t /(1 + c t )(y t − ¯ x ′ t b t−1 ) + ¯ x ′ t b t−1 = (1 − d t )¯ x ′ t b t−1 + d t y t .<label>(16)</label></formula><p>which is a weighted average of the plug-in prediction ¯ x ′ t b t−1 and the true value y t . This gives the remaining error asê</p><formula xml:id="formula_25">obtainˆ obtainˆy t = ¯ x ′ t [V t−1 ¯ x t (y t − ¯ x ′ t b t−1 )/(1 + c t ) + b t−1 ] = c t /(1 + c t )(y t − ¯ x ′ t b t−1 ) + ¯ x ′ t b t−1 = (1 − d t )¯ x ′ t b t−1 + d t y t .<label>(16)</label></formula><p>which is seen to be smaller than the plug-in prediction error by a constant factor. The normalization of <ref type="formula" target="#formula_2">(12)</ref> is straightforward, and the result is a normal density function, the mean given by the plug-in predictor and the variance by τ = (1 + c t ) 2 σ 2 .</p><p>If we in (12) replace the variance by the minimized variancê s t /t and try to normalize the result the normalizing integral will be infinite. To make it finite would require hyper-parameters. Consider instead the maximization problem</p><formula xml:id="formula_27">max σ 2 n t=m+1 f (y t | y t−1 , X t ; σ 2 , b t ).<label>(18)</label></formula><p>The maximizing σ 2 isˆτ</p><formula xml:id="formula_28">isˆ isˆτ n = ˆ s n − ˆ s m n − m = 1 n − m n t=m+1ê t=m+1ˆt=m+1ê 2 t ,</formula><p>which gives the maximized product (2πeˆτ2πeˆτ n ) −(n−m)/2 . By normalizing over y t , we get the normalized conditional density functionˆf functionˆ functionˆf (y t | y t−1 , X t )</p><formula xml:id="formula_29">= K −1 (y t−1 )ˆ τ −1/2 t−1 1 + (y t − ˆ y t ) 2 ˆ τ t−1 −(t−m)/2 .</formula><p>The normalizing integral is given by</p><formula xml:id="formula_30">K(y t−1 ) = √ π 1 − d t Γ t − m − 1 2 /Γ t − m 2 .</formula><p>The proof is omitted. We need t &gt; m + 1 to make the normalizer non-zero. For t &gt; m + 1, the conditional density function is given byˆf byˆ byˆf (y t | y t−1 , X t ) = K −1</p><formula xml:id="formula_31">t−1 ˆ τ −(t−m)/2 t ˆ τ −(t−m−1)/2 t−1 .</formula><p>We see that again the predictor that maximizes the conditional density function is the plug-in predictor ¯ x ′ t b t−1 . By putting the initial density function as some prespecified function q(y m+1 | X m+1 ), which will not play a role in comparison of different models, we get the desired parameter-free density functionˆf</p><formula xml:id="formula_32">functionˆ functionˆf (y n | X n ) = q(y m+1 | X m+1 ) n t=m+2ˆf t=m+2ˆ t=m+2ˆf (y t | y t−1 , X t ).</formula><p>The negative logarithm of this gives the sequentially normalized least squares (SNLS) criterion:</p><formula xml:id="formula_33">SNLS(n, k) = n − m 2 ln(2πeˆτ2πeˆτ n ) + n t=m+1 ln(1 + c t ) + 1 2 ln n + O(1),<label>(19)</label></formula><p>where Stirling's formula has been applied to the Gamma function, and constant terms are implicit in the O(1) term. The SNLS criterion can be used for subset selection and order estimation for both small and large data sets. One of its distinguished properties is the fact that unlike the regular NML universal model it has no hyper-parameters. We conclude this section by a large data set behavior of the SNLS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 If the regressor variables</head><formula xml:id="formula_34">¯ x t satisfy 1 n X n X ′ n = 1 n n i=1 ¯ x i ¯ x ′ i → Σ<label>(20)</label></formula><p>with Σ non-singular, then</p><formula xml:id="formula_35">SNLS(n, k) = n − m 2 ln(2πeˆτ2πeˆτ n ) + 2k + 1 2 ln n + o(ln n).</formula><p>The proof of this and all subsequent theorems are left to the full version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">FIXED REGRESSION MATRIX</head><p>The first theorem shows the mean square deviations in the three representations of data <ref type="formula" target="#formula_15">(9)</ref>, (10), and (11), which are of some interest, and which we will need later on. Since we need the recursive formulas <ref type="formula" target="#formula_3">(13)</ref>, <ref type="formula" target="#formula_5">(14)</ref>, <ref type="formula" target="#formula_7">(15)</ref> we give the results for t &gt; m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 If the regressor variables are non-random satisfying (20) and the data generated by (8), then</head><formula xml:id="formula_36">1 n − m n t=m+1 Ee 2 t = σ 2 1 + 1 n − m n t=m+1 c t (21) 1 n − m n t=m+1 Eê 2 t = σ 2 1 − 1 n − m n t=m+1 d t (22) 1 n − m n t=1 EˆǫEˆǫ 2 t (n) − m t=1 EˆǫEˆǫ 2 t (m) = σ 2 , (23)</formula><p>where the expectation is with the parameters b and σ.</p><p>The next theorem shows the asymptotic optimality of the SNLS model in terms of logarithmic prediction errors, see <ref type="bibr" target="#b8">[9]</ref>, both in the mean and almost surely, in the case where the regressor matrix is fixed. <ref type="formula" target="#formula_2">(20)</ref> hold, and let the data be generated by <ref type="bibr" target="#b7">(8)</ref>. Then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 Let the assumption</head><formula xml:id="formula_37">E SNLS(n, k) = n − m 2 ln(2πeσ 2 )+ k + 1 2 ln n+o(ln n),<label>(24)</label></formula><p>for almost all parameters b and σ. Also,</p><formula xml:id="formula_38">SNLS(n, k) = n − m 2 ln(2πeσ 2 ) + k + 1 2 ln n + o(ln n)<label>(25)</label></formula><p>almost surely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">AR MODELS</head><p>We then consider the case where the data are generated by an AR model,</p><formula xml:id="formula_39">y t = k i=1 a i y t−i + ǫ t , t ≥ 1,<label>(26)</label></formula><p>in which the regressor matrix is random, determined by the the data y n , and where we write the coefficients as a i to avoid confusing them with b i , where the subindex refers to time i. The following theorem shows the almost sure asymptotic optimality of the SNLS model also in this case. <ref type="bibr">(26)</ref>, where the roots of the polynomial 1 − k i=1 a i z i are outside the unit circle, and ǫ t is an i.i.d. zero-mean Gaussian process with variance σ 2 . The process is also assumed to be ergodic and stationary with E¯ x t ¯ x ′ t = Σ nonsingular. Then forˆσforˆ forˆσ 2 n = (1/n) n i=1ˆǫi=1ˆ i=1ˆǫ 2 i (n), we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4 Let the data be generated by an AR model</head><formula xml:id="formula_40">lnˆτlnˆ lnˆτ n = lnˆσlnˆ lnˆσ 2 n − k n − m ln n (1 + o(1)),<label>(27)</label></formula><p>almost surely, and</p><formula xml:id="formula_41">SNLS(k, n) = n − m 2 ln(2πeˆσ2πeˆσ 2 n ) + k + 1 2 ln n + o(ln n)</formula><p>almost surely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">SIMULATION STUDY</head><p>We study the behavior of the proposed SNLS model selection criterion in a simulation study where the AIC, BIC, PLS, and SNLS <ref type="bibr">(Eq. (19)</ref>) methods are used to estimate the order of an AR model. The scripts, in R language, needed to reproduce the experiments are available for download <ref type="bibr" target="#b0">1</ref> . The true order was varied between k * = 1, . . . , 10, and the sample sizes were n = 100, 200, 400, 800, 1600, 3200. The parameters of the AR models are generated by sampling parameter vectors uniformly at random from the range [−1, 1] k * and rejecting combinations that result in unstable processes, until 3000 accepted (stable) models were produced per each (n, k * ) pair. The criteria were evaluated for orders up to k = 15, and the order minimizing each criterion was chosen as the estimate. <ref type="table" target="#tab_1">Tables 1 and 2</ref> report the percentage of correctly estimated orders for each true order k * and sample size n. For the lowest orders, k = 1, 2, the BIC criterion is clearly the most accurate one and wins for almost all sample sizes; this was expected since BIC is known to have a tendency to underestimate rather than overestimate the order. Likewise, it is not too surprising that AIC, which a priori favors more complex models than the other criteria, wins for the smallest sample size whenever k ≥ 5. For the orders k = 3, 4, 5, BIC, PLS, NML, and SNLS share the first place, the last one somewhat more often than the others. For orders k = 5, . . . , 10, <ref type="table" target="#tab_2">Table 2</ref>, SNLS is clearly the best method, with the exception of the smallest sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the Academy of Finland under the project Civi and by the Finnish Funding Agency for Technology and Innovation under the projects Kukot and PMMA. In addition, this work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Percentages of correctly estimated orders, k  *  = 1, . . . , 5 (to be continued in Table 2) The score of the best 
method in each case is typeset in boldface. 

sample size, n 
50 
100 
200 
400 
800 
1600 
3200 
k = 1 
AIC 
70.5 
71.3 
72.0 
70.0 
71.4 
70.8 
70.9 
BIC 
93.5 
96.9 
97.9 
98.0 
99.4 
99.5 
99.4 
PLS 
75.8 
86.3 
91.1 
93.5 
96.7 
97.8 
98.1 
NML 
82.5 
88.3 
89.7 
91.5 
94.3 
95.9 
96.6 
SNLS 
78.5 
87.5 
92.2 
93.9 
97.0 
98.1 
98.3 
k = 2 
AIC 
52.1 
58.0 
64.1 
64.6 
66.4 
69.0 
68.4 
BIC 
61.3 
69.3 
78.2 
83.0 
88.0 
90.0 
93.8 
PLS 
52.7 
65.5 
76.4 
81.7 
86.6 
89.7 
93.5 
NML 
59.7 
68.3 
76.9 
82.4 
86.4 
89.8 
93.6 
SNLS 
53.8 
66.1 
77.5 
82.1 
86.3 
90.1 
93.6 
k = 3 
AIC 
47.1 
55.5 
59.2 
63.6 
66.5 
68.6 
69.2 
BIC 
49.5 
63.5 
72.3 
79.2 
84.6 
88.3 
92.2 
PLS 
45.3 
61.8 
71.7 
79.1 
84.8 
88.7 
92.6 
NML 
49.7 
63.1 
72.1 
79.5 
84.5 
88.3 
92.4 
SNLS 
46.5 
63.0 
71.1 
79.3 
84.9 
88.6 
92.6 
k = 4 
AIC 
42.8 
52.5 
60.1 
63.3 
65.4 
66.5 
67.5 
BIC 
45.7 
59.6 
67.8 
76.5 
82.6 
88.3 
91.4 
PLS 
42.1 
58.3 
68.5 
77.0 
82.5 
88.3 
91.9 
NML 
45.0 
60.2 
68.0 
76.7 
82.5 
88.0 
91.6 
SNLS 
42.4 
59.2 
69.4 
77.0 
82.4 
88.5 
92.0 
k = 5 
AIC 
39.7 
49.6 
56.9 
60.5 
65.7 
67.1 
66.8 
BIC 
39.0 
52.1 
65.4 
74.8 
80.5 
85.8 
90.4 
PLS 
39.1 
53.4 
65.8 
75.0 
81.0 
86.1 
90.4 
NML 
39.2 
52.1 
66.2 
74.8 
81.0 
85.8 
90.4 
SNLS 
39.4 
54.2 
66.1 
76.0 
81.0 
86.0 
90.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Percentages of correctly estimated orders, k  *  = 6, . . . , 10 (continued from Table 1). The score of the best method 
in each case is typeset in boldface. 

sample size, n 
50 
100 
200 
400 
800 
1600 
3200 
k = 6 
AIC 
37.9 
51.0 
56.5 
59.5 
64.1 
68.7 
68.2 
BIC 
35.4 
51.8 
62.9 
71.3 
79.8 
86.6 
90.4 
PLS 
34.7 
52.3 
62.9 
72.0 
80.1 
86.7 
90.7 
NML 
35.6 
52.8 
63.2 
71.7 
79.9 
86.5 
90.6 
SNLS 
36.3 
53.3 
64.0 
72.4 
80.3 
86.8 
90.5 
k = 7 
AIC 
33.7 
45.4 
55.3 
59.6 
63.6 
65.7 
67.3 
BIC 
29.2 
43.4 
59.1 
69.5 
77.9 
82.8 
88.6 
PLS 
30.0 
44.7 
60.5 
70.0 
78.5 
82.9 
88.6 
NML 
28.8 
44.2 
59.8 
69.8 
78.3 
83.0 
88.4 
SNLS 
30.1 
46.5 
61.2 
70.6 
79.4 
83.2 
88.9 
k = 8 
AIC 
34.4 
45.9 
55.7 
59.6 
64.5 
66.3 
67.6 
BIC 
26.9 
43.0 
57.6 
69.1 
78.2 
81.4 
86.5 
PLS 
28.8 
44.6 
58.8 
69.1 
77.9 
82.0 
86.4 
NML 
26.9 
43.6 
58.2 
69.7 
78.8 
81.8 
86.8 
SNLS 
28.8 
45.9 
59.2 
69.8 
79.0 
82.4 
86.8 
k = 9 
AIC 
30.0 
44.1 
52.8 
59.0 
64.3 
64.9 
69.1 
BIC 
23.1 
39.1 
52.8 
66.1 
75.7 
82.2 
86.3 
PLS 
23.8 
40.2 
53.3 
67.1 
75.9 
81.5 
86.3 
NML 
22.4 
39.6 
53.7 
66.8 
76.6 
82.3 
86.7 
SNLS 
24.6 
42.0 
55.0 
67.8 
76.7 
82.2 
86.9 
k = 10 
AIC 
28.5 
43.9 
51.5 
59.3 
64.2 
67.1 
67.7 
BIC 
20.6 
35.7 
51.0 
66.1 
74.4 
81.4 
85.5 
PLS 
20.1 
35.7 
50.7 
65.0 
73.4 
80.8 
84.8 
NML 
20.2 
37.1 
51.9 
66.8 
74.6 
81.4 
85.8 
SNLS 
21.4 
37.9 
52.3 
66.5 
74.8 
81.8 
85.6 </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The minimum description length principle in coding and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2743" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Universal sequential coding of single messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shtarkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Problems of Information Transmission</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning using factorized NML universal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Silander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Information Theory and Applications Workshop</title>
		<meeting>Information Theory and Applications Workshop</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ITA-08)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The last-step minimax algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Takimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th International Con</title>
		<meeting>11th International Con</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Order determination and adaptive control of ARX models using the PLS criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Hemerly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recursive estimation of autoregressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Mcdougall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Poskit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statist. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="166" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On predictive least squares principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A predictive least squares principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA J. Math. Control Inform</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fisher information and stochastic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MDL denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2537" to="2543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MDL denoising revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Myllymäki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:cs/0609138</idno>
		<imprint>
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Information and Complexity in Statistical Modeling, Information Science and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">140</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some theorems in least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Plackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="149" to="157" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
