<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimistic Replication for Internet Data Services</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Verlag</publisher>
				<availability status="unknown"><p>Copyright Springer Verlag</p>
				</availability>
				<date type="published" when="2000-10">October 2000. 1914</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Saito</surname></persName>
							<email>yasushi@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
							<email>levy@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimistic Replication for Internet Data Services</title>
					</analytic>
					<monogr>
						<title level="m">This report originally appeared in the 14th International Confer-ence in Distributed Computing (DISC)</title>
						<meeting> <address><addrLine>Toledo, Spain</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Springer Verlag</publisher>
							<date type="published" when="2000-10">October 2000. 1914</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a new replication algorithm that supports replica-tion of a large number of objects on a diverse set of nodes. The algorithm allows replica sets to be changed dynamically on a per-object basis. It tolerates most types of failures, including multiple node failures, network partitions, and sudden node retirements. These advantages make the algorithm particularly attractive in large cluster-based data services that experience frequent failures and configuration changes. We prove the correctness of the algorithm and show that its performance is near-optimal.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>Our work derives from the Porcupine cluster-based mail server project <ref type="bibr" target="#b19">[19]</ref>. Porcupine connects up to several hundred offthe-shelf computers to serve billions of mail messages per day. Porcupine's architecture is fully dynamic; any node can potentially manage any user's profile and store any user's email messages. For each incoming message, Porcupine chooses a set of nodes on which to replicate and store the message (called a replica set), based on node load and message affinity. A cluster membership service and a distributed naming service keep track of the locations of users' messages. The dynamic message placement yields many benefits: performance improvement via balanced load and flexible support for system configuration changes by message migration.</p><p>From the viewpoint of a replication service, Porcupine presents an environment very different from traditional database systems. Following are the key characteristics of Porcupine and a discussion of how they define the goals of our replication service:</p><p>Frequent failures: With hundreds of nodes in the cluster, a part of the system is always down. First, the algorithm must provide strong fault tolerance by maintaining replica consistency even when some of the nodes are down, sometimes permanently. Second, the algorithm must be non-blocking, that is, it must allow reads and writes to any replica any time regardless of whether peer replicas are reachable.</p><p>Changing replica sets: Porcupine needs to change email message replica sets automatically to react to node additions and removals. We must support dynamic addition and removal of replicas while allowing contents updates to the object.</p><p>Small object size: The unit of replication in Porcupine is an email message whose average size is 5K bytes, as opposed to many gigabytes typical in database systems. We need to minimize the space overhead of per-object data structures used to maintain replica consistency.</p><p>Selective replication: Porcupine stores billions of email messages, each of which is in its own replica set. Our algorithm needs to be quiescent, that is, it should incur no computational and space overhead when no update is in progress. Moreover, email messages are deleted frequently. Thus, our system should support quick object deletion without leaving any data structures behind.</p><p>Weak consistency requirements: Services such as email do not demand strict replica consistency because the possibility of inconsistent data is inherent in the environment; for example, unreliable network transport can cause delivery delay or duplicate messages. Thus, we only need to support eventual consistency of replica state.</p><p>These service requirements are not unique to email -in fact, they are shared by many other Internet applications, including Usenet <ref type="bibr" target="#b20">[20]</ref>, Internet-based BBS services (e.g., slashdot.org or delphi.com), naming services <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>, and widearea mirroring of Web or FTP data <ref type="bibr" target="#b17">[17]</ref>. All these services are potential applications of our replication algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Data replication has been studied and deployed widely. However, previous work in this area has addressed the goals of our algorithm only in a piecemeal fashion and has not solved all of the problems that we face in our intended environment.</p><p>Traditional replication algorithms include primary-copy algorithms used widely in commercial database systems <ref type="bibr" target="#b2">[3]</ref>, quorum consensus algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, and atomic broadcast protocols <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref>. They try to achieve single-copy semantics, that is, they give users an illusion of having a single, highly available copy of an object. Although generic, these algorithms fail to address problems we face -frequent failures and frequent changes -because they sacrifice availability by prohibiting accesses to a replica when data is not provably up to date.</p><p>Mobile replicated database systems (e.g., Bayou <ref type="bibr" target="#b15">[16]</ref>, and Roam <ref type="bibr" target="#b18">[18]</ref>) share some of our goals: elimination of a single point of failure, handling frequent failures, and dynamic replica addition and deletion. The main difference between their solutions and ours is that these systems focus on minimizing the communication overhead, whereas we focus on minimizing the space and the computation overhead. For example, the techniques used by these systems, including on-demand polling and a semantic log for describing updates, reduce the communication cost but increase both the computation and the storage overhead.</p><p>Our algorithm is most closely related to multi-master widearea replicated services, including Active Directory <ref type="bibr" target="#b12">[13]</ref>, Xerox Clearinghouse <ref type="bibr" target="#b5">[6]</ref>, and Usenet <ref type="bibr" target="#b20">[20]</ref>. These systems provide non-blocking accesses, support replication of many small objects, and propagate updates efficiently over unreliable links. However, they do not support replica set changes and provide only a weak fault tolerance; for example, one failed node can stall the update propagation of the entire system. Moreover, the existing systems do not support quick object deletion and require storing update records (often called "death certificates") for an indefinitely long period.</p><p>Several systems allow dynamically changing the placement of replicas using reference-monitoring mechanisms to balance the system load <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23]</ref>. They update replicas by gossiping changes along a spanning tree and are unable to achieve replica consistency even under a single node failure. Our work complements them by proposing a robust mechanism that can tolerate a wider variety of failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Overview of the Algorithm</head><p>Our algorithm is based on three principles: state transfer, update resolution using Thomas write rule, and update retirement using synchronized clocks.</p><p>In its basic form, our algorithm is similar to systems such as Active Directory <ref type="bibr" target="#b12">[13]</ref> and Usenet <ref type="bibr" target="#b20">[20]</ref>. Any replica (or any node for a newly created object) can issue an update any time. A coordinator, usually the issuer of the update, propagates the update by pushing the new object state to others in background. Conflicting updates are resolved by Thomas write rule <ref type="bibr" target="#b21">[21]</ref>, that is, by attaching timestamps to them and accepting only the newest update.</p><p>Our algorithm is unique in its uniform handling of replica set updates and contents updates -in fact, an update is actually a tuple consisting of the new object contents and the new replica set. For an update that changes the replica set, the coordinator pushes the update to the union of the old and the new replica sets (called the targets of the update). A node receiving the update either modifies, creates, or deletes a replica depending on whether or not it appears in the new replica set. Thomas write rule is again used to resolve conflicts among replica set changes; the older updates are canceled by forwarding the newest update to their targets and letting them be rolled back. Overriding the older updates requires the coordinator of the newer update to discover the older updates' targets. This node discovery problem is similar to the distributed resource discovery problem <ref type="bibr" target="#b7">[8]</ref>, and the solution is also similar: the nodes that receive the update send back to the coordinator the sets of nodes they know and let the coordinator expand its target node set transitively.</p><p>We apply at-most-once messaging technique using synchronized clocks <ref type="bibr" target="#b11">[12]</ref> to retire updates. After the coordinator completes update propagation, it sends out retirement notices to the target nodes. Upon receiving a retirement notice, a node deletes the update from disk after waiting for a fixed period; the wait ensures that the node will never apply a stale update in the future.</p><p>This combination of state transfer, Thomas write rule, and quick update retirement allows our algorithm to solve our goals effectively as follows:</p><p>• Our algorithm's basic design directly achieves the three goals -non-blocking access, dynamic replica set changes, and eventual consistency.</p><p>• We achieve strong fault tolerance in two ways. First, our algorithm is fully decentralized -in particular, it lets any node take over the task of the coordinator any time. Second, its node discovery process eliminates a single point of failure quickly and lets the system tolerate a sudden node retirement without compromising replica consistency.</p><p>• Our algorithm's space overhead is quite small for two reasons. First, the state transfer architecture minimizes the size of the update record by omitting the new object contents -the object contents are usually read from the replica directly. Second, our algorithm quickly reclaims the space occupied by update records by retiring them as soon as they finish propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Structure of the Paper</head><p>The rest of the paper is structured as follows. We describe our algorithm in detail in Section 2 and show two examples in Section 3 to elucidate its behavior, in particular, the resolution of concurrent updates. Section 4 proves the correctness of our algorithm. In Section 5, we discuss several extensions to the basic algorithm to address issues that arise in practice: e.g., optimizations to make the algorithm work efficiently and the handling of long-term failures. We briefly discuss the computational and the space overhead of the algorithm in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Replication Algorithm</head><p>Although our algorithm is designed to support many objects replicated on a diverse set of nodes, it is first presented in the  context of a single object. We describe a straightforward extension of the basic algorithm to support multiple objects in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Model and Assumptions</head><p>We make the following assumptions about the environment. First, the nodes in the system can communicate through a fully connected network. Second, nodes and network links may crash, and messages may be reordered, delayed, or lost, but Byzantine failures will not occur. Finally, the nodes have loosely synchronized clocks; clock synchronization algorithms are well known and are deployed widely <ref type="bibr" target="#b13">[14]</ref>.</p><p>Our algorithm only propagates updates. Locating and reading replicas and choosing the replica placement are outside the scope of this algorithm. For locating replicas, any weakly consistent naming service can be used. For replication policy, an object could be assigned to a random set of nodes <ref type="bibr" target="#b19">[19]</ref> or to a set determined by reference-monitoring mechanisms <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b23">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Notational Conventions</head><p>All global variables are stored on stable storage and survive node crashes. Procedures marked public run as transactions that are non-preemptive and update global variables atomically. val 1 , val 2 is a tuple of two values. Send(node, proc, args. . . ) sends a message to node and requests calling the procedure proc with args. . . . Send does not wait for proc to finish; it merely queues the message. Texts after ' // ' are comments. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the types used in the algorithm. Loosely synchronized clocks order updates <ref type="bibr" target="#b13">[14]</ref>. Other types of clocks, e.g., logical clocks <ref type="bibr" target="#b10">[11]</ref>, may be used without affecting the correctness of the algorithm, but wall clocks best suit our purpose because they can order logically unrelated events (e.g., a user contacting two nodes in a cluster serially). The procedure Now() returns the current local clock value. We assume the clock resolution to be fine enough that successive calls to An update to the object is represented by the Update record. Its state field indicates the update's status. A new update starts as being ACTIVE. An update is RETIRING on the coordinator while retirement notifications are sent, and it is RETIRED after the reception of a retirement notice. An update is SUS-PENDED when it is found to conflict with a newer update, and it stays dormant until the newer update arrives and supersedes it. Target, done, and peer fields specify the set of nodes that should receive the update, have acknowledged the update, and should replicate the object, respectively. Thus, done and peer are always subsets of target. The update propagation finishes when done = target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Structures</head><p>Five persistent variables are stored per replica on a node <ref type="figure" target="#fig_2">(Fig. 2)</ref>. Two of them, gData and gPeers, are visible to the application and the rest are used internally by the replication algorithm. GData stores the actual contents of the objectwe are not concerned about the object's internal structure in this paper. GPeer shows, to the best of the node's knowledge, the replica set of the object. GU remembers the newest update applied on the object. Notice the absence of the new object contents in gU -the contents are propagated to other nodes by reading from gData directly most of the time. The exception is when gData is deleted by an update, but the object contents still need to be propagated to other nodes (this happens, for example, when the object is moved from one node to another). GSavedData is used to save the new object contents in such cases, and it is otherwise null. That gSavedData is usually null contributes to reducing the space overhead of the algorithm, because all other data structures used by the algorithm are of small and fixed size. GRetireTime is used to delete a retired update and is discussed further in Section 2.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Application Programming Interface</head><p>One procedure, UpdateObject <ref type="figure" target="#fig_3">(Fig. 3)</ref>, is called by the application. It takes two parameters, the new replica set (peer) and the new object contents (data). Passing an empty set to peer will delete the object entirely from the system. The caller of this procedure must ensure that the node stores a replica already, except when the object is being newly created. This restriction, also discussed in Section 4.2, is to prevent creating an orphan replica that is disconnected from others and is not found by the node discovery process. The implicit variable me shows the name of the node itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Update Application</head><p>The procedure ApplyUpdate <ref type="figure" target="#fig_7">(Fig. 4)</ref>, called both from the local application and from remote nodes, logs and applies the update to the replica and prepares for update propagation. It first merges the target sets of both u and the current update, gU 1 4 . This must be done even when u is to be discarded 2 so that the participants of both updates can eventually receive the newer update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Update Propagation</head><p>An update is pushed to other nodes periodically by PushApply <ref type="figure">(Fig. 5)</ref>. The target node set expands as replies come back from the target nodes 6. The propagation finishes when all the target nodes reply 4.</p><p>The function IAmCoordinator tells whether the node is designated to coordinate a particular update. For now, it just returns true, meaning that any node can be a coordinator, and that an update is flooded among all the target nodes. Having multiple coordinators does not affect the correctness of the algorithm, but it surely wastes the network bandwidth -we improve this design in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Deleting Retired Updates</head><p>While each update record occupies only a small space, it is stored even when the replica itself is removed. We need to delete updates in a timely manner; otherwise, the update records of deleted replicas will accumulate and eventually fill the disk up. An update is deleted from disk in two steps. The first step, performed periodically by PushRetire <ref type="figure">(Fig. 6)</ref>, is the update retirement; the coordinator informs the target nodes that update propagation is complete. The second step is the update removal in which we apply the at-most-once messaging algorithm <ref type="bibr" target="#b11">[12]</ref> to remove retired updates without being confused </p><note type="other">// Reject if u is stale. 2 if gU = NULL ∧ gU.ts &gt; u.ts return false // Log the update. 3 gU ← u gSavedData ← NULL // Modify the replica. if me ∈ gU.peer gData, gPeer ← data, gU.peer else gData, gPeer ← NULL, φ if gU.peer = φ // Save data; not needed when peer is // φ since everyone deletes the replica. gSavedData ← data gU.done ← gU.done ∪ {me} return true Figure 4. Local update application. This procedure logs and applies the update to the local replica. It returns whether the update was successfully applied or not.</note><p>by out-of-order update messages. Here, the node simply waits for WAIT seconds before deleting a retired update <ref type="figure">(Fig. 6</ref>, RemoveUpdate). WAIT is the sum of the maximum clock skew among nodes and the message lifetime, an interval long enough that almost all the messages will arrive at the destination within the interval.</p><p>This update removal scheme additionally requires each node to discard stale incoming network messages <ref type="figure">(Fig. 6</ref> MessageArrived). Here, each network message is stamped with the sender's clock value and is accepted by the receiver only when its timestamp is no older than WAIT on the receiver's clock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Examples</head><p>We show two examples to illustrate the behavior of the algorithm. The first example is a simple contents update. The second example demonstrates the node discovery process used to resolve conflicting replica set changes.</p><p>(1)    (5) B and C change U.state to RETIRED and reply to A. A receives the replies from B and C and changes U.state to RE-TIRED.</p><p>(6) WAIT seconds later, A, B, and C erase U from gU. </p><p>(5) (6)  <ref type="figure" target="#fig_9">Fig. 8</ref> shows a scenario in which an object replicated on nodes A and B is updated concurrently, first by A that adds C to the replica set, and next by B that adds D to the replica set. We assume that B's update is newer than A's.</p><p>(1) A issues U A :target=peer={A,B,C}} and modifies its gPeer.</p><p>Simultaneously, B issues U B : target=peer={A,B,D}} and modifies its gPeer.</p><p>(2) A pushes U A to B and C. B pushes U B to A and D. Now, for the sake of explanation, suppose C and D receive the updates before B and A do.  <ref type="figure">(Fig. 5 5)</ref>.</p><p>(6) B receives the reply from A and pushes U B to C, the only target not yet contacted. On reception of U B , C recognizes that it is not a part of the new replica set, removes its replica, and replies U B , true, {A,B,C}, {A,B,C,D}} to B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Correctness Proof</head><p>While being simple, our algorithm contains several subtleties, especially regarding replica additions and deletions. For example, how does it guarantee that all replicas receive an update, when another update is adding replicas concurrently? In this section, we prove two main safety properties of the algorithm: all nodes receive the newest update at the end of propagation, and no node accepts stale updates regardless of concurrent updates. We also argue the liveness of the algorithm, i.e., all the replicas will receive the newest update, by applying our safety arguments.</p><p>The state of the system can be viewed as a directed graph, called the knowledge graph, in which the vertices represent nodes and the edges represent the nodes' knowledge of others through gPeer and gU.targets. The graph is usually complete (i.e., no update is issued and the value of gPeer is identical on all the replicas), but it becomes incomplete during replica addition or deletion. At a high level, our proof shows that the algorithm ripples the newest update through the graph, adding edges to the graph along the way to cover all the nodes and to restore the completeness of the graph eventually.</p><p>Following are notations used in the ensuing proof. Other symbols are summarized in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>• A node "stores a replica" when gData =NULL.</p><p>• A node "has an update" when its gU.state is ACTIVE or RETIRING.</p><p>• A node "retires an update" when it sets gU.state to RE-TIRED.</p><p>• A node n 1 "knows" another node n 2 when either n 1 has an update and n 2 ∈ n 1 :gU.target, or n 2 ∈ n 1 :gPeer.</p><p>• G T ≡ G T , E G T , a knowledge graph for the object at time T 5 , is defined as follows.</p><p>G T = Nodes that store a replica or have an update.</p><formula xml:id="formula_5">E G T = {v 1 → v 2 | {v 1 , v 2 } ⊆ G T ∧ v 1 knows v 2 }</formula><p>• S T ≡ S T , E S T , an induced subgraph of G T , excludes from G T vertices that correspond to failed nodes and the associated edges. S T shows the knowledge graph in the presence of failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Correctness Criteria</head><p>Ideally, we want to prove that the algorithm keeps all the live replicas consistent regardless of types of failures. Such a guarantee, however, is impossible when nodes or links fail in a way that makes the corresponding induced subgraph disconnected. For example, suppose two nodes fail simultaneously after both have created two new replicas, as illustrated in <ref type="figure" target="#fig_11">Fig. 9</ref>. After such a failure, any update issued on the two new replicas will not reach each other, and the new replicas will remain inconsistent until the original two nodes recover. Therefore, we define the correctness only under the condition that a knowledge subgraph is at least weakly connected.</p><p>Correctness criteria: Suppose S T s is weakly connected, and no node or link fails and no new update is issued during a long enough period (T s , T e ). Let U be the newest update generated before T e . The algorithm is correct if the following conditions hold.</p><p>(1) Every node n ∈ U.peer applies U before T e .</p><p>(2) No node n ∈ U.peer stores a replica at T e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meaning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n:var</head><p>The value of variable var on node n. n:U.target,T The value of U.target on node n just before T . n:U.done,T</p><p>The value of U.done on node n just before T .</p><formula xml:id="formula_6">n 1 G T → n 2 n 1 knows n 2 in G T , i.e., (n 1 → n 2 ) ∈ E G T . n 1 G T</formula><p>; n 2 A path from n 1 to n 2 exists, i.e., n 1  .ts &lt; U.ts) is applied on any node after U is applied.</p><formula xml:id="formula_7">G T → n 2 ∨ ∃n , n 1 G T → n ∧ n G T ; n 2 .</formula><p>Notice that these criteria demand, in case of a graph disconnection, a replica consistency within each partition, and that as soon as the partitions re-integrate, all the replicas converge onto the globally newest state. Indeed, this set of criteria is as strong as any non-blocking replication algorithm can guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Invariants</head><p>Theorem 1 If S T is strongly connected, then ∀T &gt; T , S T is also strongly connected if no node or link fails during the period (T, T ). Theorem 2 If S T is weakly connected, then ∀T &gt; T , S T is weakly connected if no node or link fails during the period (T, T ).</p><p>Proof sketch. We show by induction that no transition on the subgraph can disconnect the subgraph. First, a replica creation will not disconnect the graph because a new replica is always created by "stemming out" from an existing replica (Section 2.4). Next, replica deletion does not change the graph shape because the update record is still stored on the same node. Finally, update retirement will not disconnect the graph because an update retires only after all the peer replicas have spanned the edges to one another. Theorem 2 can be proved exactly the same way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 G T is strongly connected for all T .</head><p>Proof. The graph is clearly connected in the base case in which no replica exists. Thus, G T is connected for all T from Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">All Replicas Receive the Newest Update</head><p>Now, we prove that all the nodes receive the newest update by distinguishing two cases. Theorem 4 proves that if an update retires, all the nodes must have received the update. Theorem 5 proves that when an update is unable to retire because some of its targets are dead, all the remaining nodes still receive the update. These two theorems together prove the correctness criteria (1) and (2).</p><p>Theorem 4 Suppose S T s is weakly connected, and no node or link fails and no new update is issued during a long enough period (T s , T e ). Let U be the newest update generated before T e . If a coordinator c begins the retirement of U at time T (T &lt; T e ), then S T e ⊆ c:U.done,T; that is, all the nodes in S T e have received and applied U.</p><p>Proof. We only need to prove that S T ⊆ c:U.done,T; if S T ⊆ c:U.done,T, then S T e ⊆ S T because no newer update is generated after T and no node possesses a stale update after T .</p><p>For the sake of contradiction, suppose S T ⊆ c:U.done,T. Because S T is weakly connected from Theorem 2, we can pick a node p ∈ c:U.done,T such that ∃n ∈ S T − c:U.done,T and that either n S T → p or p S T → n. Below, we show that no such pair of p and n can exist.</p><p>First, suppose a pair (p, n) with an edge p S T → n exists <ref type="figure" target="#fig_1">(Fig.  10 (a)</ref>). Let T p be the time c propagated U to p (T p &lt; T ). First, the edge p S T → n must have been created at or before T p , because U is the newest update and any other update that could have created p → n would have been rejected by p after T p . On the other hand, p S T → n must have been created after T p ; otherwise, the edge c S T → n must be in the graph <ref type="figure">(Fig. 5 6)</ref>. Because of this contradiction, this pair (p, n) cannot exist. Second, suppose only a pair (p, n ) with an edge n S T → p exists <ref type="figure" target="#fig_1">(Fig. 10 (b)</ref>).</p><p>From Theorem 3, a path c G T</p><p>; n exists in the full graph G T (remember, G T may include dead nodes). Therefore, there exists a dead or uncommunicative node q ∈ c:U.target,T along the path c G T</p><p>; n , and q makes U unable to retire in the first place. Therefore, this pair nodes (p, n ) cannot exist as well. Therefore, for U to retire, S T ⊆ c:U.done,T. → n, and we can show that a pair (p, n) with an edge p S T → n cannot exist. Now, suppose only a pair (p, n ) with an edge n S T → p exists. For the moment, lets assume the following lemma holds for any pair of nodes, n 1 and n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5 Suppose S T s is weakly connected, and no node or link fails and no new update is issued during a long enough</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 If n 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S T</head><p>→ n 2 but not n 2 S T → n 1 , then n 1 has an update.</p><p>From this lemma, n has an update, say, U . Thus, n contacts p, which in turn causes n to discover c <ref type="figure">(Fig. 5 6)</ref>, which in turn cause n to propagate U to c, thereby letting c discover n before T e <ref type="figure" target="#fig_1">(Fig. 4 1)</ref>. Therefore, a pair (p, n) with an edge n S T → p cannot exist as well. Thus, S T e ⊆ c:U.done,T e . We sketch the proof of Lemma 1. Edges can disappear only when an update retires. For an update to retire, all the target nodes need to reply, that is, edges must span between any pair of the target nodes. Thus, when n 1 S T → n 2 but not n 2 S T → n 1 , then n 1 must have an update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">No Node Receives a Stale Update</head><p>Theorem 6 Suppose no update is generated for a long period ending at T e . Let U be the newest update issued before T e . After U retires, no update older than U is applied on any node. Proof. A node may receive an older update after it retires U for two potential reasons: (1) another node that has not received U propagates a stale update, or (2) a network delay causes a message containing a stale update to be received after U retires. Theorem 4 prevents the case (1). The use of synchronized clocks, described in Section 2.7, prevents the case (2) (refer to <ref type="bibr" target="#b11">[12]</ref> for the full proof).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Liveness</head><p>Liveness is derived immediately from the work of the algorithm. The algorithm sends update or retire messages to the nodes in gU.target until it receives the replies from all the nodes. GU.target may grow as a result of reply processing <ref type="figure" target="#fig_1">(Fig. 4 1)</ref>, but because its size is finite, the coordinator will eventually push the update to all the nodes it can communicate with 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extensions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Supporting Multiple Objects</head><p>All the discussions so far have focused on a single object, but in fact, the basic algorithm can be extended easily to support multiple objects. To support multiple objects, instead of the variables gU, gSavedData, and gRetireTime, we now have a persistent table that partially maps an object ID to an update in progress for the object. An update is added to the table when an object is going to be modified <ref type="figure" target="#fig_3">(Fig. 4 3)</ref>, and is deleted from the table when it is removed <ref type="figure" target="#fig_4">(Fig. 6 7)</ref> or is superseded by a newer update for the same object <ref type="figure" target="#fig_7">(Fig. 4</ref> 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Designated Coordinator</head><p>The basic algorithm presented so far is inefficient because it floods an update among all the target nodes in PushApply, causing an update to be sent as many as (N − 1) 2 times (N is the number of replicas). By combining the use of a group membership service <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref> and a simple change to the function IAmCoordinator ( <ref type="figure" target="#fig_1">Figures 5 and 11</ref>), however, we can reduce the cost to N − 1 in the common case. In the new implementation, a node pushes or retires an update only when it is the issuer of the update or when it is designated to take over the failed issuer. Notice that because the membership service is shared by all the objects hosted on a node, its cost is amortized over many runs of the algorithm and becomes negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Delaying Update Retirements</head><p>The algorithm is further optimized by delaying and aggregating calls to PushRetire for different objects to the same node. Delaying calls to PushRetire does not affect the replica consistency; it merely delays the deletion of the update record and increases the size of the update table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Optimistic Deltas</head><p>Instead of pushing the entire object state every time, we can send optimistic deltas <ref type="bibr" target="#b1">[2]</ref> to save the network and the computational cost. Here, a coordinator simply pretends that all the replicas for the object were consistent before the update and pushes only the difference between the old and the new contents (called the optimistic delta) along with the fingerprint of the old replica contents. On the receiver side, a node applies the update when its replica's fingerprint matches the update's; otherwise, the node requests a full contents transfer from the coordinator. This technique can reduce the cost of the algorithm, especially during replica set changes, in the common case without concurrent updates. Fingerprint is any short bit-string that summarizes the replica contents. Applying a collision-resistant hash function (e.g., MD5) on the replica contents is one way to compute a fingerprint. A faster, more accurate, but slightly more spaceconsuming alternative is to store along with each replica a timestamp <ref type="figure" target="#fig_1">(Figures 1 and 4</ref>) that shows the last time the replica was modified, and to use the timestamp as a fingerprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Handling Long-term Failures</head><p>In the real world, computers often crash and never recover. Such nodes create an unbounded amount of backlog of updates that eventually fill up the disks on other nodes. Our algorithm handles such a situation automatically by purging nodes that remain down for too long.</p><p>When a node finds another node dead for more than a predefined purge period (e.g., one week), it pretends that it received affirmative replies from the dead node for all its jammed updates. The node then purges the dead node simply by removing the dead node's name from the replica sets of all the replicas stored on the node. To avoid having inconsistent data, when a node recovers after being down for the purge period or longer, it clears its disk contents and rejoins the cluster with a new node ID.</p><p>The only remaining problem is when nodes or links fail in such a way as to make the knowledge graph disconnected, and they remain failed until the purge deadline. In such case, the aforementioned scheme may make replicas permanently inconsistent. We argue below that such a scenario is highly unlikely to happen in practice.</p><p>How can a graph become disconnected? One cause of a graph disconnection is link failures (i.e., network partitioning). Another cause, which may happen without link failures, is multiple node failures combined with concurrent replica additions, illustrated in <ref type="figure" target="#fig_11">Fig. 9</ref>. Now, can a graph disconnection last until the purge period? The answer is no, for all practical purposes. First, a network partitioning would never last long because it is repaired simply by installing replacement parts. Second, the latter failure scenario would not happen in practice because it requires a combination of simultaneous replica creations and coincidental sudden long-term failures of multiple nodesthe window of vulnerability is very narrow for both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Networking and Computational Overhead</head><p>With the optimizations described in Section 5.2, our algorithm pushes an update to N replicas in the common case and aggregates retirement notices into one batch notice. In total, the algorithm sends 2(1 + 1 G )N messages per update, where G is the average aggregation factor for retirement notices. In Porcupine, the value of G is around 20 under heavy load, and the networking and the processing costs of our algorithm is close to 2N per update, which is the optimal number for an algorithm that does not batch (and thus delay) update propagation -N + N messages are always needed to propagate and acknowledge an update to N nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Space Overhead</head><p>This algorithm stores two types of data structures per replica in addition to the contents: the replica set (gPeer in <ref type="figure" target="#fig_1">Fig. 1)</ref>, and the update record (gU, gSavedData, and gRetireTime in <ref type="figure" target="#fig_1">Fig.  1</ref>).</p><p>The replica set information consumes small space -typically a few bytes per replica -and it is stored only when the replica itself is present.</p><p>An update record is stored on disk only while the update is in progress. The space consumed by update records on a node is (S + αM/R)UD. Here, (S + αM/R) shows the average space overhead of an update record on a node: S is the size of gU and gRetireTime, α is the proportion of updates that shrink the replica set size, M is the average object size, and R is the average replication factor for objects -thus, αM/R shows the time-averaged space overhead of gSavedData. U is the average number of objects updated per second and D is the average update lifetime, including the deletion wait period (Section 2.7) and delays introduced by retirement-aggregation (Section 5.2). In Porcupine, S ≈ 60 bytes, α ≈ 1/50, M ≈ 5000 bytes, R ≈ 2, U ≈ 30, and D ≈ 120. Thus, total amount of stable storage used is about four hundred kilobytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have described a new decentralized replication algorithm designed for Internet servers in this paper. Following are key features of our algorithm.</p><p>• Eventual consistency under most failure types, e.g., node</p><p>and link node failures and sudden node retirements.</p><p>• Any replica can issue updates any time.</p><p>• Support for dynamic replica addition and deletion.</p><p>• Minimal space overhead, especially, efficient object deletion.</p><p>• Minimal computational and networking overhead in the common case.</p><p>As future work, we plan to investigate the space, time, and computation complexities of the algorithm under update conflicts. In addition, we are studying the implementation of semantically richer operations, e.g., multi-object transactions, on top of our algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>type</head><label></label><figDesc>Timestamp = record time: Clock // wall-clock time. nid: NodeID // a tie-breaker. end type Update = record state: {ACTIVE, RETIRING, RETIRED, SUSPENDED} ts: Timestamp target, done, peer: SetNodeID end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Data structures used by the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Per-node, per-object global variables used by the algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. UpdateObject is called by the application to create a new object, modify the object contents, add or remove the replica set, or delete the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. An object replicated on nodes A, B, and C is updated. Gray circles indicate that nodes that have applied the update. The letter "U" indicates that the update is logged on the node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 shows a sequence of steps performed to update an object replicated on nodes A, B, and C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>A issues an update U:target=peer={A,B,C}} and modifies its replica. (2) A pushes U to B and C. (3) B applies U and returns U.ts, true, {A,B}, {A,B,C}} to A. C applies U and returns U.ts, true, {A,C}, {A,B,C}} to A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 4 )</head><label>4</label><figDesc>A receives the replies from B and C and changes U.state to RETIRING. A sends U's retirement to B and C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Conflicting updates involving replica addition. Gray circles are nodes that applied U A , and diagonally shaded circles are nodes that applied U B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>( 7 )</head><label>7</label><figDesc>B receives the replies from C and D. B changes U B .state to RETIRING and pushes U B 's retirement to A, C, and D. A, C, and D acknowledge the retirement. (8) WAIT seconds later, A, B, C, and D erase U B from gU. In the end, A, B, and D store replicas. C created a replica and later deleted it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. A scenario that disconnects a graph. The edges show knowledge among nodes. (1) Replicas A and B initially know each other. (2) A issues U 1 and creates a replica C. (3) B issues U 2 and creates a replica D. (4) A crashes before U 1 is propagated to B or D. B crashes before U 2 is propagated to A or C. In the end, two components, {C} and {D}, are both live but disconnected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. A coordinator c may fail to contact a node n in two situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>period (T s , T e ). Let U be the newest update generated before T e . If (c:U.target,T e − c:U.done,T e ) ∩ S T e = φ on a coordinator node c, then S T e ⊆ c:U.done,T e . Proof. For the sake of contradiction, suppose S T e ⊆ c:U.done,T e . Using the argument that appeared in the previous proof, we can pick a node p ∈ c:U.done,T e such that ∃n ∈ S T e − c:U.done,T and either n S Te → p or p S Te</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>var members:Figure 11 .</head><label>11</label><figDesc>Figure 11. Designated coordinator selection. A membership service stores the set of presumed live nodes in members.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>from A. Because U A .ts &lt; U B .ts, B rejects U A and replies U A .from B. Because U B .ts &gt; U A .ts, A modifies gPeer, replaces gU with U B , and replies U B .</head><label></label><figDesc></figDesc><table>) C creates a replica and replies U A .ts, true, {A,C}, 
{A,B,C}} to A. D creates a replica and replies U B .ts, true, 
{B,D}, {A,B,D}} to B. 

(4) B receives U A ts, false, {A,B} {A,B,C,D}} to A. B's 
U B .target becomes {A,B,C,D} (Fig. 4 1). On receiving 
the reply from B, A changes U A .state to SUSPENDED. 

(5) A receives U B ts, true, {A,B}, 
{A,B,C,D}} to B. Later, A may receive a reply for U A from 
C, but A ignores the reply </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 . Notational conventions used in the proof</head><label>1</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="4"> Markers such as 1 refer to lines in the program listings.</note>

			<note place="foot" n="5"> All times mentioned in the proof are hypothetical global times observed by an external agent.</note>

			<note place="foot" n="6"> Here, we assume a bounded message transmission delay. Otherwise, no algorithm can ensure liveness.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Robert Grimm, Mike Swift, Brian Bershad, and the anonymous reviewers for giving us valuable comments that improved the quality of the paper immensely. This work was supported in part by DARPA grant F30602-97-2-0226 and NSF grant EIA-9870740.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Replication using group communication over a partitioned network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Amir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Hebrew University of Jerusalem</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimistic deltas for WWW latency reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<meeting><address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Principles of Tranasction Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Newcomer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reliable communication in the presence of failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="272" to="314" />
			<date type="published" when="1987-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Agreeing on processor group membership in asynchronous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flaviu</forename><surname>Cristian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Schmuck</surname></persName>
		</author>
		<idno>CSE95-428</idno>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>UC San Diego</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Epidemic algorithms for replicated database maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wes</forename><surname>Irish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th ACM Symp. on Princ. of Distr. Computing (PODC)</title>
		<meeting><address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weighted voting for replicated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Symp. on Operating Systems Principles (SOSP)</title>
		<meeting><address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="150" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Resource Discovery in Distributed Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Harchol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lewin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th ACM Symp. on Princ. of Distr. Computing (PODC)</title>
		<imprint>
			<date type="published" when="1999-04" />
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A quorum-consensus replication method for abstract data types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Herlihy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="53" />
			<date type="published" when="1986-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decentralized Replicated-Object Protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keleher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th ACM Symp. on Princ. of Distr. Computing (PODC)</title>
		<imprint>
			<date type="published" when="1999-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time, clocks, and the ordering of events in a distributed system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="558" to="565" />
			<date type="published" when="1978-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient at-most-once messages based on synchronized clocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuba</forename><surname>Shrira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wroclawski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="142" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Windows</surname></persName>
		</author>
		<title level="m">Server Resource Kit</title>
		<imprint>
			<publisher>Microsoft Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved algorithms for synchronizing computer network clocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09" />
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of the domain name system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Mockapetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dunlap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Flexible Update Propagation for Weakly Consistent Replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">J</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><forename type="middle">M</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Demers</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Symp. on Operating Systems Principles (SOSP)</title>
		<meeting><address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10" />
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dynamic replication on the Internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajaraman</surname></persName>
		</author>
		<idno>HA6177000-980305-01-TM</idno>
		<imprint>
			<date type="published" when="1998-03" />
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Roam: A Scalable Replication System for Mobile and Distributed Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratner</surname></persName>
		</author>
		<idno>UCLA-CSD-970044</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>UC Los Angeles</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Manageability, availability and performance in Porcupine: a highly scalable, cluster-based mail service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th Symp. on Operating Systems Principles (SOSP)</title>
		<meeting><address><addrLine>Kiawah Island, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Usenet performance study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verghese</surname></persName>
		</author>
		<ptr target="http://www.research.digital-.com/wrl/projects/newsbench/" />
		<imprint>
			<date type="published" when="1998-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A majority consensus approach to concurrency control for multiple copy databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Database Systems (TODS)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="209" />
			<date type="published" when="1979-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A GossipStyle Failure Detection Service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP International Conference on Distributed Systems Platforms and Open Distributed Processing (Middleware)</title>
		<meeting>IFIP International Conference on Distributed Systems Platforms and Open Distributed essing (Middleware)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed algorithms for dynamic replication of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jajodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th ACM Symp. on Princ. of Database Systems (PODS)</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="149" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
