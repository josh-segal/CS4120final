<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Information-Maximization Clustering: Tuning Parameter Selection and Analytic Solution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
							<email>yamada@sg.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Kimura</surname></persName>
							<email>kimura@sg.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Hachiya</surname></persName>
							<email>hachiya@sg.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Information-Maximization Clustering: Tuning Parameter Selection and Analytic Solution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Information-maximization clustering learns a probabilistic classifier in an unsupervised manner so that mutual information between feature vectors and cluster assignments is maximized. A notable advantage of this approach is that it only involves continuous optimization of model parameters, which is substantially easier to solve than discrete optimization of cluster assignments. However, existing methods still involve non-convex optimization problems, and therefore finding a good local optimal solution is not straightforward in practice. In this paper , we propose an alternative information-maximization clustering method based on a squared-loss variant of mutual information. This novel approach gives a clustering solution analytically in a computationally efficient way via kernel eigenvalue decomposition. Furthermore, we provide a practical model selection procedure that allows us to objectively optimize tuning parameters included in the kernel function. Through experiments , we demonstrate the usefulness of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of clustering is to classify data samples into disjoint groups in an unsupervised manner. K-means is a classic but still popular clustering algorithm. However, since k-means only produces linearly separated clusters, its usefulness is rather limited in practice.</p><p>To cope with this problem, various non-linear clustering methods have been developed. Kernel k-means <ref type="bibr" target="#b13">(Girolami, 2002</ref>) performs k-means in a feature space induced by a reproducing kernel function. Spectral clustering <ref type="bibr" target="#b20">(Shi &amp; Malik, 2000</ref>) first unfolds non-linear data manifolds by a spectral embedding method, and then performs k-means in the embedded space. Blurring mean-shift <ref type="bibr" target="#b12">(Fukunaga &amp; Hostetler, 1975</ref>) uses a non-parametric kernel density estimator for modeling the data-generating probability density and finds clusters based on the modes of the estimated density. Discriminative clustering ( <ref type="bibr" target="#b26">Xu et al., 2005;</ref><ref type="bibr" target="#b2">Bach &amp; Harchaoui, 2008</ref>) learns a discriminative classifier for separating clusters, where class labels are also treated as parameters to be optimized. Dependencemaximization clustering <ref type="bibr" target="#b22">(Song et al., 2007;</ref><ref type="bibr">Faivi- shevsky &amp; Goldberger, 2010</ref>) determines cluster assignments so that their dependence on input data is maximized.</p><p>These non-linear clustering techniques would be capable of handling highly complex real-world data. However, they suffer from lack of objective model selection strategies <ref type="bibr">1</ref> . More specifically, the above non-linear clustering methods contain tuning parameters such as the width of Gaussian functions and the number of nearest neighbors in kernel functions or similarity measures, and these tuning parameter values need to be heuristically determined in an unsupervised manner. The problem of learning similarities/kernels was addressed in earlier works, but they considered supervised setups, i.e., labeled samples are assumed to be given. <ref type="bibr" target="#b27">Zelnik-Manor &amp; Perona (2005)</ref> provided a useful unsupervised heuristic to determine the similarity in a data-dependent way. However, it still requires the number of nearest neighbors to be determined man-ually (although the magic number '7' was shown to work well in their experiments).</p><p>Another line of clustering framework called information-maximization clustering <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2006;</ref><ref type="bibr" target="#b15">Gomes et al., 2010</ref>) exhibited the state-of-the-art performance. In this informationmaximization approach, probabilistic classifiers such as a kernelized Gaussian classifier <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2006</ref>) and a kernel logistic regression classifier ( <ref type="bibr" target="#b15">Gomes et al., 2010)</ref> are learned so that mutual information (MI) between feature vectors and cluster assignments is maximized in an unsupervised manner. A notable advantage of this approach is that classifier training is formulated as continuous optimization problems, which are substantially simpler than discrete optimization of cluster assignments. Indeed, classifier training can be carried out in computationally efficient manners by a gradient method <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2006</ref>) or a quasi-Newton method <ref type="bibr" target="#b15">(Gomes et al., 2010)</ref>. Furthermore, <ref type="bibr" target="#b0">Agakov &amp; Barber (2006)</ref> provided a model selection strategy based on the common information-maximization principle. Thus, kernel parameters can be systematically optimized in an unsupervised way.</p><p>However, in the above MI-based clustering approach, the optimization problems are non-convex, and finding a good local optimal solution is not straightforward in practice. The goal of this paper is to overcome this problem by providing a novel informationmaximization clustering method. More specifically, we propose to employ a variant of MI called squaredloss MI (SMI), and develop a new clustering algorithm whose solution can be computed analytically in a computationally efficient way via eigenvalue decomposition. Furthermore, for kernel parameter optimization, we propose to use a non-parametric SMI estimator called least-squares MI (LSMI) ( <ref type="bibr" target="#b25">Suzuki et al., 2009)</ref>, which was proved to achieve the optimal convergence rate with analytic-form solutions. Through experiments on various real-world datasets such as images, natural languages, accelerometric sensors, and speech, we demonstrate the usefulness of the proposed clustering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Information-Maximization Clustering with Squared-Loss Mutual Information</head><p>In this section, we describe our novel clustering algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formulation of Information-Maximization Clustering</head><p>Suppose we are given d-dimensional i.i.d. feature vectors of size n,</p><formula xml:id="formula_0">{x i | x i ∈ R d } n i=1 ,</formula><p>which are assumed to be drawn independently from a distribution with density p * (x). The goal of clustering is to give cluster assignments,</p><formula xml:id="formula_1">{y i | y i ∈ {1, . . . , c}} n i=1 , to the feature vectors {x i } n i=1</formula><p>, where c denotes the number of classes. Throughout this paper, we assume that c is known.</p><p>In order to solve the clustering problem, we take the information-maximization approach <ref type="bibr">(Agakov &amp; Bar- ber, 2006;</ref><ref type="bibr" target="#b15">Gomes et al., 2010)</ref>. That is, we regard clustering as an unsupervised classification problem, and learn the class-posterior probability p * (y|x) so that 'information' between feature vector x and class label y is maximized.</p><p>The dependence-maximization approach ( <ref type="bibr" target="#b22">Song et al., 2007;</ref><ref type="bibr" target="#b11">Faivishevsky &amp; Goldberger, 2010</ref>) is related to, but substantially different from the above information-maximization approach. In the dependence-maximization approach, cluster assignments {y i } n i=1 are directly determined so that their dependence on feature vectors {x i } n i=1 is maximized. Thus, the dependence-maximization approach intrinsically involves combinatorial optimization with respect to {y i } n i=1 . On the other hand, the informationmaximization approach involves continuous optimization with respect to the parameter α included in a class-posterior model p(y|x; α). This continuous optimization of α is substantially easier to solve than discrete optimization of {y i } n i=1 . Another advantage of the information-maximization approach is that it naturally allows out-of-sample clustering based on the discriminative model p(y|x; α), i.e., a cluster assignment for a new feature vector can be obtained based on the learned discriminative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Squared-Loss Mutual Information</head><p>As an information measure, we adopt squared-loss mutual information (SMI). SMI between feature vector x and class label y is defined by</p><formula xml:id="formula_2">SMI := 1 2 ∫ c ∑ y=1 p * (x)p * (y) ( p * (x, y) p * (x)p * (y) − 1 ) 2 dx,<label>(1)</label></formula><p>On Information-Maximization Clustering where p * (x, y) denotes the joint density of x and y, and p * (y) is the marginal probability of y. SMI is the Pearson divergence (Pearson, 1900) from p * (x, y) to p * (x)p * (y), while the ordinary MI <ref type="bibr" target="#b7">(Cover &amp; Thomas, 2006</ref>) is the Kullback-Leibler divergence <ref type="bibr" target="#b18">(Kullback &amp; Leibler, 1951</ref>) from p * (x, y) to p * (x)p * (y):</p><formula xml:id="formula_3">MI := ∫ c ∑ y=1 p * (x, y) log p * (x, y) p * (x)p * (y) dx.<label>(2)</label></formula><p>The Pearson divergence and the Kullback-Leibler divergence both belong to the class of Ali-Silvey-Csiszár divergences (which is also known as f -divergences, see <ref type="bibr" target="#b1">(Ali &amp; Silvey, 1966;</ref><ref type="bibr" target="#b8">Csiszár, 1967)</ref>), and thus they share similar properties. For example, SMI is nonnegative and takes zero if and only if x and y are statistically independent, as the ordinary MI.</p><p>In the existing information-maximization clustering methods <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2006;</ref><ref type="bibr" target="#b15">Gomes et al., 2010)</ref>, MI is used as the information measure. On the other hand, in this paper, we adopt SMI because it allows us to develop a clustering algorithm whose solution can be computed analytically in a computationally efficient way via eigenvalue decomposition, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Clustering by SMI Maximization</head><p>Here, we give a computationally-efficient clustering algorithm based on SMI (1).</p><p>We can express SMI as</p><formula xml:id="formula_4">SMI = 1 2 ∫ c ∑ y=1 p * (x, y) p * (x, y) p * (x)p * (y) dx − 1 2 (3) = 1 2 ∫ c ∑ y=1 p * (y|x)p * (x) p * (y|x) p * (y) dx − 1 2 . (4)</formula><p>Suppose that the class-prior probability p * (y) is set to be uniform:</p><formula xml:id="formula_5">p * (y) = 1/c. Then Eq.(4) is expressed as c 2 ∫ c ∑ y=1 p * (y|x)p * (x)p * (y|x)dx − 1 2 . (5)</formula><p>Let us approximate the class-posterior probability p * (y|x) by the following kernel model:</p><formula xml:id="formula_6">p(y|x; α) := n ∑ i=1 α y,i K(x, x i ),<label>(6)</label></formula><p>where K(x, x ′ ) denotes a kernel function with a kernel parameter t. In the experiments, we will use a sparse variant of the local-scaling kernel <ref type="bibr" target="#b27">(Zelnik-Manor &amp; Perona, 2005)</ref>:</p><formula xml:id="formula_7">K(x i , x j ) =          exp ( − ∥x i − x j ∥ 2 2σ i σ j ) if x i ∈ N t (x j ) or x j ∈ N t (x i ), 0</formula><p>otherwise,</p><p>where N t (x) denotes the set of t nearest neighbors for x (t is the kernel parameter), σ i is a local scaling factor defined as</p><formula xml:id="formula_9">σ i = ∥x i − x (t)</formula><p>i ∥, and x (t) i is the t-th nearest neighbor of x i .</p><p>Further approximating the expectation with respect to p * (x) included in Eq. <ref type="formula">(5)</ref> by the empirical average of samples</p><formula xml:id="formula_10">{x i } n i=1</formula><p>, we arrive at the following SMI approximator:</p><formula xml:id="formula_11">SMI := c 2n c ∑ y=1 α ⊤ y K 2 α y − 1 2 , (8)</formula><p>where ⊤ denotes the transpose,</p><formula xml:id="formula_12">α y := (α y,1 , . . . , α y,n ) ⊤ , and K i,j := K(x i , x j ).</formula><p>For each cluster y, we maximize α ⊤ y K 2 α y under 2 ∥α y ∥ = 1. Since this is the Rayleigh quotient, the maximizer is given by the normalized principal eigenvector of K <ref type="bibr" target="#b16">(Horn &amp; Johnson, 1985)</ref>. To avoid all the solutions {α y } c y=1 to be reduced to the same principal eigenvector, we impose their mutual orthogonality: α ⊤ y α y ′ = 0 for y ̸ = y ′ . Then the solutions are given by the normalized eigenvectors ϕ 1 , . . . , ϕ c associated with the eigenvalues λ 1 ≥ · · · ≥ λ n ≥ 0 of K. Since the sign of ϕ y is arbitrary, we set the sign as</p><formula xml:id="formula_13">ϕ y = ϕ y × sign(ϕ ⊤ y 1 n ),</formula><p>where sign(·) denotes the sign of a scalar and 1 n denotes the n-dimensional vector with all ones.</p><p>On the other hand, since</p><formula xml:id="formula_14">p * (y) = ∫ p * (y|x)p * (x)dx ≈ 1 n n ∑ i=1 p(y|x i ; α) = α ⊤ y K1 n ,</formula><p>and the class-prior probability p * (y) was set to be uniform, we have the following normalization condition:</p><formula xml:id="formula_15">α ⊤ y K1 n = 1/c.</formula><p>Furthermore, probability estimates should be nonnegative, which can be achieved by rounding up negative outputs to zero. Taking these issues into account,</p><formula xml:id="formula_16">cluster assignments {y i } n i=1 for {x i } n i=1</formula><p>are determined as</p><formula xml:id="formula_17">y i = argmax y [max(0 n , ϕ y )] i max(0 n , ϕ y ) ⊤ 1 n ,</formula><p>where the max operation for vectors is applied in the element-wise manner and <ref type="bibr">[·]</ref> i denotes the i-th element of a vector. Note that we used K ϕ y = λ y ϕ y in the above derivation.</p><p>We call the above method SMI-based clustering (SMIC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Kernel Parameter Choice by SMI Maximization</head><p>Since the above clustering approach was developed in the framework of SMI maximization, it would be natural to determine the kernel parameters so that SMI is maximized. A direct approach is to use the above SMI estimator SMI also for kernel parameter choice. However, this direct approach is not favorable because SMI is an unsupervised SMI estimator (i.e., SMI is estimated only from unlabeled samples</p><formula xml:id="formula_18">{x i } n i=1</formula><p>). In the model selection stage, however, we have already obtained labeled samples {(x i , y i )} n i=1 , and thus supervised estimation of SMI is possible. For supervised SMI estimation, a non-parametric SMI estimator called least-squares mutual information (LSMI) ( <ref type="bibr" target="#b25">Suzuki et al., 2009</ref>) was shown to achieve the optimal convergence rate. For this reason, we propose to use LSMI for model selection, instead of SMI (8).</p><p>LSMI is an estimator of SMI based on paired samples</p><formula xml:id="formula_19">{(x i , y i )} n i=1 .</formula><p>The key idea of LSMI is to learn the following density-ratio function,</p><formula xml:id="formula_20">r * (x, y) := p * (x, y) p * (x)p * (y) ,<label>(9)</label></formula><p>without going through density estimation of p * (x, y), p * (x), and p * (y). More specifically, let us employ the following density-ratio model:</p><formula xml:id="formula_21">r(x, y; θ) := ∑ ℓ:y ℓ =y θ ℓ L(x, x ℓ ),<label>(10)</label></formula><p>where L(x, x ′ ) is a kernel function with kernel parameter γ. In the experiments, we will use the Gaussian kernel:</p><formula xml:id="formula_22">L(x, x ′ ) = exp ( − ∥x − x ′ ∥ 2 2γ 2 ) .<label>(11)</label></formula><p>The parameter θ in the above density-ratio model is learned so that the following squared error is minimized:</p><formula xml:id="formula_23">1 2 ∫ c ∑ y=1 ( r(x, y; θ) − r * (x, y) ) 2 p * (x)p * (y)dx.<label>(12)</label></formula><p>Among n cluster assignments {y i } n i=1 , let n y be the number of samples in cluster y. Let θ y be the parameter vector corresponding to the kernel bases {L(x, x ℓ )} ℓ:y ℓ =y , i.e., θ y is the n y -dimensional subvector of θ = (θ 1 , . . . , θ n ) ⊤ consisting of indices {ℓ | y ℓ = y}. Then an empirical and regularized version of the optimization problem (12) is given for each y as follows:</p><formula xml:id="formula_24">min θy [ 1 2 θ ⊤ y H (y) θ y − θ ⊤ y h (y) + δθ ⊤ y θ y ] ,<label>(13)</label></formula><p>where δ (≥ 0) is the regularization parameter.</p><formula xml:id="formula_25">H (y)</formula><p>is the n y × n y matrix and h <ref type="bibr">(y)</ref> is the n y -dimensional vector defined as H</p><formula xml:id="formula_26">(y) ℓ,ℓ ′ := n y n 2 n ∑ i=1 L(x i , x (y) ℓ )L(x i , x (y) ℓ ′ ), h<label>(y)</label></formula><formula xml:id="formula_27">ℓ := 1 n ∑ i:yi=y L(x i , x (y) ℓ ),</formula><p>where</p><formula xml:id="formula_28">x (y) ℓ</formula><p>is the ℓ-th sample in class y (which corresponds to θ</p><formula xml:id="formula_29">(y) ℓ ).</formula><p>A notable advantage of LSMI is that the solution θ <ref type="bibr">(y)</ref> can be computed analytically as θ</p><formula xml:id="formula_30">(y) = ( H (y) + δI) −1 h<label>(y)</label></formula><p>.</p><p>Then a density-ratio estimator is obtained analytically as follows:</p><formula xml:id="formula_31">r(x, y) = ny ∑ ℓ=1 θ (y) ℓ L(x, x (y) ℓ ).</formula><p>The accuracy of the above least-squares densityratio estimator depends on the choice of the kernel parameter γ and the regularization parameter δ. They can be systematically optimized based on crossvalidation as follows ( <ref type="bibr" target="#b25">Suzuki et al., 2009</ref>). The sam-</p><formula xml:id="formula_32">ples Z = {(x i , y i )} n i=1 are divided into M disjoint sub- sets {Z m } M m=1</formula><p>of approximately the same size. Then a density-ratio estimator r m (x, y) is obtained using Z\Z m (i.e., all samples without Z m ), and its out-ofsample error (which corresponds to Eq.(12) without irrelevant constant) for the hold-out samples Z m is computed as</p><formula xml:id="formula_33">CV m := 1 2|Z m | 2 ∑ x,y∈Zm r m (x, y) 2 − 1 |Z m | ∑ (x,y)∈Zm r m (x, y).</formula><p>This procedure is repeated for m = 1, . . . , M , and the average of the above hold-out error over all m is computed. Finally, the kernel parameter γ and the regularization parameter δ that minimize the average holdout error are chosen as the most suitable ones.</p><p>Based on the expression of SMI given by Eq. <ref type="formula">(3)</ref>, an SMI estimator called LSMI is given as follows:</p><formula xml:id="formula_34">LSMI := 1 2n n ∑ i=1 r(x i , y i ) − 1 2 ,<label>(14)</label></formula><p>where r(x, y) is a density-ratio estimator obtained above. Since r(x, y) can be computed analytically, LSMI can also be computed analytically.</p><p>We use LSMI for model selection of SMIC. More specifically, we compute LSMI as a function of the kernel parameter t of K(x, x ′ ) included in the cluster-posterior model (6), and choose the one that maximizes LSMI.</p><p>MATLAB implementation of the proposed clustering method is available from 'http://sugiyamawww.cs.titech.ac.jp/˜sugi/software/SMIC'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Existing Methods</head><p>In this section, we qualitatively compare the proposed approach with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectral Clustering</head><p>The basic idea of spectral clustering <ref type="bibr" target="#b20">(Shi &amp; Malik, 2000</ref>) is to first unfold non-linear data manifolds by a spectral embedding method, and then perform kmeans in the embedded space. More specifically, given sample-sample similarity W i,j ≥ 0, the minimizer of the following criterion with respect to {ξ i } n i=1 is obtained under some normalization constraint:</p><formula xml:id="formula_35">n ∑ i,j W i,j 1 √ D i,i ξ i − 1 √ D j,j ξ j 2 ,</formula><p>where D is the diagonal matrix with i-th diagonal element given by D i,i := ∑ n j=1 W i,j . Consequently, the embedded samples are given by the principal eigenvectors of D − 1 2 W D − 1 2 , followed by normalization. Note that spectral clustering was shown to be equivalent to a weighted variant of kernel k-means with some specific kernel ( <ref type="bibr" target="#b10">Dhillon et al., 2004</ref>).</p><p>The performance of spectral clustering depends heavily on the choice of sample-sample similarity W i,j . Zelnik-Manor &amp; <ref type="bibr" target="#b27">Perona (2005)</ref> proposed a useful unsupervised heuristic to determine the similarity in a data-dependent manner, called local scaling:</p><formula xml:id="formula_36">W i,j = exp ( − ∥xi−xj ∥ 2 2σiσj )</formula><p>, where σ i is a local scaling factor de-</p><formula xml:id="formula_37">fined as σ i = ∥x i − x (t)</formula><p>i ∥, and x (t) i is the t-th nearest neighbor of x i . t is the tuning parameter in the local scaling similarity, and t = 7 was shown to be useful <ref type="bibr" target="#b27">(Zelnik-Manor &amp; Perona, 2005;</ref><ref type="bibr" target="#b23">Sugiyama, 2007)</ref>. However, this magic number '7' does not seem to work always well in general.</p><formula xml:id="formula_38">If D − 1 2 W D − 1 2</formula><p>is regarded as a kernel matrix, spectral clustering will be similar to the proposed SMIC method described in Section 2.3. However, SMIC does not require the post k-means processing since the principal components have clear interpretation as parameter estimates of the class-posterior model (6). Furthermore, our proposed approach provides a systematic model selection strategy, which is a notable advantage over spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Blurring Mean-Shift Clustering</head><p>Blurring mean-shift <ref type="bibr" target="#b12">(Fukunaga &amp; Hostetler, 1975</ref>) is a non-parametric clustering method based on the modes of the data-generating probability density.</p><p>In the blurring mean-shift algorithm, a kernel density estimator <ref type="bibr" target="#b21">(Silverman, 1986)</ref> is used for modeling the data-generating probability density:</p><formula xml:id="formula_39">p(x) = 1 n n ∑ i=1 K ( ∥x − x i ∥ 2 /σ 2 ) ,</formula><p>where K(ξ) is a kernel function such as a Gaussian kernel K(ξ) = e −ξ/2 . Taking the derivative of p(x) with respect to x and equating the derivative at x = x i to zero, we obtain the following updating formula for sample x i (i = 1, . . . , n):</p><formula xml:id="formula_40">x i ←− ∑ n j=1 W i,j x j ∑ n j ′ =1 W i,j ′ , where W i,j := K ′ ( ∥x i − x j ∥ 2 /σ 2 )</formula><p>and K ′ (ξ) is the derivative of K(ξ). Each mode of the density is regarded as a representative of a cluster, and each data point is assigned to the cluster which it converges to. <ref type="bibr" target="#b4">Carreira-Perpiñán (2007)</ref> showed that the blurring mean-shift algorithm can be interpreted as an EM algorithm <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref>, where</p><formula xml:id="formula_41">W i,j /( ∑ n j ′ =1 W i,j ′ )</formula><p>is regarded as the posterior probability of the i-th sample belonging to the j-th cluster. Furthermore, the above update rule can be expressed in a matrix form as X ←− XP , where X = (x 1 , . . . , x n ) is a sample matrix and P := W D −1 is a stochastic matrix of the random walk in a graph with adjacency W <ref type="bibr" target="#b6">(Chung, 1997)</ref>. D is defined as</p><formula xml:id="formula_42">D i,i := ∑ n j=1 W i,j and D i,j = 0 for i ̸ = j.</formula><p>If P is independent of X, the above iterative algorithm corresponds to the power method <ref type="bibr" target="#b14">(Golub &amp; Loan, 1996)</ref> for finding the leading left eigenvector of P . Then, this algorithm is highly related to the spectral clustering which computes the principal eigenvectors of D − 1 2 W D − 1 2 (see Section 3.1). Although P depends on X in reality, <ref type="bibr" target="#b3">Carreira-Perpiñán (2006)</ref> insisted that this analysis is still valid since P and X quickly reach a quasi-stable state.</p><p>An attractive property of blurring mean-shift is that the number of clusters is automatically determined as the number of modes in the probability density estimate. However, this choice depends on the kernel parameter σ and there is no systematic way to determine σ, which is restrictive compared with the proposed method. Another critical drawback of the blurring mean-shift algorithm is that it eventually converges to a single point <ref type="bibr" target="#b5">(Cheng, 1995)</ref>, and therefore a sensible stopping criterion is necessary in practice. Although Carreira-Perpiñán (2006) gave a useful heuristic for stopping the iteration, it is not clear whether this heuristic always works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we experimentally evaluate the performance of the proposed and existing clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Illustration</head><p>First, we illustrate the behavior of the proposed method using artificial datasets described in the top row of <ref type="figure" target="#fig_0">Figure 1</ref>. The dimensionality is d = 2 and the sample size is n = 200. As a kernel function, we used the sparse local-scaling kernel <ref type="formula" target="#formula_8">(7)</ref> for SMIC, where the kernel parameter t was chosen from {1, . . . , 10} based on LSMI with the Gaussian kernel (11).</p><p>The top graphs in <ref type="figure" target="#fig_0">Figure 1</ref> depict the cluster assignments obtained by SMIC, and the bottom graphs in <ref type="figure" target="#fig_0">Figure 1</ref> depict the model selection curves obtained by LSMI. The results show that SMIC combined with LSMI works well for these toy datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>Next, we systematically compare the performance of the proposed and existing clustering methods using various real-world datasets such as images, natural languages, accelerometric sensors, and speech.</p><p>We compared the performance of the following methods, which all do not contain open tuning parame- ters and therefore experimental results are fair and objective: K-means (KM), spectral clustering with the self-tuning local-scaling similarity (SC) <ref type="bibr" target="#b27">(Zelnik-Manor &amp; Perona, 2005</ref>), mean nearest-neighbor clustering (MNN) <ref type="bibr" target="#b11">(Faivishevsky &amp; Goldberger, 2010)</ref>, MI-based clustering for kernel logistic models (MIC) <ref type="bibr" target="#b15">(Gomes et al., 2010)</ref> with model selection by maximumlikelihood MI ( <ref type="bibr" target="#b24">Suzuki et al., 2008)</ref>, and the proposed SMIC.</p><p>The clustering performance was evaluated by the adjusted Rand index (ARI) <ref type="bibr" target="#b17">(Hubert &amp; Arabie, 1985)</ref> between inferred cluster assignments and the ground truth categories. Larger ARI values mean better performance, and ARI takes its maximum value 1 when two sets of cluster assignments are identical. In addition, we also evaluated the computational efficiency of each method by the CPU computation time.</p><p>We used various real-world datasets including images, natural languages, accelerometric sensors, and speech: The USPS hand-written digit dataset ('digit'), the Olivetti Face dataset ('face'), the 20-Newsgroups dataset ('document'), the SENSEVAL-2 dataset ('word'), the ALKAN dataset ('accelerometry'), and the in-house speech dataset ('speech'). Detailed explanation of the datasets is omitted due to lack of space.</p><p>For each dataset, the experiment was repeated 100 times with random choice of samples from a pool. Samples were centralized and their variance was normalized in the dimension-wise manner, before feeding them to clustering algorithms.</p><p>The experimental results are described in <ref type="table">Table 1</ref>. For the digit dataset, MIC and SMIC outperform KM, SC, and MNN in terms of ARI. The entire computation time of SMIC including model selection is faster than KM, SC, and MIC, and is comparable to MNN which does not include a model selection procedure. For the <ref type="table">Table 1</ref>. Experimental results on real-world datasets (with equal cluster size). The average clustering accuracy (and its standard deviation in the bracket) in terms of ARI and the average CPU computation time in second over 100 runs are described. The best method in terms of the average ARI and methods judged to be comparable to the best one by the t-test at the significance level 1% are described in boldface. Computation time of MIC and SMIC corresponds to the time for computing a clustering solution after model selection has been carried out. For references, computation time for the entire procedure including model selection is described in the square bracket. face dataset, SC, MIC, and SMIC are comparable to each other and are better than KM and MNN in terms of ARI. For the document and word datasets, SMIC tends to outperform the other methods. For the accelerometry dataset, MNN and SMIC work better than the other methods. Finally, for the speech dataset, MIC and SMIC work comparably well, and are significantly better than KM, SC, and MNN.</p><p>Overall, MIC was shown to work reasonably well, implying that model selectoin by maximum-likelihood MI is practically useful. SMIC was shown to work even better than MIC, with much less computation time.</p><p>The accuracy improvement of SMIC over MIC was gained by computing the SMIC solution in a closedform without any heuristic initialization. The computational efficiency of SMIC was brought by the analytic computation of the optimal solution and the class-wise optimization of LSMI (see Section 2.4).</p><p>The performance of MNN and SC was rather unstable because of the heuristic averaging of the number of nearest neighbors and the heuristic choice of local scaling. In terms of computation time, they are rela- Class-imbalance was realized by setting the sample size of the first class m times larger than other classes. The results for m = 1 are the same as the ones reported in <ref type="table">Table 1</ref>. tively efficient for small-to medium-sized datasets, but they are expensive for the largest dataset, digit. KM was not reliable for the document and speech datasets because of the restriction that the cluster boundaries are linear. For the digit, face, and document datasets, KM was computationally very expensive since a large number of iterations were needed until convergence to a local optimum solution.</p><p>Finally, we performed similar experiments under imbalanced setup, where the the sample size of the first class was set to be m times larger than other classes.</p><p>The results are summarized in <ref type="table" target="#tab_0">Table 2</ref>, showing that the performance of all methods tends to be degraded as the degree of imbalance increases. Thus, clustering becomes more challenging if the cluster size is imbalanced. Among the compared methods, the proposed SMIC still worked better than other methods.</p><p>Overall, the proposed SMIC combined with LSMI was shown to be a useful alternative to existing clustering approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel informationmaximization clustering method, which learns classposterior probabilities in an unsupervised manner so that the squared-loss mutual information (SMI) between feature vectors and cluster assignments is maximized. The proposed algorithm called SMI-based clustering (SMIC) allows us to obtain clustering solutions analytically by solving a kernel eigenvalue problem. Thus, unlike the previous information-maximization clustering methods <ref type="bibr" target="#b0">(Agakov &amp; Barber, 2006;</ref><ref type="bibr" target="#b15">Gomes et al., 2010</ref>), SMIC does not suffer from the problem of local optima. Furthermore, we proposed to use an optimal non-parametric SMI estimator called leastsquares mutual information (LSMI) for data-driven parameter optimization. Through experiments, SMIC combined with LSMI was demonstrated to compare favorably with existing clustering methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustrative examples. Cluster assignments obtained by SMIC (top) and model selection curves obtained by LSMI (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 . Experimental results on real-world datasets under imbalanced setup. ARI values are described in the table.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> &apos;Model selection&apos; in this paper refers to the choice of tuning parameters in kernel functions or similarity measures, not the choice of the number of clusters.</note>

			<note place="foot" n="2"> Note that this unit-norm constraint is not essential since the obtained solution is renormalized later.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Ryan Gomes for providing us his program code of information-maximization clustering. MS was supported by SCAT, AOARD, and the FIRST program. MY and MK were supported by the JST PRESTO program, and HH was supported by the FIRST program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Kernelized infomax clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Silvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DIFFRAC: A discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast nonparametric clustering with Gaussian blurring mean-shift. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian mean shift is an EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">´ A</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="767" to="776" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift, mode seeking, and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spectral Graph Theory. American Mathematical Society</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information-type measures of difference of probability distributions and indirect observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studia Scientiarum Mathematicarum Hungarica</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="229" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Kernel k-means, spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM SIGKDD</publisher>
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A nonparametric information theoretic clustering algorithm. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faivishevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with application in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mercer kernel-based clustering in feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="780" to="784" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Loan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<imprint>
			<publisher>Johns Hopkins University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Matrix Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Density Estimation for Statistics and Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A dependence maximization view of clustering. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="815" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensionality reduction of multimodal labeled data by local Fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1027" to="1061" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approximating mutual information by maximum likelihood density ratio estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mutual information estimation reveals global associations between stimuli and biological processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1537" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
