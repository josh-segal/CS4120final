<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lazy Approximation for Solving Continuous Finite-Horizon MDPs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
							<email>lihong@cs.rutgers.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
							<email>mlittman@cs.rutgers.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>RL</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Laboratory Dept. of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lazy Approximation for Solving Continuous Finite-Horizon MDPs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Solving Markov decision processes (MDPs) with continuous state spaces is a challenge due to, among other problems, the well-known curse of dimensionality. Nevertheless, numerous real-world applications such as transportation planning and telescope observation scheduling exhibit a critical dependence on continuous states. Current approaches to continuous-state MDPs include discretizing their transition models. In this paper , we propose and study an alternative, discretization-free approach we call lazy approximation. Empirical study shows that lazy approximation performs much better than discretization, and we successfully applied this new technique to a more realistic planetary rover planning problem.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction and Previous Work</head><p>Probabilistic planning focuses on decision making under environment uncertainty, in contrast to conventional AI planning <ref type="bibr" target="#b7">(Fikes &amp; Nilsson 1971)</ref>. One of the principal models for probabilistic planning is Markov decision processes (MDPs) <ref type="bibr" target="#b11">(Puterman 1994)</ref>. Dynamic programming (DP) <ref type="bibr" target="#b1">(Bellman 1957</ref>) is the most common approach to solving MDPs, but it suffers from the well-known curse of dimensionality, which observes that state spaces increase exponentially with the number of dimensions used to describe them. In addition to high dimensionality, many real-world applications are continuous, raising additional challenges for solution algorithms. Existing methods, including those studied extensively in the reinforcement-learning <ref type="bibr" target="#b13">(Sutton &amp; Barto 1998</ref>) literature, are typically for discrete MDPs. Function approximation <ref type="bibr" target="#b4">(Boyan, Moore, &amp; Sutton 1995)</ref> provides a possible solution when the MDP is continuous, but it is usually difficult to analyze and can fail to converge in many cases <ref type="bibr" target="#b0">(Baird 1995;</ref><ref type="bibr" target="#b2">Bertsekas &amp; Tsitsiklis 1996)</ref>.</p><p>More recently, several techniques have been put forward to tackle MDPs with continuous state spaces. <ref type="bibr" target="#b3">Boyan &amp; Littman (2001)</ref> describe a class of MDPs called timedependent MDPs (TiMDPs), in which transitions take place along a single, irreversible continuous dimension. They provide an algorithm for computing an exact finite-horizon value function by DP when the transition probabilities are <ref type="bibr">Copyright c</ref> 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. discrete and rewards are piecewise linear (PWL). This idea was extended to higher dimensional state spaces by <ref type="bibr" target="#b6">Feng et al. (2004)</ref>, who examined MDPs with two special types of reward models: piecewise constant (PWC) and piecewise linear convex (PWLC). <ref type="bibr" target="#b10">Munos &amp; Moore (2002)</ref> have discussed various ways for variable resolution in continuous state spaces.</p><p>Although these approaches were successfully applied to realistic problems such as transportation planning, telescope observation scheduling, and rover planning, they make a common assumption that the transition model of the MDP is discrete, meaning positive probabilities are only assigned to a finite set of outcomes. However, probability density functions (pdfs) of transitions of many problems encountered in practice are continuous in nature. For example, in rover planning the duration and energy consumption of an action is best modelled as a normal distribution. For the earlier algorithms to be applied to such problems, they must discretize the continuous pdfs with sufficiently high resolution to preserve accuracy. Such discretization can result in exponential blow-ups, severely limiting their applicability to large-scale problems. Furthermore, additional errors will be introduced by discretization.</p><p>In this paper, we go one step further by developing an alternative approach to handling continuous state-space MDPs with continuous transition pdfs. The philosophy of our approach is lazy approximation (LA, for short): In contrast to the aforementioned methods that approximate/discretize an MDP at the first step and then solve the approximate model exactly, the new method postpones the approximation stage and is able to control the tradeoff between accuracy and compactness of the computed value functions. For convenience, the discretization method will be referred to as DM.</p><p>The rest of the paper is organized as follows. The next section introduces the basic idea of LA in the simplest case, and then discusses how to extend it to more general situations. Then, we provide several insights into the advantages of LA, particularly its ability to control approximation error and compactness directly, and describe a series of empirical validations. Finally, we present the application of the technique to a planetary rover-planning problem. Space limitation does not allow us to present all technical details. Interested readers are referred to a longer paper by <ref type="bibr" target="#b9">Li &amp; Littman (2005)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Lazy-Approximation Method</head><p>LA will be developed in three steps. Starting with the notation and background used in later discussions, we move on to the simplest case with one dimensional state space and PWC transition models. Finally, we extend the idea to more complex situations by removing these assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>A continuous, finite-horizon MDP is described as a four tuple: M = X, A, R, T , where X ⊂ k is a k dimensional state space, A is a finite set of actions, R(x, a) is the expected immediate reward by taking action a ∈ A in state x ∈ X, and T (x |xa) = Pr{x t+1 = x |x t = x, a t = a} is the Markovian transition function. For convenience, we adopt the convention that X = [0, 1) k . Given an MDP M , our objective is to compute its optimal value function at finite horizons: V n (x) with horizon n = 0, 1, 2, · · · , T . The Bellman equation provides a foundation for dynamic programming:</p><formula xml:id="formula_0">V n+1 (x) = max a∈A R(x, a) + X T (x |xa)V n (x )dx . (1)</formula><p>We define V 0 (x) ≡ 0, and assume R(x, a) is PWC. The transition function T can be either abstract or relative <ref type="bibr" target="#b3">(Boyan &amp; Littman 2001)</ref>. As will be shown shortly, DP with a relative model T will increase the order of polynomial functions V n as n increases, while an abstract T will retain the order. For this reason, we will focus on the more challenging relative models, i.e., T (x |xa) = Pr{x − x|a}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Basic Idea</head><p>Consider the simplest case with k = 1 and PWC T (x |xa). Since T (x |xa) is for a relative model, the integral in Eqn (1) is in fact a convolution of V n (x) and Pr{x − x|a}. If both V n (x) and Pr{x − x|a} are PWC, then after a DP step at horizon n + 1 (i.e., Eqn (1)), the value function V n+1 (x) is in general PWL, as shown in <ref type="figure" target="#fig_0">Fig 1 (b)</ref>.</p><p>Unfortunately, directly convolving V n+1 with T (x |xa) at horizon n + 2 will in turn produce a piecewise quadratic value function V n+2 . The order of V n (x) increases with n, which renders such a computation intractable in practice. To make it possible to compute the value function efficiently at higher horizons, we could do approximation before V n+2 is computed. Specifically, if we approximate V n+1 by a PWC function ¯ V n+1 <ref type="figure" target="#fig_0">(Fig 1 (c)</ref>), it is then used in Eqn (1) to obtainˆVobtainˆ obtainˆV n+2 , which is a reasonable approximation of V n+2 when ¯ V n+1 is close enough to V n+1 . This approximation procedure can be iterated and continues to higher horizons. </p><formula xml:id="formula_1">LA : T T T ⇓ ⇓ ⇓ V 0 ≡ 0 DP =⇒ ¯ V 1 DP =⇒ˆV=⇒ˆ =⇒ˆV 2 LA =⇒ ¯ V 2 DP =⇒ˆV=⇒ˆ =⇒ˆV 3 LA =⇒ ¯ V 3 DP =⇒ ··· DM : T T T ⇓ ⇓ ⇓ ˙ T ˙ T ˙ T ⇓ ⇓ ⇓ V 0 ≡ 0 DP =⇒ ¯ W 1 DP =⇒ ¯ W 2 DP =⇒ ¯ W 3 DP =⇒ ···</formula><p>Figure 2: Complete procedures of lazy approximation (LA) and discretization method (DM) for solving finite-horizon MDPs. ˆ V n are PWL; ¯ V n and ¯ W n are PWC; ˙ T is discrete.</p><p>LA needs to turn a PWL function into a PWC one. Clearly, there are a number of reasonable schemes for making this transformation. A well-known relation between ||V − ˜ V || ∞ and the quality of policy induced from˜Vfrom˜ from˜V <ref type="bibr" target="#b12">(Singh &amp; Yee 1994)</ref> suggests we minimize the L ∞ error:</p><formula xml:id="formula_2">n = ||ˆV||ˆ ||ˆV n − ¯ V n ||∞ = sup x | ˆ V n (x) − ¯ V n (x)|.<label>(2)</label></formula><p>We will be able to bound the approximation error of ¯ V n later in terms of n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dealing with Non-PWC Transitions</head><p>In practice, however, the transition functions T are not always PWC. In such cases, we could approximate T by a PWC ¯ T . Note that such an additional approximation will introduce error in the resulting value-function approximations. Our empirical studies later show that, even in this case, the LA approach still outperforms DM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dealing with Higher Dimensional Spaces</head><p>A function is PWC in a k-dimensional space if there is a rectangular partition P of the state space such that (i) each piece P ∈ P is a hyper-rectangle (i.e., the Cartesian product of intervals at each dimension), and (ii) the function value within each P ∈ P is constant. For simplicity, we will refer to the term "hyper-rectangle" by "rectangle".</p><p>The computation of Eqn (1) becomes more complicated when k &gt; 1. We observed that the convolution of two PWC functions produces a function defined on a rectangular partition P and within each P ∈ P the function is of the form:</p><formula xml:id="formula_3">ˆ V n+1 ( x) = k i=1 (aixi + bi), x ∈ P.<label>(3)</label></formula><p>Although this form seems disappointing at first glance, it has a nice property that its extreme values are always obtained at the vertices of rectangle P . Therefore, we have an exact way to compute ¯ V n as an approximation tô V n so that the L ∞ error, n , is minimized: It suffices to find the minimum and maximum values and take the midpoint, which can be done in Θ(k) time.</p><p>A natural choice of data structures for rectangular partitioning of a continuous space is kd-trees <ref type="bibr" target="#b8">(Friedman, Bentley, &amp; Finkel 1977)</ref>. But, there could be better choices in some situations by incorporating background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dealing with Discrete State Components</head><p>In real-life applications, it is common for the state space to consist of both continuous and discrete components. That is, the state space is X ×S, where X ⊂ k is continuous as before and S is discrete. For example, a rover has continuous state components (e.g., remaining time and energy) as well as discrete components such as whether particular subtasks have been achieved or the status of observation equipment.</p><p>As shown below, such a situation does not pose any essential difficulty to LA. Therefore, we will stick to the notation V n (x) to represent the value function in other parts of the paper, and only in this subsection will we use V n (x, s)to distinguish these two types of state components. Specifically, the transition model T (x , s |xsa) can be factored into: T (x , s |xsa) = T (x |xsas ) · T (s |xsa). Consequently, the integral of Eqn (1) is rewritten as:</p><formula xml:id="formula_4">S,X T (x , s |xs)V n (x, s)dx ds = S T (s |xsa) X T (x |xsas )V n (x, s)dx ds = s ∈S T (s |xsa) X T (x |xsas )V n (s, s)dx .</formula><p>This derivation reduces the problem to the one considered before, and each dynamic-programming step will still produce a function with pieces in the form of Eqn (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Analysis</head><p>A complete analysis for LA is indeed difficult and beyond the scope of this paper. Instead, we will provide some insights on why it tends to be better than DM, including its error-and compactness-control mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Control</head><p>We first observe that both the max and addition operators of a set of PWC functions produces another PWC function. This allows us to focus on the integral in Eqn (1) and simplify analysis by dropping the inessential max and R(x, a).</p><p>Define n as before and ε n to be the largest error in approximating the true V n by ¯ V n :</p><formula xml:id="formula_5">εn = || ¯ V n − V n ||∞ = sup x | ¯ V n − V n |.<label>(4)</label></formula><p>We can bound ε n+1 in terms of n+1 and ε n :</p><formula xml:id="formula_6">εn+1 = || ¯ V n+1 − V n+1 ||∞ ≤ || ¯ V n+1 − ˆ V n+1 ||∞ + ||ˆV||ˆ ||ˆV n+1 − V n+1 ||∞ = n+1 + sup x X T (x |xa) ¯ V n (x ) − V n (x ) dx ≤ n+1 + εn sup x X T (x |xa)dx = n+1 + εn.<label>(5)</label></formula><p>Therefore, if 1 , 2 , · · · , n are kept small, we will end up with an accurate value function approximation ¯ V n at horizon n. There are examples showing that the bound in Eqn (5) is tight.</p><p>Similarly, we can bound the approximation error of DM. Let ¯ W n be the function computed by the DM at horizon n, and the approximation error be</p><formula xml:id="formula_7">ξn = || ¯ W n − V n ||∞ = sup x | ¯ W n − V n |.</formula><p>Denote the discretized transition of T by D = {d i }-a discrete pdf. In other words, D partitions the state space into i ∆ i , each ∆ i corresponding to a d i ∈ D. Now, define ρ n , which captures the smoothness of V n :</p><formula xml:id="formula_8">ρn = i di sup D i V n − inf D i V n .<label>(6)</label></formula><p>Hence, the smoother V n is, the smaller ρ n is. Using these definitions, we have the following bound on ξ n+1 in terms of ξ n and ρ n :</p><formula xml:id="formula_9">ξn+1 ≤ ρn + ξn.<label>(7)</label></formula><p>This bound is also tight. It is desirable to have a smooth V n or a high resolution so that ρ n is then small and ξ n+1 will be close to ξ n , meaning the approximation error is small. In practice, however, it is usually unknown a priori how smooth V n is, and thus difficult to decide the appropriate resolution r for discretization beforehand. Besides, the smoothness of V n may not be uniform-only a certain state subspace may need fine discretization.</p><p>In contrast, LA controls ε n+1 explicitly by keeping n small. We emphasize here that n is provided by the user and is controllable. We believe this direct approach is easier for people and provides more flexibility in making tradeoff between accuracy and running time. More insights will be provided after discussing the compactness-control mechanism in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compactness Control</head><p>In our experiment as well as analysis, we find that the memory (space) and running time required to computê V n+1 (especially the integral) largely depends on the sizes of ¯ V n and T , that is, how many rectangular pieces they have. In order to prevent their sizes from growing without control, merging neighboring regions with similar values is usually necessary. During merging, a small positive δ is provided and two neighboring regions are merged into one if their value difference is less than δ. Merging is also necessary for DM. Otherwise, if the resolution of discretization at each dimension is r, then each DP step can, in the worst case, multiply the function size by a factor of r k .</p><p>Unlike DM, LA tries to keep the function compact by using the true transition, which is usually much more compact than a discretized representation. Consequently, the complexity of the resulting value-function approximations largely depends on how complex the target value function actually is. In this way, we could avoid unnecessarily high resolution that is difficult to eliminate in DM. If, unfortunately, the target function is too complicated to represent compactly, LA will also suffer. But, this is not because of the algorithm itself, but of the nature of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Brief Summary</head><p>From the results above, we conclude that LA is more flexible than DM: On the one hand, it directly makes n small to control the growth of ε n ; on the other hand, it is able to make a tradeoff between ε n and the compactness of ¯ V n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Study</head><p>We conducted two sets of experiments, one on synthetic problems, and the other on a larger and more realistic problem of planetary rover planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Synthetic Problems</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">In almost all cases, LA requires much less running time</head><p>and value-function representation sizes to achieve the same error level. For example, the speedup is usually about 2 orders of magnitude (notice the logarithmic scale on the y-axis).</p><p>2. It is interesting to notice that LA requires more time to achieve very small RMSE in <ref type="figure">Fig 4 (b)</ref>. This is not surprising as the approximation scheme adopted in our experiment is to minimize the L ∞ error, rather that RMSE. In comparison, LA is very good at keeping L ∞ error small (c.f. <ref type="figure">, Fig 4 (a,c)</ref>).</p><p>3. Comparing the growth of the fixed-resolution and fixedepsilon curves of LA in <ref type="figure">Fig 4 suggests</ref> it may be better to fix n and then increase the accuracy in approximating the transition model.</p><p>We next study how errors evolve as horizon increases. In order to help visualize how well LA controls the error over horizons, we show the true value functions, as well as the approximations computed by DM and LA, at horizons 1, 5, and 10 of a randomly selected but typical run <ref type="figure">(Fig 5)</ref>. We found that although both algorithms produce value functions of comparable approximation errors at early horizons, the error of DM grows faster than LA, and ends up taking much longer to find a much larger representation of a much less accurate result at horizon 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to Rover Planning</head><p>Planning under uncertainties with resource limitation ( <ref type="bibr" target="#b5">Bresina et al. 2002)</ref> such as the Mars rover planning problem is the motivation behind this paper. A number of non-trivial sources of uncertainty exist in rover operation, including the duration and power consumption of actions. Furthermore, the typical size of problems is prohibitively large and, therefore, exact, optimal solutions are not possible in most cases. Recent work modelled the problem as a finite-horizon MDP ( <ref type="bibr" target="#b5">Bresina et al. 2002;</ref><ref type="bibr" target="#b6">Feng et al. 2004</ref>), and given the MDP model, the objective is to approximate the optimal value function for each horizon length to generate near-optimal plans.</p><p>The rover planning problem is as follows. Given a set of locations, the associated target object at each location, and the paths between locations, the rover is to travel around and take pictures of targets before running out of time or energy. The utility of taking a picture is dependent on the target as well as when the picture is taken. Before taking a picture, the rover has to perform a sequence of actions such as identifying its location, tracking the object, navigating, and initializing its instruments, etc., and some of the actions are also dependent on other actions and the rover's state.</p><p>Even for a minimal-size problem involving only two locations and targets, the state space is very large-it has a discrete component of dimension 12 (4096 discrete states) and a continuous component of dimension 2, and there are 14 actions (some are stochastic with uncertain durations and power consumptions according to Gaussian distributions). It is impossible to discretize the MDP model and obtain accurate value functions using classical methods like value iteration in a reasonable amount of time, even if human knowledge is employed to prune unreachable states.</p><p>We applied LA to a version of this problem specified by NASA researchers. PWC functions were used to approximate normal distributions appearing in action durations and energy consumptions. We found that LA produced reasonable and consistent results even if the PWC transition approximations are not complex. In particular, <ref type="figure">Fig 6</ref> shows the computed value function at horizon 7 for the initial rover configuration (i.e., its initial discrete state), using a PWC transition representation of size 64.</p><p>The graph, with plateaus and humps, are typical for value functions if there are finitely many goal states with positive utility and resource constraints <ref type="bibr" target="#b6">(Feng et al. 2004</ref>). The flat, zero-valued regions correspond to situations where either time or energy is not sufficient to perform a task at any target (and hence the expected utility is 0). When the rover has more resources, it has positive utility expectations, and the more resources it has, the higher the expectation is. At the corner around the point (1, 1) is a hump because the amount of resources allows the rover to carry out a second task, increasing its utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Future Work There are several interesting directions for future work. First, we are investigating more time-and space-efficient ways for doing the intermediate stages of lazy approximation, including improving the approximation and merging mechanisms. Second, we are seeking other data structures that best fit the purpose of efficiently representing functions for computing Eqn (1). Note that kd-trees may not be the best choice. A promising approach is using the spatial relation among pieces of the rectangular participation. Third, it may be helpful to look into and compare rectangular partitions with others such as the Kuhn triangulations <ref type="bibr" target="#b10">(Munos &amp; Moore 2002)</ref>, which also provides possible thoughts for better merging mechanisms. Conclusions In this paper, we proposed and studied an approach to solving continuous, finite-horizon MDPs. In contrast to traditional discretization methods that approximate the MDP model beforehand, the new approach retains a continuous model and defers approximation until necessary. Some insights were given to explain the advantage of lazy approximation, and it was shown that, on a set of synthetic problems, lazy approximation performs consistently much better than the discretization method. It was also successfully applied to a large, real-life planning problem for planetary rovers. We look forward to its application to other non-trivial probabilistic-planning problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of lazy approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 2 provides a comparative illustration of LA and DM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>this section, we report results on randomly generated problems. Two types of MDPs are considered: (I) There is a positive, PWC reward function at a terminating horizon 10 steps away, other rewards are 0, and the transition functions are PWC; (II) Same as (I) except that the transition function is a normal distribution. We first study how values of n in LA and resolution r in DM affect the running time and size of value-function ap- proximations. Figs 3 and 4 plot the running time and size as functions of L ∞ -error and root mean squared error (RMSE) of a typical run. The data in Fig 3 were obtained by varying r and n in DM and LA, respectively. In Fig 4, since the true transition was approximated by a PWC function, we also in- vestigated this affect by either (i) fixing the size of the PWC transition representation to be 20 and varying n , or (ii) fix- ing n = 0.01 and varying the size of the PWC transition representation. Therefore, there are two series of data corre- sponding to LA in Fig 4. Several observations follow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Results of a typical run on a problem instance of type (I). In the DM curves, we varied the resolution r from 10 to 500; in the LA curves, we varied n from 0.1 to 0.0001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Value functions at horizons 1, 5, 10 (from left to right). DM (with resolution of 20) took 0.1542s and produced a value function at horizon 10 of size 401. LA (with = 0.01) took only 0.008s and ended up with a function of size 33 at horizon 10.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Nicolas Meuleau, Zhengzhu Feng, Emmanuel Benazera, Victor Lee, and the RL 3 members for help in conducting experiments. The Rutgers Center for Advanced Information Processing (CAIP) provided their high-end machines for our experiments. This research is supported by NASA (solicitation number: NRA2-38169).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning (ICML-95)</title>
		<meeting>the Twelfth International Conference on Machine Learning (ICML-95)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neuro-Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Athena Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact solutions to timedependent MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<idno>NIPS-00</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1026" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<title level="m">Proceedings of the ICML-95 Workshop on Value Function Approximation</title>
		<meeting>the ICML-95 Workshop on Value Function Approximation</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Planning under continuous time and resource uncertainty: A challenge for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bresina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramkrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Washington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI-02)</title>
		<meeting>the Eighteenth Conference on Uncertainty in Artificial Intelligence (UAI-02)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic programming for structured continuous Markov decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Washington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI-04)</title>
		<meeting>the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">STRIPS: A new approach to the application of theorem proving to problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for finding best matches in logarithmic expected time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lazy approximation: A new approach for solving continuous finite-horizon MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<idno>577</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Rutgers University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variable resolution discretization in optimal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moore</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="291" to="323" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>WileyInterscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An upper bound on the loss from approximate optimal-value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
