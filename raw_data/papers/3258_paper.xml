<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Sparse Matrix-Vector Multiply in Five Minutes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hormozd</forename><surname>Gahvari</surname></persName>
							<email>hormozd@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoemmen</surname></persName>
							<email>mhoemmen@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
							<email>demmel@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Yelick</surname></persName>
							<email>yelick@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking Sparse Matrix-Vector Multiply in Five Minutes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a benchmark for evaluating the performance of Sparse matrix-dense vector multiply (abbreviated as SpMV) on scalar uniprocessor machines. Though SpMV is an important kernel in scientific computation, there are currently no adequate benchmarks for measuring its performance across many platforms. Our work serves as a reliable predictor of expected SpMV performance across many platforms, and takes no more than five minutes to obtain its results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sparse matrix-dense vector multiply (SpMV) is a common operation in scientific codes. It is especially prevalent in iterative methods to solve linear systems. Given this, it would be very convenient for consumers to have a convenient way of knowing which machine to buy for their calculations, and for vendors to know how well their machines perform.</p><p>There are currently no convenient ways for vendors to know how well their machines perform SpMV. The current standard method for ranking computers' ability to perform scientific compuatations, the Top 500 List <ref type="bibr" target="#b8">[9]</ref>, uses only the LINPACK benchmark <ref type="bibr" target="#b7">[8]</ref>. LINPACK measures the speed of solution of a system of linear equations, which is not representative of all the operations that are performed in scientific computing. There is a benchmark suite under development called the High Performance Computing Challenge Suite (HPCC) that seeks to remedy this <ref type="bibr" target="#b4">[5]</ref>. The HPCC suite contains benchmarks that seek to measure computers' performance in performing several different operations, including LINPACK.</p><p>The benchmark we will present here is proposed for inclusion into this suite, as none of the other benchmarks in it are suited for approximating the performance of SpMV. We will see why in the next section. One requirement for inclusion in the HPCC suite is a short run-time, which explains our goal of running in five minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPROXIMATING SPMV PERFORMANCE</head><p>A critical difference between SpMV and other operations benchmarked in the HPCC suite is that the performance of SpMV depends strongly on the data, i.e. the size and nonzero pattern of the sparse matrix. Since practical sparse matrices do vary widely in these properties, this means that existing benchmarks will not be predictive of SpMV performance, and that we will need to time SpMV itself on a representative set of test matrices.</p><p>Furthermore, as machines grow in capacity over time, no fixed set of test matrices would be adequate to test performance. For example, a matrix so large that it cannot be stored in cache on today's platforms (an important size to test) may well fit in cache in a few years. This would make SpMV appear to run at a much higher fraction of peak performance, and be unrepresentative of practical problem sizes, which will also grow over time. This, combined with the sheer size of a collection of fixed test matrices, means we will have to generate appropriate test matrices on the fly that appropriately approximate practical sparse matrices.</p><p>Finally, SpMV performance can vary significantly depending on small changes in the data structure used to store the matrix, and in the corresponding algorithm that accesses it to implement SpMV. Just as the LINPACK benchmark depends on tuned BLAS for a true assessment of a machine's performance, we also need to engage in a reasonable level of tuning effort. To make this portable, fast and fair (in the sense that a similar level of machine-dependent and matrixdependent tuning is done whenever the benchmark is used), we will rely on the OSKI automatic tuning system <ref type="bibr" target="#b10">[11]</ref>. Now we explain the above points in more detail.</p><p>The main reason different matrix sizes and nonzero patterns impact the performance of SpMV is that they lead to different memory access patterns. Since only the nonzeros (and their locations in the matrix) are stored, memory accesses cannot all be unit-stride. Typically the matrix is still streamed through in unit stride, but the vector it multiplies (the source vector) is accessed indirectly. Our experiments lead us to propose using four matrix properties to characterize practical matrices, and to generate corresponding test matrices on the fly: size (total memory for the matrix and source vector), density (average number of nonzeros per row of the matrix), block size (can the matrix be stored as a collection of dense r-by-c blocks for some r &gt; 1 and/or c &gt; 1?), and "bandedness" (the distribution of the distances of the nonzeros from the main diagonal).</p><p>We propose 3 categories of SpMV problem sizes:</p><p>• Small: everything fits in cache.</p><p>• Medium: the source vector fits in cache but the matrix does not.   and different performance. Performance typically peaks for a given density for small or medium sizes, and gradually falls off as the problem dimension increases. <ref type="figure" target="#fig_5">Figures 3(a)</ref>, 4(a), and 5(a) show this behavior in data gathered from running SpMV on a set of 275 matrices taken from the online collection <ref type="bibr" target="#b2">[3]</ref>. Each circle is a test matrix, with its x-coordinate equal to its dimension, y-coordinate equal to its density, and color coded by speed in MFLOP/s. Dark lines separate the small, medium and large matrices. The selected matrices are listed in <ref type="bibr" target="#b5">[6]</ref>. No performance tuning has been done on these matrices. The compressed sparse-row (CSR) format, is used for storing the matrices because it was found in <ref type="bibr" target="#b9">[10]</ref> to be the best general-purpose unoptimized sparse matrix storage format across multiple platforms. An example of this format is seen in <ref type="figure" target="#fig_1">Figure 1</ref>. The nonzero entries are stored in the values array, the index of each entry that starts a new row is stored in the row start array, and the column each entry belongs to is stored in the col idx array. Data was obtained on the platforms in <ref type="table" target="#tab_0">Table I</ref>. None of the other benchmarks in the HPCC suite use indirect accesses that would let us predict the performance shown here. Indeed other research <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> has shown them to be unreliable as benchmarks for SpMV. <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b11">[12]</ref> also looked at two other approaches for benchmarking SpMV. One was to develop performance bounds. The bounds served as very reliable upper and lower bounds across multiple platforms, but could not offer any hints as to what the expected performance would be on them. Another approach was to use "machine balance", but it was also inadequate on some platforms.</p><p>This motivates us to benchmark SpMV by performing the actual operation. There are three existing benchmarks that do this <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, but do not meet our goals. <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b3">[4]</ref> do not measure the performance of any performance  optimizations. <ref type="bibr" target="#b0">[1]</ref> allows for performance optimizations, but they must be user-supplied. It also does not measure solely SpMV performance; rather, it measures the performance of the conjugate gradient operation, which is very rich in SpMV but also contains dense vector updates and outer products. Also <ref type="bibr" target="#b0">[1]</ref> uses a single kind of random matrix that is not representative of many practical matrices. Now we consider block-sizes and performance tuning. Some sparse matrices, particularly those arising from finite element applications, have a natural block structure that can be exploited to improve performance. Others do not have such a structure and are only suited to being run without such optimizations.</p><p>We use the register blocking optimization <ref type="bibr" target="#b9">[10]</ref> to measure optimized SpMV performance, as it was found in <ref type="bibr" target="#b9">[10]</ref> to be the the most widely applicable of all the possible optimizations, and it is implemented in an automatic tuning system <ref type="bibr" target="#b10">[11]</ref>. This requires us to use the blocked compressed sparse-row matrix storage format, which is illustrated in <ref type="figure" target="#fig_3">Figure 2</ref> using 2 × 2 register blocks that are color-coded for clarity. The difference between BCSR and CSR is that in BCSR, the blocks are stored contiguously, the row start array says which element starts the next block row, and the col idx array says which block column each block belongs to. In general, it is possible to have blocks of an arbitrary blocksize r × c. Different blocksizes work best with different matrices on different machines. The problem of which one is best is addressed in detail in <ref type="bibr" target="#b9">[10]</ref>. show that register blocking SpMV on the matrices from the test suite yields different speedups on each of the platforms tested, highlighting the importance of measuring tuned in additioned to untuned SpMV. Register blocked performance data was obtained using the OSKI automatic tuning system <ref type="bibr" target="#b10">[11]</ref>.</p><p>Finally, we consider bandedness. By examining many practical sparse matrices, we find that many of them exhibit a somewhat banded structure in the following sense: a large fraction of the nonzeros in any row are located relatively close to the diagonal, as measured as the percentage of entries that lie in bands 10(i − 1) to 10i percent away from the diagonal, where 1 ≤ i ≤ 10). <ref type="figure">Figure II</ref> shows how we divide the matrices into bands.</p><p>The statistics for each matrix in our test suite are given in <ref type="bibr" target="#b5">[6]</ref>. Ignoring this bandedness in generating random test    matrices tends to underpredict performance, so we generate our matrices to match the statistics in <ref type="bibr" target="#b5">[6]</ref>.</p><formula xml:id="formula_0">5 @ @ 4 @ @ @ @ 3 @ @ @ @ @ @ 2 @ @ @ @ @ @ @ @ 1 @ @ @ @ @ @ @ @ 2 @ @ @ @ @ @ 3 @ @ @ @ 4 @ @ 5</formula><p>Randomly generated matrices matching all these criteria do a reasonably good job approximating the performance of the real-life matrices they were created to model, as <ref type="bibr">Figures 8-10</ref> show. In these plots, each real matrix is represented by an R, and is connected by a line to its synthetic counterpart, which is represented by an S. The tuned plots are color-coded by largest blocksize dimension.</p><p>One thing to note is that while a number of the matrices in our test suite are symmetric, the matrix generator we use does not generate symmetric matrices. To generate data from which we could accurately gauge how well the synthetic matrices performed, we ran the symmetric matrices from our test suite with symmetry disabled. We will return to the issue of symmetry in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE BENCHMARK</head><p>We will now use what we have developed in the past two sections to construct a benchmark for SpMV. Our first task is to define the set of matrices from which we will take data. The test suite we used in the previous section has dimensions ranging from 512 to nearly a million, with densities ranging from 1 to almost 400 nonzero entries per row. We will consider square matrices of similar dimensions, but only ones that are powers of two ranging from 2 9 to 2 20 . The number of nonzero entries per row will range within <ref type="bibr">[24,</ref><ref type="bibr">34]</ref> = 29 ± 5, since 29 is the average number of nonzero entries per row in the suite. The distribution of nonzero entries will made to match statistics taken from all of the matrices in the test suite, which are shown in <ref type="table" target="#tab_0">Table II</ref>. The register blocksizes for optimized SpMV will come from the set {1, 2, 3, 4, 6, 8} × {1, 2, 3, 4, 6, 8}. These were the blocksizes most commonly found in work in <ref type="bibr" target="#b9">[10]</ref> that used a set of test matrices for which tuning showed benefits.</p><p>We take SpMV data from these matrices and report as output four MFLOP rates: unblocked maximum, unblocked median, blocked maximum, and blocked median. The unblocked numbers are taken only from data gathered for matrices with 1 × 1 blocks, and represent the case of the real-life matrices for which tuning was attempted but found to be of no benefit. The tuned numbers are taken from the rest of the data, and represent the case of the real-life matrices for which there was a benefit to tuning. Through these numbers, we seek to capture best-case and expected-case performance. <ref type="figure" target="#fig_9">Figure 7</ref>, which shows the performance of benchmark matrices within our search space for two particular blocksizes on the Itanium 2, illustrates why we select these numbers.</p><p>With the max numbers, we are looking to capture peak performance, and with the median numbers, we are looking to capture performance when it levels off. When forced to report one number, as required by the HPCC suite's rules, we will report the blocked median. <ref type="table" target="#tab_0">Table III</ref> shows the output for the three platforms tested. <ref type="figure" target="#fig_1">Figures 11-13</ref> show that the numbers, especially the median numbers, give a good indicator of expected SpMV performance. Table IV reassures us that we tested small, medium, and large SpMV problems, and thus got a good set of data on which to base our benchmark numbers.</p><p>However, each of these runs took over 2 hours. If we want to cut this down, we will need to prune the test space run by the benchmark in such a way that we can capture the same data while running far fewer SpMV trials. Looking back at   1) For large problem dimensions, the performance hardly changes with small fluctuations in nnz/row. This is not the case for small problem dimensions.</p><p>2) The performance levels off for large problem dimensions, so the statistics will hardly change if we drop the very largest problem dimensions from the test space.</p><p>To help us handle the first observation, we introduce what we call a threshold dimension. This is a problem dimension below which we consider the task of creating a matrix and performing SpMV with it to be "free." Each register block size for which we generate matrices will have its own threshold dimension. All values of nnz/row are to be tested for problem dimensions below the threshold dimension. Above it, we will be free to cut out values as we see fit.</p><p>The actual decision of which parts of the search space to cut out is made by a runtime estimator that first estimates the runtime of the benchmark on the entire search space and then cuts out certain parts of it until a user-specified time constraint is satisfied, in our case five minutes. The estimation is carried out by, for each register blocksize tested, running an SpMV trial (both matrix generation and performing the actual multiplication) for a matrix whose dimension is the threshold dimension as defined above and then doubling this value to obtain runtime estimates for running an SpMV trial for each successive problem dimension in the test space. Here, nnz/row is kept at the midpoint of the selected range. The computed estimates are then added up, yielding an estimate for the runtime of the entire benchmark.</p><p>The estimator then decides which elements of the search space to cut using the following iteration:</p><p>1) Reduce the number of values of nnz/row to test by 1 and adjust the runtime estimate accordingly. 2) If the time limit is still exceeded, cut off the largest dimension to be tested and go back to testing the full nnz/row range, adjusting the estimate accordingly. 3) Repeat the previous two steps until the time limit is satisfied.</p><p>In this way, the largest problem dimension tested is kept as large as possible to ensure that the benchmark tests small, medium, and large problems as defined in the previous section, while still dramatically cutting down on the runtime, as we will see shortly. First, though, we note that cutting out values of nnz/row to test cuts out data points that we need to compute our statistics, since the full nnz/row range is going to be tested for problem dimensions below the threshold dimension. To correct this problem, we refill the data points by either duplication if we only test one nnz/row value or interpolation if we test more than one. In the latter case, the endpoints of the nnz/row range are required to be included among the nnz/row values tested.</p><p>In the case of each of the three tested platforms, the estimator reduced the maximum matrix dimension to 2 18 and the number of nnz/row values to test to one for large enough problems. <ref type="table" target="#tab_3">Table V</ref> shows that this resulted in dramatic savings in runtime while maintaining the accuracy of the data, <ref type="table" target="#tab_0">Table VI</ref> shows the runtime savings, and Table VII shows the proportion of problem sizes tested by the shortened benchmark run. The difference in max numbers between the full and abbreviated runs, which would ideally be the same, fall within the bounds of measurement noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS AND DIRECTIONS FOR FUTURE WORK</head><p>We have presented a benchmark that quickly and effectively evaluates the fitness of different architectures for performing SpMV. The benchmark runs in at most five minutes and gives a good indicator of expected SpMV performnce on multiple platforms. There are however substantial areas for future work. These range from improving the benchmark itself to extending it to new platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Improving the Benchmark</head><p>The most obvious question about the benchmark right now is the why the number it outputs for the tuned maximum is so high. For all the platforms tested, it is too high when compared with the performance of SpMV on real-life matrices. Future work to address this problem is needed. Another area where more work can be done is in the generation of synthetic matrices. We have identified four parameters (size, density, block-size, bandedness) that characterize the performance of test matrices, bu there are still gaps in the ability of synthetic matrices to model real-life ones, and closing these gaps will help lead to a more accurate benchmark.</p><p>Another case the benchmark does not currently handle explicitly is that of symmetric matrices. Many matrices from real-life applications are symmetric. <ref type="figure" target="#fig_1">Figures 13-15</ref> show that our benchmark retains some predictive power when symmetry is taken into account, but there are many symmetric matrices for which it could do better. Thus, finding a way to integrate symmetric matrices into the ones used by our benchmark to run its trials would very much improve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extending the Benchmark to New Platforms</head><p>Our benchmark currently only works on scalar uniprocessor machines. These are not the only machines on which SpMV is performed. Vector and parallel machines are also common platforms on which SpMV is run, making a benchmark designed for those kinds of architectures very useful. In the case of vector machines, there are sparse matrix data structures specifically created for them that should be used, such as segmented scan <ref type="bibr" target="#b1">[2]</ref>. So the benchmark can be extended to vector machines by changing the data structure used for storing sparse matrices to one that is optimized for vector machines.</p><p>In the case of parallel machines, many more issues come into play. The matrix, instead of belonging to just one processor, is instead distributed over many processors. This will require a whole new benchmark, and one that will be sought after by many, as the benchmarks in the HPCC suite expects all of its benchmarks to have parallel versions <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported in part by the National Science Foundation under CNS-0325873, ACI-0090127, and ACI-9619020, and by the California State MICRO program, and by gifts from Intel Corporation, Hewlett-Packard, and Microsoft. Experiments in this paper were performed on the computer facilities of the Berkeley Benchmarking and Optimization (BeBOP) group at UC Berkeley and on the Mobius Cluster at The Ohio State University. The information presented here does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.          </p><formula xml:id="formula_1">R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R SR S R S R SR SR S R S R S R S R S R SR SR S R S R S R S R S R S R S R S R S R SR S R S R S R S R S R S R S R S R S R S R S R S R SR S R S R S R S R S small</formula><formula xml:id="formula_2">R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S</formula><formula xml:id="formula_3">R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R SR S R S R SR S R SR S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R</formula><formula xml:id="formula_4">R S R S R SR S R S R S R SR S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R SR S R S R SR SR SR SR S R S R SR S R S R S R S R S R S R S R S R S R S R S R S R S R SR S R S R S R S R S R S R S R S R S R S R S R S R S R S R S R S</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Large: neither the source vector nor the matrix fit in cache. These different sizes lead to different memory traffic patterns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Compressed Sparse-Row Matrix Storage Format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Blocked Compressed Sparse-Row (BCSR) matrix storage format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Small, Medium, Large behavior on the Pentium 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Small, Medium, Large behavior on the Itanium 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Small, Medium, Large behavior on the Opteron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Matrix divided up into bands. For simplicity of illustration, this matrix is only divided up into 5 bands instead of 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance of benchmark matrices on the Itanium 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 ,</head><label>7</label><figDesc>Figure 7, here are two key observations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Matrices</head><label></label><figDesc>Sorted by Problem Size MFLOP/s Real vs. Synthetic Matrices, Tunable Blocked, P4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 8. Performance of synthetic vs. real matrices on the Pentium 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Matrices</head><label></label><figDesc>Fig. 9. Performance of synthetic vs. real matrices on the Itanium 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Matrices</head><label></label><figDesc>Fig. 10. Performance of synthetic vs. real matrices on the Opteron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Matrices</head><label></label><figDesc>Fig. 12. Performance of benchmark vs. real matrices on the Itanium 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Matrices</head><label></label><figDesc>Fig. 13. Performance of benchmark vs. real matrices on the Opteron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>MatricesMatricesMatrices</head><label></label><figDesc>Fig. 14. Performance of benchmark vs. real matrices on the Pentium 4 with symmetry taken into account. Triangles represent symmetric matrices and circles represent nonsymmetric ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>TABLE I</head><label>I</label><figDesc></figDesc><table>PLATFORMS TESTED 

Pentium 4 Itanium 2 
Opteron 

Speed 
2.4 GHz 
1 GHz 
1.4 GHz 

Cache 
512 KB 
3 MB 
1 MB 

Compiler 
gcc 3.4.4 
icc 9.0 
gcc 3.2.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TABLE II DISTRIBUTION OF NONZERO ENTRIES IN MATRIX TEST SUITE</head><label>II</label><figDesc></figDesc><table>Distance From Diagonal Entries In This Range 

0-10% 
65.9% 

10-20% 
11.4% 

20-30% 
5.84% 

30-40% 
6.84% 

40-50% 
2.85% 

50-60% 
1.86% 

60-70% 
1.44% 

70-80% 
2.71% 

80-90% 
0.774% 

90-100% 
0.387% 

TABLE III 

BENCHMARK RESULTS, FULL RUN 

Unblocked 
Blocked 

Max Median 
Max 
Median 

Pentium 4 
699 
307 
1961 
530 
Itanium 2 
443 
343 
2177 
753 
Opteron 
396 
170 
1178 
273 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>TABLE IV PROPORTION OF PROBLEM SIZES TESTED BY THE BENCHMARK, FULL</head><label>IV</label><figDesc></figDesc><table>RUN 

Pentium 4 Itanium 2 Opteron 

Small 
17% 
33% 
23% 
Medium 
42% 
50% 
44% 
Large 
42% 
17% 
33% 

(a) Unblocked 

(b) 3x3 Blocks 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>TABLE V BENCHMARK RESULTS, 5 MINUTE RUN</head><label>V</label><figDesc></figDesc><table>Unblocked 
Blocked 

Max Median 
Max 
Median 

Pentium 4 
692 
362 
1937 
555 
Itanium 2 
442 
343 
2181 
803 
Opteron 
394 
188 
1178 
286 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>TABLE VI BENCHMARK RUNTIME COMPARISON</head><label>VI</label><figDesc></figDesc><table>Runtime (original) Runtime (condensed) 

Pentium 4 
150 minutes 
3 minutes 
Itanium 2 
128 minutes 
3 minutes 
Opteron 
149 minutes 
3 minutes 

TABLE VII 

PROPORTION OF PROBLEM SIZES TESTED BY THE BENCHMARK, 5 
MINUTE RUN 

Pentium 4 Itanium 2 Opteron 

Small 
20% 
40% 
27% 
Medium 
50% 
60% 
53% 
Large 
30% 
0% 
20% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fatoohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fineberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weeratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NAS Parallel Benchmarks</title>
		<imprint>
			<date type="published" when="1994-03" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segmented Operations for Sparse Matrix Computation on Vector Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Belloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zagha</surname></persName>
		</author>
		<idno>CMU-CS-93-173</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">University of Florida Sparse Matrix Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sparsebench: a sparse iterative benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eijkhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vorst</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/benchmark/sparsebench" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to the HPC Challenge Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luszczek</surname></persName>
		</author>
		<idno>UT- CS-05-544</idno>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Knoxville</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Benchmarking Sparse Matrix-Vector Multiply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gahvari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-12" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">SciMark 2.0 Java Benchmark for Scientific Computing</title>
		<ptr target="http://math.nist.gov/scimark2" />
		<imprint/>
		<respStmt>
			<orgName>National Institute of Science and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">HPL -A Portable Implementation of the High-Performance Linpack Benchmark for Distributed-Memory Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petitet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Whaley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cleary</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/benchmark/hpl" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supercomputer Sites</surname></persName>
		</author>
		<ptr target="http://www.top500.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Automatic performance tuning of sparse matrix kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OSKI: A library of automatically tuned sparse matrix kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SciDAC 2005, ser. Journal of Physics: Conference Series</title>
		<meeting>SciDAC 2005, ser. Journal of Physics: Conference Series<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Institute of Physics Publishing</publisher>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance Optimizations and Bounds for Sparse MatrixVector Multiply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing</title>
		<meeting>Supercomputing<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
