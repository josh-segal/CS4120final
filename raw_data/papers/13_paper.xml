<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
							<email>james.henderson@cui.unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept Computer Science</orgName>
								<orgName type="department" key="dep2">Dept Linguistics</orgName>
								<orgName type="department" key="dep3">Depts Linguistics and Computer Science</orgName>
								<orgName type="department" key="dep4">Dept Computer Science</orgName>
								<orgName type="institution" key="instit1">Univ Geneva</orgName>
								<orgName type="institution" key="instit2">Univ Geneva merlo@</orgName>
								<orgName type="institution" key="instit3">Univ Geneva musillo@</orgName>
								<orgName type="institution" key="instit4">Univ Illinois at U-C</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept Computer Science</orgName>
								<orgName type="department" key="dep2">Dept Linguistics</orgName>
								<orgName type="department" key="dep3">Depts Linguistics and Computer Science</orgName>
								<orgName type="department" key="dep4">Dept Computer Science</orgName>
								<orgName type="institution" key="instit1">Univ Geneva</orgName>
								<orgName type="institution" key="instit2">Univ Geneva merlo@</orgName>
								<orgName type="institution" key="instit3">Univ Geneva musillo@</orgName>
								<orgName type="institution" key="instit4">Univ Illinois at U-C</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Musillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept Computer Science</orgName>
								<orgName type="department" key="dep2">Dept Linguistics</orgName>
								<orgName type="department" key="dep3">Depts Linguistics and Computer Science</orgName>
								<orgName type="department" key="dep4">Dept Computer Science</orgName>
								<orgName type="institution" key="instit1">Univ Geneva</orgName>
								<orgName type="institution" key="instit2">Univ Geneva merlo@</orgName>
								<orgName type="institution" key="instit3">Univ Geneva musillo@</orgName>
								<orgName type="institution" key="instit4">Univ Illinois at U-C</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
							<email>titov@uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept Computer Science</orgName>
								<orgName type="department" key="dep2">Dept Linguistics</orgName>
								<orgName type="department" key="dep3">Depts Linguistics and Computer Science</orgName>
								<orgName type="department" key="dep4">Dept Computer Science</orgName>
								<orgName type="institution" key="instit1">Univ Geneva</orgName>
								<orgName type="institution" key="instit2">Univ Geneva merlo@</orgName>
								<orgName type="institution" key="instit3">Univ Geneva musillo@</orgName>
								<orgName type="institution" key="instit4">Univ Illinois at U-C</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Successes in syntactic tasks, such as statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. ( <ref type="bibr" target="#b9">Wong and Mooney, 2007)</ref>) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates ( <ref type="bibr" target="#b3">Musillo and Merlo, 2006;</ref><ref type="bibr" target="#b1">Merlo and Musillo, 2008)</ref>. In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank <ref type="bibr">(Mar- cus et al., 1993)</ref> ) and semantic dependencies (extracted both from PropBank ( <ref type="bibr" target="#b5">Palmer et al., 2005)</ref> and NomBank ( <ref type="bibr" target="#b2">Meyers et al., 2004</ref>) under a unified representation.</p><p>We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency <ref type="bibr">(Titov and Hender- son, 2007a</ref>) and dependency parsing <ref type="bibr" target="#b8">(Titov and Henderson, 2007b</ref>). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is therefore applicable to other problems where two independent, but related, levels of representation are being learnt, such as statistical machine translation.</p><p>Currently the largest model we have trained achieves 80.5% macro-average F1 performance for the joint task, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Probability Model</head><p>Our probability model is a joint generative model of syntactic and semantic dependencies. The two dependency structures are specified as the sequence of actions for a synchronous parser, which requires each dependency structure to be projectivised separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Synchronous derivations</head><p>The derivations for syntactic dependency trees are the same as specified in <ref type="bibr" target="#b8">(Titov and Henderson, 2007b)</ref>, which are based on the shift-reduce style parser of <ref type="bibr" target="#b4">(Nivre et al., 2006</ref>). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. The derivations for semantic dependency graphs use virtually the same set of actions, but impose fewer constraints on when they can be applied, due to the fact that a word in a semantic dependency graph can have more than one parent. An additional action predicate s was introduced to label a predicate with sense s.</p><p>Let T d be a syntactic dependency tree with derivation</p><formula xml:id="formula_0">D 1 d , ..., D m d d ,</formula><p>and T s be a semantic dependency graph with derivation D 1 s , ..., D ms s . To define derivations for the joint structure T d , T s , we need to specify how the two derivations are synchronised, and in particular make the important choice of the granularity of the synchronisation step. Linguistic intuition would perhaps suggest that syntax and semantics are connected at the clause level -a big step size -while a fully integrated system would synchronise at each parsing decision, thereby providing the most communication between these two levels. We choose to synchronise the construction of the two structures at every word -an intermediate step size. This choice is simpler, as it is based on the natural total order of the input, and it avoids the problems of the more linguistically motivated choice, where chunks corresponding to different semantic propositions would be overlapping.</p><p>We divide the two derivations into the chunks between shifting each word onto the stack,</p><formula xml:id="formula_1">c t d = D b t d d , ..., D e t d d and c t s = D b t s s , ..., D e t s s , where D b t d −1 d = D b t s −1 s = shift t−1</formula><p>and</p><formula xml:id="formula_2">D e t d +1 d = D e t s +1 s = shift t .</formula><p>Then the actions of the synchronous derivations consist of quadruples C t = (c t d , switch, c t s , shift t ), where switch means switching from syntactic to semantic mode. This gives us the following joint probability model, where n is the number of words in the input.</p><formula xml:id="formula_3">P (T d , T s ) = P (C 1 , . . . , C n ) = t P (C t |C 1 , . . . , C t−1 )<label>(1)</label></formula><p>The probability of each synchronous derivation chunk C t is the product of four factors, related to the syntactic level, the semantic level and the two synchronising steps.</p><formula xml:id="formula_4">P (C t |C 1 , . . . , C t−1 ) = P (c t d |C 1 , . . . , C t−1 )× P (switch|c t d , C 1 , . . . , C t−1 )× P (c t s |switch, c t d , C 1 , . . . , C t−1 )× P (shift t |c t d , c t s , C 1 , . . . , C t−1 )<label>(2)</label></formula><p>These synchronous derivations C 1 , . . . , C n only require a single input queue, since the shift operations are synchronised, but they require two separate stacks, one for the syntactic derivation and one for the semantic derivation.</p><p>The probability of c t d is decomposed into derivation action D i probabilities, and likewise for c t s .</p><formula xml:id="formula_5">P (c t d |C 1 , . . . , C t−1 ) = i P (D i d |D b t d d ,. . ., D i−1 d , C 1 ,. . ., C t−1 )<label>(3)</label></formula><p>The actions are also sometimes split into a sequence of elementary decisions <ref type="bibr" target="#b7">Titov and Henderson, 2007a</ref>).</p><formula xml:id="formula_6">D i = d i 1 , . . . , d i n , as discussed in (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Projectivisation of dependencies</head><p>These derivations can only specify projective syntactic or semantic dependency graphs. Exploratory data analysis indicates that many instances of non-projectivity in the complete graph are due to crossings of the syntactic and semantic graphs. The amount of non-projectivity of the joint syntactic-semantic graph is approximately 7.5% non-projective arcs, while summing the nonprojectivity within the two separate graphs results in only roughly 3% non-projective arcs.</p><p>Because our synchronous derivations use two different stacks for the syntactic and semantic dependencies, respectively, we only require each individual graph to be projective. As with many dependency parsers <ref type="bibr" target="#b4">(Nivre et al., 2006;</ref><ref type="bibr" target="#b8">Titov and Henderson, 2007b</ref>), we handle non-projective (i.e. crossing) arcs by transforming them into noncrossing arcs with augmented labels. 1 Because our syntactic derivations are equivalent to those of ( <ref type="bibr" target="#b4">Nivre et al., 2006</ref>), we use their HEAD methods to projectivise the syntactic dependencies.</p><p>Although our semantic derivations use the same set of actions as the syntactic derivations, they differ in that the graph of semantic dependencies need not form a tree. The only constraints we place on the set of semantic dependencies are imposed by the use of a stack, which excludes crossing arcs. Given two crossing arcs, we try to uncross them by changing an endpoint of one of the arcs. The arc (p, a), where p is a predicate and a is an argument, is changed to (p, h), where h is the syntactic head of argument a. Its label r is then changed to r/d where d is the syntactic dependency of a on h. This transformation may need to be repeated before the arcs become uncrossed. The choice of which arc to transform is done using a greedy algorithm and a number of heuristics, without doing any global optimisation across the data.</p><p>This projectivisation method is similar to the HEAD method of ( <ref type="bibr" target="#b4">Nivre et al., 2006</ref>), but has two interesting new characteristics. First, syntactic dependencies are used to projectivise the semantic dependencies. Because the graph of semantic roles is disconnected, moving across semantic arcs is often not possible. This would cause a large number of roles to be moved to ROOT. Second, our method changes the semantic argument of a given predicate, whereas syntactic dependency projectivisation changes the head of a given dependent. This difference is motivated by a predicate-centred view of semantic dependencies, as it avoids changing a predicate to a node which is not a predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Learning Architecture</head><p>The synchronous derivations described above are modelled with an Incremental Sigmoid Belief Network (ISBN) <ref type="bibr" target="#b7">(Titov and Henderson, 2007a)</ref>. ISBNs are dynamic Bayesian Networks which incrementally specify their model structure based on the partial structure being built by a derivation. They have previously been applied to constituency and dependency parsing. In both cases the derivations were based on a push-down automaton, but ISBNs can be directly applied to any automaton. We successfully apply ISBNs to a two-stack automaton, without changing the machine learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Incremental Sigmoid Belief Networks</head><p>ISBNs use vectors of latent variables to represent properties of parsing history relevant to the next decisions. Latent variables do not need to be annotated in the training data, but instead get induced during learning. As illustrated by the vectors S i in <ref type="figure" target="#fig_0">figure 1</ref>, the latent feature vectors are used to estimate the probabilities of derivation actions D i . Latent variable vectors are connected to variables from previous positions via a pattern of edges determined by the previous decisions. Our ISBN model distinguishes two types of latent states: syntactic states, when syntactic decisions are considered, and semantic states, when semantic decision are made. Different patterns of interconnections are used for different types of states. We use the neural network approximation <ref type="bibr">(Titov and Hender- son, 2007a</ref>) to perform inference in our model.</p><p>As also illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, the induced latent variables S i at state i are statistically dependent on both pre-defined features of the derivation history D 1 , . . . , D i−1 and the latent variables for a finite set of relevant previous states S i , i &lt; i. Choosing this set of relevant previous states is one of the main design decisions in building an ISBN model. By connecting to a previous state, we place that state in the local context of the current decision. This specification of the domain of locality determines the inductive bias of learning with ISBNs. Thus, we need to choose the set of local (i.e. connected) states in accordance with our prior knowledge about which previous decisions are likely to be particularly relevant to the current decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Layers and features</head><p>To choose previous relevant decisions, we make use of the partial syntactic and semantic dependency structures which have been decided so far in the parse. Specifically, the current latent state vector is connected to the most recent previous latent state vectors (if they exist) whose configuration shares a node with the current configuration, as specified in <ref type="table">Table 1</ref>. The nodes are chosen because their properties are thought to be relevant to the current decision. Each row of the table indicates which nodes need to be identical, while each Closest Current Syn-Syn Srl-Srl Syn- <ref type="table">Srl  Input  Input  +  +  +  Top  Top  +  +  +  RDT  Top  +  +  LDT  Top  +  +  HT  Top  +  +  LDN  Top  +  +  Input  Top  +   Table 1</ref>: Latent-to-latent variable connections. Input= input queue; Top= top of stack; RDT= rightmost right dependent of top; LDT= leftmost left dependent of top; HT= Head of top; LDN= leftmost dependent of next (front of input).</p><p>column indicates whether the latent state vectors are for the syntactic or semantic derivations. For example, the first row indicates edges between the current state and a state which had the same input as the current state. The three columns indicate that this edge holds within syntactic states, within semantic states, and from syntactic to semantic states. The fourth cell of the third row, for example, indicates that there is an edge between the current semantic state on top of the stack and the most recent semantic state where the rightmost dependent of the current top of the semantic stack was at the top of the semantic stack. Each of these relations has a distinct weight matrix for the resulting edges in the ISBN, but the same weight matrix is used at each position where the relation applies. Training and testing times scale linearly with the number of relations.</p><p>The pre-defined features of the parse history which also influence the current decision are specified in table 2. The model distinguishes argument roles of nominal predicates from argument roles of verbal predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>Given a trained ISBN as our probability estimator, we search for the most probable joint syntactic-semantic dependency structure using a beam search. Most pruning is done just after each shift operation (when the next word is predicted). Global constraints (such as label uniqueness) are not enforced by decoding, but can be learnt.</p><p>For the system whose results we submitted, we then do a second step to improve on the choice of syntactic dependency structure. Because of the lack of edges in the graphical model from semantic to syntactic states, it is easy to marginalise out the semantic structure, giving us the most probable syntactic dependency structure. This syntactic structure is then combined with the semantic struc-  ture from the first stage, to get our submitted results. This second stage does not maximise performance on the joint syntactic-semantic dependency structure, but it better fits the evaluation measure used to rank systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head><p>The experimental set-up common for all the teams is described in the introduction ( <ref type="bibr" target="#b6">Surdeanu et al., 2008</ref>). The submitted model has latent variable vectors of 60 units, and a word frequency cut-off of 100, resulting in a small vocabulary of 1083 words. We used a beam of size 15 to prune derivations after each shift operation to obtain the joint structure, and a beam of size 40 when performing the marginalisation. Training took approximately 2.5 days on a standard PC with 3.0 GHz Pentium4 CPU. It took approximately 2 hours to parse the entire testing set (2,824 sentences) and an additional 3 hours to perform syntactic parsing when marginalising out the semantic structures. <ref type="bibr">2</ref> Shortly after the submission deadline, we trained a 'large' model with a latent variable vector of size 80, a word frequency cut-off of 20, and additional latent-to-latent connections from semantics to syntax of the same configuration as the last column  To explore the relationship between the two components of the model, we removed the edges between the syntax and the semantics in the submitted model. This model's performance drops by about 3.5% for semantic role labelling, thereby indicating that the latent annotation of parsing states helps semantic role labelling. However, it also indicates that there is much room for improvement in developing useful semantic-specific features, which was not done for these experiments simply due to constraints on development time.</p><p>To test whether joint learning degrades the accuracy of the syntactic parsing model, we trained a syntactic parsing model with the same features and the same pattern of interconnections as used for the syntactic states in our joint model. The resulting labelled attachment score was non-significantly lower (0.2%) than the score for the marginalised inference with the joint model. This result suggests that, though the latent variables associated with syntactic states in the joint model were trained to be useful in semantic role labelling, this did not have a negative effect on syntactic parsing accuracy, and may even have helped.</p><p>Finally, an analysis of the errors on the development set for the submitted model paints a coherent picture. We find attachment of adjuncts particularly hard. For dependency labels, we make the most mistakes on modification labels, while for semantic labels, we find TMP, ADV, LOC, and PRN particularly hard. NomBank arcs are not learnt as well as PropBank arcs: we identify PropBank SRL arguments at F1 70.8% while Nombank arguments reach 58.1%, and predicates at accuracy 87.9% for PropBank and 74.9% for NomBank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>While still preliminary, these results indicate that synchronous parsing is an effective way of building joint models on separate structures. The generality of the ISBN design used so far suggests that ISBN's latent feature induction extends well to estimating very complex probability models, with little need for feature engineering. Nonetheless, performance could be improved by task-specific features, which we plan for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An ISBN for estimating P (d i k |history(i, k)) -one of the elementary decisions. Variables whose values are given in history(i, k) are shaded, and latent and current decision variables are unshaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Pre-defined features. syn=syntactic stack; 
sem=semantic stack. Input= input queue; Top= 
top of stack; RDT= rightmost dependent of top; 
LDT= leftmost dependent of Top; HT= Head of 
top; LDN= leftmost dependent of next (front of 
input); A0-A5 of Top/Input= arguments of top of 
stack / input. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Scores on the development set and the final testing sets (percentages). D= development set; W=WSJ; B=Brown; WB=WSJ+Brown; of table 1. This model took about 50% longer in training and testing. In table 3, we report results for the marginalised inference ('submitted') and joint inference for the submitted model, and the results for joint inference with the 'large' model. The larger model improves on the submitted results by almost 1.5%, a signifi- cant improvement. If completed earlier, this model would have been fifth overall, second for syntactic LAS, and fifth for semantic F1.</figDesc><table></table></figure>

			<note place="foot" n="1"> During testing, these projectivised structures are then transformed back to the original format for evaluation.</note>

			<note place="foot" n="2"> A multifold speed-up with a small decrease in accuracy can be achieved by using a small beam.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly funded by European Community FP7 grant 216594 (CLASSiC, www.classic-project.org), Swiss NSF grant 114044, and Swiss NSF fellowships PBGE2-117146 and PBGE22-119276. Part of this work was done when G. Musillo was visiting MIT/CSAIL, hosted by Prof. Michael Collins.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing for highprecision semantic role labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Musillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of CoNLL</title>
		<meeting>s of CoNLL<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The nombank project: An interim report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop: Frontiers in Corpus Annotation</title>
		<editor>Meyers, A., editor, HLT-NAACL</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate semantic parsing of the Proposition Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Musillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of NAACL</title>
		<meeting>s of NAACL<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pseudo-projective dependency parsing with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNNL</title>
		<meeting>of CoNNL<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="221" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Proposition Bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="71" to="105" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of CoNLL-2008</title>
		<meeting>s of CoNLL-2008<address><addrLine>Manchester,UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constituent parsing with incremental sigmoid belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of ACL&apos;07</title>
		<meeting>s of ACL&apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="632" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent variable model for generative dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of IWPT&apos;07</title>
		<meeting>s of IWPT&apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of ACL&apos;07</title>
		<meeting>s of ACL&apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
