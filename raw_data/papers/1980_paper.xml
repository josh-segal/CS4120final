<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GePSeA: A General-Purpose Software Acceleration Framework for Lightweight Task Offloading *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
							<email>feng@cs.vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GePSeA: A General-Purpose Software Acceleration Framework for Lightweight Task Offloading *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hardware-acceleration techniques continue to be used to speed-up the execution of scientific codes. To do so, software developers identify portions of these codes that are amenable for offloading and map them to hardware accelerators. However, offloading such tasks to specialized hardware accelerators is non-trivial. Furthermore, these accelerators can add significant cost to a computing system. Consequently, we propose a framework called GePSeA (General Purpose Software Acceleration Framework), which uses a small fraction of the computational power on multi-core architectures to &quot;onload&quot; complex application-specific tasks. Specifically, GePSeA provides a lightweight process that acts as a helper agent to the application by executing application-specific tasks asynchronously and efficiently. We then apply the GePSeA framework to a real application , namely mpiBLAST, an open-source computational biology application , and demonstrate significant application-level benefits.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hardware-acceleration techniques continue to assist in improving the performance of scientific codes. These accelerators have traditionally focused on speeding-up the computationally intensive portions of an application such as Fourier transformations, finite-element methods (FEMs), and dense linear algebra. A secondary benefit of hardware accelerators is their capability to serve as dedicated asynchronous engines, as compared to general-purpose CPUs that are designed to be shared between multiple processes.</p><p>As high-end computing systems and their associated applications continue to grow in scale and complexity, the need to offload more complex tasks, like asynchronous data management, becomes increasingly important. However, such tasks are orthogonal to the primary purpose of existing hardware accelerators. Consequently, offloading these tasks to hardware accelerators would result in a significant increase in the complexity and overall cost of a computing system. On the other hand, multi-and many-core architectures have become ubiquitous with quad-and hex-core processors already available and 16-and 80-core processors on the roadmap <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>These trends point to the fact that today, and more so in the future, each physical node will have a massive number of processing units which, for some tasks, can be viewed as low-cost alternatives to expensive hardware accelerators.</p><p>In our previous work, we proposed ProOnE <ref type="bibr" target="#b11">[11]</ref>, a protocol onload engine that utilizes a small fraction of the computational power of multi-core architectures and allows for efficient onload of communication-related aspects in largescale systems. In this paper, we extend our previous design and propose GePSeA 1 -a general-purpose, softwareacceleration framework, which can also onload more complex application-specific tasks. Specifically, GePSeA provides a number of utility components that support different functionalities as well as a generic interface for applications to utilize these components through simple plug-ins. While this paper presents three categories of utility components: (i) datamanagement components, (ii) memory-management components, and (iii) synchronization and coordination components, the general-purpose nature of GePSeA allows it to be extended to other categories as well. In our design, different utility components are aggregated together into a lightweight process referred to as a software accelerator. This process acts as a helper agent to the application by executing lightweight, application-specific tasks asynchronously and efficiently.</p><p>Like ProOnE, GePSeA does not aim to replace tasks that dedicated hardware-based accelerators such as GPUs and Cell processors excel at. Instead, it leverages the existing hardware-offloaded features of such accelerators and extends them by onloading more complex, application-specific tasks that cannot be easily offloaded. We demonstrate the efficacy of GePSeA by applying it to mpiBLAST, an open-source bioinformatics application and delivering a performance improvement with 2.05x speed-up.</p><p>The rest of the paper is organized as follows. Section 2 presents background and related work on accelerators and frameworks for parallel programming, followed by the design of our proposed software accelerator framework, i.e., GePSeA, in Section 3. We then present a case study with mpiBLAST in Section 4, followed by a detailed experimental evaluation in Section 5. Section 6 presents our conclusions and discussion for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>While there exist many research efforts on developing specialized hardware accelerators <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>, very little exists with respect to software accelerators. However, there has been considerable interest in the research community to develop multi-core-aware applications <ref type="bibr" target="#b18">[18]</ref>. In this paper, we show how our GePSeA framework provides software-based acceleration to dramatically improve the performance of our case-study application, mpiBLAST.</p><p>Our work is distinct from other frameworks and libraries <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b8">8]</ref> available for the development of parallel applications. Frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">5]</ref> and libraries such as MPICH2 and OpenMP are collections of library functions that are intended for the quick development of parallel programs. In contrast, our framework is not intended for the development of parallel applications; rather, it is for the development of software accelerators that assist parallel applications.</p><p>In <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>, the authors study several techniques that can be used for overlapping I/O, computation, and communication in parallel applications written with MPI and MPI-IO. Using GePSeA, applications can offload any of the aforementioned tasks, including disk I/O and communication.</p><p>In <ref type="bibr" target="#b18">[18]</ref>, the authors present a communication engine to exploit the cores in multi-core systems using various multi-threading techniques. The authors plan to integrate their engine with MPICH2 in the future. This work is most closely related to ours. However, GePSeA differs in that it provides an infrastructure to quickly build application-specific software accelerators for any application (whether or not using MPICH2) as well as to efficiently schedule onloaded tasks to maximize the use of a compute node's system resources.</p><p>In summary, our work differs from the existing literature with respect to its capabilities and underlying architecture, and at the same time, forms a complementary contribution to other existing literature that can be simultaneously utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The GePSeA Framework Design</head><p>The GePSeA framework is designed to function as independently of the application as possible. Much like existing hardware accelerators, GePSeA onloads several utility functions on a dedicated unit (subset of cores in this case) and provides a simple interface that applications can use to take advantage of such functionality. These tasks are executed asynchronously, allowing the application to continue its respective processing. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the design.</p><p>While GePSeA is a general-purpose framework that allows arbitrary tasks to be "onloaded," we focus on three categories of components in this paper: (a) data-management components, (b) memory-management components and (c) synchronization and coordination components, as shown in <ref type="figure">Figure 2</ref>. We describe the details of each of these categories as well as some of the components within these categories below. While the basic idea of some of these components is not new, e.g., <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b13">13]</ref>, our work focuses on providing such functionality in the context of a general-purpose software accelerator, which presents its own unique challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data-Management Utilities</head><p>The data-management utilities deal with moving I/O data associated with the application, including input data, output data, and transitional meta-data that is created and destroyed during application execution. We present four utilities within this category: (i) distributed data caching, (ii) data streaming service, (iii) distributed data sorting, and (iv) data compression engine. We note that while this paper presents softwarebased designs for each of these utilities, this does not preclude GePSeA from taking advantage of hardware implementations when they are available. When such hardware units are available, GePSeA would simply replace these modules with the hardware ones while retaining the interface it exposes to applications. Thus, from an application's perspective, the actual implementation of these utilities does not matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributed Data Caching:</head><p>This component provides the caching capability for an entire input database across all of system memory. The input data used by many applications (terabytes in size) is typically several times larger than the amount of memory on each node (gigabytes in size). However, for large-scale systems, the total system memory is tens of terabytes in size which, on the whole, is sufficient to cache the entire input data in many cases. The distributed data caching component performs this task by trapping I/O calls, reading the entire input data into the system memory, and responding to I/O requests from the distributed memory cache, instead of from the disk or filesystem.</p><p>The main challenge in distributed data caching is the addressability of the data. Specifically, should the application be aware of the actual locality of the data segment, or should this information be hidden and handled internally by the data caching component? For most scalable applications, I/O transactions involve moving reasonably large amounts of data (e.g., a few megabytes at a time). So, the overhead added due <ref type="figure">Figure 2</ref>: GePSeA Architecture to trapping I/O calls, automatically figuring out the location, and fetching data is typically not a major portion of the overall I/O cost. Furthermore, implicitly managing data locality makes it simpler and intuitive for applications to use this component. Thus, we chose to hide data locality. Data movement is completely handled by this component through appropriate communication that is initiated and terminated within the components and never exposed to the applications.</p><p>Data Streaming Service: While distributed data caching moves data from the filesystem to system memory thus reducing I/O overhead, the amount of time taken for data movement is still large, especially when the amount of computation per data unit is small (e.g., in search algorithms). Thus, to keep the application fed with data, techniques such as prefetching are needed. The data streaming service handles this by swapping out unused data with fresh data that the application would use. This component includes distributed coordination with other instances of the GePSeA helper agents in order to minimize duplication of data (i.e., data is swapped between two nodes instead of replicating and utilizing more memory than needed). In addition, because this coordination is completely handled by the GePSeA helper agents, the prefetching and swapping is done in a completely asynchronous manner without disturbing the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Compression Engine:</head><p>As the name suggests, this component deals with (de)compressing data. However, together with regular byte-stream compression, this engine also provides capabilities for application-specific compression, as presented in our previous work <ref type="bibr" target="#b4">[4]</ref>. Specifically, this engine can either view the data as a stream of bytes or as high-level application-specific objects that are converted to meta-data that is much smaller in size. Once communicated over the network, this meta-data is converted back to the actual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory-Management Utilities</head><p>Memory-management utilities deal with handling system memory, allowing applications to access the entire memory of the system rather than just the local nodes memory.</p><p>Global Memory Aggregator: Operations like caching, indexing, and searching can all perform better with larger memory. Since the cost of remote memory access is typically much lower than the cost of disk access, these components can effectively utilize the free memory available on other nodes. The global memory aggregator efficiently allows applications to utilize the memory on the entire cluster instead of just their local memory. Specifically, this primitive presents a global address space to its upper layers and maintains a mapping of the locations on the global address space with the actual node and physical address of these locations. Unlike the distributed data caching service, this component does not perform the global address translation to the actual physical address and physical node transparently; this is because memory accesses are typically much smaller in size than I/O accesses and are expected to have very little overhead. Thus, applications explicitly control and manage data placement on the system. Data movement, however, is completely handled by the global memory aggregator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coordination and Synchronization Utilities</head><p>For applications using a large number of processes, coordination and synchronization between the processes can be a major portion of the application execution. This category deals with utility components that improve such tasks.</p><p>Global Process State Management: Managing the data processing performed by application processes requires sharing certain information about each node, such as whether the process on that node is idle and waiting for communication or what fragment of the data it currently hosts. The performance and scalability of applications largely depends on how efficiently global process state is maintained. Thus, the global process-state management primitive aims at maintaining an up-to-date and complete information about the status of the different nodes in the cluster.</p><p>Bulletin Board Service: This task provides an addressable memory that can be read or written to by any other node in the cluster system. The bulletin board itself is distributed memory placed on different nodes in the system. However, from an application's perspective, this is a contiguous chunk of memory that is available to publish information. Together with the efficient movement of data, this component also handles the synchronization required in order to avoid data corruption.</p><p>Reliable Advertising Service: The reliable advertising service addresses reliable and efficient ways of distributing information across the entire system. Where available, this task can internally utilize the unreliable multicast features provided by networks such as InfiniBand, while providing software reliability on top of it. On other architectures, such reliability is handled using reliable protocols such as TCP. This task also includes other capabilities such as protection against overwrite (i.e., two continuous messages from the same host will ensure that the first message is read by the host before the second is delivered), host transparent advertising (i.e., the remote host does not have to actively provide a buffer to receive the advertisements from other nodes), and advertisement filtering (i.e., getting rid of irrelevant advertisements), and various others that need to be handled efficiently.</p><p>Distributed Lock Management: A distributed lock manager allows lock-based synchronization between multiple nodes to avoid race conditions while accessing shared resources. While such locking capability can be provided using the atomic operations provided by high-speed networks such as InfiniBand, the GePSeA helper agents can enhance these features by providing capabilities, such as request queuing and group-wise shared locks, that cannot be easily provided in hardware. For environments where such networks are not available, the GePSeA agents can perform both the atomic operations as well as the request queueing and shared locks.</p><p>Dynamic Load Balancing: Load imbalance among the nodes of the cluster can result in a bottlenecks that can limit the scalability of parallel applications. This component provides mechanisms to balance the load on each node using the reliable advertising service component to keep track of availability of all the nodes in the system. In this approach, each node announces its availability when idle to an elected node called the scheduler. The scheduler then assigns the work to the nodes. Each node periodically queries the scheduler to obtain the information about the work assignment to itself and to all the other nodes in the cluster. Multiple schedulers can be used to avoid hot spots but are outside the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Study with mpiBLAST</head><p>The GePSeA framework is a general-purpose software accelerator that can be used by many applications. To demonstrate its capabilities, we describe a case study using a popular sequence-search application called mpiBLAST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">mpiBLAST Overview</head><p>mpiBLAST <ref type="bibr" target="#b7">[7]</ref> is one the most popular sequence-search applications used in computational biology. It internally uses the NCBI BLAST toolkit <ref type="bibr" target="#b0">[1]</ref>, the de facto "gold standard" for sequential pairwise sequence-search that is ubiquitously used in biomedical research. The BLAST tool searches one or multiple input query sequences against a database of known nucleotide (DNA) or amino acid sequences. A similarity score is calculated for each close match based on a statistical model. The similarity of the comparison is measured by the match with the highest score. As a result, the sequences in the database that are most similar to the query sequence are reported, along with their matches scored beyond a certain threshold. Therefore, the BLAST process is essentially a top-k search, where k can be specified by the user, with a default value of 500.</p><p>The core of the mpiBLAST algorithm is based on database segmentation. Before the search, the raw sequence database is formatted, partitioned into fragments, and stored in a shared storage space. mpiBLAST then organizes parallel processes into one master and many workers, as shown in <ref type="figure">Figure ?</ref>?. The master breaks down the search job in a Cartesian-product manner and maintains a list of unsearched tasks, each represented as a pair of a query sequence and a database fragment. Whenever a worker becomes idle, it asks the master for a unsearched task and copies the needed fragment to its local disk (if the fragment has not been cached locally) and performs a BLAST search on its assignment. Upon finishing a task, a worker reports its local results to the master for centralized result merging. Once the results of searching a query sequence against all database fragments have been collected, the master calls the standard NCBI BLAST output function to format and print out results of this query to the output file. By default, those results contain the top 500 database sequences with highest similarity to the query sequence along with their matches. The above process repeats until all tasks have been searched. With database segmentation, mpiBLAST can deliver super-linear speed-up when searching sequence databases larger than the memory of a single node. In recent developments, mpiBLAST has evolved to use a more scalable parallel approach that allows different queries to be concurrently searched as well <ref type="bibr" target="#b9">[9]</ref>.</p><p>mpiBLAST, like most parallel sequence-search algorithms, follows a scatter-search-gather model. The scatter stage consists of query and/or database segmentation. In the search stage, each worker searches the query against the assigned database portion. Finally, the gather stage consists of merging of output results from individual workers.   <ref type="figure" target="#fig_2">Figure 4</ref> shows components at each layer of accelerator framework used by the mpiBLAST application. We developed asynchronous output consolidation, runtime output compression and hot-swap database fragments plug-ins for mpiBLAST. Using these plug-ins mpiBLAST can offload considerable amount of work to accelerator that can be executed in parallel. Though we developed these plug-ins for mpiBLAST, we believe they can be used by other sequence search applications such that follow scatter-searchgather model described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">mpiBLAST over GePSeA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Asynchronous Output Consolidation Plug-In</head><p>This plug-in utilizes the processing capability of the accelerator to merge/sort the data that is distributed across all multiple nodes in parallel. This is an important capability since result merging can be done asynchronously by the accelerators while the applications can continue their respective application processing. For example, if the first node has gotten its results earlier than the other nodes, it does not have to wait for till the others are done to perform the merge. Instead, it can hand over this data to the accelerator; this accelerator can wait for the other nodes and sort the data incrementally as the other nodes finish their task.</p><p>To remove the bottleneck due to single writer, each accelerator has the capability to write the output results directly to the output file on a shared storage. Accelerator writes the results into a separate file for each query assigned to it by the scheduler. After all the queries have been processed the result files are sorted and merged into a single output file. This work of merging and writing output data is divided fairly among the accelerators using load balancing primitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Runtime Output Compression Plug-In</head><p>The data compression engine provides capabilities to compress the actual data. We conducted tests on the compressibility of the BLAST output and found that when the output was in the standard pairwise alignment text format that the output could be compresses to less than 10 percent of its original size using gzip. This is mainly due to the redundancy found in the BLAST output. The runtime output compression can be used to compress the output before transfer thereby significantly reducing the transfer time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Hot-Swap Database Fragments Plug-In</head><p>Currently, in sequence search applications, the database is pre-partitioned into a number of fragments that are distributed over different nodes. Normally worker node searches the database it holds. However balancing the work load on each worker may require a node to hot-swap the portion of the database it is currently processing with the other portions ondemand. This feature swaps the database fragments asynchronously across the nodes while hosts can continue their respective application processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>In this section, we evaluate the accelerator for mpiBLAST and do a performance analysis. For all our experiments, we used the GenBank nr database, a protein type repository frequently searched against by bioinformatics researchers. The size of the raw nr database is nearly 1 GB, consisting of 1,986,684 peptide sequences. We used ICE cluster residing in Synergy Lab at Virginia Tech for our experiments. ICE cluster has 9 nodes with each node equipped with two dual-core AMD Opteron 2218 processors. Thus, each node has 4 cores. Memory and cache size on each node is 4 GB and 1024 KB, respectively. The interconnect is 1-Gbps Ethernet.</p><p>For our experiments, we pre-partitioned the NIH GenBank nr database into 8 fragments using the mpiformatdb utility. For most experiments, the input query sets containing different number of sequences were chosen randomly from nr database. For the other experiments, pseudo-random query sets were chosen in order to better control the output size and to better analyze the efficacy of specific features of our accelerator.</p><p>Our experimental methodology includes running mpiBLAST with an accelerator and without an accelerator on a given set of processors on the ICE cluster for the same set of input query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Accelerator on an Existing Core</head><p>By an existing core, we mean a core that is already running some application process, e.g., worker process. Each node of the cluster has a total of 4 cores. We ran one worker process on each of these 4 cores with each worker process explicitly bound to a core using the physcpubind utility available on NUMA machines. Our accelerator (one per node) ran on one of these 4 already-occupied cores, as per the scheduling strategy of the operating system, as shown in <ref type="figure">Figure 5</ref>.</p><p>We conducted the experiments running mpiBLAST with and without an accelerator for 8, 16, 24 and 36 workers, where each worker ran on a separate core. For this experiment, 300 input query sequences were randomly chosen from the nr database.</p><p>Figure 5: (a) Each worker process runs on a separate core. (b) Core 2 is shared between worker process P2 and accelerator A1 <ref type="figure" target="#fig_4">Figure 6</ref> shows significant performance improvement using the software accelerator. For 36 workers, we observed as much as a 2.05x speed-up with an accelerator (per node) against no accelerator. The speed-up increases with increase in worker processes. This improvement is attributed to the overlapping of worker computation and communication (between worker and the writer). Offloading of result merging and writing tasks to an accelerator also contributes significantly to this speed-up. These results were particularly interesting as we are running the accelerator on an oversubscribed core (against running exclusively on a spare core thereby committing it additional system resources) and still observe significant improvement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accelerator on a Spare Core</head><p>By spare core, we mean a core that does not run any application process. In this experiment, on a given node, we run 3 worker processes each bound explicitly to a core with the accelerator bound explicitly to the fourth spare core. Therefore for 9 nodes, we have total of 27 workers with one accelerator on each node. <ref type="figure">Figure 8</ref> shows significant speed-up of mpiBLAST with a maximum of 1.68x for 27 workers. We noticed that while the CPU utilization of a worker process is nearly 100%, the CPU utilization of an accelerator is only 2-5%. Therefore, running the accelerator exclusively on a spare core results in under-utilization of system resources.  <ref type="formula">(2)</ref> 3 worker processes running on 3 cores and an accelerator running on the fourth spare core. Thus, for a 9-node configuration, we compare between 36 mpiBLAST worker processes running on 36 different cores without an accelerator versus 27 mpiBLAST worker processes with an accelerator on each node. Even with one worker less per node, we still see speed-up as much as 1.4x with 36 workers. Refer to <ref type="figure" target="#fig_0">Figure 10</ref>. Even though accelerator utilizes only 2-5% CPU cycles, it still performs better when replaced by a worker whose CPU utilization is close to 100%.</p><p>As mentioned earlier that running accelerator on an spare core result in under-utilization of CPU but even then it performs better than being replaced by a worker process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Worker Search Time</head><p>We analyzed the variation of search time and the non-search time of each worker with increase in number of worker processes. For mpiBLAST, search time refers to the computation time while non-search time refers to non-computation time.</p><p>We observed that for a fairly large number of input query sequences, percentage of search time of a worker to total time decreases rapidly from 92.2% to nearly 71% as shown in <ref type="figure">Fig</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Asynchronous Output Consolidation</head><p>We tested distributed output consolidation feature provided by accelerator as described in section 4.2.1. In this experiment we compared output result consolidation done by only one accelerator in the cluster (chosen statically) against result consolidation done by each accelerator in the cluster. Results are shown in <ref type="figure" target="#fig_0">Figure 12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Dynamic Load Balancing</head><p>We tested dynamic load balancing functionality provided by accelerator as described in the design section. We compared dynamic load balancing with static allocation of result merging and writing assignment. We see close to 14% of performance improvement from the <ref type="figure" target="#fig_0">Figure 13</ref>. With highly uneven queries this difference could be very high.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Run-Time Output Compression</head><p>We tested compression functionality provided by accelerator as described in section 4.2.2. Negative values <ref type="figure" target="#fig_0">Figure 14</ref> signify increase in running time of mpiBLAST using the com-pression engine. This is contrary to our expectations. However, for compression at run time to be effective, network latency must exceed the time required to compress and uncompress the data. We believe that size of result data generated by our experiments is not large enough to make positive impact on the running time. However we do observe a that running time decreases with increase in worker processes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we presented a general-purpose software acceleration framework called GePSeA that dedicates a small subset of the available cores on a multi-core equipped node to "onload" complex application-specific tasks. The proposed framework does not aim to replace tasks that dedicated hardware-based accelerators such as GPGPUs and Cell processors specialize in. Instead, it utilizes the existing hardwareoffloaded features of such accelerators, but extends them by onloading more complex application-specific functionality that cannot be easily offloaded. Together with the detailed design of GePSeA, we also presented a case study with mpiBLAST, an open-source computational biology application and demonstrated more than 205% improvement in the overall application performance.</p><p>Our future work would involve extending GePSeA to support new abstract features so that the accelerator can be used by more applications. We also plan to use our framework with other parallel applications and also conduct conduct extensive performance and usability studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Using Accelerators for Parallel Applications</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter-Search-Gather Model of Parallel Sequence Search Applications [2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accelerator Framework used by mpiBLAST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Speed-up: Accelerator on an Existing Core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: (a) Each of 3 worker processes runs on separate core, core 4 unused (b) Each of 3 worker processes runs on separate core, accelerator A1 on fourth spare core</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (a) 4 worker processes per node running on separate cores (b) 3 worker processes and an accelerator per node running on separate cores</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Speed-up using accelerator with different number of workers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>- ure 11 .</head><label>11</label><figDesc>However mpiBLAST with accelerator reported over 99% of the total worker time as search time consistently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Worker search time as a percentage of total worker time with and without accelerator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Reduction in running time of mpiBLAST due to asynchronous result consolidation feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Reduction in running time of mpiBLAST due to dynamic load balancing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Effect of using compression engine to compress the output at runtime</figDesc></figure>

			<note place="foot" n="1"> GePSeA is pronounced like the word gypsy. 1</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Altschula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gisha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Millerb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyersc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lipmana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maintainable Software Architecture for Fast and Modular Bioinformatics Sequence Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Archuleta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Tilevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu Chun Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd IEEE International Conference on Software Maintenance</title>
		<meeting>of the 23rd IEEE International Conference on Software Maintenance</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Aumage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Namyst</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A True Multi-Protocol MPI for High-Performance Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mpich/</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 15th International Parallel and Distributed Processing Symposium</title>
		<meeting>of 15th International Parallel and Distributed essing Symposium</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archuleta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DCA: A distributed CCA framework based on MPI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bramley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of High-Level Parallel Programming Models and Supportive Environments</title>
		<meeting>of High-Level Parallel Programming Models and Supportive Environments</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intel Corp. Tera-scale computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The design, implementation, and evaluation of mpiblast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Linux Clusters: The HPC Revolution</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Symposium on Operating System Design and Implementation</title>
		<meeting>of Symposium on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel Genomic Sequence-Searching on an Ad-Hoc Grid: Experiences, Lessons Learned, and Implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archuleta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM SC2006: The International Conference on High-Performance Computing, Networking, and Storage</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overlapping Communication and Computation in MPI by Multithreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Jiayin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Yongwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guangwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Parallel and Distributed Processing Techniques and Applications</title>
		<meeting>of International Conference on Parallel and Distributed essing Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ProOnE: A General Purpose Protocol Onload Engine for Multi-and Many-Core Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Argonne National Laboratory</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hardware and software systems for accelerating common bioinformatics sequence analysis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biosilico</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High Performance Distributed Lock Management Services using Network-based Remote Atomic Operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mamidala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symposium on Cluster Computing and the Grid (CCGrid)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing the Performance of MPI-IO Applications by Overlapping I/O, Computation and Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>of Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The cell project at ibm research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibm Research</surname></persName>
		</author>
		<ptr target="http://www.research.ibm.com/cell/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Larrabee: A many-core x86 architecture for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Abrash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Junkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Cavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Espasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Grochowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;08: ACM SIGGRAPH 2008 papers</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BioSCAN: A Dynamically Reconfigurable Systolic Array for Biosequence Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Dettloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research on Integrated Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multithreaded communication engine for multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Trahay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Namyst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Symposium on Parallel and Distributed Processing</title>
		<meeting>of IEEE International Symposium on Parallel and Distributed essing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<ptr target="http://www.gpgpu.org" />
		<title level="m">General-purpose computation using graphics hardware</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimized Distributed Data Sharing Substrate in Multi-Core Commodity Clusters: A Comprehensive Study with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symposium on Cluster Computing and the Grid (CCGrid)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
