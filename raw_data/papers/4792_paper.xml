<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Labeling of Multinomial Topic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
							<email>xshen@uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Labeling of Multinomial Topic Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Statistical topic models</term>
					<term>multinomial distribu- tion</term>
					<term>topic model labeling</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Statistical topic modeling has attracted much attention recently in machine learning and text mining <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> due to its broad applications, including extracting scientific research topics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref>, temporal text mining <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, spatiotemporal text mining <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, authortopic analysis <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref>, opinion extraction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref>, and information retrieval <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>. Common to most of this work is the idea of using a multinomial word distribution (also called a unigram language model) to model a topic in text.</p><p>For example, the multinomial distribution shown on the left side of <ref type="table" target="#tab_1">Table 1</ref> is a topic model extracted from a collection of abstracts of database literature. This model gives high probabilities to words such as "view", "materialized", and "warehouse," so it intuitively captures the topic "materialized view." In general, a different distribution can be regarded as representing a different topic.</p><p>Many different topic models have been proposed, which can extract interesting topics in the form of multinomial distributions automatically from text. Although the discovered topic word distributions are often intuitively meaningful, a major challenge shared by all such topic models is to accurately interpret the meaning of each topic. Indeed, it is generally very difficult for a user to understand a topic merely based on the multinomial distribution, especially when the user is not familiar with the source collection. It would be hard to answer questions such as "What is a topic model about?" and "How is one distribution different from another distribution of words?".</p><p>Without an automatic way to interpret the semantics of topics, in existing work of statistical topic modeling, people generally either select top words in the distribution as primitive labels <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref>, or generate more meaningful labels manually in a subjective manner <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. However, neither of these options is satisfactory. Consider the following topic extracted from a collection of database literature:  It is difficult for someone not familiar with the database domain to infer the meaning of the topic model on the left just from the top terms. Similar examples can be found in scientific topics, where extracting top terms is not very useful to interpret the coherent meaning of a topic. For example, a topic labeled with "insulin glucose mice diabetes hormone" 1 may be a good topic in medical science, but makes little sense to common audience.</p><p>Manual labeling also has its own problems. Although manually generated labels are usually more understandable and better capture the semantics of a topic (see <ref type="table" target="#tab_1">Table 1</ref>), it requires a lot of human effort to generate such labels.</p><p>A more serious problem with manual labeling is that the labels generated are usually subjective and can easily be biased towards the user's personal opinions. Moreover, relying on human labeling also makes it hard to apply such topic models to online tasks such as summarizing search results.</p><p>Thus it is highly desirable to automatically generate meaningful labels for a topic word distribution so as to facilitate interpretations of topics. However, to the best of our knowledge, no existing method has been proposed to automatically generate labels for a topic model or a multinomial distribution of words, other than using a few top words in the distribution to label a topic. In this paper, we study this fundamental problem which most statistical topic models suffer from and propose probabilistic methods to automatically label a topic.</p><p>What makes a good label for a topic? Presumably, a good label should be understandable to the user, could capture the meaning of the topic, and distinguish a topic from other topics. In general, there are many possible choices of linguistic components as topic labels, such as single terms, phrases, or sentences. However, as we could learn from Table 1, single terms are usually too general and it may not be easy for a user to interpret the combined meaning of the terms. A sentence, on the other hand, may be too specific, thus it could not accurately capture the general meaning of a topic. In between these two extremes, a phrase is coherent and concise enough for a user to understand, while at the same time, it is also broad enough to capture the overall meaning of a topic. Indeed, when labeling topic models manually, most people prefer phrases <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. In this paper, we propose a probabilistic approach to automatically labeling topic models with meaningful phrases.</p><p>Intuitively, in order to choose a label that captures the meaning of a topic, we must be able to measure the "semantic distance" between a phrase and a topic model, which is challenging. We solve this problem by representing the semantics of a candidate label with a word distribution and casting this labeling problem as an optimization problem involving minimizing the Kullback-Leibler divergence between the topic word distribution and a candidate label word distribution, which can be further shown to be maximizing mutual information between a label and a topic model. The proposed methods are evaluated using two text data sets with different genres (i.e., literature and news). The results of experiments with user study show that the proposed labeling methods are quite effective and can automatically generate labels that are meaningful and useful for interpreting the topic models.</p><p>Our methods are general and can be applied to labeling a topic learned through all kinds of topic models such as PLSA, LDA, and their variations. Indeed, it can be applied as a post-processing step to any topic model, as long as a topic is represented with a multinomial distribution over words. Moreover, the use of our method is not limited to labeling topic models; our method can also be used in any text management tasks where a multinomial distribution over words can be estimated, such as labeling document clusters and summarizing text. By switching the context where candidate labels are extracted and where the semantic distance between a label and a topic is measured, we can use our method to generate labels that can capture the content variation of the topics over different contexts, allowing us to interpret topic models from different views. Thus our labeling methods also provide an alterative way of solving a major task of contextual text mining <ref type="bibr" target="#b17">[18]</ref>.</p><p>The rest of the paper is organized as follows. In Section 2, we formally define the problem of labeling multinomial topic models. In Section 3, we propose our probabilistic approaches to generating meaningful phrases as topic labels. The variation of this general method is discussed in Section 4, followed by empirical evaluation in Section 5, discussion of related work in Section 6, and our conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head><p>Given a set of latent topics extracted from a text collection in the form of multinomial distributions, our goal is, informally, to generate understandable semantic labels for each topic. We now formally define the problem of topic model labeling. We begin with a series of useful definitions.</p><p>Definition 1 (Topic Model) A topic model θ in a text collection C is a probability distribution of words {p(w|θ)}w∈V where V is a vocabulary set. Clearly, we have</p><formula xml:id="formula_0">w∈V p(w|θ) = 1.</formula><p>Intuitively, a topic model can represent a semantically coherent topic in the sense that the high probability words often collectively suggest some semantic theme. For example, a topic about "SVM" may assign high probabilities to words such as "supporting", "vector" and "kernel." It is generally assumed that there are multiple such topic models in a collection.</p><p>Definition 2 (Topic Label) A topic label, or a "label ", l, for a topic model θ, is a sequence of words which is semantically meaningful and covers the latent meaning of θ.</p><p>Words, phrases, and sentences are all valid labels under this definition. In this paper, however, we only use phrases as topic labels.</p><p>For the example above, a reasonable label may be "supporting vector machine."</p><p>Definition 3 (Relevance Score) The relevance score of a label to a topic model, s(l, θ), measures the semantic similarity between the label and the topic model. Given that l1 and l2 are both meaningful candidate labels, l1 is a better label for θ than l2 if s(l1, θ) &gt; s(l2, θ).</p><p>With these definitions, the problem of Topic Model Labeling can be defined as follows:</p><p>Given a topic model θ extracted from a text collection, the problem of single topic model labeling is to (1) identify a set of candidate labels L = {l1, ..., lm}, and (2) design a relevance scoring function s(li, θ). With L and s, we can then select a subset of n labels with the highest relevance scores L θ = {l θ,1 , ..., l θ,n } for θ.</p><p>This definition can be generalized to label multiple topics. Let Θ = {θ1, ..., θ k } be a set of k topic models, and L = {l1, ..., lm} be a set of candidate topic labels. The problem of multiple topic model labeling is to select a subset of ni labels, Li = {li,1, ..., li,n i }, for each topic model θi. In most text mining tasks, we would need to label multiple topics.</p><p>In some scenarios, we have a set of well accepted candidate labels (e.g., the Gene Ontology entries for biological topics). However, in most cases, we do not have such a candidate set. More generally, we assume that the set of candidate labels can be extracted from a reference text collection, which is related to the meaning of the topic models. For example, if the topics to be labeled are research themes in data mining, the reasonable labels could be extracted from the KDD conference proceedings. In most text mining tasks, it would be natural to use the text collection to be mined as our reference text collection to extract candidate labels.</p><p>Therefore, a natural work flow for solving the topic labeling problem would be (1) extracting a set of candidate labels from a reference collection; (2) finding a good relevance scoring function; (3) using the score to rank candidate labels w.r.t. each topic model; and (4) select top ranked ones to label the corresponding topic.</p><p>However, many challenges need to be solved in order to generate good topic labels automatically. As discussed in Section 1, a good set of labels for a topic should be (1) understandable, (2) semantically relevant, (3) covering the whole topic well, and (4) discriminative across topics. Without prior domain knowledge, extracting understandable candidate labels is non-trivial. Since a topic model and a label have different representations, it is also difficult to compare their semantics. As a result, there is no existing method to measure the semantic relevance between a topic model and a label. Even with a good measure for semantic relevance, it is still unclear how we can ensure that the label would fully cover the meaning of a topic and also capture the difference between different topic models.</p><p>In the next section, we propose a probabilistic approach to generating labels for topic models automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBABILISTIC TOPIC LABELING</head><p>To generate labels that are understandable, semantically relevant, discriminative across topics, and of high coverage of each topic, we first extract a set of understandable candidate labels in a preprocessing step, then design a relevance scoring function to measure the semantic similarity between a label and a topic, and finally propose label selection methods to address the inter-topic discrimination and intra-topic coverage problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Candidate Label Generation</head><p>As discussed in Section 1, compared with single terms and sentences, phrases appear to be more appropriate for labeling a topic. Therefore, given a reference collection C, the first task is to generate meaningful phrases as candidate labels. Phrase generation has been addressed in existing work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6]</ref>. In general, there are two basic approaches:</p><p>Chunking/Shallow Parsing: Chunking (Shallow Parsing) is a common technique in natural language processing, which aims at identifying short phrases, or "chunks" in text. A chunker often operates on text with part of speech tags, and uses the tags to make decisions of chunking according to some grammar, or through learning from labeled training sets. In our work, we extract the chunks/phrases frequently appearing in the collection as candidate labels.</p><p>The advantage of using an NLP chunker is that the phrases generated are grammatical and meaningful. However, the accuracy of chunking usually depends heavily on the domain of the text collection. For example, if the model is trained with news articles, it may not be effective on scientific literature. Even in scientific literature, processing biology literature is much different from processing computer science publications.</p><p>Ngram Testing: Another type of method is to extract meaningful phrases from word ngrams based on statistical tests. The basic idea is that if the words in an ngram tend to co-occur with each other, the ngram is more likely to be an n-word phrase.</p><p>There are many methods for testing whether an ngram is a meaningful collocation/phrase <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref>. Some methods rely on statistical measures such as mutual information <ref type="bibr" target="#b6">[7]</ref>. Others rely on hypothesis testing techniques. The null hypothesis usually assumes that "the words in an ngram are independent", and different test statistics have been proposed to test the significance of violating the null hypothesis. Two famous hypothesis testing methods showing good performance on phrase extraction are χ 2 Test and Student's T-Test <ref type="bibr" target="#b14">[15]</ref>.</p><p>The advantage of such an ngram testing approach is that it does not require training data, and is applicable to text collection of any ad hoc domains/topics. The disadvantage is that the top ranked ngrams sometimes are not linguistically meaningful, and it usually only works well for bigrams.</p><p>In our experiments, we compare both approaches to extract the set of candidate labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Relevance Scoring</head><p>We propose two relevance scoring functions to rank labels by their semantical similarity to a topic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Zero-Order Relevance</head><p>The semantics of a latent topic θ is fully captured by the corresponding multinomial distribution. Intuitively any reasonable measure of the semantic relevance of a label to a topic should compare the label with this distribution in some way. One possibility is to define the semantic relevance score of a candidate phrase l = u0u1...um (ui is a word) as</p><formula xml:id="formula_1">Score = log p(l|θ) p(l) = 0≤i≤m log p(ui|θ) p(ui)</formula><p>where the independence of u i s is assumed. The basic idea of this approach is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Basically, a phrase containing more "important" (high p(w|θ)) words in the topic distribution is assumed to be a good label. p(ui) is to correct the bias toward favoring short phrases and can be estimated using some background collection B, or simply set to uniform. With this method, we essentially score a candidate phrase based on the likelihood that the phrase is "generated" using the topic model θ as opposed to some background word distribution.</p><p>We say that this method captures the "zero-order relevance" since no context information from the reference collection is considered. Although this method is simple and intuitively easy to understand, the semantic information of the label is ignored and the information carried by the entire topic distribution is not fully utilized. A highly ranked label may happen to consist of many high probability words but have quite different meaning from the topic. A topic in computer science containing "tree" and "apple" may not be about "apple tree". We now propose another method based on deeper analysis of semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">The First-order Relevance</head><p>The semantics of a topic model should be interpreted in a context. For example, a topic about "rule", "association", "correlated", "frequency" is difficult to be labeled without a context of data mining. To "decode" the meaning of the topic conveyed by a multinomial distribution, a suitable context should be considered. In such a context, terms with higher probabilities in the distribution are more likely to appear when the topic θ is covered in a document. A natural context to interpret a topic is the original collection from which the topic model is extracted.</p><p>As discussed in Section 2, one challenge in topic labeling is the mismatch of the representation of a topic model and a label. Our idea is thus to represent a candidate label also with a multinomial distribution of words, which we can then use to compare with the topic model distribution to decide whether the label and the topic have the same meaning. Ideally, let us assume that there is also a multinomial distribution {p(w|l)} decided by label l. We can measure the closeness of {p(w|l)} and {p(w|θ)} using the KullbackLeibler(KL) divergence D(θ||l). Intuitively, this KL divergence can capture how good l is as a label for θ. If l is a perfect label for θ, these two distributions should perfectly match each other, thus the divergence would be zero. The basic idea of this method is illustrated in <ref type="figure">Figure 2</ref>. Figure 2: Illustration of first order relevance A larger circle means a higher probability.</p><p>Unfortunately, there is no clue about this unknown distribution {p(w|l)}. To use this relevance score, we thus would need to approximate {p(w|l)}. One way to approximate {p(w|l)} is to include a context collection C, and estimate a distribution {p(w|l, C)} to substitute {p(w|l)}. This approximation is reasonable: Consider the scenario when a person is unfamiliar with the meaning of a phrase, he/she would look at the context of the phrase first, and then decide whether the phrase is good to label a topic. For example, as in <ref type="figure">Figure 2</ref>, to label a database research topic, a reasonable context could be the SIGMOD conference proceedings. "Clustering algorithm" is a much better label for θ than "hash join" is, because the multinomial distribution estimated based on the context of "clustering algorithm" better matches the topic distribution θ than that based on the context of "hash join." We refer to the reference collection C as the context of topic model labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relevance Scoring Function</head><p>Formally, the relevance scoring function of label l w.r.t. topic model θ is defined as the negative KL divergence of {p(w|θ)} and {p(w|l)}. With the introduction of the context C, this scoring function can be rewritten as follows:</p><formula xml:id="formula_2">Score(l, θ) = −D(θ||l) = − w p(w|θ) log p(w|θ) p(w|l) = − w p(w|θ) log p(w|C) p(w|l, C) − w p(w|θ) log p(w|θ) p(w|C) − w p(w|θ) log p(w|l, C) p(w|l) = w p(w|θ) log p(w, l|C) p(w|C)p(l|C) − D(θ||C) − w p(w|θ) log p(w|l, C) p(w|l) = w p(w|θ)P MI(w, l|C) − D(θ||C) + Bias(l, C)</formula><p>From this rewriting, we see that the scoring function can be decomposed into three components. The second component is the KL divergence between the topic and the labeling context. Intuitively, if we use humanity literature as the context to label a data mining topic, the relevance score will be lower since it is not as trustworthy as computer science literature. However, this divergence is identical for all candidate labels, thus can be ignored in ranking labels. The third component can be viewed as a bias of using context C to infer the semantic relevance of l and θ. Consider the scenario that l is a good label for θ according to some prior knowledge, such as a domain ontology, but does not appear in C. In this case, C is biased to be used to infer the semantics of l w.r.t. θ. Practically, Bias(l, C) can be utilized to incorporate priors of candidate labels. When both the topic models and the candidate labels are generated from the collection C, we simply assume that there is no bias.</p><p>Interestingly, the first component can be written as the expectation of pointwise mutual information between l and the terms in the topic model given the context (E θ (P M I(w, l|C))). Without any prior knowledge on the label-context bias, we rank the candidate labels with E θ (P M I(w, l|C)), where P M I(w, l|C) can all be pre-computed independently of the topic models to be labeled.</p><p>Note that P M I(w, l|C) is undefined if p(w, l|C) = 0. One simple strategy is to ignore such w in the summation. A more reasonable way is to smooth p(w, l|C) with methods like Laplace smoothing.</p><p>This relevance function is called the first-order relevance of a label to a topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Intuitive Interpretation</head><p>Ranking candidate labels based on E θ (P M I(w, l|C)) is technically well motivated. However, is this a reasonable formalization in reality? What does this ranking function essentially capture? In this section, we give an intuitive interpretation of this semantic relevance scoring function. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we can construct a weighted graph, where each node is either a term in a topic model (weighted with {p(w|θ)}) or a candidate label. Each edge between a label and a topical term is then weighted with the pointwise mutual information P M I(w, l|C), which is often used to measure semantic associations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Thus the weight of each node indicates the importance of the term to this topic, while the weight of each edge indicates how strongly the label and the term are semantically associated.</p><p>The scoring function E θ (P M I(w, l|C)) would rank a label node higher, if it generally has stronger semantic relation (thicker edge) to those important topical words (larger circles). Intuitively, the labels selected in this way are meant to cover the entire topic model well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">High Coverage Labels</head><p>The third criterion of a good label is the high intra-topic coverage. We expect a label to cover as much semantic information of a topic as possible. Indeed, if we only extract one label for each topic, the semantic relevance function already guarantees that the the label covers maximum semantic information of θ. However, one label usually only partially covers a topic. When selecting multiple labels, we naturally expect the new labels to cover different aspects of the topic, not the information covered by the labels already selected.</p><p>Intuitively, in <ref type="figure" target="#fig_2">Figure 3</ref>, let us assume that "clustering algorithm" is already selected to label the upper topic. However, there are still important topical nodes (e.g., "dimensional", "reduce") weakly covered, or not covered by the label. We thus expect the second label to cover this missing information as much as possible, thus we prefer "dimension reduction", rather than labels like "clustering technique".</p><p>To implement this intuition, we propose to select labels with the Maximal Marginal Relevance (MMR) <ref type="bibr" target="#b4">[5]</ref> criterion. MMR is commonly used in information retrieval tasks, where both high relevance and low redundancy of retrieval results are desired.</p><p>Specifically, we select labels one by one, by maximizing the MMR criterion when selecting each label:</p><formula xml:id="formula_3">ˆ l = arg max l∈L−S [λScore(l, θ) − (1 − λ) max l ∈S Sim(l , l)]</formula><p>where S is the set of labels already selected,</p><formula xml:id="formula_4">Sim(l , l) = −D(l ||l) = − w p(w|l ) log p(w|l ) p(w|l)</formula><p>, and λ is a parameter to be empirically set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discriminative Labels</head><p>The previous criteria all only consider the labeling of a single topic. When a set of topics are presented, achieving inter-topic discrimination would be another criterion to consider. A label with high relevance scores to many topic models would not be very useful in this case even though it may be a good label for each individual topic. Intuitively, in <ref type="figure" target="#fig_2">Figure 3</ref>, "clustering algorithm" is a better label for the upper topic than "data management", since the former covers the important nodes well and exclusively.</p><p>In principle, we expect a good label to have high semantic relevance to the target topic model, and low relevance to other topic models. We thus propose the following modified scoring function:</p><formula xml:id="formula_5">Score (l, θi) = Score(l, θi) − µScore(l, θ 1,...,i−1,i+1,...,k )</formula><p>where θ 1,...,i−1,i+1,...,k (short as θ−i) is the semantics carried by all other topics than θi, and µ controls the discriminative power. Score(l, θ−i) can be modeled as</p><formula xml:id="formula_6">Score(l, θ −i ) = −D(θ −i ||l) rank = E θ −i (P M I(w, l|C)) ≈ 1 k − 1 j=1,..,i−1,i+1,..,k w p(w|θ j )(P M I(w, l|C)) = 1 k − 1 j=1..k E θ j (P M I(w, l|C)) − 1 k − 1 E θ i (P M I(w, l|C))</formula><p>which leads to</p><formula xml:id="formula_7">Score (l, θi) ≈ (1+ µ k − 1 )E θ i (P M I(w, l|C))− µ k − 1 j=1..k E θ j (P M I(w, l|C))</formula><p>We use Score (l, θ) to rank the labels, which achieves the needed discrimination across topic models.</p><p>With the methods proposed in this section, we are able to generate labels for multinomial topic models, which are understandable, semantically relevant, discriminative across topics, and of high coverage inside topics.</p><p>Although it is motivated to label multinomial topic models, the use of our approach is not limited to this. Variations in using the labeling approach could lead to different interesting applications. In the following section, we present two possible applications of topic model labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VARIATIONS OF TOPIC LABELING</head><p>In the previous section, we proposed the probabilistic framework and methods to label topic models, in which we assume that there is a multinomial representation for each topic model, and a context collection to generate candidate labels and measure the semantic relevance of a candidate label to a topic. In this section, we relax the assumption and introduce some variations of topic labeling, which can lead to many interesting text mining applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Labeling Document Clusters</head><p>The topic labeling framework, which is proposed to label topic models, essentially consists of a multinomial word distribution, a set of candidate labels, and a context collection. Thus it could be applied to any text mining problems, in which a multinomial distribution of word is involved.</p><p>In some tasks, such as topic modeling and many information retrieval tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, a multinomial word distribution is explicit. In other tasks, however, a multinomial word distribution may not be directly available; in such tasks, we can also apply our method by extracting a multinomial distribution. For example, given a group of documents G = {d1, ..., dm}, a multinomial word distribution can be easily constructed using the maximum likelihood estimation:</p><formula xml:id="formula_8">pG(w) = d∈G c(w, d) d ∈G w c(w , d )</formula><p>The proposed multinomial topic model labeling methods can be easily applied to generating labels for {pG(w)}. Such labels can thus be used to interpret the original group of documents. This is extremely valuable as many text management tasks involve a group/groups of documents, whose latent semantics is difficult to present. For example, document clustering partitions a collection of documents into groups, where a good label for each group may help the user understand why these documents are grouped together.</p><p>Labeling a cluster of documents is also valuable for many other tasks, such as search result summarization and modelbased feedback <ref type="bibr" target="#b26">[27]</ref>. In fact, the topic labeling method can be applied to any mining problems where a multinomial distribution of words can be estimated, such as term clustering, annotation of frequent patterns in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context Sensitive Labeling</head><p>Another possible variation is to switch the context collection, i.e., label a topic model extracted from one collection with another collection as the context. Although we normally would like to label a topic using the collection from which the topic is extracted as the context, it may be interesting sometimes to label/interpret a topic model in different contexts. Such cross-context interpretation can help us understand the variations of a topic and the connections between different contexts. For example, interpreting a topic discovered from one research area (e.g., database) in the context of another related research area (e.g., information retrieval) may reveal interesting connections between the two areas (e.g., an interdisciplinary research theme). Since our method can work on any context, we can easily use it to achieve such cross-context interpretation of topics, and contextual text mining in general. Indeed, a major task of contextual text mining is to extract topics and compare their variations in different contexts (e.g., time <ref type="bibr" target="#b16">[17]</ref>, location <ref type="bibr" target="#b15">[16]</ref>, authorship <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref>, etc). In the existing work, this is done by designing a specific statistical topic model with contextual structure, and fitting the data directly with the model. Topic labeling provides an alternative way to track the context-sensitive semantics of a general topic. By using different context collections, the semantics of candidate labels and topic models are biased towards the context. The labeling algorithm thus can generate different labels for the same topic, from the view in different contexts. Such technique can be applied to many contextual text mining tasks, such as temporal, spatiotemporal text mining, and author-topic analysis.</p><p>In Section 5, we show that variations of the general topic labeling framework are effective for different mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS AND RESULTS</head><p>In this section, we present the results of our evaluation of the effectiveness of the proposed methods for automatically labeling multinomial topic models using two data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Data Sets: We explore two different genres of document collections: the SIGMOD conference proceedings, and the Associated Press (AP) news dataset. To construct the first dataset, we downloaded 1848 abstracts of SIGMOD proceedings between the year 1975 and 2006, from the ACM digital library 2 . The second data collection contains a set of 2246 AP news articles, downloaded from http://www.cs.princeton .edu/∼blei/lda-c/ap.tgz. We built an index for each collection and implemented the topic labeling methods proposed in Section 3 with the Lemur toolkit 3 . Candidate Labels: We generate two sets of candidate labels with different methods: (1) extract noun phrases chunked by an NLP Chunker 4 ; (2) extract most significant 2-grams using the N-gram Statistics Package <ref type="bibr" target="#b0">[1]</ref>. We use the T-Test to test the significance of 2-grams, and extract those with the highest T-Scores <ref type="bibr" target="#b14">[15]</ref>. More specifically, we extract the top 1000 candidate 2-grams ranked by T-Score and top 1000 chunked noun phrases ranked by their frequencies. The ngrams with the highest T-Scores and the most frequent noun phrases are presented in <ref type="table" target="#tab_3">Table 2</ref>  Topic Models: From each dataset, we extract a number of topics using two representative statistical topic models, the PLSA <ref type="bibr" target="#b10">[11]</ref> and LDA <ref type="bibr" target="#b3">[4]</ref>. A background component model is added into PLSA to absorb the non-informative words, as suggested in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b16">[17]</ref>; this will make the topic models more distinguishable and readable. We do not use such a background model, or prune stopwords for LDA, in order to test the robustness of our topic labeling methods. We extracted 30 and 50 major topics from the SIGMOD and AP dataset, respectively. A subset of example topics is shown in <ref type="table">Table 3</ref>, where we list the words of the highest probabilities for each topic in the bottom row. We can see that for some topics, especially those from news articles (AP), it is hard to tell the latent meaning merely from the top words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness of Topic Labeling</head><p>We first show some sample results of our topic labeling method in <ref type="table">Table 3</ref>; for comparison, we also show the humangenerated labels for the same topics. It is clear that the automatically generated labels can all capture the meaning of the topic to some extent; indeed, most of them are as good as human generated labels (e.g., "clustering algorithm" and "data streams"), though some are not (e.g., "air force"). Some topics are difficult to interpret even by human (e.g., "death sentence").</p><p>To quantitatively evaluate the effectiveness of the automatic labeling methods, we ask three human assessors to compare the results generated by different methods. Specifically, for each of the most salient topics generated with PLSA (12 topics from SIGMOD and 18 topics from AP), we present to the annotators the labels generated by different methods in a random order, together with the word  <ref type="table">Table 3</ref>: Sample topics and system-generated labels</p><p>The second row contains the automatically generated labels. The third row presents the manually generated labels. The fourth row shows the words of highest probabilities in the topic distribution.</p><p>distribution and the most relevant documents to this topic to help a human assessor interpret the topic. A baseline method is included in comparison, which simply uses the top k terms in the word distribution as the topic labels. The other methods included in the comparison are shown in   Given the labels generated by n (n = 2, 3...) systems, we ask the assessors to rank the systems according to the quality of the labels they generated. For each topic, they will assign a score of n − k to a system if it is ranked at the k'th place. If the labels from several systems are difficult to be distinguished/ranked, we first give them an arbitrary ranking, and then equal their scores. For example, if there are three systems, one is significantly better, and the other two are hard to tell, we will assign score 2 to the first system, and 0.5 to each of the rest two systems. We then average the scores of each system over all topics.  A higher score means that the system tends to be ranked higher.</p><p>Basic results: In <ref type="table" target="#tab_8">Table 5</ref>, we compare the labels generated using the baseline method (i.e., picking high probability words), 0-order relevance (ngrams, normalized with background probability p(w|B)), and 1st-order relevance. For each group of systems, we compare both the top 1 label, and the top 5 labels they generate. From this table, we can make several observations: (1) In all cases, the labels extracted with 1st-order relevance are most preferred by the assessors, indicating that the first-order relevance method is overall the best presumably due to the fact that it can capture the overall topic distribution through context. <ref type="formula">(2)</ref> The preference of first-order relevance labels over the baseline labels is more significant on SIGMOD than on AP. This is likely because phrases are more frequently used and more discriminative in scientific literature than in the news domain. For example, informative phrases such as "mining association rules" are quite common in database literature, whereas common phrases in news articles tend to be general terms such as "united states" and "last year." This suggests that phrases are generally good labels for scientific topics, but for other genres of text, it may be interesting to explore other candidate labels, such as short sentences. <ref type="formula">(3)</ref> The preference of the first-order relevance labels over the baseline labels is stronger when five labels are considered than when one label is considered. This may be because the preference is amplified when more labels are considered.</p><p>The labels of 0-order relevance seem to be comparable with those of the baseline except in one case (i.e., 5 labels on SIGMOD) when the 0-order relevance labels are strongly preferred. This again suggests that phrases are not so useful for labeling topics in the news domain, but they are more useful for the literature domain. Overall these results show that the first-order relevance method for automatic labeling of topic models is the best among all the methods.  Ngrams vs. noun phrases as candidate labels: To see which of the two methods for generating candidate labels (i.e., ngrams and noun phrases) is better, we compare them on both data sets in <ref type="table" target="#tab_10">Table 6</ref>. Interestingly, for the SIGMOD dataset, using statistically significant ngrams as labels is much better than using noun phrases generated by the NLP Chunker, while on the AP dataset the performance of the two types of candidate labels is closer, and in some cases the noun phrases perform even better than ngrams. This may be because the models used by the NLP Chunker are trained on general domains, and not tuned for parsing scientific literature. In general, using significant ngrams appears to be more robust; moreover, this method can also be applied to any genre of texts. Normalization in 0-order relevance: We now look into the influence of the normalization strategy on the performance of 0-order relevance. In <ref type="table" target="#tab_12">Table 7</ref>, we compare 1st-order relevance with 0-order relevance when using two different normalization strategies -uniform normalization (normalization with uniform distribution) and background normalization (normalization with a background distribution). We see that background normalization, although intuitively appealing, does not really help here. The using of background normalization fails to decrease the difference between the 0-order relevance to the better method, the 1-order relevance. Indeed, it even makes the 0-order labels worse when applied on AP. The reason is because the topic models extracted with PLSA are already discriminative due to the use of a background component model (see Section 5.1), thus further normalization with background is not useful, and uniform normalization is actually more robust in this case.  To see if background normalization is useful when the extracted topic models are not discriminative (i.e., high probability words are non-informative words), we apply the topic labeling techniques to the topic models extracted with LDA, where neither is a background model included, nor are the stopwords pruned. The results are selectively presented in <ref type="table" target="#tab_18">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization</head><p>In <ref type="table" target="#tab_18">Table 8</ref>, we show three sample topics extracted with LDA and their corresponding labels generated using 1st-order relevance and 0-order relevance with different normalization methods. Without pruning stopwords or using a background model, we end up having many non-informative words on the top of each topic model; to better illustrate the meaning of each topic, at the bottom part of the topic word distribution, we also present some more discriminative terms. We see that the 1st-order relevance still generates good discriminative labels even though the high probability words of the original topic model are all non-informative words. This is because the 1st-order relevance captures the entire context of the topic model. In contrast, with uniform normalization, the 0-order relevance would be biased to assign high scores to non-informative phrases such as "real data" and "their data" when the top probability terms of the topic model are non-informative (e.g., "the", "their") or too general (e.g., "data", "large"). With normalization by background model p(w), we can penalize a phrase with noninformative or general words, thus alleviate this problem. However, in this way, the top ranked labels tend to be too specific to cover the general meaning of the topic (e.g., "integrity constraints", "transitive closure", etc). Thus overall we see that modeling the semantic relevance with first-order relevance is most robust because the semantics is inferred based on the context of the entire distribution. Upper bound analysis: How much room is there to further improve the topic labeling method? We can answer this question by looking into how much worse the automatically generated labels are than those generated manually. Thus we ask a human annotator to generate topic labels manually, and ask two different assessors to compare the system-generated labels with the human-generated labels. In <ref type="table" target="#tab_14">Table 9</ref>, we see that although the system-generated labels are good, the assessors still consider human-generated labels to be better. This implies that there is still much room to improve the quality of the automatically generated topic labels. Interestingly, the difference between system generated and human generated labels is less significant on the SIGMOD data than on the AP data, suggesting that literature topics may be easier to label than news topics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Labeling Document Clusters</head><p>In Section 4.1, we discussed that the topic labeling method could actually be applied to any text information management tasks where a multinomial word distribution is involved. Here we look into one such application -labeling document clusters. In this experiment, we cluster the SIG-MOD abstracts with the K-Medoids algorithm <ref type="bibr" target="#b12">[13]</ref>, and try to utilize the topic labeling method to label the clusters. Specifically, we estimate a multinomial word distribution for each cluster based on its member documents using the maximum likelihood estimator. The proposed topic labeling technique can then be applied on the estimated term distributions, and the top ranked phrases are used to label the original cluster.  The generated cluster labels, along with the number of documents and the title of the medoid document of each cluster are shown in <ref type="table" target="#tab_1">Table 10</ref>. By comparing the cluster labels with the medoid documents, we see that the topic labeling technique is also effective to label document clusters. This experiment shows that the use of topic labeling is not limited to statistical topic modeling; it is potentially applicable to any tasks in which such a multinomial term distribution is involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Context-Sensitive Labeling</head><p>In this section, we evaluate the effectiveness of our topic labeling method for cross-context labeling/interpretation of topic models. We extract 30 topics from SIGMOD proceedings, but use the phrases extracted from SIGIR abstracts, and KDD abstracts to label the topics. We simulate the scenario in which the system does not know where the topics are extracted, thus it would simply use any context collection "familiar" to the system (i.e., SIGIR or KDD collections in our experiments). The results are presented in <ref type="table" target="#tab_1">Table 11</ref>.</p><p>The results are interesting. The labels generated from different contexts generally capture the biased meaning of   the topic from the view of that context. For example, the database topic about R-tree and other index structures assigns high probability to words like "tree" and "trees"; the results show that, when interpreted in the data mining context, these high probability words may cause the topic to be labeled with "decision trees" and "tree algorithms", suggesting a different, but related interpretation of "tree." Also, our results suggest that when seeing a topic word distribution with high probability words such as "sampling" and "estimation", database researchers may interpret it as "selectivity estimation" or "approximate answers", while information retrieval researchers interpret it as about "parameter estimation" of "mixture models", which is more relevant to their background. This experiment shows that the topic labeling technique can be exploited to infer context-sensitive semantics of a topic model through labeling a general topic with different contexts. This provides an alternative way to solve a major task in contextual text mining: extracting general topics and analyzing the variation of their meanings over contexts.</p><p>Note that this effect can be only achieved when the firstorder semantic relevance is used, since the zero-order relevance is independent of a context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>To the best of our knowledge, no existing work has formally studied the problem of automatic labeling of multinomial topic models. There has been a large body of work on statistical topic models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, most of which uses a multinomial word distribution to represent a topic. In some recent work, <ref type="bibr" target="#b22">[23]</ref> generalized the representation of a topic model as a multinomial distribution over ngrams. Such topics are labeled with either top words in the distribution or manually selected phrases. The method we proposed can automatically generate meaningful phrase labels for multinomial topic models and can be applied as a post-processing step for all such topic models, to interpret the semantics of these topics models extracted from text data.</p><p>As we use phrases as candidate labels, our work is related to phrase extraction, including shallow parsing/chunking in natural language processing (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>), and N-gram phrase extraction with statistical approaches (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6]</ref>). A better phrase extraction method could benefit topic labeling as a better preprocessing procedure.</p><p>Text summarization aims at extracting/generating sentence summaries for one/multiple documents (e.g., <ref type="bibr" target="#b20">[21]</ref>). The summary can be as short as titles <ref type="bibr" target="#b11">[12]</ref>. However, no existing work has been done for summarizing a multinomial distribution of words, or a statistical topic model. Since most topic models assume that a document covers multiple topics, it is also difficult to cast topic model labeling as summarizing documents. The topic labeling approach, on the other hand, provides a novel method to label a set of documents.</p><p>A major task in contextual text mining is to extract topics and compare their content variations over different contexts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Our proposed topic labeling approach provides an alternative way to infer the context-sensitive semantics of topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>Statistical topic modeling has been well studied recently, with applications to many machine learning and text mining tasks. Despite its high impact, however, there is no existing method which could automatically generate interpretable labels capturing the semantics of a multinomial topic model. Without understandable labels, the use of topic models in real world applications is seriously limited. In this paper, we formally study the problem of automatic labeling of multinomial topic models, and propose probabilistic approaches to label multinomial word distributions with meaningful phrases. We cast the labeling problem as an optimization problem involving minimizing KullbackLeibler divergence between word distributions and maximizing mutual information between a label and a topic model. Empirical experiments show that the proposed approach is effective and robust when applied on different genres of text collections to label topics generated using various statistical topic models (e.g., PLSA and LDA). The proposed topic labeling methods can be applied as a post-processing step to label any multinomial distributions in any text context. With reasonable variations, this approach can be applied to any text mining tasks where a multinomial term distribution can be estimated. This includes labeling a cluster of documents and inferring the variation of semantics of a topic over different contexts.</p><p>There are many possible extensions to this work. First, there is room to further improve the quality of topic labels, including a potentially better way to select candidate labels. Second, how to incorporate prior knowledge, such as a domain ontology, is also an interesting research direction. Third, it would be interesting to study how to generate labels for hierarchical topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472. We thank Jing Jiang for her help on evaluating the labeling effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of zero-order relevance A larger circle means a higher probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interpretation of label selection A label having high PMI with many high probability topic words would be favored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Variant possible labels for a topic model</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>SIGMOD 
AP 
2-gram 
noun phrase 
2-gram 
noun phrase 
database systems 
this paper 
he said 
the united states 
database system 
the problem 
more than 
the government 
object oriented 
a set 
united states 
last year 
query processing 
the data 
new york 
the country 
data base 
the database 
last year 
the nation 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Sample candidate labels</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>System 
Cand. Labels 
Relevance Score 
NGram-1 
Ngrams 
First-order 
NGram-0-U 
Ngrams 
0-order, uniform normaliz. 
NGram-0-B 
Ngrams 
0-order, norm. with p(w|B) . 
Chunk-1 
NP Chunks 
First-order 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Systems Compared in Human Evaluation</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Effectiveness of topic labeling</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 : Ngrams vs. noun phrases as labels</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Uniform vs. background normailzation for 
0-order relevance (5 labels) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Comparison with human generated labels 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 10 : Labeling document clusters: K-Medoids</head><label>10</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Labeling LDA topics: 1st-order relevance is most robust 

Topic 
Labels 
Topic 
Labels 
tree 
SIGMOD Labels 
views 
SIGMOD Labels 
trees 
r tree, b trees, 
view 
materialized views, 
spatial 
index structures 
materialized 
view maintenance, data warehouses 
r 
KDD Labels 
maintenance KDD Labels 
b 
tree algorithm, decision trees 
warehouse 
decision support 
disk 
tree construction 
tables 
business intelligence 
dependencies SIGMOD Labels 
sampling 
SIGMOD Labels 
functional 
multivalued dependencies, 
estimation 
selectivity estimation 
cube 
functional dependencies, iceberg cube 
approximate random sampling, approximate answers 
multivalued 
SIGIR labels 
histograms 
SIGIR Labels: 
iceberg 
term dependency 
selectivity 
distributed retrieval, 
buc 
independence assumption 
histogram 
parameter estimation, mixture models 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head>Table 11 : Labeling database topics with different contexts</head><label>11</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> www.cs.cmu.edu/∼lemur/science/topics.html, Topic 26</note>

			<note place="foot" n="2"> http://www.acm.org/dl 3 http://www.lemurproject.org/ 4 http://opennlp.sourceforge.net/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The design, implementation, and use of the ngram statistics package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="370" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS &apos;05: Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR &apos;98</title>
		<meeting>SIGIR &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diverse topic phrase extraction through latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM &apos;06</title>
		<meeting>ICDM &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="834" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language Modeling and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl.1</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Introduction to special issue on machine learning approaches to shallow parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hammerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="551" to="558" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;99</title>
		<meeting>ACM SIGIR&apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new probabilistic model for title generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding groups in data. an introduction to cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><forename type="middle">;</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Series in Probability and Mathematical Statistics. Applied Probability and Statistics</title>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pachinko allocation: Dag-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;06: Proceedings of the 23rd international conference on Machine learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic approach to spatiotemporal theme pattern mining on weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW &apos;06</title>
		<meeting>WWW &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering evolutionary theme patterns from text: an exploration of temporal text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of KDD&apos;05</title>
		<meeting>eeding of KDD&apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="198" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A mixture model for contextual text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD &apos;06</title>
		<meeting>KDD &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="649" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical entity-topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="680" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD &apos;02</title>
		<meeting>KDD &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="408" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic author-topic models for information discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD&apos;04</title>
		<meeting>KDD&apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A note on topical n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2005-071</idno>
	</analytic>
	<monogr>
		<title level="m">University of Massachusetts</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topics over time: a non-markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD &apos;06</title>
		<meeting>KDD &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR &apos;06</title>
		<meeting>SIGIR &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast statistical parsing of noun phrases for document indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth conference on Applied natural language processing</title>
		<meeting>the fifth conference on Applied natural language processing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="312" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM &apos;01</title>
		<meeting>CIKM &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A cross-collection mixture model for comparative text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Velivelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD&apos;04</title>
		<meeting>KDD&apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="743" to="748" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
