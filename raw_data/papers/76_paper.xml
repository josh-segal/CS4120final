<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Accuracy of Extracting Information from Unstructured Text Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
							<email>eugeneag@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
							<email>silviu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Accuracy of Extracting Information from Unstructured Text Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H31 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval General Terms Algorithms</term>
					<term>Experimentation Keywords Language modeling</term>
					<term>information extraction</term>
					<term>named entity extraction</term>
					<term>relation extraction</term>
					<term>context language modeling</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Exploiting lexical and semantic relationships in large unstructured text collections can significantly enhance managing, integrating, and querying information locked in unstructured text. Most notably, named entities and relations between entities are crucial for effective question answering and other information retrieval and knowledge management tasks. Unfortunately, the success in extracting these relationships can vary for different domains, languages, and document collections. Predicting extraction performance is an important step towards scalable and intelligent knowledge management, information retrieval and information integration. We present a general language modeling method for quantifying the difficulty of information extraction tasks. We demonstrate the viability of our approach by predicting performance of real world information extraction tasks, Named Entity recognition and Relation Extraction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">OVERVIEW</head><p>The vast amount of information that exists in unstructured text collections is still primarily accessible via keyword querying at the document level. Unfortunately, this method of information access largely ignores the underlying lexical and semantic relationships between terms and entities in the text. These relationships can be extremely valuable for answering questions, browsing the documents, and managing information associated with the entities of interest. Additionally, document retrieval relevance could be improved if we detect meaningful terms (e.g., named entities such as dates, persons, organizations, and locations); and related entities (e.g., pairs of entities such as "person's birth date" and "person who invented a device") could be used directly to answer questions. Furthermore, indexing entities and relationships would support more intelligent document browsing and navigation, and would allow for richer interactions with the document collections Hence, being able to reliably extract such relationships from text may be of vital importance to knowledge management and information retrieval. However, real collections can exhibit properties that make them difficult for information extraction. At the same time, tuning an information extraction system for a given collection, or porting an information extraction system to a new language, can require significant human and computational effort. Hence, predicting if an extraction task will be successful (i.e., the required information can be extracted with high accuracy) is extremely important for adapting, deploying, and maintaining information extraction systems, and, ultimately, for managing and retrieving information in large text collections.</p><p>We observe that document collection properties, such as typical text contexts surrounding the entities or relation tuples, can affect difficulty of an extraction task. In this paper, we present a first general approach to use context language models for predicting whether an extraction task will succeed for a given document collection.</p><p>More specifically, we consider two crucial information extraction tasks: Named Entity Recognition, and Relation Extraction.</p><p>• Named Entity Recognition (NER) is a task of identifying entities such as "Person", "Organization", and "Location" in text. The ability to identify such entities has been established as an important pre-processing task in several areas including information extraction, machine translation, information retrieval, and question answering. NER often serves as an important step in the Relation Extraction task described next.</p><p>• Relation Extraction (RE), is a task of identifying semantic relationships between entities in the text, such as "person's birth date", which relates a person name in the text to a date that is the person's birth date. Once the tuples for this relation (e.g., &lt;"Albert Einstein", "14 March 1879"&gt;) are identified, they can be used to directly answer questions such as "When was Albert Einstein born?"</p><p>Most state of the art NER and RE systems rely on local context to identify entities or determine the relationship between target entities. In NER, contextual patterns such as "Mr." or "mayor of" are often used for hypothesizing occurrences of entities and classifying such identified entities, especially when they are polysemous or of a foreign origin. The local context is also important for the RE task. Intuitively, if the context surrounding the entities of interest for a given relation looks similar to the general text of the documents (i.e., there are no consistent and obvious "clues" that the entities or relationships of interest are present), then the RE task for that relation will be hard. While NER systems can resort to dictionary lookups in some cases (e.g., for the "Location" entities, dictionaries can be particularly helpful), for others (e.g., people's names or organizations) high accuracy may not be possible. In contrast, if the text context around entities in the collection tends to contain telltale clues, such as "Mr." preceding a person name, the extraction task is expected to be easier, and higher accuracy achievable.</p><p>Our approach formalizes and exploits this observation by building two language models for the collection -a task-specific context language model for the extraction task, and the overall background model for the collection. We can then compare the two language models and compute the divergence of the taskspecific language model from that of the overall collection model. If the divergence is high (i.e., the task-specific language model is different from the overall model), the extraction task is expected to be easier than if the divergence is low (i.e., the taskspecific language model is very similar to the document language model).</p><p>Interestingly, our approach can be potentially helpful for other applications, including better term weighting for information retrieval, and supporting active learning for interactive information extraction. For example, we could derive improved term weighs for domain-specific retrieval tasks such as birthday finding by incorporating context model weights. We will discuss other promising future directions of this work in Section 5.</p><p>The rest of this paper is organized as follows. In the next section we review related work. In Section 3 we present our formal model and the algorithms for building the language models. In Section 4 we present experimental results for the NER and RE tasks over large document collections. In Section 5 we present our conclusions, and discuss potential future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work explores language modeling for information extraction and thus touches on areas of information retrieval, information extraction, and language modeling.</p><p>Our approach is partly inspired by the work of CronenTownsend, Zhou, and Croft [9] on predicting query performance by measuring the similarity of a language model LMQ derived from the retrieved documents for a query and a language model for the whole target collection of documents LMColl. Using simple unigram language models, they showed that the relative entropy between the query and collection language models correlates with the average precision in several TREC test collections. In this paper, we apply similar language modeling techniques to the task of predicting information extraction performance.</p><p>Language modeling, typically expressed as the problem of predicting the occurrence of a word in text or speech, has been an active area of research in speech recognition, optical character recognition, context-sensitive spelling, and machine translation. An in-depth analysis of this problem in natural language processing is presented in <ref type="bibr" target="#b19">[20]</ref>, Chapter 6. Language modeling has also been used to improve term weighting in information retrieval (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> and others). However, in previous work LM was used as a tool for improving the specific system performance, whereas in our work we attempt to predict performance for general extraction tasks.</p><p>An important distinction of our work is that we consider taskspecific contexts. As our results indicate, using the locality in the overall document collection may not be sufficient, as local context models can become similar to the background model for overall document collection. Our approach is similar in spirit to the use of entity language models described in <ref type="bibr" target="#b22">[23]</ref> for classifying and retrieving named entities. Our work is complementary as we present a general approach for modeling the performance of extraction tasks including both named entity recognition and relation extraction.</p><p>For the named entity recognition task, numerous ways of exploiting local context were proposed, from relatively simple character-based models such as <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b18">[19]</ref> to complex models making use of various lexical, syntactic, morphological, orthographical information, such as <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b9">[10]</ref>. In this work, we show that we can predict the difficulty of identifying several types of named entities by using relatively simple context language models. This study can be viewed as complementary to Collins' work <ref type="bibr" target="#b7">[8]</ref> on the difficulty of identifying named entity boundaries, regardless of entity type.</p><p>Relation extraction systems rely on variety of features (e.g., syntactic, semantic, lexical, co-occurrence), but all depend heavily on context. Once the entities are identified, it is the textual context that expresses the relationship between the entities. Partially supervised relation extraction systems (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and others) rely on the text contexts of example facts to derive extraction patterns.</p><p>For relation extraction, the task difficulty was previously analyzed by considering the complexity of the target extraction templates ( <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b16">[17]</ref>). Another promising approach described in <ref type="bibr" target="#b12">[13]</ref> modeled the task domain variability by considering the different paraphrases used to express the same information in the text. In contrast, our work quantifies the difference between the contexts around the entities and unrelated text contexts. If the contexts of the example facts are similar to the background text, an extraction system is expected to have more difficulty deriving extraction patterns and recognizing the relevant entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODELING EXTRACTION DIFFICULTY</head><p>In this section we describe the general approach we take for modeling the difficulty of an extraction task, and hence the expected performance of an extraction system on the task (Section 3.1). Then, in Section 3.2, we describe the algorithms for computing the language models to make our predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>As we discussed, the textual context (i.e., the local properties of the text surrounding the entities and relations of interest) can be of crucial importance to extraction accuracy. Intuitively, if the contexts in which the entities occur are similar to the general text then extraction is expected to be difficult. Otherwise, if there are strong contextual clues, the extraction should be easier and we should expect higher extraction accuracy.</p><p>To quantify the notion of context, we use a basic unigram language model, which is essentially a probability distribution over the words in the text's vocabulary. In this study, we derive this probability distribution from the histogram of words occurring in the local context of target entities by using maximum likelihood estimation. Our purpose is to compare the language model associated with an entity type or relationship LMC with a background language model for the whole target text, denoted by LMBG. Therefore, no smoothing of these models is necessary. Intuitively, if the background language model for the collection is very similar to the language model constructed from the context of the valid entities then the task is expected to be hard. Otherwise (if LMC is very different from LMBG), the task is expected to be easier.</p><p>A common way to measure the difference between two probability distributions is relative entropy, also known as the Kullback-Leibler divergence:</p><formula xml:id="formula_0">񮽙 ∈ ⋅ = V w BG C i C BG C w LM w LM w LM LM LM ) ( ) ( log ) ( ) || ( KL</formula><p>In Information Theory, KL-divergence represents the average number of bits wasted by encoding messages drawn from the distribution LMC using as model the distribution LMBG.</p><p>Alternatively, we can measure how different two models are by using cosine similarity, which represents the cosine of the angle between the two language models seen as vectors in a multidimensional space in which each dimension corresponds to one word in the vocabulary:</p><formula xml:id="formula_1">|| || || || ) , ( Cosine C BG BG C BG C LM LM LM LM LM LM ⋅ &gt; ⋅ &lt; =</formula><p>The closer the cosine is to 1, the smaller the angle and thus, the more similar the two models. Hence, to measure the difference of the two models LMC and LMBG we define CDist as:</p><formula xml:id="formula_2">CDist(LMC || LMBG) = 1 -Cosine(LMC, LMBG)</formula><p>to maintain symmetry with the KL metric, with larger values indicating larger difference between models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing the Language Models</head><p>We now describe how to construct a language model for a given extraction task. For clarity, we describe a unigram language model, but our methodology can be extended to higher-order features. For syntax-based extraction systems, we could parse the text and incorporate that information into the model as in <ref type="bibr" target="#b5">[6]</ref>. However, as we will show experimentally, a simple unigram model is sufficient to make useful predictions.</p><p>To construct the task-specific context language model LMC we search the collection for occurrences of valid entities (or relation tuples). While for the NER and RE tasks LMC is constructed slightly differently (as described below), the overall approach is to consider the text context to be the K words surrounding the entities in question.</p><p>More specifically, the language model for NER is constructed as outlined in <ref type="figure">Figure 3</ref>.1. We scan the document collection D, searching for occurrences of each known entity Ei. When an entity is detected, we add to LMC up to K terms to the right and to the left of the entity.</p><p>The algorithm for constructing a task-specific language model for RE is outlined in <ref type="figure">Figure 3</ref>.2. The procedure is similar to the NER algorithm above. We scan the document collection D, searching for occurrences of each known example tuple Ti for the target relation. For this, we search for all attributes of Ti in the text. If all entities are present, and occur within K words of each other, we increment the LMC counts of all the words between the leftmost and the rightmost entities. If the entities in a relation tuple are close together (i.e., there are fewer than K words separating the entities in the text), we include all the terms separating the entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3.1: NER Context language model construction.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3.2: RE Context language model construction.</head><p>Unfortunately, we don't have all the valid entities available (i.e., when predicting whether a task will succeed without going through the complete extraction process). Hence, our model is build based on sampling the collection using a small (20-40) sample of the known entities or tuples by providing only these example entities as input to the NER and RE language model construction algorithms above. For a large corpus, the samplebased model is expected to be a reasonable approximation of the complete task specific language model. The background language model, LMBG is derived through maximum likelihood estimation using the word frequencies in each document collection. When we discard stopwords from LMC we also discard them from LMBG. </p><formula xml:id="formula_3">For each term w in d [start +1],…, d [end -1] Increment count of w in LMC Normalize LMC return LMC ConstructNERLanguageModel (Entities E, Documents D, K ) For each document d in D For each entity Ei in E if Ei is present in d For each instance of Ei spanning from start to end For each term w in d [start -K], …, d [start-1] Increment count of w in LMC For each term w in d [end + 1], …, d [end + K] Increment count of w in LMC Normalize LMC return LMC</formula><p>In order to interpret the divergence of a task specific language model LMC from the background language model, we build a reference context language model LMR (also denoted as RANDOM). We construct LMR, by taking random samples of words in the vocabulary (excluding stopwords) of the same size as the entity samples. We then use these words input to Algorithm 3.1. Using LMR we can then compute the "reference" divergence of a context language model from the background model for a given sample size. For large sample sizes, LMR is expected to approximate the background model. Indeed, <ref type="figure">Figure  3</ref>.1 reports that for larger random word sample sizes, LMR becomes more similar to the background model, and the divergence steadily decreases.  We also use LMR to normalize our divergence measures to be robust to different entity sample sizes and collection sizes. For this, we compute the normalized divergence as the ratio of the KL-divergence value of LMC, and the KL-divergence value of LMR, as compared to the background distribution LMBG. We can similarly compute normalized cosine distance as the ratio of the CDIST values of LMC and LMR compared to LMBG.</p><p>Constructing the context models LMC and LMR can be done efficiently by using any off-the-shelf search engine and considering only the documents retrieved by search for the example entities or tuples, and run Algorithms 3.1 and 3.2 only over these reduced document sets. Having described constructing the language models for extraction tasks, we now turn to experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluated our prediction for two real-world tasks: Named Entity Recognition (NER) and Relation Extraction (RE). We first describe the experimental setup (Section 4.1), including the datasets, entity and relation types, and parameter settings we considered. Then we describe our experiments for predicting NER difficulty (Section 4.2), followed by our experiments on predicting RE difficulty (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>In order to design a realistic evaluation we focused on two extraction tasks, NER and RE, over large document collections.</p><p>The overall goal of the experiments is to determine if the language models, constructed from a realistically small sample of the extractions of interest, can make useful predictions about the observed accuracy of the extraction task for that collection.   To validate our extraction performance predictions for the NER task, we used as reference the top performing systems in the CoNLL shared task competition, which were evaluated over a manually annotated subset of news articles from the same RCV1 corpus as described above. Moreover, we built the samples of named entities by randomly sampling the set of named entities present in the training set provided by the CoNLL competition organizers (described in <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b26">[27]</ref>). To validate our performance predictions on the RE task, we used a bootstrapping-based extraction system similar to Snowball <ref type="bibr" target="#b0">[1]</ref>, which is heavily dependent on the example entities and the text context in which they appear to derive extraction patterns. For comparison, we also report RANDOM, the divergence of the random keyword sample-based language model, LMR.</p><p>In our experiments we explored the following parameters:</p><p>• Context size: number of words to the left and to the right of entity to include as context.</p><p>• Maximum distance separating the entities (for RE task) • Divergence metric, CDist or KL: The language model divergence metrics defined in Section 3.1. We found that CDIST is strongly correlated with KL, and does not provide additional information. Therefore, for clarity and brevity, we report only the KL values for our experimental evaluation.</p><p>• Example set size S: number of randomly drawn entities (or relation tuples).</p><p>• Random sample size R: number of randomly drawn terms to estimate the background model. For all experiments, R was equal to the value of S above for each task.</p><p>• Stopwords: we analyze two cases, when stopwords (common English words such as prepositions, conjunctions, numerals, etc.) are included in the language model, and when they are excluded. In both cases, we discard punctuation.</p><p>• N-gram size N: We considered word unigrams, bigrams and trigrams as features of the language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Predicting NER Difficulty</head><p>In order to explore the parameter space and evaluate the accuracy of our predictions, we use as reference the reported performance of the top five systems in the CoNLL 2003 shared task competition <ref type="bibr" target="#b26">[27]</ref>, which is summarized in <ref type="table" target="#tab_3">Table 4</ref>.3. According to the reported numbers, the Person (PER) and Location (LOC) entities are the "easiest" to extract, whereas the Miscellaneous (MISC) and Organization (ORG) are relatively difficult.      <ref type="figure">1/10, discarding stopwords)</ref>. <ref type="table" target="#tab_3">Tables 4.4a</ref> and 4.4b report the prediction results using stopwords in the language model <ref type="table" target="#tab_3">(Table 4</ref>.4a) and discarding the stopwords <ref type="table" target="#tab_3">(Table 4</ref>.4b). As expected, the context language models are more similar to the background model when stopwords are included, but in both cases the conclusions are the same. This is encouraging, as it shows that our approach may work even for languages where no lexical information (such as stopwords) is known a priori.</p><p>A drawback of our current approach is that we do not consider how easy it is to identify entities of a given type based on sources of information other than context, such as morphology, internal capitalization, or gazetteer lists. Consequently, our system may not be able to predict accurately the extraction performance of fully-featured systems for entities with various intrinsic properties that make them easier or harder to identify independent of context (e.g. the real performance for MISC is somewhat lower than expected based on our prediction due to the fact that capitalization and length varies to a much larger degree for MISC entities than for the other entity types).</p><p>We now consider the sensitivity of our results to sample size <ref type="figure" target="#fig_2">(Figure 4</ref>.1) and N-gram size <ref type="figure" target="#fig_2">(Figure 4.2)</ref>. <ref type="figure" target="#fig_2">Figure 4</ref>.1 reports normalized KL divergence for RCV 1/100 for seed sample sizes of 10, 20, 30, 40, and 50. As we can see, for sample sizes greater than 20 our predictions do not change. Hence, sample size of 20 seed entities will be used for our subsequent experiments.   We now show that our prediction technique also applies to languages other than English, by evaluating it for the named entity task on the Spanish and Dutch collections used in the CoNLL 2002 shared task evaluation <ref type="bibr" target="#b23">[24]</ref>. As we discussed, porting information extraction systems to new domains and new languages can require significant effort. For languages other than English, annotated data and other language specific resources are less readily available. Hence, developing systems for these languages is typically more difficult, and this is also shown by the lower performance of state-of-the-art NER systems on Spanish and Dutch <ref type="table" target="#tab_3">(Tables 4.6</ref>      This is confirmed by the actual results for Spanish. For Dutch, the best performing systems in CoNLL 2002 also performed much better for PER than for ORG and MISC, LOC being, similarly to English, an outlier. Our system predicts that LOC is a difficult entity type to extract based on context for all languages mainly because many of the relevant corresponding contexts in these languages are stopwords, which occur frequently throughout the text. However, real systems were able to identify the LOC entities because the percentage of actual entities seen both in the training and in the test is typically greater than for the other entity types. This aspect of the problem is not modeled by our approach, and can be addressed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Predicting RE Difficulty</head><p>We now turn to predicting performance of relation extraction tasks (RE). The goal is to predict which relations are "difficult" to extract, and which ones are "easy". <ref type="table" target="#tab_3">Table 4</ref>.10 reports the actual extraction accuracy on the RE task using a simple bootstrapping-based information extraction system similar to Snowball <ref type="bibr" target="#b0">[1]</ref> and KnowItAll <ref type="bibr" target="#b11">[12]</ref>. We report the precision on each task estimated by sampling 100 facts from the extracted relation instances. As we can see, the BORN and DIED relations are "easy" for the extraction system (exhibiting precision of as high as 97%), whereas INVENT and WROTE are relatively "hard" (exhibiting precision as low as 50%).    To further investigate the required effort needed for robust prediction on the RE task, in <ref type="figure" target="#fig_2">Figure 4</ref>.3 we report the predictions for varying the seed sample size from 10 to 40 relation tuples. As we can see, our predictions remain relatively stable for sample sizes of at least 20 seed tuples. Interestingly, adding additional seed tuples beyond 20 does not improve the overall prediction accuracy as our approach: while our method distinguishes between the "easy" and "hard" relations correctly, it is not able to further distinguish between the two "hard" relations, namely that the WROTE relation is more difficult to extract than the INVENT relation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>We presented a general, domain and language independent approach for predicting extraction performance. We have shown that our language modeling approach is effective for predicting extraction accuracy for tasks such as named entity recognition and relation extraction, both tasks crucial for high accuracy and domain-specific information retrieval and information management.</p><p>As our experiments indicate, starting with even a small sample of available entities can be sufficient for making a reasonable prediction about extraction accuracy. Our results are particularly encouraging as we consider a relatively simple model that does not require extra information to that typically available to modern NER and RE systems.</p><p>Extending our method to use more sophisticated language models can further improve our predictions. For languages where reliable NLP tools are available, one promising direction would be to incorporate syntactic features, and to apply techniques such as co-reference resolution to build richer and more accurate context language models. Additionally, incorporating gazetteer lists similar to those typically used by the NER systems can further improve prediction accuracy. Another interesting direction for future work is to correlate our predictions with the actual accuracy values for more fine-grained predictions.</p><p>Furthermore, our results could be applied for building interactive information extraction systems that could guide the user by requesting more examples for the extraction tasks predicted to be "difficult". Such an interactive system could more effectively focus valuable human effort on the "difficult" extraction tasks, where it is most sorely needed.</p><p>As we have shown, our approach is general and languageindependent. With amounts of new information available in text increasing daily, our techniques could be extremely valuable for developing, maintaining, and deploying information extraction technology for better information access.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 . 1 : The average KL-divergence between the context language models for random samples of words and the background language model.</head><label>31</label><figDesc>Figure 3.1: The average KL-divergence between the context language models for random samples of words and the background language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 . 1 : Normalized KL-divergence for context models for varying sample size (RCV 1</head><label>411</label><figDesc>Figure 4.1: Normalized KL-divergence for context models for varying sample size (RCV 1/100, context size 2, discarding stopwords).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.2 reports the normalized KL divergence for RCV 1/100 for language models created with N-grams of size 1, 2, and 3 words. Interestingly, the single-word (unigram) model appears to be as predictive as the two-word (bigram) and the three-word (trigram) models. In general, higher order N-gram models tend to be sparse, and hence may not be useful for our problem. Therefore, we will report results for the simpler one word models for the subsequent experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 2 : Normalized KL-divergence for context models for varying N-gram size (RCV 1/ 100 , discarding stopwords).</head><label>42100</label><figDesc>Figure 4.2: Normalized KL-divergence for context models for varying N-gram size (RCV 1/100, discarding stopwords).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 . 3 : Normalized KL divergence of context models for varying sample size averaged over 3 runs for each sample size (Encyclopedia, 2 -word context size, discarding stopwords).</head><label>432</label><figDesc>Figure 4.3: Normalized KL divergence of context models for varying sample size averaged over 3 runs for each sample size (Encyclopedia, 2-word context size, discarding stopwords).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>ConstructRELanguageModel (Tuples T, Documents D, K ) For each document d in D For each tuple Ti=(ti 1 ,ti 2 ) in T If ti 1 or ti 2 not present in d continue For each pair of adjacent instances of ti 1 ,ti 2 occurring at positions start and end and separated by fewer than K words</head><label>ConstructRELanguageModel</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .1: Document collections used in experiments.</head><label>4</label><figDesc></figDesc><table>The document collections used for these experiments are 
reported in Table 4.1. The Reuters RCV1 documents were drawn 
from the collection used in the CoNLL 2003 [17] NER shared 
task evaluation. The EFE (Spanish) and the De Morgen (Dutch) 
documents were the datasets used in the CoNLL 2002 NER 
shared task evaluation. Note that while the Spanish and the 
Dutch collections are small, they are a "standard" dataset for 
NER evaluation. For the RE experiments, we used Encarta, a 
large online encyclopedia document collection. 

For all experiments, we start with a small sample (10-40) of 
entities or relation tuples, drawn at random from a list of known 
valid entities or tuples. In Table 4.2 we report the specifics of 
the extraction tasks used for the experiments. 

Type of Task 
Sample Extractions (Description) 
Task 

Location names 
LOC 

Miscellaneous named entities 
MISC 

Organization names 
ORG 

NER 
(Named Entity 
Recognition) 
Person names 
PER 

Person's birth dates 
BORN 

Person's death dates 
DIED 

Person's inventions 
INVENT 

RE 
(Relation 
Extraction) 
Person's writings 
WROTE 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>2: Entities and relations used in experiments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 .gazetteers, which were likely to contain most locations that news articles may talk about and thus, covering most of the locations in the test. Context Size 1 Sample 1 Sample 2 Sample 3 Average Normalized</head><label>4</label><figDesc></figDesc><table>3: F-measures on the Reuters RCV1 collection 
reported by the top 5 systems participating in the 
CoNLL 2003 Shared Task competition. 

We report the results of our system on predicting NER difficulty 
in Tables 4.4a, 4.4b, and 4.5. The first two tables present the 
results obtained on a smaller subset of the Reuters RCV1 corpus, 
of 3.5 million words, while the latter shows the results obtained 
for a 10 times bigger subset of the same corpus (35 million 
words). It is remarkable that the language models estimated on 
the smaller corpus make extremely similar predictions to those 
estimated on a corpus 10 times larger. 

Our ranking identifies ORG and PER entities as "easy" to extract 
entity types and LOC and MISC as hard to extract. These 
correlate with the results reported by the participants in the 
CoNLL 2003 Shared Task competition (Table 4.2), with the 
exception of the LOC entities. We believe this happens for three 
reasons: first, the location entities in the test set overlap to a 
large degree with the locations in the training data; second: more 
than for the other entity types considered, indicative contexts of 
LOC entities are represented by stopwords (e.g. in, from, to), 
third, all systems shown in Table 4.3 except [19] used extensive 
lists of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>4a: Absolute and Normalized KL-divergence of 
LM C for varying context sizes for 3 random samples of 
20 entities (RCV 1/100, including stopwords). 

Context Size 1 
Context Size2 
Context Size 3 

Absolute Normalized Abs. 
Norm. 
Abs. 
Norm. 

LOC 
2.52 
1.03 
1.78 
1.19 
1.48 
1.17 

MISC 
3.22 
1.33 
2.30 
1.53 
1.83 
1.44 

ORG 
5.27 
2.17 
4.40 
2.93 
3.81 
3.00 

PER 
7.64 
3.14 
6.27 
4.18 
5.62 
4.43 

RANDOM 
2.43 
1.50 
1.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>4b: Absolute and normalized KL-divergence of 
LM C for varying context sizes for 3 random samples of 
20 entities (RCV 1/100, discarding stopwords). 

Context Size 1 
Context Size 2 
Context Size 3 

Absolute Normalized Abs. 
Norm. 
Abs. 
Norm. 

LOC 
1.76 
0.88 
1.20 
1.06 
0.98 
1.07 
MISC 
2.51 
1.26 
1.67 
1.47 
1.29 
1.40 
ORG 
4.25 
2.12 
3.36 
2.95 
2.83 
3.08 
PER 
5.88 
2.94 
4.68 
4.11 
4.10 
4.46 
RANDOM 
2.00 
1.14 
0.92 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>5: Absolute and normalized KL-divergence of 
LM C for varying context sizes for 3 random samples of 
20 entities (RCV </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>6: F-measures on the EFE newswire articles 
(Spanish) reported by top 3 systems participating in the 
CoNLL 2002 Shared Task NER competition. 

Carreras 
et al. [4] 

Wu 
et al.[29] 

Florian 
[14] 
Average 
LOC 

79.59 
80.47 
77.50 
79.19 

MISC 

75.41 
73.04 
73.25 
73.9 

ORG 

71.36 
67.90 
69.17 
69.48 

PER 

81.47 
79.40 
74.05 
78.31 

Overall 
77.05 
75.36 
73.30 
75.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>7: F-measures on De Morgen newspaper 
articles (Dutch) reported by top 3 systems participating 
in the CoNLL 2002 Shared Task NER competition. 

Tables 4.8 and 4.9 report the average KL divergence for LOC, 
MISC, ORG, and PER entities for the two languages by using 
random samples of 20 entities of each type. As we can see, our 
model predicts that PER entities are much easier to extract based 
on context than the other entities in both Spanish and Dutch. 

Context size 1 Context Size 2 Context Size 3 
LOC 
2.86 
1.18 
2.53 
1.39 
2.17 
1.42 
MISC 
4.19 
1.73 
3.86 
2.12 
3.60 
2.35 
ORG 
3.44 
1.42 
2.90 
1.59 
2.51 
1.64 
PER 
4.86 
2.01 
4.21 
2.31 
3.91 
2.56 
RANDOM 2.42 
1.82 
1.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>8: Absolute and normalized KL Divergence 
(averaged over 3 samples of 20 entities) for varying 
context sizes for the CoNLL 2002 Spanish NER task. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>9: Absolute and normalized KL Divergence 
(averaged over 3 samples of 20 entities) using different 
context sizes for the CoNLL 2002 Dutch NER task. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>10: Precision for the RE task on the 
Encyclopedia collection for the INVENT, BORN, 
DIED, and WROTE relations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 4 .10 reports the absolute and normalized KL divergence values computed from the models built by discarding common English stopwords. As we can see, the KL divergence values of the BORN and DIED relations are higher than the KL values</head><label>4</label><figDesc></figDesc><table>for 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>11: Absolute and normalized KL divergence 
for INVENT, BORN, DIED, and WROTE relations for 
varying context sizes (Encyclopedia, 20 sample entities, 
discarding stopwords). 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Luis Gravano for ideas and discussions that inspired this work. We also thank Eric Brill and the anonymous referees for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: Extracting Relations from Large Plain-Text Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DL</title>
		<meeting>DL</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the complexity of a domain with respect to an information extraction task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MUC-7</title>
		<meeting>MUC-7</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Analysis of the AskMSR Question-Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named Entity Extraction using AdaBoost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Márques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Simple Named Entity Extractor using AdaBoost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Márques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Padró</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting Syntactic Structure for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Named Entity Recognition with a Maximum Entropy Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ranking Algorithms for Named Entity Extraction: Boosting and the Voted Perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting Query Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-VLC</title>
		<meeting>EMNLP-VLC</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language Independent NER using a Unified Model of Internal and Contextual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Web-scale information extraction in KnowItAll: preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic textual entailment: generic applied modeling of language variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glickman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Methods for Text Understanding and Mining Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Named Entity Recognition as a House of Cards: Classifier Stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named Entity Recognition through Classifier Combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information Extraction: Beyond Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complexity of event structure in IE scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yangarber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<title level="m">Bootstrapping for Text Learning Tasks, IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Named Entity Recognition with Character-Level Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perez-Carballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<title level="m">Natural Language Information Retrieval: Progress Report, in Information Processing and Management Journal</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An exploration of Entity Models, Collective Classification and Relation descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LinkKDD</title>
		<meeting>LinkKDD</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Surface Text Patterns for a Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth National Conference on Artificial Intelligence</title>
		<meeting>the Sixteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2002</title>
		<meeting>CoNLL-2002</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural Language Processing and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Robust Risk Minimization based Named Entity Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
