<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SLICE: A Novel Method to Find Local Linear Correlations by Constructing Hyperplanes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tang</surname></persName>
							<email>tangliang@cs.scu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjie</forename><surname>Tang</surname></persName>
							<email>tangchangjie@cs.scu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Center for Birth Defects Monitoring</orgName>
								<address>
									<postCode>610041</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SLICE: A Novel Method to Find Local Linear Correlations by Constructing Hyperplanes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data Mining</term>
					<term>Linear Correlation</term>
					<term>Principal Component Analysis</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Finding linear correlations in dataset is an important data mining task, which can be widely applied in the real world. Existing correlation clustering methods combine clustering with PCA to find correlation clusters in dataset. These methods may miss some correlations when instances are sparsely distributed. Previous studies are limited to find the primary linear correlation of the dataset. However, there may be some interesting local linear correlations exist in data subsets. This paper develops an efficient approach to seek multiple local linear correlations in dataset. The main contributions of this paper are: (1) analyzing the limitations of applying current methods on finding linear correlations in data subsets; (2) developing a novel algorithm, SLICE (significant local linear correlation searching), to find multiple local linear correlations in data subsets. The basic idea of SLICE is using a heuristic to construct hyperplanes that represent linear correlations; (3) conducting extensive experiments to show that SLICE is effective to find correct correlations in both synthetic and real-world datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linear correlations reveal the linear dependencies among several features in a dataset. Finding theses correlations is an interesting research topic, since it has many realworld applications. For example, in sensor network, a latent pattern, which is the linear correlation of multiple time series data received, can be used to detect the data evolution <ref type="bibr" target="#b11">[12]</ref>.</p><p>Principal component analysis (PCA) is able to capture linear correlations in a dataset <ref type="bibr" target="#b3">[4]</ref>. It involves the computation of eigenvalue and eigenvectors of a data covariance matrix or singular value decomposition of a data matrix <ref type="bibr" target="#b3">[4]</ref>. PCA assumes that all instances in a dataset are in the same correlation. However, instances collected from the real world may have different characteristics, so the linear dependencies among features may be different in different data subsets. For example, analysis on gene expression values in microarray data shows that some linear dependency among gene data only exists under certain situation <ref type="bibr" target="#b9">[10]</ref>. In this case, it is not appropriate to apply PCA to analyze the dataset, since only certain part of the dataset shows the linear correlation.</p><p>Correlation clustering methods, such as 4C, try to seek clusters in a linear correlation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. The most difference between correlation clustering and our work is that correlation clustering seeks the linear correlations of clusters, while the goal of our work is to seek linear correlations of tuples.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, a method called CARE is proposed to find local linear correlations. This method adopts PCA to analyze the linear correlations on both feature subsets and instance subsets. CARE focuses on finding the primary linear correlations from the whole dataset. It is not suitable to find linear correlations that exist in data subsets.</p><p>This study focuses on finding multiple linear correlations in data subsets. We refer such correlations as local linear correlations. The main challenge for us is that the number of data subsets is so large that it is hard to enumerate all subsets in reasonable runtime. Thus, we design a method to search linear correlations by constructing hyperplanes. The time complexity of our proposed method is polynomial.</p><p>Our Contributions: (1) analyzing the limitations of applying current methods on finding linear correlations in data subsets; (2) developing a heuristic algorithm SLICE (significant local linear correlation searching) to find multiple local linear correlations in data subsets. The basic idea of our method is using a heuristic to construct hyperplanes that represent linear correlations; (3) conducting extensive experiments to show that SLICE is effective to find correct correlations in both synthetic and realworld datasets.</p><p>Paper organization: the rest of this paper is organized as follows. Section 2 discusses related works. Section 3 states the concept of significant local linear correlation, and describes our method of searching significant local linear correlations. Section 4 comparatively analyzes the experimental results of our method and related methods. Finally, this Section 5 discusses future works, and concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are three previous studies related to this paper: PCA, correlation clustering, and CARE. We discuss them below.</p><p>Principal components analysis (PCA) is a useful method to detect linear correlations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. PCA transforms the original data space to another orthogonal data space. Given a data matrix, PCA finds the eigenvectors and eigenvalues of that data matrix. The eigenvectors represent the directions with maximal variances of the data by performing singular value decomposition (SVD) to the data matrix <ref type="bibr" target="#b3">[4]</ref>. There are other features transformation methods can be used, such as linear regression analysis (LRA) <ref type="bibr" target="#b4">[5]</ref>, linear discriminate analysis (LDA) <ref type="bibr" target="#b5">[6]</ref>, principal component regression (PCR) <ref type="bibr" target="#b6">[7]</ref>, and supervise probabilistic PCA (SPPCA) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Some clustering methods are developed to find data clusters in subspaces <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. They adopt PCA to project data points in some subspace, where the points of a cluster are close to each other. Recently, some correlations clustering methods are proposed to seek the linear correlations of clusters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. One typical correlation clustering algorithm is 4C <ref type="bibr" target="#b9">[10]</ref>, which generates -neighborhoods first. However, it is hard to generate high quality -neighborhoods to find linear correlations, when the distribution of instances in the correlations is sparse. <ref type="figure" target="#fig_2">Figure 1</ref> shows an example of this drawback of 4C. Example 1. <ref type="figure" target="#fig_2">Figure 1</ref> shows an example of a drawback of correlation clustering. Let S 1 , S 2 be two local linear correlations, A be a -neighborhood. The points on S 1 and S 2 are distributed sparsely. If the value of parameter  in algorithm 4C is small, few -neighborhoods can be generated. This situation is shown as <ref type="figure" target="#fig_2">Figure 1</ref> (a). On the other hand, if  is large, as shown in <ref type="figure" target="#fig_2">Figure 1</ref> (b), -neighborhoods would contain points in S 1 and S 2 , so they don't exhibit any linear dependency. As a result, few correlation clusters could be created in these -neighborhoods.   From Example 1, we can see that it is not suitable to apply 4C to finding local linear correlations.</p><p>CARE is a recently proposed method to find local linear correlations in both feature subsets and instance subsets <ref type="bibr" target="#b0">[1]</ref>. For the problem of finding correlations in data subsets, it assumes that there is only one correlation in a dataset. So the limitation of CARE is that it can only find the primary linear correlations in dataset. CARE applies PCA to get the hyperplane of the whole dataset at first. Then it adopts a heuristic method to delete the "farthest" point to the hyperplane. This deletion process is repeated until the number of data points equals to the requirement defined by user. CARE may decrease the accuracy of results, since the deleted points may belong to some correct correlations. Moreover, there may be no global linear correlation can be found on the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Significant Local Linear Correlation Searching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Significant Local Linear Correlation Searching</head><p>Firstly, we give a brief overview of how to adopt PCA method to find the global linear correlation. Here, the "global" means the correlation is discovered from the whole dataset.  <ref type="figure">Figure 2</ref> shows a hyperplane of a linear correlation:</p><formula xml:id="formula_0">x 1 + 0.33x 2 -x 3 = 0.</formula><p>The normal vector of this hyperplane is [1, 0.33, -1] T , which is the eigenvector of the smallest eigenvalue (variance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. An example of a hyperplane and its normal vector</head><p>We use the definition of strongly correlated feature subset proposed in <ref type="bibr" target="#b0">[1]</ref> to measures both accuracy and significance of the local correlation.</p><formula xml:id="formula_1">Definition 1 (Significant Local Linear Correlation). Given a data matrix D containing M tuples, each tuple has d features. Let S = 1 { ,..., } m jj xx (0 ≤ j t ≤ M, t = 1, 2, …, m) is the subset of D, {λ i }(1≤ i ≤ d)</formula><p>be the eigenvalues of the covariance matrix of S, where λ 1 ≤ λ 2 ≤ … ≤ λ d . The data subset S is in a significant local linear correlation if following conditions are true:</p><formula xml:id="formula_2">11 ( , ) / kd tt tt f S k       (1) m / M  (2)</formula><p>where ,  and k (k ≤d) are user defined parameters. The meanings of these user defined parameters are as same as in <ref type="bibr" target="#b0">[1]</ref>.</p><p>The accuracy is measured by the objective function f(S, k) because the each eigenvalue λ i indicates the variance on corresponding eigenvector's direction. Geometrically, f(S, k) determines the "thickness" of the represented hyperplane. On the other hand, the significance is measured by parameter  which determines the minimal size of the data subset. More tuples covered by the local linear correlation, more significant the correlation would be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Design of SLICE</head><p>In this section, we present our algorithm to find significant local linear correlations in dataset. We call our algorithm as SLICE (significant local linear correlation searching).</p><p>The computation cost would be very large if each subset is enumerated and tested to make sure it is in a significant local linear correlation or not. Given a data matrix with M tuples, there are total 2 M data subsets to be tested, which is impractical for real-world applications. Due to this reason, we develop an approximate approach SLICE to find the significant linear correlations. SLICE cannot guarantee to find all existed significant linear correlations every time, but it is capable to reach them in a high probability within an acceptable computational cost in real datasets. Our extensive experimental results show that SLICE can discover more than 95% all linear correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Heuristic Searching Method</head><p>Our SLICE adopts a heuristic to find the data subset within a significant local linear correlation. Suppose the dataset has M tuples and d features, the heuristic searching begins with an initial seed (d tuples), and let S be the set of the d tuples, then S absorbs the next "best" tuple in rest tuples iteratively until the value of f(S, k) becomes larger than  , which is introduced in Definition 1. If |S| satisfies the requirement in Definition 1, the linear correlation established on S can be regarded as a significant local linear correlation. The concept of the "best" of a tuple will be discussed later in this section. We introduce the definition of distance from a tuple to a hyperplane firstly. Definition 2 (Distance from a tuple to a hyperplane). Given a d dimensional data subset S and a tuple x. The distance from x to the hyperplane established on S, denoted as d(x, S), is defined as follows.</p><formula xml:id="formula_3">( , ) ( { }, ) ( , ) d x S f S x k f S k    (3)</formula><p>where f(S, k) is defined in Definition 1.</p><p>According to Definition 1 and Definition 2, we can see that if the distance from a tuple to the hyperplane is small, the increase of f(S, k) would be small after S absorbs this tuple. So, in the process of searching, the tuple with the minimal distance to the hyperplane is the "best" tuple to be absorbed. As a result, the accuracy of the hyperplane would be higher.</p><p>Our approach to find the "best" tuple is efficient and incremental. We maintain the mean and covariance matrix of S in whole searching to improve the efficiency of calculating the value of f(S, k). xx , let n = |S|, x 0 be the mean of S and C S be the covariance matrix of S. When S absorbs a new tuple x, the new mean x 0 ' and new covariance matrix C S ' can be obtained by following equations.</p><formula xml:id="formula_4">00 1 ' 11 n x x x nn   (4) 0 0 0 0 '' 1 ' ( ) 11 T T T SS C n C x x xx x x nn      (5)</formula><p>By Lemma 1, the computational complexity of updating the covariance matrix of S in each iteration is O(d 2 ) (d is the dimensionality). Considering the computational complexity of PCA, the time complexity of finding the minimum distance between a tuple and the hyperplane is O(d 2 +d 3 ). As the dimensionality d is a constant, the distance calculation can be finished in a constant time consuming. Thus, finding the "best" tuple in each step costs O(n(d 2 +d 3 )) at most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Finding All Significant Local Linear Correlations</head><p>The second key point of SLICE is how to arrange the initial seeds to start the searching to find all significant local linear correlations. If the number of features is d and the size of dataset is M, there are d M C different initial seeds to be tested. It is easy to see that the computational cost is large when the dimensionality is large.</p><p>Fortunately, we can use a pruning strategy to set the initial seeds for searching. Then the whole process can converge very fast. The pruning strategy is based on following practical assumptions:</p><p> A significant local liner correlation covers a large portion of tuples. That is, parameter  should be large. The reason lies that correlations with few tuples may be caused by noise or coincidence of data distribution. Those correlations are meaningless to user.  The overlaps of significant local linear correlations are all relatively small, which means parameter  should be small. Parameter  controls the "thickness" of hyperplane indicated by the significant local linear correlation in geometric view. As showed in <ref type="figure" target="#fig_4">Figure 3</ref>, if the hyperplanes are "thin", the intersection of portions is small. Intuitively, if the overlap of two significant local linear correlations is large, these two correlations should be very similar.</p><p>By these two assumptions, SLICE does not choose tuples as initial seeds which have been absorbed by a hyperplane. Although it cannot guarantee to find all significant local linear correlations, our experimental study shows that the probability for SLICE to reach all of them is larger than 95% (see Experimental Study Section). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>SS: data subsets that are in a significant local linear correlation.</p><formula xml:id="formula_5">Begin 1 M  |D|, C  D 2 SS   3 while |C| &gt; d 4 seed   5 while |seed| &lt; d 6 p  RANDOM(C, d) // select a tuple from C 7 seed  seed∪{p} 8 end 9 S  SEARCH(D, seed, , , k) // call heuristic searching method 10 if |S| ≥ M•  then 11 SS  SS ∪{S} 12 end 13 C  (C -S) 14 end</formula><p>End.</p><p>Note: In Step 1, we maintain a set C of candidate initial seeds that to be invoked in heuristic searching. Subroutine RANDOM(C, d) in Step 6 is to select d tuples from the set C randomly. Subroutine SEARCH(D, seed, , , k) in Step 9 is to invoke the heuristic searching method discussed in the Subsection 3.2.1.</p><p>The time complexity of Algorithm 1 is O(t •n(d 2 +d 3 )), where t is the times of invoking searching method. In the worst case, t = n/d. If  is small, the number of tuples got in Subroutine SEARCH(D, seed, , , k) is small. In this case, t would be large. So we can see that t is associated with parameter . Our experimental results</p><p>show that t is less than n in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Study</head><p>To evaluate the performance of SLICE, we test it on several synthetic datasets and a real world dataset. SLICE is implemented using Matlab 7.6. The experiments are performed on a 1.8GHz PC with 2G memory running Windows Server 2003 operating system. The characteristics of experimental datasets are presented as follows.</p><p> Synthetic datasets: we generate 500 distinct datasets. These synthetic datasets are categorized to 5 different groups. Each group consists of 100 different synthetic datasets, and has a distinct label, such as "D300F4C3". The label indicates the characteristics of each dataset in this group. For example, "D300F4C3" means each dataset in this group has 300 tuples with 4 features, and contains 3 predefined local linear correlations. All feature values of each tuple are generated in <ref type="bibr">[-200, 200]</ref> randomly. Gaussian noise with mean 0 and variance 1.0 is added into each dataset.</p><p> Real world dataset: we use NBA statistics dataset 1 to test the performance of SLICE. We use all 458 players' statistical scores in season 2006. The features in this dataset include minutes, assists, rebounds and so on, which are usually used for basketball specialists to evaluate basketball players in NBA. The meaning of each feature can be found at the website where we download the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effectiveness Evaluation</head><p>In order to evaluate the results of algorithms, we conduct SLICE on these 500 synthetic datasets. We compare the discovered correlations with predefined correlations in datasets. If SLICE finds all predefined correlations, we mark this run as a success. We record the number of success times over all 500 synthetic datasets.</p><p>We set k equals to 1. The values of ,  are list in <ref type="table" target="#tab_1">Table 1</ref>. We choose these values according to the dimensionality and predefined correlations of dataset. That is, different dataset has different these two parameter values. <ref type="table" target="#tab_2">Table 2</ref> shows the success rate 2 of SLICE on the 5 groups of synthetic datasets. From <ref type="table" target="#tab_2">Table 2</ref>, we can conclude that our SLICE has high probability to reach all linear correlations in these synthetic datasets.  </p><formula xml:id="formula_6"> =0.0006  =0.3  =0.0006  =0.2  =0.0006  =0.2  =0.0001  =0.2  =0.0001  =0.18</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparative evaluation on synthetic datasets</head><p>Next, we compare SLICE with 4C and CARE, since 4C and CARE are mostly recent works close to ours to the best of our knowledge. Therefore, for this kind of datasets, the generated   neighborhoods contain tuples in different correlations that would mislead searching the correct correlation clusters. In the experiment, we vary the parameters of 4C to get correlation clusters as large as possible. <ref type="figure">Figure 4</ref> illustrates the best result got by 4C in dataset D300F3C3, where the parameters  =300.0,  =6,  =2,  =0.1 and  =50. As shown in <ref type="figure">Figure 4</ref>, 3 significant local linear correlations found by SLICE are represented by 3 hyperplanes. They match the predefined correlations. However, only one correlation cluster is found by 4C on a part of one hyperplane. From the experimental results on dataset D300F3C3, we can see that no   neighborhood is created when the parameter  is small. On the other hand, if parameter  is large, the   neighborhoods would contain several hyperplanes' tuples, and no correlation cluster can be found. Due to the space limit, we only give the results in dataset D300F3C3 here <ref type="figure">(Figure 4</ref>). Similar results are found in other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Comparison with algorithm CARE:</head><p>CARE is another type of method which is designed to find local linear correlations with both feature subset and tuple subset <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this experiment, we use CARE to find linear correlations in tuple subset. <ref type="figure" target="#fig_7">Figure 5</ref> illustrates the hyperplane found by CARE in dataset D300F3C3. CARE only finds one of three predefined correlations. Based on this result, we can see the main limitation of CARE is that it is only capable to find the primary linear correlation of the dataset. Due to the space limit, we only give the results in dataset D300F3C3 here ( <ref type="figure" target="#fig_7">Figure 5</ref>). Similar results are found in other datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Comparative evaluation on a real-world dataset</head><p>As the authors of <ref type="bibr" target="#b0">[1]</ref> had demonstrated that CARE can better results than 4C, we just compare our SLICE with CARE in NBA dataset. In our experiments, we use SLICE and CARE to discover the linear correlations among minutes, assists, rebounds attributes. We use the same parameters in CARE and SLICE ( = 0.0006,  = 0.5, k = 1). <ref type="table" target="#tab_3">Table 3</ref> lists the linear correlations discovered by CARE and SLICE. From <ref type="table" target="#tab_3">Table 3</ref>, we can see that SLICE finds two different linear correlations, while CARE just finds one linear correlation. In common sense, we know that there are different roles of players in a basketball team. Different players have different responsibilities in a match. For example, the player on Guard position is mainly in charge with assistance. So, we believe the results of SLICE are much closer to the real world compared the result of CARE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency evaluation</head><p>We generate 10 datasets with different data sizes to evaluate the scalability of SLICE on data size. Moreover, we generate 10 datasets with different dimensionalities to evaluate the scalability of SLICE on feature size. All these synthetic datasets has 4 predefined local linear correlations. The parameter k = 1, and  = 0.0001,  = 0.2.   <ref type="figure" target="#fig_8">Figure 6</ref> we can see that the runtime of SLICE is increased linearly when the number of tuples is increased from 500 to 4200. From <ref type="figure" target="#fig_9">Figure 7</ref>, we can see that the runtime of SLICE is increased when the number of features is increased from 3 to 13. It is interesting to see that the runtime is decreased sharply when the number of features is larger than 13. We analyze the reason lies that the value of  remains the same, so the local linear correlations may contain more tuples when the number of features is large than 13. Correspondingly, the times of invoking searching in SLICE is decreased. Thus, we conclude that the value of  should be smaller when there are more features in datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Finding linear correlations in dataset has many real world applications. In this paper, we propose a method to find local linear correlations in data subsets. The main contributions of this paper include: (1) analyzing the limitations of applying current methods on finding linear correlations in data subsets; (2) developing a novel algorithm to find multiple local linear correlations in data subsets. The basic idea is using a heuristic to construct hyperplanes that represent linear correlations; (3) conducting extensive experiments to show that our algorithm is effective to find correct correlations in both synthetic and real-world datasets.</p><p>In future, we plan to integrate user interests in our method to find interesting local linear correlations. Furthermore, developing a method that can guarantee to find all correct linear correlations in polynomial time complexity in each execution is a challenging work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Two kinds of -neighborhoods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 1 (Incremental calculation for covariance and mean).</head><label></label><figDesc>Given a set of tuples S = 1 { ,..., } m jj</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Three hyperplanes with three overlapped parts Algorithm 1 describes the implementation details of SLICE. Algorithm 1 SLICE (D, , , k) Input: D: a d-dimensional data set D, , , k: user defined parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>a) Comparison with algorithm 4C:</head><label></label><figDesc>4C is a kind of correlation clustering algorithm [10]. This algorithm, likes DBSCAN, clusters instances into   neighborhoods at first. In the synthetic datasets, each correlation intersects with another correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 4(a) D300F3C3 dataset. Fig. 4(b) Correlation found by 4C in D300F3C3 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 (b)</head><label>5</label><figDesc>Fig. 5(b) Correlations found by SLICE in NBA dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Varying data size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Varying feature size From Figure 6 we can see that the runtime of SLICE is increased linearly when the number of tuples is increased from 500 to 4200. From Figure 7, we can see that the runtime of SLICE is increased when the number of features is increased from 3 to 13. It is interesting to see that the runtime is decreased sharply when the number of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Let D A B be a data matrix containing M tuples, each tuple has d features, where A is the set of features and B is the set of tuples. C D is the covariance matrix of D. Let {λ i } (1 ≤ i ≤ d) be the eigenvalues of C D , where λ 1 ≤ λ 2 ≤ … ≤ λ N .2 , …, a d ] T be the eigenvector of the smallest eigenvalue. The hyperplane a 1 x 1 + a 2 x 2 +…+ a d x d = c established is exactly a global linear correlation, where c is a constant.</head><label></label><figDesc></figDesc><table>In 
order to get the linear correlations by PCA, we only need to find the smallest 
eigenvalues and their corresponding eigenvectors. Let a = [a 1 , a Example 2. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Values of  ,  for synthetic datasets 

D300F3C3 
D400F3C4 
D600F3C4 
D600F4C4 
D500F4C5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 . success rates of SLICE for each group of datasets</head><label>2</label><figDesc></figDesc><table>D300F3C3 
D400F3C4 
D600F3C4 
D600F4C4 
D500F4C5 
100% 
95% 
99% 
97% 
100% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 . success rates of SLICE for each group of datasets CARE 0.090645 * minutes -0.976344 * assists -0.196303 * rebounds = 0.796832 SLICE 0.157510 * minutes -0.898574 * assists -0.409580 * rebounds = 8.273140 0.112557 * minutes -0.121851 * assists -0.986146 * rebounds = -8.695694</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://sports.espn.go.com/nba/statistics</note>

			<note place="foot" n="2"> We define the success rate is the percent of SLICE finds all predefined correlations over 500 datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CARE: Finding Local Linear Correlations in High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 24th IEEE International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="130" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Finding Generalized Projected Clusters in High Dimensional Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>ACM SIGMOD</publisher>
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Fast Algorithms for Projected Clustering</title>
		<imprint>
			<publisher>ACM SIGMOD</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Principal Component Analysis. Sprinter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Analysis of Variance in Complex Experimental Designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>WileyInterscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kufunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic Press</publisher>
			<pubPlace>San Diego, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Second Course in Statistics: Regression Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mendenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sincich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised Probabilistic Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ACM SIGMOD</publisher>
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computing Clusters of Correlation Connected Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kailing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM SIGMOD</publisher>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deriving Quantitative Models for Correlation Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Achtert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kroger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<title level="m">Streaming Pattern Discovery in Multiple TimeSeries. In: VLDB 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="497" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Local Dimensionality Reduction: A New Approach to Indexing High Dimensional Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
