<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iWARP Redefined: Scalable Connectionless Communication over High-Speed Ethernet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">J</forename><surname>Rashti</surname></persName>
							<email>mohammad.rashti@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory Argonne</orgName>
								<orgName type="institution">Queen&apos;s University Kingston</orgName>
								<address>
									<region>ON, IL</region>
									<country>Canada, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">E</forename><surname>Grant</surname></persName>
							<email>ryan.grant@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory Argonne</orgName>
								<orgName type="institution">Queen&apos;s University Kingston</orgName>
								<address>
									<region>ON, IL</region>
									<country>Canada, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Afsahi</surname></persName>
							<email>ahmad.afsahi@queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory Argonne</orgName>
								<orgName type="institution">Queen&apos;s University Kingston</orgName>
								<address>
									<region>ON, IL</region>
									<country>Canada, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Mathematics and Computer Science Argonne National Laboratory Argonne</orgName>
								<orgName type="institution">Queen&apos;s University Kingston</orgName>
								<address>
									<region>ON, IL</region>
									<country>Canada, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">iWARP Redefined: Scalable Connectionless Communication over High-Speed Ethernet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>iWARP</term>
					<term>Ethernet</term>
					<term>datagram</term>
					<term>Message Passing Interface</term>
				</keywords>
			</textClass>
			<abstract>
				<p>iWARP represents the leading edge of high performance Ethernet technologies. By utilizing an asynchronous communication model, iWARP brings the advantages of OS bypass and RDMA technology to Ethernet. The current specification of iWARP is only defined over connection-oriented transports such as TCP. The memory requirements of many connections along with TCP&apos;s flow and reliability controls lead to scalability and performance issues for large-scale HPC and datacenter applications. In this research, we propose guidelines to extend iWARP over datagrams to provide better scalability and performance. While the proposed extension is designed for use in both HPC and datacenters, the emphasis of this paper is on HPC applications. We present our software implementation of datagram-iWARP over UDP and MPI over datagram-iWARP. Our microbenchmark and MPI application results show performance and memory usage benefits for MPI applications, promoting the use of datagram-iWARP for large-scale HPC applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Despite recognized performance inefficiencies, Ethernet currently accounts for more than half of the interconnection networks in the top 500 supercomputers <ref type="bibr" target="#b28">[30]</ref>. It is due to its easy deployment and low cost of ownership that Ethernet is ubiquitously used in commercial and research clusters, serving High Performance Computing (HPC) and datacenter systems.</p><p>The large overhead that Gigabit and 10-Gigabit Ethernet network protocol processing puts on the CPU cores has led to critical CPU availability and performance issues <ref type="bibr" target="#b7">[8]</ref>. For this, a wide range of efforts started to boost Ethernet efficiency, especially targeting its latency for HPC. The first major attempt was offloading TCP/IP processing using stateless offload (e.g. offloading checksum, segmentation and reassembly, etc.) and stateful TCP Offload Engines (TOE) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Another major approach on top of TOE has been equipping Ethernet with techniques such as Remote Direct Memory Access (RDMA) and zero-copy communication that have traditionally been associated with other high performance interconnects such as InfiniBand <ref type="bibr" target="#b11">[12]</ref>. iWARP (Internet Wide Area RDMA Protocol) <ref type="bibr" target="#b23">[25]</ref> was the first standardized protocol to integrate such features into Ethernet, effectively reducing Ethernet latency and increasing host CPU availability by taking advantage of RDMA, kernel bypass capabilities, zero copy and non-interrupt based asynchronous communication <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b22">[24]</ref>. Rather than the traditional kernel level socket API, iWARP provides a userlevel interface on top of TCP/IP stack that can be used in both LAN and WAN environments, thus, efficiently bypassing kernel overheads such as data copies, synchronization and context switching.</p><p>Despite its contributions to improving Ethernet efficiency, the current specification of iWARP lacks functionality to support the whole spectrum of Ethernet based applications. The current iWARP standard is only defined on reliable connection-oriented transports. Such a protocol suffers from scalability issues in large-scale applications due to memory requirements associated with multiple inter-process connections. In addition, some applications and data services do not require the reliability overhead and implementation complexity and cost associated with connection-oriented transports such as TCP. For example, HPC applications running on a system area network do not require the complexities associated with TCP. A connectionless protocol is a lighter weight protocol that can improve the communication performance as well.</p><p>In this paper, we propose to extend the iWARP standard on top of the User Datagram Protocol (UDP) in order to utilize the inherent scalability, low implementation cost and the minimal overhead of datagram protocols. We provide guidelines and discuss the required extensions to different layers of the current iWARP standard in order to support the connectionless UDP transport. Our proposal is designed to co-exist with and to be consistent and compatible with the current connection-oriented iWARP. While the proposed extension is designed to be used in datacenter and HPC clusters, the emphasis of this paper is on HPC applications.</p><p>Our implementation of datagram-iWARP in software reveals performance benefits that can be potentially achieved when using datagrams in iWARP-based Ethernet clusters. Our verbs level microbenchmark results show that the datagram-iWARP improves the communication latency up to 30%. Our MPI level results also show up to 14% small message latency reduction and up to 20% large message bandwidth improvement when using Message Passing Interface (MPI) <ref type="bibr" target="#b16">[17]</ref> on top of datagram-iWARP. We also observe that MPI applications can substantially benefit in performance and memory resource usage when running on datagram-iWARP, compared to the connection-based iWARP; more than 30% less memory usage and more than 40% runtime reduction for some HPC applications on a 64-core cluster.</p><p>The rest of this paper is organized as follows. In Section II, we provide background about the current iWARP standard and discuss some of its shortcomings for HPC and datacenter applications.</p><p>In Section III, we propose guidelines for changes to the iWARP for datagram support. Section IV describes our implementation of iWARP over UDP. Section V and Section VI include the experimental platform and evaluation results respectively. Section VII discusses some related scholarly work and finally, Section VIII concludes the paper and points to future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. IWARP STANDARD</head><p>Proposed by RDMA Consortium <ref type="bibr" target="#b23">[25]</ref> in 2002 to the IETF <ref type="bibr" target="#b12">[13]</ref>, the iWARP specification defines a multi-level processing stack on top of standard TCP/IP over Ethernet. The stack is designed to decouple the processing of Upper Layer Protocol (ULP) data from the operating system (OS) and reduce the host CPU utilization by avoiding intermediate copies during data transfer (zero copy). To achieve these goals, iWARP needs to be fully offloaded, for example on top of stateless or stateful TOE.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, at the top layer, iWARP provides a set of descriptive user-level interfaces called iWARP verbs <ref type="bibr" target="#b9">[10]</ref>. The verbs interface bypasses the OS kernel and is defined on top of an RDMA enabled stack. A network interface card (NIC) that supports the RDMA stack as described in iWARP standard is called an RDMA-enabled NIC or RNIC. An RNIC implements both iWARP stack and TOE functionality in hardware.</p><p>The RDMA protocol (RDMAP) layer supplies communication primitives for verbs layer <ref type="bibr" target="#b24">[26]</ref>. The data transfer primitives are Send, Receive, RDMA Write and RDMA Read that are passed as work requests (WR) to a Queue Pair (QP) data structure. The WRs are processed asynchronously by the RNIC, and the completion is notified either by polled Completion Queue (CQ) entries or by event notification <ref type="bibr" target="#b9">[10]</ref>.</p><p>Verbs layer WRs are delivered in order from RDMAP to the lower layers. The Send and RDMA Write operations require a single message for data transfer, while the RDMA Read needs a request by the consumer (data sink), followed by a response from the supplier (data source) <ref type="bibr" target="#b24">[26]</ref>. RDMAP is designed as a stream-based layer. Operations in the same RDMAP stream are processed in the order of their submission. The Direct Data Placement (DDP) layer is designed to directly transfer data between the user buffer and the RNIC without intermediate buffering <ref type="bibr" target="#b25">[27]</ref>. The packet based DDP layer matches the data sink at the RDMAP layer with the incoming data segments based on two types of data placements models: tagged and untagged. The tagged model, used for one-sided RDMA Write and Read operations, has a sender-based buffer management in which the initiator provides a pre-advertised reference to the data buffer address at the remote side. The untagged model uses a two-sided Send/Receive semantic, where the receiver both handles buffer management and specifies the receive buffer address <ref type="bibr" target="#b25">[27]</ref>.</p><p>Due to DDP being a message-based protocol, out-oforder placement of message segments is possible, therefore DDP assures delivery of a complete message upon arrival of all segments. In the current iWARP specification, DDP assumes that the lower layer provides in order and correct delivery of messages.</p><p>The lower layer protocol (LLP) on which the iWARP stack is running can be either TCP or SCTP <ref type="bibr" target="#b19">[21]</ref>. Due to the message-oriented nature of DDP, the iWARP protocol requires an adaptation layer to put boundaries on DDP messages transferred over the stream oriented TCP protocol. The Marker PDU Alignment (MPA) protocol <ref type="bibr" target="#b4">[5]</ref> inserts markers into DDP data units prior to passing them to the TCP layer. It also re-assembles marked data units from the TCP stream and removes the markers before passing them to the DDP. The MPA layer is not needed on top of messageoriented transports such as SCTP because intermediate devices do not fragment message-based packets as they would with stream-based ones, removing the middle-box fragmentation issue that the MPA layer solves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortcomings of the Current Standard</head><p>The current iWARP standard offers a range of capabilities that increase the efficiency of Ethernet in modern HPC and datacenters clusters. Taking advantage of the wellknown reliable transports in the TCP/IP protocol suite is one of its key advantages. Reliability has in fact been a major force for designing the current iWARP standard on top of connection-oriented transports. The LLP for DDP and MPA is assumed to be a point-to-point reliable stream, established prior to iWARP communication. This requirement makes it easy for the ULP to assume reliable communication of user data. In addition, the independence of individual streams makes iWARP able to enforce error management on a per stream basis. Such a standard is a fit for applications that require strict reliability at the lower layer, including data validation, flow control and in order delivery. Examples for such applications are reliable datacenter services such as database servers, file services, financial applications and policy enforcement systems (e.g. security applications, etc.).</p><p>On the other hand, there is a growing demand for applications such as voice and video streaming that find the strict connection-based semantics of iWARP unnecessary. For such cases, the current iWARP standard imposes barriers to application scalability in large systems, due to its explicit connection oriented nature and reliability measures. The following subsections point to the shortcomings of the current standard and their relevant implications. As such, there are strong motivations for extending the iWARP standard with datagram transport.</p><p>1) Memory usage. The pervasiveness of the Ethernet in modern clusters places a huge demand on the scalability of the iWARP standard. The scale of high performance clusters is increasing rapidly and can soon reach to a million cores. A similar trend can be observed for datacenters. An obvious drawback of the connection-oriented iWARP is the connection memory usage that can exponentially grow with the number of processes. This dramatically increases the application's memory footprint, unveiling serious scalability issues for large-scale applications.</p><p>As the number of required connections increases, memory usage grows proportionally at different network stack layers. In a software implementation of iWARP at the TCP/IP layer, each connection will require a set of socket buffers allocated, in addition to the data structure required to maintain the connection state information. Although the socket buffers are not required in a hardware implementation of iWARP due to zero-copy on the fly processing of data, making a lot of connections will have other adverse effects. Due to limited RNIC cache for connection state information, maintaining out-of-cache connections will require extra memory requests by the RNIC, which implies extra overhead on communciation time.</p><p>The other major place of memory usage is the application layer. Specifically, the communication libraries such as MPI pre-allocate memory buffers per connection to be used for fast buffering and communication management <ref type="bibr" target="#b13">[14]</ref>.</p><p>2) Performance. In addition to memory usage problem, connection oriented protocols such as TCP, with their inherent flow-control and congestion management limit performance <ref type="bibr" target="#b10">[11]</ref>. HPC applications running on a local cluster do not require the complexities of TCP flow and congestion management. UDP offers a much lighter weight protocol that can significantly reduce the latency of individual messages, closing the latency gap between iWARP and other high speed interconnects. In addition, many datacenter applications such as those using media streaming protocols over WAN are currently running on top of unreliable datagram transports such as UDP. Due to such semantic discrepancies, the current connection-oriented specification of iWARP makes it impossible for such applications to take advantage of iWARP's major benefits such as zero copy and kernel bypass.</p><p>3) Fabrication cost. The complexities associated with stream based LLPs such as TCP and SCTP translate into expensive and non-scalable hardware implementations. This becomes especially important with modern multi-core systems where multiple processes could utilize the offloaded stack. A heavyweight protocol such as SCTP or even TCP can partially support multiple parallel on-node requests, due to implementation costs associated with hardware level parallelism <ref type="bibr" target="#b22">[24]</ref>. This means that a small portion of many cores available on the node will be able to simultaneously utilize the actual hardware on the NIC. This can lead to serialization of the communication.</p><p>4) Hardware level operations. iWARP lacks useful operations such as hardware level multicast and broadcast. These operations, if supported, can be utilized in applications with MPI collectives and also media streaming services. iWARP does not support such operations primarily because the underlying TCP protocol is not able to handle multicast and broadcast operations. An extension of iWARP to datagrams will boost iWARP's position as a leading solution for high performance Ethernet. Next section presents our proposal for such an extension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATAGRAM-IWARP</head><p>The current iWARP standard and its main layers (RDMAP and DDP) are explicitly designed for connectionoriented LLPs. Therefore, there are semantic discrepancies with datagram protocols such as UDP that need to be addressed in our design. There are implications in the standard that make the definition of unreliable and datagram services viable in the current iWARP framework (for example Sections 3.2 and 8.2 of DDP specification <ref type="bibr" target="#b25">[27]</ref>).</p><p>In this proposal, we try to keep the current welldeveloped specification of iWARP, while extending its functionality to support datagram traffic. In the first step, we highlight parts of the standard at different layers that are incompatible with datagram semantics. Then we propose guidelines to address such incompatibilities. In this paper we cover the untagged model of the DDP layer and the tagged model will be covered in our future research. It is important to note that these proposals should not be considered as exact modifications of the standard. We rather point to major places of the standard for modification to support datagram transport. <ref type="figure" target="#fig_2">Fig. 2</ref> presents major changes required at each layer of the current standard. Categorized details can be found in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modifications to the Verbs Layer</head><p>We do not necessarily require introducing new verbs for the datagram mode. The existing set of iWARP verbs can be adapted to accept datagram related input and act according to the datagram service. Here we point to some major parts of the verbs specification that need to be changed to support datagram transport:</p><p>• Currently, there is only one type of QP, the connectionbased QP. Thus, there has been no need for QP type definition. With the new extension, new QP type(s) must be added to distinguish datagram-iWARP from connection-based iWARP. More details will be discussed in part B of Section III.</p><p>• QP creation and its input modifiers need to be changed.</p><p>For example, to specify the transport type (connected or datagram) a new input modifier should be added to the QP attribute structure.</p><p>• Specification of the QP modify verb needs to change, to accommodate the new definition of the QP states and the required input data for datagram QPs. As an example, the datagram QPs need a pre-established datagram socket to be passed to modify the QP into the Ready To Send (RTS) state <ref type="bibr" target="#b9">[10]</ref>.</p><p>• An address handle is required for each send WR posted to the datagram QP to specify the receiver's IP address and UDP port related to the remote QP.</p><p>• Completion notifications structure needs to be changed to accommodate the new WR structure. In particular, the work completion structure should be changed to include the source address. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reliable, In-order Delivery</head><p>Reliable service is a fundamental assumption in the current iWARP standard. This assumption is not necessarily in opposition to the use of datagrams. In datagram-iWARP design we introduce two types of datagram services, Unreliable Datagram (UD) and Reliable Datagram (RD). Subsequently QP types need to be defined at the verbs and RDMAP layers. The defined QP types are: unreliable datagram and reliable datagram for UDP and Reliable Connection (RC) for the current TCP-based QPs.</p><p>The datagram-iWARP over UD transport assumes no reliability or order of delivery from the LLP. In the untagged model which is the focus of this paper, the incoming messages will be matched to the posted receive WRs at the data sink, in order of their arrival at the DDP layer which is not necessarily their order of issue at the data source. While keeping the iWARP data integrity checksum mechanism (e.g. CRC), the rest of reliability measures are left to the application protocol. Such a service is very useful for applications with high error resiliency (such as media streaming applications in datacenters) and applications that can efficiently provide their own required level of reliability. An example can be the applications running in low error rate environments such as closed Local or System Area Networks where standard reliability measures impose too much performance overhead.</p><p>For the RD service, the LLP is assumed to guarantee that messages from a specific data source are delivered correctly and in-order. Such a definition implies a logical pseudoconnection between the local QP and the remote QP. However, DDP/RDMAP layers are not required to keep state information for such a logical connection. Similar to UD, DDP and RDMAP for RD service are required to pass the messages in the order they have received them. To keep the scalability advantages of using a connectionless transport, the LLP reliability service is assumed to be lightweight and should require no or minimal buffering for individual remote sockets. The way the LLP (here, a reliable form of UDP) provides reliability and the mechanism by which such a service is configured is outside of the scope of the extended iWARP specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Streams and Connections</head><p>Currently, the RDMAP and DDP layer streams are assigned to underlying LLP connections that are assumed to be pre-established. Since connections are conceptually not supported in a datagram (connectionless) model, no connection establishment and teardown is required. For datagram-iWARP, a previously created UDP socket is required for each QP. In this case, the ULP transitions the QP into iWARP mode after creating the QP and assigning the lower layer socket. This operation is done locally without negotiating any parameters (such as CRC settings) with any other peer. Such parameters need to be pre-negotiated by the ULP. This also implies that the ULP is no longer required to configure both sides for iWARP at the same time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. QP Identification</head><p>For the untagged model, the DDP layer provides a queue number (QN) in the DDP header to identify the destination QP <ref type="bibr" target="#b25">[27]</ref>, which is currently not fully utilized by the RDMAP. The RDMAP only uses its first 2 bits <ref type="bibr" target="#b24">[26]</ref>. In the datagram-iWARP, we currently assume assignment of a single datagram QP to a UDP socket. In such a model, no QN is required to identify the source and destination QPs. An optional model of the datagram service can assign multiple datagram QPs to a single socket, similar to multiple streams per LLP connection in the current iWARP. Such a case benefits from the QN field in the DDP header.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Message Segmentation</head><p>Unlike TCP byte-oriented service, UDP datagrams will arrive at the LLP in their entirety and thus the concept of message segmentation and out-of-order placement at the DDP layer is irrelevant to the datagram service. This implies that the DDP layer does not need its provisions for segmented message arrival over the datagram transport (including message offset (MO) and even MSN for some cases). For messages larger than maximum datagram size (64KB), segmentation and reassembly is done at the application layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Completion of WRs</head><p>In the connected mode, a WR is considered complete when the LLP layer can guarantee its reliable delivery. The same semantics can be used for RD transport. However, for the UD transport we no longer require an LLP guarantee. Thus, a WR should be considered complete as soon as it is accepted by the LLP for delivery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. MPA Layer</head><p>Since each DDP message will be encapsulated into one UDP datagram, no markers are required for iWARP over UDP. Therefore, the MPA layer (specifically the marker functionality) is not needed for the datagram service. This will improve the performance of the datagram transport since MPA processing has shown to impose significant overhead on the performance of iWARP due to marker placement complexities <ref type="bibr" target="#b0">[1]</ref>, in addition to increasing the overall size of the required data transmission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SOFTWARE IMPLEMENTATION</head><p>To evaluate the proposed datagram extension to the iWARP standard we have developed a software implementation of datagram-iWARP. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the layered stack of this implementation which is built on top of an available software-based iWARP code from Ohio Supercomputer Center (OSC) <ref type="bibr" target="#b20">[22]</ref>. Our implementation can be used on top of both reliable and unreliable UDP protocols.</p><p>Our evaluation in this paper is on top of unreliable (regular) UDP. To assess our implementation in a standard way, we have completed an OpenFabrics (OF) verbs interface <ref type="bibr" target="#b21">[23]</ref> on top of the native software iWARP verbs. We have also used the OF verbs interface to adapt an existing MPI implementation <ref type="bibr" target="#b13">[14]</ref> on top of our iWARP stack. The next subsections discuss some features of our implementation at both iWARP and MPI levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Software Datagram-iWARP</head><p>As mentioned above, we have used the OSC software iWARP implementation as our code base and extended that code in the datagram domain. Here we list a number of features for our implementation:</p><p>• Complete implementation and integration of iWARP over UDP into the TCP-based iWARP stack from OSC. This has been done by introducing new native verbs to support datagram semantics.</p><p>• Using CRC error checking at the lower DDP layer for datagrams.</p><p>• Using a round-robin polling method on operating sockets, to ensure a fair service to all QPs in the software RNIC. to avoid extra sender and receiver side copies, for improved performance and CPU availability. In I/O vector calls (sendmsg and recvmsg), message data and header can be gathered/scattered from/to non-contiguous data buffers to/from the datagram. Therefore an intermediate copy is not required to make the datagram.</p><p>• Avoiding segmentation of DDP messages into MTU-size datagrams. This option, which is possible due to messageoriented nature of UDP, positively contributes to the performance of datagram-iWARP.</p><p>• Implementation of standard OF verbs: These verbs were originally designed for InfiniBand (called OpenIB verbs). Currently they are known as OF verbs and are utilized to implement iWARP verbs abstraction as well. We use the native verbs to implement the OF verbs. The OF verbs are utilized at the MPI layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MPI over Datagram-iWARP</head><p>To evaluate the performance and memory usage of a datagram based iWARP for HPC applications, we have adapted MVAPICH <ref type="bibr" target="#b18">[20]</ref> on top of OF verbs over software iWARP. We have used the hybrid channel from the MVAPICH-Aptus over InfiniBand <ref type="bibr" target="#b13">[14]</ref>. MVAPICH-Aptus is an available MPI implementation that offers a hybrid (UD and RC) channel over OF verbs. The hybrid channel offers a dynamic channel management over InfiniBand's UD and RC transports, meant to offer scalability for MPI applications on large scale InfiniBand clusters. The channel starts with UDbased QPs for each process, and based on a set of policies, establishes RC connections to a selected set of other processes, up to a maximum number of RC QPs. This strategy makes applications scale better by limiting the resource-greedy RC connections and putting most of the communication on UD QPs. Reliability has been added for UD communication at the MPI layer, using acknowledgments and timeouts <ref type="bibr" target="#b13">[14]</ref>.</p><p>The MVAPICH code has been modified in several ways to adapt it over the iWARP standard and our software implementation. Here is a list of some modifications made to the implementation:</p><p>• Transforming MVAPICH UD-based connection management to the datagram-iWARP: This includes establishing datagram sockets and relevant address handles to be used as the underlying LLP (UDP) sockets required by datagram-iWARP.</p><p>• Transforming MVAPICH InfiniBand RC-based connection management: The MVAPICH hybrid channel uses on-demand RC connection management <ref type="bibr" target="#b13">[14]</ref>. Due to semantic differences between TCP and InfiniBand RC, the handshaking steps for MVAPICH dynamic connection establishment have been modified. The new arrangement also piggybacks some new required information and in addition, performs the socket connections at the very last handshake stage.</p><p>• Changing or disabling parts of the code relying on incompatibilities between iWARP and InfiniBand. This includes functions unsupported in the iWARP implementation such as immediate data, DDP tagged model, GRH headers, Shared Receive Queues (SRQ), eXtended Reliable Connections (XRC), Service Levels (SL), LIDs and LID mask control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL PLATFORM</head><p>We use two different clusters for our experiments. Cluster C1 is a set of four nodes, each with two quad-core 2GHz AMD Opteron processors, 8GB RAM, 512KB L2 cache per core and 8MB shared L3 cache per processor chip. The nodes are interconnected through NetEffect 10GE cards connected to a Fujitsu 10GE switch. The OS on C1 cluster nodes is Fedora 12 (kernel 2.6.31).</p><p>Cluster C2 contains 16 nodes, each with two dual-core 2.8GHz Opteron processors, with 1MB L2 cache per core, 4GB RAM and a Myricom 10GE adapter <ref type="bibr" target="#b17">[18]</ref> connected to a Fulcrum 10GE switch. The OS on C2 cluster nodes is Ubuntu with kernel version 2.6.27.</p><p>The reason for using two clusters for the evaluation of this work is to show how application performance and memory usage scale using datagram-iWARP on two different architectures. In particular, the number of cores per node for C1 and C2 is different. With C2 having half of the C1 core-per-node ratio and twice the number of cores in total, its inter-node communication share will be four times that of the C1. This is expected to yield more application performance and scalability, since the proposed extension only affects MPI inter-node communications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>We assess the performance of our UD-based datagramiWARP implementation using verbs and MPI level microbenchmarks over cluster C1. We also evaluate the effect of datagram-iWARP on the performance and memory of some MPI applications on both C1 and C2 clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Microbenchmark Performance Results</head><p>We use microbenchmarks to test the performance of the UDP-based iWARP compared to that of the standard TCPbased iWARP. At the verbs layer, we present latency results for both native verbs and OF verbs on top of them. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the verbs layer ping-pong latency results. We clearly observe that in most cases the UD latency is lower than that of the RC, primarily due to the following reasons:</p><p>• Due to no reliability measures, communication over UDP offers a lighter and consequently faster network processing path, compared to the TCP-based communication.</p><p>• Markers are a significant source of overhead on all message sizes. UDP path is bypassing the MPA layer markers, while TCP-based communication requires markers due to the stream oriented nature of TCP.</p><p>• The closed dedicated cluster provides an almost error-free environment where strict reliability measures of the TCP protocol are considered purely overhead compared to the unreliable UDP. The reason for UD latency being significantly more than RC latency at 64KB is that it is exceeding the maximum datagram size and the benchmark needs to segment the messages into 64KB chunks. The plots in <ref type="figure" target="#fig_4">Fig. 4</ref> also show a small overhead for the OF verbs implementation on top of the native verbs.</p><p>At the MPI layer, we present microbenchmark results for latency and bandwidth. For all datagram tests we use the MPI level reliability provisions that exist in MVAPICH code for the UD transport <ref type="bibr" target="#b13">[14]</ref>. These provisions include sequence numbers, acknowledgements sent for every 50 messages, and timeouts (a fraction of a second) in case acknowledgements are not received or out-of-sequence packets are received. Such provisions are satisfactory for the relatively error-free local area networks that are used for these tests, while not adding unnecessary overhead. <ref type="figure">Fig. 5</ref> includes the ping-pong latency comparison of MPI over datagram and connection based iWARP for different message sizes. Results show the superiority of the datagrammode MPI performance over the connection mode, which is mainly carried from the verbs performance benefits. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the bidirectional bandwidth results at the MPI level. For this test, we use two pairs of processes on two nodes, communicating in the opposite directions. In each pair, one of the processes posts a window of non-blocking receive calls. The other process in the pair posts a window of non-blocking send calls. Synchronization then occurs at the end. As observed, MPI-UD offers a higher bidirectional bandwidth for most of the message sizes, meaning that we can better saturate the network using datagrams. The improvement is about 20% for large messages. Lighter protocol processing and minimal reliability measures are the advantages of UD-based communication that make the benchmark capable of pushing more data on the wire in each direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MPI Application Results</head><p>1) Application Performance. <ref type="figure">Fig. 7</ref>  The results are reported for class B of CG, MG and LU benchmarks from NAS Parallel Benchmark (NPB) suit version 2.4 <ref type="bibr">[19]</ref>, as well as <ref type="bibr">Radix [28]</ref> and SMG2000 <ref type="bibr" target="#b2">[3]</ref> applications. All results are presented for 4, 8, 16, 32 and 64 processes (64-process results are only on C2 cluster).</p><p>The results for both communication time and application runtime clearly show that we can expect considerable performance benefits when using datagram communication. In addition to reasons for superiority of datagram-based communication discussed above, the lowered complexity of UDP should theoretically create the availability of more CPU cycles for applications' computation phases, which would lead to lower overall runtimes.</p><p>2) Application Memory Usage. One of the strongest motivations to extend iWARP standard to datagram domain is to improve its memory usage in order to make it scalable for large scale parallel jobs.</p><p>Socket buffers are the most contributors to the memory usage at the OS level. However, in many operating systems including Linux, the Slab allocation system <ref type="bibr" target="#b1">[2]</ref> is used in which a pool of buffers are pre-allocated and assigned to the sockets when data is being communicated. This mechanism that is primarily used to alleviate the memory fragmentation effects hides the contribution of socket buffer sizes to the overall application memory usage. Therefore, the socket buffer allocation is not reflected in the total memory of the system, unless the pre-allocated slab buffers are filled and new buffers are reallocated due to high instantaneous network usage.</p><p>At the MPI layer, MVAPICH pre-allocates a number of general buffer pools with different sizes for each process. For the datagram QP that is established in both connection and datagram based modes, a number of buffers are picked from these pools and pre-posted as receive buffers to the QP. Once a new connection-based QP is established, a default number of 95 receive buffers are picked from the pools and posted to the QP. With a default size of 8KB for each buffer, a rough estimate of 800KB or 200 memory pages of 4KB size are required per connection for each process. To measure the memory usage for the software-based iWARP, we use the total number of memory pages allocated to each MPI job in Linux, reported by the Linux proc files system. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the improvement percentage for datagram-iWARP over connection-oriented iWARP. As observed, the overall memory usage of MPI applications can benefit from using datagrams. For most cases, the results also show an increasing trend in application memory saving, from 4 processes to 64 processes. This trend clearly implies higher memory saving on a larger cluster.</p><p>The benefit for some applications in NPB (such as CG) are relatively low and do not scale very well with the number of processes. This is primarily because each process in such applications usually communicates with a few partners. Therefore, the number of connections for each process will not scale exponentially with the number of job processes. This means that the memory benefit can decrease or stay at the same level (the results are correspondingly better for the C2 cluster due to greater inter-node communication). This is however not the case for SMG2000 and Radix. In these applications a process may communicate with all of the other processes, and therefore the number of dynamically allocated connections will increase exponentially with the number of processes in the job. This is why we see significant increase in memory saving when the scale of the MPI job increases.</p><p>An obvious observation in both performance and memory usage benchmarks is that the C2 cluster results are significantly better than that of the C1 cluster. As discussed in Section V, with the same number of processes, the communication between the nodes is quadrupled in C2. This also translates in more inter-process connections, which implies more memory saving on C2. The results lead to this conclusion that when the amount of inter-node communication rises, so do the benefits of using datagramiWARP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>The implementation of the current iWARP standard in software, originated from a project by OSC <ref type="bibr" target="#b20">[22]</ref> that provides both user-space <ref type="bibr" target="#b5">[6]</ref> and kernel space <ref type="bibr" target="#b6">[7]</ref> implementations. The user-space part of this software implementation is the code-base for implementing datagramiWARP.</p><p>Another project that has recently completed its main functionality is the SoftRDMA project at IBM Zurich laboratory. This work is meant to be integrated into the Open Fabrics Enterprise Distribution (OFED) <ref type="bibr" target="#b21">[23]</ref> stack as a software iWARP solution <ref type="bibr" target="#b15">[16]</ref>. Our proposal in this paper is the first and the only work in this area that extends the iWARP standard to datagram transport and utilizes it in HPC applications.</p><p>Beside the iWARP solution, there have been other approaches with the goal of improving Ethernet efficiency using modern user-level libraries of other high performance interconnects. One is the Myrinet Express (MX) over Ethernet (MXoE) <ref type="bibr" target="#b17">[18]</ref> to provide the high-performance functionality of Myrinet MX user-level library on top of Ethernet networks. Open-MX project <ref type="bibr" target="#b8">[9]</ref> is an open-source implementation of MXoE. Another work in the similar direction is InfiniBand over Ethernet (IBoE) that is also called RDMA over Ethernet (RDMAoE) and is designed to take advantage of InfiniBand's RDMA stack while simply replacing InfiniBand's link layer with Ethernet. This technology encapsulates InfiniBand reliable and unreliable service data inside Ethernet frames. An evaluation of RDMAoE can be found in <ref type="bibr" target="#b27">[29]</ref>.</p><formula xml:id="formula_0">4 1 6 6 4 C 1 C G . B C 1 L U . B C 1 M G . B C 1 S M G 2 0 0 0 C 1 R a d i x C 2 C G . B C 2 L U . B C 2 M G . B C 2 S M G 2</formula><p>A new set of standards referred to as Converged Enhanced Ethernet (CEE) has opened up issues revolving around providing advanced features over Ethernet networks. Some industry vendors and researchers <ref type="bibr" target="#b3">[4]</ref> are also proposing to include RDMA functionality over CEE (RDMAoCEE).</p><p>There has been some past work on the InfiniBand network to equip MPI with the InfiniBand UD transport for scalability purposes on large scale clusters. In MVAPICH-UD project <ref type="bibr" target="#b14">[15]</ref> an InfiniBand UD-based channel is designed for MVAPICH MPI implementation which has shown considerable memory usage benefits over the RC-based channel. The MVAPICH-Aptus, which is used as the base of our MPI work in this paper, is a continuation of the MVAPICH-UD work in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this paper we discussed some challenges facing the current iWARP standard and tried to address them by extending the standard to the datagram domain. We implemented the iWARP over UD in software to assess its potential benefits for HPC applications. Our experiments reveal that a datagram-enabled iWARP increases the scalability of large-scale HPC applications, while potentially improving their performance at the same time. In addition, verbs level microbenchmark results clearly show the potential benefits that other kinds of applications, such as datacenter services can receive from datagram-iWARP.</p><p>Our verbs level microbenchmark results show that the datagram-iWARP improves the communication latency up to 30%. Our MPI level results also show up to 14% small message latency reduction and up to 20% large message bandwidth improvement when using MPI on top of datagram-iWARP. The application results also show more than 40% runtime improvement and more than 30% application memory usage reduction for some MPI applications on a 64-core cluster. In addition, the runtime improvement and memory usage reduction trend for most of the applications imply more application memory savings and runtime improvement on larger clusters.</p><p>This work presents a first step in standardization of datagram-iWARP and can be continued in a number of directions, including reliable datagram (reliable-UDP), tagged model, and socket interface for datacenter applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. iWARP standard stack compared to host-based TCP/IP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Extensions to the stack for datagram-iWARP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The software implementation of datagram-iWARP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Verbs ping-pong latency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 5. MPI ping-pong latency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 7. MPI application communication time and rumtime benefits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>(MSN in the DDP header). The error message can be placed into an Error queue that replaces the Terminate queue of the connected mode.</head><label></label><figDesc></figDesc><table>Transport 
error 
management 
and 
connection 
teardown/termination requirements in the current standard 
will be the responsibility of the datagram LLP, if a reliable 

Verbs 
Modify verbs &amp; data structures to 
accept datagrams. 

Define datagram QPs &amp; WRs 

No streams/connections. No message 
segmentation. Use UDP sockets and 
functions. 

MPA is bypassed for datagrams 

Use UDP for UD QPs and lightweight 
Reliable UDP for RD QPs. 

RDMAP 

DDP 

MPA 

Transport 
(TCP/IP) 

service is being used (i.e. RD mode). Error management at 
the higher layers (e.g. DDP or RDMAP) needs to be 
modified to suit the datagram service. For example, the 
current standard requires an abortive termination of a stream 
at all layers and an abortive error must be surfaced to the 
ULP, should an error be detected at the RDMAP layer for 
that stream [26]. Since such a requirement does not apply to 
datagrams, an abortive error must be surfaced to the ULP 
and the QP must simply go into the Error state without 
requiring any stream termination. This makes the QP unable 
to communicate with any other pair, until the QP is reset and 
modified to the RTS state by the application. Instead of the 
stream termination message sent to the other side, a simple 
error message should be transferred, identifying the 
erroneous message number using the Message Sequence 
Number </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>presents MPI application performance results, including total communication time and application runtime. For measuring communication time we aggregate the time spent in communication primitives: MPI blocking and non-blocking send and receive and MPI wait calls.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the Impact of Supporting Out-of-Order Communication on In-order Performance with iWARP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagvat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE/ACM International Supercomputing Conference (SC07)</title>
		<meeting>the 2007 IEEE/ACM International Supercomputing Conference (SC07)<address><addrLine>Reno, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Slab Allocator: An Object-Caching Kernel Memory Allocator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 USENIX Summer Technical Conference (USTC&apos;94)</title>
		<meeting>the 1994 USENIX Summer Technical Conference (USTC&apos;94)<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semicoarsening Multigrid on Distributed Memory Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Falgout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1823" to="1834" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Remote Direct Memory Access over the Converged Enhanced Ethernet Fabric: Evaluating the Options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crupnicoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17 th IEEE Symposium on High Performance Interconnects (HotI&apos;09)</title>
		<meeting>the 17 th IEEE Symposium on High Performance Interconnects (HotI&apos;09)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Marker PDU Aligned Framing for TCP Specification (Version 1.0),&quot; RDMA Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Culley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Elzur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baily</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design and Implementation of the iWARP Protocol in Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dalessandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wyckoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Parallel and Distributed Computing and Systems Conference (PDCS&apos;05)</title>
		<meeting>the Parallel and Distributed Computing and Systems Conference (PDCS&apos;05)<address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dalessandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Devulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wyckoff</surname></persName>
		</author>
		<title level="m">iWARP Protocol Kernel Space Software Implementation,&quot; 6 th Workshop on Communication Architecture for Clusters (CAC&apos;06</title>
		<meeting><address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
	<note>Proceedings of the 20 th IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS&apos;06)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Performance Characterization of a 10-Gigabit Ethernet TOE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13 th International Symposium on High Performance Interconnects (HotI&apos;05)</title>
		<meeting>the 13 th International Symposium on High Performance Interconnects (HotI&apos;05)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design and Implementation of Open-MX: High-Performance Message Passing over Generic Ethernet Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brice</forename><surname>Goglin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22 nd IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS&apos;08)</title>
		<meeting>the 22 nd IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS&apos;08)<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
	<note>8 th Workshop on Communication Architecture for Clusters (CAC&apos;08)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">RDMA Protocol Verbs Specification (version 1.0),&quot; RDMA Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Culley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinkerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TCP Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Internet Protocol Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000-06" />
		</imprint>
		<respStmt>
			<orgName>Cisco Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">InfiniBand Architecture Specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Infiniband Trade Association</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Release 1.2.1</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<ptr target="http://www.ietf.org" />
	</analytic>
	<monogr>
		<title level="j">Internet Engineering Task Force</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MVAPICH-Aptus: Scalable High-Performance Multi-Transport MPI over InfiniBand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22 nd IEEE International Parallel and Distributed Processing Symposium (IPDPS&apos;08)</title>
		<meeting>the 22 nd IEEE International Parallel and Distributed Processing Symposium (IPDPS&apos;08)<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High Performance MPI Design using Unreliable Datagram for Ultra-Scale InfiniBand Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21 st ACM International Conference on Supercomputing (ICS07)</title>
		<meeting>the 21 st ACM International Conference on Supercomputing (ICS07)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Software iWARP Driver for OpenFabrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented in OpenFabrics Alliance 2010 Sonoma Workshop</title>
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
		<respStmt>
			<orgName>IBM Zurich Research Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MPI: A Message Passing Interface Standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mpi</forename><surname>Forum</surname></persName>
		</author>
		<ptr target="http://www.mpi-forum.org/docs/mpi-11-html/mpi-report.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Myricom Homepage</surname></persName>
		</author>
		<ptr target="http://www.nas.nasa.gov/Resources/Software/npb.html" />
	</analytic>
	<monogr>
		<title level="j">NAS Parallel Benchmarks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MVAPICH: MPI over InfiniBand, iWARP and RDMAoE</title>
		<ptr target="http://mvapich.cse.ohio-state.edu/" />
		<imprint/>
		<respStmt>
			<orgName>Network-Based Computing Laboratory ; Ohio State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Stream Control Transmission Protocol</title>
		<editor>SCTP),&quot; Editor: R. Stewart</editor>
		<imprint>
			<biblScope unit="volume">4960</biblScope>
			<date type="published" when="2007-09" />
		</imprint>
		<respStmt>
			<orgName>Network Working Group</orgName>
		</respStmt>
	</monogr>
	<note>IETF</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Software Implementation and Testing of iWARP Protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohio Supercomputer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Center</surname></persName>
		</author>
		<ptr target="http://www.osc.edu/research/network_file/projects/iwarp/iwarp_main.shtml" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openfabrics Alliance</surname></persName>
		</author>
		<ptr target="http://www.openfabrics.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">10-Gigabit iWARP Ethernet: Comparative Performance Analysis with InfiniBand and Myrinet-10G</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Afsahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21 st IEEE International Parallel and Distributed Processing Symposium (IPDPS&apos;07)</title>
		<meeting>the 21 st IEEE International Parallel and Distributed Processing Symposium (IPDPS&apos;07)<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
	<note>th Workshop on Communication Architecture for Clusters (CAC&apos;07</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rdma</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="http://www.rdmaconsortium.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An RDMA Protocol Specification (version 1.0),&quot; RDMA Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Culley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Direct Data Placement over Reliable Transports (version 1.0),&quot; RDMA Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinkerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Culley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Message Passing and Shared Address Space Parallelism on an SMP Cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="186" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RDMA over Ethernet -A Preliminary Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Subramoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on High Performance Interconnects for Distributed Computing (HPIDC&apos;09)</title>
		<imprint>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<ptr target="http://www.top500.org/" />
		<title level="m">Top 500 Supercomputer Sites</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
