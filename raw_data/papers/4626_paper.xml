<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LogSig: Generating System Events from Raw Textual Logs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
							<email>taoli@cs.fiu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Shing</forename><surname>Perng</surname></persName>
							<email>perng@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 S.W. 8th Street Miami</addrLine>
									<postCode>33199</postCode>
									<settlement>Florida</settlement>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IBM T.J Watson Research Center</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<addrLine>11200 S.W. 8th Street Miami, 19 Skyline Drive Hawthorne</addrLine>
									<postCode>33199, 10532</postCode>
									<settlement>Florida</settlement>
									<region>NY</region>
									<country>U.S.A, U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LogSig: Generating System Events from Raw Textual Logs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I54 [Pattern Recognition]: Applications</term>
					<term>H4m [Information Systems Applications]: Miscellaneous</term>
					<term>G23 [Discrete Mathematics]: Applications General Terms Algorithms, Experimentation Keywords Event generation, Message signature, System logs</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Modern computing systems generate large amounts of log data. System administrators or domain experts utilize the log data to understand and optimize system behaviors. Most system logs are raw textual and unstructured. One main fundamental challenge in automated log analysis is the generation of system events from raw textual logs. Log messages are relatively short text messages but may have a large vocabulary , which often result in poor performance when applying traditional text clustering techniques to the log data. Other related methods have various limitations and only work well for some particular system logs. In this paper, we propose a message signature based algorithm logSig to generate system events from textual log messages. By searching the most representative message signatures, logSig categorizes log messages into a set of event types. logSig can handle various types of log data, and is able to incorporate human&apos;s domain knowledge to achieve a high performance. We conduct experiments on five real system log data. Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Each log message consists of a sequence of terms. Some of the terms are variables or parameters for a system event, such as the host name, the user name, IP address and so on. Other terms are plain text words describing semantic information of the event. For example, three sample log messages of the Hadoop system <ref type="bibr" target="#b2">[3]</ref> describing one type of events about the IPC (Inter-Process Communication) subsystem are listed below:</p><p>The three messages contain many different words(or terms), such as the date, the hours, the handler name, and the port number. People can identify them as the same event type because they share a common subsequence: "INFO: org.apache.hadoop.ipc.Server: IPC Server:starting ". Let's consider how the three log messages are generated by the system. The Java source code for generating them is described below: logger = Logger.getLogger ("org.apache.hadoop.ipc.Server"); logger.info("IPC Server "+handlerName+": starting");</p><p>where logger is the log producer for the IPC subsystem. Using different parameters, such as handlerName, the code can output different log messages. But the subsequence "INFO: org.apache.hadoop.ipc.Server: IPC Server : starting " is fixed in the source code. It will never change unless the source code has been modified.</p><p>Therefore, the fixed subsequence can be viewed as a signature for an event type. In other words, we can check the signatures to identify the event type of a log message. Other parameter terms in the log message should be ignored, since messages of the same event type can have different parameter terms. Note that some parameters, such as the handlerName in this example, consist of different numbers of terms. Consequently, the position of a message signature may vary in different log messages. Hence, the string matching similarity proposed in <ref type="bibr" target="#b5">[6]</ref> would mismatch some terms. Another method IPLoM proposed in <ref type="bibr" target="#b17">[18]</ref> also fails to partition log messages using the term count since the length of handlerName is not fixed and three log messages have different numbers of terms.</p><p>Given an arbitrary log message, we do not know in advance which item is of its signature, or which term is its parameter. That is the key challenge we aim to address in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>In this paper, we first describe the drawbacks of traditional text clustering techniques for event generation from log messages. We show that, it is difficult for the bag-of-word model to accurately partition log messages. We also analyze that, the string kernel based approach would be inefficient when the log vocabulary size is large. In addition, we discuss some limitations of related approaches proposed in previous literatures.</p><p>Then, we propose logSig algorithm to generate system events from textual log messages. logSig algorithm tries to find k message signatures to match all given messages as much as possible, where k is specified by the user. We conduct experiments on five real system log data. Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.</p><p>The rest of the paper is organized as follows: Section 2 describes the related work about system event generation from textual logs. Then, we formulate the problem of the system events generation in Section 3. In Section 4, we present an overview of the logSig algorithm. Section 5 discusses some detailed implementation issues. Section 6 proposes two approaches for incorporating the domain knowledge to improve the accuracy of the logSig algorithm. In Section 7, we present the experimental studies on five real system log data. Finally, Section 8 concludes our paper and discusses the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>It has been shown in <ref type="bibr" target="#b23">[24]</ref> that log messages are relatively short text messages but could have a large vocabulary size. This characteristic often leads to a poor performance when using the bag-of-words model in text mining on log data. The reason is that, each single log message has only a few terms, but the vocabulary size is very large. Hence, the vector space established on sets of terms would be very sparse. The string kernel can be used to extract deep semantic information (e.g., the order of terms) to improve the performance of the clustering algorithm. It maps a string to a high dimensional vector to represent all possible term orders. However, because of the large vocabulary size, the dimensionality of the transformed space would be very high. Although the kernel trick does not have to explicitly create those high dimensional vectors, clustering algorithms would still be influenced by the high dimensionality due to the Curse of Dimensionality <ref type="bibr" target="#b24">[25]</ref>.</p><p>The related work about the log data analysis can be broadly summarized into two categories. One category is on system event generation from raw log data <ref type="bibr" target="#b5">[6]</ref> [9] <ref type="bibr" target="#b17">[18]</ref>  <ref type="bibr" target="#b25">[26]</ref> and the other category is on analyzing patterns from system events <ref type="bibr" target="#b21">[22]</ref> [30] <ref type="bibr" target="#b12">[13]</ref> [15] <ref type="bibr" target="#b9">[10]</ref> [29] <ref type="bibr" target="#b13">[14]</ref>. Our work in this paper belongs to the first category. A word matching similarity measurement is introduced in <ref type="bibr" target="#b5">[6]</ref> for clustering the log messages. One problem is that, if most terms of a log message are parameter terms, then this type of log messages may not have many matched common words. This method is denoted as StringMatch in this paper. <ref type="bibr" target="#b17">[18]</ref> develops a 4-steps partitioning method IPLoM for clustering the log messages based on inherent characteristics of the log format. However, the method can only be useful for strictly formatted log data. The logSig algorithm proposed in this paper can handle various types of log data without much prior knowledge about the log format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM FORMULATION</head><p>The goal of this paper is to identify the event type of each log message according to a set of message signatures. Given a log message and a set of signatures, we need a metric to determine which signature best matches this log message. Therefore, we propose the Match Score metric first. Notations: Let D be a set of log messages, D = {X1, ..., XN }, where Xi is the ith log message, i = 1, 2, ..., N . Each Xi is a sequence of terms, i.e., Xi = wi 1 wi 2 ....wi n i . A message signature S is also a sequence of terms S = wj 1 wj 2 ....wj n .</p><p>Given a sequence X = w1w2...wn and a term wi, wi ∈ X indicates wi is a term in X. X −{wi} denotes a subsequence w1...wi−1wi+1...wn. |X| denotes the length of the sequence X. LCS(X, S) denotes the Longest Common Subsequence between two sequences X and S. Definition 1. (Match Score) Given a log message Xi and a message signature S, the match score is computed by the function below:</p><formula xml:id="formula_0">match(Xi, S) = |LCS(Xi, S)| − (|S| − |LCS(Xi, S)|) = 2|LCS(Xi, S)| − |S|.</formula><p>Intuitively, |LCS(Xi, S)| is the number of terms in Xi matched with S. |S| − |LCS(Xi, S)| is the number of terms in Xi not matched with S. match(Xi, S) is the number of matched terms minus the number of not-matched terms. We illustrate this by a simple example below: Example 1. A log messages X = abcdef and a message signature S = axcey. The longest common subsequence LCS(X, S) = ace. The matched terms are "a","c","e", shown by underline words in <ref type="table" target="#tab_0">Table 1</ref>. "x" and "y" in S are not matched with any term in X. Hence, match(X, S) = |ace|− |xy| = 1.</p><p>Note that this score can be negative. match(Xi, S) is used to measure the degree of the log message Xi owning the signature S. If two log messages Xi and Xj have the same signature S, then we regard Xi and Xj as of the same event type. The longest common subsequence matching is a widely used similarity metric in biological data analysis <ref type="bibr" target="#b6">[7]</ref> [19], such as RNA sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>If all message signatures S1, S2,...,S k are known, identifying the event type of each log message in D is straightforward. But we don't know any message signature at the beginning. Therefore, we should partition log messages and find their message signatures simultaneously. The optimal result is that, within each partition, every log message matches its signature as much as possible. This problem is formulated below.</p><p>Problem 1. Given a set of log messages D and an integer k, find k message signatures S = {S1, ..., S k } and a k-partition C1,...,C k of D to maximize</p><formula xml:id="formula_1">J(S, D) = k 񮽙 i=1 񮽙 X j ∈C i match(Xj, Si).</formula><p>The objective function J(S, D) is the summation of all match scores. It is similar to the k-means clustering problem. The choice of k depends on the user's domain knowledge to the system logs. If there is no domain knowledge, we can borrow the idea from the method finding k for k-means <ref type="bibr" target="#b10">[11]</ref>, which plots clustering results with k. We can also display generated message signatures for k = 2, 3, .. until the results can be approved by experts.</p><p>Comparing with k-means clustering problem Problem 1 is similar to the classic k-means clustering problem, since a message signature can be regarded as the representative of a cluster. People may ask the following questions: Why we propose the match function to find the optimal partition? Why not use the LCS as the similarity function to do k-means clustering? The answer for the two questions is that, our goal is not to find good clusters of log messages, but to find the message signatures of all types of log messages. K-means can ensure every two messages in one cluster share a subsequence. However, it cannot guarantee that there exists a common subsequence shared by all (or most) messages in one cluster. We illustrate this by the following example.</p><p>Example 2. There are three log messages X1: "abcdef, X2: "abghij" and X3: "xyghef". Clearly, LCS(X1, X2)=2, LCS(X2, X3)=2, and LCS(X1, X3)=2. However, there is no common subsequence that exists among all X1, X2 and X3. In our case, it means there is no message signature to describe all three log messages. Hence, it is hard to believe that they are generated by the same log message template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Analysis</head><p>Problem 1 is an NP-hard problem, even if k = 1. When k = 1, we can reduce the Multiple Longest Common Subsequence problem to the Problem 1. The Multiple Longest Common Subsequence problem is a known NP-hard <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lemma 1. Problem 1 is an NP-hard problem when k = 1.</p><formula xml:id="formula_2">Proof: Let D = {X1, ..., XN }. When k = 1, S = {S1}. Construct another set of N sequences Y = {Y1, ..., YN }, in which each term is unique in both D and Y. Let D 񮽙 = D∪Y, J(S, D 񮽙 ) = 񮽙 X j ∈D match(Xj, S1) + 񮽙 Y l ∈Y match(Y l , S1)</formula><p>Let S * 1 be the optimal message signature for D 񮽙 , i.e.,</p><formula xml:id="formula_3">S * 1 = arg max S 1 J({S1}, D 񮽙 ).</formula><p>Then, the longest common subsequence of X1,...,XN must be an optimal solution S * 1 . This can be proved by contradiction as follows. Let S lcs be the longest common subsequence of X1,...,XN . Note that S lcs may be an empty sequence if there is no common subsequence among all messages. Case 1: If there exists a term wi ∈ S * 1 , but wi / ∈ S lcs . Since wi / ∈ S lcs , wi is not matched with at least one message in X1,...,XN . Moreover, Y1,...,YN are composed by unique terms, so wi cannot be matched with any of them. In D 񮽙 , the number of messages not matching wi is at least N + 1, which is greater than the number of messages matching wi. Therefore,</p><formula xml:id="formula_4">J({S * 1 − {wi}}, D 񮽙 ) &gt; J({S * 1 }, D 񮽙 ),</formula><p>which contradicts with S * 1 = arg max</p><formula xml:id="formula_5">S 1 J({S1}, D 񮽙 ).</formula><p>Case 2: If there exists a term wi ∈ S lcs , but wi / ∈ S * 1 . Since wi ∈ S lcs , X1,...,XN all match wi. The total number of messages that match wi in D 񮽙 is N . Then, there are N remaining messages not matching wi: Y1,...,YN . Therefore,</p><formula xml:id="formula_6">J({S lcs }, D 񮽙 ) = J({S * 1 }, D 񮽙 ),</formula><p>which indicates S lcs is also an optimal solution to maximize objective function J on D 񮽙 . To sum up the two cases above, if there is a polynomial time-complexity solution to find the optimal solution S * 1 in D 񮽙 , the Multiple Longest Common Subsequence problem for X1,...,XN can be solved in polynomial time as well. However, Multiple Longest Common Subsequence problem is an NP-hard problem <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lemma 2. If when k = n Problem 1 is NP-hard, then when k = n + 1 Problem 1 is NP-hard, where n is a positive integer. Proof-Sketch: This can be proved by contradiction. We can construct a message Y whose term set has no overlap to the term set of messages in D in a linear time. Suppose the optimal solution for k = n and D is C = {C1, ..., C k }, then the optimal solution for k = n + 1 and D ∪ {Y } should be C 񮽙 = {C1, ..., C k , {Y }}. If there is a polynomial time solution for Problem 1 when k = n + 1, we could solve Problem 1 when k = n in polynomial time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ALGORITHM OVERVIEW</head><p>In this section, we first present an approximated version of Problem 1 and then present our logSig algorithm. logSig algorithm consists of three steps. The first step is to separate every log message into several pairs of terms. The second step is to find k groups of log messages using local search strategy such that each group share common pairs as many as possible. The last step is to construct message signatures based on identified common pairs for each message group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Term Pair Generation</head><p>The first step of logSig algorithm is converting each log message into a set of term pairs. For example, there is a log message collected from FileZilla <ref type="bibr" target="#b1">[2]</ref> client:</p><p>2010-05-02 11:34:06 Command: mkdir ".indexes"</p><p>We extract each pairwise of terms and preserve the order of two terms. Then, the converted pairs are as follows:</p><p>{2010-05-02,11:34:06}, {2010-05-02,Command:}, {2010-05-02, mkdir}, {2010-05-02, ".indexes" } {11:34:06,Command:}, {11:34:06,mkdir}, {11:34:06,".indexes"}, {Command:,mkdir}, {Command:,".indexes"}, {mkdir,".indexes"}</p><p>The converted term pairs preserve the order information of message terms. On the other hand, the computation on the discrete term pairs is easier than a sequence. A similar idea was proposed in string kernel <ref type="bibr" target="#b15">[16]</ref> for text classification. Their output is a high dimensional vector, and our output is a set of pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Log Messages Partition</head><p>The second step is to partition log messages into k groups based on converted term pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">The Approximated Version of Problem 1</head><p>Notations: Let X be a log message, R(X) denotes the set of term pairs converted from X, and |R(X)| denotes the number of term pairs in R(X).</p><p>Problem 2. Given a set of log messages D and an inte-</p><formula xml:id="formula_7">ger k, find a k-partition C = {C1, ..., C k } of D to maximize objective function F (C, D):</formula><formula xml:id="formula_8">F (C, D) = k 񮽙 i=1 | 񮽙 X j ∈C i R(Xj )|. Object function F (C, D)</formula><p>is the total number of common pairs over all groups. Intuitively, if a group has more common pairs, it is more likely to have a longer common subsequence. Then, the match score of that group would be higher. Therefore, maximizing function F is approximately maximizing J in Problem 1. Lemma 4 shows the average lower bound for this approximation.</p><p>Lemma 3. Given a message group C, it has n common term pairs, then the length of the longest common subsequence of messages in C is at least 񮽙 √ 2n񮽙.</p><p>Proof-sketch: Let l be the length of a longest common subsequence of messages in C. Let T (l) be the number of term pairs that generated by that longest common subsequence.</p><p>Since each term pair has two terms, this sequence can generate at most</p><formula xml:id="formula_9">񮽙 l 2 񮽙 pairs. Hence, T (l) ≤ 񮽙 l 2 񮽙 = l(l − 1)/2.</formula><p>Note that each term pair of the longest common subsequence is a common term pair in C. Now, we already know T (l) = n, so T (l) = n ≤ l(l − 1)/2. Then, we have l ≥ ≥ √ 2n񮽙. Lemma 4. Given a set of log messages D and a k-partition</p><formula xml:id="formula_10">C = {C1, ..., C k } of D, if F (C, D) ≥ y, y is a constant, we</formula><p>can find a set of message signatures S such that on average: </p><formula xml:id="formula_11">J(S, D) ≥ |D| · 񮽙 2y k 񮽙 Proof-sketch: Since F (C, D) ≥ y,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Local Search</head><p>The logSig algorithm applies the local search strategy to solve Problem 2. It iteratively moves one message to another message group to increase the objective function as large as possible. However, unlike the classic local search optimization method, the movement is not explicitly determined by objective function F (·). The reason is that, the value of F (·) may only be updated after a bunch of movements, not just after every single movement. We illustrate this by the following example.</p><p>Example 3. Message set D is composed of 100 "ab" and 100 "cd". Now we have 2-partition C = {C1, C2}. Each message group has 50% of each message type as shown in <ref type="table" target="#tab_2">Table 2</ref>. The optimal 2-partition is C1 has 100 "ab" and C2 X X X X X X X X X term pair group C1 C2</p><p>"ab" 50 50 "cd" 50 50</p><p>has 100 "cd", or in the reverse way. However, beginning with current C1 and C2, F (C, D) is always 0 until we move 50 "ab" from C2 to C1, or move 50 "cd" from C1 to C2. Hence, for first 50 movements, F (C, D) cannot guide the local search because no matter what movement you choose, it is always 0.</p><p>Therefore, F (·) is not proper to guide the movement in the local search. The decision of every movement should consider the potential value of the objective function, rather than the immediate value. So we develop the potential function to guide the local search instead. Notations: Given a message group C, R(C) denotes the union set of term pairs from messages of C. For a term pair r ∈ R(C), N (r, C) denotes the number of messages in C which contains r. p(r, C) = N (r, C)/|C| is the portion of messages in C having r .</p><p>Definition 2. Given a message group C, the potential of C is defined as φ(C),</p><formula xml:id="formula_12">φ(C) = 񮽙 r∈R(C) N (r, C)[p(r, C)] 2 .</formula><p>The potential value indicates the overall "purity" of term pairs in C. φ(C) is maximized when every term pair is contained by every message in the group. In that case, for each r, N (r, C) = |C|, φ(C) = |C| · |R(C)|. It also means all term pairs are common pairs shared by every log message. φ(C) is minimized when each term pair in R(C) is only contained by one message in C. In that case, for each r,</p><formula xml:id="formula_13">N (r, C) = 1, |R(C)| = |C|, φ(C) = 1/|C|. Definition 3. Given a k-partition C = {C1, ..., C k } of a message set D, the overall potential of D is defined as Φ(D), Φ(D) = k 񮽙 i=1 φ(Ci),</formula><p>where φ(Ci) is the potential of Ci, i = 1, ..., k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection between Φ and F :</head><p>Objective function F computes the total number of common term pairs in each group. Both Φ and F are maximized when each term pair is a common term in its corresponding message group. Let's consider the average case.</p><p>Lemma 5. Given a set of log messages D and a k-partition Lemma 5 implies, in the average case, if we try to increase the value of F to be at least y, we have to increase the overall potential Φ to be at least y · |D|/k. As for the local search algorithm, we mentioned that Φ is easier to optimize than</p><formula xml:id="formula_14">C = {C1, ..., C k } of D, if F (C, D) ≥ y,</formula><formula xml:id="formula_15">F . Let ΔiX − → j Φ(D) denote the increase of Φ(D) by moving X ∈ D from group Ci into group Cj , i, j = 1, ..., k, i 񮽙 = j. Then, by Definition 3, ΔiX − → j Φ(D) = [φ(Cj ∪ {X}) − φ(Cj)] −[φ(Ci) − φ(Ci − {X})],</formula><p>where φ(Cj ∪ {X}) − φ(Cj) is the potential increase brought by inserting X to Cj , φ(Ci)−φ(Ci−{X}) is the potential loss brought by removing X from Ci. Algorithm 1 is the pseudocode of the local search algorithm in logSig. Basically, it iteratively updates every log message's group according to ΔiX − → j Φ(D) to increase Φ(D) until no more update operation can be done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 logSig_localsearch (D, k)</head><p>Parameter: D : log messages set; k: the number of groups to partition; Result: C : log message partition;</p><p>1: C ← RandomSeeds(k) 2: C 񮽙 ← ∅ // Last iteration's partition 3: Create a map G to store message's group index 4: for C i ∈ C do 5:</p><formula xml:id="formula_16">for X j ∈ C i do 6: G[X j ] ← i 7:</formula><p>end for 8: end for 9: while C 񮽙 = C 񮽙 do 10:</p><formula xml:id="formula_17">C 񮽙 ← C 11: for X j ∈ D do 12: i ← G[X j ]</formula><p>13:</p><formula xml:id="formula_18">j * = arg max j=1,..,k Δ iX − → j Φ(D)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>if i 񮽙 = j * then 15:</p><formula xml:id="formula_19">C i ← C i − {X j } 16: C j * ← C j * ∪ {X j } 17: G[X j ] ← j * 18</formula><note type="other">: end if 19: end for 20: end while 21: return C Why choose this Potential Function? Given a message group C, let g</note><formula xml:id="formula_20">(r) = N (r, C)[p(r, C)] 2 , then φ(C) = 񮽙 r∈R(C) g(r)</formula><p>. Since we have to consider all term pairs in C, we define φ(C) as the sum of all g(r). As for g(r), it should be a convex function. <ref type="figure" target="#fig_1">Figure 1</ref> shows a curve of g(r) by varying the number of messages having r, i.e., N (r, C). The reason for why g(r) is convex is that, we hope , |C| = 100 to give larger awards to r when r is about to being a common term pair. That is because, if N (r, C) is large, then r is more likely to be a common term pair. Only when r becomes a common term pair, it can increase F (·). In other words, r has more potential to increase the value of objective function F (·), so the algorithm should pay more attention to r first.</p><p>In the experimental section, we will empirically compare the effectiveness of our proposed potential function Φ with the objective function function F in the local search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Message Signature Construction</head><p>The final step of logSig algorithm is to construct the message signature for each message group. Recall that a message signature is a sequence of terms that has a high match score to every message in the corresponding group. So it could be constructed by highly frequent term pairs identified in the second step.</p><p>Lemma 6. Let S be an optimal message signature for a message group C, the occurrence number of every term wj ∈ S must be equal or greater than 񮽙|C|/2񮽙.</p><p>The proof of Lemma 6 is similar to the proof of Lemma 1. If there exists a term w 񮽙 j ∈ S only appearing in less than 񮽙|C|/2񮽙 messages, we can have: J({S − {w 񮽙 j }}, C) &gt; J({S}, C), then S is worse than S − {w 񮽙 j }. Thus, S is not optimal.</p><p>Lemma 6 indicates that we only need to care about those terms which appear in at least one half of the messages in a group. By scanning every message in a group, we could obtain the optimal sequence of those terms. Since log messages are usually very short, there are only a few term whose occurrence number is equal or greater than 񮽙|C|/2񮽙. Therefore, there are not many candidate sequences generated by those terms. We enumerate each one of them and select the best one to be the message signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ALGORITHM IMPLEMENTATION</head><p>In this section, we discuss some detailed issues about the implementation of the logSig algorithm.</p><p>Term Pair Histogram To efficiently compute the potential value of each message group Ci ∈ C, a histogram is maintained for each group. The histogram is implemented by a hash table, whose key is a term pair and the value is the number of messages containing that term pair in the group. Then, for a term pair r, c(r, Ci) and p(r, Ci) can be obtained in a constant time complexity.</p><p>Approximately Computing ΔΦ(D) The straightforward way to compute ΔiX </p><formula xml:id="formula_21">− → j Φ(D) is O( 񮽙 k l=1 |R(C l )|)</formula><p>, which is the total number of term pairs in the entire data set. In log data, this number is even tens of times greater than |D|. Hence, this computation cost is not affordable for each single log message.</p><p>Our approximated solution is to consider the change of N (r, C), for each r ∈ R(X). The reason is that, |C| is large. The impact to |C| by inserting or removing one message could be ignored comparing to the impact to N (r, C). |C)| ≈ |C| + 1 ≈ |C| − 1. So |C| can be treated as a constant in one exchange operation in local search. Then,</p><formula xml:id="formula_22">∂φ(C) ∂N(r, C) = ∂[[N (r, C)] 3 /|C| 2 ] ∂N(r, C) = 3[N (r, C)] 2 |C| 2 = 3[p(r, C)] 2 . ΔiX − → j Φ(D) ≈ 3 · 񮽙 r∈R(X) [[p(r, Cj)] 2 − [p(r, Ci)] 2 ].</formula><p>By utilizing the histograms, p(r, Cj) and p(r, Ci) can be obtained in a constant time. Hence, for each log message X ∈ D, the time complexity of finding the largest ΔiX</p><formula xml:id="formula_23">− → j Φ(D) is at most O(k · |R(X)|). Although ΔiX − → j Φ(D)</formula><p>is would underestimate the actual change of Φ(D), it can save a lot of computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithm Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convergence and Local Optima</head><p>In the local search algorithm, exchanging messages' groups only increases the overall potential Φ(D). There is no operation to decrease the overall potential. Φ(D) is not infinite, which is less than or equal to |R(D)| · |D|. Therefore, the local search algorithm would converge.</p><p>Similar to other local search based optimization algorithms, logSig may converge into a local optima as well. However, logSig's potential function provides a more reliable heuristic to guide the optimization process. It is much less likely to stop at a local optima.</p><p>Time Complexity and Space Complexity The time complexity of converting log messages into term pairs is O(|D| · L 2 ), where L is the maximum length of log messages. For every Xj ∈ D, L 2 ≥ |R(Xj )|. For the local search, each iteration goes through every message. So the time complexity of an iteration is O(k · L 2 · |D|). Let t be the number of iterations, so the time complexity of the local search is O(t · k · L 2 · |D|). Our experiments shows that t is usually 3 to 12 for most system log data.</p><p>The space cost of the algorithm is mainly determined by the term pair histograms created for every message group. The total space complexity is the total number of term pairs, O(|R(D)|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">INCORPORATING DOMAIN KNOWLEDGE</head><p>In real world applications, analyzing system behaviors mainly relies on the generated system events, so the event generation algorithm should be reliable. Current approaches for grouping log messages into different event types are totally unsupervised. In practice, for most sorts of logs (e.g., Hadoop system logs), there are some domain knowledge about a fixed catalog of Java exceptions. In addition, the log generation mechanisms implicitly create some associations between the terminologies and the situations. Incorporating domain knowledge into the clustering process could improve the performance of event generation. In this section, we present two approaches for incorporating domain knowledge to improve the accuracy of logSig algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Term Feature Layer</head><p>Some terms or words in log messages share some common features which can help our algorithm identify the log message type. The features are domain knowledge from human experts. For example, "2011-02-01" and "2011-01-02" are different terms, but they are both dates; "192.168.0.12" and "202.119.23.10" are different terms, but they are both IP addresses.</p><p>logSig algorithm allows users to provide a list of features. Each feature is described by a regular expression. An additional feature layer is built to incorporate those features for representing log messages. The feature layer is created by regular expression matching. For example, we have following regular expressions 1 :</p><p>Timestamp : \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} FilePath : "(\w|\\)+"</p><p>Then, we could scan the log message X1 to create the feature layer Y1 as shown in <ref type="table" target="#tab_3">Table 3</ref>. The feature layer Y1 contains </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Constraints on Message Signature</head><p>In reality, domain experts usually identify the event type of a log message by scanning keywords or phrases. For example, as for SFTP/FTP logs, they would be sensitive to those phrases: "Command", "No such file", "Error", "Transfer failed" and so on. Those sensitive phrases should be included in the generated message signature to express system events. On the other hand, domain experts also know that some trivial phrases should be ignored, such as the message IDs and the timestamps. Those trivial phrases should not be included in any message signature. To sum up, those knowledge can be transferred as constraints on message signatures. Those constraints can help the event generation algorithm to improve its accuracy. Unlike traditional constraints in semi-supervised clustering, such as Must-Link or CannotLink <ref type="bibr" target="#b27">[28]</ref>, our constraints are placed in the subsequences (or phrases) of messages, not on the messages themselves.</p><p>Constraint-based logSig Algorithm To incorporate constraints on message phrases, Problem 1 can be revised as follows:</p><p>Problem 3. Given a set of log messages D, a set of sensitive phrases PS and trivial phrases PT , find k message signatures S = {S1, ..., S k } with a k-partition C1,...,C k of D to maximize:</p><formula xml:id="formula_24">J 񮽙 (S, D, PS, PT ) = λ k 񮽙 i=1 񮽙 X j ∈C i match(Xj, Si) + (1 − λ) k 񮽙 i=1 [ 񮽙 Ps∈P S match(Ps, Si) − 񮽙 Ps∈P T match(Pt, Si)],</formula><p>where λ is a user-defined parameter between 0 and 1.</p><p>The revised optimization problem can be solved by constraintbased logSig Algorithm. The basic idea of constraint-based logSig is to increase the weight of term pairs in R(PS) and decrease the weight of term pairs in R(PT ). Let wr denote the weight of pair r. The only revised part in logSig algorithm is ΔiX − → j Φ(D). By multiplying the weight wr, it becomes:</p><formula xml:id="formula_25">ΔiX − → j Φ(D) ≈ 3 񮽙 r∈R(X) [[p(r, Cj)] 2 − [p(r, Ci)] 2 ] · wr, and wr = ⎧ ⎨ ⎩ 1.0, r / ∈ R(PS), r / ∈ R(PT ) λ 񮽙 , r ∈ R(PS), r / ∈ R(PT ) 1/λ 񮽙 , r / ∈ R(PS), r ∈ R(PT ) ,</formula><p>where λ 񮽙 ≥ 1 is a user-defined parameter that can be approximately derived from the original parameter λ. λ 񮽙 is utilized to increase(and decrease) the importance of term pairs in sensitive phrases(and trivial phrases). The choice of λ 񮽙 depends on users' confidence in those sensitive phrases and trivial phrases. In experiments, we let λ 񮽙 = 10.0. We show the performances of the algorithm when varying λ 񮽙 from 0.0 to 20.0 and explain why we choose 10 in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Platforms</head><p>We implement our algorithm and other comparative algorithms in Java 1.6 platform. <ref type="table" target="#tab_4">Table 5</ref> summarizes our experimental enviroment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Data Collection</head><p>We collect log data from 5 different real systems, which are summarized in <ref type="table" target="#tab_5">Table 6</ref>. Logs of FileZilla <ref type="bibr" target="#b1">[2]</ref>, PVFS2 <ref type="bibr" target="#b3">[4]</ref> Apache <ref type="bibr" target="#b0">[1]</ref> and Hadoop <ref type="bibr" target="#b2">[3]</ref> are collected from the server machines/systems in the computer lab of a research center. Log data of ThunderBird <ref type="bibr" target="#b4">[5]</ref> is collected from a supercomputer in Sandia National Lab. The true categories of log messages are obtained by specialized log parsers. For instance, FillZilla's log messages are categorized into 4 types: "Command ", "Status", "Response", "Error ". Apache error log messages are categorized by the error type: "Permission denied ", "File not exist" and so on. The vocabulary size is an important characteristic of log data. <ref type="figure" target="#fig_3">Figure 2</ref> exhibits the vocabulary sizes of the 5 different logs along with the data size. It can be seen that some vocabulary size could become very large if the data size is large. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparative Algorithms</head><p>We compare our algorithm with 7 alternative algorithms in this experiment. Those algorithms are described in <ref type="table" target="#tab_7">Table  7</ref>. 6 of them are unsupervised algorithms which only look at the terms of log messages. 3 of them are semi-supervised algorithms which are able to incorporate the domain knowledge. <ref type="bibr">IPLoM [18]</ref> and StringMatch <ref type="bibr" target="#b5">[6]</ref> are two methods proposed in recent related literatures . VectorModel <ref type="bibr" target="#b22">[23]</ref>, Jaccard <ref type="bibr" target="#b24">[25]</ref>, StringKernel <ref type="bibr" target="#b15">[16]</ref> are traditional methods for text clustering. VectorModel and semi-StringKernel are implemented by k-means clustering algorithm <ref type="bibr" target="#b24">[25]</ref>. Jaccard and StringMatch are implemented by k-medoid algorithm <ref type="bibr" target="#b11">[12]</ref>, since they cannot compute the centroid point of a cluster. As for Jaccard, the Jaccard similarity is obtained by a hash table to accelerate the computation. VectorModel and StringKernel use Sparse Vector <ref type="bibr" target="#b22">[23]</ref> to reduce the computation and space costs.  Message signature based method proposed in this paper semi-logSig logSig incorporating domain knowledge semi-StringKernel Weighted string kernel based k-means semi-Jaccard</p><p>Weighted Jaccard similarity based k-medoid semi-logSig, semi-StringKernel and semi-Jaccard are semi-supervised versions of logSig, StringKernel and Jaccard respectively. To make a fair comparison, all those semisupervised algorithms incorporate the same domain knowledge offered by users. Specifically, the 3 algorithms run on the same transformed feature layer, and the same sensitive phrases PS and trivial phrases PT . Obviously, the choices of features, PS and PT have a huge impact to the performances of semi-supervised algorithms. But we only compare a semisupervised algorithm with other semi-supervised algorithms. Hence, they are compared under the same choice of features, PS and PT . The approaches for those 3 algorithms to incorporate with features, PS and PT are described as follows:</p><p>Feature Layer: Replacing every log message by the transformed sequence of terms with features.</p><p>PS and PT : As for semi-StringKernel, replacing Euclidean distance by Mahalanobis distance <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_26">DM (x, y) = 񮽙 (x − y) T M (x − y).</formula><p>where matrix M is constructed according to term pairs PS, PT and λ 񮽙 . As for semi-Jaccard, for each term, multiply a weight λ 񮽙 (or 1/λ 񮽙 ) if this term appears in PS ( or PT ).</p><p>Jaccard, StringMatch and semi-Jaccard algorithms apply classic k-medoid algorithm for message clustering. The time complexity of k-medoid algorithm is very high: O(tn 2 ) <ref type="bibr" target="#b26">[27]</ref>, where t is the number of iterations, n is the number of log messages. As a result, those 3 algorithms are not capable of handling large log data. Therefore, for the accuracy comparison, we split our log files into smaller files by time frame, and conduct the experiments on the small log data. The amounts of log messages, features, term pairs in PS and PT are summarized in <ref type="table" target="#tab_8">Table 8</ref>. In Section 7.5, larger logs are used to test the scalability. <ref type="table" target="#tab_6">Table 4</ref> shows the accuracy comparison of generated system events by different algorithms. The accuracy is evaluated by F-measure (F1 score) <ref type="bibr" target="#b22">[23]</ref>, which is a traditional metric combining precision and recall. Since the results of k-medoid, k-means and logSig depend on the initial random seeds, we run each algorithm for 10 times, and put the average F-measures into <ref type="table" target="#tab_6">Table 4</ref>. From this table, it can be seen that StringKernel and logSig outperform other algorithms in terms of the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Quality of Generated Events</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Average F-Measure</head><p>Jaccard and VectorModel apply the bag-of-word model, which ignores the order information about terms. Log messages are usually short, so the information from the bag-ofword model is very limited. In addition, different log messages have many identical terms, such as date, username. That's the reason why the two methods cannot achieve high F-measures. IPLoM performs well in ThunderBird log data, but poorly in other log data. The reason is that, the first step of IPLoM is to partition log message by the term count. One type of log message may have different numbers of terms. For instance, in FileZilla logs, the length of Command messages depends on the type of SFTP/FTP command in the message. But for ThunderBird, most event types are strictly associated with one message format. Therefore, IPLoM could easily achieve the highest score.</p><p>Due to the Curse of dimensionality <ref type="bibr" target="#b24">[25]</ref>, k-means based StringKernel is not easy to converge in a high dimensional space. <ref type="figure" target="#fig_3">Figure 2</ref> shows that, 50K ThunderBird log messages contain over 30K distinct terms. As a result, the transformed space has over (30K) 2 = 900M dimensions. It is quite sparse for 50K data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Message Signatures</head><p>Generated message signatures are used as descriptors for system events, so that users can understand the meanings of those events. Due to the space limit, we cannot list all   <ref type="table" target="#tab_9">Table 9</ref> shows generated signatures of FileZilla and Apache Error by semi-logSig, in which features are indicated by italic words. As for FileZilla log, each message signature corresponds to a message category, so that the F-measure of FileZilla could achieve 1.0. But for Apache Error log, Only 4 message signatures are associated with corresponding categories. The other 2 signatures are generated by two ill-partitioned message groups. They cannot be associates with any category of Apache Error logs. As a result, their "Associated Category" in <ref type="table" target="#tab_9">Table 9</ref> are "N/A". Therefore, the overall F-measure on Apache error log in <ref type="table" target="#tab_6">Table 4</ref> is only 0.7707.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Parameter Setting</head><p>All those algorithms have the parameter k, which is the number of events to create. We let k be the actual number of message categories. String kernel method has an additional parameter λ, which is the decay factor of a pair of terms. We use StringKernel λ to denote the string kernel method using decay factor λ. In our experiments, we set up string kernel algorithms using three different decay factors: StringKernel0.8, StringKernel0.5 and StringKernel0.3.</p><p>As for the parameter λ 񮽙 of our algorithm logSig, we set λ 񮽙 = 10 based on the experimental result shown by <ref type="figure">Figure 6</ref>. For each value of λ 񮽙 , we run the algorithm for 10 times, and plot the average F-measure in this figure. It can be seen that, the performance becomes stable when λ 񮽙 is greater than 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.4">Effectiveness of Potential Function</head><p>To evaluate the effectiveness of the potential function Φ, we compare our proposed logSig algorithm with another logSig algorithm which uses the objective function F to guide its local search. <ref type="figure">Figure 7</ref> shows the average Fmeasures of the two algorithms on each data set. Clearly, our proposed potential function Φ is more effective than F in all data sets. In addition, we find logSig algorithm using F always converges within 2 or 3 iterations. In other words, F is more likely to stop at a local optima in the local search. The reason for this has been discussed in Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Scalability</head><p>Scalability is an important factor for log analysis algorithms. Many high performance computing systems generate more than 1Mbytes log messages per second <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Fig- ure 3</ref>, <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref> show the average running time comparison for all algorithms on the data sets with different sizes. We run each algorithm 3 times and plot the average running times. IPLoM is the fastest algorithm. The running times of other algorithms depend on the number of iterations. Clearly, k-medoid based algorithms are not capable of handling large log data. Moreover, StringKernel is not efficient even though we use Sparse Vector to implement the computation of its kernel functions. We keep track of its running process, and find out the low speed convergence is mainly due to the high dimensionality. <ref type="figure">Figure 8</ref> shows the scalability of logSig algorithm on ThunderBird logs and Apache Error logs. Its actual running time is approximated linear with the log data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we show the drawbacks of traditional methods and previous log message analyzing methods. To address the limitations of existing methods, we propose logSig, a message signature based method for events creation from system log messages. logSig utilizes the common subsequence information of messages to partition and describe the events generated from log messages.</p><p>As for the future work, we will integrate the structural learning techniques into our framework to capture the structural information of log messages. We hope those structures could improve the performance of the logSig algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>y is a constant, then in the average case, Φ(D) ≥ y · |D|/k. Proof-sketch: Since F (C, D) ≥ y, there are at least y com- mon term pairs distributed in message groups. For each common term pair ri, let Ci be its corresponding group. On average, |Ci| = |D|/k. Note that the common pair ri ap- pears in every message of Ci, so N (ri, Ci) = |Ci| = |D|/k and p(ri, Ci) = 1. There are at least y common term pairs, by Definition 2, we have Φ(D) ≥ y · |D|/k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Function g(r), |C| = 100 to give larger awards to r when r is about to being a common term pair. That is because, if N (r, C) is large, then r is more likely to be a common term pair. Only when r becomes a common term pair, it can increase F (·). In other words, r has more potential to increase the value of objective function F (·), so the algorithm should pay more attention to r first. In the experimental section, we will empirically compare the effectiveness of our proposed potential function Φ with the objective function function F in the local search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>j</head><label></label><figDesc>Φ(D) is enumer- ating all term pairs in Ci and Cj. The computation cost of finding the maximum ΔiX</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Vocabulary size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 8 :</head><label>38</label><figDesc>Figure 3: Average Running Time for FileZilla logs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Example of Match Score 

X a b c d e f 
S a x c 
e y 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>񮽙 2y k 񮽙. If we choose this longest common subsequence as the message signature, each log message can match at least 񮽙 񮽙 2y k 񮽙 terms of the signature. As a result, the match score of each log message is at least 񮽙 񮽙 2y k 񮽙. D has |D| messages.(C, D) is approx- imately maximizing the original objective function J(S, D). But F (C, D) is</head><label></label><figDesc>easier to optimize because it deals with dis- crete pairs.</figDesc><table>on average, each group 
has at least y/k common pairs. Then for each group, by 
Lemma 3, the length of the longest common subsequence 

must be at least 񮽙 

Then, we have the total match score J(S, D) ≥ 
|D| · · 
񮽙 

2y 
k 

񮽙 on average. 

Lemma 4 shows that, maximizing the F </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Example of two message groups</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Example of Feature Layer X 1 2010-05-02 11:34:06 Command: mkdir ".indexes" Y 1 Timestamp Command: mkdir FilePath more semantic information.</head><label>3</label><figDesc></figDesc><table>It can gather similar terms and 
reduce noisy terms. In addition, regular expression is easy 
for domain experts to write. Using a sophisticated regular 
language toolkit, a feature layer can be built efficiently. 
1 The grammar of regular expressions is defined by Java Reg-
ular Library 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Experimental Machine</head><label>5</label><figDesc></figDesc><table>OS 
CPU 
bits Memory JVM Heap Size 
Linux 2.6.18 Intel Xeon(R) @ 
2.5GHz, 8 core 

64 
16G 
12G 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 : Summary of Collected System Logs</head><label>6</label><figDesc></figDesc><table>System 
Description 
#Messages #Terms 
Per Mes-
sage 

#Category 

FileZilla 
SFTP/FTP Client 
22,421 
7 to 15 
4 
ThunderBird Supercomputer 
3,248,239 
15 to 30 
12 
PVFS2 
Parallel File System 95,496 
2 to 20 
11 
Apache 
Error 

Web Server 
236,055 
10 to 20 
6 

Hadoop 
Parallel Computing 
Platform 

2,479 
15 to 30 
11 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Average F-Measure Comparison 
h h h h h h h h h h h h h 

Algorithm 
Log Data FileZilla PVFS2 ThunderBird Apache Error Hadoop 

Jaccard 
0.3794 
0.4072 
0.6503 
0.7866 
0.5088 
VectorModel 
0.4443 
0.5243 
0.4963 
0.7575 
0.3506 
IPLoM 
0.2415 
0.2993 
0.8881 
0.7409 
0.2015 
StringMatch 
0.5639 
0.4774 
0.6663 
0.7932 
0.4840 
StringKernel0.8 
0.4462 
0.3894 
0.6416 
0.8810 
0.3103 
StringKernel0.5 
0.4716 
0.4345 
0.7361 
0.9616 
0.3963 
StringKernel0.3 
0.4139 
0.6189 
0.8321 
0.9291 
0.4256 
logSig 
0.6949 
0.7179 
0.7882 
0.9521 
0.7658 

semi-Jaccard 
0.8283 
0.4017 
0.7222 
0.7415 
0.4997 
semi-StringKernel0.8 
0.8951 
0.6471 
0.7657 
0.8645 
0.7162 
semi-StringKernel0.5 
0.7920 
0.4245 
0.7466 
0.8991 
0.7461 
semi-StringKernel0.3 
0.8325 
0.7925 
0.7113 
0.8537 
0.6259 
semi-logSig 
1.0000 
0.8009 
0.8547 
0.7707 
0.9531 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 : Summary of comparative algorithms</head><label>7</label><figDesc></figDesc><table>Algorithm 
Description 
VectorModel 
Vector space model proposed in information re-
treival 
Jaccard 
Jaccard similarity based k-medoid algorithm 
StringKernel 
String kernel based k-means algorithm 
IPLoM 
Iterative partition method proposed in [18] 
StringMatch 
String matching method proposed in [6] 
logSig 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 8 : Summary of small log data</head><label>8</label><figDesc></figDesc><table>Measure 
#Message 
#Feature 
|R(P) S | 
|R(P) T | 
FileZilla 
8555 
10 
4 
4 
ThunderBird 
5000 
10 
11 
9 
PVFS2 
12570 
10 
10 
1 
Apache Error 
5000 
2 
4 
2 
Hadoop 
2479 
2 
7 
3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 9 : Message SignaturesStatus: ... Status Date Hours Number Number Response: Number Response Date Hours Number Number Command: Command Date Hours Number Number Error: File transfer failed Error Apache Error Timestamp ( 13 ) Permission denied: /home/bear-005/users/xxx/public_html/ke/.htaccess pcfg_openfile: unable to check htaccess file ensure it is readable Permission denied Timestamp Error [ client ] File does not exist: /opt/website/sites/users.cs.fiu.edu/ data/favicon.ico File does not exist</head><label>9</label><figDesc></figDesc><table>System Log Message Signature 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work is supported in part by NSF grants IIS-0546280 and HRD-0833093.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Http</forename><surname>Apache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Server</surname></persName>
		</author>
		<ptr target="http://httpd.apache.org/" />
	</analytic>
	<monogr>
		<title level="j">An Open-Source HTTP Web Server</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">FileZilla: An open-source and free FTP/SFTP solution</title>
		<ptr target="http://filezilla-project.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hadoop : An Open-Source MapReduce computing platform</title>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PVFS2 : The state-of-the-art parallel I/O and high performance virtual file system</title>
		<ptr target="http://pvfs.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://www.cs.sandia.gov/~jrstear/logs/" />
		<title level="m">ThunderBird: A supercomputer in Sandia National Laboratories</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One graph is worth a thousand logs: Uncovering hidden structures in massive system event logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mordechai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML/PKDD</title>
		<meeting>ECML/PKDD<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RNA multiple structural alignment with longest common subsequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bereg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Optimization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating constraints and metric learning in semi-supervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incremental learning of system log formats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="90" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling probabilistic measurement correlations for problem determination in large-scale distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Distributed Computing Systems (ICDCS&apos;09)</title>
		<meeting>the 29th International Conference on Distributed Computing Systems (ICDCS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="623" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the k in k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Data Mining: Concepts and Techniques, 2ed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering actionable patterns in event data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Perng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="493" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructing comprehensive summaries of large event sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiernan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Terzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM KDD</title>
		<meeting>ACM KDD<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="417" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An integrated framework on mining logs files for computing system management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM KDD</title>
		<meeting>ACM KDD<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="page" from="776" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text classification using string kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The complexity of some problems on subsequences and supersequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="322" to="336" />
			<date type="published" when="1978-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering event logs using iterative partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makanju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Zincir-Heywood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Milios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM KDD</title>
		<meeting>ACM KDD<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1255" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding patterns in biological sequences by longest common subsequencesand shortest common supersequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BIBE</title>
		<meeting>BIBE<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Alert detection in system logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Oliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stearley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of ICDM</title>
		<meeting>cedings of ICDM<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="page" from="959" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What supercomputers say: A study of five system logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Oliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stearley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DSN 2007</title>
		<meeting>DSN 2007<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Event summarization for system management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Perng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM KDD</title>
		<meeting>ACM KDD<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08" />
			<biblScope unit="page" from="1028" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards informatic analysis of syslogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stearley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Cluster Computing</title>
		<meeting>IEEE International Conference on Cluster Computing<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-09" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-N</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LogTree: A framework for generating system events from raw textual logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
		<meeting>ICDM<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">S. Theodoridis and r. e. Konstantinos Koutroumbas. Pattern Recognition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schrodl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithmic approach to event summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD</title>
		<meeting>ACM SIGMOD<address><addrLine>Indianapolis, Indiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining console logs for large-scale system problem detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
