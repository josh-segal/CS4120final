<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Value-Function Approximation via Online Linear Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
							<email>lihong@cs.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
							<email>mlittman@cs.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University Piscataway</orgName>
								<address>
									<postCode>08854</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Value-Function Approximation via Online Linear Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One of the key problems in reinforcement learning (RL) is balancing exploration and exploitation. Another is learning and acting in large or even continuous Markov decision processes (MDPs), where compact function approximation has to be used. In this paper, we provide a provably efficient , model-free RL algorithm for finite-horizon problems with linear value-function approximation that addresses the exploration-exploitation tradeoff in a principled way. The key element of this algorithm is the use of a hypothesized online linear-regression algorithm in the recently proposed KWIK framework. We show that, if the sample complexity of the KWIK online linear-regression algorithm is polynomial, then the sample complexity of exploration of the RL algorithm is also polynomial. Such a connection provides a promising approach to efficient RL with function approximation via studying a simpler setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the key problems in reinforcement learning <ref type="bibr">(Sut- ton &amp; Barto 1998</ref>) is the exploration-exploitation tradeoff, which strives to balance two competing types of behavior of an autonomous agent in an unknown environment: the agent can either make use of its current knowledge about the environment to maximize its cumulative reward (i.e., to exploit), or sacrifice short-term rewards to gather information about the environment (i.e., to explore) in the hope of increasing future long-term return.</p><p>Exploration can be framed as a dual control problem, and (in principle) can be solved optimally in a Bayesian manner. However, this approach is computationally intractable and it is often not obvious how to select a prior distribution for learning <ref type="bibr" target="#b7">(Duff 2002)</ref>. We only consider non-Bayesian approaches in this paper. <ref type="bibr" target="#b30">Thrun (1992)</ref> surveyed a number of popular exploration rules, including the competencemap approach for continuous MDPs <ref type="bibr">(Thrun &amp; M ¨ oller 1992</ref>), but little can be said about their performance guarantees. In fact, some of them have provably poor performance in certain situations. Recently, there has been a growing interest in formally analyzing the sample complexity of exploration <ref type="bibr" target="#b11">(Kakade 2003</ref>) in finite-state Markovian environments. This line of work has significantly advanced understanding of the exploration-exploitation dilemma, but has not been merged with approaches for function approximation needed for scaling up. In contrast, this paper is concerned with intelligent exploration in large or even continuous environments where compact function approximation has to be used. In particular, we focus on the special case where the value function is represented as a linear combination of predefined features. 1 In contrast to previous work on linear value-function approximation, our algorithm explicitly addresses the question of balancing exploration and exploitation, and we formally analyze its sample complexity. This algorithm works by reducing a finite-horizon reinforcement-learning problem to a sequence of related online linear regression problems, each of which is solved by a hypothesized admissible algorithm, denoted A 0 , in the recently proposed KWIK framework <ref type="bibr" target="#b25">(Strehl &amp; Littman 2008)</ref>. Roughly speaking, A 0 is admissible if it predicts the target value of an example near-accurately except in a polynomial (in relevant quantities that we will make clear) number of examples where it signals "I don't know".</p><p>The main contribution of this paper is an efficient reduction to KWIK online linear regression from reinforcement learning with linear value-function approximation and an efficient, built-in exploration scheme. This reduction shows how important questions in reinforcement learning, such as the sample complexity of exploration and value-function approximation error, may depend on the related quantities in the simpler setting of KWIK online linear regression. Thus, this connection allows us to study a simpler problem as a means to solve the significantly more difficult RL problem.</p><p>Although A 0 is hypothetical right now, a close relative has been successfully created by <ref type="bibr" target="#b25">Strehl &amp; Littman (2008)</ref>. In particular, the existing algorithm requires that the function to be learned be precisely linear, whereas our algorithm must be tolerant to nearly linear target functions. However, the close relationship between the two problems gives us hope that A 0 can be created under some mild conditions. The rest of the paper is organized as follows. Section 2 defines the KWIK online regression problem and specifies the conditions for the admissible algorithm A 0 . Section 3 reviews the RL notation briefly, and then describes the reduction in detail. We provide a few theoretical results including the sample complexity of exploration as well as error bounds for the learned linear value functions, both of which scale up nicely. Finally, Section 4 discusses the relationship between this work to previous results, and Section 5 concludes the paper by pointing out a few research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KWIK Online Linear Regression</head><p>KWIK stands for "Know What It Knows", and represents a new framework for learning that is particularly well suited for use in RL settings. An essential element of a KWIK learner is that it is able to compute certain quantities to measure how confident it is in its predictions. A simple example is confidence bounds for parameter estimation, which is widely used in statistics and machine learning. Such confidence information is particularly useful for a few purposes. In reinforcement learning, for instance, confidence bounds are used to select actions to guide exploration <ref type="bibr" target="#b2">(Auer 2002;</ref><ref type="bibr" target="#b24">Strehl &amp; Littman 2005)</ref>.</p><p>We first define the KWIK Online Regression Framework, adopting terminology from <ref type="bibr" target="#b25">Strehl &amp; Littman (2008)</ref>. We use 񮽙x񮽙 to denote the Euclidean norm of a vector x ∈ R d where d ∈ N is the dimension. Definition 1 At timestep t = 1, 2, 3, · · · , a KWIK online regression agent acts according to the following protocol:</p><formula xml:id="formula_0">• First, it receives an input vector x t ∈ R d . • Second, it provides an outputˆyoutputˆ outputˆy t ∈ [−1, 1] ∪ {Ξ}, where</formula><p>Ξ is a special value indicating that the agent is not confident in its prediction and thus refuses to predict a numeric value between −1 and 1. We calî y t valid ifˆyifˆ ifˆy t 񮽙 = Ξ.</p><p>• Finally, the agent observes the (possibly noisy) ground truth</p><formula xml:id="formula_1">y t ∈ [−1, 1].</formula><p>The problem becomes a KWIK online linear regression problem if we must impose certain assumptions on the functional relation between x t and y t . Assumption 1 We make the following assumptions for the KWIK online regression problem:</p><formula xml:id="formula_2">A. (Bounded-input assumption) 񮽙x t 񮽙 ≤ 1 for all t. B. (Semi-linearity assumption) There exist some (unknown) vector w * ∈ R d and a small number ξ ∈ [0, 1) such that 񮽙w * 񮽙 ≤ 1 and |E[y t |x t ] − w * · x t | ≤ ξ for all t.</formula><p>We call the quantity ξ the slack value. Assumption 1A is reasonable as practical problems often have bounded inputs and we can re-scale the inputs so that this assumption holds. Assumption 1B essentially states that the target function being learned is "almost" linear, and the distance to being linearity is measured by the slack value ξ. This assumption is less restrictive than it might appear at the first glance. In practice, with an expanded set of features we can often approximate a learning target by a function linear in the features. This is a common trick to capture nonlinearity via linear functions in many situations including kernelbased learning <ref type="bibr" target="#b24">(Shawe-Taylor &amp; Cristianini 2004</ref>).</p><p>Note that we make no assumption on the sequence of inputs x t , except that 񮽙x t 񮽙 is at most 1. In particular, x t can depend on previous inputs {x 1 , · · · , x t−1 }, which is fundamentally different from the usual i.i.d. assumption made in supervised-learning problems (e.g., see Hastie, <ref type="bibr" target="#b8">Tibshirani, &amp; Friedman (2003)</ref>). As we shall see later, this difference is important when we move to the RL setting in the next section. We next define admissibility of KWIK online regression algorithms.</p><p>Definition 2 A KWIK online regression algorithm A is admissible if, for any given 񮽙, δ &gt; 0, the following two conditions are satisfied with probability at least 1 − δ:</p><p>• Whenever A predicts a validˆyvalidˆ validˆy t 񮽙 = Ξ, we have that</p><formula xml:id="formula_3">|ˆy|ˆy t − E[y t |x t ]| ≤ 񮽙 + ξ.</formula><p>• The number of timesteps t for whichˆywhichˆ whichˆy t = Ξ is bounded by some function ζ(d, 1//, 1/δ) that is polynomial in d, 1//, and 1/δ. We call ζ the sample complexity of A. An admissible, polynomial-time algorithm is proposed recently by <ref type="bibr" target="#b25">Strehl &amp; Littman (2008)</ref> for KWIK online linear regression when ξ = 0; namely, the target function, E[y t |x t ], is a linear function of x t . When we allow ξ &gt; 0, however, the problem becomes significantly more difficult, as illustrated by the following example.</p><formula xml:id="formula_4">Example 1 Fix any ξ &gt; 񮽙 &gt; 0. The input dimension is d = 1</formula><p>, and the target function f to learn is the zero function (which is trivially linear): f (x) ≡ 0. Let the input at time t be x t = min{tβ, 1} for some small β &gt; 0, then the corresponding output is y t = f (x t ) = 0. Assume that the learner knows f is noise free. But since it does not know that f is exactly linear, it has to predict conservatively to handle the worst-case situations. At time t where ξ β &lt; t &lt; 1 β , the learner has to say "I don't know" since, based on the training data up to time t − 1, the possible range of y t , which is</p><formula xml:id="formula_5">[− 2tξ t−1 , 2tξ t−1 ]</formula><p>, is too wide to guarantee a prediction error of at most ξ + 񮽙. By letting β ↓ 0, the number of Ξ does not depend on 񮽙 or ξ, and is unbounded.</p><p>It may be appealing to change Definition 2 to tolerate a prediction error of cξ+񮽙 for some constant c &gt; 1. While this modification may allow admissible KWIK online regression algorithms which may be useful on its own, it will lead to an exponential growth of value-function approximation error in the reduction given in Section 3. Therefore, in order to allow for the existence of algorithms that are admissible in the sense of Definition 2, we have to make certain assumptions on the process that generates the data [񮽙x t , y t 񮽙] t∈N . Such assumptions may place restrictions on, for example, the accumulative loss of the best linear hypothesis, w * · x, on the sequence of data. Whether these assumptions are reasonable depends on the applications at hand. For the purpose of this paper, they are denoted abstractly by C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 2</head><p>Under certain conditions C on the process generating the samples [񮽙x t , y t 񮽙] t∈N , there exists a KWIK online linear regression algorithm A 0 that is admissible and takes polynomial running time in every timestep. We denote its sample complexity by ζ 0 (d, 1//, 1/δ), and its perstep computation complexity by τ 0 (d, 1//, 1/δ).</p><p>In the rest of the paper, we assume that condition C holds whenever we apply the base algorithm A 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning with Linear Value-Function Approximation</head><p>Given the background terminology and assumptions in the previous section, we consider reinforcement learning that involves sequential prediction and control. After a brief introduction to notation and terminology, we describe a reduction to KWIK online linear regression from RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We consider environments modeled as finite-horizon Markov decision processes <ref type="bibr" target="#b23">(Puterman 1994)</ref>, or MDPs for short. An MDP M can be described by a six-tuple 񮽙S, A, T, R, H, µ 0 񮽙, where S is a set of states, A is a finite set of actions, T is the transition function with T (s, a, s 񮽙 ) denoting the probability of reaching s 񮽙 from s by taking action a, R is a bounded reward function with R(s, a) ∈ [0, 1] denoting the expected immediate reward gained by taking action a in state s, H ∈ N is the horizon, and µ 0 is a start-state distribution. <ref type="bibr">2</ref> An MDP is said to be finite (or infinite) if the state space S is finite (or infinite). For convenience, define the set of stages as</p><formula xml:id="formula_6">[H] = {1, 2, · · · , H}.</formula><p>An episode is a sequence of H state transitions: </p><formula xml:id="formula_7">񮽙s 1 , a 1 , r 1 , s 2 , · · · , s H , a H , r H , s H+1 񮽙,</formula><formula xml:id="formula_8">π : S × [H] → A.</formula><p>Specifically, π(s, h) ∈ A is the action the agent will take if s is the current state at stage h. Given a policy π, we define the state-value function, V π h (s), as the expected cumulative reward received by executing π starting from state s at stage h until the episode terminates at stage H. Similarly, the state-action value function (a.k.a. the Q-function), Q π h (s, a), is the expected cumulative reward received by taking action a in state s at stage h and following π until the episode terminates at stage H. A reinforcement-learning agent attempts to learn an optimal policy π * whose value functions at stage h are denoted by V * h (s) and Q * h (s, a), respectively. It is known that</p><formula xml:id="formula_9">V * h = max π V π h and Q * h = max π Q π h .</formula><p>A greedy policy at stage h, denoted π Q h , with respect to a value function Q h is one that selects actions with maximum Q-values; namely, π Q h (s, h) = arg max a Q h (s, a). The greedy policy with respect to Q * h is optimal for stage h. The Bellman equation plays a central role to many RL algorithms including the one we will describe: for any s ∈ S, a ∈ A, h ∈ [H],</p><formula xml:id="formula_10">Q * h (s, a) = R(s, a) + 񮽙 s 񮽙 ∈S 񮽙 T (s, a, s 񮽙 ) max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙</formula><p>where Q * H+1 is understood to be the zero function.</p><p>Given the complete model of a finite MDP (i.e., the sixtuple), standard algorithms exist for finding the optimal value function and the optimal policy, including linear programming, value iteration, and policy iteration <ref type="bibr" target="#b23">(Puterman 1994)</ref>. However, if the transition and/or reward functions are unknown, the agent has to learn the optimal value function or policy by interacting with the environment. Algorithms such as Q-learning with 񮽙-greedy exploration (Sutton &amp; Barto 1998) do not address the exploration problem efficiently and may be highly inefficient in some domains <ref type="bibr" target="#b30">(Thrun 1992;</ref><ref type="bibr" target="#b17">Koenig &amp; Simmons 1996)</ref>.</p><p>Recently, there has been a growing interest in formally analyzing the efficiency of exploration strategies in finite MDPs. For any fixed 񮽙, Kakade (2003) defines the sample complexity of exploration of an RL algorithm A to be the number of timesteps t such that the non-stationary policy at time t, A t , is not 񮽙-optimal from the current state s t at time t (i.e., V At (s t ) ≤ V * (s t ) − 񮽙). An algorithm A is then said to be PAC-MDP (Probably Approximately Correct in Markov Decision Processes) if, for any 񮽙 &gt; 0 and δ ∈ (0, 1), its sample complexity of exploration is less than some polynomial in |S|, |A|, 1//, 1/δ, and 1/(1 − γ), with probability at least 1 − δ ( <ref type="bibr" target="#b26">Strehl et al. 2006</ref>). Examples of PAC-MDP algo- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Efficient RL with Linear Value Functions</head><p>In this subsection, we propose an algorithm, A H , for Hhorizon reinforcement learning in which a linear value function is used. In particular, we assume a set of d features are predefined: φ : S × A 񮽙 → [−1, 1] d . A Q-function can then be represented compactly by a weight vector</p><formula xml:id="formula_11">w h ∈ R d for each h ∈ [H]: ˆ Q h (s, a) = w h · φ(s, a)</formula><p>. Such a linear approximation scheme is widely used to solve large-scale RL problems <ref type="bibr" target="#b18">(Lagoudakis &amp; Parr 2003;</ref><ref type="bibr" target="#b27">Sutton &amp; Barto 1998)</ref>.</p><p>Similarly to the previous section, we make a semilinearity assumption for the value function at every stage. Remember that outputs in the KWIK online regression (c.f., Definition 1) are in [−1, 1], we need to re-scale the value function to the same range by dividing Q * h by H:</p><formula xml:id="formula_12">Assumption 3 (Semi-linearity assumption) For every stage h ∈ [H], there exist some (unknown) vector w * h ∈ R d and a small number ξ h &gt; 0 such that 񮽙w * h 񮽙 ≤ 1 and 񮽙 񮽙 񮽙 񮽙 Q * h (s, a) H − w * h · φ(s, a) 񮽙 񮽙 񮽙 񮽙 ≤ ξ h<label>(1)</label></formula><p>for all s and a. Whether it is required to know ξ h depends on whether such information is needed by A 0 .</p><p>Algorithm 1 gives a formal description of A H . Basically, A H learns the optimal value functions Q * h by treating them as H related KWIK online linear regression problems. It runs H copies of the base algorithm A 0 to update the weight vector w h for each stage h. By the Bellman equation, the Q-function at stage h is defined recursively as the sum of immediate reward at stage h and the expected optimal Q-value of the next states. Therefore, the algorithm im-</p><formula xml:id="formula_13">S 1 S 2 S 3 S 4</formula><p>Stage h = 2:</p><p>(w 2 is used to compute q 2,L and q 2,R ) (3) choose greedy a 2 = R since both q 2,L and q 2,R are valid (4) use r 1 + q 2,a2 to update w 1 as this backup value is "trusted"</p><formula xml:id="formula_14">Stage h = 1:</formula><p>(w 1 is used to compute q 1,L and q 1,R )</p><p>(1) choose exploratory a 1 = L since</p><formula xml:id="formula_15">q 1,L = Ξ</formula><p>(2) no backup is needed as this is the first horizon</p><p>Stage h = 3:</p><p>(w 3 is used to compute q 3,L and q 3,R )</p><p>(5) choose exploratory a 3 = R since q 3,R = Ξ (6) do not use r 2 +q 3,a3 to update w 2 as this value is "unknown" proves its value-function estimates by performing Bellmanbackup-style updates. A central idea behind the efficiency of the reduction is that we only use a backup value when it is "known". A backup value is "known" when the prediction made by A 0 is valid, and thus by Assumption 2 must be near-accurate to the true value. <ref type="figure">Figure 1</ref> gives a simple H-horizon example for H = 3. It illustrates how to choose actions and how to select backup values to do learning. A quick observation about A H is that, if the per-step computation complexity of A 0 is τ 0 , then the per-step computation complexity of A H is O(|A| τ 0 ), when execution of lines 17-21 are amortized to every timestep. So, the perstep computation complexity scales nicely from regression problems to sequential decision making in MDPs, in contrast to algorithms such as sparse sampling <ref type="bibr">(Kearns, Man- sour, &amp; Ng 2002</ref>) that scales exponentially in the horizon length. We next turn to the more difficult questions of sample complexity of exploration and value-function approximation error bounds. Observe state s h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 Suppose Assumption 3 holds. If</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for all a ∈ A do 6:</p><formula xml:id="formula_16">Use A (h) 0 to compute q h,a ∈ [0, H] ∪ {Ξ} as a prediction for Q * h (s h , a). Here, if A (h) 0</formula><p>gives a valid prediction, then this prediction has to be multiplied by H to obtain q h,a due to the normalization (1) we have used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>end for <ref type="bibr">8:</ref> if q h,a = Ξ for some a ∈ A then 9:</p><formula xml:id="formula_17">a h ← a // do exploration 10: L h ← FALSE // Q * h (s h , a h ) is "unknown"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>else <ref type="bibr">12:</ref> a h ← arg max a q h,a // do exploitation 13: </p><formula xml:id="formula_18">L h ← TRUE // Q * h (s h , a</formula><formula xml:id="formula_19">H 񮽙 h=1 񮽙 h · ζ 0 񮽙 d, 1 񮽙 h , 1 δ h 񮽙񮽙 ;</formula><p>III. With probability at least 1 − 񮽙 H l=1 δ l , all valid Q-value predictions at stage h differ from the true values by at most</p><formula xml:id="formula_20">H · H 񮽙 l=h (񮽙 l + ξ l ) .</formula><p>Before proving this theorem, we first mention a few implications of it. The following corollary, which follows immediately from Theorem 1, indicates that the sample complexity and error bounds of the KWIK online linear regression algorithm A 0 scale nicely to the analogous quantities in the more complicated, H-horizon RL problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 1</head><p>If we let 񮽙 h = 񮽙 0 , δ h = δ 0 , and ξ h = ξ 0 for all stage h in Theorem 1, then:</p><formula xml:id="formula_21">I. The number of Ξ outputted at stage h is O 񮽙 Hζ 0 񮽙 d, 1 񮽙0 , 1 δ0 񮽙񮽙 ;</formula><p>II. The total number of Ξ outputted during the whole run of</p><formula xml:id="formula_22">A H in all stages is O 񮽙 H 2 ζ 0 񮽙 d, 1 񮽙0 , 1 δ0 񮽙񮽙 ;</formula><p>III. With probability at least 1 − Hδ 0 , all valid Q-value predictions at stage h differ from the true values by at most</p><formula xml:id="formula_23">H(H − h + 1) (񮽙 0 + ξ 0 ) = O(H 2 (񮽙 0 + ξ 0 )).</formula><p>Using Corollary 1, we can prove the following theorem about the sample complexity of exploration of A H . Our focus is to provide the first polynomial sample complexity bound, although it is possible to improve the bounds using a more careful analysis. </p><formula xml:id="formula_24">O 񮽙 H 3 񮽙 ζ 0 񮽙 d, H 3 񮽙 , H δ 񮽙 log 1 δ 񮽙</formula><p>episodes, with probability at least 1 − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>Due to space limitation, we only provide proof sketches for the theorems given in the previous subsection. Before proving the sample complexity of exploration bound, we first provide two useful lemmas. The first is simple and the proof is omitted.</p><p>Lemma 1 Let f 1 and f 2 be two real-valued functions on the same finite domain X; namely, f i :</p><formula xml:id="formula_25">X 񮽙 → R, for i = 1, 2. If max x∈X |f 1 (x) − f 2 (x)| ≤ ∆ for some ∆ &gt; 0, then |max x∈X f 1 (x) − max x∈X f 2 (x)| ≤ ∆.</formula><p>Lemma 2 Let π be a policy for an H-horizon MDP. Let s 1 be a fixed start state of an episode, and s h be the state visited at stage h of this episode. Then,</p><formula xml:id="formula_26">V * 1 (s 1 ) − V π 1 (s 1 ) = E π 񮽙 H 񮽙 h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π(s h , h)) 񮽙 񮽙 ,</formula><p>where E π stands for the expectation with respect to the probability distributions of trajectories ρ = [s 1 , s 2 , · · · , s H , s H+1 ] generated by policy π.</p><p>PROOF. We let r h denote the reward received at stage h by following π. Note that both s h and r h are random variables whose distributions are completely determined by policy π as s 1 is fixed. Then,</p><formula xml:id="formula_27">V * 1 (s 1 ) = Q * 1 (s 1 , π * (s 1 , 1)) = Q * 1 (s 1 , π(s 1 , 1)) + 񮽙 Q * 1 (s 1 , π * (s 1 , 1)) − Q * 1 (s 1 , π(s 1 , 1)) 񮽙 = E π [r 1 + V * 2 (s 2 )] + 񮽙 Q * 1 (s 1 , π * (s 1 , 1)) − Q * 1 (s 1 , π(s 1 , 1)) 񮽙 .</formula><p>We apply the derivation above for V * h (s h ) recursively up to stage H, and obtain</p><formula xml:id="formula_28">V * 1 (s 1 ) = E π [r 1 + r 2 + · · · + r H ] +E π 񮽙 񮽙 H h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π(s h , h)) 񮽙񮽙 . By definition, V π 1 (s 1 ) = E π [r 1 + r 2 + · · · + r H ]</formula><p>, which immediately proves the lemma. 񮽙 Proof of Theorem 1. The theorem can be proved by mathematical induction. For h = H, the theorem is ensured by Assumption 2. For the induction step, assume the theorem holds for all stages l &gt; h where h &lt; H and we consider stage h. Due to operations of Algorithm 1, the transitions from s h to s h+1 in all episodes can be categorized into two groups: (i) L h+1 = TRUE, and (ii) L h+1 = FALSE. Transitions belonging to case (i) consist of a stream of data for A (h) 0 to run according to the KWIK online regression protocol defined in Definition 1, and Assumption 2 guarantees that there are at most ζ 0 (d, 1// h , 1/δ h ) timesteps for which Ξ is outputted.</p><p>On the other hand, by induction hypothesis, case (ii) happens at most 񮽙 H l=h+1 ζ 0 (d, 1// l , 1/δ l ) times. Therefore, the total number of Ξ outputted in stage h is at most</p><formula xml:id="formula_29">ζ 0 (d, 1// h , 1/δ h ) + H 񮽙 l=h+1 ζ 0 (d, 1// l , 1/δ l ),</formula><p>which is what we desire to prove for part I.</p><p>Part II follows directly from part I. For part III, the target function to learn at stage h is given by˜Q</p><formula xml:id="formula_30">by˜ by˜Q h (s, a) = R(s, a) + 񮽙 s 񮽙 ∈S T (s, a, s 񮽙 ) max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ),</formula><p>wherê Q h+1 is the function learned by A H in stage h + 1. <ref type="bibr">3</ref> By the induction hypothesis, we have</p><formula xml:id="formula_31">񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h+1 (s 񮽙 , a 񮽙 ) − Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 ≤ H 񮽙 H l=h+1 (񮽙 l + ξ l ) for all (s 񮽙 , a 񮽙 ) whenever Ξ is not outputted. LetˆQLetˆ LetˆQ h be 3</formula><p>Strictly speaking, ˆ Q h+1 may change over time and thus˜Qthus˜ thus˜Q h is not a stationary learning target. But, this fact does not affect our analysis as long asˆQasˆ asˆQ h+1 is always bounded between</p><formula xml:id="formula_32">Q * h+1 − H P H l=h+1 (񮽙 l + ξ l ) and Q * h+1 + H P H l=h+1 (񮽙 l + ξ l ). the function A (h) 0</formula><p>learns, then for any (s, a) we have</p><formula xml:id="formula_33">񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − ˜ Q h (s, a) 񮽙 񮽙</formula><p>񮽙 ≤ H(񮽙 h + ξ h ) due to Assumptions 2 and 3. Combining all these facts, we have for any (s, a):</p><formula xml:id="formula_34">񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − Q * h (s, a) 񮽙 񮽙 񮽙 ≤ 񮽙 񮽙 񮽙ˆQ񮽙ˆ 񮽙ˆQ h (s, a) − ˜ Q h (s, a) 񮽙 񮽙 񮽙 + 񮽙 񮽙 񮽙˜Q񮽙˜ 񮽙˜Q h (s, a) − Q * h (s, a) 񮽙 񮽙 񮽙 ≤ H (񮽙 h + ξ h ) + 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 s 񮽙 ∈S T (s, a, s 񮽙 ) 񮽙 max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ) − max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 ≤ H(񮽙 h + ξ h ) + max s 񮽙 ∈S 񮽙 񮽙 񮽙 񮽙 max a 񮽙 ∈AˆQ ∈Aˆ ∈AˆQ h+1 (s 񮽙 , a 񮽙 ) − max a 񮽙 ∈A Q * h+1 (s 񮽙 , a 񮽙 ) 񮽙 񮽙 񮽙 񮽙 ≤ H(񮽙 h + ξ h ) + H H 񮽙 l=h+1 (񮽙 l + ξ l ) = H H 񮽙 l=h (񮽙 l + ξ l ),</formula><p>where the last inequality is due to Lemma 1. </p><formula xml:id="formula_35">1 − 񮽙 H h=1 δ h = 1 − δ 2 , we have for each h, Q * h (s h , π * (s h , h)) − Q * h (s h , π i (s h , h)) ≤ Q * h (s, π * (s h , h)) − ˆ Q h (s h , π i (s h , h)) +O 񮽙 H 2 (񮽙 h + ξ h ) 񮽙 = Q * h (s, π * (s h , h)) − ˆ Q h (s h , π i (s h , h)) + O 񮽙 񮽙 H 񮽙 ≤ Q * h (s, π * (s h , h)) − ˆ Q h (s h , π * (s h , h)) + O 񮽙 񮽙 H 񮽙 ≤ O 񮽙 H 2 (񮽙 h + ξ h ) 񮽙 + O 񮽙 񮽙 H 񮽙 = O 񮽙 񮽙 H 񮽙 , (2)</formula><p>where the first and last inequalities are due to Corollary 1(III), and the second due to the fact that π i is greedy with respect tô Q h when no Ξ is outputted. For any fixed start state s 1 , Lemma 2 asserts that</p><formula xml:id="formula_36">V * 1 (s 1 ) − V πi 1 (s 1 ) = E πi 񮽙 H 񮽙 h=1 񮽙 Q * h (s h , π * (s h , h)) − Q * h (s h , π i (s h , h)) 񮽙 񮽙 .</formula><p>Combined with Equation (2) and the fact that case (i) happens with probability p i , the equality above implies</p><formula xml:id="formula_37">V * 1 (s 1 ) − V πi 1 (s 1 ) = O(񮽙 + Hp i ). When p i ≤ p 0 for some threshold p 0 = O( 񮽙 H ), we have V * 1 (s 1 ) − V πi 1 (s 1 ) = O(񮽙) and also E s1∼µ0 [V * (s 1 ) − V πi (s 1 )] = O(񮽙), indicating that the policy π i is indeed O(񮽙)-optimal.</formula><p>We claim that with high probability p i &gt; p 0 will hold only a polynomial number of episodes. Specifically, Corollary 1 asserts that Ξ is outputted O <ref type="figure" target="#fig_3">(H 2 ζ 0 (d, H 3</ref> 񮽙 , H δ )) times. Using the inequality of <ref type="bibr" target="#b9">Hoeffding (1963)</ref>, with probability at least 1 − δ 2 , the number of episodes i with p i &gt; p 0 is</p><formula xml:id="formula_38">O 񮽙 H 2 ζ 0 (d, H 3 񮽙 , H δ ) p 0 log 1 δ 񮽙 .</formula><p>Substituting p 0 = O( 񮽙 H ) and applying the union bound to the two cases (p i &gt; p 0 and p i ≤ p 0 ) gives the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">An Extension to Discounted RL</head><p>While we have focused on finite-horizon RL problems in this paper, it is often easier to model environments by discounted MDPs <ref type="bibr" target="#b23">(Puterman 1994;</ref><ref type="bibr" target="#b27">Sutton &amp; Barto 1998</ref>), which are specified by a five-tuple, 񮽙S, A, T, R, γ񮽙, where γ ∈ [0, 1) is a discount factor. Changes in notation and terminology are necessary since there is no notion of horizon in this setting. Specifically, we only need to consider stationary policies and value functions: a policy is a mapping from states to actions: π : S 񮽙 → A; the value functions, such as Q π γ (s, a) and Q * γ (s, a), are defined as the expected cumulative γ-discounted reward.</p><p>An observation for discounted MDPs is that rewards in the future are exponentially down-weighted. Since rewards are bounded, rewards received after a large number of timesteps contribute little to the value of the current state. Therefore, we may transform a γ-discounted MDP M γ into an H-horizon MDP M H so that the optimal value functions of M H and M γ differ by at most 񮽙, provided</p><formula xml:id="formula_39">H = Ω 񮽙 log 1 񮽙(1−γ) 1 − γ 񮽙 .</formula><p>It is worth mentioning that even if the optimal value function in M γ , Q * γ , is near linear, the intermediate value functions in M H , Q * h , need not be near linear. We note that this problem may be resolved by using different sets of features at different stages. That is, we require features φ h : S × A 񮽙 → [−1, 1] d at stage h, and assume that Q * h satisfies Assumption 3 with small slack ξ h . Algorithm A H can then be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work in this paper is most related to the original KWIK online linear regression framework proposed by <ref type="bibr" target="#b25">Strehl &amp; Littman (2008)</ref>. The only difference in problem formulation is that we make a semi-linearity assumption while they assume exact linearity. This change is necessary if we allow Bellman-backup-style updates on the value functions since the backup value (i.e., is unavoidably biased and it is unreasonable to assume the target function at stage h − 1 remains linear for all possible biases introduced in stage h. The second significant difference is how KWIK online linear regression is applied to RL problems. <ref type="bibr" target="#b25">Strehl &amp; Littman (2008)</ref> adopt a model-based approach: they assume the MDP state transitions are governed by a set of linear equations with white, Gaussian noise, and then apply KWIK online linear regression to learn the transition matrices, and finally solve the learned MDP model to obtain a policy that either explores or exploits. Even if an MDP can be accurately modelled as a linear system and approximate planning is concerned, however, solving a continuous MDP remains a challenging task <ref type="bibr" target="#b6">(Chow &amp; Tsitsiklis 1989;</ref><ref type="bibr" target="#b13">Kearns, Mansour, &amp; Ng 2002;</ref><ref type="bibr" target="#b16">Kocsis &amp; Szepesvári 2006</ref>). In contrast, the model-free approach taken in this paper avoids this problem completely by learning the value function directly. With a learned linear value function, finding the greedy action takes only O(|A| d) time per step.</p><p>Another related work is metric-E 3 (Kakade, <ref type="bibr" target="#b10">Kearns, &amp; Langford 2003)</ref>, which also addresses the problem of efficient exploration in continuous MDPs. They also use a model-based approach, and develop sample complexity of exploration in terms of the so-called cover number of an MDP-a number that describes how complex the MDP is. Similarly, they also make an assumption on the availability of an efficient, continuous MDP solver, which might limit the use of their algorithm in practice for the same reason.</p><p>The KWIK online linear regression framework we described is related to the online learning model of linear functions, where a rich set of beautiful results have been established in the last two decades (e.g., the works by <ref type="bibr" target="#b3">Bernstein (1992)</ref>, Cesa-Bianchi, <ref type="bibr" target="#b5">Long, &amp; Warmuth (1996)</ref>, <ref type="bibr" target="#b14">Kivinen &amp; Warmuth (1997)</ref>, <ref type="bibr" target="#b15">Klasner &amp; Simon (1995)</ref>, <ref type="bibr" target="#b19">Littlestone, Long, &amp; Warmuth (1995)</ref>, <ref type="bibr" target="#b20">Long (1997), and</ref><ref type="bibr">Vovk (1998)</ref>). In this model, input data are not assumed to be i.i.d. (as what the paper does), and the outputs are roughly a linear function of the inputs. Cumulative absolute and squared error bounds are developed under various assumptions. The main difference between that model and ours is that we require the learner to be aware of the accuracy of its prediction. However, ideas and results in the online learning area may turn out useful for our framework as well.</p><p>Recently, <ref type="bibr" target="#b22">Peters &amp; Schaal (2007)</ref> proposes an interesting reduction from RL to reward-weighted regression. While they consider the specific task of following a given trajectory in rigid-body systems whose dynamics are governed by a set of equations with unknown parameters, this paper focuses on learning in general MDPs where the learner is not provided with such near-optimal trajectories.</p><p>A H is similar to delayed Q-learning ( <ref type="bibr" target="#b26">Strehl et al. 2006</ref>) in that both of them allow a value backup to happen only if the backup value is "trusted". In finite MDPs, it is sufficient for delayed Q-learning to become confident by averaging over a large set of samples. In the case of using linear function approximation, A H relies on A 0 to do so. In turn, A 0 is expected to resort to more complicated reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The connection between KWIK online regression and reinforcement learning opens a number of interesting directions. First, we are currently developing a concrete algorithm A 0 under mild conditions C, and plan to compare it against the standard RL algorithms with other exploration strategies. With such an algorithm at hand, it is possible to improve the bounds provided in Section 3.2 to make them more practical.</p><p>Second, it is important to find more efficient reductions for discounted RL that do not involve an intermediate conversion to an H-horizon problem, as described in Section 3.4. Also, a more careful analysis is needed for the discounted case.</p><p>Third, it is observed that a similar reduction in Algorithm 1 applies to KWIK online nonlinear regression. This fact allows us to use more expressive classes of nonlinear value functions, which may lead to better policies in some problems.</p><p>Finally, we expect fruitful applications of KWIK online regression to (associative) bandit problems <ref type="bibr">(Abe, Bier- mann, &amp; Long 2003;</ref><ref type="bibr" target="#b1">Auer 2000;</ref><ref type="bibr" target="#b2">2002;</ref><ref type="bibr" target="#b20">Long 1997</ref>). These problems are classic settings for studying the explorationexploitation dilemma, and have found important applications in the fast growing market of Internet sponsored search (e.g., <ref type="bibr" target="#b7">(Gonen &amp; Pavlov 2007)</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Copyright c 񮽙 2007, authors listed above. All rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>rithms include E 3 (Kearns &amp; Singh 2002), RMAX (Brafman &amp; Tennenholtz 2002), MBIE (Strehl &amp; Littman 2005), and delayed Q-learning (Strehl et al. 2006).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 7 )Figure 1 :</head><label>71</label><figDesc>Figure 1: An illustration of the operations of A H in a 3-horizon MDP. Two actions are allowed in every state: {L, R}. The same notation as in Algorithm 1 is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proof for Theorem 2 .</head><label>2</label><figDesc>For episode i, let p i be the probabil- ity of entering some state s for which Ξ is outputted, when start states are drawn from µ 0 . Denote by π i the policy used by A H in episode i. LetˆQLetˆ LetˆQ h be the value-function estimate of the algorithm for stage h. Consider any state trajectory ρ = [s 1 , s 2 , · · · , s H , s H+1 ] generated by policy π i . Two situations can occur: (i) Ξ is outputted (maybe multiple times) in ρ, and (ii) Ξ is not out- putted in ρ. The probabilities of cases (i) and (ii) are p and 1 − p, respectively. When case (ii) happens, with probability at least</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>r</head><label></label><figDesc>h−1 +q h,a h H in line 19 of Algorithm 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>񮽙 h and δ h in Algorithm 1, then: I. The number of Ξ outputted in stage h ∈ [H] is at most H</head><label></label><figDesc></figDesc><table>A 

(h) 
0 

is run 
with parameters 񮽙 

l=h 

ζ 0 
񮽙 
d, 
1 
񮽙 l 
, 
1 
δ l 
񮽙 
; 

II. The total number of Ξ outputted during the whole run of 

Algorithm 1 Algorithm A H for H-horizon reinforcement 
learning by a reduction to A 0 . The base algorithm A 0 is 
run for each stage h to maintain a separate weight vector 
w h ∈ R d so that the value-function estimate at stage h isˆQ isˆ isˆQ h (s, a) = H · w h · φ(s, a). 
0: Inputs: A, H, φ, 񮽙 h , and δ h for h ∈ [H]. 
1: Initialize H copies of A 0 , one for each h ∈ [H]. The 
copy at stage h is run with parameters 񮽙 h and ξ h , and is 
denoted by A 

(h) 

0 . 
2: for episode i = 1, 2, 3, · · · do 

3: 

for stage h = 1, 2, 3, · · · , H do 

4: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Theorem 2 Given any 񮽙, δ &gt; 0, assume we can find a set of d features so that Assumption 3 holds with ξ h = O( 񮽙 H 3 ). If we run A (h) 1 with 񮽙 h = O( 񮽙 H 3 ) and δ h = δ 2H in Algorithm 1, then the policy used by the agent is 񮽙-optimal except</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Examples of features used in the linear-value-functionapproximation context include polynomial features, radial basis functions, and tile coding, etc. (Lagoudakis &amp; Parr 2003; Sutton 1996; Sutton &amp; Barto 1998; van Roy 1998). In this paper, we do not address the problem of selecting or constructing features, which is itself a challenging problem. In practice, good features are often designed by domain experts, and may also be automatically generated (Parr et al. 2007).</note>

			<note place="foot" n="2"> In general, an H-horizon MDP may have transition probabilities and reward function dependent on the stage. We choose a simpler definition for ease of exposition. The results and analysis in the paper apply to the general case with minor modifications.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We appreciate the helpful discussion with Alex Strehl. The anonymous reviewers also provided constructive comments and links to relevant works that have improved the paper. The work is primarily supported by NSF-ITR-0325281.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reinforcement learning with immediate rewards and linear hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Biermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="293" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An improved on-line algorithm for learning linear evaluation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Annual Conference on Computational Learning Theory (COLT-00)</title>
		<meeting>the Thirteenth Annual Conference on Computational Learning Theory (COLT-00)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitationexploration trade-offs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Absolute error bounds for learning linear functions online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Conference on Computational Learning Theory (COLT-92)</title>
		<meeting>the Fifth Annual Conference on Computational Learning Theory (COLT-92)</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-maxa general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Worst-case quadratic loss bounds for prediction using linear functions and gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="604" to="619" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The complexity of dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="466" to="488" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An incentive-compatible multi-armed bandit mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Amherst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimal Learning: Computational Procedures for Bayes-Adaptive Markov Decision Processes. Ph.D. Dissertation</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the Twenty-Sixth Annual ACM Symposium on Principles of Distributed Computing</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Elements of Statistical Learning: Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploration in metric state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML-03</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the Sample Complexity of Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A sparse sampling algorithm for near-optimal planning in large Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="193" to="208" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exponentiated gradient versus gradient descent for linear predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From noise-free to noise-tolerant and from on-line to batch learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Klasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Conference on Computational Learning Theory (COLT-95)</title>
		<meeting>the Eighth Annual Conference on Computational Learning Theory (COLT-95)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bandit based MonteCarlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Machine Learning (ECML-06)</title>
		<meeting>the Seventeenth European Conference on Machine Learning (ECML-06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="227" to="250" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Least-squares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On-line learning of linear functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On-line evaluation and prediction using linear functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Annual Conference on Computational Learning Theory (COLT-97)</title>
		<meeting>the Tenth Annual Conference on Computational Learning Theory (COLT-97)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing feature generation for value-function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Painter-Wakefield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML-07</title>
		<meeting>the Twenty-Fourth International Conference on Machine Learning (ICML-07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforcement learning by reward-weighted regression for operational space control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML-07</title>
		<meeting>the Twenty-Fourth International Conference on Machine Learning (ICML-07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theoretical analysis of model-based interval estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Conference on Machine Learning (ICML-05)</title>
		<meeting>the Twenty-Second Conference on Machine Learning (ICML-05)</meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
	<note>Kernel Methods for Pattern Analysis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online linear regression and its application to model-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PAC model-free reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Machine Learning (ICML-06)</title>
		<meeting>the Twenty-Third International Conference on Machine Learning (ICML-06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="881" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Successful examples using sparse coarse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8 (NIPS-95</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Active exploration in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 4 (NIPS-91)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Van Nostrand Reinhold. 527-559. van Roy, B. 1998. Learning and Value Function Approximation in Complex Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches</title>
		<editor>White, D. A., and Sofge, D. A.</editor>
		<meeting><address><addrLine>Cambridge, MA. Vovk, V</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="364" to="370" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. Dissertation</note>
	<note>Advances in Neural Information Processing Systems 10 (NIPS-97)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
