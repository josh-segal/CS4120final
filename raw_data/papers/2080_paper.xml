<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributed I/O with ParaMEDIC: Experiences with a Worldwide Supercomputer *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
							<email>balaji@mcs.anl.gov‡</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mathematical and Computing Science</orgName>
								<orgName type="institution" key="instit1">North Carolina State University</orgName>
								<orgName type="institution" key="instit2">Virginia Bioinformatics Institute</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
							<email>feng@cs.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archuleta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsuoka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">¶</forename><forename type="middle">A</forename><surname>Warren</surname></persName>
							<email>anwarren@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">£</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Setubal</surname></persName>
							<email>setubal@vbi.vt.edu¶</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">£</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
							<email>lusk@mcs.anl.gov‡</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mathematical and Computing Science</orgName>
								<orgName type="institution" key="instit1">North Carolina State University</orgName>
								<orgName type="institution" key="instit2">Virginia Bioinformatics Institute</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
							<email>thakur@mcs.anl.gov‡</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mathematical and Computing Science</orgName>
								<orgName type="institution" key="instit1">North Carolina State University</orgName>
								<orgName type="institution" key="instit2">Virginia Bioinformatics Institute</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Foster</surname></persName>
							<email>foster@mcs.anl.gov‡</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mathematical and Computing Science</orgName>
								<orgName type="institution" key="instit1">North Carolina State University</orgName>
								<orgName type="institution" key="instit2">Virginia Bioinformatics Institute</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz`katz`</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Computation &amp; Technology</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Scalable Computing and Multicore Division</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jhà</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Computation &amp; Technology</orgName>
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Scalable Computing and Multicore Division</orgName>
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinpaugh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Coghlan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science</orgName>
								<orgName type="institution">Argonne National Laboratory</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mathematical and Computing Science</orgName>
								<orgName type="institution" key="instit1">North Carolina State University</orgName>
								<orgName type="institution" key="instit2">Virginia Bioinformatics Institute</orgName>
								<address>
									<settlement>Virginia Tech</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Reed˜</roleName><forename type="first">D</forename><surname>Reed˜</surname></persName>
						</author>
						<title level="a" type="main">Distributed I/O with ParaMEDIC: Experiences with a Worldwide Supercomputer *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>distributed I/O</term>
					<term>bioinformatics</term>
					<term>BLAST</term>
					<term>grid computing</term>
					<term>cluster computing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Achieving high performance for distributed I/O on a wide-area network continues to be an elusive holy grail. Despite enhancements in network hardware as well as software stacks, achieving high-performance remains a challenge. In this paper, our worldwide team took a completely new and non-traditional approach to distributed I/O, called ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing, by utilizing application-specific transformation of data to orders-of-magnitude smaller meta-data before performing the actual I/O. Specifically, this paper details our experiences in deploying a large-scale system to facilitate the discovery of missing genes and constructing a genome similarity tree by encapsulating the mpiBLAST sequence-search algorithm into ParaMEDIC. The overall project involved nine different computational sites spread across the U.S. generating more than a petabyte of data, that was &quot;teleported&quot; to a large-scale facility in Tokyo for storage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth in the scale and complexity of scientific applications over the past few decades, the requirements for compute, memory and storage resources are now greater than ever before. With the onset of petascale and exascale computing, issues related to managing such grand-scale resources, particularly related to data I/O, need to be carefully studied. For example, applications including genomic sequence search and the emergent field of metagenomics, large-scale data mining, data visual analytics and communication profiling on ultra-scale parallel computing platforms, generate massive amounts of data that needs to be managed for later processing or archival.</p><p>To add to the complexity of this problem is the issue of resource locality. While system sizes have certainly grown over the past few years, most researchers do not have local access to systems of the scale required by their applications. Therefore, researchers access such large systems remotely to perform the required computations and move the generated data to their local systems after the computation is complete. Similarly, many applications tend to require multiple resources simultaneously for efficient execution. For example, applications that perform large computations and generate massive amounts of output data are becoming increasingly common. While several large-scale supercomputers provide either the required compute power or the storage resources, very few provide both. Thus, data generated at one site often has to be moved to a different site for storage and/or analysis.</p><p>To alleviate issues related to moving such massive data across sites, there has been a lot of monetary and intellectual investment in high-speed distributed network connectivity <ref type="bibr">[2,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b31">33]</ref>. However, the utility of these investments is limited in the light of three primary observations: (i) in spite of the effort put into high-speed distributed networks, such infrastructure is scarce and does not provide end-to-end connectivity to a very high percentage of the scientific community, (ii) the amount of data generated by many applications is so large that even at 100% network efficiency, the I/O time for these applications can significantly dominate their overall execution time and (iii) based on recent trends and published results, existing distributed I/O mechanisms have not been able to achieve a very high network utilization for "real data" on high-speed distributed networks, particularly for single stream data transfers <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b34">36]</ref>.</p><p>To resolve such issues on a global scale, we proposed a new non-traditional approach for distributed I/O known as ParaMEDIC (Parallel Metadata Environment for Distributed I/O and Computing) <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>. ParaMEDIC uses application-specific semantic information to process the data generated by treating it as a collection of high-level abstract objects, rather than as a generic bytestream. It uses such information to transform the data into orders-of-magnitude smaller metadata before transporting it over the distributed environment and regenerating it at the target site. All data transformation, movement, and regeneration is done while the application is executing, giving the illusion of an ultra-fast teleportation device for large-scale data over distributed environments.</p><p>At a high-level ParaMEDIC is similar to standard compression algorithms. However, the term "compression" typically has a connotation that the data is dealt as a generic byte-stream. Since ParaMEDIC uses a more abstract application-specific representation of the data to achieve a much larger reduction in the data size, we use the terminology of "metadata transformation" in this case.</p><p>Because ParaMEDIC utilizes application semantics to generate metadata, it loses some portability compared to traditional bytestream based distributed I/O. For example, an instance of ParaMEDIC's metadata transformation in the context of the mpiBLAST sequence search application is described in Section 3.2. However, by giving up some portability, ParaMEDIC can potentially attain tremendous savings in the amount of actual distributed I/O performed, consequently resulting in substantial performance gains. Further, through the use of a generic framework with an application plugin model, different applications can utilize the overall framework in an easy and flexible manner.</p><p>In this paper, we demonstrate how we used ParaMEDIC to tackle two large-scale computational biology problems-discovering missing genes and adding structure to genetic sequence databases-on a worldwide supercomputer. The overall worldwide supercomputer comprised nine different supercomputers distributed at seven sites across the U.S., and one large-scale storage facility located in Japan. The overall experiment consisted of sequence searching the entire microbial genome database against itself generating more than a petabyte of data that was transported to Tokyo for storage. We present several insights gained from this large scale run which will be of immense value to other researchers performing such large global-scale distributed computation and I/O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large-Scale Computational Biology: A Peek at Compute and Storage Requirements</head><p>In this section we discuss details on different aspects of computational biology with a focus on the compute and storage requirements of large-scale applications in this domain. With the advent of rapid DNA sequencing, the amount of genetic sequence data available to researchers has increased exponentially <ref type="bibr" target="#b7">[9]</ref>. The GenBank database, a comprehensive database that contains genetic sequence data for more than 260,000 named organisms, has exhibited exponential growth since its inception over 25 years ago <ref type="bibr" target="#b11">[13]</ref>. This information is available for researchers to search new sequences against and infer homologous relationships between sequences or organisms. This helps progress a wide range of projects, from assembling the Tree of Life <ref type="bibr" target="#b16">[18]</ref> to pathogen detection <ref type="bibr" target="#b18">[20]</ref> and metagenomics <ref type="bibr" target="#b19">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence Searching</head><p>Unfortunately, the exponential growth of sequence databases necessitates faster search algorithms to sustain reasonable search times. The Basic Local Alignment Search Tool (BLAST), which is the de facto standard for sequence searching, uses heuristics to prune the search space and decrease search time with an accepted loss in accuracy <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8]</ref>. mpiBLAST parallelizes BLAST using several techniques including database fragmentation, query segmentation <ref type="bibr" target="#b14">[16]</ref>, parallel input-output <ref type="bibr" target="#b25">[27]</ref>, and advanced scheduling <ref type="bibr" target="#b36">[38]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, mpiBLAST uses a master-worker model and performs a scatter-search-gather-output execution flow. During the scatter, the master splits the database and query into multiple pieces and distributes them among worker nodes. Each worker then searches the query segment against the database fragment that it was assigned. The results are gathered by the Master, formatted, and output to the user. Depending on the size of the query and that of the database, such output generated can be large <ref type="table" target="#tab_1">(Table 1)</ref>. Thus, for environments with limited I/O capabilities, such as distributed systems, the output step can cause significant overheads.  Genome annotation is the process of associating information with a genomic sequence. Part of this process includes determining (by computational analysis) the location and structure of protein-encoding and RNAencoding genes, also known as making gene calls. It is important to be as accurate and as sensitive as possible in making gene calls: avoiding false positives and missing real genes. Gene prediction in prokaryotes (bacteria and archaea) typically involves evaluating the coding potential of genomic segments which are delimited by conserved nucleotide motifs. The most widely used gene finding programs <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b26">28]</ref> build a sequence model based on statistical properties of genes known to be (very likely) real genes in a given genome. This model is then used to evaluate the likelihood that an individual segment codes for a gene. In using this method it is almost always the case that some genes with anomalous composition are missed. Another popular method for locating genes is to compare genomic segments with a database of gene sequences found in similar organisms. If the sequence is conserved, the segment being evaluated is likely to be a coding gene (this is the "similarity method"). Genes that do not fit a given genomic pattern and do not have similar sequences in current annotation databases may be systemically missed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discovering Missing Genes</head><p>One way to detect missed genes is to use the similarity method and compare raw genomes against each other, as opposed to comparing a raw genome to a database of known genes. If gene a in genome A and gene b in genome B have been missed and a is similar to b, then this method will find both. However, this involves performing an all-to-all comparison of the entire microbial genome database against itself. This task is heavily compute and data intensive, requiring thousands of compute processors and generating more than a petabyte of output data that needs to be stored for processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adding Structure to Genetic Sequence Databases</head><p>One of the major issues with sequence searching is the structure of the sequence database itself. Currently, these databases are unstructured and represented as a flat file and each new sequence that is discovered is simply appended to the end of the file. Without more intelligent structuring, a query sequence has to be compared to every sequence in the database forcing the best-case to take just as long as the worst-case. By organizing and providing structure to the database, searches can be performed more efficiently by discarding irrelevant portions entirely.</p><p>One way to structure the sequence database is to create a sequence similarity tree where "similar" sequences are closer together in the tree than dissimilar sequences. The connections in the tree are created by determining how "similar" the sequences are to each other determined through sequence searches. To create every connection, however, the entire database has to be searched against itself resulting in an output size of N 2 values (where N is the number of sequences in the database).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview of ParaMEDIC-enhanced mpiBLAST</head><p>In our previous work <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12]</ref>, we provided a detailed description of ParaMEDIC. Here we present a brief summary of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The ParaMEDIC Framework</head><p>ParaMEDIC provides a framework for decoupling computation and I/O in applications relying on large quantities of both. Specifically, it does not hinder application computation. However, as the output data is generated, the framework differs from traditional distributed I/O; it uses application-semantic information to process the data generated by treating it as a collection of high-level application-specific objects rather than as a generic byte-stream. It uses such information to transform the data into orders-ofmagnitude smaller metadata before transporting it over the distributed environment and regenerating it at the target site. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, ParaMEDIC provides several capabilities including support for data encryption and integrity as well as data transfer in distributed environments (either directly via TCP/IP communication or through global file-systems). However, the primary semantics-based metadata creation is done by the application plugins. Most application plugins are specific to each application, and thus rely on knowledge of application semantics. These plugins provide two functionalities: (i) processing output data generated by the application to create metadata and (ii) converting metadata back to the final output. Together with application-specific plugins, ParaMEDIC also provides application-independent components such as data compression, data integrity, and data encryption. These can be used in conjunction with the application-specific plugins or independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trading Computation with I/O:</head><p>The amount of computation required in ParaMEDIC is higher than what is required by the original application; after the output is generated by the application processes, it has to be further processed to generate the metadata, sent to the storage site, and processed yet again to regenerate the final output. However, the I/O cost achieved can potentially be significantly reduced by using this framework. In other words, ParaMEDIC trades (a small amount of) additional computation for (potentially large) reduction in I/O cost. With respect to the additional computational cost incurred, ParaMEDIC is quite generic with respect to the metadata processing required by the different processes. For many applications, it is possible to tune the amount of post-processing performed on the output data, with the general trend being-the more the post-processing computation, the better the reduction in the meta-data size. That is, an application plugin can perform more processing of the output data to reduce the I/O cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integration with mpiBLAST</head><p>In a cluster environment, most of the mpiBLAST execution time is spent on the search itself as the BLAST string-matching algorithm is computationally intensive, compared to which the cost of formatting and writing the results is minimal, especially when many advanced clusters are configured with high-performance parallel file-systems. However, in a distributed environment, the output typically needs to be written over a wide-area network to a remote file-system. Hence, the cost of writing the results can easily dominate the execution profile of mpiBLAST and become a severe performance bottleneck. By replacing the traditional distributed I/O framework with ParaMEDIC (as shown at the top of <ref type="figure" target="#fig_2">Figure 3</ref>), we can provide large reduction in the amount of data I/O performed. For example, as we will see in Section 5, a mpiBLAST-specific instance of ParaMEDIC reduces the volume of data written across a wide-area network by more than two orders of magnitude.  ParaMEDIC then re-runs mpiBLAST at the storage site by taking as input the same query sequence and the temporary database to generate and write output to the storage system. The overhead in rerunning mpiBLAST at the storage site is quite small as the temporary database that is searched is substantially smaller, with only about 250 sequences in it, compared to the several millions of sequences in large DNA databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ParaMEDIC on a World-wide Supercomputer</head><p>In this paper, to accommodate the compute and storage requirements of the computational biology applications discussed in Section 2, we utilize a world-wide supercomputer that, in aggregate, provides the required compute power and storage resources. The worldwide supercomputer comprises nine high-performance computing systems at seven different sites across the U.S. and a largescale storage facility in Japan, to create a single high-performance distributed computing system. The specifics of each individual system are in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dealing with Dynamic Availability of Compute Clients and Other Faults</head><p>Several systems in our worldwide supercomputer operate in batch-mode. Users submit jobs to system queues and are scheduled to execute on the available resources. That is, compute resources are not available in a dedicated manner, but become available when our job is scheduled for execution and become unavailable when our job terminates.</p><p>To handle this issue, we segment the overall work to be performed into small tasks that can be performed independently (i.e., sequentially, concurrently or out-of-order). The management of tasks is done by a centralized server running on a dedicated resource. As each job is executed, it contacts this server for the next task, computes the task, transfers the output to metadata and transmits the metadata to the storage site. This approach has two benefits. First, the clients are completely stateless. That is, if a client job terminates before it has finished its computation or metadata transmission to the storage site, the servers handle this failure by reassigning the task to a different compute client. The second advantage is if the metadata corresponding to a tasks is either not received completely or is corrupted, the server just discards the data and reassigns the task to another compute node. Thus, I/O failures are never catastrophic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architectural Heterogeneity</head><p>One of the key impediments to large-scale distributed systems is system heterogeneity. Many distributed systems, such as the one used in this paper, cannot obtain a homogeneous environment in either hardware or software and efficient use of the system requires overcoming this obstacle. The worldwide supercomputer used in this paper contains six different CPU architectures (e.g., IBM PowerPC 970FX, IBM PowerPC 440, AMD Opteron, SiCortex MIPS64, Intel Xeon, Intel Itanium2), five different network interconnects (e.g., Gigabit Ethernet, 10-Gigabit Ethernet, InfiniBand, IBM proprietary 3D toroidal network, and SiCortex Kautz graph), and eight variations of the Linux operating system.</p><p>To deal with this issue, all data being transferred over the network has to be converted to an architecture-independent format.</p><p>Since the total amount of data that is generated and has to be moved to the storage site in enormous, this can significantly impact on traditional distributed I/O. However, with ParaMEDIC, only metadata generated by processing the actual output is transferred across the wire. Since this metadata is orders-of-magnitude smaller as compared to the actual output, such byte manipulation to deal with heterogeneity has minimal impact on overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Utilizing the Parallelism in Compute Nodes</head><p>In traditional file I/O, there are two levels of parallelism. First, multiple I/O servers are aggregated into a parallel file-system to take advantage of the aggregate storage bandwidth of these servers. Second, multiple compute clients, that process different tasks, write data to such file-systems in parallel as well. Most parallel file-systems are optimized for such access to give the best performance.</p><p>With ParaMEDIC, there are three I/O components: (i) compute clients that perform I/O, (ii) post-processing servers that process the metadata to regenerate the original output and (iii) I/O servers which host the file-system. Similar to the traditional I/O model, the first and third components are already parallelized. That is, multiple streams of data being written in parallel by different compute clients and the I/O servers parallelize each stream of data that is being written to them. However, in order to achieve the best performance, it is important that the second component, post-processing servers, be parallelized as well.</p><p>Parallelizing the post-processing servers adds its own overhead and complexity mainly with respect to synchronization between the different parallel processes. To avoid this, we utilize an embarrassingly parallel approach for these servers. Each incoming stream of data is allocated to a separate process till a maximum number of processes is reached, after which the incoming data requests are queued till a process becomes available again. Thus, different processes do not have to share any information and can proceed independently. The advantage of this approach is its simplicity and lack of synchronization required between different parallel post-processing servers. The disadvantage, however, is that the number of data streams generated from the post-processing servers is equal to the number of incoming data streams. That is, if only two tasks are active at one time, only two streams of data are written to the actual storage system. Thus, the performance might not be optimal. However, in most cases, we expect the number of incoming streams to be sufficiently large to not face such performance issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Handling Large Communication Latencies with Disconnected I/O</head><p>As seen in <ref type="table" target="#tab_3">Table 2</ref>, the computational sites are between 9,000 and 11,000 kilometers away from the storage site. At these distance, communication latencies are in tens of milliseconds. Such large latencies can severely limit the effectiveness of a synchronous remote file-systems that can be used for distributed I/O, since each synchronization operation has to make round-trip hops on the network. To overcome the bottleneck incurred by such high-latency, our worldwide supercomputer utilizes a lazy asynchronous I/O approach. By caching the output data locally before performing the actual output, clients can perform their computations while disconnected from the remote file-system. After a substantial amount of data is generated a bulk transfer of the metadata occurs, thereby maximally utilizing the bandwidth available between the sites and mitigating the effect of high-latency.</p><p>An issue with this approach of disconnected computation is fault-tolerance. Once a task is assigned to a compute client, the server is completely disconnected from this client. After the computation is complete, the client reconnects and sends the generated metadata. Though, this two-phase synchronization model is more error prone and requires additional state information in the server, it makes the compute clients truly stateless even when the actual computation is going on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments, Measurements and Analysis</head><p>In this paper, we use ParaMEDIC to search the entire microbial genome database against itself. Several supercomputing centers within the U.S. perform the computation, while the data generated is stored at a large storage resource in Tokyo. This section  <ref type="figure">Figure 4</ref> illustrates the amount of data transmitted between the compute and the storage sites for different number of post-processing threads, and the factor of reduction in the amount of data. Each post-processing thread processes one segment of data which has the output for 10,000 query sequences. Most segments have approximately similar output sizes, so the amount of data communicated over the distributed network increases linearly with the number of segments, and hence the number of post-processing threads. ParaMEDIC, on the other hand, processes the generated data and converts it into metadata before performing the actual transfer. Thus, the actual data that is transferred over the network is much smaller. For example, with 288 threads, mpiBLAST communicates about 100GB of data, while ParaMEDIC only communicates about 108MB-a 900-fold reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data I/O Overheads</head><p>We also illustrate the I/O time in <ref type="figure">Figure 5</ref>. As shown, ParaMEDIC outperforms mpiBLAST by more than two orders-of-magnitude. This result is attributed to multiple aspects. First, given the shared network connection between the two sites, the achievable network performance is usually much lower than within the cluster. Thus, with mpiBLAST transferring the entire output over this network, its performance would be heavily impacted by the network performance. Second, the distance between the two sites causes the communication latency to be high. Thus, file-system control messages tend to take a long time to be exchanged, resulting in further loss in performance. Third, for mpiBLAST, since the wide-area network is a bottleneck, the number simultaneously transmitted data streams does not matter; communication is serialized in the network. However, with ParaMEDIC, since the wide-area network is no longer a bottleneck, it can more effectively utilize the parallelism in the data streams to better take advantage of the storage bandwidth available at the storage site, as desribed in more detail in Section 5.2. <ref type="figure" target="#fig_5">Figure 6</ref>(a) illustrates the storage bandwidth utilization achieved by mpiBLAST, ParaMEDIC, and the MPI-IO-Test benchmark, that is used as an indication of the peak performance capability of the I/O subsystem. We notice that the storage utilization of mpiBLAST is very poor compared to ParaMEDIC. This is because, for mpiBLAST, the I/O is limited by the wide-area network bandwidth. Thus, though more than 10,000 processors are performing the compute part of the task, the network connecting the On the other hand, ParaMEDIC uses more than 90% of the storage system capability (shown by MPI-IO-test). When the number of processing threads is low (x-axis in <ref type="figure">figure)</ref>, ParaMEDIC uses about half the storage capability. However, as the number of processing threads increases, the I/O utilization of ParaMEDIC increases as well. In <ref type="figure">Figure 7</ref>(a), we remove the file-system network bottleneck and directly perform I/O on the local nodes. Storage utilization achieved in this case is significantly higher than going over the network. Even in this case, ParaMEDIC completely uses the storage capability with more than 90% efficiency. <ref type="figure">Figure 7</ref>(b) shows the percentage breakup of the time spent. Similar to the case with the Lustre file-system, a significant portion of the time is still spent on I/O. Thus, again, ParaMEDIC can be expected to scale and fully use even faster storage resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Storage Bandwidth Utilization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Though this paper only deals with enhancing the mpiBLAST application through ParaMEDIC, the idea is relevant for many other applications as well. For example, applications that have natively been built for distributed environments such as SETI@home <ref type="bibr" target="#b33">[35]</ref> and other BOINC applications <ref type="bibr" target="#b0">[1]</ref> can easily use similar ideas, and can benefit aspects in which such techniques are possible. In the field of communication profiling with MPE <ref type="bibr" target="#b2">[4]</ref>, we have also done some preliminary work that utilizes metadata transformation of profiled data through ParaMEDIC. Specifically, based on the observation that most scientific applications have a very uniform and periodic communication pattern, we perform a fourier transform on the data to identify this periodicity and use this as an abstract block. The metadata comprises one complete abstract block and just the differences for all other blocks. Our preliminary numbers in this field have demonstrated between 2 and 5-fold reduction in the I/O time using ParaMEDIC. Work on other application fields including earthquake modeling and remote visualization is ongoing as well, with promising preliminary results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Efficient I/O access for scientific applications in distributed environments has been an ongoing subject of research for various parallel and distributed file-systems <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34]</ref>. There has also been work on explicit data transfer protocols such as GridFTP <ref type="bibr" target="#b4">[6]</ref>.</p><p>Other efforts include providing remote data access through MPI-IO <ref type="bibr" target="#b27">[29]</ref>. RIO <ref type="bibr" target="#b17">[19]</ref> introduced a proof-of-concept implementation that allows applications to access remote files though ROMIO <ref type="bibr" target="#b35">[37]</ref>. RFS <ref type="bibr" target="#b23">[25]</ref> enhanced the remote write performance with active buffering, by optimizing overlap between applications computation and I/O. Studies have also been done in translating MPI-IO calls into operations of lower level data protocols such as Logistic Network <ref type="bibr" target="#b24">[26]</ref>. However, all these approaches deal with data as a byte-stream. Conversely, our approach focuses on aggressively reducing the amount of I/O data to be communicated by taking advantage of application semantics and dealing with data as high-level abstract units, rather than a stream of bytes.</p><p>Semantic-based data transformation is not new. Several semantic compression algorithms have been investigated in compressing relational databases <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref>. Leveraging the table semantics, these algorithms first build descriptive models of the database using data mining techniques such as clustering, and strip out data that can be regenerated. In the multimedia field, context-based coding techniques (similar to semantics-based approaches) have been widely used in various video compression standards <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b29">31]</ref>. With aid of context modeling, these techniques efficiently identify redundant information in the media. Although sharing the same goal of reducing data to store or transfer with ParaMEDIC, these data compression studies do not address the remote I/O issue.</p><p>Thus, ParaMEDIC utilizes ideas from different fields to provide a novel approach for distributed I/O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Rapid growth of computational power is enabling computational biology to tackle increasingly large problems such as discovering missing genes and providing structure to genetic sequence databases. However, as the problems grow larger, so does the data consumed and produced by the applications. For many applications, the required compute power and storage resources cannot be found at a single location, precipitating the transfer of large amounts of data across the wide-area network. ParaMEDIC mitigates this issue by pursuing a non-traditional approach to distributed I/O. By trading computation for I/O, ParaMEDIC utilizes application semantics information to transform the output to orders-of-magnitude smaller metadata. In this paper, we presented our experiences in solving large-scale computational biology problems by utilizing nine different high-performance compute sites within the U.S. to generate a petabyte of data, that was transferred to a large-scale storage facility in Tokyo using ParaMEDIC's distributed I/O capability. We demonstrated that ParaMEDIC can achieve a performance improvement of several orders of magnitude compared to traditional I/O. In future, we plan to evaluate semantic-based compression for other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level Algorithm of mpiBLAST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ParaMEDIC Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ParaMEDIC and mpiBLAST Integration Figure 3 depicts how mpiBLAST is integrated with ParaMEDIC. First, on the compute site (the left cloud in Figure 3), once the output is generated by mpiBLAST, the mpiBLAST application plugin for ParaMEDIC processes this output to generate orders-of-magnitude smaller metadata. ParaMEDIC transfers this metadata to the storage site. At the storage site, a temporary (and much smaller) database that contains only the result sequences is created by extracting the corresponding sequence data from a local database replica. ParaMEDIC then re-runs mpiBLAST at the storage site by taking as input the same query sequence and the temporary database to generate and write output to the storage system. The overhead in rerunning mpiBLAST at the storage site is quite small as the temporary database that is searched is substantially smaller, with only about 250 sequences in it, compared to the several millions of sequences in large DNA databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Data I/O Overheads: (a) Total amount of data communicated and (b) Factor of Improvement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Storage Bandwidth Utilization using Lustre: (i) Storage Utilization Improvement and (ii) Computation and I/O time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 (</head><label>6</label><figDesc>Figure 6(b) illustrates the percentage breakup of the time spent in ParaMEDIC's post-processing phase. A significant portion of the time spent is in the I/O part. This shows that in spite of using a fast parallel file system such as Lustre, ParaMEDIC is still bottlenecked by the I/O subsystem. In fact, our analysis has shown that in this case the bottleneck lies in the 1-Gigabit Ethernet network subsystem connecting the storage nodes. Thus, we expect that even for systems with faster I/O subsystems, ParaMEDIC will further scale up and continue to use a significant portion of the I/O bandwidth provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Estimated Output of an All-to-All NT Search</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>In the next subsections, we address the issues with working on such a large-scale distributed system that are not immediately apparent on smaller-scale systems.</figDesc><table>Name 
Location 
Cores 
Architecture 
Memory 
Network 
Storage 
Operating 
Distance from 
(GB) 
(TB) 
System 
Storage (km) 

SystemX 
Virginia 
2200 
IBM PowerPC 
4 
InfiniBand 
NFS (30) 
Linux 2.6.21 
11,000 
Tech 
970FX 

Breadboard 
Argonne 
128 
AMD Opteron 
4 
10-Gigabit 
NFS (5) 
Linux 2.6.20 
10,000 
Compute Cluster 
National Lab 
Ethernet 

BlueGene/L 
Argonne 
2048 
IBM PowerPC 
1 
Proprietary 
PVFS (14) 
Linux CNK 
10,000 
Supercomputer 
National Lab 
440 

SiCortex 
Argonne 
5832 
SiCortex SC5832 
3 
Proprietary 
NFS (4) 
Linux 2.6.15 
10,000 
Supercomputer 
National Lab 

Jazz 
Argonne 
700 
Intel Xeon 
1-2 
Gigabit 
GFS (10) 
Linux 2.4.29 
10,000 
Compute Cluster 
National Lab 
Ethernet 
PVFS (10) 

NSF TeraGrid 
University of 
320 
Intel Itanium2 
4 
Myrinet 2000 
NFS (4) 
Linux 2.4.21 
10,000 
Distributed System 
Chicago 

NSF TeraGrid 
San Diego Super-
60 
Intel Itanium2 
4 
Myrinet 2000 
GPFS (50) 
Linux 2.4.21 
9,000 
Distributed System 
computer Center 

Oliver 
Louisiana State 
512 
Intel Xeon 
4 
InfiniBand 
Lustre (12) 
Linux 2.6.9 
11,000 
Compute Cluster 
University 

Open Science 
United 
200 
AMD Opteron 
1-2 
Gigabit 
-
Linux 2.4/2.6 
11,000 
Grid 
States 
Intel Xeon 
Ethernet 

TSUBAME 
Tokyo Institute 
72 
AMD Opteron 
16 
Gigabit 
Lustre (350) 
Linux 2.6 
0 
Storage Cluster 
of Technology 
Ethernet 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Specification of the Different Systems that Formed our Worldwide Supercomputer</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We would like to thank the following people for their technical help in managing the large-scale run and other experiments associated with this paper: (i) R. <ref type="bibr">Kettimuthu</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://boinc.berkeley.edu" />
		<title level="m">BOINC: Berkeley Open Infrastructure for Network Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://www.cse.scitech.ac.uk/disco/mew14-cd/Talks/Force10VanCampen.pdf" />
		<title level="m">Price/Performance as the Key to Cluster &amp; Grid Interconnects</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://www-unix.mcs.anl.gov/perfvis/download/index.htm" />
		<title level="m">MPE : MPI Parallel Environment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surfnet</surname></persName>
		</author>
		<ptr target="http://www.surfnet.nl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Globus Striped GridFTP Framework and Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Allcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bresnahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kettimuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>SC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Biol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Issues in searching molecular sequence databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Altshul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Boguski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wootton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Genet</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="148" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spartan: a model-based semantic compression system for massive data tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garofalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantics-Based Distributed I/O for mpiBLAST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archuleta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kettimuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming<address><addrLine>Salt Lake City, Utah</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semantics-Based Distributed I/O with the ParaMEDIC Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<idno>ANL/MCS-P1477- 0108</idno>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Karsch-Mizrachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genbank. Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PVFS: A Parallel File System For Linux Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ligon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LSC</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-based adaptive binary arithmetic coding in the h.264/avc video compression standard. Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Design, Implementation, and Evaluation of mpiBLAST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Darling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ClusterWorld Conference &amp; Expo and the 4th International Conference on Linux Cluster: The HPC Revolution</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying Bacterial Genes and Endosymbiont DNA with Glimmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Delcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Bratke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prospects for building the tree of life from large sequence databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Driskell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Burleigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>O&amp;apos;meara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="issue">5699</biblScope>
			<biblScope unit="page" from="1172" to="1174" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Remote I/O: Fast access to distant storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnaiyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mogill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on I/O in Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Whole Genome, Physics-Based Sequence Alignment for Pathogen Signature Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Gans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th SIAM Conference on Parallel Processing for Scientific Computing</title>
		<meeting><address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
	<note>electronic version unavailable</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bioinformatic insights from metagenomics through visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Havre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-J</forename><surname>Webb-Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Brockma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Systems Bioinformatics Conference, 2005. Proceedings. 2005 IEEE</title>
		<imprint>
			<date type="published" when="2005-08-11" />
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of the andrew file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Winter Technical Conference</title>
		<imprint>
			<date type="published" when="1988-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic compression and pattern extraction with fascicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Madar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB &apos;99: Proceedings of the 25th International Conference on Very Large Data Bases</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Itcompress: an iterative semantic compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tung</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RFS: Efficient and flexible remote file access for MPI-IO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MPI-IO/L: efficient remote i/o for mpi-io via logistical networking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Atchley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Data Access for Parallel BLAST</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Genemark. hmm: new solutions for gene finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borodovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Message Passing Interface Forum. MPI-2: Extensions to the Message-Passing Standard</title>
		<imprint>
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Generic Coding of Moving Pictures and Associated Audio Information -Part 2: Video. ITU-T and ISO/IEC JTC1, ITU-T Recommendation H.262 ISO/IEC</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="818" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Visual Objects -Part 2: Visual. ISO/IEC JTC1, ISO/IEC</title>
		<imprint>
			<date type="published" when="1999-04" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="496" to="498" />
		</imprint>
	</monogr>
	<note>Amendment 4 (streaming profile</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">NFS: Network File System Protocol Specification. Network Working Group RFC1094</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nowicki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grids, the teragrid, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="68" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GPFS: A shared-disk file system for large computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Seti@home: The search for extraterrestrial intelligence</title>
		<ptr target="http://setiathome.ssl.berkeley.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wide Area Filesystem Performance using Lustre on the TeraGrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TeraGrid Conference</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data Sieving and Collective I/O in ROMIO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of Massively Parallel Computation</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parallel Genomic Sequence-Search on a Massively Parallel System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Computing Frontiers</title>
		<meeting><address><addrLine>Ischia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
