<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-core and Network Aware MPI Topology Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Javad</forename><surname>Rashti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Green</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Balaji</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Argonne National Laboratory</orgName>
								<address>
									<settlement>Argonne</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Afsahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Kingston</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gropp</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-core and Network Aware MPI Topology Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MPI</term>
					<term>virtual topology</term>
					<term>physical topology</term>
					<term>multi-core</term>
					<term>network</term>
				</keywords>
			</textClass>
			<abstract>
				<p>MPI standard offers a set of topology-aware interfaces that can be used to construct graph and Cartesian topologies for MPI applications. These interfaces have been mostly used for topology construction and not for performance improvement. To optimize the performance, in this paper we use graph embedding and node/network architecture discovery modules to match the communication topology of the applications to the physical topology of multi-core clusters with multi-level networks. Micro-benchmark results show considerable improvement in communication performance when using weighted and network-aware mapping. We also show that the implementation can improve communication and execution time of the applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the emerging many-core architectures and high performance interconnects offering more parallelism and performance, clusters are expected to move towards exascales in the next few years <ref type="bibr" target="#b0">[1]</ref>. Such systems are becoming increasingly hierarchical in their node architecture and interconnection network. Communication at various hierarchies demonstrate different performance levels. It is therefore critical for the communication libraries to efficiently handle the communication demands of High Performance Computing (HPC) applications on such hierarchical systems.</p><p>Message Passing Interface (MPI) <ref type="bibr" target="#b1">[2]</ref> is the predominant messaging standard for HPC applications. MPI provides a set of interfaces that are designed to assist the library to construct a virtual topology out of the application's communication pattern, and to support remapping (reordering) the processes to the available cores in a way that optimizes performance. However, most MPI libraries merely provide a trivial implementation of these functions and lack the support for the remapping feature. In this work, we have designed the MPI non-distributed topology functions (MPI_Graph_create and MPI_Cart_create) for efficient process remapping over hierarchical clusters. We have integrated the node physical topology with network architecture and used graph embedding tools inside MPI library to override the current trivial implementation of the topology functions and efficiently reorder the initial process mapping. We have evaluated our implementation on two different InfiniBand <ref type="bibr" target="#b2">[3]</ref> clusters using micro-benchmarks and MPI applications. Microbenchmark results show up to 60% communication time improvement for Cartesian topologies with data exchange between the neighbors. The application results show up to 48% communication time improvement, and up to 26% runtime improvement.</p><p>The rest of this paper is organized as follows. We briefly describe the MPI topology functions and the motivation behind this work in Section 2. Related work is covered in Section 3. Section 4 discusses our design and implementation. Section 5 presents the experimental results, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>MPI defines a set of virtual-topology definition functions for graph and Cartesian structures <ref type="bibr" target="#b1">[2]</ref>. MPI_Graph_create and MPI_Cart_create are collective calls that accept a virtual topology and return a new MPI communicator enclosing the desired topology. If the user opts for reordering, the function may reorder the process ranks for an efficient process-to-core mapping. The topology accepted by these functions is in a non-distributed form, meaning that all nodes have a full view of the entire structure and pass the whole information to the function. Recently, distributed graph topology functionality has been added to the MPI standard <ref type="bibr" target="#b1">[2]</ref> to support large-scale systems. In these functions, each node has a limited neighborhood view of the graph, and all processes collectively construct the virtual topology in a distributed fashion.</p><p>Although process topology functionality is not new to the MPI standard, HPC applications that utilize such functionality use them mainly for the constructed virtual topology (e.g., a Cartesian topology). Thus, the ability of this interface to support process reordering for better communication performance has widely remained unutilized, mostly because MPI implementations merely construct the virtual topology, and have no process remapping for performance improvement. In this paper, we focus on the design and implementation of MPI non-distributed topology functions to improve the performance of the applications. We will cover the distributed topology functions in a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>There has been some past research on topology-aware communication in MPI. The authors in <ref type="bibr" target="#b3">[4]</ref> look at the algorithms for mapping virtual topology to physical topology in MPI using a hierarchical tree structure to represent the hardware topology. This work defines a cost function that is the sum of the communication costs over the links. This paper presents an implementation for a specific machine (an HP server), with architecture similar to multi-core machines.</p><p>The work in <ref type="bibr" target="#b4">[5]</ref> presents a set of graph embedding algorithms with hardware topology represented using a hierarchical tree similar to <ref type="bibr" target="#b3">[4]</ref>, but with a more comprehensive mathematical analysis for different architectures. A major contribution is the optimization of two different cost functions. The first function is the sum of the communication costs over the interconnection links, and the second is a load balancing function that minimizes the number of expensive communications by any one process. This work experiments with a set of NEC SX-6 machines, each with up to eight shared-memory vector processors, connected through a proprietary network.</p><p>The author in <ref type="bibr" target="#b5">[6]</ref> argues that the MPI topology-aware functionality at the time lacks precision and accuracy. The paper suggests an extension to MPI where weighted communication graphs can be used in order to produce a better solution when using the topology functionality. The paper also suggests extensions to allow dynamic process reordering. No implementation of the suggested functionality is attempted.</p><p>The authors in <ref type="bibr" target="#b6">[7]</ref> summarize the work in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b5">[6]</ref>, suggesting some additional changes to MPI as in <ref type="bibr" target="#b5">[6]</ref>. The paper also shows that a non-trivial implementation of the MPI topology functions can provide great performance gains on SMP systems.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, functionality is implemented to map weighted communication graphs to weighted node architecture graphs using Scotch graph partitioning software <ref type="bibr" target="#b8">[9]</ref>. The communication graph weights are chosen based on the total communication volume. Unlike our work in this paper, the work in <ref type="bibr" target="#b7">[8]</ref> does not use or implement MPI topology functions. It rather calculates the mapping outside MPI, before the application is started. It also does not consider network hierarchy.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, the authors propose TreeMatch algorithm to calculate a near-optimal mapping of processes to resources on a NUMA cluster. Similar to the work in <ref type="bibr" target="#b7">[8]</ref>, this paper is concentrated on node architecture and does not consider network hierarchy. It does not use or implement MPI topology functions either. It rather calculates the mapping using MPICH2 process manager and hwloc <ref type="bibr" target="#b13">[14]</ref>. The paper presents MPI results using simulation and NAS benchmarks on a 4 NUMA-node 96-core machine.</p><p>Recently, the authors in <ref type="bibr" target="#b10">[11]</ref> have explained the distributed topology functions in MPI 2.2 and discussed possible methods for implementing them in the future. In <ref type="bibr" target="#b11">[12]</ref> the authors propose an automated framework to detect regular communication patterns in applications. The framework finds the dimensions of a possibly regular pattern and maps it to mesh/torus processor topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design and Implementation of MPI Topology Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design of the Graph Topology Function</head><p>Our design of both MPI graph and Cartesian topology functions is based on an underlying graph structure. We also use graphs to represent both virtual and physical topologies inside the MPI implementation. Using graphs at the underlying layer, we can use static mapping of virtual to physical topology graphs in order to find the suboptimal mapping of processes to processor cores.</p><p>Virtual topology is constructed as a graph in which vertices represent processes and links represent the existence (or significance) of the inter-process communications. We use the normalized total communication volume between two processes as the metric for communication significance. MPI_Cart_create and MPI_Graph_create functions do not support weighted edges, meaning there is no differentiation among edges of the virtual topology. This is a critical shortcoming, since it is usual that the communication between some pairs is more significant than others. To realize how supporting weighted graphs can increase the effectiveness of process reordering, we use edge replication in MPI_Graph_create input to account for weighted edges. This approach, although not much scalable, can support realistic nonuniform communication patterns.</p><p>The graph representation for the cluster's physical topology consists of two distinct but integrated parts: node architecture and network architecture. The node architecture graph includes weighted edges that represent the communication performance between any two cores. We assume higher communication performance between the cores with closer proximity. Our representation of the network comprises network distances between any two cores. The network architecture includes weighted graph edges that represent the communication performance of the network path between the nodes on which the cores reside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation of Topology Functions</head><p>In this section, we present details about the implementation of the MPI_Graph_create and MPI_Cart_create functions inside MVAPICH2 <ref type="bibr" target="#b12">[13]</ref>. We use the Scotch graph processing library <ref type="bibr" target="#b8">[9]</ref> to map virtual to physical topology graphs in MPI_Graph_create. The library defines an undirected source graph, which can represent the virtual topology. Each vertex and edge of the source graph is weighted, to account for the computation and communication weight of the corresponding process and link, respectively. Similarly, the target machine architecture is represented by an undirected architecture target graph with weights for vertices and edges representing the processing power of the processor and communication performance of the link, respectively <ref type="bibr" target="#b8">[9]</ref>.</p><p>The hardware locality (hwloc) library <ref type="bibr" target="#b13">[14]</ref> provides a portable abstraction of the underlying machine architecture. It detects architectural components of the nodes such as processor sockets, cores, caches, memory, SMT and NUMA architecture. The architecture is represented as a tree, with nodes at the top level and logical cores at the leaves. This library can assist MPI to construct the physical topology of the machine.</p><p>To have a complete view of the cluster's physical architecture and go beyond a flat network assumption, we add a network discovery part in our physical architecture discovery. This module extracts the network distance between any two nodes in the cluster and merges that information with the node architecture extracted by hwloc. In this paper, our network discovery module uses InfiniBand subnet manager <ref type="bibr" target="#b2">[3]</ref> tools (i.e., ibtracert utility) to discover the network distance between Infiniband nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Implementation of Graph Topology Function.</head><p>To supply MPI with the application virtual topology, we extract the exact amounts of data transfer between processes by profiling the applications using probes inside the MPI library. We give the highest weight to the maximum pairwise communication volume. The normalized weights range from 1 to 10. We use edge replication to represent edge weights.</p><p>MPI_Graph_create calls the Scotch library if the user opts for reordering. Scotch builds a weighted graph out of the user-supplied graph. The topology table of a server node is also created using hwloc at each process. The communication performance between two logical cores on a node is calculated based on the depth of their common ancestor in the node topology tree. For example, if two cores reside on different sockets, their common ancestor will be the node itself, with the lowest depth in the tree, translating into the lowest communication performance.</p><p>The process with rank zero (in MPI_COMM_WORLD) will perform a network discovery operation using ibtracert to extract the distance between any two InfiniBand nodes. This information is then scattered to other processes to be integrated into the node architecture to have the full physical topology architecture. Topology graph edge loads (input to Scotch) represent the performance of the communication path between the connecting vertices. Network distance, defined as the number of hops between two nodes, is used to calculate the physical topology edge loads. The farthest nodes get the load of 1 on their graph edge. The closest nodes get the maximum network hop count as their edge load. The intra-node graph loads are calculated in a way that are always larger than the closest network distance, indicating the fact that intra-node communication is cheaper than inter-node communication. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of how graph loads are assigned based on the system and network architecture. N1-N4 are multi-core (here, 2-way quad-core) nodes, connected through three switches (S1-S3) in a tree-like network. In this architecture, d1 (the path between two cores of the same socket) will have the highest load value in the graph, while the path between N2 and N3 (d4) will have the lowest load value, indicating the lowest performance path in the network. Thus we have: d1 &gt; d2 &gt; d3 &gt; d4 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cartesian Topology Implementation.</head><p>Since the Scotch library does not support Cartesian topology, we internally convert the MPI_Cart_create topology to a graph and use MPI_Graph_create for reordering. In this conversion, the user view of the Cartesian topology remains intact. Cartesian topologies do not specify weights for graphs. Therefore, the converted topology will be a non-weighted graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We have conducted our experiments on two clusters. The first cluster has four servers, each with two quad-core 2GHz AMD Opteron 2350 processors (a total of 32 cores). The servers have 512KB L2 cache per core, 2MB shared L3 cache per processor and 8GB memory. On both clusters, we use MVAPICH2 1.5 <ref type="bibr" target="#b12">[13]</ref> as our code-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Micro-benchmark Results</head><p>We start our evaluation with two micro-benchmarks that put processes in Cartesian arrangements such as Torus and Hypercube. The tests are run on the first (32-core) cluster. The first micro-benchmark (Cartesian-model exchange) constructs a 2D/3D torus or a 5D hypercube. Each process runs 1000 iterations, each with a computation followed by exchanging messages with the neighbors in all dimensions. We report the average iteration time. We find the virtual-topology graph of each test by profiling it in an initial run. The graph is then supplied to the same program as input for MPI_Graph_create. To evaluate the effect of graph weights, we carry significantly heavier communication on one of the dimensions. <ref type="figure">Fig. 2</ref>. Runtime improvement of topology-aware mapping over block mapping for 2D-torus, 3D-torus and 5D-hypercube in the Cartesian-model exchange micro-benchmark <ref type="figure">Fig. 2</ref> shows the improvement of the topology-aware mapping compared to block mapping for an 8×4 2D-torus, a 4×4×2 3D-torus and a 5D-hypercube. The processes communicate more heavily on one dimension (the longer dimension for torus). For the micro-benchmarks, to particularly show the effect of network-aware mapping, we put two subsequent nodes on different switches. As shown in <ref type="figure">Fig. 2</ref>, for all topologies the weighted graph shows significant improvement compared to the block mapping. It is because with a weighted graph the library differentiates between heavier and lighter dimensions and will try to map the processes of the heavier dimension on one node to take advantage of shared-memory performance. On the other hand, the non-weighted Cartesian topology does not make any differentiation between different dimensions. Consequently, it may map the heavier dimension across the network leading to worse performance. The other observation is the improvement when using network-aware mapping, especially for larger message. It shows that even when the difference in network distance is as little as two switches we can observe some improvement.</p><p>The second micro-benchmark (dimensional collectives) examines the case where processes form a Cartesian arrangement and perform collective communications on one dimension of the topology. We arrange 32 processes in an 8×4 2D-torus and engage them in MPI_Alltoall collective operations on the longer dimension. <ref type="figure" target="#fig_2">Fig. 3</ref> clearly shows the improvement when using topology-aware mapping. The reason for on par performance between weighted graph and non-weighted graph is that the communication is done only on one dimension; therefore both graphs result in the same mapping. The network-aware mapping also shows similar improvement, because all eight processes of the collective dimension are mapped to the same node.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Results for MPI Applications</head><p>To see the effect of our implementation on MPI applications, we have adapted some MPI application benchmarks (NAS and LAMMPS) to use MPI graph topology function. We profiled the original applications to discover their virtual topology graph in order to supply them to MPI_Graph_create function. In LAMMPS, processes communicate in a 2D-torus. Processes in CG.32 also form a logical 8×4 2D-torus, however they do not always follow the torus links for communication. Processes in MG.32 communicate in the form of a reordered 5D-hypercube. To be consistent, regardless of application's logical structure, we always use a graph topology function. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the improvement of the topology-aware mapping for these applications compared to block and cyclic mappings on the 32-core cluster. This excludes the time in MPI_Graph_create to create the communicator. For most applications, the benefit over cyclic mapping is considerable while there is less benefit over block mapping. This is because processes mostly communicate with their adjacent ranks, thus the ideal mapping is close to block mapping. In LAMMPS (especially the friction workload), where the communication volume is not equal on different links, we can see more improvement with weighted and network-aware graphs, compared to non-weighted graph where sometimes it is even worse than the block mapping. We are currently investigating the reasons behind the poor performance of the weighted graph results for LAMMPS-Couple. To show the benefits of our design on a larger test bed, we have also evaluated our work on a second cluster with 16 nodes (using 8 cores per node, for a total of 128-cores) using some of the application workloads. The results are presented in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>In <ref type="figure" target="#fig_5">Fig. 5</ref> we observe more improvement for most of the workloads, which indicates the performance scalability with the machine size. Higher improvement in the 128-core case for some workloads such as LAMMPS-friction is partially because some of the neighbors that would fall on the same node in block mapping in 32-core case fall on different nodes in 128-core case. Therefore reordering is more effective for the latter case. LAMMPS-couple shows a considerable difference in communication pattern between 32-core and 128-core cases. While for 32 cores, processes communicate to their neighbors almost symmetrically, the pattern becomes asymmetric in 128-core case (a process mostly communicates with two partners). Thus we see higher difference between non-weighted and weighted/network-aware results for 128 cores. Such behavior is also observed for pour workload to some extent, leading to more improvement compared to block mapping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Overhead</head><p>Our implementation of MPI_Graph_create imposes an overhead compared to the trivial implementation. <ref type="table" target="#tab_0">Table 1</ref> shows the approximate overhead and its scalability in LAMMPS application. This one-time overhead is amortized in application runtime. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we presented design and implementation of MPI non-distributed graph and Cartesian functions in MVAPICH2 for multi-core nodes connected through multi-level InfiniBand networks. The Cartesian-model micro-benchmarks show that the effect of reordering process ranks can be significant, and when the communication is heavier on one dimension the benefits of using weighted and network-aware graphs (instead of non-weighted graph / Cartesian functions) are considerable. We also modified some MPI applications with MPI_Graph_create. The evaluation results show that MPI applications can benefit from topology-aware MPI_Graph_create.</p><p>As for the future work, we intend to evaluate the effect of topology awareness on other MPI applications, design a more general communication cost/weight model for graph mapping, and design and implement MPI distributed topology functions for more scalability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of physical topology distances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>They are interconnected through Mellanox ConnectX InfiniBand cards [15] via three Mellanox InfiniBand switches, similar to Fig. 1. The nodes run Linux Fedora 12 kernel version 2.6.31. The machines use OFED 1.5.2 to access the InfiniBand network. For the second cluster, we have 16 servers, each with two hexa-core 3GHz Intel Xeon X5670 processors (a total of 192 cores). There is a 12MB multi-level cache per processor, and 24GB memory per machine. The servers use Mellanox ConnectX2 InfiniBand cards. Eight servers are connected to a Mellanox InfiniBand switch, and the remaining servers are connected to another switch. Each ofswitches is connected to two upper layer InfiniBand switches. The machines run Redhat Enterprise Linux 5 with kernel version 2.6.18-194. The nodes use OFED version 1.5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>also shows the results for a 16×2 2D-torus. These results are reported since the longer dimension (on which collectives are performed) has the length of 16, which does not fit into one node, therefore network communication is inevitable even after topology-aware remapping. As the results suggest, network-aware mapping shows improvement compared to other graph mappings, especially for larger message sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Runtime improvement of topology-aware mapping over block mapping for 2D-torus in the dimensional collective (Alltoal) micro-benchmark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Application communication time and runtime improvement of topology-aware mapping over block and cyclic mappings on the 32-core cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 5 .</head><label>5</label><figDesc>Fig 5. Application communication time and runtime improvement of topology-aware mapping over block and cyclic mappings on the 128-core cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 -Time to create the communicator in MPI_Graph_create for LAMMPS</head><label>1</label><figDesc></figDesc><table>System 
Job size 
(#processes) 

Trivial 
(ms) 

Non-weighted 
Graph (ms) 

Weighted 
Graph (ms) 

Network-aware 
Graph (ms) 
8 
0.3 
7.3 
7.3 
7.9 
Cluster 1 
16 
0.3 
7.6 
7.7 
8.1 
32 
0.5 
8.6 
8.7 
9 
Cluster 2 
128 
5.1 
31.3 
31.7 
31.7 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://www.exascale.org/" />
	</analytic>
	<monogr>
		<title level="j">IESP: International Exascale Software Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">MPI Forum: MPI: A Message-Passing Interface Standard</title>
		<imprint>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://www.infinibandta.org/" />
	</analytic>
	<monogr>
		<title level="j">IBTA: InfiniBand Architecture Specification</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rank reordering strategy for MPI topology creation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hatazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5 th Euro PVM/MPI Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springger-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implementing the MPI Process Topology Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Conference on Supercomputing</title>
		<meeting><address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMP-aware Message Passing Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17 th IEEE Parallel and Distributed Processing Symposium (IPDPS)</title>
		<meeting><address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What MPI could (and cannot) do for Mesh-partitioning on Nonhomogeneous Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13 th Euro PVM/MPI Conference</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4192</biblScope>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards an Efficient Process Placement Policy for MPI Applications in Multi-core Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clet-Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16 th Euro PVM/MPI Conference</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5759</biblScope>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scotch and libScotch 5.1 User&apos;s Guide. Bacchus team</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pellegrini</surname></persName>
		</author>
		<ptr target="https://gforge.inria.fr/docman/view.php/248/5709/scotch_user5.1.pdf" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>INRIA Bordeaux Sud-Ouest</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Near-Optimal Placement of MPI processes on Hierarchical NUMA Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jeannot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroPar 2010 conference</title>
		<meeting>EuroPar 2010 conference<address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Scalable Process Topology Interface of MPI 2.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ritzdorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Supinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Concurr. Comp.-Pract. E</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="293" to="310" />
			<date type="published" when="2010" />
			<publisher>John Wiley &amp; Sons, Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated Mapping of Regular Communication Graphs on Mesh Interconnects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhatele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17 th International Conference on High Performance Computing (HiPC)</title>
		<meeting><address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="http://mvapich.cse.ohio-state.edu/" />
		<title level="m">MVAPICH: MPI Over InfiniBand, 10GigE/iWARP and RoCE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">hwloc: A Generic Framework for Managing Hardware Affinities in HPC Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Broquedis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clet-Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moreaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Furmento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Namyst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18 th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP&apos;10)</title>
		<meeting>the 18 th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP&apos;10)<address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<ptr target="http://www.mellanox.com/" />
	</analytic>
	<monogr>
		<title level="j">Mellanox Technologies</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
