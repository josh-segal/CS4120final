<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Value Function Approximation on Non-Linear Manifolds for Robot Motor Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">£񮽙ý</forename><surname>Hirotaka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hachiya</forename><forename type="middle">Ý</forename><surname>Christopher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towell</forename><forename type="middle">Ý</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethu</forename><surname>Vijayakumar</surname></persName>
						</author>
						<title level="a" type="main">Value Function Approximation on Non-Linear Manifolds for Robot Motor Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The least squares approach works efficiently in value function approximation, given appropriate basis functions. Because of its smoothness, the Gaussian kernel is a popular and useful choice as a basis function. However, it does not allow for discontinuity which typically arises in real-world reinforcement learning tasks. In this paper, we propose a new basis function based on geodesic Gaussian kernels, which exploits the non-linear manifold structure induced by the Markov decision processes. The usefulness of the proposed method is successfully demonstrated in a simulated robot arm control and Khepera robot navigation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Value function approximation is an essential ingredient of reinforcement learning (RL), especially in the context of solving Markov Decision Processes (MDPs) using policy iteration methods <ref type="bibr" target="#b0">[1]</ref>. In problems with large discrete state space or continuous state spaces, it becomes necessary to use function approximation methods to represent the value functions. A least squares approach using a linear combination of predetermined under-complete basis functions has shown to be promising in this task <ref type="bibr" target="#b1">[2]</ref>. Fourier functions (trigonometric polynomials), Gaussian kernels <ref type="bibr" target="#b2">[3]</ref>, and wavelets <ref type="bibr" target="#b3">[4]</ref> are popular basis function choices for general function approximation problems. Both Fourier bases (global functions) and Gaussian kernels (localized functions) have certain smoothness properties that make them particularly useful for modeling inherently smooth, continuous functions. Wavelets provide basis functions at various different scales and may also be employed for approximating smooth functions with local discontinuity.</p><p>Typical value functions in RL tasks are predominantly smooth with some discontinuous parts <ref type="bibr" target="#b4">[5]</ref>. To illustrate this, let us consider a toy RL task of guiding an agent to a goal in a grid world (see <ref type="figure" target="#fig_0">Fig.1</ref>(a)). In this task, a state corresponds to a two-dimensional Cartesian position of the agent. The agent can not move over the wall, so the value function of this task is highly discontinuous across the wall. On the other hand, the value function is smooth along the maze since neighboring reachable states in the maze have similar values (see <ref type="figure" target="#fig_0">Fig.1(b)</ref>). Due to the discontinuity, simply employing Fourier functions or Gaussian kernels as basis functions</p><p>The authors acknowledge financial support from MEXT (Grant-in-Aid for Young Scientists 17700142 and Grant-in-Aid for Scientific Research (B) 18300057), the Okawa Foundation, and EU Erasmus Mundus Scholarship. tend to produce undesired, non-optimal results around the discontinuity, affecting the overall performance significantly. Wavelets could be a viable alternative, but are over-complete bases-one has to appropriately choose a subset of basis functions, which is not a straightforward task in practice.</p><p>Recently, the article <ref type="bibr" target="#b4">[5]</ref> proposed considering value functions defined not on the Euclidean space, but on graphs induced by the MDPs (see <ref type="figure" target="#fig_0">Fig.1(c)</ref>). Value functions which usually contain discontinuity in the Euclidean domain (e.g., across the wall) are typically smooth on graphs (e.g., along the maze). Hence, approximating value functions on graphs can be expected to work better than approximating them in the Euclidean domain.</p><p>The spectral graph theory <ref type="bibr" target="#b5">[6]</ref> showed that Fourier-like smooth bases on graphs are given as minor eigenvectors of the graph-Laplacian matrix. However, their global nature implies that the overall accuracy of this method tends to be degraded by local noise. The article <ref type="bibr" target="#b6">[7]</ref> defined diffusion wavelets, which posses natural multi-resolution structure on graphs. The paper <ref type="bibr" target="#b7">[8]</ref> showed that diffusion wavelets could be employed in value function approximation, although the issue of choosing a suitable subset of basis functions from the over-complete set is not discussed-this is not straightforward in practice due to the lack of a natural ordering of basis functions.</p><p>In the machine learning community, Gaussian kernels seem to be more popular than Fourier functions or wavelets because of their locality and smoothness <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Furthermore, Gaussian kernels have 'centers', which alleviates the difficulty of basis subset choice, e.g., uniform allocation <ref type="bibr" target="#b1">[2]</ref> or sample-dependent allocation <ref type="bibr" target="#b10">[11]</ref>. In this paper, we therefore define Gaussian kernels on graphs (which we call geodesic Gaussian kernel), and propose using them for value function approximation. Our definition of Gaussian kernels on graphs employs the shortest paths between states rather than the Euclidean distance, which can be computed efficiently using the Dijkstra algorithm <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Moreover, an effective use of Gaussian kernels opens up the possibility to exploit the recent advances in using Gaussian processes for temporal difference learning <ref type="bibr" target="#b10">[11]</ref>.</p><p>When basis functions defined on the state space are used for approximating the state-action value function, they should be extended over the action space. This is typically done by simply copying the basis functions over the action space <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In this paper, we propose a new strategy for this extension, which takes into account the transition after taking actions. This new strategy is demonstrated to work very well when the transition is predominantly deterministic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FORMULATION OF THE REINFORCEMENT LEARNING PROBLEM</head><p>In this section, we briefly introduce the notation and reinforcement learning (RL) formulation that we will use across the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Markov Decision Processes</head><p>Let us consider a Markov decision process (MDP) <ref type="bibr">´Ë񮽙</ref>  Let 񮽙´×µ µ Ë Ë be a deterministic policy which the agent follows. In this paper, we focus on deterministic policies since there always exists an optimal deterministic policy <ref type="bibr" target="#b1">[2]</ref>. </p><formula xml:id="formula_0">Let É 񮽙´××񮽙´×× µ µ Ë¢</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Least Squares Policy Iteration</head><p>In practice, the optimal policy 񮽙 £ ´×µ can not be directly obtained since Ê´×× µ and È´×× × ¼ µ are usually unknown; even when they are known, direct computation of 񮽙 £ ´×µ is often computationally intractable.</p><p>To cope with this problem, the paper <ref type="bibr" target="#b1">[2]</ref> proposed approximating the state-action value function É 񮽙´××񮽙´×× µ using a  <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, but they can not approximate discontinuous functions well. Recently, more sophisticated methods of constructing suitable basis functions have been proposed, which effectively make use of the graph structure induced by MDPs <ref type="bibr" target="#b4">[5]</ref>. In this section, we introduce a novel way of constructing basis functions by incorporating the graph structure; while relation to the existing graph-based methods is discussed in the separate report <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MDP-Induced Graph</head><p>Let 񮽙 be a graph induced by an MDP, where states Ë are nodes of the graph and the transitions with non-zero transition probabilities from one node to another are edges. The edges may have weights determined, e.g., based on the transition probabilities or the distance between nodes. The graph structure corresponding to an example grid world shown in <ref type="figure" target="#fig_0">Fig.1</ref>(a) is illustrated in <ref type="figure" target="#fig_0">Fig.1(c)</ref>. In practice, such graph structure (including the connection weights) are estimated from samples of a finite length. We assume that the graph 񮽙 is connected. Typically, the graph is sparse in RL tasks, i.e., 񮽙 񮽙 Ò´Ò ½µ񮽙¾, where 񮽙 is the number of edges and Ò is the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ordinary Gaussian Kernels</head><p>Ordinary Gaussian kernels (OGKs) on the Euclidean space are defined as</p><formula xml:id="formula_1">Ã ´×× × ¼ µ µ Ü Ô 񮽙´×× 񮽙´×× × ¼ µ ¾ ¾񮽙 ¾ 񮽙 񮽙 (5)</formula><p>where <ref type="formula">񮽙񮽙´××</ref>  depend only on the difference between two positions; more specifically, Gaussian kernels depend only on the distance between two positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Geodesic Gaussian Kernels</head><p>On graphs, a natural definition of the distance would be the shortest path. So we define Gaussian kernels on graphs based on the shortest path:</p><formula xml:id="formula_2">Ã ´×× × ¼ µ ÜÔ 񮽙 ËÈ´×× × ¼ µ ¾ ¾񮽙 ¾ 񮽙 񮽙 (7)</formula><p>where ËÈ´×× × ¼ µ denotes the shortest path from state × to state × ¼ . The shortest path on a graph can be interpreted as a discrete approximation to the geodesic distance on a nonlinear manifold <ref type="bibr" target="#b5">[6]</ref>. For this reason, we call Eq. <ref type="formula">(7)</ref> a geodesic Gaussian kernel (GGK). Shortest paths on graphs can be efficiently computed using the Dijkstra algorithm <ref type="bibr" target="#b11">[12]</ref>. With its naive implementation, computational complexity for computing the shortest paths from a single node to all other nodes is Ç´Ò ¾ µ, where Ò is the number of nodes. If the Fibonacci heap is employed, the computational complexity can be reduced to Ç´Ò ÐÓÓ Ò · 񮽙µ <ref type="bibr" target="#b12">[13]</ref>, where 񮽙 is the number of edges. Since the graph in value function approximation problems is typically sparse (i.e., 񮽙 񮽙 Ò ¾ ), using the Fibonacci heap provides significant computational gains. Furthermore, there exist various approximation algorithms which are computationally very efficient (see <ref type="bibr" target="#b14">[15]</ref> and and references therein).</p><p>Analogous to OGKs, we need to extend GGKs to the stateaction space for using them in the LSPI method. A naive way is to just employ Eq.(6), but this can cause a 'shift' in the Gaussian centers since the state usually changes when some action is taken. To incorporate this transition, we propose defining the basis functions as the expectation of Gaussian functions after transition, i.e.,</p><formula xml:id="formula_3">񮽙 񮽙·´񮽙 ½µÑ´××½µÑ´×× µ µ Á ´񮽙 񮽙 񮽙´񮽙µ񮽙´񮽙µ µ 񮽙 × ¼ ¾Ë È´×× × ¼ µÃ´× ¼ 񮽙 񮽙´񮽙µ񮽙´񮽙µ µ񮽙 (8)</formula><p>This shifting scheme is expected to work well when the transition is predominantly deterministic (see Sec.IV and Sec.V-A for experimental evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extension to Continuous State Spaces</head><p>So far, we focused on discrete state spaces. However, the concept of GGKs can be naturally extended to continuous state spaces, which is explained here. First, the continuous state space is discretized, which gives a graph as a discrete approximation to the non-linear manifold structure of the continuous state space. Based on the graph, we construct GGKs in the same way as the discrete case. Finally, the discrete GGKs are interpolated, e.g., using a linear method to give continuous GGKs.</p><p>Although this procedure discretizes the continuous state space, it must be noted that the discretization is only for the purpose of obtaining the graph as a discrete approximation of the continuous non-linear manifold; the resulting basis functions themselves are continuously interpolated and hence, the state space is still treated as continuous as opposed to other conventional discretization procedures. <ref type="formula">3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18</ref>   </p><formula xml:id="formula_4">→ → → → ↓ → → → → ↑ → → → → → → → ↓ → ↓ → ↑ → → ↓ → → ↑ ↓ ↓ ↓ ↓ → ↑ → → ↓ → ↑ ↑ ↓ ↓ ↓ ↓ → ↑ → → ↓ → → → → → → → → → → → → ↓ → → → ↓ → → ↑ → → → → → → → → → → → ↑ ↑ ↑ → → → → → → ↓ → → → → ↓ → → → → ↓ → ↑ → → ↓ ↓ ↓ ↓ → ↓ → ↑ ↑ → → → → → ↓ ↓ → ↓ → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ → → → → ↑ → ↓ ↓ ↓ ↓ ↓ ↓ → → → ↑ → → → → → ↑ ↑ ↑ → → ↑ → ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ 1 2</formula><formula xml:id="formula_5">→ → ↓ → → ↓ → ↓ → → → → ↑ → → → → ↑ → → → → → ↓ → → → → → → → → → → → ↑ → → → → → ↓ → → → → → ↑ → → → ↑ ↑ ↑ ↓ → → → → ↓ → ↓ → → → ↑ → ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → → → → ↑ ↑ ↑ ↑ ↑ ↓ ↓ → → → ↓ ↓ ↓ → → ↑ → → → → ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → → → → ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ → → → → → → → → ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → → ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ → → → ↓ ↓ → → → → ↓ ↓ → → → → → → ↓ → ↓ → → → → → → ↓ → ↓ ↓ → → ↓ → → ↓ → ↓ ↓ ↓ → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → → → ↓ ↓ ↓ ↓ ↓ ↓ → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ → → ↓ → ↓ → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ 1</formula><note type="other">2 3 4 5 6 7 8 9 1011121314151617181920 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 (b) Three-room maze</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL COMPARISON</head><p>In this section, we report the results of extensive and systematic experiments for illustrating the difference between GGKs and other basis functions. We employ two standard grid world problems illustrated in <ref type="figure">Fig.2</ref>, and evaluate the goodness of approximated value functions by computing the mean squared error (MSE) with respect to the optimal value function and the goodness of obtained policies by calculating the fraction of states from which the agent can get to the goal optimally (i.e., in the shortest number of steps). ¾¼ series of random walk of length ¿¼¼ are gathered as training samples, which are used for estimating the graph as well as the transition probability and expected reward. We set the edge weights in the graph to ½ (which is equivalent to the Euclidean distance between two nodes). We test GGKs, OGKs, graph-Laplacian eigenfunctions (GLEs) <ref type="bibr" target="#b4">[5]</ref>, and diffusion wavelets (DWs) <ref type="bibr" target="#b7">[8]</ref>.</p><p>This simulation is repeated ½¼¼ times for each maze and each method, randomly changing training samples in each run. The mean of the above scores as a function of the number of bases is plotted in <ref type="figure">Fig.4</ref>. Note that the actual number of bases is four times more because of the extension of basis functions over the action space (see Eq. <ref type="formula">(6)</ref> and Eq. <ref type="formula">(8)</ref>). GGKs and OGKs are tested with small/medium/large Gaussian widths. <ref type="figure">Fig.3</ref> depicts MSEs of the approximated value functions for each method. They show that MSEs of GGKs with width ½, OGKs with width ½, GLEs, and DWs are very small and decrease as the number of kernels increases. On the other hand, MSEs of GGKs and OGKs with medium/large width are large and increase as the number of kernels increases. Therefore, from the viewpoint of approximation quality of the value functions, the width of GGKs and OGKs should be small. <ref type="figure">Fig.4</ref> depicts the fraction of optimal states in the obtained policy. They show that overall GGKs with medium/large width give much better policies than OGKs, GLEs, and DWs. An interesting finding from the graphs is that GGKs tend to work better if the Gaussian width is large, while OGKs show the opposite trend; this may be explained as follows. Tails of OGKs extend across the wall. Therefore, OGKs with large width tend to produce undesired value function and erroneous policies around the partitions. This tail effect can be alleviated if the Gaussian width is made small. However, this in turn makes the approximated value function fluctuating; so the resulting policies are still erroneous. The fluctuation problem with a small Gaussian width seems to be improved if the number of bases is increased, while the tail effect with a large Gaussian width still remains even when the number of bases is increased. On the other hand, GGKs do not suffer from the tail problem thanks to the geodesic construction. Therefore, GGKs allows us to make the width large without being affected by the discontinuity across the wall. Consequently, smooth value functions along the maze are produced and hence better policies can be obtained by GGKs with large widths. This result highlights a helpful property since it alleviates the practical issue of determining the values of the Gaussian width parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPLICATIONS</head><p>As discussed in the previous section, the proposed GGKs bring a number of preferable properties for making value function approximation effective. In this section, we investigate the application of the GGK-based method to the challenging problems of a (simulated) robot arm control and mobile robot navigation and demonstrate its usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Robot Arm Control</head><p>We use a simulator of a two-joint robot arm (moving in a plane) illustrated in <ref type="figure">Fig.5(a)</ref>. The task is to lead the end effector ('hand') of the arm to an object while avoiding the obstacles. Possible actions are to increase or decrease the angle of each joint ('shoulder' and 'elbow') by 񮽙 degrees in the plane, simulating coarse stepper motor joints. Thus the state space Ë is the ¾-dimensional discrete space consisting of two joint angles as illustrated in <ref type="figure">Fig.5(b)</ref>. The black area in the middle corresponds to the obstacle in the joint angle state space. The action space 񮽙 involves 񮽙 actions: increase or decrease one of the joint angles. We give a positive immediate reward ·½ when the robot's end effector touches the object; otherwise the robot receives no immediate reward. Note that actions which make the arm collide with obstacles are not allowed. The discount factor is set to ­ 񮽙 ¼ 񮽙񮽙. In this environment, we can change the joint angle exactly by 񮽙 degrees, so the environment is deterministic. However, because of the obstacles, it is difficult to explicitly compute an inverse kinematic model; furthermore, the obstacles introduce discontinuity in value functions. Therefore, this robot arm control task is an interesting test bed for investigating the behaviour of GGKs.</p><p>We collected training samples from 񮽙¼ series of ½¼¼¼ random arm movements, where the start state is chosen randomly in each trial. The graph induced by the above MDP consists of ½½¼¼ nodes and we assigned uniform weights to the edges. There are totally ½½ goal states in this environment (see <ref type="figure">Fig.5(b)</ref>), so we put the first ½½ Gaussian centers at the goals and the remaining centers are chosen randomly in the state space. For GGKs, kernel functions are extended over the action space using the shifting scheme (see Eq. <ref type="formula">(8)</ref>) since the transition is deterministic in this experiment. <ref type="figure" target="#fig_3">Fig.6</ref> illustrates the value functions approximated using GGKs and OGKs <ref type="bibr" target="#b2">3</ref> . The graphs show that GGKs give a nice smooth surface with obstacle-induced discontinuity sharply preserved, while OGKs tend to smooth out the discontinuity. This makes a significant difference in avoiding the obstacle: from 'A' to 'B' in <ref type="figure">Fig.5(b)</ref>, the GGK-based value function results in a trajectory that avoids the obstacle (see <ref type="figure" target="#fig_3">Fig.6(a)</ref>). On the other hand, the OGK-based value function yields a trajectory that tries to move the arm through the obstacle by following the gradient upward (see <ref type="figure" target="#fig_3">Fig.6(b)</ref>). The latter causes the arm to get stuck behind the obstacle. <ref type="figure" target="#fig_5">Fig.7</ref> summarizes the performance of GGKs and OGKs measured by the percentage of successful movements (i.e., the end effector reaches the target) averaged over ¿¼ independent runs. More precisely, in each run, totally 񮽙¼¼¼¼ training samples are collected using a different random seed, a policy is then computed by the GGK-or OGK-based method using LSPI, and the obtained policy is tested. This graph shows that GGKs remarkably outperform OGKs since the arm can successfully avoid the obstacle. The performance of OGK does not go beyond ¼񮽙񮽙 even when the number of kernels is increased. This is caused by the 'tail effect' of ordinary Gaussian functions; the OGK-based policy can not lead the end effector to the object if it starts from the bottom-left half of the state space When the number of kernels is increased, the performance of both GGKs and OGKs once gets worse at around 񮽙 񮽙 ¾¼. This would be caused by our kernel center allocation strategy: the first ½½ kernels are put at the goal states and the remaining kernel centers are chosen randomly. When 񮽙 is less than or equal to ½½, the approximated value function tends to have a unimodal profile since all kernels are put at the goal states. However, when 񮽙 is larger than ½½, this unimodality is broken and the surface of the approximated value function gets slightly fluctuated. This small fluctuation can cause an error in policies and therefore the performance is degraded at around 񮽙 ¾ ¼ . This performance degradation tends to be improved as the number of kernels is further increased.</p><p>Overall, the above result shows that when GGKs are combined with our kernel center allocation strategy, almost perfect policies can be obtained with a very small number of kernels. Therefore, the proposed method is computationally very advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robot Agent Navigation</head><p>The above simple robot arm control simulation shows that the GGK method is promising. Here we apply GGKs to a more challenging task of a mobile robot navigation, which involves a high-dimensional and continuous state space.</p><p>We employ a Khepera robot illustrated in <ref type="figure">Fig.8(a)</ref> on a navigation task. A Khepera is equipped with 񮽙 infra-red <ref type="bibr" target="#b2">3</ref> For illustration purposes, let us display the state value function Î 񮽙´×µ񮽙´×µ µ Ë Ë Ê, which is the expected long-term discounted sum of rewards the agent receives when the agent takes actions following policy 񮽙 from state ×.</p><p>From the definition, it can be confirmed that Î 񮽙´×µ񮽙´×µ is expressed Î 񮽙´×µ񮽙´×µ µ É 񮽙´××´×µµ񮽙´××񮽙´××´×µµ.   sensors ('s1' to 's8' in the <ref type="figure">figure)</ref> which measure the strength of the reflected light returned from surrounding obstacles. Each sensor produces a scalar value between ¼ and ½¼¾¿ (which may be regarded as continuous): the sensor obtains the maximum value ½¼¾¿ if an obstacle is just in front of the sensor and the value decreases as the obstacle gets farther till it reaches the minimum value ¼. Therefore, the state space Ë is 񮽙-dimensional and continuous. The Khepera has two wheels and takes the following 񮽙 defined actions: forward, left-rotation, right-rotation and backward (i.e., the action space 񮽙 contains 񮽙 actions). The speed of the left and right wheels for each action is described in <ref type="figure">Fig.8(a)</ref> in the bracket (the unit is pulse per 10 milliseconds). Note that the sensor values and the wheel speed are highly stochastic due to the change of the ambient light, noise, the skid etc. Furthermore, perceptual aliasing occurs due to the limited range and resolution of sensors. Therefore, the state transition is highly stochastic. We set the discount factor to ­ 񮽙 ¼ 񮽙񮽙.</p><p>The goal of the navigation task is to make the Khepera explore the environment as much as possible. To this end, we give a positive reward ·½ when the Khepera moves forward and a negative reward ¾ when the Khepera collides with an obstacle. We do not give any reward to the left/right rotation and backward actions. This reward design encourages the Khepera to go forward without hitting obstacles, through which extensive exploration in the environment could be achieved.</p><p>We collected training samples from ¾¼¼ series of ½¼¼ random movements in a fixed environment with several obstacles (see <ref type="figure">Fig.9(a)</ref>). Then we constructed a graph from the gathered samples by discretizing the continuous state space using the Self-Organizing Map (SOM) <ref type="bibr" target="#b15">[16]</ref>. The number of nodes (states) in the graph is set to 񮽙񮽙񮽙 (equivalent with the SOM map size of ¾¾ ¢ ¾¾); this value is computed by the standard rule-of-thumb formula 񮽙 Ô Ò <ref type="bibr" target="#b16">[17]</ref>, where Ò is the number of samples. The connectivity of the graph is determined by the state transition probability computed from the samples, i.e., if there is a state transition from one node to another in the samples, an edge is established between these two nodes and the edge weight is set according to the Euclidean distance between them. <ref type="figure">Fig.8(b)</ref> illustrates an example of the obtained graph structure-for visualization purposes, we projected the 񮽙-dimensional state space onto a ¾-dimensional subspace spanned by´</p><formula xml:id="formula_6">by´ ½ ½ ¼ ¼ ½ ½ ¼ ¼µ񮽙 ´¼ ¼ ½ ½ ¼ ¼ ½ ½µ񮽙 (9)</formula><p>The 񮽙-th element in the above bases corresponds to the output of the 񮽙-th sensor (see <ref type="figure">Fig.8(a)</ref>). Therefore, the projection onto this subspace roughly means that the horizontal axis corresponds to the distance to the left/right obstacle, while the vertical axis corresponds to the distance to the front/back obstacle. For clear visibility, we only displayed the edges whose weight is less than ¾¾¼. This graph has a notable feature: the nodes around the region 'B' in the figure are   directly connected to the nodes at 'A', but are not directly connected to the nodes at 'C', 'D', and 'E'. This implies that the geodesic distance from 'B' to 'C', 'D', or 'E' is large, although the Euclidean distance is small.</p><formula xml:id="formula_7">↑ ↑ ↑ ↑ ↑ ↑ ↑ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ⊃ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑↑ ↑ ↑ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑↑ ↑ ↑ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓↓ ↓ ↑↑ ↑ ↑ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓↓ ↓ ↓ ↓ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ ⊂ ⊂ ⊂ ⊂ ⊂ ↓ ↓ ↓ ↓ ↓ ↓ ↓↓ ↓ ↑↑ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ↓↓ ↓ ↓ ↓↓ ↓ ↑↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂⊂ ↓ ↓ ↓↓ ↓ ↑ ↑ ↑ ↑ ↑ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ↓⊂ ↓ ↑⊂ ↑ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂⊂ ⊂ ⊂ ⊂⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂⊂⊂⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ ⊂ (b) Ordinary Gaussian kernels</formula><p>Since the transition from one state to another is highly stochastic in the current experiment, we decided to simply duplicate the GGK function over the action space (see Eq. <ref type="formula">(6)</ref>). For obtaining continuous GGKs, GGK functions need to be interpolated (see Sec.III-D). We may employ a simple linear interpolation method in general. However, the current experiment has unique characteristics-at least one of the sensor values is always zero since the Khepera is never completely surrounded by obstacles. Therefore, samples are always on the surface of the 񮽙-dimensional hypercube-shaped state space. On the other hand, the node centers determined by the SOM are not generally on the surface. This means that any sample is not included in the convex hull of its nearest nodes and we need to extrapolate the function value. Here, we simply add the Euclidean distance between the sample and its nearest node when computing kernel values; more precisely, for a state × that is not generally located on a node center, the GGK-based basis function is defined as</p><formula xml:id="formula_8">񮽙 񮽙·´񮽙 ½µÑ´××½µÑ´×× µ µ Á ´񮽙 񮽙 񮽙´񮽙µ 񮽙´񮽙µ µ µ Ü Ô 񮽙´´´´×× 񮽙´´´´×× 񮽙 ×µ · Ë È ´ ××´񮽙µ ××´񮽙µ µµ ¾ ¾񮽙 ¾ 񮽙 񮽙 (10)</formula><p>where 񮽙 × is the node closest to × in the Euclidean distance. <ref type="figure" target="#fig_0">Fig.10</ref> illustrates an example of actions selected at each node by the GGK-based and OGK-based policies. We used ½¼¼ kernels and set the width to ½¼¼¼. The symbols '񮽙', '񮽙', '񮽙', and '񮽙' in the figure indicates forward, backward, left rotation, and right rotation actions. This shows that there is a clear difference in the obtained policies at the state 'C'; the backward action is most likely to be taken by the OGKbased policy while the left/right rotation are most likely to be taken by the GGK-based policy. This causes a significant difference in the performance. To explain this, let us assume that the Khepera is at the state 'C', i.e., it faces the wall. The GGK-based policy guides the Khepera from 'C' to 'A' via 'D' or 'E' by taking left/right rotation actions and it can avoid the obstacle successfully. On the other hand, the OGK-based policy leads Khepera from 'C' to 'A' via 'B' by taking backward actions; then the forward action is taken at 'B'. Thus, the Khepera returns to 'C' again and ends up moving back and forth between 'C' and 'B' (see also the attached video).</p><p>For the performance evaluation, we use a more complicated environment than the one used for gathering training samples (see <ref type="figure">Fig.9</ref>). Thus we are evaluating how well the obtained policies can be generalized to an unknown environment. In this test environment, we let the Khepera run from a fixed starting position (see <ref type="figure">Fig.9</ref>(b)) and take ½½¼ steps following the obtained policy. We compute the sum of rewards, i.e., ·½ for the forward action. If the Khepera collides with an obstacle before ½½¼ steps, we stop the evaluation. The mean test performance over ¾¼ independent runs is depicted in <ref type="figure" target="#fig_0">Fig.11</ref> as a function of the number of kernels. More precisely, in each run, we construct a graph based on the training samples taken from the training environment and put the specified number of kernels randomly on the graph. Then, a policy is learned by the GGK or OGK-based LSPI method using the training samples. The test performance is measured 񮽙 times for each policy and the average is output. <ref type="figure" target="#fig_0">Fig.11</ref> shows that GGKs significantly outperform OGKs, demonstrating that GGKs are promising even in the challenging setting with a highdimensional continuous state space. <ref type="figure" target="#fig_0">Fig. 12</ref> depicts the computation time of each method as a function of the number of kernels. This shows that the computation time monotonically increases as the number of kernels increases and the GGK-based and OGK-based methods have comparable computation time. This implies that the computation time of the GGK functions is negligible. Given that the GGK-based method works much better than the OGK-based method with a smaller number of kernels, the proposed method could be regarded as a computationally efficient alternative to the standard OGK-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND OUTLOOK</head><p>We proposed a new basis construction method for value function approximation. The proposed geodesic Gaussian kernels (GGKs) have several preferable properties such as the smoothness along the graph and easy computability. We demonstrated the practical usefulness of the proposed method for challenging applications: both the robot arm reaching with obstacles and the Khepera exploration experiments showed quantitative improvements as well as intuitive, interpretable behavioral advantages evident from the experiments.</p><p>Experiments in Sec.IV showed that GGKs with large width has larger MSEs than that with smaller width, but GGKs with large width gave better policies than that with smaller width. We conjecture that the GGKs with large width give smoother value functions and they result in stable policies. Although this explanation would be intuitively reasonable, it needs to be elucidated in a more rigorous way.</p><p>It is shown that the policies obtained by GGKs are not so sensitive to the choice of the width of the Gaussian kernels, i.e., a reasonable large width works very well. This is a very useful property in practice. Also, the heuristics of putting Gaussian centers on goal states is shown to work quite well. Even so, it is an important future direction to develop a method for optimally tuning the width as well as the location parameters, e.g., based on the statistical machine learning theory <ref type="bibr" target="#b8">[9]</ref>.</p><p>We defined the Gaussian kernels on the state space, and then extended them over the action space. If we define basis functions directly on the state-action space, the quality of value function approximation and the computational efficiency could be further improved. Our future research will focus on this topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustrative example of an RL task of guiding an agent to a goal in the grid world. (a) Black areas are walls over which the agent cannot move while the goal is represented in gray. Arrows on the grids represent one of the optimal policies. (b) Optimal state value function (in log-scale). (c) Graph induced by the MDP and a random policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .Fig. 4 .</head><label>234</label><figDesc>Fig. 2. Two intricated mazes used for simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 5. A two-joint robot arm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Number of successful trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 8. Khepera robot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Examples of obtained policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Computation time.</figDesc></figure>

			<note place="foot" n="1"> For the moment, we focus on discrete state spaces. In Sec.III-D, we extend the proposed method to the continuous state space.</note>

			<note place="foot" n="2"> There are two alternative approaches: Bellman residual minimization and fixed point approximation. We take the latter approach following the suggestion in the reference [2].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Least-squares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ten Lectures on Wavelets. Philadelphia and Pennsylvania: Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proto-value functions: Developmental reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory</title>
		<meeting><address><addrLine>Providence, R.I.</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Value function approximation with diffusion wavelets and Laplacian eigenfunctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, B. Schölkopf, and J. Platt</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="843" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reinforcement learning with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Bonn, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fibonacci heaps and their uses in improved network optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Fredman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="615" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geodesic Gaussian kernels for value function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2006 Workshop on Information-Based Induction Sciences</title>
		<meeting>2006 Workshop on Information-Based Induction Sciences<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11-02" />
			<biblScope unit="page" from="316" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computing the shortest path: A* search meets graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harrelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-Organizing Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SOM toolbox for Matlab 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vesanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Himberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alhoniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parhankangas</surname></persName>
		</author>
		<idno>A57</idno>
		<ptr target="http://www.cis.hut.fi/projects/somtoolbox/package/papers/techrep.pdf" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
