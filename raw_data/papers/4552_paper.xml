<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Worst-Case Comparison between Temporal Difference and Residual Gradient with Linear Function Approximation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
							<email>lihong@cs.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>110 Frelinghuysen Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Worst-Case Comparison between Temporal Difference and Residual Gradient with Linear Function Approximation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases. This paper employs techniques from online learning of linear functions and provides a worst-case (non-probabilistic) analysis to compare these two types of algorithms when linear function approximation is used. No statistical assumptions are made on the sequence of observations, so the analysis applies to non-Markovian and even adversarial domains as well. In particular, our results suggest that RG may result in smaller temporal differences , while TD(0) is more likely to yield smaller prediction errors. These phenomena can be observed even in two simple Markov chain examples that are non-adversarial.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) is a learning paradigm for optimal sequential decision making <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996;</ref><ref type="bibr" target="#b14">Sutton &amp; Barto, 1998</ref>) and has been successfully applied to a number of challenging problems. In the RL framework, the agent interacts with the environment in discrete timesteps by repeatedly observing its current state, taking an action, receiving a real-valued reward, and transitioning to a next state. A policy is a function that maps states to actions; semantically, it specifies what action to take given the current state. The goal of an agent is to optimize its policy in order to maximize the expected long-term return, namely, the discounted sum of rewards it receives by following the policy. An important step in this optimization process is policy evaluation-the problem of evaluating expected returns of a fixed policy. This problem is often the most challenging step in approximate policy-iteration algorithms <ref type="bibr" target="#b1">(Bertsekas &amp; Tsitsiklis, 1996;</ref><ref type="bibr" target="#b7">Lagoudakis &amp; Parr, 2003)</ref>. Temporal difference (TD) is a family of algorithms for policy evaluation <ref type="bibr" target="#b13">(Sutton, 1988)</ref> and has received a lot of attention from the community. Unfortunately, it is observed (e.g., <ref type="bibr" target="#b0">Baird (1995)</ref>) that TD methods may diverge when they are combined with function approximation. An alternative algorithm known as residual gradient (RG) was proposed by <ref type="bibr" target="#b0">Baird (1995)</ref> and enjoys guaranteed convergence to a local optimum. Since RG is similar to TD(0), a particular instance of the TD family, we will focus on RG, TD(0), and a variant of TD(0) in this paper.</p><p>Despite convergence issues, little is known that compares RG and TD(0). Building on previous work on online learning of linear functions <ref type="bibr" target="#b3">(Cesa-Bianchi et al., 1996</ref>) and a similar analysis by <ref type="bibr" target="#b11">Schapire and Warmuth (1996)</ref>, we provide a worst-case (nonprobabilistic) analysis of these algorithms and focus on two evaluation metrics: (i) total squared prediction error, and (ii) total squared temporal difference. The former measures accuracy of the predictions, while the latter measures consistency and is closely related to the Bellman error <ref type="bibr" target="#b14">(Sutton &amp; Barto, 1998</ref>).</p><p>Either metric may be preferred over the other in different situations. For instance, <ref type="bibr" target="#b7">Lagoudakis and Parr (2003)</ref> argue that TD solutions tend to preserve the shape of the value function and is more suitable for approximate policy iteration, while there is evidence that minimizing squared Bellman errors is more robust in general <ref type="bibr" target="#b8">(Munos, 2003)</ref>. Our analysis suggests that TD can make more accurate predictions, while RG can result in smaller temporal differences. All terms will be made precise in the next section. Although our theory focuses on worst-case upper bounds, we also provide numerical evidence and expect the resulting insights to give useful guidance to RL practitioners in deciding which algorithm best suits their purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Fully observable environments in RL are often modelled as Markov decision processes <ref type="bibr" target="#b10">(Puterman, 1994)</ref>, which are equivalent to induced Markov chains when controlled by a fixed policy. Here, however, we consider a different model that is suitable for worst-case analysis, as introduced in the next subsection. This model makes no statistical assumption about the observations, and thus our results apply to much more general situations including partially observable or adversarial environments that subsume Markov chains.</p><p>Some notation is in order. We use bold-face, lower-case letters to denote real-valued column vectors such as v. Their components are denoted by the corresponding letter with subscripts such as v t . We use 񮽙·· to denote the Euclidean, or 񮽙 2 -norm: 񮽙v񮽙 = √ v 񮽙 v where v 񮽙 is the transpose of v. For a square matrix M , the set of eigenvalues of M , known as the spectrum of M , is denoted σ(M ). If M is symmetric, its eigenvalues must be real, and its largest eigenvalue is denoted ρ(M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Sequential Online Learning Model</head><p>Our learning model is adopted from <ref type="bibr">Schapire and War- muth (1996)</ref> and is an extension of the online-learning model to sequential prediction problems. Let k be the dimension of input vectors. The agent maintains a weight vector of the same dimension and uses it to make predictions. In RL, input vectors are often feature vectors of states or state-action pairs, and are used to approximate value functions <ref type="bibr" target="#b14">(Sutton &amp; Barto, 1998)</ref>. Learning proceeds in discrete timesteps and terminates after T steps. The agent starts with an initial input vector x 1 ∈ R k and an initial weight vector</p><formula xml:id="formula_0">w 1 ∈ R k . At timestep t ∈ {1, 2, 3, · · · , T }:</formula><p>• The agent makes a predictionˆypredictionˆ predictionˆy t = w 񮽙 t x t ∈ R, where w t is the weight vector at time t. Throughout the paper, assume 񮽙x t 񮽙 ≤ X for some known constant X &gt; 0.</p><p>• The agent then observes an immediate reward r t ∈ R and the next input vector x t+1 . Based on this information, it updates its weight vector whose new value is denoted w t+1 . The change in weight is ∆w t = w t+1 − w t .</p><p>By convention, r t = 0 and x t = 0 for t &gt; T . Define the return at time t by y t = 񮽙 ∞ τ =t γ τ −t r τ , where γ ∈ [0, 1) is the discount factor. Since γ &lt; 1, it effectively diminishes future rewards exponentially fast. A quick observation is that y t = r t + γy t+1 , which is analogous to the Bellman equation for Markov chains <ref type="bibr" target="#b14">(Sutton &amp; Barto, 1998)</ref>. The agent attempts to mimic y t by its predictionˆypredictionˆ predictionˆy t , and the prediction error is e t = y t − ˆ y t . Our first evaluation metric is the total squared prediction error :</p><formula xml:id="formula_1">񮽙 P = 񮽙 T t=1 e 2 t = 񮽙e񮽙 2 .</formula><p>Another useful metric in RL is the temporal differences (also known as TD errors), which measures how consistent the predictions are. In particular, the temporal difference at time t is d t = r t + γw 񮽙 t x t+1 − w 񮽙 t x t , and the total squared temporal difference is</p><formula xml:id="formula_2">񮽙 T D = 񮽙 T t=1 d 2 t = 񮽙d񮽙 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Previous Work</head><p>Previous convergence results of TD and RG often rely heavily on certain stochastic assumptions of the environment such as the assumption that the sequence of observations, [(x t , r t )] t∈N , are generated by an irreducible and aperiodic Markov chain. Tsitsiklis and Van Roy (1997) first proved convergence of TD with linear function approximation, while they also pointed out the potential divergence risk when nonlinear approximation is used.</p><p>To resolve the instability issue of TD <ref type="formula">(0)</ref>, <ref type="bibr" target="#b0">Baird (1995)</ref> proposed the RG algorithm, but also noted that RG may converge more slowly than TD(0) in some problems. Such an observation was later proved by <ref type="bibr" target="#b12">Schoknecht and Merke (2003)</ref>, who used spectral analysis to compare the asymptotic convergence rates of the two algorithms. Although their results are interesting, they only apply to quite limited cases where, for example, a certain matrix associated with TD updates has real eigenvalues only (which does not hold in general). More importantly, they study synchronous updates while TD and RG are often applied asynchronously in practice. Furthermore, their results assume that the value function is represented by a lookup table, but the initial motivation of studying RG was to develop a provably convergent algorithm when function approximation is used.</p><p>Schapire and Warmuth (1996) were also concerned with similar worst-case behavior of TD-like algorithms within the model described in Subsection 2.1. They defined a new class of algorithms called TD * (λ), which is very similar to the TD(λ) algorithms of Sutton (1988). They developed worst-case bounds for the total squared prediction error of TD * (λ), but not the total squared temporal difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Algorithms</head><p>The algorithms we consider all update the weight vector incrementally and differ only in the update rules. TD(0) uses the following rule:</p><formula xml:id="formula_3">∆w t = ηd t x t ,<label>(1)</label></formula><p>where η ∈ (0, 1) is the step-size parameter controlling aggressiveness of the update. Although TD(0) is widely used in practice, analysis turns out to be easier with a close relative of it, TD * (0). This algorithm differs from TD(0) in that it adapts the step-size based on the input vectors ( <ref type="bibr" target="#b11">Schapire and Warmuth (1996)</ref> defined TD * (0) in a different, but equivalent, form):</p><formula xml:id="formula_4">∆w t = ηd t x t 1 − γηx 񮽙 t x t+1 .<label>(2)</label></formula><p>Due to space limitation, we only provide results for TD * (0), but similar results hold for TD(0). It is expected, and also supported by the numerical evidence in Section 4, that TD(0) and TD * (0) have similar behavior and performance in practice. For this reason, we refer to both algorithms as TD in the rest of the paper if there is no risk of confusion. In contrast, RG uses the following update rule:</p><formula xml:id="formula_5">∆w t = ηd t (x t − γx t+1 ) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Main Results</head><p>This section contains the main theoretical results. We will first describe how to evaluate an algorithm in the worst-case scenario. For completeness, we also summarize the squared prediction error bounds for TD * (0) due to <ref type="bibr" target="#b11">Schapire and Warmuth (1996)</ref>. Then, we analyze total squared temporal difference bounds and RG.</p><p>Our analysis makes a few uses of matrix theory (see, e.g., <ref type="bibr" target="#b5">Horn and Johnson (1986)</ref>), and several technical lemmas are found in the appendix. Two basic facts about ρ(M ) will be used repeatedly: (i) if M is negative-definite, then ρ(M ) &lt; 0; and (ii) the Rayleigh-Ritz theorem <ref type="bibr" target="#b5">(Horn &amp; Johnson, 1986</ref>, Theorem 4.2.2) states that ρ(M ) = max v񮽙 =0</p><formula xml:id="formula_6">v 񮽙 M v v 񮽙 v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Criterion</head><p>Analogous to other online-learning analysis, we treat 񮽙 P and 񮽙 T D as total losses, and compare the total loss of an algorithm to that of an arbitrary weight vector, u. We wish to prove that this difference is small for all u, including the optimal (in any well-defined sense) but unknown vector u * .</p><p>The prediction using vector u at time t is y u t = u 񮽙 x t . Accordingly, the prediction error and temporal difference at time t are e u t = y t − y u t and d u t = r t + γu 񮽙 x t+1 − u 񮽙 x t , respectively. The total squared prediction error and total squared temporal difference of u are</p><formula xml:id="formula_7">񮽙 u P = 񮽙e u 񮽙 2 = 񮽙 T t=1 񮽙 y t − u 񮽙 x t 񮽙 2 and 񮽙 u T D = 񮽙d u 񮽙 2 = 񮽙 T t=1 񮽙 r t + γu 񮽙 x t+1 − u 񮽙 x t 񮽙 2 ,</formula><p>respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Squared Prediction Errors of TD * (0)</head><p>Using step-size η = 1 X 2 +1 , Schapire and Warmuth (1996) showed a worst-case upper bound:</p><formula xml:id="formula_8">񮽙 P ≤ 񮽙 1 + X 2 񮽙 񮽙 񮽙 u P + 񮽙w 1 − u񮽙 2 2 񮽙 1 − γ 2 .</formula><p>Furthermore, if E and W are known beforehand such that 񮽙 u P ≤ E and 񮽙w 1 − u񮽙 ≤ W , then the step-size η can be optimized by η =</p><formula xml:id="formula_9">W X √ E+X 2 W</formula><p>to yield an asymptotically better bound:</p><formula xml:id="formula_10">񮽙 P ≤ 񮽙 u P + 2W X √ E + X 2 W 2 1 − γ 2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Squared Temporal Differences of TD * (0)</head><p>We will extend the analysis of <ref type="bibr">Schapire and War- muth (1996)</ref> to the new loss function 񮽙 T D by examining how the potential function, 񮽙w t − u񮽙 2 , evolves when a single update is made at time t. It can be shown <ref type="bibr">(Schapire &amp; Warmuth, 1996, Eqn 8</ref>) that</p><formula xml:id="formula_11">− −w 1 − u񮽙 2 ≤ η 2 X 2 e 񮽙 D 񮽙 De + 2ηe 񮽙 D 񮽙 (e u − e),</formula><p>where</p><formula xml:id="formula_12">D = ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 1 −γ 0 · · · 0 0 0 1 −γ · · · 0 0 . . . 0 0 · · · 1 −γ 0 0 0 · · · 0 1 −γ 0 0 · · · 0 0 1 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ .<label>(5)</label></formula><p>Define f = De. According to Lemma A.1(1), d u = De u , and hence the inequality above is rewritten as:</p><formula xml:id="formula_13">− −w 1 − u񮽙 2 ≤ η 2 X 2 f 񮽙 f − 2ηf 񮽙 D −1 f + 2ηf 񮽙 D −1 d u .</formula><p>Using the fact that 2p 񮽙 q ≤ ≤p񮽙</p><formula xml:id="formula_14">2 + 񮽙q񮽙 2 for p = η √ b D −− f , q = √ bd u ,</formula><p>and arbitrary b &gt; 0, the inequal- </p><formula xml:id="formula_15">ity becomes − −w 1 − u񮽙 2 ≤ f 񮽙 M 1 f + bb u T D , where</formula><formula xml:id="formula_16">M 1 = η 2 X 2 I + η 2 b D −1 D −− − η(D −1 + D −− ) (6</formula><formula xml:id="formula_17">(1 + γ) 2 񮽙 X 2 + 1 b(1 − γ) 2 񮽙 񮽙 bb u T D + 񮽙w 1 − b񮽙 2 񮽙 ,</formula><p>when the step-size is</p><formula xml:id="formula_18">η = 1 (1 + γ) 񮽙 X 2 + 1 b(1−γ) 2 񮽙 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Worst-Case Comparison between TD and RG with Linear Function Approximation</head><p>Due to Lemma A.1 (2), we have</p><formula xml:id="formula_19">d 2 t = 񮽙 1 − γηx 񮽙 t x t+1 񮽙 2 f 2 t ≤ 񮽙 1 + γηX 2 񮽙 2 f 2 t ≤ (1 + 2γ) 2 (1 + γ) 2 f 2 t .</formula><p>Therefore, 񮽙 T D is at most</p><formula xml:id="formula_20">(1 + 2γ) 2 񮽙 X 2 + 1 b(1 − γ) 2 񮽙 񮽙 bb u T D + 񮽙w 1 − u񮽙 2 񮽙 .</formula><p>Using b = 1, we have thus proved the first main result.</p><p>Theorem 3.1. Let η be given by Eqn 7 using b = 1, then the following holds for TD * (0): </p><formula xml:id="formula_21">񮽙 T D ≤ (1+2γ) 2 񮽙 X 2 + 1 (1 − γ) 2 񮽙 񮽙 񮽙 u T D + 񮽙w 1 − u񮽙 2 񮽙 .</formula><formula xml:id="formula_22">񮽙 T D ≤ (1 + 2γ) 2 񮽙 񮽙 u T D (1 − γ) 2 + 2XW √ E 1 − γ + X 2 񮽙w 1 − u񮽙 2 񮽙 .<label>(8)</label></formula><p>Proof. Previous analysis for Theorem 3.1 yields</p><formula xml:id="formula_23">񮽙 T D ≤ (1 + 2γ) 2 񮽙񮽙 bX 2 񮽙 u T D + 񮽙w 1 − u񮽙 2 b(1 − γ) 2 񮽙 + 񮽙 񮽙 u T D (1 − γ) 2 + X 2 񮽙w 1 − u񮽙 2 񮽙񮽙 ≤ (1 + 2γ) 2 񮽙񮽙 bX 2 E + W 2 b(1 − γ) 2 񮽙 + 񮽙 񮽙 u T D (1 − γ) 2 + X 2 񮽙w 1 − u񮽙 2 񮽙񮽙 for any b &gt; 0. We may simply choose b = W X(1−γ) √ E</formula><p>, and the step-size in Eqn 7 becomes</p><formula xml:id="formula_24">η = 1 (1 + γ) 񮽙 X 2 + X √ E W (1−γ)</formula><p>񮽙 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Squared Prediction Errors of RG</head><p>By the update rule in Eqn 3 and simple algebra,</p><formula xml:id="formula_25">∆w 񮽙 t (w t − u) = ηd t (x t − γx t+1 ) 񮽙 (w t − u) = ηd t 񮽙񮽙 w 񮽙 t x t − γw 񮽙 t x t+1 − r t 񮽙 − 񮽙 u 񮽙 x t − γu 񮽙 x t+1 − r t 񮽙񮽙 = ηd t (d u t − d t ), 񮽙∆w t 񮽙 2 = η 2 d 2 t 񮽙x t − γx t+1 񮽙 2 2 ≤ η 2 d 2 t X 2 (1 + γ) 2 .</formula><p>Similar to the previous section, we use the potential function 񮽙w t − u񮽙 2 to measure progress of learning:</p><formula xml:id="formula_26">− −w 1 − u񮽙 2 ≤ T 񮽙 t=1 񮽙 񮽙w t+1 − u񮽙 2 − −w t − u񮽙 2 񮽙 = T 񮽙 t=1 񮽙 2∆w 񮽙 t (w t − u) + ∆w 񮽙 t ∆w t 񮽙 ≤ T 񮽙 t=1 񮽙 2ηd t (d u t − d t ) + η 2 d 2 t X 2 (1 + γ) 2 񮽙 = 2ηd 񮽙 d u − 2ηd 񮽙 d + η 2 X 2 (1 + γ) 2 d 񮽙 d.</formula><p>According to Lemma A.1 (1) and using the fact that</p><formula xml:id="formula_27">2p 񮽙 q ≤ ≤p񮽙 2 + 񮽙q񮽙 2 for p = η √ b D 񮽙 d, q = √</formula><p>be u , and arbitrary b &gt; 0, the inequality above is written as:</p><formula xml:id="formula_28">− −w 1 − u񮽙 2 ≤ b 񮽙e u 񮽙 2 + η 2 b d 񮽙 DD 񮽙 d + 񮽙 η 2 X 2 (1 + γ) 2 − 2η 񮽙 񮽙d񮽙 2</formula><p>Due to Lemma A.1 (3), d = ΣDe, where</p><formula xml:id="formula_29">Σ = diag 񮽙 1 1 + γη(x 1 − γx 2 ) 񮽙 x 2 , 1 1 + γη(x 2 − γx 3 ) 񮽙 x 3 , · · · , 1 1 + γη(x T −1 − γx T ) 񮽙 x T , 1 񮽙 .<label>(9)</label></formula><p>Then, the inequality above becomes:</p><formula xml:id="formula_30">− −w 1 − u񮽙 2 ≤ b 񮽙e u 񮽙 2 + e 񮽙 M 2 e, where M2 = D 񮽙 Σ " η 2 b DD 񮽙 + ` η 2 X 2 (1 + γ) 2 − 2η´I 2η´2η´I « ΣD. (10)</formula><p>Since e 񮽙 M 2 e ≤ ρ(M 2 ) 񮽙e񮽙 2 , Lemma A.5 implies the following theorems when the step-size is</p><formula xml:id="formula_31">η = 1 (1 + γ) 2 񮽙 X 2 + 1 b 񮽙 .<label>(11)</label></formula><p>Theorem 3.3. Let η be given by Eqn 11 using b = 1, then the following holds for RG:</p><formula xml:id="formula_32">񮽙 P ≤ (1 + 2γ) 2 񮽙 X 2 + 1 񮽙 (1 − γ) 2 񮽙 񮽙 u P + 񮽙w 1 − u񮽙 2 񮽙 .</formula><p>Theorem 3.4. If E and W are known beforehand such that 񮽙 u P ≤ E and 񮽙w 1 − u񮽙 ≤ W , then η can be optimized in RG so that</p><formula xml:id="formula_33">񮽙 P ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙 񮽙 u P + 2XW √ E + X 2 񮽙w 1 − u񮽙 2 񮽙 .<label>(12)</label></formula><p>Proof. Previous analysis in this subsection yields</p><formula xml:id="formula_34">񮽙 P ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙񮽙 񮽙 u P + X 2 񮽙w 1 − u񮽙 2 񮽙 + 񮽙 X 2 bb u P + 񮽙w 1 − u񮽙 2 b 񮽙񮽙 ≤ (1 + 2γ) 2 (1 − γ) 2 񮽙񮽙 񮽙 u P + X 2 񮽙w 1 − u񮽙 2 񮽙 + 񮽙 X 2 bE + W 2 b 񮽙񮽙 . We simply choose b = W X √ E</formula><p>and accordingly the stepsize in Eqn 11 becomes</p><formula xml:id="formula_35">η = 1 (1 + γ) 2 񮽙 X 2 + X √ E W 񮽙 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Squared Temporal Differences of RG</head><p>It is most convenient to turn this problem into one of analyzing the total squared prediction error in the original online-learning-of-linear-function framework ( <ref type="bibr" target="#b3">Cesa-Bianchi et al., 1996</ref>). In particular, define z t = x t − γx t+1 and thus 񮽙z t 񮽙 ≤ (1 + γ)X. Now, RG can be viewed as a gradient descent algorithm operating over the sequence of data <ref type="bibr">[(z</ref>  to obtain the following improved bound:</p><formula xml:id="formula_36">񮽙 T D ≤ 񮽙 u T D + 2W X(1 + γ) √ E + (1 + γ) 2 W 2 X 2 . (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Discussions</head><p>Based on Eqns 4, 8, 12, and 13, <ref type="table" target="#tab_3">Table 1</ref> summarizes the asymptotic upper bounds (when T → ∞) assuming E and W are known beforehand to optimize η. 1 Although our bounds are all upper bounds, results in the table suggest that, in worst cases, TD * (0) (and also TD(0)) tend to make smaller prediction errors, while RG tends to make smaller temporal differences. The gaps between corresponding bounds increase as 1 Strictly speaking, the validity of these asymptotic results relies on the assumptions that (i) √ E = o(񮽙 u P ), and (ii) W and X remain constant as T → ∞. Both assumptions are reasonable in practice.  <ref type="formula" target="#formula_3">(1)</ref> 1 + o <ref type="formula" target="#formula_3">(1)</ref> γ → 1. On the other extreme where γ = 0, all these asymptotic bounds coincide, which is not surprising as TD <ref type="formula">(0)</ref>, TD * (0), and RG are all identical when γ = 0.</p><formula xml:id="formula_37">񮽙 P // u P 񮽙 T D // u T D TD * (0) 1 1−γ 2 + o(1) (1+2γ) 2 (1−γ) 2 + o(1) RG (1+2γ) 2 (1−γ) 2 + o</formula><p>Since it is unknown whether the leading constants in <ref type="table" target="#tab_3">Table 1</ref> are optimal, the next section will provide numerical evidence to support our claims about the relative strengths of these algorithms.</p><p>It is worth mentioning that in sequential prediction or decision problems, the factor 1 1−γ often plays a role similar to the decision horizon <ref type="bibr" target="#b10">(Puterman, 1994)</ref>. Therefore, in some sense, our bounds also characterize how prediction errors and temporal differences may scale with decision horizon, in the worst-case sense.</p><p>When 񮽙 P or 񮽙 T D are relatively small, the asymptotic bounds in <ref type="table" target="#tab_3">Table 1</ref>  Since our setting is quite different from that of <ref type="bibr" target="#b12">Schoknecht and Merke (2003)</ref>, our results are not comparable to theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section presents empirical evidence in two Markov chains that supports our claims in Section 3.6.</p><p>The first is the Ring Markov chain <ref type="figure" target="#fig_2">(Figure 1 (a)</ref>), a variant of the Hall problem introduced by <ref type="bibr" target="#b0">Baird (1995)</ref> in which RG was observed to converge to the optimal weights more slowly than TD(0). The state space is a ring consisting of 10 states numbered from 0 through 9. Each state is associated with a randomly selected feature vector of dimension k = 5:</p><formula xml:id="formula_38">x (0) , · · · , x (9) ∈ R k .</formula><p>Transitions are deterministic and are indicated by arrows. The reward in every state is stochastic and is distributed uniformly in [−0.1, 0.1].</p><p>As in Hall , the value of every state is exactly 0.</p><p>The second problem is a benchmark problem known as PuddleWorld <ref type="bibr" target="#b2">(Boyan &amp; Moore, 1995)</ref>. The state space is a unit square <ref type="figure" target="#fig_2">(Figure 1 (d</ref>  The agent adopts a fixed policy that goes north or east with probability 0.5 each. Every episode takes about 40 steps to terminate. The reward is −1 unless the agent steps into the puddles and receives penalty for that; the smallest possible reward is −41. We used 16 RBF features of width 0.3, whose centers were evenly distributed in the state space. We also tried a degree-two polynomial feature: for a state s = (s 1 , s 2 ) 񮽙 , the feature vector had six components:</p><formula xml:id="formula_39">x s = 񮽙 1, s 1 , s 2 , s 1 s 2 , s 2 1 , s 2 2</formula><p>񮽙 񮽙 . Since the results are similar to those for RBF features, they are not included.</p><p>We ran three algorithms in the experiments: TD(0), TD * (0), and RG. For a fair comparison, all algorithms started with the all-one weight vector and were given the same sequence of (x t , r t ) for learning. The procedure was repeated 500 times. For Ring , each run used a different realization of feature x (s) and T = 500; for PuddleWorld , each run consisted of 50 episodes (yielding slightly less than 2000 steps in total). A wide range of step-sizes were tried, and the best choices for each discount-factor-algorithm combination were used to evaluate 񮽙 P and 񮽙 T D , respectively. <ref type="figure" target="#fig_2">Figure 1</ref> (b,c,e,f) gives the average per-step squared prediction errors and squared temporal differences for these two problems, with 99% confidence intervals plotted.</p><p>These results are consistent with our analysis: TD <ref type="formula">(0)</ref> and TD * (0) tended to make more accurate predictions, while RG did a better job at minimizing temporal differences; the differences between these algorithms were even larger as the discount factor γ approached 1. 2 Finally, as a side effect, it is verified that TD(0) and TD * (0) had essentially identical performance, although their best learning rates might differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have carried out a worst-case analysis to compare two policy-evaluation algorithms, TD and RG, when linear function approximation is used. Together with previously known results due to <ref type="bibr">Schapire and War- muth (1996)</ref> and <ref type="bibr" target="#b3">Cesa-Bianchi et al. (1996)</ref>, our results suggest that, although the TD algorithms may make more accurate predictions, RG may be a better choice when small temporal differences are desired. This claim is supported by empirical evidence in two simple Markov chains. Although the analysis is purely mathematical, we expect the implications to deepen the understanding of these two types of algorithms and can provide useful insights to RL practitioners.</p><p>There has been relatively little attention to this sort of online-learning analysis within the RL community. Our analysis shows that this kind of analysis may be helpful and provide useful insights. A few directions are worth pursuing. First, we have focused on worst-case upper bounds, but it remains open whether matching lower bounds can be found. More extensive empirical studies are also necessary to see if such worst-case behavior can be observed in realistic problems. Second, we wish to generalize the analysis of total squared temporal difference from TD(0) and TD * (0) to TD(λ) and TD * (λ), respectively. Finally, we would like to mention that, in their original forms, both TD and RG use additive updates. Another class of updates known as multiplicative updates <ref type="bibr" target="#b6">(Kivinen &amp; Warmuth, 1997</ref>) has been useful when the number of features (i.e., the k in Subsection 2.1) is large but only a few of them are relevant for making predictions. Such learning rules have potential uses in RL <ref type="bibr" target="#b9">(Precup &amp; Sutton, 1997</ref>), but it remains open whether these algorithms converge or whether worst-case error bounds similar to the ones given in this paper can be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lemmas and Proofs</head><p>Lemma A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">This lemma collects a few basic facts useful in our analysis (D is given in Eqn 5):</head><p>1. In all three algorithms,</p><formula xml:id="formula_40">d u = De u . 2. In TD * (0), d t = (1 − γηx 񮽙 t x t+1 )(e t − γe t+1 ). 3. In RG, d t = et−γet+1 1+γη(xt−γxt+1) 񮽙 xt+1 .</formula><p>Proof. 1. Since y t = r t + γy t+1 , we have</p><formula xml:id="formula_41">d u t = r t + γu 񮽙 x t+1 − u 񮽙 x t = 񮽙 y t − u 񮽙 x t 񮽙 − 񮽙 y t − r t − γu 񮽙 x t+1 񮽙 = 񮽙 y t − u 񮽙 x t 񮽙 − γ 񮽙 y t+1 − u 񮽙 x t+1 񮽙 = e u t − γe u t+1 .</formula><p>In matrix form, this is d u = De u . 2. Since w t = w t+1 − ∆w t and y t = r t + γy t+1 ,</p><formula xml:id="formula_42">d t = r t + γw 񮽙 t x t+1 − w 񮽙 t x t = r t + γ(w t+1 − ∆w t ) 񮽙 x t+1 − w 񮽙 t x t + (y t − r t − γy t+1 ) = (y t − w 񮽙 t x t ) − γ(y t+1 − w 񮽙 t+1 x t+1 ) − γ∆w 񮽙 t x t+1 = e t − γe t+1 − γηd t x 񮽙 t x t+1 1 − γηx 񮽙 t x t+1 .</formula><p>Reorganizing terms will complete the proof. 3. Similar to the proof for part <ref type="bibr">(2)</ref>  </p><formula xml:id="formula_43">D −− or D −− D −1 . Then, σ (A) ⊆ 񮽙 (1 − γ) 2 , (1 + γ) 2 񮽙 and σ (B) ⊆ 񮽙 (1 + γ) −2 , (1 − γ) −2 񮽙 . Proof. It can be verified that D 񮽙 D equals ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 1 −γ 0 · · · 0 −γ 1 + γ 2 −γ · · · 0 . . . 0 0 · · · 1 + γ 2 −γ 0 0 · · · −γ 1 + γ 2 ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ . Since D 񮽙 D is symmetric, σ 񮽙 D 񮽙 D 񮽙 ⊂ R. It fol-</formula><p>lows from Geršgorin's theorem <ref type="bibr">(Horn &amp; Johnson, 1986, Theorem 6</ref> and Proof. By Weyl's theorem <ref type="bibr">(Horn &amp; Johnson, 1986, Theorem 4.3.1)</ref>,</p><formula xml:id="formula_44">.1.1) that σ 񮽙 D 񮽙 D 񮽙 ⊆ 񮽙 (1 − γ) 2 , (1 + γ) 2 񮽙 .</formula><formula xml:id="formula_45">D −− D −1 = 񮽙 DD 񮽙 񮽙 −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.3. Let D be given by Eqn 5, then</head><formula xml:id="formula_46">σ(D −1 + D −− ) ⊆ 񮽙 2 1 + γ , 2 1 − γ 񮽙 . Proof. It can be verified that D −1 + D −− equals G = 0 B B B B B B B @ 2 γ γ 2 · · · γ T −2 γ T −1 γ 2 γ · · · γ T −3 γ T −2 . . . γ T −3 γ T −4 · · · 2 γ γ 2 γ T −2 γ T −3 · · · γ 2 γ γ T −1 γ T −2 · · · γ 2 γ 2 1 C C C C C C</formula><formula xml:id="formula_47">ρ(M 1 ) ≤ ρ 񮽙 η 2 X 2 I 񮽙 + ρ 񮽙 η 2 b D −1 D −− 񮽙 + ρ 񮽙 −η 񮽙 D −1 + D −− 񮽙񮽙 .</formula><p>The lemma then follows immediately from Lemmas A.2 and A.3.</p><p>Lemma A.5. Let M 2 be defined by Eqn 10 and suppose the step-size is given by Eqn 11, then</p><formula xml:id="formula_48">ρ(M 2 ) ≤ − (1 − γ) 2 (1 + 2γ) 2 񮽙 X 2 + 1 b 񮽙 . Proof. Let α = η 2 b and β = η 2 X 2 (1 + γ) 2 − 2η, then M 2 = D 񮽙 Σ 񮽙 αDD 񮽙 + βI 񮽙 ΣD. It is known that ρ(M 2 ) = max v1񮽙 =0 v 񮽙 1 M 2 v 1 v 񮽙 1 v 1 .</formula><p>Define v 2 = Dv 1 and we have:</p><formula xml:id="formula_49">ρ(M 2 ) = max v2񮽙 =0 v 2 Σ 񮽙 αDD 񮽙 + βI 񮽙 Σv 2 v 񮽙 2 D −− D −1 v 2 ≤ max v2񮽙 =0 (1 − γ) 2 v 2 Σ 񮽙 αDD 񮽙 + βI 񮽙 Σv 2 v 񮽙 2 v 2 ,</formula><p>where the last step is due to Lemma A.2 and the fact that M 2 is negative-definite for η 񮽙 1. Similarly, we define v 3 = Σv 2 and use the fact that</p><formula xml:id="formula_50">0 ≤ v 񮽙 2 v 2 = v 񮽙 3 Σ −2 v 3 ≤ 񮽙 1 + γ(1 + γ)ηX 2 񮽙 2 񮽙v 3 񮽙 2</formula><p>to obtain:</p><formula xml:id="formula_51">ρ(M 2 ) ≤ max v3񮽙 =0 (1 − γ) 2 v 񮽙 3 񮽙 αDD 񮽙 + βI 񮽙 v 񮽙 3 (1 + γ(1 + γ)ηX 2 ) 2 v 񮽙 3 v 3 = (1 − γ) 2 ρ 񮽙 αDD 񮽙 + βI 񮽙 (1 + γ(1 + γ)ηX 2 ) 2 ≤ (1 − γ) 2 񮽙 α(1 + γ) 2 + β 񮽙 (1 + γ(1 + γ)ηX 2 ) 2 .</formula><p>If we choose η as in Eqn 11, then the lemma follows immediately from the fact that</p><formula xml:id="formula_52">1 + γX 2 (1 + γ) 񮽙 X 2 + 1 b 񮽙 ≤ 1 + γ 1 + γ = 1 + 2γ 1 + γ .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Appearing in Proceedings of the 25 th International Confer- ence on Machine Learning, Helsinki, Finland, 2008. Copy- right 2008 by the author(s)/owner(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>)), and a start state of an episode is randomly selected in [0, 0.2] × [0, 0.2].per-step squared tem poral dif f erence (d) PuddleWorld (e) per-step squared prediction error (f) per-step squared temporal difference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Two Markov chains we used: (a) Ring and (d) PuddleWorld (Boyan &amp; Moore, 1995). All results are averaged over 500 runs, with 99% confidence intervals plotted. Ring and PuddleWorld results are in (b,c) and (e,f), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A Worst-Case Comparison between TD and RG with Linear</head><label></label><figDesc>Clearly, (G − I) −1 is symmetric, and it follows from Geršgorin's theorem that σ 񮽙 (G − I) −1 񮽙 ⊆We are now ready to prove the following lemma. Lemma A.4. ρ(M 1 ) ≤ − 2η 1+γ + η 2 񮽙 X 2 + 1 b(1−γ) 2 񮽙 where M 1 is given in Eqn 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 ) is the largest eigenvalue of M 1 , we have f 񮽙 M 1 f ≤ ρ(M 1 ) 񮽙f 񮽙 2 , and hence, − −w 1 − u񮽙 2 ≤ ≤f 񮽙 2 ρ(M 1 ) + bb u T D . Combining this with Lemma A.4, we have that 񮽙f 񮽙 2 is at most</head><label></label><figDesc></figDesc><table>) 

is a symmetric matrix. Since ρ(M </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>t , r t )] t∈{1,2,··· ,T } .2.25 񮽙 񮽙 u T D + X 2 (1 + γ) 2 񮽙u񮽙 2 񮽙 , for any u when the step-size is η = 2 3X 2 (1+γ) 2 . If E and W are known beforehand so that 񮽙 u T D ≤ E and 񮽙u񮽙 ≤ W , then η can be optimized (Theorem IV.3 of Cesa- Bianchi et al. (1996)) by η =</head><label></label><figDesc></figDesc><table>Due to Theorem IV.1 of Cesa-Bianchi et al. (1996), we 
immediately have 

񮽙 T D ≤ W 
X(1+γ)(W X(1+γ)+ 
√ 
E) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 . Asymptotic upper bounds for total squared pre- diction error and total squared temporal difference of TD * (0) and RG.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>are less useful as the 񮽙w 1 − u񮽙 2 in the bounds dominate 񮽙 P or 񮽙 T D . However, we still get similar qualitative results by comparing the constant factors of the term 񮽙w 1 − u񮽙 2 in the bounds.</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>except that ∆w t is computed by Eqn 3.</head><label></label><figDesc></figDesc><table>Two technical lemmas are useful to prove Lemma A.4. 
It should be noted that the bounds they give are tight. 
Lemma A.2. For D given in Eqn 5, let A be 
D 񮽙 D or DD 񮽙 , and B be D −1 </table></figure>

			<note place="foot" n="2"> This effect was less obvious when γ got too close to 1. This was because the trajectories in our experiments were not long enough for such γ to have full impacts.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Michael Littman, Hengshuai Yao, and the anonymous reviewers for helpful comments that improved the presentation of the paper. The author is supported by NSF under grant IIS-0325281.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Residual algorithms: Reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning (ICML-95)</title>
		<meeting>the Twelfth International Conference on Machine Learning (ICML-95)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neuro-dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Safely approximating the value function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Boyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="369" to="376" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Worst-case quadratic loss bounds for prediction using linear functions and gradient descent</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="604" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exponentiated gradient versus gradient descent for linear predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Least-squares policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Error bounds for approximate policy iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML-03)</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exponentiated gradient methods for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Machine Learning (ICML-97)</title>
		<meeting>the Fourteenth International Conference on Machine Learning (ICML-97)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="272" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Markov decision processes: Discrete stochastic dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the worstcase analysis of temporal-difference learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="95" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TD(0) converges provably faster than the residual gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schoknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML-03)</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="680" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of temporal-difference learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
