<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Structure of Least-Favorable Noise in Gaussian Vector Broadcast Channels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
						</author>
						<title level="a" type="main">The Structure of Least-Favorable Noise in Gaussian Vector Broadcast Channels</title>
					</analytic>
					<monogr>
						<title level="j" type="main">DIMACS Series in Discrete Mathematics and Theoretical Computer Science</title>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The sum capacity of the Gaussian vector broadcast channel is the saddle point of a Gaussian mutual information game in which the transmitter maximizes the mutual information by choosing the best transmit covariance matrix subject to a power constraint, and the receiver minimizes the mutual information by choosing a least-favorable noise covariance matrix subject to a diagonal constraint. This result has been established using a decision-feedback equalization approach under the assumption that the least-favorable noise co-variance matrix is non-singular. This paper generalizes the above result to the case where the least-favorable noise is singular. In particular, it is shown that the least-favorable noise is not unique, and different least-favorable noise covariance matrices are related to each other by a linear estimation relation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A communication model with a single transmitter sending independent information to multiple receivers at the same time is referred to as a broadcast channel. Although the capacity region of a general broadcast channel is still an unsolved problem, recent progress has been made in the special case of the sum capacity of Gaussian vector broadcast channels <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref> [3] <ref type="bibr" target="#b3">[4]</ref>. A Gaussian vector broadcast channel with K receivers can be modeled as follows:</p><p>(1.1)</p><formula xml:id="formula_0">Y i = H i X + Z i , i= 1, · · · , K</formula><p>where X is a vector-valued transmit signal, Y i is the vector-valued receive signal, H i is the channel matrix, and Z i is the additive Gaussian vector noise at receiver i. The solution to the Gaussian vector broadcast channel sum capacity problem is based on two key ideas. First, an achievability result can be obtained using a precoding technique that effectively pre-subtracts multi-user interference at the transmitter. The precoding technique is known as "writing-on-dirty-paper" <ref type="bibr" target="#b4">[5]</ref>. Second, a converse theorem can be obtained using the idea that the broadcast channel capacity is bounded by a cooperative bound with a least-favorable noise.</p><p>The least-favorable-noise converse is based on the following: the sum capacity of the broadcast channel is clearly bounded by the capacity of the vector channel with cooperative receivers: max I(X; Y 1 · · · Y K ). Further, Sato <ref type="bibr" target="#b5">[6]</ref> observed that the sum capacity of a broadcast channel depends only on the marginal distribution of the noises and not on their correlation. This is because receivers of a broadcast channel cannot cooperate. So, they are ignorant of the actual noise correlation. Therefore, the sum capacity of a broadcast channel must be bounded by the minimum mutual information minimized over all possible joint distributions of the noises. For a Gaussian vector broadcast channel with a convex input constraint, restricting the above minimax problem to joint Gaussian input and noise distributions is without loss of generality. Further, it can be shown that this upper bound is tight. The achievability of the minimax mutual information expression can be established using several different methods. In <ref type="bibr" target="#b1">[2]</ref>, it is shown that if a decision-feedback equalizer is used as a joint receiver in the broadcast channel, the least-favorable noise would induce a feedforward matrix that is diagonal. In addition, the feedback section can be moved to the transmitter as a precoder. Thus, with a leastfavorable noise, no receiver cooperation in a decision-feedback equalizer is necessary and min I(X; Y 1 · · · Y K ) is achievable. The above minimum can be further maximized over all possible input distributions. Since min-max is equal to max-min in a convex-concave function, this implies that (1.2) is indeed the sum capacity of a Gaussian vector broadcast channel. However, the proof in <ref type="bibr" target="#b1">[2]</ref> appears to apply only to least-favorable noise covariances that are non-singular.</p><p>In <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref>, a completely different approach is taken. Focusing on sum power constrained channels, <ref type="bibr" target="#b3">[4]</ref> showed that the precoding region for a Gaussian vector broadcast channel is precisely a "dual" multiple access channel with the transmitter and receivers interchanged. Based on this duality, <ref type="bibr" target="#b3">[4]</ref> showed that the minimization for the least-favorable noise and the maximization for the dual multiple access channel sum capacity have the same solution. In addition, <ref type="bibr" target="#b3">[4]</ref> gives an explicit solution for the least-favorable noise. The derivation in <ref type="bibr" target="#b3">[4]</ref> is most transparent if the least-favorable noise is non-singular. To circumvent the singular noise issue, <ref type="bibr" target="#b3">[4]</ref> used a sequence of non-singular noise covariance to approximate the singular noise, thus proving the result in full generality. Strictly speaking, however, this solution is not constructive when the least-favorable noise is singular.</p><p>The singular noise issue is circumvented in <ref type="bibr" target="#b2">[3]</ref> using a different technique. Although also based on duality, the treatment in <ref type="bibr" target="#b2">[3]</ref> involves a transformation of the noise covariance matrix for the broadcast channel to an input cost constraint for the multiple access channel. This approach resolves the singularity issue, but it does not characterize the entire class of least-favorable noise.</p><p>It should be noted that the duality approach <ref type="bibr" target="#b2">[3]</ref> [4] applies only to Gaussian vector broadcast channels with a power constraint. The decision-feedback equalization approach <ref type="bibr" target="#b1">[2]</ref>, on the other hand, applies to Gaussian vector broadcast channels with arbitrary convex constraints, and is therefore more general. However, neither approach has explicitly dealt with the issue of singular least-favorable noise so far.</p><p>Singular least-favorable noise is not necessarily an unlikely event. In fact, the least-favorable noise is almost always singular in a broadcast channel where the number of receive dimensions is larger than the number of transmit dimensions. The main purpose of this paper is to give a complete characterization of the leastfavorable noise for an arbitrary channel. A combination of the decision-feedback equalization approach and the duality approach is taken. The main results are, first, the decision-feedback equalizer approach can be generalized to the case of singular least-favorable noise. Second, least-favorable noises are not unique, and they can be characterized via a duality between the broadcast channel and the multiple access channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Decision-Feedback Equalization with Singular Noise</head><p>Without loss of generality, consider a Gaussian vector broadcast channel with two receivers Y = HX + Z, where</p><formula xml:id="formula_1">H T = [H T 1 H T 2 ], Y T = [Y T 1 Y T 2 ] and Z T = [Z T 1 Z T 2 ]</formula><p>. The objective of this section is to show that</p><formula xml:id="formula_2">(2.1) min Szz 1 2 log |HS xx H T + S zz | |S zz |</formula><p>is achievable even if the minimizing S zz is singular. Recall that the minimization is over all S zz with fixed block diagonal terms. The fixed block diagonals can be assumed to be identity matrices with no loss of generality. This is a generalization of the earlier result in <ref type="bibr" target="#b1">[2]</ref> where the case for non-singular S zz is solved. The earlier result showed that if S zz satisfies the Karush-Kuhn-Tucker (KKT) condition for the minimization problem</p><formula xml:id="formula_3">(2.2) S −1 zz − (HS xx H T + S zz ) −1 = 񮽙 Ψ 1 0 0 Ψ 2 񮽙 ,</formula><p>then there exists a decision-feedback equalizer (DFE) with a diagonal feedforward matrix. When the channel matrix H is full rank, the minimizing noise covariance matrix is always non-singular. However, this is not the case when H has more rows than columns. This situation corresponds to the case where there are more receiver antennas than transmit antennas in the broadcast channel. To extend the achievability result to accommodate singular least-favorable noises, both the KKT condition and the design of the DFE need to be generalized. The decision-feedback equalizer that is required to accommodate singular noise differs from the conventional structure in one crucial aspect. A conventional DFE never processes more output dimensions than input dimensions. For example, in a channel with two transmit antennas and three receive antennas, a conventional DFE always reduces the two-by-three channel to a two-by-two channel by receiver joint processing. However, in a broadcast channel, receivers cannot cooperate and joint processing is not possible. Thus, a non-trivial generalization of the conventional DFE structure is required.</p><p>Without loss of generality, S xx is assumed to be fixed and full-rank. Let S xx = V ΣV T be an eigenvalue decomposition. It is convenient to set H 񮽙 = HV √ ΣM to be the effective channel, and let the input covariance matrix be simply an identity matrix. The choice of M will be made later.</p><p>Suppose that S zz is a low-rank solution to the minimization problem (2.1). Decompose S zz in the following form:</p><formula xml:id="formula_4">(2.3) S zz = 񮽙 U 1 U 2 񮽙 񮽙 S ˜ z ˜ z 0 0 0 񮽙 񮽙 U T 1 U T 2 񮽙 ,</formula><p>where</p><formula xml:id="formula_5">S ˜ z ˜ z is invertible and [ U 1 U 2 ]</formula><p>is an orthonormal matrix. The null space of S zz must be a subspace of the null space of the channel, (because otherwise the capacity would be infinity). Thus, it must be possible to express the channel matrix H 񮽙 as:</p><formula xml:id="formula_6">(2.4) H 񮽙 = U 1 ˜ H.</formula><p>The minimization problem now becomes</p><formula xml:id="formula_7">min S˜z˜zS˜zS˜z˜ S˜z˜z 1 2 log | ˜ H ˜ H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | (2.5) s.t. U 1 S ˜ z ˜ z U T</formula><p>1 has identities on the diagonal. A necessary condition for the least-favorable noise is</p><formula xml:id="formula_8">(2.6) S −1 ˜ z ˜ z − ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 = U T 1 񮽙 Ψ 1 0 0 Ψ 2 񮽙 U 1 .</formula><p>The objective of this section is to show that if the noise covariance matrix satisfies the above condition, then there exists a decision-feedback equalizer whose feedforward matrix is diagonal. The development of decision-feedback equalizer follows that of <ref type="bibr" target="#b1">[2]</ref>. The difference between the new proof and the original treatment is highlighted here. The derivation of the decision-feedback equalizer is based on minimum meansquare error (MMSE) estimation. The key observation is that MMSE estimator is not unique when both the channel and the noise are rank deficient. Consider the MMSE estimation of X given Y = HX + Z. The MMSE estimation is a matrix multiplicationˆXmultiplicationˆ multiplicationˆX = W Y, where W satisfies a normal equation:</p><formula xml:id="formula_9">(2.7) W S yy = S xy .</formula><p>Because both S zz and H are low rank, S yy is also low rank. It is not difficult to verify that</p><formula xml:id="formula_10">(2.8) W = ˜ H T ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 U T 1 + SU T 2</formula><p>satisfies the normal equation for any choice of S. A different DFE can be designed for each choice of S. The rest of the section is devoted to showing that there exists a choice of S (along with a choice of M ) that makes the DFE feedforward matrix diagonal. The first step in the design of a decision-feedback equalizer is noise whitening. Define</p><formula xml:id="formula_11">(2.9) ˆ H = S − 1 2 ˜ z ˜ z ˜ H.</formula><p>The MMSE estimator matrix W can be re-written as</p><formula xml:id="formula_12">(2.10) W = S xxˆHxxˆ xxˆH T ( ˆ H ˆ H T + I) −1 S − 1 2 ˜ z ˜ z U T 1 + SU T 2</formula><p>The structure of the feedforward matrix involves a Cholesky factorization of</p><formula xml:id="formula_13">R b = E[(X − ˆ X)(X − ˆ X) T ]</formula><p>. It turns out that R b is independent of the choice of S. Using the matrix inversion lemma, it can be shown that</p><formula xml:id="formula_14">(2.11) R b = ( ˆ H T ˆ H + I) −1 .</formula><p>Further, W can be re-written as follows:</p><formula xml:id="formula_15">(2.12) W = ( ˆ H T ˆ H + I) −1 ˆ H T S − 1 2 ˜ z ˜ z U T 1 + SU T 2 .</formula><p>In a decision-feedback equalizer, half of the Cholesky factorization</p><formula xml:id="formula_16">R b = G −1 ∆ −1 G −T</formula><p>is placed in the feedback section, and the remaining half is placed in the feedforward section. Thus, the feedforward matrix has the form:</p><p>(2.13)</p><formula xml:id="formula_17">F = ∆ −1 G −T ˆ H T S − 1 2 ˜ z ˜ z U T 1 + S 񮽙 U T 2 .</formula><p>The goal of this section is to use the generalized least-favorable noise condition (2.6) to show that F can be made diagonal. Using the matrix inversion lemma,</p><formula xml:id="formula_18">U T 1 񮽙 Ψ 1 0 0 Ψ 2 񮽙 U 1 = S −1 ˜ z ˜ z − ( ˜ H ˜ H T + S ˜ z ˜ z ) −1 (2.14) = ˆ H(I + ˆ H ˆ H T ) −1 ˆ H T .</formula><p>With some algebra (see <ref type="bibr" target="#b1">[2]</ref> for details), it is possible to prove that, by an appropriate choice of M , the Cholesky factorization of R b can be made to give:</p><formula xml:id="formula_19">(2.15) ∆ − 1 2 G −T ˆ H T S − 1 2 ˜ z ˜ z = 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 .</formula><p>Therefore, by choosing</p><formula xml:id="formula_20">(2.16) S 񮽙 = ∆ − 1 2 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 U 2 ,</formula><p>the feedforward matrix becomes (2.17)</p><formula xml:id="formula_21">F = ∆ − 1 2 񮽙 √ Ψ 1 0 0 √ Ψ 2 񮽙 (U 1 U T 1 + U 2 U T 2 ),</formula><p>which is diagonal since (2.18)</p><formula xml:id="formula_22">U 1 U T 1 + U 2 U T 2 = [ U 1 U 1 ] 񮽙 U T 1 U T 2 񮽙 = I.</formula><p>To summarize, when the least-favorable noise is singular, it must satisfy a modified KKT condition (2.2). The decision-feedback equalizer structure with the singular noise is not unique. However, among the class of decision-feedback equalizers, there exists one whose feedforward matrix is diagonal. This is true for any fixed S xx . Thus, in a Gaussian vector broadcast channel, the minimum mutual information I(X; Y 1 , Y 2 ) is an achievable sum rate, even when the least favorable noise covariance matrix is singular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Structure of Least-Favorable Noise</head><p>3.1. Uplink-Downlink Duality. The previous section shows that whenever the noise covariance matrix satisfies the least-favorable noise condition, (2.2) or (2.6), the mutual information I(X; Y) is achievable in a broadcast channel even without receiver cooperation. However, it does not give an explicit method to compute the least-favorable noise. Neither (2.2) nor (2.6) appears to have a closedform solution. Further, although I(X; Y) is a convex function of S zz , the numerical computation of the least-favorable noise is not necessarily easy, especially when the minimizing noise covariance matrix is singular. Fortunately, there is one important special case for which the computation is easy. This is when the transmit covariance S xx is being maximized at the same time.</p><p>Consider a Gaussian vector broadcast channel with a power constraint tr(S xx ) ≤ P . Sato's outer bound states that the sum capacity is less than min max I(X; Y), where the maximization is over the power constraint and the minimization is over all possible noise correlations. The achievability result in the previous section states that a sum rate of max min I(X; Y) is achievable. Since the mutual information is concave in S xx and convex in S zz , the minimization and maximization operations can be interchanged. Thus, the sum capacity of a Gaussian vector broadcast channel is precisely</p><formula xml:id="formula_23">(3.1) C = max Sxx min Szz 1 2 log |HS xx H T + S zz | |S zz | ,</formula><p>where tr(S xx ) ≤ P and S zz has fixed diagonal terms. Surprisingly, this minimax problem has a special structure that gives it a particularly simple solution. The new ingredient is the uplink-downlink duality 1 . Uplink-downlink duality refers to the observation that the achievable rate region of a broadcast channel using "writing-on-dirty-paper" precoding is the same as the capacity region of a "dual" multiple access channel with the roles of the input and output terminals interchanged and with a sum power constraint across all input terminals. This duality was established in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b2">[3]</ref>. In this section, a different derivation of duality is given. The main purpose is to illustrate that the minimax problem can be solved efficiently via duality.</p><p>To simplify matters, let's first assume that H is square and invertible, and also that the maximizing S xx and the least-favorable S zz are both full rank. Further, assume that each receiver in the broadcast channel is equipped with a single-antenna. The starting point of the new derivation is the KKT condition for the minimax problem (3.1). Because the objective function in (3.1) is concave in S xx and convex in S zz , the KKT condition completely characterizes the saddle point. The KKT condition is:</p><formula xml:id="formula_24">H T (HS xx H T + S zz ) −1 H = λI (3.2) S −1 zz − (HS xx H T + S zz ) −1 = Ψ (3.3)</formula><p>where λ is the dual variable associated with the power constraint and Ψ is a diagonal matrix of dual variables associated with the diagonal constraint on the noise covariance matrix. Multiplying (3.3) by H T on the left and H on the right and substituting in (3.2), we obtain</p><formula xml:id="formula_25">(3.4) H T S −1 zz H = H T ΨH + λI.</formula><p>The above is equivalent to</p><formula xml:id="formula_26">(3.5) H(H T ΨH + λI) −1 H T = S zz .</formula><p>Observe that (3.5) is precisely the KKT condition for a multiple access channel with diagonal matrix Ψ as the transmit covariance matrix and tr(ΨS zz ) ≤ P as the input constraint. Since S zz has 1's on the diagonal, the constraint is equivalent to a sum power constraint on Ψ. After a proper scaling of the power constraint, Ψ, λ and S zz , it can be shown that (3.5) is precisely the KKT condition for the following <ref type="bibr" target="#b0">1</ref> Uplink-downlink duality can be generalized beyond the power constrained broadcast channels. Duality exists whenever the input constraint is a linear covariance constraint. See <ref type="bibr" target="#b6">[7]</ref> for a detailed discussion.</p><formula xml:id="formula_27">optimization problem max D 1 2 log |H T DH + I| (3.6) s.t. D is diagonal trace(D) ≤ P, D ≥ 0,</formula><p>(where D = Ψ/λ.) Thus, there is a duality between a multiple access channel and a broadcast channel. The solution to the minimax problem (3.1) can be obtained from the solution to a maximization problem (3.6). Since (3.6) is much easier to solve numerically (see e.g.</p><p>[8] <ref type="bibr" target="#b8">[9]</ref>), uplink-downlink duality gives an efficient way to solve the minimax problem (3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Least-Favorable Noise.</head><p>The duality result suggests that the least favorable noise in the minimax problem can be computed via a multiple access channel. In particular, (3.5) gives an explicit formula for the covariance matrix of the least-favorable noise. This formula has also appeared in both <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b3">[4]</ref>. However, (3.5) is derived assuming that the channel matrix H is square and invertible and both the least-favorable noise and the maximizing input covariance matrices are full rank. In the general case, this is not necessarily true. In particular, the maximizing input covariance matrix D of the dual multiple access channel may not have non-zero entries everywhere on its diagonal. When the diagonals of D are all non-zero, the KKT condition for the maximization problem (3.6) ensures that the diagonal terms of H(H T ΨH + λI) −1 H T are all 1's. Otherwise, slack variables need to be introduced and H(H T ΨH + λI) −1 H T does not necessarily have 1's on its diagonal. Thus, it cannot be a valid choice of the least-favorable noise.</p><p>The purpose of the rest of the section is to show that a least-favorable noise covariance can be obtained by adding a positive semi-definite matrix to H(H T ΨH + λI) −1 H T so that the sum of the two matrices has 1's on the diagonal. Such a positive semi-definite matrix is not unique, so the least-favorable noise covariance matrix is not unique. In fact, in some cases, the class of least favorable noises can be further enlarged by taking an additional step. Consider the candidate leastfavorable noise, re-labeled as S (0) zz :</p><formula xml:id="formula_28">(3.7) S (0) zz = H(H T ΨH + λI) −1 H T .</formula><p>It is easy to see that the water-filling input covariance matrix S xx with respect to the channel H and the noise S (0) zz is:</p><formula xml:id="formula_29">(3.8) S xx = (λI) −1 − (H T ΨH + λI) −1 .</formula><p>Now, S xx may be rank deficient. In this case, the dimension of H must be reduced, and the least-favorable noise must be re-computed using the reduced channel. The re-computed noise covariance could have fewer 1's on its diagonal, thus enlarging the class of positive semi-definite matrices that can be added to its diagonal terms. For the rest of the proof, it is assumed that this channel reduction step has already taken place. The proof is fairly lengthy. To simplify the algebra, the broadcast channel considered here is assumed to have only a single antenna at each receiver. This special case exhibits the essential features of the least-favorable noise.</p><p>The first step of the proof is to verify that even though its diagonals may not be 1's, the candidate noise covariance matrix (3.7) satisfies the least-favorable noise condition. Assuming that channel has already been reduced, S zz can be re-written as</p><formula xml:id="formula_30">(3.9) S (0) zz = U 1 S ˜ z ˜ z U T 1</formula><p>where S ˜ z ˜ z is invertible and columns of U 1 are orthonormal vectors. Further, H can be re-written as</p><formula xml:id="formula_31">(3.10) H = U 1 ˜ H,</formula><p>where˜Hwhere˜ where˜H is invertible. The strategy is to solve the minimax problem over S ˜ z ˜ z and to show that the solution is S (0)</p><formula xml:id="formula_32">zz = H(H T ΨH + λI) −1 H T . The minimax problem is now max Sxx min S˜z˜zS˜z˜S˜z˜z 1 2 log | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | (3.11) s.t. tr(S xx ) ≤ P U 1 S ˜ z ˜ z U T 1 has 1 񮽙 s on diagonal S xx ≥ 0,</formula><p>The KKT condition of the minimax problem is:</p><formula xml:id="formula_33">˜ H T ( ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z ) −1 ˜ H = λI (3.12) S −1 ˜ z ˜ z − ( ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z ) −1 = U T 1 ΨU 1 (3.13)</formula><p>Pre-multiplying the second equation above by˜Hby˜ by˜H T and post-multiplying by˜Hby˜ by˜H, it is not difficult to verify that <ref type="bibr">(3.14)</ref> H</p><formula xml:id="formula_34">(H T ΨH + λI) −1 H T = U 1 S ˜ z ˜ z U T 1 = S (0) zz . Thus, the candidate S (0)</formula><p>zz satisfies the least-favorable noise condition. The rest of the proof is devoted to showing that the least-favorable noise condition remains to be satisfied if a positive semi-definite matrix, denoted as S 񮽙 zz , is added to S (0) zz to makes its diagonal terms 1. The proof is divided into several parts. First, without loss of generality, the rows of H can be re-arranged so that the upper-left sub-matrix of S (0) zz has 1's on its diagonal. This implies that Ψ only has positive entries on its upper-left diagonal, and S 񮽙 zz has non-zero entries only in the lower-right corner:</p><formula xml:id="formula_35">(3.15) Ψ = 񮽙 Ψ 񮽙 0 0 0 񮽙 , S 񮽙 zz = 񮽙 0 0 0 S 񮽙 .</formula><p>Let n be the number of receivers in the broadcast channel. So, Ψ, S zz and S 񮽙 zz are n × n matrices. Let the dimension of Ψ 񮽙 be k × k. So, the dimension of S is (n − k) × (n − k). Also, let the rank of S (0) zz be r. Now, observe that k ≥ r. The reason for this has to do with the channel reduction step mentioned before. Note that from (3.8) the optimal S xx can be expressed as:</p><formula xml:id="formula_36">S xx = (λI) −1 − (H T ΨH + λI) −1 (3.16) = (λI) −1 H T √ Ψ(I + √ ΨHH T √ Ψ) −1 √ ΨH(λI) −1 .</formula><p>Thus, the rank of S xx is the same as the rank of H T ΨH. The channel reduction step guarantees that S xx is full rank. This implies that H T ΨH must be full rank.</p><p>For this to be true, Ψ must have non-zero diagonal entries in at least r 񮽙 positions, where r 񮽙 is the rank of H. So, k ≥ r 񮽙 . But, after the channel reduction, the rank of H is the same as the rank of S <ref type="formula" target="#formula_48">(0)</ref> zz . This proves that k ≥ r. In general, k can be strictly larger than r. Physically, this implies that in a broadcast channel, the number of active receivers can be larger than the number of transmit dimensions. Also note that since the rank of S <ref type="formula" target="#formula_48">(0)</ref> zz is r and the rank of S 񮽙 zz is (n − k), the rank of S <ref type="formula" target="#formula_48">(0)</ref> zz + S 񮽙 zz is at most (n − k + r), which is not full rank if k is strictly larger than r.</p><p>The next step in the proof involves the decomposition of S <ref type="formula" target="#formula_48">(0)</ref> zz + S 񮽙 zz along the direction U 1 . The strategy is the following. First, find an n × (n − k) matrix of orthonormal column vectors, denoted as U 2 , extending the space spanned by the columns of U 1 , such that S 񮽙 zz can be expressed as:</p><formula xml:id="formula_37">(3.17) S 񮽙 zz = 񮽙 0 0 0 S 񮽙 = 񮽙 U 1 U 2 񮽙 񮽙 S 11 S 12 S 21 S 22 񮽙 񮽙 U T 1 U T 2 񮽙 .</formula><p>Recall that U 1 contains r column vectors and U 2 contains n − k column vectors. So, [ U 1 U 2 ] contains n − k + r vectors, which do not necessarily span the whole space. (Note that this U 2 may be different from the U 2 in section 2.) Partition U 1 and U 2 into sub-matrices:</p><formula xml:id="formula_38">U 1 = 񮽙 u T 11 u T 12 񮽙 U 2 = 񮽙 u T 21 u T 22 񮽙 . (3.18)</formula><p>Two useful facts about S ij and u ij are derived next. First, <ref type="bibr">(3.19)</ref> S 11 − S 12 S −1 22 S 21 = 0. This is because</p><formula xml:id="formula_39">(3.20) 񮽙 U T 1 U T 2 񮽙 񮽙 U 1 U 2 񮽙 = I, so, from (3.17), (3.21) 񮽙 S 11 S 12 S 21 S 22 񮽙 = 񮽙 u T 11 u T 12 u T 21 u T 22 񮽙 񮽙 0 0 0 S 񮽙 񮽙 u 11 u 21 u 12 u 22</formula><p>񮽙 .</p><p>This allows S ij to be solved explicitly: Now, using <ref type="bibr">(3.30)</ref>, the difference between the two can now be simplified:</p><formula xml:id="formula_40">S 11 = u T</formula><formula xml:id="formula_41">(3.36) 񮽙 U T 1 ΨU 1 −U T 1 ΨU 1 S 12 S −1 22 −S −1 22 S 21 U T 1 ΨU 1 S −1 22 S 21 U T 1 ΨU 1 S 12 S −1 22 񮽙 .</formula><p>To prove (3.31), it remains to show that the above is equal to</p><formula xml:id="formula_42">(3.37) 񮽙 U T 1 U T 2 񮽙 Ψ 񮽙 U 1 U 2 񮽙 = 񮽙 U T 1 ΨU 1 U T 1 ΨU 2 U T 2 ΨU 1 U T 2 ΨU 2 񮽙 .</formula><p>Comparing (3.36) with (3.37), it is clear that the two are equal if the following holds:</p><formula xml:id="formula_43">(3.38) ΨU 2 = −ΨU 1 S 12 S −1 22 .</formula><p>Recall that Ψ has non-zero entries only in its upper-left diagonal. So, the left-hand side of (3.38) is</p><formula xml:id="formula_44">(3.39) ΨU 2 = 񮽙 Ψ 񮽙 0 0 0 񮽙 񮽙 u 21 u 22 񮽙 = Ψ 񮽙 u 21 .</formula><p>The right-hand side of (3.38) is <ref type="bibr">By (3.25)</ref>, the left-hand side is equal to the right-hand side. This establishes the least-favorable noise condition (3.31).</p><formula xml:id="formula_45">(3.40) −ΨU 1 S 12 S −1 22 = −Ψ 񮽙 u 11 u T 12 u −T 22 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linear Estimation Interpretation.</head><p>The previous section shows that the least-favorable noise in a Gaussian minimax mutual information game is of the type S (0) zz + S 񮽙 zz . Note that adding the term S 񮽙 zz does not change the mutual information I(X; Y). This can be seen using the following argument. The value of the minimax expression with <ref type="bibr">(S xx</ref>  </p><formula xml:id="formula_46">, S (0) zz ) is (3.41) C = 1 2 log | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | . With (S xx , S (0) zz + S 񮽙 zz ),</formula><formula xml:id="formula_47">|S 22 | · | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z + S 11 − S 12 S −1 22 S 21 | |S 22 | · |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z + S 11 − S 12 S −1 22 S 21 | = | ˜ HS xx˜Hxx˜ xx˜H T + S ˜ z ˜ z | |S˜z˜z|S˜ |S˜z|S˜z˜ |S˜z˜z | .</formula><p>Therefore the capacity expressions (3.41) and (3.42) have the same value.</p><p>In some sense, S (0) zz is the "smallest" possible noise in the class of least-favorable noises. This is because after the channel reduction step, HS xx H T is strictly positive everywhere in the span of the U 1 space. Thus, for capacity not to become infinity, the noise covariance must also be strictly positive in the space spanned by U 1 . Now, S (0) zz is the only noise covariance entirely in the U 1 space that satisfies the KKT condition. Thus, S (0) zz must be the "smallest" noise covariance that satisfies the KKT condition. S</p><p>zz is not, of course, the only such noise. The previous section shows that (S xx , S (0) zz + S 񮽙 zz ) also satisfies the least-favorable noise condition. However, the previous section did not explicitly prove that S xx is the water-filling covariance matrix for S (0) zz +S 񮽙 zz . But, because the addition of S 񮽙 zz can only reduce the minimax capacity, the fact that it does not shows that S xx must be the maximizing covariance for S (0) zz + S 񮽙 zz . Thus, the entire class of (S xx , S</p><p>zz + S 񮽙 zz ) are saddle-points of the minimax problem. u + Z 񮽙 u as noise. Clearly the second receiver cannot do better than the first one. However, the second receiver does as well as the first receiver if the linear estimation of z 0 1 + z 񮽙 1 given z 񮽙 2 is exactly z 񮽙 1 . Since z 0 1 and z 񮽙 1 are independent, this condition is equivalent to E[z 񮽙 1 |z 񮽙 2 ] = 0. But, the covariance matrix of E[z 񮽙 1 |z 񮽙 2 ] is S 11 − S 12 S −1 22 S 21 . Thus, the covariance matrix of the additional noise S 񮽙 zz must satisfy the condition (3.46) S 11 − S 12 S −1 22 S 21 = 0. The above argument shows that the entire class of least-favorable noises are related to each other via a linear estimation relation 2 . As the previous section shows, this relation is crucial in the derivation of the least-favorable condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>This paper deals with the least-favorable noise in a Gaussian vector broadcast channel. The least-favorable noise is not necessarily non-singular. The achievability proof for the broadcast channel sum capacity is extended to the case of singular least-favorable noises. The proof is based on the fact that the decision-feedback equalizer is not unique when the noise is singular. Among all possible equalizers, there exists one with a diagonal feedforward matrix.</p><p>In the second part of the paper, a new derivation for the duality between the multiple access channel and the broadcast channel is given. The duality relation gives a natural characterization of the entire class of least-favorable noises. The least-favorable noise is not unique, and all least-favorable noises are related to each other by a linear estimation relation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 .</head><label>1</label><figDesc>2) min Szz max Sxx I(X; Y 1 · · · Y K ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The class of least-favorable noises S (0) zz + S 񮽙 zz has a linear estimation inter- pretation. Let Z (0) and Z 񮽙 be independent Gaussian noise vectors with covari- ance matrices S (0) zz and S 񮽙 zz respectively. Consider an orthogonal transformation Z (0) u = [U 1 U 2 ]Z (0) and Z 񮽙 u = [U 1 U 2 ]Z 񮽙 . These two noise vectors take the formConsider two receivers, one has Z (0) u as noise and the other has Z (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>12 Su 12 S 12 = u T 12 Su 22 (3.22) S 21 = u T 22 Su 12 S 22 = u T 22 Su 22 . (3.23)12 S −1 22 S 21 = u T 12 Su 22 (u T 22 Su 22 ) −1 u T 22 Su 12 = S 11 , thus21 = u 11 u T 12 u −T 22 . The11 u 21 u 12 u 22 񮽙 񮽙 S 11 S 12 S 21 S 22 񮽙 񮽙 u T 11 u T 12 u T 21 u T 22 񮽙 . Multiply out the right-hand side. Using the fact that the upper-right sub-matrix of the right-hand side is zero, we get11 S 11 u T 12 + u 21 S 21 u T 12 + u 11 S 12 u T 22 + u 21 S 22 u T21 u T 22 )S(u 11 u T 11 + u 22 u T 22 ). Since S is full rank and (u 11 u T 11 + u 22 u T11 u T 12 + u 21 u T</head><label></label><figDesc></figDesc><table>Therefore, 

(3.24) 
S proving (3.19). 
The second useful fact is the following: 

(3.25) 
u proof is based on (3.17) 

(3.26) 
񮽙 
0 0 
0 S 

񮽙 
= 
񮽙 
u (3.27) 
0 = u 22 . 
Substituting in (3.22)-(3.23), the above is equal to 

(3.28) 
0 = (u 11 u T 
12 + u 22 ) is full rank, the above implies 
(3.29) 
u 22 = 0, 
thus proving (3.25). 
Finally, we are ready to show that S 񮽙 
. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>11 S 12 S 21 S 2211 S 12 S 21 S 22 񮽙 񮽙 񮽙 񮽙 . Using the relation S 11 − S 12 S −1 22 S 21</head><label></label><figDesc></figDesc><table>the capacity is 

(3.42) 
C = 
1 
2 
log 

񮽙 
񮽙 
񮽙 
񮽙˜HS 

񮽙˜ 񮽙˜HS xx˜Hxx˜ xx˜H T + S ˜ 

z ˜ 

z + S 񮽙 
񮽙 
񮽙 
񮽙 
񮽙 
񮽙 
񮽙 
񮽙 
S ˜ 

z ˜ 

z + S = 0 and the Schur's complement formula for 
the determinant 

(3.43) 
񮽙 
񮽙 
񮽙 
񮽙 
A B 
C D 

񮽙 
񮽙 
񮽙 
񮽙 = |D| · |A − BD −1 C|, 

it is not difficult to show that 

(3.44) 
</table></figure>

			<note place="foot" n="2"> There is a slight subtlety in that, strictly speaking, Z 񮽙 needs not be Gaussian. However, this does not affect the sum capacity. See [10] for a discussion.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(0)</head><p>zz + S 񮽙 zz satisfies the least-favorable noise condition. Starting from (3.13) <ref type="bibr">(3.30)</ref> S −1</p><p>The strategy is to simplify the above using Schur's complement formula: </p><p>and the second matrix inversion in (3.31) can be expanded similarly:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the achievable throughput of a multi-antenna Gaussian broadcast channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">submitted to IEEE Trans. Info. Theory</title>
		<imprint>
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sum capacity of Gaussian vector broadcast channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Cioffi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
	<note>submitted to</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sum capacity of the vector Gaussian broadcast channel and downlink-uplink duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
	<note>submitted to IEEE Trans. Info. Theory</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Duality, achievable rates and sum-rate capacity of Gaussian MIMO broadcast channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">submitted to IEEE Trans. Info. Theory</title>
		<imprint>
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Writing on dirty paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="441" />
			<date type="published" when="1983-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An outer bound on the capacity region of broadcast channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="377" />
			<date type="published" when="1978-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uplink-downlink duality via minimax duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Workshop on Information Theory</title>
		<imprint>
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dual decomposition approach to the sum power Gaussian vector multiple access channel sum capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information Sciences and Systems (CISS)</title>
		<imprint>
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sum power iterative waterfilling for Gaussian vector broadcast channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wonjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inter. Symp. Info. Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
	<note>Also in Asilomar Conf</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The worst additive noise under a covariance constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Diggavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<ptr target="address:weiyu@comm.utoronto.ca" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3072" to="3081" />
			<date type="published" when="2001-11" />
		</imprint>
		<respStmt>
			<orgName>Electrical and Computer Engineering Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Current address: 10 King&apos;s College Road</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
