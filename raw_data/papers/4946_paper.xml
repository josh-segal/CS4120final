<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved File Synchronization Techniques for Maintaining Large Replicated Collections over Slow Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Suel</surname></persName>
							<email>suel@poly.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Noel</surname></persName>
							<email>pnoel@bumblebee.poly.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitre</forename><surname>Trendafilov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CIS Department Polytechnic University Brooklyn</orgName>
								<address>
									<postCode>11201</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved File Synchronization Techniques for Maintaining Large Replicated Collections over Slow Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of maintaining large replicated collections of files or documents in a distributed environment with limited bandwidth. This problem arises in a number of important applications, such as synchronization of data between accounts or devices, content distibution and web caching networks , web site mirroring, storage networks, and large scale web search and mining. At the core of the problem lies the following challenge, called the file synchronization problem: given two versions of a file on different machines, say an outdated and a current one, how can we update the outdated version with minimum communication cost, by exploiting the significant similarity between the versions? While a popular open source tool for this problem called rsync is used in hundreds of thousands of installations, there have been only very few attempts to improve upon this tool in practice. In this paper, we propose a framework for remote file synchronization and describe several new techniques that result in significant bandwidth savings. Our focus is on applications where very large collections have to be maintained over slow connections. We show that a prototype implementation of our framework and techniques achieves significant improvements over rsync. As an example application, we focus on the efficient synchronization of very large web page collections for the purpose of search, mining, and content distribution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider the problem of maintaining large replicated collections of files, such as user files, web pages, or other documents, over a slow network. In particular, assume that we have two machines, ¡ and ¢ , that each hold a copy of the collection, and that files are frequently modified at one of the machines, say ¡ . Periodically, machine ¢ initiates a synchronization operation that updates all its replicas to the latest version. This operation involves identifying all files that have changed, deciding which version of the file is the latest one (if files can be changed at either location), and finally updating £ Work supported by NSF CAREER Award NSF CCR-0093400 and by New York State through the Wireless Internet Center for Advanced Technology (WICAT) at Polytechnic University. the files that have changed. Given the size of the collections, we are interested in performing the synchronization with a minimum amount of communication over the network. The above scenario arises in a number of applications, such as synchronization of user files between different machines, remote backups, mirroring of large web and ftp sites, content distribution, and web search engines, to name just a few. In many cases, updated files differ only slightly from their previous version; for example, updated web pages usually change only in a few places. In this case, instead of sending the entire updated version over the network, it would be desirable to be able to perform the update by sending only an amount of data proportional to the change between the two versions.</p><p>In this paper, we focus on this problem of updating files in a bandwidth efficient manner; we refer to this as the file synchronization problem. Our work is primarily motivated by several applications in large scale web search and content distribution discussed later, but our techniques are applicable to the more general case and we believe that file synchronization is a fundamental operation in distributed systems. We note that there is a very widely used open source software tool called rsync that addresses exactly this problem and that is described in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Our goal is to derive techniques that achieve significant savings over rsync particularly in the case of large collections and slow networks.</p><p>Before continuing, we point out a few assumptions. We assume that collections consist of unstructured files that may be modified in arbitrary ways, including insertion and deletion operations that change byte and page alignments between different versions. Thus, approaches that identify changed disk pages or bit positions or that assume fixed record boundaries do not work -though some of them are potentially useful for identifying those files that have been changed and need to be synchronized. We also note that the problem is much easier if all update operations to the files are saved in an update log that can be transmitted to the other machine. However, in many cases such logs are not available. We are also not concerned with issues of consistency in between synchronization steps, and with the question of how to resolve conflicts if changes are simultaneously performed at several locations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>. It is left up to an application to decide when and how often files should be synchronized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Applications Motivating Our Work</head><p>The rsync file synchronization tool is currently widely used to exchange user files between different machines (e.g., between a machine at work and a machine at home), to mirror web and ftp sites, and to perform backup over a network. In addition to these general scenarios, we are particularly interested in the following potential applications:</p><p>1. Sharing crawled web pages for mining and search: One of the main bottlenecks in large-scale web search and mining is the cost of crawling large sets of web pages and then keeping these pages up to date. A lot of recent work has focused on efficient strategies for recrawling changing web pages based on their past history of updates <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. An alternative or complementary approach would be to share the results of recrawls between many parties. For example, several organizations or even nodes in a P2P system <ref type="bibr" target="#b18">[19]</ref> could independently perform crawls and later exchange their results, or one centralized high-performance crawler could allow clients to obtain the latest versions of web pages of interest. As an example of the latter, the Stanford WebBase project <ref type="bibr" target="#b19">[20]</ref> enables researchers at other institutions to receive a feed of web pages from the WebBase collection. However, due to bandwidth limitations, large data sets today are still frequently shared via "sneakernet", i.e., by sending disks or tapes by mail <ref type="bibr" target="#b17">[18]</ref>. Use of file synchronization would allow other organizations to receive updated content over the network at a fraction of the current bandwidth cost. In fact, our main motivation for this work is to build a system for efficiently sharing large recrawls over a wide area network.</p><p>2. Maintaining massive collections at clients: Given the speed at which hard disk capacity is currently expanding, several researchers have considered the possibility of storing and maintaining the entire web, or large parts of the world's content, at desktop machines. For example, Garcia-Molina <ref type="bibr" target="#b16">[17]</ref> outlined a scenario where the world's changing content is distributed to end users in monthly or weekly updates shipped out on future versions of CDs or DVDs. One could also imagine instead using file synchronization to maintain large up-to-date collections at desktop clients for personalized browsing, search, or mining. Browsers such as Internet Explorer already allow users to subscribe to web pages that are then periodically downloaded and stored; the proposed techniques could be used to significantly improve the efficiency of this process.</p><p>3. Server-Friendly Web Crawling: Closely related to the first scenario is the idea of integrating file synchronization into web servers to support more efficient recrawling. We caution that there have been previous proposals to modify servers for this purpose <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, but that widespread adoption of such schemes appears unlikely for various reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Caching and Content Distribution Networks:</head><p>Several companies in the CDN space have studied and deployed file synchronization techniques. We are not aware of any published work in this direction, but file synchronization techniques are a natural approach for updating content that is widely replicated at the network edge.</p><p>5. Replication in P2P File Sharing: While much of the content in current file sharing networks is static, file synchronization could be used to maintain dynamic content that is replicated for fault tolerance or performance.</p><p>All of these scenarios have in common that they involve massive amounts of data, and thus bandwidth efficiency is of primary importance. Our goal is to design improved protocols for file synchronization that use multiple roundtrips to significantly decrease the amount of data sent over the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">File Synchronization and the rsync Tool</head><p>We now define the file synchronization problem and describe the algorithm of Tridgell and MacKerras <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, which forms the basis of the widely used rsync tool. <ref type="bibr" target="#b0">1</ref> We note that a similar approach was proposed by Pyne in a US Patent <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>The setup for the file synchronization problem is as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The rsync Algorithm</head><p>The basic approach in rsync, as well as in our algorithms, is to split a file into blocks and use hash functions to compute hashes or "fingerprints" of the blocks. These hashes are then sent to the other machine, where the recipient attempts to find matching blocks in its own file. One issue is the lack of alignment between matching blocks in the two files; this is addressed by comparing received hashes not just with the corresponding block in the other file, but with all substrings of the same size. For efficiency, hashes are composed from two different hash functions, a fast but unreliable one, and a very reliable one that is more expensive to compute. Then the steps in rsync are as follows:</p><p>1. At the client:  </p><formula xml:id="formula_0">(a) Partition % into blocks ¢ B A 8 C % ') $ D E § G F H ) P I R Q T S $ D ¨ U V Q W 0 of some block size D .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">At the client:</head><p>(a) Use the incoming stream of symbols and indices of hashes in</p><formula xml:id="formula_1">¤ # to reconstruct ¢ ¡ ¤ £ ¦ ¥ .</formula><p>The process is illustrated in <ref type="figure">Figure 2</ref>.1. All symbols and indices sent from server to client in steps (iii) and (iv) are also compressed using an algorithm similar to gzip. A checksum on the entire file is used to detect the (fairly unlikely) failure of both checksums, in which case the algorithm could be repeated with different hashes, or we can simply transfer the entire file. The reliable checksum is implemented using MD4 ( Q ¨ § © bits), but only two bytes of the MD4 hash are used since this provides sufficient power. The unreliable checksum is implemented as a § -bit "rolling checksum" that allows efficient sliding of the block boundaries by one character, i.e., the checksum for</p><formula xml:id="formula_2">( '@ I Q ¤ § 1 @ I D © 0 can be computed in con- stant time from ( '@ ¤ § @ I D U R Q W 0 . Thus,</formula><p>bytes per block are transmitted from client to server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion of rsync and Related Proposals</head><p>Clearly, the choice of block size is critical to the performance of the algorithm, but this choice depends on the degree of similarity between the two files -the more similar, the larger the optimal block size. Moreover, the location of changes in the file is also important. If a single character is changed in each block of ¤ # , then no match will be found by the server and rsync will be completely ineffective; on the other hand, if all changes are clustered in a few areas of the file, rsync will do well even with a large block size. Given these observations, some basic performance bounds based on block size and number and size of file modifications can be shown. However, rsync does not have any good performance bounds with respect to common metrics such as edit distance <ref type="bibr" target="#b33">[34]</ref>.</p><p>In practice, rsync uses a default block size of  <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> are based on recursive splitting of unmatched blocks. We will also adopt this approach, but combine it with several other ideas that save on communication costs and allow us to utilize much smaller block sizes efficiently. We note that recursive splitting, as some of our other techniques, increases the number of roundtrips between the two parties. However, as in rsync itself, the roundtrip latencies are not incurred for each file since many files can be processed simultaneously. Thus, for large collections additional roundtrips are not a problem. For small files, e.g., to fetch individual web pages, a less bandwidth efficient algorithm based on a single roundtrip would be preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Contributions</head><p>We study the file synchronization problem for large collections of files and documents, and propose and evaluate new techniques that significantly improve on previous approaches in terms of bandwidth usage. Our main contributions are:</p><p>We describe a framework for file synchronization algorithms that partitions the problem into two phases, map construction, where the two parties use a multiround protocol to determine the common parts of the corresponding files, and delta compression, where the remaining parts are encoded in relation to the common parts and then transmitted to the other side. The framework allows for a variety of algorithms and techniques.</p><p>Within the framework, we describe and implement a number of known and new techniques. In particular, we use recursive partitioning as proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. We introduce new techniques for extending matches via "continuation hashes" and for the optimized verification of suspected matches in the two files, plus several other optimizations. The techniques are related to classical problems in group testing and "searching with liars" (also known as Ulam's Problem), and insights from these problems may lead to additional moderate improvements in the future.</p><p>We evaluate the framework and techniques on several data sets, including a large set of changing web pages that we recrawled daily over several weeks. The results show that our algorithm allows the maintenance of large file collections with significantly lower communication costs than the widely used rsync tool, and in many cases comes within 2 of the best delta compressor (which provides a reasonable lower bound in practice).</p><p>The remainder of this paper is organized as follows. In the next section, we discuss some related work. Section 5 describes the new framework and algorithmic techniques. Section 6 presents the experimental evaluation of our implementation. Finally, Section 7 discusses some limitations of our current results and open problems for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The most important previous work is the rsync file synchronization algorithm proposed by Tridgell and MacKerras <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>, which is the basis of the very widely used rsync open source tool. There are a number of theoretical studies of the file synchronization problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, also called the document exchange problem in <ref type="bibr" target="#b9">[10]</ref>. In particular, Orlitsky <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> presents almost tight bounds for the problem with varying numbers of communication phases, under some assumptions about the assumed file distance metric. However, many of the theoretical algorithms assume that a hash function is reversed as part of the decoding operation; while this is allowable under the standard model for communication complexity <ref type="bibr" target="#b23">[24]</ref>, it makes the algorithms impossible to implement in practice. An exception are algorithms proposed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> that are based on recursive partitioning of blocks similar to our implementation. These algorithms can also be shown to achieve provable bounds with respect to some common file distance measures. Some limited experimental results are given in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. Recent work in <ref type="bibr" target="#b39">[40]</ref> presents a version of the rsync algorithm that updates files in-place without using additional temporary space.</p><p>Delta compression is the problem of encoding one file relative to another similar file, where both files are available during the encoding. Thus, file synchronization can be seen as a distributed version of delta compression where the two files are located at different machines. Our framework reduces the file synchronization problem to delta compression; on the other hand any algorithm for file synchronization also solves the delta compression problem, though typically at significantly higher cost. Some available open source tools for delta compression are described in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46]</ref>, and an overview of delta compression and file synchronization techniques and their applications is given in <ref type="bibr" target="#b44">[45]</ref>. A number of authors have studied problems related to identifying disk pages, files, or data records that have been changed or added or deleted, or that differ between two or more replicas; see, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. These problems differs from ours in that data is assumed to be partitioned into fixed units such as pages, records, or files that are treated as atomic. The work is nonetheless related to ours in two ways. First, it addresses the problem of efficienctly identifying files that have changed in scenarios where almost all objects are unchanged; afterwards, our file synchronization techniques can be applied to update those files. We do not focus on this aspect and instead use a fingerprint for each file as this is efficient enough for our data sets. Second, some of the results <ref type="bibr" target="#b26">[27]</ref> are also based on techniques from Group Testing, while others are based on Error Correcting Codes and probably not as useful in our context.</p><p>In addition to rsync, there are many other tools for synchronizing data between different machines. Some of these tools, such as Microsoft's ActiveSync, Palm's HotSync, or Puma Technologies' IntelliSync, are used to synchronize data between a desktop or online account and a mobile device. They typically transfer the entire file if a change has occurred, though for record-based data such as appointments and contacts only updated records are transmitted in most cases. Recent work in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref> surveys and studies synchronization techniques for handheld devices, while <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref> discuss correctness issues when files are modified at several locations.</p><p>Hash-based techniques similar to rsync have been explored by the OS and Systems community for purposes such as compression of network traffic <ref type="bibr" target="#b42">[43]</ref>, distributed file systems <ref type="bibr" target="#b30">[31]</ref>, distributed backup <ref type="bibr" target="#b10">[11]</ref>, and web caching <ref type="bibr" target="#b40">[41]</ref>. These techniques use string fingerprinting techniques proposed by Karp and Rabin <ref type="bibr" target="#b21">[22]</ref> to partition a data stream into blocks in a consistent manner on both sides of a communication link, and then send hash values to encode repeated substrings. Group Testing is a set of combinatorial problems, introduced by Dorfman <ref type="bibr" target="#b11">[12]</ref>, that deal with identifying "defective" elements in a set through a sequence of simple tests on subsets. We are not aware of previous results on the exact version of the group testing problem that arises in our work here. Group testing was used by Madej <ref type="bibr" target="#b26">[27]</ref> to identify files that have changed. Search problems with liars were introduced by <ref type="bibr">Ulam [49]</ref> and have been studied extensively over the last 50 years; see the recent survey of Pelc <ref type="bibr" target="#b36">[37]</ref> for an overview. In particular, Pelc discusses a relationship of the problem to communication over noisy channels. On the other hand, Orlitsky and Viswanathan <ref type="bibr" target="#b34">[35]</ref> recently established a relationship between Error Correcting Codes for noisy channels and one-way communication problems where a receiver has related information.</p><p>Finally, several recent studies look at the type and frequency of web page updates and propose efficient strategies for refreshing pages or other objects based on observations about their past behavior <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. These techniques are complementary to ours in the context of our web page update application. Of course, if file synchronization were to become widely deployed at web servers, then this would change the cost model assumed in current recrawling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our Framework and Algorithms</head><p>We now describe our technical contributions. We first introduce our basic framework in the next subsection, and then give a detailed description of techniques for the map construction phase of our framework in Subsections 5.2 to 5.5. Finally, Subsection 5.6 summarizes the resulting protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A Framework for File Synchronization</head><p>Recall that the client ! has a copy of an outdated file # , and the server " has a copy of the current file</p><formula xml:id="formula_3">¤ ¡ ¤ £ ¦ ¥ .</formula><p>The goal is to design a protocol between the two parties that results in ! obtaining a copy of ¤ ¡ ¤ £ ¦ ¥ , while minimizing the communication cost. As we saw in the description of the rsync algorithm, hash values can be used to identify common substrings in both files, allowing the recipient to learn about the structure of the file at the other machine.</p><p>All the algorithms in our framework consist of the following two phases:</p><p>(1) Map construction:: in this phase, the two parties use multiple roundtrips to create an approximate representation of the parts of the two files that are identical. In particular, the client will generate a map of the current file </p><formula xml:id="formula_4">with ') 0 C ( ') 0 or ') 1 0 C "?" for ) C 2 § § ¦ 4 U Q .</formula><p>In other words, is identical to in some areas (called the known areas), and unknown in the other areas, labeled by "?".   Throughout the remainder of this section, we will focus on optimized techniques for the map construction phase, since good delta compression tools for the second phase are already available from several sources. Note that in the above example, hashes are transmitted from server to client, while in rsync the client sends hashes to the server. We could in principle also send hashes from client to server, and then have the server build a map of the client file </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Techniques for Efficient Map Construction</head><p>We now focus on techniques for efficient map contruction. While the example in the previous section might indicate that there is not much to do apart from repeatedly sending hashes, the problem turns out to be surprisingly rich in terms of possibilities. All the following techniques are based on exchanging hash values, but use various ideas to minimize the number of bits needed for the hashes. In particular we employ the following ideas, described in more detail further below:</p><p>(a) Recursive splitting of the block size by powers of § . This technique was already used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. We start out with a block size of &amp; 2</p><p>or some other power of § , and split any blocks that remain unmatched by a factor of § in each round. This technique is straightforward and does not require additional explanation.</p><p>(b) Optimized match verification to minimize the number of bits that are needed to verify, beyond a reasonable doubt, that substrings in the two files match. The idea is to first send a fairly weak hash that can be used to identify a possible match, and then use an optimized protocol based on ideas from group testing to filter out any false matches. A more limited version of this approach was also proposed in <ref type="bibr" target="#b24">[25]</ref>.</p><p>(c) Local and continuation hashes to decrease the number of bits that have to be initially sent in certain cases. In particular, continuation hashes are very weak hashes that are used to entend known matches towards the left and right in both files, allowing us to recurse down to block sizes for which "global" hashes (i.e, hashes that are matched against all positions in &amp; % ) are too expensive. Local hashes trade off these two cases. The case of continuation hashes can be modeled as a version of the problem of "searching with liars", also called Ulam's problem <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>(d) Decomposable hash functions to decrease the number of bits used for the initial hashes for finding possible matches. The simple idea is that if we have already transmitted a hash value for the parent block and the left sibling, then for certain types of hash functions we could compute the hash value of the right sibling from the other two hashes. In practice though, designing appropriate hash functions to implement this is nontrivial.</p><p>In the remainder of this section, we describe these ideas in more detail. Subsection 5.6 summarizes the protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimized Match Verification</head><p>Suppose bits to allow the client to identify candidates for matches; these candidates are then verified in an optimized manner, by having the client send verification hashes of its matches back to the server. We use three different ideas for this:</p><p>(i) The client can send one verification hash for each candidate back to the server. This hash is then compared only against the block that generated the original hash. The advantage is that verification hashes are only issued by the client for those hashes that found a possible match in % , which is typically a minority of hashes.</p><p>(ii) The client can send one verification hash for several candidate matches, in essence asking: Are all these matches correct? This should only be done once some degree of confidence is achieved, since one bad apple may cause the entire group to fail the test.</p><p>(iii) After a group match has failed, we can try to salvage some of the elements in the group by reissuing hashes for individual candidates or smaller groups.</p><p>Thus, we propose using a sequence of tests on various subsets of the candidates to efficiently identify those that are correct matches with fairly high certainty. This can be modeled by the following group testing problem <ref type="bibr" target="#b11">[12]</ref>, where false matches correspond to defective items and our goal is to identify all nondefective items, or a large subset of them, through a sequence of question of the following type: Are all elements in a specified group nondefective?</p><p>The answer to the question is unreliable in the following sense: If all elements are nondefective, the answer is always correct, but if there exists a defective item, then the correct answer is only returned with probability Q U</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>61</head><p>. The cost of each question is ¡ I Q (a ¡ -bit hash for the subset and one bit for the reply), and the goal is to reliably identify nondefective items with minimum cost. We are not aware of previous results on this precise version of the problem. In practice, it appears that using only two or three batches of tests already gives close to optimal results, as we see later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Local and Continuation Hashes</head><p>In the previous subsection, we saw that at least ) in the following sense: if the correct answer is "© " ("go to the right") then this answer is always returned; otherwise with probability 61 a wrong answer is returned. The cost of each query is again ¡ I Q . Known results show that it is not optimal to verify each answer with a high probability before going down one level in the search tree. However, in our case we are performing many such searches concurrently, and we could perform group testing across these searches to more efficiently verify answers on each level; it is not clear which strategy is best in this case. The same reasoning that motivated continuation matches also leads to another possible optimization. Suppose that a block at a particular level results in a confirmed match in the client file. In that case, it is unlikely that the sibling of this block would also find a match because (1) any match that is a continuation of the first match would have already been discovered at the parent level, and (2) any other match is unlikely since the match found by the first sibling will likely extend at least slightly into the other block. Thus, if we split the processing for each block size into two phases, first a search for matches using continuation hashes on blocks adjacent to confirmed matches, and then a search using global or local hashes, then in the second phase we can omit sending hashes for any blocks whose sibling found a confirmed match in the first phase (and also for any blocks for which continuation hashes were sent but no matches found). We could extend this idea by also first sending hashes, say, for all left siblings, and then only for those right siblings whose left sibling did not find a match. We did not implement this last idea since a technique described in the next subsection already avoids sending hashes for both siblings in this case. We did implement the idea of first sending continuation hashes, and then global hashes, and observed some moderate benefits. , and it should be strong in the sense that different strings should have a probability close to 61 of mapping to the same hash value, for a ¡ -bit hash. Finally, strings that can be obtained from each other through permutation should not be mapped to the same hash too often, as such cases are quite common in practice. Some of the techniques used in practice are also limited to certain ranges of block sizes. There are trade-offs between some of these properties, and in the end we designed our own modification of the Adler checksum in rsync, which appears to perform well on all our data sets and block sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Decomposable Hash Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Overview of the Protocol</head><p>We integrate all of the above techniques into a protocol consisting of a sequence of rounds, one for each block size. Each round consists of one or more roundtrips of communication.</p><p>Consider the left part of <ref type="figure" target="#fig_1">Figure 5</ref>.2 for an example. The round starts with the server sending a set of hashes to the client; these hashes may be global, local, or continuation hashes and are usually strong enough to identify candidates for matches, but not strong enough to reliably verify the matches. The decomposability of the hash function is implemented at a lower level by surpressing the transmission of hash bits that can be computed from sibling and ancestor hashes. The client then replies with a bitmap specifying which hashes found a match candidate, immediately followed by a set of verification hashes for the candidates. Each verification hash, based on MD5, can be for a single candidate or a group of candidates. The server than receives and checks the verification hashes, and replies with a bitmap specifying which verification hash was confirmed. If the round consists of a single batch of verification hashes, then this bitmap is included into the first roundtrip of the next round, and is immediately followed by hashes for the next smaller block size. Otherwise, the client may send additional batches of verification hashes. On the right side of <ref type="figure" target="#fig_1">Figure 5</ref>.2 we see a simple example consisting of two rounds. The client first sends a request to the server, who then sends hashes for the first block size. In the example, the first round has a single batch of verification hashes, while the second round involves two batches, maybe first a set of weak hashes for each candidate, and then stronger hashes for small subsets of or</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Evaluation</head><p>We now report on a preliminary experimental evaluation of our prototype software. We first describe the experimental setup in terms of implementation, data sets, and the other tools that we compare against. Subsection 6.2 provides results on two data sets previously used to evaluate delta compression tools, and Subsection 6.3 looks at the case of our web page update application. We note that our work is still ongoing and thus some numbers may improve slightly in subsequent versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We implemented a prototype with the techniques from the previous section in around <ref type="bibr">Q &amp; 2 &amp; 2</ref> lines of C code. The program was compiled with gcc and was run on several Linux servers. The prototype was not optimized in terms of CPU performance yet, and thus we do not focus too much on this aspect. We used the zdelta delta compressor 2 for the delta compression step, and used a very simple but efficient hash table based on double hashing to search for matching blocks. The prototype can be configured easily to vary the number of rounds and the set of techniques that are applied in each round, allowing us to look at the impact of each technique.</p><p>For the hashes sent from server to client, we used two implementations: a high quality one based on MD5 that is not rolling or decomposable (thus requiring significantly more running time and moderately more bandwidth) and a more heuristic construction of a hash function that is both rolling and decomposable. For the verification hashes, we used another MD5-based hash in both implementations, since in this case we do not need the other properties. As in rsync, there is a small probability of failure on each file. Such failures are detected through the exchange of a very strong Q -byte hash value for each file in the beginning; this also allows our code to detect unchanged files at that point. We compare our prototype implementation to rsync with default block size, rsync with an optimally chosen block size for each individual file, and the zdelta <ref type="bibr" target="#b45">[46]</ref> and vcdiff <ref type="bibr" target="#b22">[23]</ref> delta compressors. We note that current delta compressors are already fairly optimized in terms of compression, and thus provide a reasonable bound on what we could hope to expect from a file synchronization tool that does not have access to the outdated file (guarding a major breakthrough in delta compression techniques). We used the following data sets for our evaluation:</p><p>1. The gcc and emacs data sets used for evaluating delta compression performance in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, consisting of versions 2.7.0 and 2. , and § days later, respectively. In the experiments, we measure the cost of updating the base set to one of the updated sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance on Benchmark Data Sets</head><p>We first look at the performance of a very basic version of our protocol that uses a decomposable hash function, recursive halving of blocks, and a separate verification hash for each candidate match, but none of the other techniques. In Figures 6.1 and 6.2, we give results for the gcc and emacs data sets, respectively. For each block, our protocol sends a hash of 2 4 3FS</p><p>8 I bits to identify candidates, and the client replies with a bytes, and we plot the total cost in KB for the entire data set under different minimum block sizes after which we terminate the recursion. We also show results for rsync with default block size, an idealized rsync that knows the best block sizes for each file, and the zdelta delta compressor. For rsync, we show the costs of client-to-server and server-to-client communication, and for our protocol we show the cost of server-toclient and client-to-server communication during the map construction phase and the cost of the final delta (in order from bottom to top).</p><p>As we see, recursive halving and the use of decomposable hashes and verification hashes already gives significant benefits. The recursion should be stopped around a block size of Q § © or bytes for best results. (At that point, the increase in the cost of the map construction phase becomes higher than the decrease in the delta compression phase.) However, the best result is still about a factor of § away from the performance of the highly optimized delta compressor. We note that without decomposable hash functions, the amount of data sent from server to client in the map building phase would be about twice as high, and as a result the optimal minimum block size is also slightly larger in that case.</p><p>Next, we consider the benefit of using an improved match verification approach and continuation hashes in the protocol. In particular, we send -bit continuation hashes for all  We see from the leftmost bars in the two groups that the simple single-roundtrip group verification already gives some improvements over the corresponding numbers in <ref type="figure">Figure 6</ref>.1. For the gcc data sets, the best choice appears to be to use continuation hashes down to Q -byte blocks; for emacs it would be © -byte blocks (not shown). Also, we see that when using continuation hashes down to Q -byte blocks, having a minimum block size for global hashes of Q § © bytes becomes better than bytes. We also experimented with the use of local hashes as described in Subsection 5.4; however, we were unable to get any significant improvements from this technique. One issue here is the "harvest rate", i.e., the percentage of hashes that result in confirmed matches. Not surprisingly, blocks that qualify for continuation hashes have a fairly high harvest rate, which is another reason why they can be profitably used for much smaller block sizes. Local hashes do not fare well in this context, though we plan to revisit this issue in future experiments. We now look at the impact of the optimized match verification techniques from Subsection 5.3. In particular, we compare the following five strategies (note that we always send 2 5 3F ¤ S bits less from server to client for continuation hashes):</p><p>(1) the trivial verification with § -bit hashes for each candidate, used in the first experiment, (2) the slightly smarter approach above where we used The results in <ref type="figure">Figure 6</ref>.4 show slight improvements for each method for gcc (the same holds for emacs). However, almost all benefits are obtained with only one or two roundtrips. We experimented with a number of other settings, but none did significantly better than the best one shown. In particular, we did not find any benefit in being very aggressive about grouping large numbers of fairly uncertain candidates and then trying to salvage candidates from failed groups. Instead, it appears to be preferable to slowly grow the size of the groups as our confidence in the candidates grows. There are a number of subtle tradeoffs at work here: for example, decreasing the number of bits sent to the server from   but results in some real matches being lost due to false positives taking their place, and ultimately a larger delta. There are several other minor optimizations that we implemented, such as first sending continuation hashes and then global hashes in the next roundtrip, or the selective use of local hashes. None of these showed significant improvements. In <ref type="table" target="#tab_12">Table 6</ref>.1 we show the best results that we were able to obtain by using all techniques; we note that this results in a total of more than <ref type="bibr" target="#b1">2</ref> roundtrips and is thus probably not good in practice since each roundtrip also requires some computation (and sometimes a scan of the files).</p><p>Looking at  as for gcc this results in a data rate of only a few hundred kbits/s over the network. For faster networks and highly redundant data sets, CPU performance would currently be a bottleneck. Finally, our current implementation is also not optimized in terms of memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance in a Web Page Update Application</head><p>We now look at the web page update application that motivated our work. Recall that we are given ten thousand pages, selected at random from the web, that were recrawled every night for several weeks. Each set has a total size of around Q 2 MB, with about Q KB per page on average. Some of the files are not updated at all between crawls, while others change only slightly. In <ref type="table" target="#tab_12">Table 6</ref>.2, we show the bandwidth cost of maintaining such collections at a client using our techniques, under different update frequencies. We observe that our techniques support the maintenance of very large replicated sets of web pages even over fairly slow links, and improve over rsync by nearly a factor of § . For example, if we synchronize the pages every two days then slightly more than § MB of data transfer suffices to maintain Q 2 &amp; 2 ¤ 2 &amp; 2 pages at a client PC, which is easily done over cable or DSL links. The result shown are the best we could get for our protocol with all optimizations, but as before there are simpler settings with fewer roundtrips that perform within to of optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding Remarks</head><p>There are a number of unresolved issues and open problems left by our work. First, the current implementation is a very early prototype, and we have not optimized it in terms of CPU performance. This may result in the CPU becoming a bottleneck for faster networks. While some overhead is due to the repeated passes over the data in the different communication phases, we believe that significant improvements are possible through careful optimization. Some minor additional improvements in bandwidth efficiency should also be possible. We plan to integrate our implementation into a system for maintaining large sets of changing web pages over wide area networks. We also intend to use the presented techniques as the basis for a new general purpose tool for file synchronization over slow links that we plan to release. Ideally, such a tool would be adaptive and thus choose the best set of parameters and number of roundtrips based on the characteristics of the data set and communication link. On the theoretical side, we are working on improved asymptotic bounds for file synchronization under some common file similarity metrics. Interestingly, the idea of continuation hashes used in this paper appears to be very promising in this context as well. A detailed study of the group testing and "searching with liars" problems discussed in this paper might also lead to slight improvements in practice. We are also studying how to improve file synchronization if we are restricted to just one or two round-trips. In this case, it seems difficult to improve significantly over rsync in practice, but we believe that at least some moderate gains are possible. Finally, we plan to look at synchronization in asymmetric cases, e.g., in cases with server broadcast capability, lower upload speed, or a bottleneck at a busy server.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 1 .rsync algorithm on a small ex- ample. The client sends a set of hashes while the server replies with a stream of literals and indices identifying hashes.</head><label>21</label><figDesc>Figure 2.1. The rsync algorithm on a small example. The client sends a set of hashes while the server replies with a stream of literals and indices identifying hashes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.1 illustrates our basic approach. Suppose that machine " contains a file ¢ ¡ &amp; £ ¦ ¥ C "BDAFHKZER", and ! contains # C "ABADFHKBCZY". Suppose " splits ¡ ¤ £ ¦ ¥</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 . 1 .</head><label>51</label><figDesc>Figure 5.1. Framework for file synchronization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 . 2 . A single round of the protocol (left), and a complete protocol (right) consisting of the client request, one round with one batch of verification hashes, one round with two batches of verification hashes, and the final delta sent from server to client.</head><label>52</label><figDesc>Figure 5.2. A single round of the protocol (left), and a complete protocol (right) consisting of the client request, one round with one batch of verification hashes, one round with two batches of verification hashes, and the final delta sent from server to client.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 . 1 .</head><label>61</label><figDesc>Figure 6.1. Performance of the basic protocol with different minimum block sizes on the gcc data set, compared to rsync and zdelta. For rsync, we show the costs of client-to-server and server-to-client communication, and for our protocol we show the cost of server-toclient and client-to-server communication during the map construction phase and the cost of the final delta (in order from bottom to top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 . 2 .</head><label>62</label><figDesc>Figure 6.2. Performance of the basic protocol with different minimum block sizes on the emacs data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 . 3 .of the protocol with continuation hashes of various minimum block size. Shown in the leftmost bar of each group is the performance with no continuation hashes but with group verification.</head><label>63</label><figDesc>Figure 6.3. Performance of the protocol with continuation hashes of various minimum block size. Shown in the leftmost bar of each group is the performance with no continuation hashes but with group verification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 . 4 .of different techniques for match verification on the gcc data set. Triv- ial verification is shown in the leftmost bar, while the other bars show optimized solutionsroundtrips per round as de- scribed.</head><label>64</label><figDesc>Figure 6.4. Performance of different techniques for match verification on the gcc data set. Trivial verification is shown in the leftmost bar, while the other bars show optimized solutions with Q ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>is no reason to believe that the match stops exactly at the power-of-two block boundaries used by the algorithm, and thus we would like to try to ex</head><label></label><figDesc>) would separate most of the true and false matches, and we could directly continue with the op- timized match verification protocol described above. Since this makes the hashes much cheaper, we can afford to work with smaller block sizes than for global hashes. This idea can be generalized to local hashes that use a lit- tle bit more than ¡ bits and that are compared only with matches in a neighborhood of size § ¤ in ¤ . The neigh- borhood can be chosen with various heuristics based on con- firmed matches of surrounding blocks of ¤ ¡ ¤ £ ¦ ¥ , using the ob- servation that most changes to files are fairly local in nature. The problem of expanding matches, say, towards the right, can be modeled as a searching game with liars [37, 49] as follows: imagine performing a binary search with unreliable comparisons (corresponding to continuation tests with block sizes D</figDesc><table>2 
5 3F 

¤ 
S 
bits 
are needed for a "global" hash that is compared to all po-

sitions in the client file. However, in certain situations we 
can do better. Consider a scenario where we have already 
found a confirmed match between blocks 

¡ 
&amp; £ 
¦ ¥ 
' 

 § 

I 
D 
¨ U 
Q 
W 0 

and 




'¡ 
 § 

¢ ¡ 

I 
D 
U 
Q 
W 0 
. There -
tend the match towards the left and right in subsequent itera-
tions with smaller block sizes. Thus, for block size 
D 

¤ £ 

C 
D 

¦ ¥ 

 § 

, 
we could send a hash for blocks 

¤ ¡ 
¤ £ 
$ ¥ 
¨ ' 

U 
D 

 § £ 

 § 

U 
Q 
W 0 
and 

T ¡ 
¤ £ 
$ ¥ 
¨ ' 

I 
D 
E  § 

I 
D 
I 
D 

 § £ 

&amp; U 
Q 
W 0 
to the client to check for matches 
only with blocks 

¤ 
% 

'¡ 
P U 
B D 

 § £ 

H  § 

¢ ¡ 

P U 
Q 
W 0 
and 

¤ 


&amp; '¡ 
I 
D 
E  § 

¨ ¡ 

I 
D 
G I 
D 

 § £ 

1 U 
Q 
0 
, 
respectively. In this case, even a very small number of bits 
(say, 

or 

per hash¦ ¥ 

 § 

, 
D 

¦ ¥ 


or 

D 

¦ ¥ 


, etc.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>. A decomposable hash function allows us to signifi- cantly save on the cost of the hashes for identifying candidate for matches: since we have already transmitted a hash for the parent block, we only have to send one additional hash per pair of sibling blocks -the hash for the other sibling can then be computed from these two. In practice, there are some additional obstacles. Since our other techniques use different numbers of bits for different blocks, we ideally would like to have a hash function that is "bit-prefix" decomposable in the sense that from the first bits of</figDesc><table>We say that a hash function 

¡ 

is composable if we can 
efficiently compute 

¡ 

F 

( ' 
$  § 

¤ 

0 
H S 
from the values 

¡ 

F 

( ' 
$  § 

0 
H S 
, 

¡ 

F 
1 
( ' 

I 
Q 
¤  § 

¤ 

0 
H S 
, 

U 

, and 

¤ 

U 

U 
R Q 
. A hash function is 
decomposable if we can efficiently compute 

¡ 

F 

( ' 

I 
Q 
¤  § 

¤ 

0 
S 

from the values 

¡ 

F 
1 
( ' 
 § 

¤ 

0 
S 
, 

¡ 

F 

( ' 
$  § 

0 
H S 
, 

¤ 

U 

, and 

¤ 

U 

U 
Q 
, 
and also 

¡ 

F 

( ' 
$  § 

0 
H S 
from 

¡ 

F 
1 
( ' 
 § 

¤ 

0 
S 
, 

¡ 

F 
1 
( ' 

I 
Q 
¤  § 

¤ 

0 
H S 
, 

¤ 

U 

, and 


U 

¡ 

F 

( ' 
$  § 

¤ 

0 
S 
and 

¡ 

F 

( ' 
$  § 

0 
H S 
we would like to be able to 
compute the first 

bits of 

¡ 

F 
1 
( ' 

I 
Q 
¤  § 

¤ 

0 
H S 
, for any 

. In ad-
dition, there are several other desirable properties of a hash 
function. It should be "rolling" so that 

¡ 

F 
1 
( ' 
I 
Q 
&amp;  § 

¤ 

I 
V Q 
W 0 
H S 
can 
be computed in constant time from 

¡ 

F 
1 
( ' 
 § 

¤ 

0 
S 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>7 .1 of gcc, and 19.28 and 19.29 of emacs. The newer versions of gcc and emacs consist</head><label>7</label><figDesc></figDesc><table>of 
Q 
2 
¤ 2 

 § 

and 
Q 

 § 
© 


files, and each collection has a size 
of around 

 § 


MB. In each case we measured the cost of 
updating all files in the older version to the newer one. 

2. A collection of ten thousand web pages that we crawled 
repeatedly every night for several weeks during Fall 
2001. Pages were selected at random from two massive 
web crawls of hundreds of millions of pages and are thus 
a fairly reasonable random sample of the web. We use 
four versions of each page: a base set and three updated 
versions crawled 

 § 

, 

 § 

2 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 6 .1. Best results for the gcc and emacs data</head><label>6</label><figDesc></figDesc><table>sets using all techniques (in KB). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>1, we see that we achieve bandwidth 
savings of a factor of 
Q 

to 

 § 

© 
over rsync with our new 
techniques, and that we are within 





to 

T 2 

of the best 
compression achieved by the delta compressor. A note con-
cerning computation times: we have not yet optimized our 
code in terms of CPU performance but expect significant fu-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 6 .</head><label>6</label><figDesc>ture improvements. The purpose of the work presented here is to show the potential benefits of the various techniques, and we plan to build a high performance version based on our conclusions. The prototype currently runs at a speed of up to a few MB of raw data per second; with a compression ratio of © 2 Q</figDesc><table>2. Cost of updating a web collection us-
ing various methods, for various update fre-
quencies. The cost is in KB for 
Q 
2 
&amp; 2 
¤ 2 
&amp; 2 
web 
pages. 

</table></figure>

			<note place="foot">© surviving candidates. Finally, the server sends a delta to the client. In our implementation, a simple parameter file is used to specify all the options and techniques that should be used in each round, such as the type and number of bits per hash, the strategy for verifying candidate hashes through individual or group hashes or for salvaging failed candidates, etc.</note>

			<note place="foot" n="2"> Available at http://cis.poly.edu/zdelta/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An optimal strategy for comparing file copies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abdel-Ghaffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Abbadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="1994-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the scalability of data synchronization protocols for PDAs and mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Starobinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trachtenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network Magazine, special issue on Scalability in Communication Networks</title>
		<imprint>
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is a file synchronizer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balasubramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM/IEEE MOBICOM&apos;98 Conference</title>
		<meeting>of the ACM/IEEE MOBICOM&apos;98 Conference</meeting>
		<imprint>
			<date type="published" when="1998-10" />
			<biblScope unit="page" from="98" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A class of randomized strategies for low-cost comparison of file copies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="170" />
			<date type="published" when="1991-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crawler-friendly web servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brandman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Performance and Architecture of Web Servers (PAWS)</title>
		<meeting>of the Workshop on Performance and Architecture of Web Servers (PAWS)</meeting>
		<imprint>
			<date type="published" when="2000-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keeping up with the changing web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The evolution of the web and implications for an incremental crawler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 26th Int. Conf. on Very Large Data Bases</title>
		<meeting>of 26th Int. Conf. on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2000-09" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synchronizing a database to improve freshness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD Int. Conf. on Management of Data</title>
		<meeting>of the ACM SIGMOD Int. Conf. on Management of Data</meeting>
		<imprint>
			<date type="published" when="2000-05" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective change detection using sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Int. Conf. on Very Large Databases</title>
		<meeting>of the 28th Int. Conf. on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="2002-09" />
			<biblScope unit="page" from="514" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Communication complexity of document exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sahinalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Vishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM-SIAM Symp. on Discrete Algorithms</title>
		<meeting>of the ACM-SIAM Symp. on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2000-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pastiche: Making backup cheap and easy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Symp. on Operating System Design and Implementation</title>
		<meeting>of the 5th Symp. on Operating System Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The detection of defective members in large population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dorfman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="436" to="440" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rate of change and other metrics: a live study of the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Symp. on Internet Technologies and Systems (ITS-97)</title>
		<meeting>of the USENIX Symp. on Internet Technologies and Systems (ITS-97)<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="page" from="147" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An adaptive model for optimizing performance of an incremental web crawler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International World Wide Web Conference</title>
		<meeting>of the 10th International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001-05" />
			<biblScope unit="page" from="106" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic algorithm for updating files over a communication link</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>of the 9th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1998-01" />
			<biblScope unit="page" from="300" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using the web efficiently: Mobile crawlers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of thw 17th Int. Conf. on Computer Science</title>
		<meeting>thw 17th Int. Conf. on Computer Science<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Webbase: Building a web warehouse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Plenary Talk at the 2003 Federated Computing Research Conference</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Terascale sneakernet: Using inexpensive disks for backup, archiving, and data exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Barclay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandenberg</surname></persName>
		</author>
		<idno>MSR-TR- 2002-54</idno>
		<imprint>
			<date type="published" when="2002-05" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="http://www.grub.org/" />
	</analytic>
	<monogr>
		<title level="j">Grub. Distributed Internet Crawler</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WebBase : A repository of web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th Int. World Wide Web Conference</title>
		<meeting>of the 9th Int. World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delta algorithms: An empirical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tichy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient randomized pattern-matching algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Engineering a differencing and compression data format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Usenix Annual Technical Conference</title>
		<meeting>the Usenix Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2002-06" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<title level="m">Communication Complexity</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multiround rsync</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
	<note>Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">File system support for delta compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">MS Thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An application of group testing to the file comparison problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Madej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th Int. Conf. on Distributed Computing Systems</title>
		<meeting>of the 9th Int. Conf. on Distributed Computing Systems</meeting>
		<imprint>
			<date type="published" when="1989-06" />
			<biblScope unit="page" from="237" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A parity structure for large remotely located replicated data files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="727" to="730" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient replicated remote file comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="651" to="659" />
			<date type="published" when="1991-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Set reconciliation with almost optimal communication complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zippel</surname></persName>
		</author>
		<idno>TR2000-1813</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A lowbandwidth network file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muthitacharoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazì Eres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th ACM Symp. on Operating Systems Principles</title>
		<meeting>of the 18th ACM Symp. on Operating Systems Principles<address><addrLine>October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="174" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Worst-case interactive communication II: Two messages are not optimal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="995" to="1005" />
			<date type="published" when="1991-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interactive communication of balanced distributions and of correlated files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Discrete Math</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="548" to="564" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical algorithms for interactive communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Information Theory</title>
		<imprint>
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One-way communication and error-correcting codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 IEEE Int. Symp. on Information Theory</title>
		<meeting>of the 2002 IEEE Int. Symp. on Information Theory</meeting>
		<imprint>
			<date type="published" when="2002-06" />
			<biblScope unit="page">394</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient location of discrepancies in multiple replicated large files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Metzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Searching games with errors -fifty years of coping with liars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pelc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="71" to="109" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Remote file transfer method and apparatus, 1995. US Patent Number 5446888</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pyne</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An algebraic approach to file synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Csirmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th ACM Int. Symp. on Foundations of Software Engineering</title>
		<meeting>of the 9th ACM Int. Symp. on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">In-place rsync: File synchronization for mobile and wireless devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Annual Technical Conference</title>
		<meeting>of the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Value-based web caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rhea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. World Wide Web Conference</title>
		<meeting>of the 12th Int. World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Low cost comparison of file copies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowdidge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burkhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Int. Conf. on Distributed Computing Systems</title>
		<meeting>of the 10th Int. Conf. on Distributed Computing Systems</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A protocol independent technique for eliminating redundant network traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGCOMM Conference</title>
		<meeting>of the ACM SIGCOMM Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient PDA synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Starobinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Algorithms for delta compression and remote file synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<editor>K. Sayood</editor>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Lossless Compression Handbook. Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">zdelta: a simple delta compression tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<idno>TR-CIS-2002-02</idno>
		<imprint>
			<date type="published" when="2002-06" />
		</imprint>
		<respStmt>
			<orgName>Polytechnic University, CIS Department</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Efficient Algorithms for Sorting and Synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000-04" />
		</imprint>
		<respStmt>
			<orgName>Australian National University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The rsync algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tridgell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mackerras</surname></persName>
		</author>
		<idno>TR-CS-96-05</idno>
		<imprint>
			<date type="published" when="1996-06" />
		</imprint>
		<respStmt>
			<orgName>Australian National University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adventures of a Mathematician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ulam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>Scribner</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
