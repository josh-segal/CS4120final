<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneously Modeling Semantics and Structure of Threaded Discussions: A Sparse Coding Approach and Its Applications *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 19-23, 2009,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Ming</forename><surname>Yang</surname></persName>
							<email>jmyang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Asia., Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
							<email>ruicai@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Asia., Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Jing</forename><surname>Wang</surname></persName>
							<email>xjwang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Asia., Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>weiwang1@fudan.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Asia., Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University. {chen_lin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneously Modeling Semantics and Structure of Threaded Discussions: A Sparse Coding Approach and Its Applications *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">July 19-23, 2009,</date>
						</imprint>
					</monogr>
					<note>Threaded discussion, sparse coding, reply reconstruction, junk identification, expert finding * This work was done during the first author was an intern in Microsoft Research Asia. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR&apos;09,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Categories and Subject Descriptors I51 [Pattern Recognition]: Models -Statistical General Terms Algorithms, Experimentation Keywords</keywords>
			</textClass>
			<abstract>
				<p>The huge amount of knowledge in web communities has motivated the research interests in threaded discussions. The dynamic nature of threaded discussions poses lots of challenging problems for computer scientists. Although techniques such as semantic models and structural models have been shown to be useful in a number of areas, they are inefficient in understanding threaded discussions due to three reasons: (I) as most of users read existing messages before posting, posts in a discussion thread are temporally dependent on the previous ones; It causes the semantics and structure to be coupled with each other in threaded discussions; (II) in online discussion threads, there are a lot of junk posts which are useless and may disturb content analysis; and (III) it is very hard to judge the quality of a post. In this paper, we propose a sparse coding-based model named SMSS to Simultaneously Model Semantics and Structure of threaded discussions. The model projects each post into a topic space, and approximates each post by a linear combination of previous posts in the same discussion thread. Meanwhile, the model also imposes two sparse constraints to force a sparse post reconstruction in the topic space and a sparse post approximation from previous posts. The sparse properties effectively take into account the characteristics of threaded discussions. Towards the above three problems, we demonstrate the competency of our model in three applications: reconstructing reply structure of threaded discussions, identifying junk posts, and finding experts in a given board/sub-board in web communities. Experimental results show encouraging performance of the proposed SMSS model in all these applications.</p>
			</abstract>
		</profileDesc>
		<revisionDesc>
				<date type="submission" when="-1" />
		</revisionDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Threaded discussion has long been a popular option for web users to communicate with others, from thousands of web forum sites, mailing lists, chat rooms to web logs, instant messaging groups, and so on. As we are in the age of Web 2.0, threaded discussion acts as an important tool to facilitate collaborative content contributions. With millions of users' contribution, highly valuable knowledge and information have been accumulated on various topics. These topics include recreation, sports, games, computers, art, society, science, home, health, etc. 1 ; and especially some topics related to our daily life which are rarely seen in traditional web pages. As a result, recent years have witnessed more and more research efforts on mining information from online discussion threads.</p><p>A discussion thread usually originates from a root post by the thread starter. <ref type="figure">Fig. 1</ref> gives an intuitive description of a discussion thread 2 . It contains 7 posts. The first post is a piece of news about the release of "SilverLight 2.0". Some users comment on this post, i.e., the 2 nd and 3 rd posts are about the "update time"; some users have further questions and initiate sub-discussions, i.e., the 5 th 6 th and 7 th posts are about "Javascript communication"; and others troll or complain in some posts, i.e., the 4 th post. As more users are joining in and making comments, the discussion thread grows, forming a nested dialogue structure as shown in the left part of <ref type="figure">Fig. 1</ref>. Furthermore, threaded discussions show rich complexity in the semantics. Since users always respond to others, previous posts affect later posts and cause the topic drift in a discussion thread, as shown in the right part of <ref type="figure">Fig. 1</ref>.</p><p>Mining discussion threads is challenging. The following reasons prevent people from fully exploring the value of discussion threads. (I) Posts in a discussion thread are temporally dependent upon each other. A newcomer may read some of the previous messages before posting. Replies indicate sharing of topics and vice versa. Hence a post is a mixture and mutation of previous posts by nature. Unfortunately, such specific orderings and intra dependencies of posts in one thread are neglected by most existing research methods. (II) As discussion threads are born to encourage content distribution and contribution, they sometimes become targets of spammers. Meanwhile, some messages are casual chitchat and are needless to analyze. Posts of either spam or chitchat are regarded as junk posts. Though chitchat might serve to foster relationships between participants, such kinds of junk posts are useless and may disturb content analysis in some applications, such as Question &amp; Answer mining, expert finding, and so on. (III) It is very hard to estimate the quality of a post. It is true that valuable posts are usually long articles; but there is also a remarkable amount of long meaningless posts that are not meant to help others, and on the contrary there are some short insightful posts that inspire a lot of people. Thus measurements solely by content length or content relevance usually do not work.</p><p>Although previous research efforts have made progress in many information retrieval scenarios, few of them are suitable for mining online discussion threads. Current related work can be mainly classified into two categories: semanticsbased or linking structure-based. Probabilistic topic models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref> fall into the former class. Their main idea is to project documents to some latent topic space. However, most topic models assume documents in one collection to be exchangeable, i.e., their probabilities are invariant to permutation. It is contrary to the reply relationships among posts. Other works such as HITS <ref type="bibr" target="#b12">[13]</ref>, PageRank <ref type="bibr" target="#b15">[16]</ref> and their successive research belong to the latter class. These research efforts try to identify the importance of content of each document. They mainly utilize link relationships among various documents. Unfortunately, most community sites such as those forums powered by vBulletin and phpBB do not provide explicit reply relationships among posts. Furthermore, semantics and structure of a threaded discussion are highly dependent on each other. That is, when semantics evolves, the dialogue structure changes, and vice versa. This is the nature of discussion threads. Most previous research efforts can not solve this problem directly as they are solely from the semantic-centric view or from the structure-centric view.</p><p>In this paper, towards simultaneously modeling semantics and structure of threaded discussions, we propose a novel sparse coding-based approach called SMSS, which has the ability to integrate both the semantic and structure aspects of threaded discussions. For modeling semantics, similar to existing topic models, our approach can discover latent topics as bases to capture semantics in discussion threads; and each post is represented as a fusion of these semantic bases. Considering that users usually only discuss one or two topics in one individual post, we also add a sparse constraint in this fusion. For modeling structure, to embed the potential reply structure, topics of each post can just be sampled from the topics of those earlier posts in the same thread. We also add a sparse constraint here because one post only tries to reply one or two previous posts. At last, to demonstrate the effectiveness of the proposed SMSS model and to investigate the above problems (I)∼(III), we evaluate our model with the following three applications, respectively:</p><p>• Reply Relationship Reconstruct. We reconstruct the reply relationship among posts based on both their semantic and structure coefficients estimated by SMSS.</p><p>Experimental results demonstrate that SMSS achieves higher precision than previous topic models.</p><p>• Junk Detection. We illustrate how SMSS is competitive on junk detection. Concretely, we introduce a background topic into the topic space and detect the junk posts based on their coefficients on the background topic. Experimental results show that SMSS outperforms classification-based algorithms and existing topic models.</p><p>• Expert Finding. We try to find out experts in a given board/sub-board. Since the key problem of expert finding is to estimate the author's average post quality, we demonstrate the value of structure information from SMSS and the capability of measuring post content quality.</p><p>This paper is organized as follows. Section 2 briefly reviews related work; Section 3 presents the details of the proposed SMSS model; and Section 4 describes how the SMSS model are leveraged in applications. Experimental results are shown in Section 5. In Section 6, we draw the conclusion and plan the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we introduce previous research efforts which are related to our work. We begin with the methodologies, briefly summarizing their innovations, advantages, and drawbacks, and then we give an overview of recent technical trends in mining threaded discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Models</head><p>Much work has been proposed to model text semantics (or topics), such as latent Dirichlet allocation (LDA) <ref type="bibr" target="#b2">[3]</ref> and the more general discrete component analysis <ref type="bibr" target="#b3">[4]</ref>. They decompose documents into a small number of topics which are distributions over words. In LDA, each document is produced by choosing a distribution over topics with a Dirichlet prior; each word is sampled from a multinomial of topicword association. Some work has been proposed to extend LDA to model multiple relationships, such as authorship <ref type="bibr" target="#b17">[18]</ref>, email <ref type="bibr" target="#b13">[14]</ref>, and etc.; while others tried to model the background, topic, and document specific words simultaneously <ref type="bibr" target="#b4">[5]</ref>. Some recent publications began to model time dependency among documents, e.g., <ref type="bibr" target="#b1">[2]</ref> tried to model the dependency in discrete time periods and <ref type="bibr" target="#b20">[21]</ref> considered time to be continuous. However, these models only considered the topic drift within two adjacent time periods, which is not suitable for the hierarchical intra dependency of posts in one discussion thread. In general, the main drawback of semantic models for discussion thread analysis is that they only capture the semantic information but ignore the temporal structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Models</head><p>There have also been studies focusing on structural modeling. A collection of documents and linkages between them are constructed as a graph. The nodal importance or nodal quality can be estimated by the structural centrality of the nodes in the graph, where the importance refers to authority, popularity, expertise or impact in various applications. In <ref type="bibr" target="#b10">[11]</ref>, several merits are presented to measure the nodal structural centrality. Classical structural models include HITS <ref type="bibr" target="#b12">[13]</ref> and PageRank <ref type="bibr" target="#b15">[16]</ref>. HITS is carried out in an iterative manner, propagating the authority and hub of one node to another. PageRank simulates the random walks of a surfer, who continuously jumps from one web-page to another linked page with a uniform probability, with a damping factor. Although structural models have been applied with remarkable success in different domains, it is not suitable to analyze threaded discussions. This is because these works highly rely on the link structure among documents; while there is usually no explicit link structure among the posts in a discussion thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mining Threaded Discussions</head><p>To the best of our knowledge, little previous work in the literature models semantics and structure of threaded discussions simultaneously. However, there are still some previous works that should be investigated, as they are related to mining threaded discussions. Most of them focus on knowledge acquisition from web forums. We can again categorize their methods into semantic models and structure models.</p><p>For the semantic models, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> targeted at extracting and ranking answers for given questions in web forums based on their content information. Methods in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b18">[19]</ref> mine the relationship among posts, relationship between post and thread title, and relationship between post and a thread based on their content similarity, respectively.</p><p>For the structure models, FGrank <ref type="bibr" target="#b7">[8]</ref> automatically generates topic hierarchy, constructs page level link graph based on topic hierarchy, modifies the PageRank algorithm in order to rank forum pages. But it cannot represent the characteristic of reply relationship among posts. EABIF <ref type="bibr" target="#b19">[20]</ref> is another work related to structure models. It built the influence network among consumers based on the time order of consumption records. It then proposed how to model the influence diffusion. We believe that it is also possible to apply this model in threaded discussions. The above studies implement the semantic decomposition and structure reconstruction in two phases, which conflict with our assumptions that structure in discussion threads usually changes along with semantics. Consequently, the reconstructed structure by prior research is not consistent with the evolving nature of semantics in threaded discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A SIMULTANEOUS MODEL OF SEMAN-TICS AND STRUCTURE</head><p>In this section, we present the details of SMSS. First, we introduce some useful concepts in threaded discussions.</p><formula xml:id="formula_0">Definition 1 (post).</formula><p>A post is a user-submitted content at a particular time stamp. Posts are contained in threads, where they appear as boxes one after another. If a post is not the first of the corresponding thread, it is referred to as a reply.</p><p>Definition 2 (thread). A discussion thread (without ambiguity we use thread hereafter) is a series of posts, usually displayed-by default-from the oldest to the latest. A thread is initiated by a root post which may contain news, questions, opinions, and so on; and is followed by a number of non-root posts. A post other than the root one must reply to one of its previous posts in the same thread.</p><p>Intuitively, a threaded discussion has the following four characteristics. We take the thread in <ref type="figure">Fig. 1</ref> as an example.</p><p>A discussion thread has several topics. The example in <ref type="figure">Fig. 1</ref> mainly discusses two topics, namely "Silverlight 1.0" and "Javascript". Accordingly, we hope we can find a semantic representation of each post, such representation demonstrates a mixture of topics in the post. Suppose there are T topics and V words; the j th topic can be described as a distribution in the word space R V , as</p><formula xml:id="formula_1">x (j) ∈ R V , 1 ≤ j ≤ T . The i th post d (i)</formula><p>is assumed to be a mixture of various topics, as:</p><formula xml:id="formula_2">d (i) T j=1 θ (i) j · x (j)</formula><p>where θ</p><formula xml:id="formula_3">(i) j is the coefficient of post d (i) on topic x (j) . To esti- mate the topic space X = { x (1) , . . . , x (T ) }, we can minimize the loss function: D − XΘ 2 F</formula><p>Here the thread contains L posts as</p><formula xml:id="formula_4">D = { d (1) , . . . , d (L) }; and the coefficient matrix Θ = { θ (1) . . . θ (L)</formula><p>}. An individual post is related to a few topics. Although one thread may contain several semantic topics, each individual post usually concentrates on a limited number of topics. For example, the 2 nd post and the 3 rd post in <ref type="figure">Fig. 1</ref> are only related to the "Silverlight 1.0" topic. Thus we may assume that the coefficient vector for each post is very sparse and introduce an L1 sparse regularizer:</p><formula xml:id="formula_5">θ (i) 1</formula><p>A post is related to its previous posts. When a user joins a thread, he/she usually reads those existing posts in the thread. If he/she is interested in some previous posts, he/she may write down their comments. Thus the semantics of the reply post is related to its previous posts, which reflects the structural characteristics of a thread. For example in <ref type="figure">Fig. 1</ref>, the 7 th post is related to both the 5 th and 6 th posts. We can formally represent such structural constraint as a regularizer:</p><formula xml:id="formula_6">θ (i) − i−1 k=1 b (i) k · θ (k) 2 F where b (i)</formula><p>k is the structural coefficient between the i th post and the k th post and which shows how the k th post affects the i th post. The fact that θ (i) corresponds to the post d (i) can be approximated by a linear combination of θ (k) in previous posts.</p><p>The reply relations are sparse. Note that in real scenarios, users usually intend to comment on one or two previous posts, for example in <ref type="figure">Fig. 1</ref>, although there are a lot of posts before the 6 th post, it is only related to the 5 th post. We again introduce an L1 regularizer to favor sparse structural coefficients:</p><formula xml:id="formula_7">b (i) 1</formula><p>Based on the above description, we define the SMSS model as follows. Given the post matrix D, topic number T , the goal of thread modeling is to estimate the value of the topic matrix X, the coefficient matrix Θ, and the structural coefficients b of each post, by minimizing the following loss function f :</p><formula xml:id="formula_8">f = D − XΘ 2 F + λ1 L i=1 θ (i) 1 +λ2 L i=1 θ (i) − i−1 k=1 b (i) k · θ (k) 2 F + λ3 L i=1 b (i) 1<label>(1)</label></formula><p>In (1), the first term encourages the post to be reconstructed well from topics; the second term constraints the topic representation to be sparse for each post; the third term encourages the post to be approximated from previous posts within the same thread; and the fourth term constrains that the structural coefficients should be sparse too. The optimization objective balances the four terms by parameters λ 1 , λ 2 , and λ 3 . In this way, both the semantic and the structural information are estimated simultaneously. Suppose we have a collection of M threads which shared the same topic matrix X, we can optimize them together by minimizing the loss functions over all the threads to obtain a globally optimal topic representation and structural coefficients as follows:</p><formula xml:id="formula_9">minimize X,{Θ (n) },{ b (i) (n) } M n=1 f (n) (·)<label>(2)</label></formula><p>Although the optimization problem is not jointly convex, we can still solve it by iteratively minimizing the convex subproblems. For more details about the optimization, please refer to the appendix section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">APPLICATIONS</head><p>Towards the three problems in threaded discussions, from Section 4.1 to Section 4.3, we conduct three experiments to answer the following questions:</p><p>• Is our model capable of recognizing structural information among posts? To answer this, we show the result of reply relationship reconstruction. • Is our model capable of capturing semantics? To answer this, we show the result of junk identification. • Is our model capable of measuring content quality? To answer this, we show the result of expert finding, since the core problem of finding experts is to estimate the overall quality of user-generated content.</p><p>Actually, the proposed solutions are straightforwardly implemented based on the proposed SMSS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reply Reconstruction</head><p>We reconstruct reply relationships among posts based on both their semantics and structures estimated by the SMSS model. Intuitively, posts with reply relations should have similar terms. Thus the task of finding reply relations can be converted into a text retrieval task. However, the major challenge here is that the length of each post is usually very short and there are usually few common words between two posts. Topic similarity is more robust and interpretable by reducing high-dimensional term representation to lowerdimensional latent topics. However, only topic similarity itself may lose some detailed information. Combining topic similarity with term similarity is more efficient for short and sparse text and has been verified in <ref type="bibr" target="#b16">[17]</ref>. Our idea here is to integrate term similarity, topic similarity, and structural similarity. The structural similarity can be seen as a lowerdimensional representations of topics. Since each individual post may only focus on one or two topics, there are still few common topics shared by two posts. Here, the structural similarity acts as a smoother of topic similarity; and Algorithm 1 Reply-Reconstruction(D (n) , ρ). Given the n th thread with length L, D (n) is the set of posts and</p><formula xml:id="formula_10">D (n) = { d 1 (n) , . . . , d L (n) }. ρ is the threshold. Rep (n)</formula><p>is the reply structure.</p><p>1: Order D (n) based on their posted times 2: for i = 1 to L do 3:</p><formula xml:id="formula_11">for j = 1 to i do 4: compute sim( d i (n) , d j (n) ) 5:</formula><p>end for 6:</p><formula xml:id="formula_12">c = arg max j sim ( d i (n) , d j (n) ) 7: if sim( d i (n) , d c (n) ) ≥ ρ then 8: Rep (n) i = d c (n) 9: else 10: Rep (n) i = d 1 (n) 11:</formula><p>end if 12: end for 13: Return the reply structure Rep (n) we will show its improvement in the experiments. Formally, the similarity of a post j and an early post i is defined as:</p><formula xml:id="formula_13">sim(i, j) = sim( d (i) , d (j) ) + w 1 · sim( θ (i) , θ (j) ) + w 2 · sim( b (i) , b (j) )<label>(3)</label></formula><p>where the similarity in each component is the cosine value of two corresponding feature vectors. We use w1 and w2 as the ratios to balance the three components. Based on this similarity measure, we propose an algorithm to analyze a thread with L posts, as shown in Algorithm 1. That is, for a new unlabeled post we compute the similarity between itself and each of its previous posts, rank the similarities, and then choose the top ranked post as a candidate parent. In case that the candidate parent is not similar enough to the new post, we assume that the new post initializes a new branch of the thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Junk Identification</head><p>A primary issue in online discussion threads is junk. For example, the fourth post in <ref type="figure">Fig. 1</ref> is a typical chitchat post. Moveover, junk has become a focus of community administrators, users, and even developers, because junk content increases the cost of maintaining a clean and healthy communication environment, and distracts users from readings <ref type="bibr" target="#b14">[15]</ref>.</p><p>A discussion thread usually focuses on a very limited number of topics, while junk posts usually have different topics and act as outliers. Moreover, junk content is similar and common among various threads. To detect just posts, we introduce a background topic in the model. To construct the background topic, we select some common words by their thread frequencies in the whole data corpus as:</p><formula xml:id="formula_14">x (bg) w = |{V w : V w ∈ d i }|/|D| (4)</formula><p>where |D| is the total number of posts in the corpus and |{Vw : Vw ∈ d i }| represents the number of posts where the term V w appears. In (2), the topic matrix X is shared by all threads. We define and normalize the background topic x (bg) as the last column of X. When optimizing (2), we fix x (bg) in the optimization process.</p><p>Finally, we propose a straightforward criterion of junk detection based on the topic coefficients θ (i) of each post. The probability of a post i being a junk is defined as:</p><formula xml:id="formula_15">p junk (i) = θ (i) bg / t θ (i) t<label>(5)</label></formula><p>In this way, posts close to the background topic are very likely to be junk. It is worth noting that in the SMSS model, the topic projection is also affected by structural constrains, and provides more accurate description of each post. We will show this advantage in the experiments by comparing some other models which only characterize a post according to its semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expert Finding</head><p>Online communities have become important places for people to seek and share expertise. The reply relationships among posts actually construct a reply network. Studies on such a reply network tell us that reply structure in a discussion thread can help to evaluate users' expertise <ref type="bibr" target="#b21">[22]</ref>. That is, posts written by experts are usually of high quality and thus attract more users' attention and more replies. However, experiments in <ref type="bibr" target="#b21">[22]</ref> were conducted on a particular site which recorded the reply relationships among posts in each discussion thread; while for most community sites, such as those forums powered by vBulletin and phpBB, there are few explicit reply relations. Fortunately, using the SMSS model and the Algorithm 1, we can approximately recover the implicit reply structure in a thread, and consequently construct a reply network G as:</p><formula xml:id="formula_16">G = (N, E), E = {ei,j, ∀i, j, user ni replies user nj} (6)</formula><p>where N is a set of nodes of G, each node corresponds to a user in the forum. E is the set of directed edges of G. The weight of the edge is the number of posts replied to this user.</p><p>Inspired by <ref type="bibr" target="#b21">[22]</ref>, we employ the HITS <ref type="bibr" target="#b12">[13]</ref> algorithm as a straightforward method to rank the users. Through this task, we try to demonstrate the capability of the reconstructed reply relations from SMSS model. The experimental results show that the performance of our method is even better than that based on the original reply relations. We will explain it detailedly in experiment part. It suggests that this method can be extended to more web communities without explicit reply structures. We will give more detailed explanation in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we present experimental results of our model in three applications. We implemented all algorithms in C# and all experiments have been executed on a server with an AMD Opteron Processor 280 2.40 GHz (4 cores) and main memory 8G bytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Set</head><p>We use two forums, Apple Discussion (discussions.apple.com) and Slashdot (www.slashdot.org), as our data sources. Apple discussion center is designed for Apple fans to post questions of Apple products and discussions of Apple new trends. Slashdot is a forum for developers, game fans, and all kinds of users to freely comment on recent technical news and events. These two forums are carefully selected because of the following reasons:</p><p>• These two forums provide explicit reply relations, which can be used as the ground truth to evaluate the performance of reply reconstruction.</p><p>• These two forums have reliable judgements for post qualities. In Slashdot, the moderators mark each post according to its quality. The score ranges from −1 ∼ 5, where higher score refers to informative and insightful posts. In Apple, users post questions, and mark helpful and correct replies in which their questions are solved.</p><p>• These two forums provide clear topic categories. In Apple, each thread belongs to one board of a specific topic. In Slashdot, most threads are tagged by users with topic phrases. We selected the largest 5 topics from each forums and filtered out unqualified threads, such as short threads which contain only one post, un-rated threads, and non-tagged threads in Slashdot. The statistic results are shown in <ref type="table">Ta</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reply Reconstruction</head><p>First, we evaluate the performance of reply reconstruction. We manually wrote a wrapper to parse pages and extract exact reply relations as the ground truth. The threshold used in Algorithm 1 was tuned based on a small data set, as ρ = 0.4, and it performed stably well for both Apple Dicussion and Slashdot. Although there are some reply hints in the titles in Slashdot, to show the generalization ability of our method, we did not use this kind of information in reply reconstruction. The evaluation metric is precision. We did not measure recall since we assigned the reply relations for all posts.</p><p>For comparison, we adopted some naive methods such as Nearest-Previous (NP), Reply-Root (RR), and Only Document Similarity (DS). NP assigns each post to its nearest previous post as the reply target; RR assigns each post to the root post as the reply target; and DS assigns each post to the post which has most similar terms. Moreover, we also compared our SMSS model with some state-of-the-art models which can provide semantic topic analysis, such as latent Dirichlet allocation (LDA) <ref type="bibr" target="#b2">[3]</ref> and the special words with background model (SWB) <ref type="bibr" target="#b4">[5]</ref>. We computed the post similarity by sim(i, j) = sim(</p><formula xml:id="formula_17">d (i) , d (j) ) + w1 · sim( θ (i) LDA , θ<label>(j)</label></formula><p>LDA ) from LDA. SWB is an extension of LDA, by allowing words in documents to be modeled either from general topics, or from post-specific "special" word distributions, or from a thread-wide background distribution. We leverage the topic distributions θ for similarity computing, as sim(i, j) = sim(</p><formula xml:id="formula_18">d (i) , d (j) ) + w 1 · sim( θ (i) SW B , θ (j) SW B ) + w 2 · sim( ψ (i) SW B , ψ<label>(j)</label></formula><p>SW B ). In the experiments, the combination weights wi for SMSS, LDA, and SWB were tuned based on a small set of data (about 60 threads from Slashdot and Apple, respectively). We only plot the performance trends of different parameters  for Slashdot in <ref type="figure" target="#fig_2">Figure 2</ref> due to space limitation. The evaluation was conducted based on the rest part of the data. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. From <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="table" target="#tab_1">Table 2</ref>, we have four observations: (I) in Slashdot, a certain number of posts reply to their thread root, few posts reply to their nearest previous posts; while in Apple Discussion, there are almost equal number of posts replying to the nearest previous neighbors and the thread roots. This is caused by different discussion styles of the two forums. Discussion threads in Apple Discussion follow a Question-Answer style. New solutions and fresh questions in replies invoke a series of discussions. However, threads in Slashdot are usually initialized by a piece of news. Interesting aspects of the news and brilliant replies arise branches of discussions. (II) The best parameters of topic similarity of SWB and LDA are both w1 = 0.1 and the improvements are very small comparing with baseline DS. The best parameter of topic similarity of SMSS is w1 = 1.2. Thus the topics of SMSS are more capable of characterizing reply relations. (III) In our experiments, SWB achieves the best performance when w 2 is very small and different w 2 s have little effect to the performance. This is because posts in threaded discussions are usually short and it is very difficult to estimate a sound coefficient for document specific word distribution. (IV) SMSS demonstrates great improvement. A major difference between SMSS and other approaches is that SMSS reconstructs the structure representation b (i) for post pi in each discussion thread. Though the best parameter of structural similarity is w 2 = 0.1, the improvement is remarkable (from 0.538 to 0.580). This indicates that, besides semantic similarities, structure similarities are more distinguishing in identifying reply relations.</p><p>Furthermore, we analyze the performance on posts with different qualities. We chose good posts whose scores are larger than 3 in Slashdot and posts marked as "Helpful" or "Solved" in Apple Discussion (about 10% among all posts). The similarity-based methods have better performance for those posts with high quality. It does make sense since posts with high quality may cause more significant replies. We will show its benefit in the following expert finding application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Junk Identification</head><p>In this subsection, we evaluate the performance of junk identification. We only used the threads from Slashdot in this task because useless posts were not explicitly annotated in Apple Discussion. To make the results more comparable among different methods, we only selected threads with more than 60 posts in the experiment. We set the posts whose score are -1 or 0 as junk posts in the ground truth. We use precision, recall, and F-Measure to measure the performance.</p><p>An intuitive idea to identify junk post is based on the statistical information of common words, as: η(p i ) = w df (w)/|p i |, where |p i | is the length of document, w is a word in p i , df (w) is the document frequency of word w in the corpus. One post is marked as a junk post if η(p i ) is larger than the average η in the corpus. We call this intuitive method DF and use it as the baseline. The SWB model also integrates background into topics, so we use it as another comparative method. To detect junk posts, we follow (5) where the θ</p><formula xml:id="formula_19">(i)</formula><p>bg is produced by SWB. Moreover, as junk identification can be regarded as a binary classification task, we also compared our model with SVM. For SVM training, We selected 2000 posts (no overlap with test data) as training set, in which posts with score 0 and -1 are positive samples, and posts with score 4 and 5 are negative samples. Features for SVM are terms with tf-idf weighting.</p><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, SMSS outperforms the other comparative methods. The reasons are listed below: (I) Junk posts are thread-dependent. There are few common junk words across different threads. Hence SVM's performance is not good. (II) SMSS is built on the statistical information of DF. But SMSS puts structural constraints during the process of projecting posts into topic space. Hence SMSS outperforms DF in terms of precision, recall and F-Measure. (III) SWB learns background that fits the corpus and thus achieves high precision. However, it does not have structural constraints. When a post can not be reconstructed well from previous posts, SMSS assigns smaller coefficients to topics, resulting in a larger ratio of p junk . Thus SMSS can detect more outlier posts, and obtain higher recall and F-Measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Expert Finding</head><p>In the subsection, we evaluate the performance of expert search. Since there is no explicit user-supplied expertise ranking data in the two forums, we need to generate a "gold standard" as the ground truth for evaluation. It is impractical to manually rate a large number of these users, in the experiments we followed the sophisticated standard from the Internet Movie Database (IMDB) 3 . In this standard, the formula for calculating each user's rating score gives a true Bayesian estimate:</p><formula xml:id="formula_20">Rating = n n + m · S + m n + m · AvgS (7)</formula><p>where n is the number of posts of a user and S is the average score of the user's posts; m is the threshold of the minimum posts for a user to be an expert candidate, and AvgS is the mean score of the posts from all the users. The advantage of this estimation formula is that it balances the bias and uncertainty of users with less posts. For users with less posts, the rating is more likely to be pushed toward the mean score AvgS; for users with more posts, the rating is more likely to be the true average score of the user's own posts. In the experiment, we obtained the top 100 experts according to the rating. Moreover, the expert ranking was carried out independently for each boards/topics, since some experts may only focus on a small number of fields. Accordingly, the m and AvgS in (7) are selected for each topic respectively. The experiments were done in Apple Discussion and Slashdot respectively. We calculated the average performance for each method.</p><p>To analyze the reply network and find out experts, we employed three typical structural models: HITS <ref type="bibr" target="#b12">[13]</ref>, PageRank <ref type="bibr" target="#b15">[16]</ref>, and EABIF <ref type="bibr" target="#b19">[20]</ref>. HITS and PageRank have been widely adopted in measuring node quality in networks; while EABIF adopts an adjusted mechanism of PageRank in the influence network. EABIF assumes that experts diffuse information to others, and long path of propagation will cause information lost. The decreasing factor is β N −1 /(N − 1)! where N is the length of propagation path and β = 2. To demonstrate the effectiveness of the re-constructed reply structures from SMSS, we implemented all structural models on both the reply network in ground truth ("Original" in <ref type="table" target="#tab_3">Table 4</ref>) and the network re-constructed by SMSS ("Reconstructed" in <ref type="table" target="#tab_3">Table 4</ref>).</p><p>Moreover, we choose one state-of-the-art language model (LM) in <ref type="bibr" target="#b0">[1]</ref> (we use the model 2, document model, since it outperforms model 1 in nearly all situations in <ref type="bibr" target="#b0">[1]</ref>) for comparison. Here, each post is treated as a document without structure. LM estimates the probability of one candidate ca being an expert in the topic t in a collection of posts. where P ca is the post written by ca, q is a word in the topic, the smooth parameter λ d = β/(β + n(d)), λ d is proportional to the post length n(d), and β is the average post length int the data set. Given the expert ground truth <ref type="formula">(7)</ref>, the evaluation metrics, including Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), and Precision at top 10 results(P@10), are calculated for each model/method on each topic; and the average performances are shown in <ref type="table" target="#tab_3">Table 4</ref>. We have the following observations: (I) LM gives a relatively high performance but it is still not good enough. This is because in threaded discussions, some posts are highly relevant to the given topic but doesn't contain insightful information, e.g. posts describing naive questions. LM cannot distinguish such posts since it only measures expertise by the number of relevant posts. (II) The performance of HITS is significantly better than PageRank and EABIF. In our opinion, it is because of the different content qualities of posts and their replies. In HITS, these two scores are treated separately (hub/authority scores) while PageRank and EABIF treat them equally. (III) Structural models perform consistently better on reconstructed reply network than the original network in terms of MRR and P@10; and perform worse on reconstructed reply network than on original network in terms of MAP. MRR and P@10 measure precision on the top results while MAP measures average precision. The reply reconstruction algorithm has higher performance in identifying replies to high quality posts (as shown in Section 5.2). Algorithm 1 filters out noisy data, prune faint replies. Thus those users receiving more significant replies will be pushed toward a higher position, which leads to a better performance ( and higher confidence) in the top results. For a real experts finding task, we argue that the top results are more important, because people usually do not have the patience to browse through the whole returned list.</p><formula xml:id="formula_21">p(ca|t) = d∈P ca q∈t {(1 − λ d )p(q|d) + λ d p(q)} n(q,t) × p(d|ca) 3 http://www.imdb.com/chart/top</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Threaded discussions are valuable data sources with lots of human knowledge in various domains. In this paper, we have presented a sparse coding-based model, to simultaneously representing semantics and structure of threaded discussions. By adding sparse constraints that each post is generated from only a few topics and each reply is related to only a few previous posts, the proposed SMSS model has advantages in three perspectives: (I) it can characterize mutual information between semantics and structure in discussion threads by modeling them simultaneously; (II) it can help identify junk posts more accurately to avoid their disturbance in content analysis; and (III) it can help find experts in a given board/sub-board (topic). We demonstrated the competency of SMSS with these three applications. The results show promising performance in various situations.</p><p>Although the results are encouraging, there is still room for further improvements. The method of identifying junk posts is straightforward, we will try other approaches in the future. We will also try to explain this model in a probabilistic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The first author is partially supported by Shanghai Leading Academic Discipline Project, Project number: B114.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>- ble 1, from which we can conclude that the two forums differ with respect to several attributes. Apple threads are shorter "Question &amp; Answer" style discussions, with average 17.84 posts, Slashdot threads are longer chatting-style communi- cations, with an average length of 176.09 posts. Slashdot users are more active with 15.32 posts per user while in Ap- ple there are about 4.69 posts per user. However, posts in threaded discussions are very terse with about 70 words on average. The following experiments are done without stem- ming and filtering stopwords.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of reply reconstruction when using different parameters of SMSS, LDA, and SWB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Descriptive statistics of data sets 
Data 
Slashdot Apple 
Number of threads 
1154 
4486 
Number of posts 
203210 
80008 
Average thread length 
176.09 
17.84 
Average words per post 
73.53 
78.36 
Number of topics 
5 
5 
Average posts per user 
15.32 
4.69 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of reply reconstruction in all 
posts v.s. high-quality posts 

Method 
Slashdot 
Apple 
All Posts Good Posts All posts Good Posts 
NP 
0.021 
0.012 
0.289 
0.239 
RR 
0.183 
0.319 
0.269 
0.474 
DS 
0.463 
0.643 
0.409 
0.628 
LDA 
0.465 
0.644 
0.410 
0.648 
SWB 
0.463 
0.644 
0.410 
0.641 
SMSS 
0.524 
0.737 
0.517 
0.772 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Performance of junk identification</head><label>3</label><figDesc></figDesc><table>Method Prec. Recall F-Measure 
SWB 
0.48 
0.22 
0.30 
SVM 
0.37 
0.24 
0.20 
DF 
0.34 
0.40 
0.36 
SMSS 
0.38 
0.45 
0.41 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Performance of expert finding</head><label>4</label><figDesc></figDesc><table>Method 
MRR. MAP 
P@10 
LM 
0.821 
0.698 
0.800 
EABIF (Original) 
0.674 
0.362 
0.243 
EABIF (Reconstructed) 
0.742 
0.318 
0.281 
PageRank (Original) 
0.675 
0.377 
0.263 
PageRank (Reconstructed) 
0.743 
0.321 
0.266 
HITS (Original) 
0.906 
0.832 
0.900 
HITS (Reconstructed) 
0.938 
0.822 
0.906 

</table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>In this section, we give an approximate solution to the sparse coding model. First we write the objective function in equation 1 in the matrix form. In the scope of appendix, we use θ (i) to denote the i th column vector in Θ, x (i) to denote the i th row vector in X, b (i) to denote the corresponding column vector for the i th post, and b</p><p>to denote the coefficient of previous post j to current post i. Then we rewrite (1) as:</p><p>where θ (&lt;i) is a T × i matrix indicating the first i columns in Θ. When Θ and X are fixed, for each b (i) :</p><p>When b and X are fixed, for each</p><p>, we have:</p><p>Let ∂f ∂ θ (i) = 0, and</p><p>We have:</p><p>When Θ and all b are fixed, suppose D is the term matrix of all M threads, we can optimize X by:</p><p>At the beginning, we assign a random initial value to X and normalize the matrix X, and then we repeat the optimization loop for a fix round c. In each round, we first optimize b in all threads by <ref type="bibr" target="#b7">(8)</ref>, Θ in all threads by (9), and X by (10) in sequence and then normalize X.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A language modeling framework for expert finding. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applying discrete PCA in data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modeling general and specific aspects of documents with a probabilistic topic model. Advances in newral information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding question-answer pairs from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-I</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31 st SIGIR</title>
		<meeting>31 st SIGIR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using conditional random fields to extract contexts and answers of questions from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11 th ACL</title>
		<meeting>11 th ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="710" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building implicit links from content for forum search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29 th SIGIR</title>
		<meeting>29 th SIGIR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29 th SIGIR</title>
		<meeting>29 th SIGIR</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting chatbot knowledge from online discussion forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11 th IJCAI</title>
		<meeting>11 th IJCAI</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="423" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Social Network Analysis: A Handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Sage Publications</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic segmentation of message hierarchies for indexing and navigation support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Donderler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16 th WWW</title>
		<meeting>16 th WWW</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic and role discovery in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corrada-Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="249" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blocking blog spam with language model disagreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AIRWeb</title>
		<meeting>of AIRWeb</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to classify short and sparse text &amp; web with hidden topics from large-scale data collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The author-topic model for authors and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thread detection in dynamic text message streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29 th SIGIR</title>
		<meeting>29 th SIGIR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personalized recommendation driven by information flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29 th SIGIR</title>
		<meeting>29 th SIGIR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Continuous time dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Expertise networks in online communities: structure and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
