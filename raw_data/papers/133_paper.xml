<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WEB CONTENT EXTRACTION THROUGH HISTOGRAM CLUSTERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kansas State University Kansas State University Manhattan</orgName>
								<address>
									<addrLine>218 Nichols Hall 231 Nichols Hall</addrLine>
									<postCode>66506, 66506</postCode>
									<settlement>Manhattan</settlement>
									<region>KS, KS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kansas State University Kansas State University Manhattan</orgName>
								<address>
									<addrLine>218 Nichols Hall 231 Nichols Hall</addrLine>
									<postCode>66506, 66506</postCode>
									<settlement>Manhattan</settlement>
									<region>KS, KS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WEB CONTENT EXTRACTION THROUGH HISTOGRAM CLUSTERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe a method to extract content text from diverse Web pages by using the HTML document&apos;s Text-To-Tag Ratio (TTR) rather than specific HTML cues that are not constant across various Web pages. We describe how to compute the TTR on a line-by-line basis and then cluster the results into content and non-content areas. The resulting TTR-histogram is not easily clustered because of its one dimensionality; therefore we present a technique to better represent the histogram in two-dimensions. Next, we compare clustering techniques such as EM, K-Means, and Farthest First-in density and distance modes-with a threshold partitioning technique on the resulting two-dimensional data. These clustering techniques are also enhanced with the use of histogram smoothing techniques. We then evaluate our approach using standard accuracy, precision and recall metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The amount of information being gathered and stored on the Internet continues to increase. The artifacts of this growing market provide interesting new research opportunities that explore social interactions, language, art, mathematics, etc. Many of these new research opportunities require the content of the Internet to be gathered, processed and stored quickly and efficiently. This effort is often hampered by the use of structure tags in HTML and XML. These tags are meaningful only to the browser that renders the document, but bear little semantic meaning to the end user. Tags and other non-content related HTML characters -images not included -comprise the majority of each page's size <ref type="bibr">(Lu, et al. 2004)</ref>, and yet, Internet researchers are forced to crawl, compute and store web content in their entirety.</p><p>This work focuses on extracting content from Web pages that are otherwise laden with structural data, links and advertisements, commonly called Text Extraction <ref type="bibr">(Soderland 1997)</ref>. This work is particularly challenging because of the difficulty in determining which part of a web page is meaningful and which part is not.</p><p>In this paper, we extend our previous work on Web content extraction with the use of the Text-To-Tag Ratio (TTR). The TTR approach to Web content extraction makes no assumptions about the particular structure of a given Web page, nor does it look for particular cues such as specific HTML tags, etc. as previous research does. The only necessary pre-condition of a page's structure is that it has some structure. With this in mind, we construct a TTR-array with the contention that for each line k in the array, the higher the TTR is for the element k relative to the mean TTR of the entire array the more likely that k represents a line of content-text within the HTML document.</p><p>In this and in previous work <ref type="bibr" target="#b7">(Weninger et al. 2008</ref>), we observe that the TTR-array closely resembles a histogram, in that each histogram bucket represents the TTR of a line in an HTML document. By that observation this paper presents Web content extraction as a histogram clustering task. Histogram clustering is a widely researched topic that is especially popular with image researchers. This is especially true among researchers who wish to use the histogram footprints of images as a means for classification, segmentation, etc. <ref type="bibr" target="#b4">(Puzicha et. al. 1999</ref>) ( <ref type="bibr" target="#b5">Sezgin et al. 2004</ref>). However, this research is largely inapplicable because of the dimensionality of images is inherently 2D, whereas <ref type="figure" target="#fig_0">Figure 1</ref> clearly shows that the TTR histogram can only originally be represented in a single dimension.</p><p>As an example, consider the news article from The Hutchinson News 1 that appeared on Wednesday, <ref type="bibr">March 19, 2008</ref>. This Web page is similar to many pages on the Web. The title banner, hyperlinks and advertisements take up most of the space on the webpage while the content of the page is confined to a relatively small space in the middle. At the bottom of the page more advertisements and images are displayed along with links to copyright and other administrative information. This paper will compare the performance of threshold partitioning to that of density and distance-based clustering techniques at the task of extracting content-text from a diverse set of Web pages. For this particular domain our empirical goal is to maximize recall because we believe that the extraction of errant content is less detrimental than the exclusion of actual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THRESHOLD PARTITIONING</head><p>In this section we describe the threshold partitioning technique. For our purposes we consider threshold partitioning to be our baseline because of the ideal results gained from our previous research. Before the threshold is applied a smoothing pass is made on the histogram. This is done because without smoothing many important content lines might be lost. In our Web content domain, these lost content lines may include the page title, a news article byline or dateline, short or one sentence paragraphs, etc. where the TTR would fall below that of the standard deviation. As a pathological example, consider a <ref type="bibr">1</ref> The Hutchinson News is available online at http://hutchnews.com. The specific article is not permanently linked. To resolve this problem we apply a Gaussian smoothing pass to h. Standard Gaussian smoothing algorithms (Weisstein 2008) are generally implemented for image processing and are continuous, and thus do not suit our purposes. Therefore the algorithm used in this approach was re-implemented as a discrete function operating in a single dimension. Equation 1 shows the construction of a Gaussian kernel (k) with a width of 2񮽙񮽙񮽙񮽙񮽙 񮽙 1.</p><formula xml:id="formula_0">񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 , 0 2񮽙񮽙񮽙񮽙񮽙. (Eq. 1)</formula><p>The size of and values within k vary according to σ because as the variance of h increases, smoothing necessity also increases. Next, Equation 2 shows that k is normalized to form kʹ.</p><formula xml:id="formula_1">񮽙ʹ 񮽙 񮽙 񮽙 񮽙 ∑ 񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 , 0 2񮽙񮽙񮽙񮽙񮽙. (Eq. 2)</formula><p>Finally, Equation 3 shows that kʹ is convolved with h in order to form a smoothed histogram (hʹ).</p><formula xml:id="formula_2">񮽙ʹ 񮽙 ʹ 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙 , 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙. (Eq. 3)</formula><p>Compared to <ref type="figure">Figure 2</ref>, hʹ, shown in <ref type="figure" target="#fig_4">Figure 3</ref>, is better suited for clustering because of the increased cohesiveness within sections and strict differences between sections. Furthermore, hʹ has a lower standard deviation (40.55 TTR) because outlying peaks and valleys are smoothed. Similarly, outliers, such as advertisements, that may occupy a single high-TTR line among many low-TTR lines, are smoothed to below the threshold.</p><p>Finally, let C be the set of content lines such that</p><formula xml:id="formula_3">d i 񮽙 C iff hʹ i ≥ σʹ. Note d i 񮽙 hʹ i .</formula><p>After elements of C are selected, each content-line is stripped of all remaining HTML tags -usually paragraph and anchor tags. Then the cleaned lines are combined and output to a file for storage, indexing, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HISTOGRAM CLUSTERING IN 2-DIMENSIONS</head><p>This section presents a density and distance-based approach to clustering 1-dimensional histograms. Specifically, in previous work we observed that when clustering algorithms are applied to 1D data, such as a Text-to-Tag Ratio (TTR) histogram (񮽙), results are consistently inaccurate. We contend that by transforming the histogram data so that it may be represented in 2-dimensions we can obtain more accurate results.</p><p>For this task, we define the two dimensions to be (1) a smoothed TTR histogram (񮽙ʹ), and (2) a derivative array of the computed from 񮽙ʹ (񮽙ʹ). These definitions came about strictly through observations and trial-and-error experimentation.  To compute 񮽙ʹ, first smooth 񮽙 in the same manner as described in Eq. 1-3 to get 񮽙ʹ. Next, find the derivatives for each element in the array; specifically, we subtract 񮽙ʹ 񮽙 from the mean of the next three elements in order to differentiate on the moving average (񮽙) instead of line-by-line as shown in Equation <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line Number</head><p>The smoothed difference array (񮽙ʹ) shows two peaks: the first at the beginning of a content section and a second at the end of a content section with relatively higher values in between. Of course, histograms can have non-continuous content sections, and in such cases an appropriate number of peaks are displayed.  Finally, we observed that when 񮽙ʹ and 񮽙ʹ are combined as conjoining dimensions ideal clustering properties emerge as shown on left in <ref type="figure" target="#fig_7">Figure 5</ref>. As illustrated on right in <ref type="figure" target="#fig_7">Figure 5</ref>., when we manually identified each point to be either content (○) or non-content (×) we observed that the dense collection of points near the origin were non-content lines and the remaining points were content lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTATION AND RESULTS</head><p>To test the effectiveness of both the threshold partitioning and clustering approaches documented in the above sections, 176 complete Web pages were downloaded by searching for the keyword "the" from Yahoo's search engine and harvesting the results. The goal of our experiments was to determine the content data of the Web pages and filter out all extraneous advertisements and site links. We determined the actual content of each Web page by opening each downloaded file in a browser and manually selecting  To evaluate the results of our experiments we used standard accuracy, precision and recall metrics where true positives are content lines that were correctly identified, true negatives are non-content lines that were correct identified, false positives are noncontent lines that were incorrectly identified as content, and false negatives are content lines that were incorrectly identified as non-content. Diff comparisons are made on a word-by-word basis between the automatically extracted content text and the manually extracted content text to determine the true positives, etc. Contrary to previously used metrics in <ref type="bibr" target="#b7">(Weninger et al. 2008</ref>) a single errant character is not prohibitively detrimental to the final result.</p><p>Initially, we tested threshold partitioning by setting the threshold at 1 standard deviation as shown in <ref type="table" target="#tab_1">Table 1</ref>. Secondly, we generated an ROC curve by applying a coefficient ranging from 0 to 19 to the threshold as shown in <ref type="figure" target="#fig_8">Figure 6</ref>. We tested density and distance clustering techniques by transforming the data into 2-dimensions as described in the previous section and then by running Farthest First (Hochbaum 1985), K-Means (MacQueen 1996) and EM <ref type="bibr" target="#b1">(Dempster et al. 1977)</ref> algorithms in distance and density <ref type="bibr" target="#b2">(Ester et al. 1996</ref>) modes with 3 clusters. With 3-clusters the non-content label is given to the cluster with the centroid closest to the origin and the remaining two clusters are labeled content. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>. For our text extraction purposes we wish to maximize recall. That is, it is far more detrimental to exclude content text than it is to include non-content text. Even so, some recall can be sacrificed to make sufficiently large gains in precision. Therefore we declare Density-Farthest First (DFF) to be the winner. Furthermore, DFF is comparable to threshold partitioning at 1σ in terms of recall (98.99% -99.51%), but DFF is substantially better in terms of precision (72.10% -61.06%). Thus, for our purposes, we declare DFF to be the overall winner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed two approaches to clustering histogram data. We showed that threshold partitioning, although simple, can be used to segment histogram data with a high degree of recall at all levels of sensitivity. Also, we showed that by generating a second dimension to the histogram via smoothed derivatives we can use standard clustering techniques to achieve high recall and precision. Finally, by comparing the results of our experiments we observed that the Farthest First clustering algorithm in density mode is best suited for extracting content areas from Web pages in this paradigm. In future work we plan to experiment with edge detection algorithms on variations on the Text-to-Tag Ratio Histogram. We also wish to empirically compare this approach with other methods of text extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 . Text-to-tag ratio histogram of a Web page from The Hutchinson News.</head><label>1</label><figDesc>Figure 1. Text-to-tag ratio histogram of a Web page from The Hutchinson News. Spikes between lines 220 and 260 represent content-text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Text To Tag Ratio Line Number Web page (d) containing the American Declaration of Independence 2 and a corresponding TTR histogram (h). h contains TTR-spikes corresponding to the relatively long preamble and proclamation sections. However, many of the abuses of the king are listed in short, one sentence phrases, and relative to the rest of the document their TTRs line below the 1σ threshold and would therefore be errantly excluded as shown on left in Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . Original/Unsmoothed Text-to-tag ratio for an American Declaration of Independence Web page (d). Horizontal line denotes the standard deviation threshold. (σ = 64 . 49 TTR). Content lines are 29- 65 inclusive.</head><label>2644965</label><figDesc>Figure 2. Original/Unsmoothed Text-to-tag ratio for an American Declaration of Independence Web page (d). Horizontal line denotes the standard deviation threshold. (σ = 64.49 TTR). Content lines are 29-65 inclusive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 . Gaussian smoothed Text-to-tag ratio for data from Figure 2 . Horizontal line denotes the standard deviation threshold. (σʹ = 40 . 55 TTR).</head><label>324055</label><figDesc>Figure 3. Gaussian smoothed Text-to-tag ratio for data from Figure 2. Horizontal line denotes the standard deviation threshold. (σʹ = 40.55 TTR). Content lines are 29-65 inclusive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. TTR derivatives (񮽙 񮽙) computed with Equation 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 . Difference histogram for corresponding TTR histogram (񮽙ʹ) from Figure 1 .</head><label>41</label><figDesc>Figure 4. Difference histogram for corresponding TTR histogram (񮽙ʹ) from Figure 1. The two large peaks represent changes between content and non-content sections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Number the content text. The text was copied into a new file and is used for comparison evaluation later.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 . On left, a scatter plot combining 񮽙ʹ and ʹ. On right, a scatter plot with the same data as the graph on left with each point manually labeled to be content (○) or non-content (×).</head><label>5</label><figDesc>Figure 5. On left, a scatter plot combining 񮽙ʹ and ʹ. On right, a scatter plot with the same data as the graph on left with each point manually labeled to be content (○) or non-content (×).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. ROC curve for threshold partitioning. Threshold ranges from 0-19σ. AUC is 98.74%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>get 񮽙 񮽙. Finally, we compute 񮽙ʹ 񮽙 񮽙 |񮽙 񮽙 񮽙 |, for all 񮽙 in 񮽙 񮽙.</head><label></label><figDesc>. Note: all experiments presented in this paper use 3.</figDesc><table>񮽙ʹ񮽙񮽙ʹ 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 
∑ 񮽙ʹ 

񮽙 


񮽙 
񮽙 񮽙ʹ 񮽙 , 0 ʹ 
(Eq. 4) 

Note that ʹ񮽙; rather because 񮽙 essentially is an array of differences 
ʹ -1. Next, we Gaussian-smooth 񮽙 in the same manner described in 
Eq. 1-3 to 0 

100 

200 

300 

400 

500 

1 
11 
21 
31 
41 
51 
61 
71 
81 
91 
101 
Text To Tag Ratio (Original) 

Line Number 

0 

50 

100 

150 

200 

250 

1 
11 
21 
31 
41 
51 
61 
71 
81 
91 
101 
Text To Tag Ratio (Smooth) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Results for threshold partitioning on 176 with the threshold at 1σ. 

Precision 
Recall 

Mean 
55.97% 
94.49% 

Median 
61.06% 
99.51% 

Std Dev. 
34.65% 
17.42% 

Num 100% 
2 
75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Results for Farthest First, K-Means, EM clustering methods in distance and 
density modes with exactly 3 clusters (1 non-content cluster, 2 content clusters). 

Accuracy 
Precision 
Recall 

Median 
Mean 
Median 
Mean 
Median 
Mean 

Density 
Farthest First 
84.96% 
77.87% 
72.10% 
60.31% 
98.99% 
93.91% 
K-Means 
78.39% 
74.83% 
58.71% 
52.55% 
99.66% 
95.82% 
EM 
72.49% 
69.75% 
45.70% 
44.62% 
100.00% 
93.09% 

Distance 
Farthest First 
79.52% 
72.33% 
94.12% 
73.53% 
90.18% 
80.50% 
K-Means 
81.78% 
76.90% 
67.49% 
59.68% 
98.80% 
92.76% 
EM 
77.98% 
74.61% 
57.80% 
52.29% 
99.67% 
95.79% 

</table></figure>

			<note place="foot" n="2"> The copy of the American Declaration of Independence used in this paper is available online at http://www.ushistory.org/ declaration/document/index.htm.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research was supported in part by the Defense Intelligence Agency. We thank Dr. Dan Andresen, Dr. David Gustafson and Dr. Doina Caragea for their insight and useful comments, and Daniel Jones, John Drouhard, Imran Hameed and Jack Hart for their assistance with this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Best Possible Heuristic for the KCenter Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operational Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="184" />
			<date type="published" when="1985-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2 nd ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (SIGKDD&apos;96)</title>
		<meeting>the 2 nd ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (SIGKDD&apos;96)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Some Methods for classification and Analysis of Multivariate Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proeedings of 5 th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histogram clustering for unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2 nd International Conference on Computer Vision and Pattern Recognition (CVPR&apos;99)</title>
		<meeting>the 2 nd International Conference on Computer Vision and Pattern Recognition (CVPR&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="602" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Survey over image thresholding techniques and quantitative performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sezgin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gaussian Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<ptr target="http://mathworld.wolfram.com/GaussianFunction.html" />
	</analytic>
	<monogr>
		<title level="m">From MathWorld -A Wolfram Web Resource</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text Extraction from the Web via Text-to-Tag Ratio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear in Proceedings of the 19th International Conference on Database and Expert Systems Applications (DEXA&apos;08) Workshop on Text-Based Information Retrieval (TIR&apos;08)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
