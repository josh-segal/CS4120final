<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Study of Poisson Query Generation Model for Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
							<email>hfang@uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign Urbana</orgName>
								<address>
									<postCode>61801</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Study of Poisson Query Generation Model for Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H33 [Informa- tion Search and Retrieval]: Retrieval Models General Terms: Algorithms Keywords: Language models</term>
					<term>Poisson process</term>
					<term>query gen- eration</term>
					<term>formal models</term>
					<term>term dependent smoothing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson distribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave differently for many smoothing methods. We show that the Poisson model has several advantages over the multinomial model, including naturally accommodating per-term smoothing and allowing for more accurate background modeling. We present several variants of the new model corresponding to different smoothing methods , and evaluate them on four representative TREC test collections. The results show that while their basic models perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The performance can be further improved with two-stage smoothing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>. Among many variants of language models proposed, the most popular and fundamental one is the query-generation language model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>, which leads to the query-likelihood scoring method for ranking documents. In such a model, given a query q and a document d, we compute the likelihood of "generating" query q with a model estimated based on document d, i.e., the conditional probPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'07, July <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr">2007</ref>, Amsterdam, The Netherlands. Copyright 2007 ACM 978-1-59593-597-7/07/0007 ...$5.00.</p><p>ability p(q|d). We can then rank documents based on the likelihood of generating the query.</p><p>Virtually all the existing query generation language models are based on either multinomial distribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref> or multivariate Bernoulli distribution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>. The multinomial distribution is especially popular and also shown to be quite effective. The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a natural choice for modeling the occurrence of a particular word in a particular position in text. Compared with multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms. However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per-term smoothing and weighting. Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.</p><p>In this paper, we propose and study a new family of query generation models based on the Poisson distribution. In this new family of models, we model the frequency of each term independently with a Poisson distribution. To score a document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the likelihood of the query given by the estimated Poisson model. In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing. Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.</p><p>As in the existing work on multinomial language models, smoothing is critical for this new family of models. We derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on multinomial distributions. We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing. In particular, a key difference is that the Poisson model can naturally accommodate perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a generative model. We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model. This advantage is seen for both one-stage and two-stage smoothing. Another potential advantage of the Poisson model is that its corresponding background model for smoothing can be improved through using a mixture model that has a closed form formula. This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.</p><p>The rest of the paper is organized as follows. In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing methods which lead to different retrieval functions. In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of retrieval. We then design empirical experiments to compare the two families of language models in Section 4. We discuss the related work in 5 and conclude in 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">QUERY GENERATION WITH POISSON PROCESS</head><p>In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document. In most existing work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, people assume that each query word is sampled independently from a multinomial distribution. Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Generation Process</head><p>Let V = {w1, ..., wn} be a vocabulary set. Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document. We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.</p><p>Suppose t is the time period during which the author composed the text. With a homogeneous Poisson process, the frequency count of each event, i.e., the number of occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time. The probability density function of such a Poisson Distribution is given by P (c(wi, w) = k|λit) = e −λ i t (λit) k k! Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.</p><p>With n such independent Poisson processes, each explaining the generation of one term in the vocabulary, the likelihood of w to be generated from such Poisson processes can be written as</p><formula xml:id="formula_0">p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e −λ i ·|w| (λi · |w|) c(w i ,w) c(wi, w)!</formula><p>where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w). We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.</p><p>Let D = {d1, ..., dm} be an observed set of document samples generated from the Poisson process above. The maximum likelihood estimate (MLE) of λi is</p><formula xml:id="formula_1">ˆ λi = d∈D c(wi, d) d∈D</formula><p>w ∈V c(w , d) Note that this MLE is different from the MLE for the Poisson distribution without considering the document lengths, which appears in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Given a document d, we may estimate a Poisson language model Λ d using d as a sample. The likelihood that a query q is generated from the document language model Λ d can be written as</p><formula xml:id="formula_2">p(q|d) = w∈V p(c(w, q)|Λ d ) (1)</formula><p>This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term. In practice, we have the flexibility to choose the vocabulary V . In one extreme, we can use the vocabulary of the whole collection. However, this may bring in noise and considerable computational cost. In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the nonquery terms. As a compromise, we may conflate all the non-query terms as one single pseudo term. In other words, we may assume that there is exactly one "non-query term" in the vocabulary for each query. In our experiments, we adopt this "pseudo non-query term" strategy.</p><p>A document can be scored with the likelihood in Equation 1. However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero probability to the term, causing the probability of the query to be zero. As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Smoothing in Poisson Retrieval Model</head><p>In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. In general, we have to discount the probabilities of some words seen in the text to leave some extra probability mass to assign to the unseen words. In Poisson language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1). Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word. Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1. In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Bayesian Smoothing using Gamma Prior</head><p>Following the risk minimization framework in <ref type="bibr" target="#b10">[11]</ref>, we assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e.,</p><formula xml:id="formula_3">Λ d = λ d,1 , ..., λ d,|V | .</formula><p>A document is assumed to be generated from a potentially different model. Given a particular document d, we want to estimate Λ d . The rate of a term is estimated independently of other terms. We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β:</p><formula xml:id="formula_4">Gamma(λ|α, β) = β α Γ(α) λ α−1 e −βλ</formula><p>For each term w, the parameters αw and βw are chosen to be αw = µ * λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the "collection language model". The posterior distribution of Λ d is given by</p><formula xml:id="formula_5">p(Λ d |d, C) ∝ w∈V e −λw (|d|+µ) λ c(w,d)+µλ C,w −1 w</formula><p>which is a product of |V | Gamma distributions with parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have</p><formula xml:id="formula_6">ˆ λ d,w = λ d,w λ d,w p(λ d,w |d, C)dλ d,w = c(w, d) + µλC,w |d| + µ</formula><p>This is precisely the smoothed estimate of multinomial language model with Dirichlet prior <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Interpolation (Jelinek-Mercer) Smoothing</head><p>Another straightforward method is to decompose the query generation model as a mixture of two component models. One is the document language model estimated with maximum likelihood estimator, and the other is a model estimated from the collection background, p(·|C), which assigns non-zero rate to w.</p><p>For example, we may use an interpolation coefficient between 0 and 1 (i.e., δ ∈ [0, 1]). With this simple interpolation, we can score a document with</p><formula xml:id="formula_7">Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2)</formula><p>Using the maximum likelihood estimator for p(·|d), we have</p><formula xml:id="formula_8">λ d,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e −λ d,w |q| (λ d,w |q|) c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e −λ d,w |q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e −λ d,w |q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C)</formula><p>We can also use a Poisson language model for p(·|C), or use some other frequency-based models. In the retrieval formula above, the first summation can be computed efficiently. The second summation can be actually treated as a document prior, which penalizes long documents.</p><p>As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo "non-queryterm", denoted as "N". Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as</p><formula xml:id="formula_9">Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e −λ d,w (λ d,w |q|) c(w,q) e −λ d,C |q| (λ d,C ) c(w,q) ) + log (1 − δ)e −λ d,N |q| + δe −λ C,N |q| 1 − δ + δe −λ C,N |q| (3)</formula><p>where</p><formula xml:id="formula_10">λ d,N = |d|− w∈q c(w,d) |d|</formula><p>and λC,N = |C|− w∈q c(w,C) |C| .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Two-Stage Smoothing</head><p>As discussed in <ref type="bibr" target="#b28">[29]</ref>, smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and <ref type="formula">(2)</ref> to explain the common terms in the query. In order to distinguish the content and non-discriminative words in a query, we follow <ref type="bibr" target="#b28">[29]</ref> and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λ d and the other being a query background language model p(·|U ). p(·|U ) models the "typical" term frequencies in the user's queries. We may then score each document with the query likelihood computed using the following two-stage smoothing model:</p><formula xml:id="formula_11">p(c(w, q)|Λ d , U ) = (1 − δ)p(c(w, q)|Λ d ) + δp(c(w, q)|U ) (4)</formula><p>where δ is a parameter, roughly indicating the amount of "noise" in q. This looks similar to the interpolation smoothing, except that p(·|Λ d ) now should be a smoothed language model, instead of the one estimated with MLE.</p><p>With no prior knowledge on p(·|U ), we could set it to p(·|C). Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.</p><p>The empirical study of the smoothing methods is presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ANALYSIS OF POISSON LANGUAGE MODEL</head><p>From the previous section, we notice that the Poisson language model has a strong connection to the multinomial language model. This is expected since they both belong to the exponential family <ref type="bibr" target="#b25">[26]</ref>. However, there are many differences when these two families of models are applied with different smoothing methods. From the perspective of retrieval, will these two language models perform equivalently? If not, which model provides more benefits to retrieval, or provides flexibility which could lead to potential benefits? In this section, we analytically discuss the retrieval features of the Poisson language models, by comparing their behavior with that of the multinomial language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Equivalence of Basic Models</head><p>Let us begin with the assumption that all the query terms appear in every document. Under this assumption, no smoothing is needed. A document can be scored by the log likelihood of the query with the maximum likelihood estimate:</p><formula xml:id="formula_12">Score(d, q) = w∈V log e −λ d,w |q| (λ d,w |q|) c(w,q) c(w, q)!<label>(5)</label></formula><p>Using the MLE, we have</p><formula xml:id="formula_13">λ d,w = c(w,d) w∈V c(w,d)</formula><p>. Thus</p><formula xml:id="formula_14">Score(d, q) ∝ c(w,q)&gt;0 c(w, q) log c(w, d) w∈V c(w, d)</formula><p>This is exactly the log likelihood of the query if the document language model is a multinomial with maximum likelihood estimate. Indeed, even with Gamma smoothing, when plugging</p><formula xml:id="formula_15">λ d,w = c(w,d)+µλ C,w |d|+µ</formula><p>and λC,w = c(w,C)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|C|</head><p>into Equation 5, it is easy to show that</p><formula xml:id="formula_16">Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6)</formula><p>which is exactly the Dirichlet retrieval formula in <ref type="bibr" target="#b27">[28]</ref>. Note that this equivalence holds only when the document length variation is modeled with Poisson process.</p><p>This derivation indicates the equivalence of the basic Poisson and multinomial language models for retrieval. With other smoothing strategies, however, the two models would be different. Nevertheless, with this equivalence in basic models, we could expect that the Poisson language model performs comparably to the multinomial language model in retrieval, if only simple smoothing is explored. Based on this equivalence analysis, one may ask, why we should pursue the Poisson language model. In the following sections, we show that despite the equivalence in their basic models, the Poisson language model brings in extra flexibility for exploring advanced techniques on various retrieval features, which could not be achieved with multinomial language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Term Dependent Smoothing</head><p>One flexibility of the Poisson language model is that it provides a natural framework to accommodate term dependent (per-term) smoothing. Existing work on language model smoothing has already shown that different types of queries should be smoothed differently according to how discriminative the query terms are. <ref type="bibr" target="#b6">[7]</ref> also predicted that different terms should have a different smoothing weights. With multinomial query generation models, people usually use a single smoothing coefficient to control the combination of the document model and the background model <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. This parameter can be made specific for different queries, but always has to be a constant for all the terms. This is mandatory since a multinomial language model has the constraint that w∈V p(w|d) = 1. However, from retrieval perspective, different terms may need to be smoothed differently even if they are in the same query. For example, a non-discriminative term (e.g., "the", "is") is expected to be explained more with the background model, while a content term (e.g., "retrieval", "bush") in the query should be explained with the document model. Therefore, a better way of smoothing would be to set the interpolation coefficient (i.e., δ in Formula 2 and Formula 3) specifically for each term. Since the Poisson language model does not have the "sum-to-one" constraint across terms, it can easily accommodate per-term smoothing without needing to heuristically twist the semantics of a generative model as in the case of multinomial language models. Below we present a possible way to explore term dependent smoothing with Poisson language models.</p><p>Essentially, we want to use a term-specific smoothing coefficient δ in the linear combination, denoted as δw. This coefficient should intuitively be larger if w is a common word and smaller if it is a content word. The key problem is to find a method to assign reasonable values to δw. Empirical tuning is infeasible for so many parameters. We may instead estimate the parameters "∆ = {δ1, ..., δ |V | }" by maximizing the likelihood of the query given the mixture model of p(q|ΛQ) and p(q|U ), where ΛQ is the "true" query model to generate the query and p(q|U ) is a query background model as discussed in Section 2.2.3.</p><p>With the model p(q|ΛQ) hidden, the query likelihood is</p><formula xml:id="formula_17">p(q|∆, U ) = Λ Q w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U ))P (ΛQ|U )dΛQ</formula><p>If we have relevant documents for each query, we can approximate the query model space with the language models of all the relevant documents. Without relevant documents, we opt to approximate the query model space with the models of all the documents in the collection. Setting p(·|U ) as p(·|C), the query likelihood becomes</p><formula xml:id="formula_18">p(q|∆, U ) = d∈C π d w∈V ((1−δw)p(c(w, q)| ˆ Λ d )+δwp(c(w, q)|C))</formula><p>where</p><formula xml:id="formula_19">π d = p( ˆ Λ d |U ). p(·|ˆΛ·|ˆ ·|ˆΛ d ) is an estimated Poisson lan- guage model for document d.</formula><p>If we have prior knowledge on p( ˆ Λ d |U ), such as which documents are relevant to the query, we can set π d accordingly, because what we want is to find ∆ that can maximize the likelihood of the query given relevant documents. Without this prior knowledge, we can leave π d as free parameters, and use the EM algorithm to estimate π d and ∆. The updating functions are given as</p><formula xml:id="formula_20">π (k+1) d = π d w∈V ((1 − δw)p(c(w, q)| ˆ Λ d ) + δwp(c(w, q)|C)) d∈C π d w∈V ((1 − δw)p(c(w, q)| ˆ Λ d ) + δwp(c(w, q)|C)) and δ (k+1) w = d∈C π d δwp(c(w, q)|C)) (1 − δw)p(c(w, q)| ˆ Λ d ) + δwp(c(w, q)|C))</formula><p>As discussed in <ref type="bibr" target="#b28">[29]</ref>, we only need to run the EM algorithm for several iterations, thus the computational cost is relatively low. We again assume our vocabulary containing all query terms plus a pseudo non-query term. Note that the function does not give an explicit way of estimating the coefficient for the unseen non-query term. In our experiments, we set it to the average over δw of all query terms.</p><p>With this flexibility, we expect Poisson language models could improve the retrieval performance, especially for verbose queries, where the query terms have various discriminative values. In Section 4, we use empirical experiments to prove this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixture Background Models</head><p>Another flexibility is to explore different background (collection) models (i.e., p(·|U ), or p(·|C)). One common assumption made in language modeling information retrieval is that the background model is a homogeneous model of the document models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Similarly, we can also make the assumption that the collection model is a Poisson language model, with the rates λC,w = d∈C c(w,d) |C|</p><p>. However, this assumption usually does not hold, since the collection is far more complex than a single document. Indeed, the collection usually consists of a mixture of documents with various genres, authors, and topics, etc. Treating the collection model as a mixture of document models, instead of a single "pseudo-document model" is more reasonable. Existing work of multinomial language modeling has already shown that a better modeling of background improves the retrieval performance, such as clusters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>, neighbor documents <ref type="bibr" target="#b24">[25]</ref>, and aspects <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. All the approaches can be easily adopted using Poisson language models. However, a common problem of these approaches is that they all require heavy computation to construct the background model. With Poisson language modeling, we show that it is possible to model the mixture background without paying for the heavy computational cost.</p><p>Poisson Mixture <ref type="bibr" target="#b2">[3]</ref> has been proposed to model a collection of documents, which can fit the data much better than a single Poisson. The basic idea is to assume that the collection is generated from a mixture of Poisson models, which has the general form of</p><formula xml:id="formula_21">p(x = k|P M ) = λ p(λ)p(x = k|λ)dλ p(·|λ)</formula><p>is a single Poisson model and p(λ) is an arbitrary probability density function. There are three well known Poisson mixtures <ref type="bibr" target="#b2">[3]</ref>: 2-Poisson, Negative Binomial, and the Katz's K-Mixture <ref type="bibr" target="#b8">[9]</ref>. Note that the 2-Poisson model has actually been explored in probabilistic retrieval models, which led to the well-known BM25 formula <ref type="bibr" target="#b21">[22]</ref>.</p><p>All these mixtures have closed forms, and can be estimated from the collection of documents efficiently. This is an advantage over the multinomial mixture models, such as PLSI <ref type="bibr" target="#b7">[8]</ref> and LDA <ref type="bibr" target="#b0">[1]</ref>, for retrieval. For example, the probability density function of Katz's K-Mixture is given as</p><formula xml:id="formula_22">p(c(w) = k|αw, βw) = (1 − αw)η k,0 + αw βw + 1 ( βw βw + 1 ) k</formula><p>where η k,0 = 1 when k = 0, and 0 otherwise. With the observation of a collection of documents, αw and βw can be estimated as</p><formula xml:id="formula_23">βw = cf (w) − df (w) df (w) and αw = cf (w) N βw</formula><p>where cf (w) and df (w) are the collection frequency and document frequency of w, and N is the number of documents in the collection. To account for the different document lengths, we assume that βw is a reasonable estimation for generating a document of the average length, and use β = βw avdl |q| to generate the query. This Poisson mixture model can be easily used to replace P (·|C) in the retrieval functions 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Possible Flexibilities</head><p>In addition to term dependent smoothing and efficient mixture background, a Poisson language model has also some other potential advantages. For example, in Section 2, we see that Formula 2 introduces a component which does document length penalization. Intuitively, when the document has more unique words, it will be penalized more. On the other hand, if a document is exactly n copies of another document, it would not get over penalized. This feature is desirable and not achieved with the Dirichlet model <ref type="bibr" target="#b4">[5]</ref>. Potentially, this component could penalize a document according to what types of terms it contains. With term specific settings of δ, we could get even more flexibility for document length normalization.</p><p>Pseudo-feedback is yet another interesting direction where the Poission model might be able to show its advantage. With model-based feedback, we could again relax the combination coefficients of the feedback model and the background model, and allow different terms to contribute differently to the feedback model. We could also utilize the "relevant" documents to learn better per-term smoothing coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>In Section 3, we analytically compared the Poisson language models and multinomial language models from the perspective of query generation and retrieval. In this section, we compare these two families of models empirically. Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two-stage smoothing. Using Poisson mixture as background model also improves the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Since retrieval performance could significantly vary from one test collection to another, and from one query to another, we select four representative TREC test collections: AP, Trec7, Trec8, and Wt2g(Web). To cover different types of queries, we follow <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>, and construct short-keyword (SK, keyword title), short-verbose (SV, one sentence description), and long-verbose (LV, multiple sentences) queries. The documents are stemmed with the Porter's stemmer, and we do not remove any stop word. For each parameter, we vary its value to cover a reasonably wide range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to Multinomial</head><p>We compare the performance of the Poisson retrieval models and multinomial retrieval models using interpolation (JelinekMercer, JM) smoothing and Bayesian smoothing with conjugate priors. <ref type="table">Table 1</ref> shows that the two JM-smoothed models perform similarly on all data sets. Since the Dirichlet Smoothing for multinomial language model and the Gamma Smoothing for Poisson language model lead to the same retrieval formula, the performance of these two models are jointly presented. We see that Dirichlet/Gamma smoothing methods outperform both Jelinek-Mercer smoothing methods. The parameter sensitivity curves for two Jelinek-Mercer  <ref type="figure" target="#fig_0">Figure 1</ref>. Clearly, these two methods perform similarly either in terms of optimality  <ref type="table">Table 1</ref>: Performance comparison between Poisson and Multinomial retrieval models: basic models perform comparably; term dependent two-stage smoothing significantly improves Poisson</p><note type="other">Data Query JM-Multinomial JM-Poisson Dirichlet/Gamma Per-term 2-Stage Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88</note><p>An asterisk (*) indicates that the difference between the performance of the term dependent two-stage smoothing and that of the Dirichlet/Gamma single smoothing is statistically significant according to the Wilcoxon signed rank test at the level of 0.05.</p><p>or sensitivity. This similarity of performance is expected as we discussed in Section 3.1. Although the Poisson model and multinomial model are similar in terms of the basic model and/or with simple smoothing methods, the Poisson model has great potential and flexibility to be further improved. As shown in the rightmost column of <ref type="table">Table 1</ref>, term dependent two-stage Poisson model consistently outperforms the basic smoothing models, especially for verbose queries. This model is given in Formula 4, with a Gamma smoothing for the document model p(·|d), and δw, which is term dependent. The parameter µ of the first stage Gamma smoothing is empirically tuned. The combination coefficients (i.e., ∆), are estimated with the EM algorithm in Section 3.2. The parameter sensitivity curves for Dirichlet/Gamma and the per-term two-stage smoothing model are plotted in <ref type="figure" target="#fig_2">Figure 2</ref>. The per-term two-stage smoothing method is less sensitive to the parameter µ than Dirichlet/Gamma, and yields better optimal performance.  In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Term Dependent Smoothing</head><p>To test the effectiveness of the term dependent smoothing, we conduct the following two experiments. In the first experiment, we relax the constant coefficient in the simple Jelinek-Mercer smoothing formula (i.e., Formula 3), and use the EM algorithm proposed in Section 3.2 to find a δw for each unique term. Since we are using the EM algorithm to iteratively estimate the parameters, we usually do not want the probability of p(·|d) to be zero. We then use a simple Laplace method to slightly smooth the document model before it goes into the EM iterations. The documents are then still scored with Formula 3, but using learnt δw. The results are labeled with "JM+L." in  An asterisk (*) in Column 3 indicates that the difference between the "JM+L." method and JM method is statistically significant; an asterisk (*) in Column 5 means that the difference between term dependent two-stage method and query dependent two-stage method is statistically significant; PT stands for "per-term".</p><p>With term dependent coefficients, the performance of the Jelinek-Mercer Poisson model is improved in most cases. However, in some cases (e.g., Trec7/SV), it performs poorly. This might be caused by the problem of EM estimation with unsmoothed document models. Once non-zero probability is assigned to all the terms before entering the EM iteration, the performance on verbose queries can be improved significantly. This indicates that there is still room to find better methods to estimate δw. Please note that neither the perterm JM method nor the "JM+L." method has a parameter to tune.</p><p>As shown in <ref type="table">Table 1</ref>, the term dependent two-stage smoothing can significantly improve retrieval performance. To understand whether the improvement is contributed by the term dependent smoothing or the two-stage smoothing framework, we design another experiment to compare the perterm two-stage smoothing with the two-stage smoothing method proposed in <ref type="bibr" target="#b28">[29]</ref>. Their method managed to find coefficients specific to the query, thus a verbose query would use a higher δ. However, since their model is based on multinomial language modeling, they could not get per-term coefficients. We adopt their method to the Poisson two-stage smoothing, and also estimate a per-query coefficient for all the terms. We compare the performance of such a model with the per-term two-stage smoothing model, and present the results in the right two columns in <ref type="table" target="#tab_1">Table 2</ref>. Again, we see that the "per-term" two-stage smoothing outperforms the "per-query" two-stage smoothing, especially for verbose queries. The improvement is not as large as how the perterm smoothing method improves over Dirichlet/Gamma. This is expected, since the per-query smoothing has already addressed the query discrimination problem to some extent. This experiment shows that even if the smoothing is already per-query, making it per-term is still beneficial. In brief, the per-term smoothing improved the retrieval performance of both one-stage and two-stage smoothing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mixture Background Model</head><p>In this section, we conduct experiments to examine the benefits of using a mixture background model without extra computational cost, which can not be achieved for multinomial models. Specifically, in retrieval formula 3, instead of using a single Poisson distribution to model the background p(·|C), we use Katz's K-Mixture model, which is essentially a mixture of Poisson distributions. p(·|C) can be computed efficiently with simple collection statistics, as discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Query  The performance of the JM retrieval model with single Poisson background and with Katz's K-Mixture background model is compared in <ref type="table" target="#tab_4">Table 3</ref>. Clearly, using K-Mixture to model the background model outperforms the single Poisson background model in most cases, especially for verbose queries where the improvement is statistically significant. <ref type="figure" target="#fig_4">Figure 3</ref> shows that the performance changes over different parameters for short verbose queries. The model using K-Mixture background is less sensitive than the one using single Poisson background. Given that this type of mixture  cost, it would be interesting to study whether using other mixture Poisson models, such as 2-Poisson and negative Binomial, could help the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.</p><p>Language models have been shown to be effective for many retrieval tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4]</ref>. The most popular and fundamental one is the query-generation language model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>. All existing query generation language models are based on either multinomial distribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13]</ref> or multivariate Bernoulli distribution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. We introduce a new family of language models, based on Poisson distribution. Poisson distribution has been previously studied in the document generation models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>, leading to the development of one of the most effective retrieval formula BM25 <ref type="bibr" target="#b22">[23]</ref>. <ref type="bibr" target="#b23">[24]</ref> studies the parallel derivation of three different retrieval models which is related to our comparison of Poisson and multinomial. However, the Poisson model in their paper is still under the document generation framework, and also does not account for the document length variation. <ref type="bibr" target="#b25">[26]</ref> introduces a way to empirically search for an exponential model for the documents. Poisson mixtures <ref type="bibr" target="#b2">[3]</ref> such as 2-Poisson <ref type="bibr" target="#b21">[22]</ref>, Negative multinomial, and Katz's KMixture <ref type="bibr" target="#b8">[9]</ref> has shown to be effective to model and retrieve documents. Once again, none of this work explores Poisson distribution in the query generation framework.</p><p>Language model smoothing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and background structures <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> have been studied with multinomial language models. <ref type="bibr" target="#b6">[7]</ref> analytically shows that term specific smoothing could be useful. We show that Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a generative model, and is able to efficiently better model the mixture background, both analytically and empirically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Poisson and multinomial performs similarly with Jelinek-Mercer smoothing smoothing methods are shown in Figure 1. Clearly, these two methods perform similarly either in terms of optimality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Term dependent two-stage smoothing of Poisson outperforms Dirichlet/Gamma In the following subsections, we conduct experiments to demonstrate how the flexibility of the Poisson model could be utilized to achieve better performance, which we cannot achieve with multinomial language models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: K-Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Data 
Q 
JM 
JM 
JM+L. 
2-Stage 2-Stage 
(MAP) PT: 
No 
Yes 
Yes 
No 
Yes 
AP 
SK 
0.203 
0.204 
0.206 
0.223 
0.226* 
SV 
0.183 
0.189 
0.214* 
0.204 
0.217* 
Trec7 
SK 
0.168 
0.171 
0.174 
0.186 
0.185 
SV 
0.176 
0.147 
0.198* 
0.194 
0.196 
Trec8 
SK 
0.239 
0.240 0.227* 
0.257 
0.256 
SV 
0.234 
0.223 
0.249* 
0.242 
0.246* 
Web 
SK 
0.250 0.236 
0.220* 
0.291 
0.307* 
SV 
0.217 
0.232 
0.261* 
0.273 
0.292* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Term dependent smoothing improves re-
trieval performance 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>K-Mixture background model improves re-
trieval performance 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>We present a new family of query generation language models for retrieval based on Poisson distribution. We derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing. We compare the new models with the popular multinomial retrieval models both analytically and experimentally. Our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences. In particular, we show that Poisson has an advantage over multinomial in naturally accommodating per-term smoothing. We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models. Furthermore, we show that a mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model. Our work opens up many interesting directions for further exploration in this new family of models. Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work. It is also appealing to find robust methods to learn the per-term smoothing coefficients without additional computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We thank the anonymous SIGIR 07 reviewers for their useful comments. This material is based in part upon work supported by the National Science Foundation under award numbers IIS-0347933 and 0425852.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poisson mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="190" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language Modeling and Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A formal study of information retrieval heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Using Language Models for Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Enschede, Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 25th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;99</title>
		<meeting>ACM SIGIR&apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distribution of content words and phrases in text and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="59" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Corpus structure, language models, and ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic IR models based on query and document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Modeling and IR workshop</title>
		<meeting>the Language Modeling and IR workshop</meeting>
		<imprint>
			<date type="published" when="2001-06-01" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic relevance models based on document and query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Modeling and Information Retrieval</title>
		<editor>W. B. Croft and J. Lafferty</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relevance-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cluster-based retrieval using language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modelling documents with multiple poisson distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Margulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="227" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-98 Workshop on Learning for Text Categorization</title>
		<meeting>AAAI-98 Workshop on Learning for Text Categorization</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Formal multiple-bernoulli models for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="540" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hidden Markov model information retrieval system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Probability, random variables and stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papoulis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;94</title>
		<meeting>SIGIR&apos;94</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Text REtrieval Conference (TREC-3)</title>
		<editor>D. K. Harman</editor>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A parallel derivation of probabilistic information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roelleke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language model information retrieval with document expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;01</title>
		<meeting>ACM SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09" />
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stage language models for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;02</title>
		<meeting>ACM SIGIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002-08" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
