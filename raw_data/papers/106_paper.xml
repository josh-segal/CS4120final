<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VARIATIONAL BAYESIAN IMAGE PROCESSING ON STOCHASTIC FACTOR GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Lane Dept. of Computer Science and Electrical Engineering West</orgName>
								<orgName type="institution">Virginia University</orgName>
								<address>
									<postCode>WV26506-6109</postCode>
									<settlement>Morgantown</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VARIATIONAL BAYESIAN IMAGE PROCESSING ON STOCHASTIC FACTOR GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-patch-based models</term>
					<term>stochastic factor graphs</term>
					<term>nonlocal dependency</term>
					<term>sparsity priors</term>
					<term>variational Bayesian inference</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we present a patch-based variational Bayesian framework of image processing using the language of factor graphs (FGs). The variable and factor nodes of FGs represent image patches and their clustering relationship respectively. Unlike previous probabilistic graphical models, we model the structure of FGs by a latent variable, which gives the name &quot;stochastic factor graphs&quot;(SFGs). A sparsity-based prior is enforced to the local distribution functions at factor nodes, which leads to a class of variational expectation-maximization (VEM) algorithms on SFGs. VEM algorithms allow us to infer graph structure along with the target of inference from the observation data. This new framework can systematically exploit nonlocal dependency in natural images as justified by the experimental results in image denoising and inpainting applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>To overcome the curse of dimensionality, it is often convenient to make the Markovian assumption that the probability structure of images can be defined locally. Such assumption is at the foundation of both classical Markov random field (MRF) models (refer to recent high-order extension <ref type="bibr" target="#b0">[1]</ref>) and popular transform-domain models (e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>). However, important image structures such as edges and textures are often characterized by self-similarity that is beyond the reach of local models. In recent years, a flurry of socalled patch-based image models have been proposed to exploit the nonlocal dependency in natural images and found several successful applications in texture synthesis <ref type="bibr" target="#b3">[4]</ref>, super-resolution <ref type="bibr" target="#b4">[5]</ref>, image inpainting <ref type="bibr" target="#b5">[6]</ref> and image denoising <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this paper, we attempt to set patch-based image processing on a rigorous ground using the language of factor graphs (FGs). Specifically, patches generated by translational operators correspond to variable nodes V and the clustering relationship among patches is characterized by a collection of factor nodes F (refer to <ref type="figure">Fig. 1a</ref>). When compared with the classical MRFs (in which edge connectivity is defined by location-related geometric proximity), the structure of FG in this work is recursively defined by intensityrelated structural similarity, which dictates its stochastic nature. Therefore, given observation data y (e.g., noisy or incomplete images), we have to estimate the latent variable defining graph structure T : V → F along with the target of inference x (e.g., clean and complete images). A variational EM (VEM)-based technique <ref type="bibr" target="#b9">[10]</ref> is developed to support efficient inference on the proposed SFG, which iteratively updates the estimate of x and T.</p><formula xml:id="formula_0">URL: http : //www.csee.wvu.edu/ ∼ xinl/demo/sf g.html</formula><p>Under the SFG-based framework, it becomes easier to compare and unify different patch-based image processing algorithms. Various image prior models have been proposed for p(x|T) -e.g., from parametric Gaussian distributions <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref> to nonparametric models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> and most recent sparsity-based transform-domain models <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The discovery of graph structure (i.e., update T) can be implemented by k-nearest-neighbor (kNN) finding or k-means clustering (many more powerful tools can be borrowed from the literature of data clustering). Even though the theoretic convergence of iterative VEM algorithm remains elusive, we have found through experiments that it has promising performance in several image recovery applications including denoising and inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REPRESENTING IMAGES BY STOCHASTIC FACTOR GRAPHS</head><p>A FG is a bipartite graph G = (V + F, T) that expresses the dependency structure among variable nodes V = {Bm} M m=1 by a collection of factor nodes F = {fn} N n=1 . For notational simplicity, we use a mapping function T : F → V to denote the edge (graph structure) information. At node fn, we use |fn| to denote its degree -i.e., the total number of variable nodes it is connected to. Throughout this paper, we use x, B, x to denote a single pixel, a patch of fixed size B × B and the whole image respectively. Given an image x sized H × W , we simply put all patches generated by translation into V (with maximum overlap and boundary extension, we have M = HW ).</p><p>The primary motivation for representing images by FGs is due to their capability of decomposing a high-dimensional pdf f (x) into the product of local functions at factor nodes</p><formula xml:id="formula_1">f (x) = 񮽙 n fn(Dn)<label>(1)</label></formula><p>where Dn denotes the collection of variable nodes connected to a common factor node fn and fn(Dn) is the local pdf 1 . The key challenge lies in the definition of edge connectivity at factor nodes (i.e., the graph structure T). Conventional MRF models connect patches that are geometrically adjacent to each other (and hence characterize local dependencies only). However, important structures such as edges and textures are often characterized by nonlocal, self-similar dependency that is beyond the reach of local models. <ref type="figure">Fig. 1b</ref> gives a specific example of how nonlocal dependency looks like -all patches connected to the same factor node are structurally similar (e.g., small Euclidean distance or large pearson product-moment correlation) but are spatially distant from each other. The power of FGs largely depends on the choice of local pdf fn(Dn)'s as well as the graph structure |T|. There are several choices for local pdfs. For instance, one might impose parametric models -e.g., Gaussian distributions such as N ( 񮽙 mx, Cx) have been adopted by learning-based approach <ref type="bibr" target="#b4">[5]</ref>. An alternative approach is through Least-Square regression <ref type="bibr" target="#b10">[11]</ref> which models B0 as the linear combination of B1, ..., B k . It is also possible to obtain the local pdfs by sparse representations in the transform domain <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Specifically, one can normalize-and-sort (for the purpose of maximizing the sparsity) the patches linked by the same factor node to form a data array D -i.e., D = [B0, B1, ..., B k ] (this array can be 2D if we reshape patches into vectors or 3D if we simply pack 2D patches together). Conceptually similar to existing transform-based models, we can characterize transform coefficients S = T D by the distribution of exponential families, which essentially translates the a priori information into the sparsity constraint of S</p><formula xml:id="formula_2">f (x) ≈ exp(− 񮽙 n ||Sn||) (2)</formula><p>where ||Sn|| denotes the norm of data array (e.g., L1 implies a Laplacian distribution). The structure of FGs is largely determined by its capacity N = |F| and degree distribution {|fn|} N n=1 . The capacity of a FG can be understood from an associative memory perspective <ref type="bibr" target="#b11">[12]</ref> each factor node stores a novel relationship among variable nodes. When N → 0, FG model can only describe the class of images that are strongly self-similar (showing little positional differentiation). To handle complex structures, it is desirable to have N → ∞ even though some factor nodes might be empty (left for learning new association rules). We note that such theoretic prediction is supported by the empirical findings of visual cortex in human vision system (HVS) -its capacity measured by the number of neurons dramatically increases as the visual pathway reaches higher level of cortexes. The degree distribution more directly affects the structure of FG and reflects our a priori knowledge of image structures. Image structures of strong self-similarity (e.g., regular textures and edges) tend to give high-connectivity node |fn| &gt;&gt; 1; while structures of weak self-similarity (e.g., irregular textures) often produce low connectivity |fn| ≈ 1.</p><p>The framework of SFG-based modeling is useful to unify and compare various image processing algorithms built upon different models. For example, conventional MRFs and their high-order extensions (e.g, Field-of-Expert <ref type="bibr" target="#b0">[1]</ref>) can be viewed as FG with pre-fixed graph structure in which only spatially adjacent patches are connected to a common factor node. Image recovery under overcomplete expansions (wavelet <ref type="bibr" target="#b1">[2]</ref> or DCT <ref type="bibr" target="#b2">[3]</ref>) can also be interpreted as special cases of fixed FGs with M ≥ HW and |fn| = 1 (i.e., FGs degenerate into local models). Texture synthesis by nonparametric resampling <ref type="bibr" target="#b3">[4]</ref> and its related exemplar-based inpainting <ref type="bibr" target="#b5">[6]</ref> associate a patch and its nearest neighbor in the patch space (i.e., |fn| = 2, ∀n). Inclusion of k-nearest-neighbors (kNN) in learning-based approach <ref type="bibr" target="#b4">[5]</ref> and nonlocal-mean denoising <ref type="bibr" target="#b6">[7]</ref> give rise to |fn| &gt;&gt; 1. Current state-of-the-art image denoising algorithm BM3D <ref type="bibr" target="#b8">[9]</ref> can be interpreted as using a FG with factor nodes located at a down-sampled lattice (e.g., N = M/16 when down-sampling ratio is 4) and generated by a variant of kNN search (it also involves a one-time update of graph structure T).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VARIATIONAL BAYESIAN INFERENCE ON FACTOR GRAPHS</head><p>Sparsity-based prior function defined by Eq. (2) is generic but depends on the graph structure T. In various image processing tasks, T is often unknown and has to be estimated along with x (target of inference) from the noisy or incomplete observation y. A fully Bayesian treatment of the latent variable T is to marginalize it out by</p><formula xml:id="formula_3">p(x|y) = 񮽙 p(x, T|y)dT = 񮽙 p(x|T, y)p(T|y)dT.<label>(3)</label></formula><p>However, such approach involves the calculation of posterior probability p(T|y), which is computationally intractable. A compromised solution is via Laplacian approximation, namely</p><formula xml:id="formula_4">p(x|y) ≈ p(x|y, T * )<label>(4)</label></formula><p>where T * = arg max T p(y|T) is a maximum-likelihood estimate (assuming a uniform prior p(T)). Although it is possible (at least theoretically) to evaluate the likelihood term at various T values by Monte-Carlo methods, the computational cost is prohibitive. The basic idea behind variational approaches <ref type="bibr" target="#b9">[10]</ref> is to approximate the likelihood function p(y|T) by a lower bound F(q(x), T) that is easier to calculate in practice (e.g., q(x) = f (x) can be obtained by mean field approximation). Specifically, we can into</p><formula xml:id="formula_5">L(T) = lnp(y|T) = ln p(x, y|T) p(x|y, T) = 񮽙 dxq(x) ln p(x, y|T)q(x) p(x|y, T)q(x) = 񮽙 dxq(x) ln p(x, y|T) q(x) + 񮽙 dxq(x) ln q(x) p(x|y, T) = F (q(x), T) + KL[q(x)||p(x|y, T)] ≥ F(q(x), T)(5)</formula><p>where F (q(x), T) is the negative to a well-known quantity in statistical physics as f ree energy and KL[q||p] ≥ 0 denotes the Kullback-Leibler divergence. Under such formulation, the ML estimation of T can be obtained by a variational expectationmaximization (VEM) algorithm which iteratively maximizes the lower bound F(q(x), T) or minimizes the free energy (parenthesized superscript t = 1, ..., tmax denotes the iteration number)</p><p>• E-step:</p><formula xml:id="formula_6">x (t+1) ← arg max x F (q(x), T (t) ), • M-step: T (t+1) ← arg max T F(q (t+1) (x), T).</formula><p>Loosely speaking, E-step and M-step respectively refine the estimate of inference target x and graph structure T. Note that detailed implementation of VEM algorithm depends on the observation model which varies from application to application (we will consider two examples next).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we take two concrete examples to elaborate on the implementation details of VEM algorithm: image denoising (y is a noisy version of x) and image recovery (y is an incomplete version of x). In both cases, we compare this work with previous benchmark schemes on three classes of images with decreasing amount of self-similarity: regular textures, regular edges and irregular textures (refer to <ref type="figure" target="#fig_1">Fig. 2)</ref>. In addition to MSE-based fidelity measures, we also report SSIM performance <ref type="bibr" target="#b12">[13]</ref> which has shown to better reflect the structural similarities between two images. The main objective is to validate the hypothesis that SFG-based models are capable of characterizing a wide range of structures in natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Denoising</head><p>We first study the classical image denoising problem: y = x + w where w ∼ N (0, σ 2 w ). Depending on the formulation of local pdf at factor nodes, the E-step (updating of x based on T) can be implemented by linear/nonlinear filters in the spatial domain (e.g., nonlocal mean <ref type="bibr" target="#b6">[7]</ref> and its extension <ref type="bibr" target="#b7">[8]</ref>) or in the transform domain (e.g., DCT <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>). We have adopted a Wiener filtering implementation in 2D-DCT domain (i.e., we treat each patch as a row vector). The implementation of M-step is also flexible: we have opted to apply a fast kmeans algorithm <ref type="bibr" target="#b14">[15]</ref>    while other choices (e.g., kNN search) also exist. The parameters of VEM algorithm in our experiments are fixed: B = 7, N = 100, tmax = 20 (they have not been optimized). MATLAB codes of SFG-based denoising can be accessed at http://www.csee.wvu. edu/ ∼ xinl/demo/sfg.html. <ref type="table" target="#tab_1">Tables I and II</ref> include the PSNR and SSIM performance comparison between our SFG denoising and five benchmark schemes: Shape-adaptive DCT <ref type="bibr" target="#b13">[14]</ref>, Gaussian-Scalar-Mixture <ref type="bibr" target="#b1">[2]</ref>, Field-ofExpert <ref type="bibr" target="#b0">[1]</ref>, Block-matching 3D <ref type="bibr" target="#b8">[9]</ref> and Total-Least-Square <ref type="bibr" target="#b10">[11]</ref>. It can be observed that even without any optimization, ours has achieved comparable performance to state-of-the-art denoising scheme BM3D. Since patch-based schemes (BM3D, TLS and SFG) are capable of exploiting nonlocal dependency in an image, they have achieved noticeably better performance than those based on local models (especially for the class of regular textures). <ref type="figure">Fig.  3</ref> includes the comparison of denoised images among different schemes. The readers are referred to the above website for direct visual inspection.  In our second experiment, we study the application of VEM algorithm into image inpainting -i.e., the recovery of missing pixels in an image. In inpainting scenario, likelihood term p(y|x) is a simply binary indicator function 1 x∈Ω c I where ΩI denotes the inpainting domain. Unlike denoising, the E-step is implemented by a hard thresholding operator (motivated by the recent work <ref type="bibr" target="#b2">[3]</ref>, we obtain an approximately fixed-point solution by gradually decreasing the threshold). The implementation of M-step is based on kNN search (i.e., |fn| = k +1). Three parameters (patch size B, neighborhood size k and graph size N ) can be adjusted by the user. Different parameter settings are adopted for three classes of images respectively. More details can be found from the accompanying source codes available from the above link.  The five benchmark schemes in our inpainting experiments include: overcomplete DCT-based scheme <ref type="bibr" target="#b2">[3]</ref>, Field-of-Expert <ref type="bibr" target="#b0">[1]</ref>, exemplar-based method <ref type="bibr" target="#b5">[6]</ref>, BM3D-based (our own extension of <ref type="bibr" target="#b8">[9]</ref> into inpainting) and Least-Square Prediction scheme. <ref type="table" target="#tab_1">Tables  III and IV</ref> include the PSNR and SSIM performance comparison between our SFG-based inpainting and five benchmark schemes. Note that both objective metrics are calculated for pixels within the inpainting domain only, which explains low PSNR/SSIM values when inpainting technique fails. Again it can be observed that overall SFG-based inpainting achieves the best performance. Local models become more competent for images of irregular textures. <ref type="figure" target="#fig_2">Fig. 4</ref> includes the comparison of inpainted images by different schemes. We observe again that local models are often inferior to nonlocal ones for regular textures and edges. For irregular textures, visual quality comparison becomes more tricky and even SSIM metric appears to become less effective (e.g., for test5, FoE gives the second best SSIM performance though its subjective quality appears the worst).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND PERSPECTIVES</head><p>Patch-based image representations using the language of FGs provide a promising framework for unifying existing image models. The degree distribution of FGs carries useful information about the tradeoff between local and nonlocal dependencies of the source. Variational techniques, powered by sparsity-based priors, can offer computationally feasible solutions to difficult inference problems. Under this new framework, it is desirable to explore more sophisticated strategies of constructing FGs (e.g., M &gt; HW if rotational and reflection operators are included). It is also possible to further improve the performance of VEM algorithm by more powerful data clustering and adaptive choosing of model parameters. SFGbased models have intriguing connection to self-organization and associative memory <ref type="bibr" target="#b11">[12]</ref>. In the long run, we believe it is important to combine the strengths of local and nonlocal models using a fully (yet empirical) Bayesian approach which has been elegantly solved by HVS through the process of evolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Fig. 1. a) Illustration of representing an image of a diagonal edge by a FG containing 3 factor nodes and 16 variable nodes; b) a specific example of factor node: a patch B0 and its 3 NNs connected by fn (|fn| = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three classes of test images (test1-test6 from left to right) with decreasing amount of self-similarity: a) regular textures; b) regular edges; c) irregular textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Subjective quality comparison of inpainted images by different algorithms (top-down: test1, test3, test5; left-to-right: DCT, FoE, EXP, BM3D, LSP and SFG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table I .</head><label>I</label><figDesc></figDesc><table>PSNR(dB) performance comparison among different de-
noising schemes for six test images at the noise level σw = 25 
(boldface indicates the best two schemes for each image). 

image SADCT 
GSM 
FoE 
BM3D 
TLS 
SFG 
test1 
0.8674 
0.9112 0.7620 
0.9295 
0.9238 0.9406 
test2 
0.7930 
0.8149 0.5860 
0.8215 
0.7783 0.8569 
test3 
0.8463 
0.8439 0.8477 
0.8887 
0.8500 0.8014 
test4 
0.8380 
0.7859 0.8257 
0.8520 
0.8132 0.7631 
test5 
0.8156 
0.8177 0.7810 
0.8139 
0.8130 0.8393 
test6 
0.7939 
0.8020 0.8137 
0.8294 
0.7950 0.8405 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table II . SSIM performance comparison among different denoising schemes for six test images at the noise level σw = 25.</head><label>II</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table III .</head><label>III</label><figDesc></figDesc><table>PSNR(dB) performance comparison among different 
inpainting schemes for six test images. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table IV .</head><label>IV</label><figDesc></figDesc><table>SSIM performance comparison among different inpaint-
ing schemes for six test images. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">High-order markov random fields for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-05" />
			<pubPlace>Providence RI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image denoising using scale mixtures of gaussians in the wavelet domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Strela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2003-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising-part i: theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">G</forename><surname>Guleryuz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="539" to="554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Comp. Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised patch-based image regularization and representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boulanger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="555" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Variational algorithms for approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University College London ; Gatsby Computational Neuroscience Unit</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD. Thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising using total least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2730" to="2742" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-organization and associative memory: 3rd edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using the triangle inequality to accelerate k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
