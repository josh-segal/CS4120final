<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Probabilistic Clustering-Projection Model for Discrete Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Corporate Technology</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Corporate Technology</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Siemens Corporate Technology</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Probabilistic Clustering-Projection Model for Discrete Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For discrete co-occurrence data like documents and words, calculating optimal projections and clustering are two different but related tasks. The goal of projection is to find a low-dimensional latent space for words, and clustering aims at grouping documents based on their feature representations. In general projection and clustering are studied independently, but they both represent the intrinsic structure of data and should reinforce each other. In this paper we introduce a probabilistic clustering-projection (PCP) model for discrete data, where they are both represented in a unified framework. Clustering is seen to be performed in the projected space, and projection explicitly considers clustering structure. Iterating the two operations turns out to be exactly the variational EM algorithm under Bayesian model inference, and thus is guaranteed to improve the data likelihood. The model is evaluated on two text data sets, both showing very encouraging results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modelling discrete data is a fundamental problem in machine learning, pattern recognition and statistics. The data is usually represented as a large (and normally sparse) matrix, where each entry is an integer and characterizes the relationship between corresponding row and column. For example in document modelling, the "bag-of-words" methods represent each document as a row vector of occurrences of each word, ignoring any internal structure and word order. This is taken as the working example in this paper, but the proposed model is generally applicable to other discrete data.</p><p>Data projection and clustering are two important tasks and have been widely applied in data mining and machine learning (e.g., principal component analysis (PCA) and k-means <ref type="bibr" target="#b0">[1]</ref>). Projection is also referred as feature mapping that aims to find a new representation of data, which is low-dimensional and physically meaningful. On the other hand, clustering tries to group similar data patterns together, and thus uncovers the structure of data. Traditionally these two methods are studied separately and mainly on continuous data. However in this paper we investigate them on discrete data and treat them jointly.</p><p>Projection on discrete data differs from the case on continuous space, where, for example, the most popular technology PCA tries to find the orthogonal dimensions (or factors) that explains the covariance of data dimensions. However, one cannot make the same orthogonal assumption on the low-dimensional factors of discrete data and put the interests on the covariance anymore. Instead, it is desired to find the independent latent factors that explain the co-occurrence of dimensions (e.g., words). In text modelling, if we refer the factors as topics, the projection actually represent each document as a data point in a low-dimensional topic space, where a co-occurrence factor actually suggests more or less a cluster of words (i.e., a group of words often occurring together). Intuitively, if the projected topic space is informative enough, it should also be highly indicative to reveal the clustering structure of documents. On the other hand, a truly discovered clustering structure reflects the shared topics within document clusters and the distinguished topics across document clusters, and thus can offer evidence for the projection side. Therefore, it is highly desired to consider the two problems in a unified model.</p><p>In this paper a novel probabilistic clustering-projection (PCP) model is proposed, to jointly handle the projection and clustering for discrete data. The projection of words is explicitly formulated with a matrix of model parameters. Document clustering is then incorporated using a mixture model on the projected space, and we model each mixture component as a multinomial over the latent topics. In this sense this is a clustering model using projected features for documents if the projection matrix is given, and a projection model with structured data for words if the clustering structure is known. A nice property of the model is that we can perform clustering and projection iteratively, incorporating new information on one side to the updating of the other. We will show that they are corresponding to a Bayesian variational EM algorithm that improves the data likelihood iteratively. This paper follows the basic idea in our earlier paper <ref type="bibr" target="#b13">[14]</ref>, and gives some new insights as well as a thorough empirical study. This paper is organized as follows. The next section reviews related work. Section 3 introduces the PCP model and explicitly points out the clustering and projection effects. In Section 4 we present inference and learning algorithm. Then Section 5 presents experimental results and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>PCA is perhaps the most well-known projection technique, and has its counterpart in information retrieval called latent semantic indexing <ref type="bibr" target="#b3">[4]</ref>. For discrete data, an important related work is probabilistic latent semantic indexing (pLSI) <ref type="bibr" target="#b6">[7]</ref> which directly models latent topics. PLSI can be treated as a projection model, since each latent topic assigns probabilities to a set of words, and thus a document can be treated as generated from a mixture of multiple topics. However, the model is not built for clustering and, as pointed by Blei et al. <ref type="bibr" target="#b1">[2]</ref>, it is not a proper generative model, since it treats document IDs as random variables and thus cannot generalize to new documents. Latent Dirichlet allocation (LDA) <ref type="bibr" target="#b1">[2]</ref> generalizes pLSI by treating the topic mixture parameters as variables drawn from a Dirichlet distribution. This model is a well-defined generative model and performs much better than pLSI, but the clustering effect is still missing. On the other side, document clustering has been intensively investigated and the most popular method is probably partition-based algorithms like k-means (see, e.g., <ref type="bibr" target="#b0">[1]</ref>). Non-negative matrix factorization (NMF) <ref type="bibr" target="#b10">[11]</ref> is another candidate and is shown to obtain good results in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Despite that plenty of work has been done in either clustering or projection, the importance of considering both in a single framework has been noticed only recently, e.g., <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b11">[12]</ref>. Both works are concerned about document clustering and projection on continuous data, while lacking the probabilistic interpretations to the connections among documents, clusters and factors. Buntine et al. <ref type="bibr" target="#b2">[3]</ref> noticed this problem for discrete data and pointed out that the multinomial PCA model (or discrete PCA) takes clustering and projection as two extreme cases. Another closely related work is the so-called two-sided clustering, like <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b4">[5]</ref>, which aims to clustering words and documents simultaneously. In <ref type="bibr" target="#b4">[5]</ref> it is implicitly assumed a one-to-one correspondence between the two sides of clusters. <ref type="bibr" target="#b7">[8]</ref> is a probabilistic model for discrete data, but it has similar problems as in pLSI and not generalizable to new documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PCP Model</head><p>We consider a corpus D containing D documents, with vocabulary V having V words. Following the notation in <ref type="bibr" target="#b1">[2]</ref>, each document d is a sequence of N d words that is denoted by</p><formula xml:id="formula_0">w d = {w d,1 , . . . , w d,N d },</formula><p>where w d,n is a variable for the nth word in w d and denotes the index of the corresponding word in V.</p><p>To simplify explanations, we use "clusters" for components in document clustering structure and "topics" for projected space for words. Let M denote the number of clusters and K the dimensionality of topics. Roman letters d, m, k, n, j are indices for documents, clusters, topics, words in w d , and words in V. They are up to D, M, K, N d , V , respectively. Letter i is reserved for temporary index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Probabilistic Model</head><p>The PCP model is a generative model for a document corpus. <ref type="figure" target="#fig_0">Figure 1</ref> (left) illustrates the sampling process in an informal way. To generate one document d, we first choose a cluster from the M clusters. For the mth cluster, the cluster center is denoted as θ m and defines a topic mixture over the topic space. Therefore θ m is a K-dimensional vector and satisfies θ m,k ≥ 0, K k=1 θ m,k = 1 for all m = 1, . . . , M . The probability of choosing a specific cluster m for document d is denoted as π m , and π := {π 1 , . . . , π M } satisfies π m ≥ 0, M m=1 π m = 1. When document d chooses cluster m, it defines a document-specific topic mixture θ d , which is obtained exactly from the cluster center θ m . Note that everything is discrete and two documents belonging to the same cluster will have the same topic mixtures. Words are then sampled independently given topic mixture θ d , in the same way as in LDA. Each word w d,n is generated by first choosing a topic z d,n given the topic mixture, and then sampling the word given the projection β. β is the K ×V matrix where β k,j specifies the probability of generating word j given topic k, β k,j = p(w j = 1|z k = 1). Therefore each row β k,: defines a multinomial distribution for all words over topic k and satisfies</p><formula xml:id="formula_1">β k,j ≥ 0, V j=1 β k,j = 1.</formula><p>To complete the model, we put a Dirichlet prior Dir(λ) for all the cluster centers θ 1 , . . . , θ M , and a symmetric Dirichlet prior Dir(α/M, . . . , α/M ) for the mixing weights π. Note that they are sampled only once for the whole corpus.</p><p>Finally we obtain the probabilistic model formally illustrated in <ref type="figure">Figure</ref>  </p><formula xml:id="formula_2">θ d = θ m ; (b) For each of the N d words w d,n : i. Choose a topic z d,n ∼ Mult(θ d ); ii. Choose a word w d,n ∼ Mult(β z d,n ,: ).</formula><p>Denote θ as the set of M cluster centers {θ 1 , . . . , θ M }, the likelihood of the corpus D can be written as</p><formula xml:id="formula_3">L(D; α, λ, β) = π θ D d=1 p(w d |θ, π; β)dP (θ; λ) dP (π; α),<label>(1)</label></formula><p>where p(θ; λ) = M m=1 p(θ m ; λ), and the likelihood of document d is a mixture:</p><formula xml:id="formula_4">p(w d |θ, π; β) = M c d =1 p(w d |θ, c d ; β)p(c d |π).<label>(2)</label></formula><p>Given mixture component c d , likelihood term p(w d |θ, c d ; β) is then given by</p><formula xml:id="formula_5">p(w d |θ c d ; β) = N d n=1 K z d,n =1 p(w d,n |z d,n ; β)p(z d,n |θ c d ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PCP as a Clustering Model</head><p>As can be seen from <ref type="formula" target="#formula_4">(2)</ref> and <ref type="formula" target="#formula_5">(3)</ref>, PCP is a clustering model when the projection β is assumed known. The essential terms now are the probabilities of clusters p(m|π) = π m , probabilistic clustering assignment for documents p(w d |θ m ; β), and cluster centers θ m , for m = 1, . . . , M . Note from (3) that cluster centers θ m are not modelled directly with words like p(w|θ m ), but with topics, p(z|θ m ). This means we are not clustering documents in word space, but in topic space. This is analogous to clustering continuous data on the latent space found by PCA <ref type="bibr" target="#b5">[6]</ref>, and K is exactly the dimensionality of this space. To obtain the probability that document d belongs to cluster m, we project each word into topic space, and then calculate the distance to cluster center θ m by considering all the words in w d . This explains (3) from perspective of clustering.</p><p>To improve generalization and avoid overfitting, we put priors to θ m and π and treat them as hidden variables, as usually done in mixture modelling. The prior distributions are chosen to be Dirichlet that is conjugate to multinomial. This will make model inference and learning much easier (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PCP as a Projection Model</head><p>A projection model aims to learn projection β, mapping words to topics. As can be seen from (3), the topics are not modelled directly with documents w d , but with cluster centers θ m . Therefore if clustering structure is already known, PCP will learn β by using the richer information contained in cluster centers, not just individual documents. In this sense, PCP can be explained as a projection model with structured data and is very attractive because clustered documents are supposed to contain less noise and coarser granularity. This will make the projection more accurate and faster.</p><p>As a projection model, PCP is more general than pLSI because document likelihood (3) is well defined and generalizable to new documents. Although LDA uses similar equation as (3), the topic mixture θ d is only sampled for current document and no inter-similarity of documents is directly modelled. Documents can only exchange information via the hyperparameter for θ d 's, and thus its effect to β is only implicit. On the contrary, PCP directly models similarity of documents and incorporate all information to learn β.</p><p>As discussed in <ref type="bibr" target="#b1">[2]</ref>, projection β can be smoothed by putting a common prior to all the rows. If only the maximum a posteriori (MAP) estimate of β is considered, the effect of smoothing turns out to add a common factor to each entry of β before normalization each row. This is also straightforward in PCP model and we will not discuss it in detail for simplicity. In the experiments we will use this smoothing technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference and Learning</head><p>In this section we consider model inference and learning. As seen from <ref type="figure" target="#fig_0">Figure 1</ref>, for inference we need to calculate the a posteriori distribution of latent variables ˆ p(π, θ, c, z) := p <ref type="figure">(π, θ, c, z|D, α, λ, β)</ref>, including both effects of clustering and projection. Here for simplicity we denote π, θ, c, z as groups of π m , θ m , c d , z d,n , respectively. This requires to compute (1), where the integral is however analytically infeasible. A straightforward Gibbs sampling method can be derived, but it turns out to be very slow and inapplicable to high dimensional discrete data like text, since for each word we have to sample a latent variable z. Therefore in this section we suggest an efficient variational method by introducing variational parameters for latent variables <ref type="bibr" target="#b8">[9]</ref>. Then we can maximize the data likelihood by iteratively updating these parameters and obtain a variational EM algorithm until convergence. The interesting thing is that this algorithm is equivalent to performing clustering and projection iteratively, which we will discuss in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variational EM Algorithm</head><p>The idea of variational EM algorithm is to propose a joint distribution q(π, θ, c, z) for latent variables conditioned on some free parameters, and then enforce q to approximate the a posteriori distributions of interests by minimizing the KLdivergence D KL (qˆpqˆp) with respect to those free parameters. We propose a variational distribution q over latent variables as the following</p><formula xml:id="formula_6">q(π, θ, c, z|η, γ, ψ, φ) = q(π|η) M m=1 q(θ m |γ m ) D d=1 q(c d |ψ d ) N d n=1 q(z d,n |φ d,n ), (4)</formula><p>where η, γ, ψ, φ are groups of variational parameters, each tailoring the variational a posteriori distribution to each latent variable. In particular, η specifies an M -dim. Dirichlet for π, γ m specifies a K-dim. Dirichlet for distinct θ m , ψ d specifies an M -dim. multinomial for indicator c d of document d, and φ d,n specifies a K-dim. multinomial over latent topics for word w d,n . It turns out that minimization of the KL-divergence is equivalent to maximization of a lower bound of the log likelihood ln p(D|α, λ, β), derived by applying Jensen's inequality <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_7">L q (D) = E q [ln p(π|α)] + M m=1 E q [ln p(θ m |λ)] + D d=1 E q [ln p(c d |π)] + D d=1 N d n=1 E q [ln p(w d,n |z d,n , β)p(z d,n |θ, c d )] − E q [ln q(π, θ, c, z)].<label>(5)</label></formula><p>The optimum is found by setting the partial derivatives with respect to each variational and model parameter to be zero, which corresponds to the variational E-step and M-step, respectively. In the following we separate these equations into two parts and interpret them from the perspective of clustering and projection, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Updates for Clustering</head><p>As we mentioned in Section 3.2, the specific variables for clustering are documentcluster assignments c d , cluster centers θ m , and cluster probabilities π. It turns out that their corresponding variational parameters are updated as follows:</p><formula xml:id="formula_8">ψ d,m ∝ exp K k=1 Ψ (γ m,k ) − Ψ ( K i=1 γm,i) N d n=1 φ d,n,k + Ψ (ηm) − Ψ ( M i=1 ηi) ,<label>(6)</label></formula><formula xml:id="formula_9">γ m,k = D d=1 ψ d,m N d n=1 φ d,n,k + λ k , ηm = D d=1 ψ d,m + α M ,<label>(7)</label></formula><p>where Ψ (·) is the digamma function, the first derivative of the log Gamma function. ψ d,m are the a posteriori probabilities p(c d = m) that document d belongs to cluster m, and define a soft cluster assignment for each document. γ m,k characterize the cluster centers θ m and are basically the kth coordinate of θ m on the topic space. Finally η m control the mixing weights for clusters and define the probability of cluster m. φ d,n,k are the variational parameters that measure the a posteriori probability that word w d,n in document d is sampled from topic k. They are related to projection of words and assumed fixed at the moment. These equations seem to be complicated and awful, but they turn out to be quite intuitive and just follow the standard clustering procedure. In particular, -ψ d,m is seen from <ref type="formula" target="#formula_8">(6)</ref> to be a multiplication of two factors p 1 and p 2 , where p 1 includes the γ terms in the exponential and p 2 the η terms. Since η m controls the probability of cluster m, p 2 acts as a prior term for ψ d,m ; p 1 can be seen as the likelihood term, because it explicitly measures the probability of generating w d from cluster m by calculating the inner product of projected features and cluster centers. Therefore, (6) directly follows from Bayes' rule, and a normalization term is needed to ensure M m=1 ψ d,m = 1. -γ m,k is updated by summing over the prior position λ k and the empirical location, the weighted sum of projected documents that belong to cluster k. -Similar to γ m,k , η k is empirically updated by summing over the belongingnesses of all documents to cluster k. α/M acts as a prior or a smoothing term, shared by all the clusters.</p><p>Since these parameters are coupled, clustering is done by iteratively updating (6) and <ref type="bibr" target="#b6">(7)</ref>. Note that the words are incorporated into the clustering process only via the projected features N d n=1 φ d,n,k . This means that the clustering is performed not in word space, but in the more informative topic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Updates for Projection</head><p>If ψ, γ, η are fixed, projection parameters φ and β are updated as:</p><formula xml:id="formula_10">φ d,n,k ∝ β k,w d,n exp M m=1 ψ d,m Ψ (γ m,k ) − Ψ ( K i=1 γm,i) ,<label>(8)</label></formula><formula xml:id="formula_11">β k,j ∝ D d=1 N d n=1 φ d,n,k δj(w d,n ),<label>(9)</label></formula><p>where δ j (w d,n ) = 1 if w d,n takes word index j, and 0 otherwise. Please recall that φ d,n,k is the a posteriori probability that word w d,n is sampled from topic k, and β k,j measures the probability of generating word j from topic k. Normalization terms are needed to ensure K k=1 φ d,n,k = 1 and V j=1 β k,j = 1, respectively. Update (9) for β k,j is quite intuitive, since we just sum up all the documents that word j occurs, weighted by their generating probabilities from topic k. For update of φ d,n,k in (8), β k,w d,n is the probability that topic k generates word w d,n and is thus the likelihood term; the rest exponential term defines the prior, i.e., the probability that document d selects topic k. This is calculated by taking into account the clustering structure and summing over all cluster centers with corresponding soft weights. Therefore, the projection model is learned via clusters of documents, not simply individual ones. Finally we iterate <ref type="formula" target="#formula_10">(8)</ref> and <ref type="formula" target="#formula_11">(9)</ref> until convergence to obtain the optimal projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>As guaranteed by variational EM algorithm, iteratively performing the given clustering and projection operations will improve the data likelihood monotonically until convergence, where a local maxima is obtained. The convergence is usually very fast, and it would be beneficial to initialize the algorithm using some simple projection models like pLSI.</p><p>The remaining parameters α and λ control the mixing weights π and cluster centers θ m a priori, and they can also be learned by setting their partial derivatives to zero. However, there are no analytical updates for them and we have to use computational methods like Newton-Raphson method as in <ref type="bibr" target="#b1">[2]</ref>.</p><p>The PCP model can also be seen as a Bayesian generalization of the TTMM model <ref type="bibr" target="#b9">[10]</ref>, where π and θ m are directly optimized using EM. Treating them as variables instead of parameters would bring more flexibility and reduce the impact of overfitting. We summarize the PCP algorithm in the following table:   <ref type="formula" target="#formula_8">(6)</ref>; (b) Update cluster centers γ m,k and mixing weights η k by (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Projection: Calculate the clustering term</head><formula xml:id="formula_12">M m=1 ψ d,m Ψ (γ m,k ) − Ψ ( K i=1 γm,i)</formula><p>for each document d and iterate the following steps until convergence: (a) Update word projections φ d,n,k by (8); (b) Update projection matrix β by (9). 4. Update α and λ if necessary. 5. Calculate the lower bound (5) and go to Step 2 if not converged.</p><p>In this section we illustrate experimental results for the PCP model. In particular we compare it with other models in the following three perspectives:</p><p>-Document Modelling: How good is the generalization in PCP model? -Word Projection: Is the projection really improved in PCP model? -Document Clustering: Will the clustering be better in PCP model?</p><p>We will make comparisons based on two text data sets. The first one is Reuters-21578, and we select all the documents that belong to the five categories moneyfx, interest, ship, acq and grain. After removing stop words, stemming and picking up all the words that occur at least in 5 documents, we finally obtain 3948 documents with 7665 words. The second data set consists of four groups taken from 20Newsgroup, i.e., autos, motorcycles, baseball and hockey. Each group has 1000 documents, and after the same preprocessing we get 3888 documents with 8396 words. In the following we use "Reuters" and "Newsgroup" to denote these two data sets, respectively. Before giving the main results, we illustrate one case study for better understanding of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study</head><p>We run the PCP model on the Newsgroup data set, and set topic number K = 50 and cluster number M = 20. α is set to 1 and λ is set with each entry being 1/K. Other initializations are chosen randomly. The algorithm runs until the improvement on L q (D) is less than 0.01% and converges after 10 steps. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates part of the results. In (a) 10 topics are shown with 10 words that have highest assigned probabilities in β. The topics are seen to be very meaningful and each defines one projection for all the words. For instance, topic 5 is about "bike", and 1, 7, 9 are all talking about "car" but with different subtopics: 1 is about general stuffs of car; 7 and 9 are specifying car systems and purchases, respectively. Besides finding topic 6 that covers general terms for "hockey", we even find two topics that specify the hockey teams in US (4) and Canada (8). These topics provide the building blocks for document clustering. <ref type="figure" target="#fig_3">Figure 2</ref>(b) gives the 4 cluster centers that have highest probabilities after learning. They define topic mixtures over the whole 50 topics, and for illustration we only show the given 10 topics as in (a). Darker color means higher weight. It is easily seen that they are corresponding to the 4 categories autos, motorcycles, baseball and hockey, respectively. If we sort all the documents with their true labels, we obtain the document-cluster assignment matrix as shown in <ref type="figure" target="#fig_3">Figure 2</ref>(c). Documents that belong to different categories are clearly separated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Document Modelling</head><p>In this subsection we investigate the generalization of PCP model. We compare PCP with pLSI and LDA on the two data sets, where 90% of the data are used  for training and the rest 10% are held out for testing. The comparison metric is perplexity, which is conventionally used in language modelling and defined as</p><formula xml:id="formula_13">Perp(D test ) = exp(− ln p(D test )/ d |w d |)</formula><p>, where |w d | is the length of document d. A lower perplexity score indicates better generalization performance.</p><p>We follow the formula in <ref type="bibr" target="#b1">[2]</ref> to calculate perplexity for pLSI. For PCP model, we take the similar approach as in LDA, i.e., we run the variational inference and calculate the lower bound (5) as the likelihood term. M is set to be the number of training documents for initialization. As suggested in <ref type="bibr" target="#b1">[2]</ref>, a smoothing term for β is used and optimized for LDA and PCP. All the three models are trained until the improvement is less than 0.01%. We compare all three algorithms using different K's, and the results are shown in <ref type="table" target="#tab_2">Table 2</ref>. PCP outperforms both pLSI and LDA in all the runs, which indicates that the model fits the data better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Word Projection</head><p>All the three models pLSI, LDA and PCP can be seen as projection models and learn the mapping β. To compare the quality, we train a support vector machine  (SVM) on the low-dimensional representations of these models and measure the classification rate. For pLSI, the projection for document d is calculated as the a posteriori probability of latent topics conditioned on d, p(z|d). This can be computed using Bayes' rule as p(z|d) ∝ p(d|z)p(z). In LDA it is calculated as the a posteriori Dirichlet parameters for d in the variational E-step <ref type="bibr" target="#b1">[2]</ref>. In PCP model this is simply the projection term N d n=1 φ d,n,k which is used in clustering. We train a 10-topic model on the two data sets and then train a SVM for each category. Note that we are reducing the feature space by 99.8%. In the experiments we gradually improve the number of training data from 10 to 200 (half positive and half negative) and randomize 50 times. The performance averaged over all categories is shown in <ref type="figure" target="#fig_4">Figure 3</ref> with mean and standard deviation. It is seen that PCP obtains better results and learns a better word projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Document Clustering</head><p>In our last experiment we demonstrate the performance of PCP model on document clustering. For comparison we implement the original version of NMF algorithm <ref type="bibr" target="#b10">[11]</ref>, and a k-means algorithm that uses the learned features by LDA. The k-means and PCP algorithms are run with the true cluster number, and we tune the dimensionality K to get best performance.</p><p>The experiments are run on both two data sets. The true cluster number is 5 for Reuters and 4 for Newsgroup. For comparison we use the normalized mutual information <ref type="bibr" target="#b12">[13]</ref>, which is just the mutual information divided by the maximal entropy of the two cluster sets. The results are given in <ref type="table" target="#tab_3">Table 3</ref>, and it can be seen that PCP performs the best on both data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper proposes a probabilistic clustering-projection model for discrete cooccurrence data, which unifies clustering and projection in one probabilistic model. Iteratively updating the two operations turns out to be the variational inference and learning under Bayesian treatments. Experiments on two text data sets show promising performance for the proposed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Informal sampling process (left) and plate model (right) for the PCP model. In the left figure, dark arrows show dependencies between entities and the dashed line separates the clustering and projection effects. In the plate model, rectangle means independent sampling, and hidden variables and model parameters are denoted as circles and squares, respectively. Observed quantities are marked in black.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 (right), using standard plate model. c d takes value {1, . . . , M } and acts as the indicator variable saying which cluster document d takes on out of the M clus- ters. All the model parameters are α, λ, β and amount to 1 + M + K × (V − 1). The following procedure describes the sampling process for the whole corpus: 1. Choose model parameter α, λ, β; 2. For the mth cluster, choose θ m ∼ Dir(λ), m = 1, . . . , M ; 3. Choose the mixing weight π ∼ Dir(α/M, . . . , α/M ); 4. For each document w d : (a) Choose a cluster m with mixing weights π, and obtain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>The PCP Algorithm 1. Initialize model parameters α, λ and β. Choose M &gt; 0 and K &gt; 0. Choose initial values for φ d,n,k , γ m,k and η k . 2. Clustering: Calculate the projection term N d n=1 φ d,n,k for each document d and iterate the following steps until convergence: (a) Update cluster assignments ψ d,m by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A case study of PCP model on Newsgroup data. (a) shows 10 topics and 10 associated words for each topic with highest generating probabilities. (b) shows 4 clusters and the topic mixture on the 10 topics. Darker color means higher value. (c) gives the assignments to the 4 clusters for all the documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Classification results on Reuters (a) and Newsgroup (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 . Perplexity comparison for pLSI, LDA and PCP on Reuters and Newsgroup</head><label>2</label><figDesc></figDesc><table>Reuters 
Newsgroup 
K 
5 
10 20 30 40 50 
5 
10 
20 
30 
40 
50 
pLSI 1995 1422 1226 1131 1128 1103 2171 2018 1943 1868 1867 1924 
LDA 1143 892 678 599 562 533 2083 1933 1782 1674 1550 1513 
PCP 1076 882 670 592 555 527 2039 1871 1752 1643 1524 1493 

(a) 
(b) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 . Comparison of clustering using different methods</head><label>3</label><figDesc></figDesc><table>NMF 
LDA+k-means 
PCP 
Reuters 
0.246 
0.331 
0.418 
Newsgroup 
0.522 
0.504 
0.622 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is multinomial PCA multi-faceted clustering or dimensionality reduction?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perttu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the 9th International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Harshman. Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-clustering documents and words using bipartite spectral graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive dimension reduction for clustering high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual ACM SIGIR Conference</title>
		<meeting>the 22nd Annual ACM SIGIR Conference<address><addrLine>Berkeley, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical models for co-occurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<idno>AIM-1625</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Theme Topic Mixture Model: A Graphical Model for Document Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the parts of objects with nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Document clustering via adaptive subspace iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dirichlet enhanced latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 10th International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
