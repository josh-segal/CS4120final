<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Probabilistic Principal Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
							<email>spyu@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Corporate Technology, Information and Communications</orgName>
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<email>kai.yu@siemens.com</email>
							<affiliation key="aff1">
								<orgName type="department">Corporate Technology, Information and Communications</orgName>
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
							<email>volker.tresp@siemens.com</email>
							<affiliation key="aff1">
								<orgName type="department">Corporate Technology, Information and Communications</orgName>
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
							<email>kriegel@dbs.ifi.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Science</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
							<email>mingrui.wu@tuebingen.mpg.de</email>
							<affiliation key="aff2">
								<orgName type="department">MPI for Biological Cybernetics</orgName>
								<orgName type="institution">T ¨ ubingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Probabilistic Principal Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H3 [Information Storage and Retrieval]: Content Anal- ysis and Indexing-Indexing methods General Terms Algorithms</term>
					<term>Theory</term>
					<term>Measurement</term>
					<term>Performance Keywords Dimensionality reduction</term>
					<term>Principal component analysis</term>
					<term>Su- pervised projection</term>
					<term>Semi-supervised projection</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S 2 PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S 2 PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data mining problems often suffer from the high dimensionality of the data, for the reason of learnability or computational efficiency. Therefore dimensionality reduction, which is also called feature transformation or document indexing, is of great importance and has been extensively studied (see, e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref>). The most popular method is probably the principal component analysis (PCA), which performs a singular value decomposition (SVD) to the data matrix and obtains the sub-eigenspace with large singular values <ref type="bibr" target="#b9">[10]</ref>.</p><p>Traditional dimensionality reduction methods are unsupervised, i.e., they only focus on observations or input data. However, in discriminant analysis where the prediction value or output is available, it would be more helpful to incorporate this information into the mapping and derive a supervised projection for input data. Since this projection is designed for the specific prediction problem, it could be substantially different from unsupervised projection. A more interesting setting is semi-supervised projection where we have only part of input data labeled, along with a large number of unlabeled data. This is often true in real world problems because labeling is expensive, or unlabeled data are very easy to obtain. An ideal projection method should be able to take into account both the observed labeling information and the unlabeled inputs. There exist many supervised projection algorithms in the literature such as linear discriminative analysis (LDA), partial least squares (PLS) and many others (see, e.g., <ref type="bibr" target="#b7">[8]</ref> for an overview). However, these methods cannot incorporate the unlabeled data into the mapping, which will cause problems when we have only very few labeled points.</p><p>In this paper we propose a supervised PCA model called SPPCA, which extends the probabilistic PCA model <ref type="bibr" target="#b16">[17]</ref> to incorporate label information into the projection. SPPCA takes into account not only the inter-covariance between inputs and outputs, but also the intra-covariance of both. More interestingly, the model can be further extended to model unlabeled data as well, which we call semi-supervised PCA or S 2 PPCA. This model allows us to elegantly use all the available information to define the mapping. We derive an efficient EM learning algorithm for both models, and provide some theoretical justifications for the model behavior. Experimental results on various learning tasks show promising performance for both SPPCA and S 2 PPCA models. This paper is organized as follows. After reviewing previous work in Section 2, we formally introduce SPPCA and S 2 PPCA models in Section 3 and derive an EM learning algorithm in Section 4. We then presents some theoretical justifications in Section 5 with further discussions. Finally Section 6 illustrates experimental results and Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PREVIOUS WORK</head><p>In this section we review some previous work on unsupervised and supervised projections. In what follows we consider a set of N objects (e.g., images), and each object n is described by an M -dimensional feature vector xn ∈ X ⊂ R M . For dimensionality reduction we aim to derive a mapping Ψ : X → Z which maps the input features into a K-dimensional space (K &lt; M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Projection</head><p>Probably the most popular unsupervised projection is principal component analysis <ref type="bibr">(PCA)</ref>. <ref type="bibr">Let X = [x1, . . . , xN ]</ref> denote the input matrix after centralization, i.e., we subtract the sample mean from each input. In PCA we want to find the principal components which illustrate the directions with maximal variances of the data. Let X = VDU be the singular value decomposition (SVD) of X, where V and U are N × N and M × M column orthogonal matrices, respectively, and D is N × M diagonal matrix with singular values sorted in descending order along the diagonal. Then it is known that the first K columns of U, which we denote UK , defines the mapping Ψ. The projections of X onto the principal component space are given as VK DK , where VK contains the first K columns of V, and DK is the top left</p><formula xml:id="formula_0">K × K sub matrix of D.</formula><p>Unlike PCA which is maximizing the global covariance of the data, there also exist "local" projection algorithms such as locally linear embedding <ref type="bibr" target="#b14">[15]</ref>, which tries to preserve the local structure of the data after projecting into a low dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supervised Projection</head><p>When each data point n is associated with not only low level features xn but also some outputs y n ∈ Y ⊂ R L (i.e., classification or regression targets), unsupervised projection such as PCA may be not able to project the data into useful directions (see <ref type="figure" target="#fig_2">Figure 2</ref>). Therefore many supervised projection methods are introduced to make use of the output information. Linear discriminant analysis (LDA) focuses on multi-class classification and finds projection directions that separate the data best (see <ref type="bibr" target="#b3">[4]</ref>). The number of projection dimensions is however limited by L−1. Partial least squares (PLS) <ref type="bibr" target="#b17">[18]</ref> originates from regression analysis and finds the directions of maximal covariance between inputs and outputs sequentially. It however ignores the intra covariance of either inputs or outputs, and its generalization performance on new dimensions of outputs is restricted (see discussions in <ref type="bibr" target="#b5">[6]</ref>). Other related works include <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> which consider this problem from different perspectives.</p><p>In the situations where we have few labeled points and a large amount of unlabeled data, all these supervised projection methods are however not able to use the unlabeled data. This is often the case in computer vision, information retrieval and bioinformatics, where labeling is expensive and unlabeled data are sufficient and cheap to obtain. We call this setting the semi-supervised projection, and in the next section we will propose a model which can deal with semisupervised projection naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE SPPCA MODEL</head><p>In this section we first review a probabilistic model for PCA in Section 3.1, and then present our supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic PCA (PPCA)</head><p>While PCA originates from the analysis of data variances, in statistics community there exists a probabilistic explanation for PCA, which is called probabilistic PCA or PPCA in the literature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>. PPCA is a latent variable model and defines a generative process for each object x as (see <ref type="figure" target="#fig_0">Figure 1</ref>(a) for an illustration)</p><formula xml:id="formula_1">x = Wxz + µ x + x,</formula><p>where z ∈ R K are called the latent variables, and Wx is a M × K matrix called factor loadings. In this probabilistic model, latent variables z are conventionally assumed as a Gaussian distribution with zero mean and unit variance, i.e., z ∼ N (0, I), and x defines a noise process which also takes an isotropic Gaussian form as x ∼ N (0, σ 2 x I), with σ 2</p><p>x the noise level. Additionally, we have parameters µ x ∈ R M which allow non-zero means for the data. It is shown that PPCA has strong connections to PCA. In particular when σ 2</p><p>x → 0, the projections of data x onto the K-dimensional principal subspace in PCA are identical to the latent variables z up to a rotation and scaling factor <ref type="bibr" target="#b16">[17]</ref>. We summarize the related results in the following proposition without proof, since this is simply a corollary of Theorem 2 in Section 5.</p><formula xml:id="formula_2">Proposition 1. Let Sx = 1 N N n=1 (xn − µ x )(xn − µ x )</formula><p>be the sample covariance matrix for data {xn} N n=1 , and λ1 ≥ . . . ≥ λM be its eigenvalues with eigenvectors u1, . . . , uM , then if the latent space in PPCA model is K-dimensional,</p><formula xml:id="formula_3">(i)</formula><p>The maximum likelihood estimate of Wx is given as</p><formula xml:id="formula_4">Wx = UK (ΛK − σ 2 x I) 1 2 R,</formula><p>where</p><formula xml:id="formula_5">ΛK = diag(λ1, . . . , λK ), UK = [u1, . . . , uK ],</formula><p>and R is an arbitrary K × K orthogonal matrix.</p><p>(ii) The mean projections z * for new input x * is given as</p><formula xml:id="formula_6">z * = R ΛK − σ 2 x I 1 2 Λ −1 K U K (x * − µ x ).</formula><p>As a probabilistic framework, PPCA provides additional benefits over PCA such as a fast EM learning procedure, a principled way of handling missing entries, and a possibility of considering mixture of PCA models. PPCA is also closely related to factor analysis models, but the modeling perspectives are different (see <ref type="bibr" target="#b16">[17]</ref> for more discussions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised PPCA (SPPCA)</head><p>The key point of PPCA model is that all the M dimensions of x are conditionally independent given the latent variables z, due to the isotropic property of the noise process. This indicates that the principal components in PPCA are the K latent variables which best explain the data covariance.</p><p>When supervised information is available, each object x is associated with an output value y ∈ Y, e.g., y ∈ R for regression task and y ∈ {+1, −1} for classification task. In general we believe there are covariances between input space X and output space Y (since otherwise the supervised learning task is not learnable), and it is reasonable to extend PPCA to model this covariance as well. Furthermore, when there are more than one learning tasks (i.e., in a multi-task learning setting <ref type="bibr" target="#b4">[5]</ref>), the covariances between different tasks can also be modeled by latent variables. We now formally describe our proposed model family which we call the supervised probabilistic principal component analysis (SPPCA). Let the number of outputs be L, and each object x be associated with an output vector y = [y1, . . . , yL] ∈ Y ⊂ R L . In SPPCA the observed data (x, y) is generated from a latent variable model as</p><formula xml:id="formula_7">x = Wxz + µ x + x, y = f (z, Θ) + y,</formula><p>where f (z, Θ) = [f1(z, θ1), . . . , fL(z, θL)] encode the values of L deterministic functions f1, . . . , fL with parameters Θ = {θ1, . . . , θL}. Here z ∼ N (0, I) are the latent variables shared by both inputs x and outputs y, and the two noise models are independent to each other and both defined as isotropic Gaussians: x ∼ N (0, σ 2 x I), y ∼ N (0, σ 2 y I). We use two noise levels σ 2 x and σ 2 y for inputs and outputs, respectively, and it is also straightforward to define different noise levels for different outputs if desired. See <ref type="figure" target="#fig_0">Figure 1</ref>(b) for an illustration of the model.</p><p>In SPPCA model we keep the nice property of conditional independence, i.e., all the input and output dimensions are conditionally independent to each other given the latent variables. If we integrate out the latent variables z, the likelihood of observation (x, y) is obtained as</p><formula xml:id="formula_8">P (x, y) = P (x, y|z)P (z) dz = P (x|z)P (y|z)P (z) dz,</formula><p>where z ∼ N (0, I), and from the latent variable model,</p><formula xml:id="formula_9">x|z ∼ N (Wxz + µ x , σ 2 x I), y|z ∼ N (f (z, Θ), σ 2 y I).</formula><p>(1) After observing N pairs, the likelihood of all the observa-</p><formula xml:id="formula_10">tions D = {(xn, y n )} N n=1 , with i.i.d. assumption, is simply P (D) = N n=1 P (xn, y n ).</formula><p>In the following we consider the simplest model in this family, i.e., we assume each function f , = 1, . . . , L, is linear in z: </p><formula xml:id="formula_11">f (z, θ ) = w y z + µ y ,</formula><formula xml:id="formula_12">= [w 1 y , . . . , w L y ] and µ y = [µ 1 y , . . . , µ L y ] .</formula><p>The reason why we choose this form for f is that the EM learning is simple (see the next section), and we have closed form solution (see Section 5). We will discuss other forms of f in Section 5.3 which may need special approximation techniques.</p><p>Let us denote</p><formula xml:id="formula_13">W = Wx Wy , µ = µ x µ y , Φ = σ 2 x I 0 0 σ 2 y I ,</formula><p>then based on the model assumption, it is easily seen that (x, y) are jointly Gaussian distributed, with mean µ and covariance Φ + WW . All the parameters for the SPPCA model are Ω := {Wx, Wy, µ x , µ y , σ 2 x , σ 2 y }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-Supervised PPCA (S 2 PPCA)</head><p>In SPPCA model, we assume we observe both the inputs x and outputs y for every data point. In many real world problems, however, we may only observe the outputs for a small portion of data, and have many unlabeled data in which only inputs x are known. This may be because some measurements are unobservable, the labeling cost is too high, or simply we have a large amount of unlabeled data available. Learning in this situation is in general called semi-supervised learning. For learning a projection, an ideal model would incorporate both the unlabeled inputs and the partially labeled outputs to define the mapping.</p><p>This can be easily done under the SPPCA framework. Let the number of labeled and unlabeled data points be N1 and N2, respectively, with N = N1 + N2. The whole observation is now</p><formula xml:id="formula_14">D = D1 D2 = {(xn, y n )} N 1 n=1 {x n } N n =N 1 +1 .</formula><p>The likelihood, with the independence assumption of all the data points, is calculated as where P (xn, y n ) is calculated as in SPPCA model, and P (x n ) = P (x n |z n )P (z n ) dz n . Due to its applicability to semi-supervised projection, we call it semi-supervised PPCA or S 2 PPCA in this paper. <ref type="figure" target="#fig_0">Figure 1</ref>(c) illustrates this model.</p><formula xml:id="formula_15">P (D) = P (D1)P (D2) = N 1 n=1 P (xn, y n ) N n =N 1 +1 P (x n ),</formula><p>Under the additional assumptions that all the f 's are linear, it can be easily checked that all the likelihood terms in this product are Gaussians. This makes the model easy to learn. Other forms of f will be discussed in Section 5.3.</p><p>When N2 = 0, S 2 PPCA degrades to SPPCA which is purely supervised. This means one can view SPPCA as a special case of S 2 PPCA model with no unlabeled data. From the perspective of probabilistic modeling, S 2 PPCA can also be viewed as an SPPCA model where all the y's for the N2 unlabeled points are missing. Due to this close relationship, in the following we use SPPCA to denote both models unless clearly specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Projections in SPPCA Models</head><p>Analogous to the PPCA model, in SPPCA models the projection of data point x is directly given in the latent variables z. If we know all the parameters Ω, calculating this projection is simply an inference problem. To do this we can apply Bayes' rule and calculate the a posteriori distribution of z. Therefore we can obtain not only the mean projection vector, but also the uncertainty of the projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Projection for Fully Observed Data</head><p>When both inputs x and outputs y are observed, we can calculate the a posteriori distribution of z given (x, y) as</p><formula xml:id="formula_16">P (z|x, y) ∝ P (x, y|z)P (z) = P (x|z)P (y|z)P (z).<label>(2)</label></formula><p>Since all the three terms on the right hand side are Gaussians, this distribution is also a Gaussian N (µ z , Σz), with</p><formula xml:id="formula_17">µz = A −1 1 σ 2 x W x (x − µ x ) + 1 σ 2 y W y (y − µ y ) , Σz = A −1 ,</formula><p>where A is a K × K matrix defined as</p><formula xml:id="formula_18">A = 1 σ 2 x W x Wx + 1 σ 2 y W y Wy + I.<label>(3)</label></formula><p>This means that the projection is µ z with uncertainty Σz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Projection for Pure Input Data</head><p>For a test data x * that has no output information, what are the most likely latent variables z * ? This can also be done using Bayes' rule</p><formula xml:id="formula_19">P (z * |x * ) ∝ P (x * |z * )P (z * ).<label>(4)</label></formula><p>This turns out again to be a Gaussian N (µ z|x , Σ z|x ), with</p><formula xml:id="formula_20">µ z|x = (W x Wx + σ 2 x I) −1 W x (x * − µ x ), Σ z|x = σ 2 x (W x Wx + σ 2 x I) −1 .</formula><p>This result looks similar as that in PPCA model, but the projection is now supervised because the learning of Wx is influenced by those observed outputs. This is clarified in the next section and will be theoretically justified in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LEARNING IN SPPCA MODELS</head><p>Learning in probabilistic models reduces to maximizing the data (log) likelihood with respect to all the model parameters. In case of SPPCA model, the log likelihood of the whole observation D is</p><formula xml:id="formula_21">L = N n=1 log P (xn|zn)P (y n |zn)P (zn) dzn.</formula><p>For SPPCA analytical solution exists, and we summarize it later in Theorem 2. For S 2 PPCA model, however, there is no analytical solution since all the outputs for the unlabeled data are missing. Fortunately we can derive an EM algorithm <ref type="bibr" target="#b2">[3]</ref> which is applicable to both models. The EM algorithm iterates the two steps expectation (Estep) and maximization (M-step) until convergence, and it is guaranteed to find a local minima of the data likelihood.</p><p>In the E-step, we fix the model parameters (Ω for SPPCA models) and calculate the expected distributions of latent variables (all the zn's for SPPCA models), and in the Mstep we fix this distribution and maximize the complete data likelihood with respect to the model parameters. As will be discussed later, EM learning for SPPCA models is important because it can deal with very large data sets, and it has, in particular for SPPCA model with no unlabeled points, no local minima problem up to a rotation factor (see Section 5). For simplicity we only outline the update equations in the following and omit details (see <ref type="bibr" target="#b16">[17]</ref> for a similar derivation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EM Learning for SPPCA</head><p>We first consider the SPPCA model without unlabeled data. In the E-step, for each data point n we estimate the distribution of zn given observation (xn, y n ). This is done using (2), and we calculate the sufficient statistics as</p><formula xml:id="formula_22">zn = A −1 1 σ 2 x W x (xn − µ x ) + 1 σ 2 y W y (y n − µ y ) ,<label>(5)</label></formula><formula xml:id="formula_23">znz n = A −1 + znzn ,<label>(6)</label></formula><p>where · denotes the expectation with respect to the posterior distribution P (zn|xn, y n ) given in (2). In the M-step, we maximize the complete log-likelihood</p><formula xml:id="formula_24">˜ L = N n=1 P (zn|xn, y n ) log P (xn|zn)P (y n |zn)P (zn) dzn</formula><p>with respect to the model parameters, holding P (zn|xn, y n ) fixed from the E-step. This can be done by setting the partial derivatives with respect to each parameter to be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning in SPPCA Model -Primal Form</head><p>Require: N data points {(xn, y n )} N n=1 with inputs xn ∈ R M and outputs y n ∈ R L . A desired dimension K &lt; M . 1: Calculate the sample means (7) and center the data via xn ⇐ xn − µ x , y n ⇐ y n − µ y . 2: Initialize model parameters Ω randomly. 3: repeat 4:</p><p>{E-step} 5:</p><p>for n = 1 to N do 6:</p><p>Calculate sufficient statistics (5) and (6)</p><note type="other">; 7: end for 8: {M-step} 9: Update Wx and Wy via (8); 10: Update σ 2 x and σ 2 y via (9) and (10); 11: until the change of Ω is smaller than a threshold. Output: Parameters Ω and projection vectors {zn} N n=1 which are obtained from E-step. For test data x * , the mean projection z</note><formula xml:id="formula_25">* = (W x Wx + σ 2 x I) −1 W x (x * − µ x ).</formula><p>For means of x and y we have</p><formula xml:id="formula_26">˜ µ x = 1 N N n=1</formula><p>xn,</p><formula xml:id="formula_27">˜ µ y = 1 N N n=1</formula><p>y n ,</p><p>which are just the sample means. Since they are always the same in all EM iterations, we can center the data by subtracting these means in the beginning and ignore these parameters in the learning process. So for simplicity we change the notations xn and y n to be the centered vectors in the following. The mapping matrices Wx and Wy are updated as</p><formula xml:id="formula_29">Wx = X ZC −1 , Wy = Y ZC −1 ,<label>(8)</label></formula><p>where for clarity we use matrix notations</p><formula xml:id="formula_30">X = [x1, . . . , xN ] , Y = [y 1 , . . . , y N ] and Z = [z1, . . . , zN ] .</formula><p>Matrix C is defined to be the sum of all second-order sufficient statistics of the data, i.e., C = N n=1 znz n . Finally the noise levels are updated as˜σ</p><formula xml:id="formula_31">as˜ as˜σ 2 x = 1 M N N n=1 xn 2 + tr( W x WxC) − 2 tr(X WxZ )<label>(9)</label></formula><formula xml:id="formula_32">˜ σ 2 y = 1 LN N n=1 y n 2 + tr( W y WyC) − 2 tr(Y WyZ )<label>(10)</label></formula><p>where · denotes vector 2-norm, and tr(·) denotes matrix trace. The whole algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EM Learning for S 2 PPCA</head><p>The log likelihood of the observations in S 2 PPCA model is a sum of two parts: L1 = log P (D1) which contains all the labeled points, and L2 = log P (D2) which includes all unlabeled points. Therefore in E-step we need to deal with them differently. For a labeled points (xn, y n ) ∈ D1, the latent variables zn are estimated via (5) and (6), the same as in SPPCA model. For an unlabeled point x n ∈ D2, the distribution of z n is only conditioned on input x n , which can be calculated via (4), with sufficient statistics (the data are assumed centered already):</p><formula xml:id="formula_33">z n = (W x Wx + σ 2 x I) −1 W x x n ,<label>(11)</label></formula><formula xml:id="formula_34">z n z n = (W x Wx + σ 2 x I) −1 + z n z n ,<label>(12)</label></formula><p>Algorithm 2 Learning in S 2 PPCA Model -Primal Form</p><p>Require: N1 labeled data points {(xn, y n )} N 1 n=1 and N2 unlabeled points {x n } N n =N 1 +1 , with inputs x ∈ R M and observed outputs y ∈ R L . A desired dimension K &lt; M . 1: Calculate the sample means (7) and center the data via xn ⇐ xn − µ x , y n ⇐ y n − µ y , x n ⇐ x n − µ x . 2: Initialize model parameters Ω randomly. 3: repeat 4:</p><p>{E-step} 5:</p><p>for n = 1 to N1 do 6:</p><p>Calculate <ref type="formula" target="#formula_22">(5)</ref> and <ref type="formula" target="#formula_23">(6)</ref> for labeled data n; 7:</p><p>end for 8:</p><p>for n = N1 + 1 to N do 9:</p><p>Calculate <ref type="formula" target="#formula_33">(11)</ref> and <ref type="formula" target="#formula_16">(12)</ref> for unlabeled data n ; 10:</p><p>end for 11:</p><p>{M-step} 12:</p><p>Update Wx and Wy via <ref type="formula" target="#formula_18">(13)</ref> and <ref type="formula" target="#formula_19">(14)</ref>;</p><note type="other">13: Update σ 2 x and σ 2 y via (15) and (16); 14: until the change of Ω is smaller than a threshold. Output: Parameters Ω and projection vectors {zn} N n=1 which are obtained from E-step. For test data x * , the mean projection z</note><formula xml:id="formula_35">* = (W x Wx + σ 2 x I) −1 W x (x * − µ x ).</formula><p>where here · denotes the expectation with respect to the posterior distribution P (z n |x n ) given in (4). The M-step is similarly obtained by setting the partial derivatives of the complete log likelihood with respect to each parameter to zero. For the two mapping matrices, we have the updates</p><formula xml:id="formula_36">Wx = (X 1 Z1 + X 2 Z2)(C1 + C2) −1 ,<label>(13)</label></formula><formula xml:id="formula_37">Wy = Y Z1C −1 1 ,<label>(14)</label></formula><p>where X1, Z1, C1 are defined for labeled data, i.e.,</p><formula xml:id="formula_38">X1 = [x1, . . . , xN 1 ] , Z1 = [z1, . . . , zN 1 ] , C1 = N 1 n=1</formula><p>znz n , and X2, Z2, C2 are similarly defined for unlabeled data. It is seen that the update for Wx depends on both labeled data and unlabeled data, while Wy only depends on the labeled data. Updates for the noise levels are similar to those in SPPCA model, except that for σ 2</p><p>x we need to consider both labeled data and unlabeled data:</p><formula xml:id="formula_39">˜ σ 2 x = 1 M N N n=1 xn 2 + tr W x Wx(C 1 + C 2 ) −2 tr Wx(Z 1 X 1 + Z 2 X 2 ) ,<label>(15)</label></formula><formula xml:id="formula_40">˜ σ 2 y = 1 LN 1 N 1 n=1 y n 2 + tr( W y WyC 1 ) − 2 tr(Y WyZ 1 ) .<label>(16)</label></formula><p>The whole algorithm is summarized in Algorithm 2. When N2 = 0, i.e., we have no unlabeled data, the learning algorithm reduces to SPPCA learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EM Learning in Dual Form</head><p>It is known that when the number of data points is less than the number of features, i.e., N &lt; M , it is more efficient to consider the dual solution for PCA in which we perform SVD to the Gram matrix K = XX . The canonical PCA is sometimes called the primal solution. For SPPCA we have a similar dual solution, and it can be directly derived from the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Learning in S 2 PPCA Model -Dual Form</head><p>Require: N1 labeled data points {(xn, y n )} N 1 n=1 and N2 unlabeled points {x n } N n =N 1 +1 , with inputs x ∈ R M and observed outputs y ∈ R L . A desired dimension K &lt; M . 1: Calculate Gram matrix K with Kij = x i xj and center it using <ref type="bibr">(27)</ref>. Center the outputs via y n ⇐ y n − µ y . 2: Initialize Z, C and model parameters Ω randomly. 3: repeat {The EM-step} 4:</p><p>Calculate Z1 and C1 using <ref type="formula" target="#formula_16">(21)</ref> and <ref type="formula" target="#formula_16">(22)</ref>; 5:</p><p>Calculate Z2 and C2 using (23) and (24); 6:</p><p>Update σ 2 x and σ 2 y via (25) and (26); 7: until the change of Ω is smaller than a threshold. Output: Parameters Ω and projection vectors {zn} N n=1 . The mean projection z * for test data</p><formula xml:id="formula_41">x * is z * = C Z KZ + σ 2 x C 2 −1 Z k(X, x * ) where k(X, x * ) = [x 1 x * , . . . , x N x * ]</formula><p>and is centered via (28).</p><p>EM learning in previous subsections. To avoid the tedious mathematics in the main text, we put the derivation details into Appendix and summarize the algorithm in Algorithm 3.</p><p>Since SPPCA can be viewed as a special case of S 2 PPCA, here we only give the algorithm for S 2 PPCA model. One important observation in the dual solution is that all the calculation involving input data X can be done via innerproduct, e.g., in the Gram matrix K we have Kij = x i xj. This motivates us to consider non-linear PCA where we first map the data into a new feature space (via, e.g., basis functions), and then perform PCA in that space with a proper definition of inner-product. This is the idea behind kernel PCA <ref type="bibr" target="#b15">[16]</ref>, and we put detailed discussion into Appendix. In the dual form, the time complexity is O(mN 2 K) plus O(N 2 M ) which is the one-time calculation of Gram matrix, and the space complexity is O N 2 . Both of them are now quadratic in the number of data points N . The time for projecting a test data point is now O(N M ). Similar to the case for PCA, in situations where M &gt; N , i.e., we have more features than the number of data points, the dual form is more efficient than the primal form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computational Issues</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">THEORETICAL JUSTIFICATION</head><p>In this section we provide some theoretical analysis for SPPCA model and show how the supervised information influences the projection. The proofs are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Primal Form Solution</head><p>Recall that matrix Φ is a (M + L) × (M + L) diagonal matrix with all the noise levels in diagonal, i.e., Φ = diag(σ 2 x , . . . , σ 2 x , σ 2 y , . . . , σ 2 y ). For SPPCA model we obtain the following analytical solutions for mapping matrix Wx 1 Note that we only need to calculate the diagonal entries for matrix trace in the updates for noise levels. and Wy. This makes it easier to compare SPPCA with related models such as PCA.</p><p>Theorem 2. Let S denote the normalized sample covariance matrix for centered observations {(xn, y n )} N n=1 , i.e., </p><formula xml:id="formula_42">S = 1 N N n=1 Φ − 1 2 xn y n xn y n Φ − 1 2 = 1 σ 2 x Sx 1 σxσy</formula><formula xml:id="formula_43">Wy = σyUy(ΛK − I) 1 2 R,<label>(17)</label></formula><p>where ΛK = diag(λ1, . . . , λK ), Ux (Uy) contains the first M (last L) rows of [u1, . . . , uK ], and R is an arbitrary K × K orthogonal rotation matrix.</p><p>(ii) Projection z * for centered new input x * is given as</p><formula xml:id="formula_45">z * = 1 σx R (Λ K − I) − 1 2 U x Ux + (Λ K − I) −1 −1 U x x * .</formula><p>In the special case that L = 0, the model is unsupervised and</p><formula xml:id="formula_46">S = 1 σ 2 x</formula><p>Sx holds. Then (17) degrades to σxUx(ΛK − I) 1 2 R, which recovers the PPCA solution. Ux is seen to be column orthogonal in this case, and the mapping z * of x * is (scaled) standard PCA mapping when σ 2</p><p>x → 0 and R = I. This proves Proposition 1 which is a corollary of this theorem.</p><p>When L &gt; 0, SPPCA solutions explain not only the sample covariance of inputs Sx, but also the intra-covariance of outputs Sy (if L &gt; 1) and the inter-correlations between inputs and outputs, Sxy and Syx. Therefore, one column of Wx is the direction that best explains the whole system from the perspective of inputs, and thus are biased by the outputs. Unlike the case of PCA, the learned Wx in SP-PCA needs not to be column orthogonal. This means we are only learning an affine mapping for x. If necessary, it is straightforward to find the orthogonal basis by performing SVD to matrix Wx.</p><p>In both SPPCA and PPCA the learned Wx has an arbitrary rotation factor R. This means the mapping is invariant under a rotation of latent space, as can be seen from the equation for z * . Therefore the SPPCA model can only find the latent principal subspace, which has been mentioned in <ref type="bibr" target="#b16">[17]</ref> for PPCA. Thus the EM algorithm in Section 4 can find different mappings with different initializations, but they define the same subspace and do not change the structure of projected data. If necessary, this ambiguity can be removed by eigen-decomposing W</p><p>x Wx + W y Wy = R (ΛK − I)R and uncovering the rotation factor R.</p><p>A final comment is that Theorem 2 may be not applicable to large-scale problems since we have to form a big square matrix of size M + L. This is however not necessary for the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dual Form Solution</head><p>In the dual form, we do not obtain the mapping matrix Wx, but the projected vectors directly. The following theorem gives the analytical solution in the dual form.</p><formula xml:id="formula_47">Theorem 3. Let K = 1 σ 2 x K + 1 σ 2</formula><p>y YY , and λ1 ≥ . . . ≥ λN be its eigenvalues with eigenvectors v1, . . . , vN , then if the latent space in SPPCA model is K-dimensional, the following results hold: 2 (i) The projection vectors of training data, which are encoded in rows of matrix Z, are calculated as</p><formula xml:id="formula_48">Z = √ N VK D 1 2 R,<label>(19)</label></formula><p>where <ref type="bibr">v1, . . . , vK ]</ref>, and R is an arbitrary K × K orthogonal rotation matrix.</p><formula xml:id="formula_49">D := I−N Λ −1 K , ΛK = diag(λ1, . . . , λK ), VK = [</formula><p>(ii) Projection z * for new input x * is given as</p><formula xml:id="formula_50">z * = √ N R D − 1 2 V K KVK + D −1 V K k(X, x * ), with k(X, x * ) centered via (28).</formula><p>It is seen from this theorem that when there is no output in SPPCA, i.e.,</p><formula xml:id="formula_51">K = 1 σ 2</formula><p>x K, SPPCA reduces to the dual form of PCA as desired. This theorem directly applies for non-linear mappings if the inner-product is defined in a reproducing kernel Hilbert space (RKHS) <ref type="bibr" target="#b15">[16]</ref>, and leads to the kernel PCA solution when L = 0.</p><p>Theorem 3 presents a nice explanation for SPPCA model: we just use the outputs to modify the Gram matrix of input data, and control the trade-off via the ratio of noise levels. The model complexity remains the same (i.e., quadratic in N ) no matter how many output dimensions we have. Our previous work <ref type="bibr" target="#b19">[20]</ref> shares this same property and derives the supervised projection via an eigenvalue problem. But it cannot be elegantly extended to semi-supervised projections, and has problems to deal with large-scale data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussions</head><p>Previous two subsections give some theoretical results for SPPCA model. There exists however no such a closed-form solution for S 2 PPCA. One can only empirically analyze the behavior of this model.</p><p>In the EM learning algorithm we are learning the maximum likelihood (ML) estimates for the two mapping matrices Wx and Wy. In the probabilistic framework we can also assign a prior to them to reduce overfitting. For instance, we can assign an isotropic Gaussian prior for each column of Wx, and if we consider the maximum a posteriori (MAP) estimate this prior corresponds to a smooth term in the update equations. For simplicity we do not consider this prior here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section we empirically investigate the performance of SPPCA models. The supervised tasks here are multiclass classification and multi-label classification. Our basic setting is that we train a supervised projection model using the input features and label information, and then test the classification performance for test data using the projected features. Since the test data are assumed known in the training phase, for S 2 PPCA we will be able to use these unlabeled data to train the mapping. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets</head><p>We test the proposed model on 9 multi-class and 2 multilabel classification problems. These problems include face recognition, gene classification and text categorization. Some statistics of these data sets are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For face recognition we use four data sets Yale, ORL, PIE and YaleB (the extended Yale Face Database B). <ref type="bibr" target="#b2">3</ref> The Yale data set contains 165 gray-scale images in GIF format of 15 individuals. There are 11 images per subject, one per different facial expression or configuration such as center-light, left-light, happy or surprised. The ORL database contains 10 different images of each of 40 distinct subjects. For some subjects, the images were taken at different times with varying lighting and facial details. The PIE databases we use contains 170 images for each of 68 people. These images are the five near frontal poses under different illuminations and expressions. For YaleB we have 38 individuals and around 64 near frontal images under different illuminations per individual. All the face images are manually aligned, cropped and resized to 32 × 32 pixels. We then normalize each image to have Euclidean distance 1.</p><p>We consider three gene expression data sets 11 Tumors, 14 Tumors and Lung Cancer for gene classification. <ref type="bibr" target="#b3">4</ref> The 11 Tumors describes 11 various human tumor types, and 14 Tumors describes 14 tumor types with 12 normal tissue types. For Lung Cancer we need to classify 4 lung cancer types and normal tissues. The characteristic of these data is that the number of data points is small, but the input dimensionality is very high.</p><p>The two textual data sets we use are taken from 20News-group and TDT2. 20Newsgroup contains 20,000 news articles posted in 20 news groups. We remove the words that occur less than 5 times, and obtain 19,928 documents with 25,284 words. The TDT2 corpus we use consists of the documents collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11,021 documents which are classified into 96 semantic categories. In our experiments, we keep the largest 20 categories and remove those documents that are assigned to more than one categories. This leaves us 8,692 documents with totally 35,452 words. For both of these data sets we </p><formula xml:id="formula_52">(c) Projection dimension K = 20.</formula><p>use TF-IDF features and normalize each document to have Euclidean distance 1. For multi-label classification we use Yeast and RCV1. The Yeast data set is formed by micro-array expression data and phylogenetic profiles with 2,417 genes in total and 103 input dimensions. There are 14 groups and each gene can belong to multiple groups. The other data is a subset of the RCV1-v2 text data set, provided by Reuters and corrected by Lewis et al. <ref type="bibr" target="#b10">[11]</ref>. We use the training set provided by Lewis, which contains 103 labels, 23,149 documents with 15,500 words after we remove words that occur less than 5 times. We also extract TF-IDF features and normalize each document to have Euclidean distance 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setting</head><p>For the multi-class classification tasks, we randomly pick up a small number of labeled data points for training (2 for those data sets with less than 500 data points, and 5 for the others), and test the classification error rate on the unlabeled data. We will in general compare the following six algorithms if applicable:</p><p>• PCA: Unsupervised projection. Note that we use both the training and test data to derive the mapping.</p><p>• LDA: Linear discriminant analysis.</p><p>• PLS: Partial least squares.</p><p>• SPPCA: Supervised probabilistic PCA.</p><p>• S 2 PPCA: Semi-supervised probabilistic PCA. We allow S 2 PPCA to use the test data to train the mapping.</p><p>• Full: All the features are used without projection.</p><p>For all the projection methods, we project the data into a space of 5, 10 and 20 dimensions, and train a nearestneighbor classifier for the test points using new features with Euclidean distance. For Full we directly train the nearestneighbor classifier using original features. For PLS, SPPCA and S 2 PPCA, we translate the one column output to the "One of C" setting, i.e., each class has one column with binary labels. For multi-label classification, we pick up 5 positive examples from each label to obtain the training data. For all projection methods we project to 5, 10 and 20 dimensions, and then train a linear SVM classifier for each label. The comparison metrics are F1-Macro, F1-Micro and AUC (Area Under ROC Curve) score. The candidate algorithms are almost the same as multi-class setting, except LDA which is not applicable to this task. The C in SVM is fixed as 100, and from our experience it is not sensible for all algorithms.</p><p>In all these comparisons, the iteration number for SPPCA and S 2 PPCA is set to 1000. Both the noise levels σ 2 x and σ 2 y are set to 10 −5 initially. It turns out that PLS gets memory problems when applied to large dimensions. We repeat each experiments 50 times independently, 5 and the results are illustrated in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of Results</head><p>The first observation is that in most cases the supervised</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. DERIVATION OF DUAL FORM</head><p>We only focus on S 2 PPCA model here. Let C = C1 + C2, and</p><formula xml:id="formula_53">X = X1 X2 , Z = Z1 Z2 , K = XX = X1X X2X = K1 K2</formula><p>then <ref type="formula">(</ref> </p><p>by collecting zn in columns and transposing it. Sufficient statistics (6) can be written in terms of C1:</p><formula xml:id="formula_55">C1 = N 1 n=1 znz n = N1 A −1 + Z 1 Z1.<label>(22)</label></formula><p>Similarly, we obtain the following two updates for unlabeled data:</p><formula xml:id="formula_56">Z2 = 1 σ 2 x K2ZC −1 1 σ 2 x C −1 Z KZC −1 + I −1 , (23) C2 = N2 1 σ 2 x C −1 Z KZC −1 + I −1 + Z 2 Z2.<label>(24)</label></formula><p>In M-step, we only need to update variances σ 2 x and σ 2 y as </p><formula xml:id="formula_57">˜ σ 2 x = 1 M N tr(K) − tr( Z K Z C −1 ) ,<label>(25)</label></formula><p>which can be easily verified from <ref type="formula" target="#formula_31">(9)</ref> and <ref type="formula" target="#formula_32">(10)</ref>. Therefore, it is seen that all interesting terms in the EM algorithm take input data into account only via the innerproduct. This nice property allows us to extend the linear SPPCA model to non-linear mappings by first mapping the input data into a new feature space (via, e.g., basis functions) and then performing SPPCA in that space. This can also be done via kernel trick for which we only need to define a kernel function κ(·, ·) for each pair of input data <ref type="bibr" target="#b15">[16]</ref>.</p><p>Since we are now working on centered data in the feature space, we can achieve this by modifying K as</p><formula xml:id="formula_59">K = K − 1 N 11 K − 1 N K11 + 1 N 2 11 K11 ,<label>(27)</label></formula><p>where 1 denotes the all one column vector of length N <ref type="bibr" target="#b15">[16]</ref>. For kernel vector k(X, x * ), it can also be centered by</p><formula xml:id="formula_60">˜ k = k − 1 N 11 k − 1 N K1 + 1 N 2 11 K1.<label>(28)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PROOF OF THEOREM 2</head><p>We give a sketch here. The mapping matrices are obtained by finding a fixed point in the EM algorithm in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PROOF OF THEOREM 3</head><p>We give a sketch here. We define B = ZC −1 and rewrite (21) and <ref type="formula" target="#formula_16">(22)</ref>   <ref type="formula" target="#formula_31">(19)</ref> with R = Q , and the update equation for new test data can be easily obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of the three models PPCA, SPPCA and S 2 PPCA. X and Y denote respectively the input and output matrices, where each row is one data point. f 1 x , . . . , f M x are the M input features, and f 1 y , . . . , f L y are the L outputs. On the top f 1 z , . . . , f K z are the K latent variables in each model. They are all in circles because they are variables in the probabilistic models. The arrows denote probabilistic dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where the parameters θ = {w y , µ y } include the linear co- efficients and intercept. Then we can group all the f 's and write f (z, Θ) = Wyz + µ y , a similar form as the generative model for x where Wy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Projection directions for a toy data. They are fully labeled on the left, and only partially labeled on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>In the primal form (i.e., Algorithm 1 and 2), the time complexity for both algorithms is O m(M + L)N K , with m the number of iterations. 1 It is linear in the number of data points N and the input dimension M . The space complexity is O (M + L)N , which is also linear in both N and M . The projection for a test data point is just a linear operation and costs O(M K) time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and λ1 ≥ . . . ≥ λ (M +L) be its eigenvalues with eigenvectors u1, . . . , u (M +L) , then if the latent space in SPPCA model is K-dimensional, the following results hold: (i) The maximum likelihood estimates of Wx and Wy are Wx = σxUx(ΛK − I) 1 2 R,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>13) can be written as Wx = X ZC −1 . This leads to W x Wx = C −1 Z KZC −1 , W x x = C −1 Z k(X, x), (20) which are the building blocks for the dual form. The matrix K = XX has each entry the inner-product of data points of corresponding row and column, Kij = x i xj, and k(X, x) = Xx is a N -dimensional column vector with the i-th entry x i x. In the E-step, we first rewrite A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>At the fixed point, this simplifies to SW = WW W + W. Let W = UDV be the SVD of W. Then each column u of U satisfies dSu = (d + d 3 )u, with d the corresponding singular value. Therefore solving an eigenvalue problem for S gives the mapping matrix W, and plugging them into (4) gives the mapping z * for x * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>To solve B we denote the SVD of B KB as QDQ , then we can obtain B B = 1 N QD (I + D) −1 Q after some mathematics. Then define U := BQ √ N D −1/2 (I + D) 1/2 , we have U U = I and U KU = N (I + D). This clearly defines a SVD for K, so by definition we have Λ = N (I + D). Then we can solve for B from U and Λ, which yields B = 1 √ N U I − N Λ −1 1/2 Q . This recovers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Statistics of the multi-class data sets (top) and multi-label data sets (bottom)</head><label>1</label><figDesc></figDesc><table>Category # Data # Dim # Class 
Yale 
Face 
165 
1024 
15 
ORL 
Face 
400 
1024 
40 
PIE 
Face 
11554 
1024 
68 
YaleB 
Face 
2414 
1024 
38 
11 Tumors 
Gene 
174 
12533 
11 
14 Tumors 
Gene 
308 
15009 
26 
Lung Cancer 
Gene 
203 
12600 
5 
20Newsgroup 
Text 
19928 
25284 
20 
TDT2 
Text 
8692 
35452 
20 

Category # Data # Dim # Class 
Yeast 
Gene 
2417 
103 
14 
RCV1 
Text 
23149 
15500 
103 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Results for Multi-class Classification Tasks. Bold face indicates lowest error rate. Symbols indicate that the best method is significantly better than the competitors (p-value 0.01 in Wilcoxon rank sum test).</head><label>2</label><figDesc></figDesc><table>Task 
Full 
PCA 
LDA 
PLS 
SPPCA 
S 2 PPCA 
Yale 
0.5656 ± 0.0394 
0.6690 ± 0.0333 
0.6133 ± 0.0471 
0.6440 ± 0.0383 
0.7007 ± 0.0402 
0.7121 ± 0.0393 
ORL 
0.3308 ± 0.0347 
0.5593 ± 0.0263 
0.5302 ± 0.0444 
0.5505 ± 0.0294 
0.5459 ± 0.0305 
0.5287 ± 0.0286 
PIE 
0.6988 ± 0.0085 
0.9325 ± 0.0032 
0.7066 ± 0.0177 
0.8781 ± 0.0058 
0.8780 ± 0.0116 
0.8452 ± 0.0037 
YaleB 
0.6360 ± 0.0160 
0.9895 ± 0.0023 
0.5328 ± 0.0251 
0.9546 ± 0.0066 
0.9701 ± 0.0088 
0.9800 ± 0.0034 
11 Tumors 
0.3161 ± 0.0566 
0.5409 ± 0.0490 
0.4505 ± 0.0755 
N/A 
0.5226 ± 0.0636 
0.5130 ± 0.0491 
14 Tumors 
0.6084 ± 0.0360 
0.7363 ± 0.0286 
0.7161 ± 0.0481 
N/A 
0.7312 ± 0.0371 
0.7138 ± 0.0296 
Lung Cancer 
0.3680 ± 0.1148 
0.3768 ± 0.0939 
0.3225 ± 0.1658 
N/A 
0.4287 ± 0.1338 
0.3896 ± 0.0923 
20Newsgroup 
0.6135 ± 0.0155 
0.9070 ± 0.0177 
0.9140 ± 0.0208 
N/A 
0.9030 ± 0.0162 
0.9126 ± 0.0116 
TDT2 
0.1875 ± 0.0233 
0.6664 ± 0.0657 
0.7834 ± 0.0782 
N/A 
0.6236 ± 0.0739 
0.3686 ± 0.0349 

(a) Projection dimension K = 5. 

Task 
Full 
PCA 
LDA 
PLS 
SPPCA 
S 2 PPCA 
Yale 
0.5656 ± 0.0394 
0.5993 ± 0.0312 
0.5279 ± 0.0460 
0.5698 ± 0.0386 
0.6101 ± 0.0447 
0.5916 ± 0.0433 
ORL 
0.3308 ± 0.0347 
0.4049 ± 0.0293 
0.3625 ± 0.0468 
0.4048 ± 0.0349 
0.3832 ± 0.0409 
0.3509 ± 0.0287 
PIE 
0.6988 ± 0.0085 
0.8573 ± 0.0051 
0.5496 ± 0.0185 
0.8062 ± 0.0068 
0.7105 ± 0.0161 
0.6942 ± 0.0047 
YaleB 
0.6360 ± 0.0160 
0.9308 ± 0.0046 
0.3846 ± 0.0282 
0.8762 ± 0.0108 
0.7976 ± 0.0242 
0.7986 ± 0.0117 
11 Tumors 
0.3161 ± 0.0566 
0.3682 ± 0.0655 
0.3926 ± 0.0667 
N/A 
0.3801 ± 0.0624 
0.3297 ± 0.0664 
14 Tumors 
0.6084 ± 0.0360 
0.6868 ± 0.0288 
0.6212 ± 0.0430 
N/A 
0.6322 ± 0.0363 
0.6120 ± 0.0331 
Lung Cancer 
0.3680 ± 0.1148 
0.3493 ± 0.0996 
0.3225 ± 0.1658 
N/A 
0.6235 ± 0.1520 
0.3517 ± 0.1063 
20Newsgroup 
0.6135 ± 0.0155 
0.9039 ± 0.0172 
0.8943 ± 0.0292 
N/A 
0.8931 ± 0.0242 
0.8548 ± 0.0138 
TDT2 
0.1875 ± 0.0233 
0.5531 ± 0.0742 
0.6878 ± 0.1068 
N/A 
0.5346 ± 0.0885 
0.2794 ± 0.0327 

(b) Projection dimension K = 10. 

Task 
Full 
PCA 
LDA 
PLS 
SPPCA 
S 2 PPCA 
Yale 
0.5656 ± 0.0394 
0.5437 ± 0.0414 
0.5793 ± 0.0438 
0.5216 ± 0.0435 
0.5093 ± 0.0391 
0.5001 ± 0.0589 
ORL 
0.3308 ± 0.0347 
0.3323 ± 0.0310 
0.2944 ± 0.0398 
0.3366 ± 0.0331 
0.3271 ± 0.0372 
0.2755 ± 0.0286 
PIE 
0.6988 ± 0.0085 
0.7999 ± 0.0060 
0.4352 ± 0.0186 
0.7454 ± 0.0092 
0.5912 ± 0.0146 
0.5361 ± 0.0090 
YaleB 
0.6360 ± 0.0160 
0.8304 ± 0.0096 
0.3004 ± 0.0227 
0.7695 ± 0.0148 
0.5619 ± 0.0276 
0.5652 ± 0.0172 
11 Tumors 
0.3161 ± 0.0566 
0.3267 ± 0.0635 
0.3926 ± 0.0667 
N/A 
0.4470 ± 0.0691 
0.3012 ± 0.0582 
14 Tumors 
0.6084 ± 0.0360 
0.6379 ± 0.0360 
0.5822 ± 0.0388 
N/A 
0.5669 ± 0.0347 
0.5674 ± 0.0372 
Lung Cancer 
0.3680 ± 0.1148 
0.3584 ± 0.0953 
0.3225 ± 0.1658 
N/A 
0.6487 ± 0.1540 
0.4092 ± 0.1107 
20Newsgroup 
0.6135 ± 0.0155 
0.9160 ± 0.0220 
0.8001 ± 0.0425 
N/A 
0.6254 ± 0.0420 
0.6568 ± 0.0146 
TDT2 
0.1875 ± 0.0233 
0.4582 ± 0.1441 
0.1524 ± 0.0622 
N/A 
0.1566 ± 0.0509 
0.1520 ± 0.0210 

</table></figure>

			<note place="foot" n="2"> We use the same notation for eigenvalues as in Theorem 2 because it can be proved that they are identical up to a scaling factor of N .</note>

			<note place="foot" n="3"> See http://www.ews.uiuc.edu/∼dengcai2/Data/data.html. 4 They are available at http://www.gems-system.org.</note>

			<note place="foot" n="5"> For the four face recognition tasks we use the split versions available from the web site.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We proposed a supervised and a semi-supervised PCA in this paper, and derived an efficient EM algorithm for model learning. Empirical results show that the proposed model obtains good performance and scales well for large data sets.</p><p>In this paper we mainly focus on the Gaussian noise model for the outputs y. One can define other likelihood models for specific tasks, e.g., the probit likelihood for classification, but then we lose the nice closed-form solutions as described in Theorem 2 and 3. This is because in E-step of the EM learning the a posteriori distribution of z is no longer a Gaussian (see <ref type="formula">(2)</ref>). To solve this problem we can apply the expectation-propagation (EP) <ref type="bibr" target="#b12">[13]</ref> algorithm to sequentially approximate each likelihood term P (y |z) as a Gaussian for z. Then the approximated a posteriori distribution of z is still a Gaussian, and the EM algorithm can still be applied to find the optimal projection matrices. Empirically comparing this algorithm with the basic ones would be part of the future work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Prediction by supervised principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm (with discussion)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Pattern Classification. Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminant analysis by Gaussian mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="158" to="176" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relations between two sets of variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Beyond Anova: Basics of Applied Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>John Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unifying review of linear Gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computaion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="345" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statisitical Scoiety, B</title>
		<imprint>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Soft modeling by latent variables; the nonlinear iterative partial least squares approach. Perspectives in Probability and Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
		<editor>Honour of M.S. Bartlett</editor>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature extraction via generalized uncorrelated linear discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Janardan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-label informed latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Volker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Annual International ACM SIGIR Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
