<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Database Algorithms for Phase Change Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><surname>Chen</surname></persName>
							<email>shimin.chen@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
							<email>phillip.b.gibbons@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Nath</surname></persName>
							<email>sumann@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit2">Intel Labs Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Database Algorithms for Phase Change Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Phase change memory (PCM) is an emerging memory technology with many attractive features: it is non-volatile, byte-addressable, 2-4X denser than DRAM, and orders of magnitude better than NAND Flash in read latency, write latency, and write endurance. In the near future, PCM is expected to become a common component of the mem-ory/storage hierarchy for a wide range of computer systems. In this paper, we describe the unique characteristics of PCM, and their potential impact on database system design. In particular, we present analytic metrics for PCM endurance, energy, and latency, and illustrate that current approaches for common database algorithms such as B +-trees and Hash Joins are suboptimal for PCM. We present improved algorithms that reduce both execution time and energy on PCM while increasing write endurance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Phase change memory (PCM) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> is an emerging nonvolatile memory technology with many attractive features. Compared to NAND Flash, PCM provides orders of magnitude better read latency, write latency and endurance, <ref type="bibr" target="#b0">1</ref> and consumes significantly less read/write energy and idle power <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. It is byte-addressable, like DRAM memory, but consumes orders of magnitude less idle power than DRAM. PCM offers a significant density advantage over DRAM, which means more memory capacity for the same chip area and also implies that PCM is likely to be cheaper than DRAM when produced in mass market quantities <ref type="bibr" target="#b21">[22]</ref>. While the first wave of PCM products target mobile handsets <ref type="bibr" target="#b23">[24]</ref>, in the near future PCM is expected to become a common component of the memory/storage hierarchy for laptops, PCs, and servers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>An important question, then, is how should database systems be modified to best take advantage of this emerging trend towards PCM? While there are several different proposals for how PCM will fit within the memory hierarchy <ref type="bibr" target="#b9">[10]</ref> (as SATA/PCIe based data storage or DDR3/QPI based memory), recent computer architecture and systems studies all propose to incorporate PCM as the bulk of the system's main memory <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. Thus, in the PCM-DB project <ref type="bibr" target="#b18">[19]</ref>, we are focusing on the use of PCM as the primary main memory for a database system. This paper highlights our initial findings and makes the following three contributions.</p><p>First, we describe the unique characteristics of PCM and its proposed use as the primary main memory (Section 2). Several attractive properties of PCM make it a natural candidate to replace or compliment battery-backed reliable memory for general database systems <ref type="bibr" target="#b17">[18]</ref>, and DRAM in main memory database systems <ref type="bibr" target="#b10">[11]</ref>. However, a unique challenge arises in effectively using PCM: Compared to its read operations, PCM writes incur higher energy consumption, higher latency, lower bandwidth, and limited endurance. Therefore, we identify reducing PCM writes as an important design goal of PCM-friendly algorithms. Note that this is different from the goals of flash-friendly algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, which include reducing the number of erases and random writes at much coarser granularity (e.g., 256KB erase blocks and 4KB flash pages).</p><p>Second, we present analytic metrics for PCM endurance, energy, and latency. While we believe that PCM may have a broad impact on database systems in general, this paper focuses on its impact on core database algorithms. In particular, we use these metrics to design PCM-friendly algorithms for two core database techniques, B + -tree index and hash joins (Section 3). In a nutshell, these algorithms re-organize data structures and trade off an increase in PCM reads for reducing PCM writes, thereby achieving an overall improvement in all three metrics.</p><p>Third, we show experimentally, via a cycle-accurate X86-64 simulator enhanced with PCM support, that our new algorithms significantly outperform prior algorithms in terms of time, energy and endurance (Section 4), supporting our analytical results. Moreover, sensitivity analysis shows that the results hold for a wide range of PCM parameters.</p><p>The paper concludes by discussing related work (Section 5) and highlighting a few of the many interesting open research questions regarding the impact of PCM main memory on database systems (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PHASE CHANGE MEMORY</head><p>In this section, we discuss PCM technology, its properties relative to other memory technologies, its proposed use as  <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. the primary main memory, and the key challenge of overcoming its write limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PCM Technology</head><p>Phase change memory (PCM) is a byte-addressable nonvolatile memory that exploits large resistance contrast between amorphous and crystalline states in so-called phase change materials such as chalcogenide glass. The difference in resistance between the high-resistance amorphous state and the low-resistance crystalline state is typically about five orders of magnitude and can be used to infer logical states of binary data (high represents 0, low represents 1).</p><p>Programming a PCM device involves application of electric current, leading to temperature changes that either SET or RESET the cell, as shown schematically in <ref type="figure" target="#fig_0">Figure 1</ref>. To SET a PCM cell to its low-resistance state, an electrical pulse is applied to heat the cell above the crystalization temperature Tcryst (but below the melting temperature T melt ) of the phase change material. The pulse is sustained for a sufficiently long period for the cell to transition to the crystalline state. On the other hand, to RESET the cell to its highresistance amorphous state, a much larger electrical current is applied in order to increase the temperature above T melt . After the cell has melted, the pulse is abruptly cut off, causing the melted material to quench into the amorphous state. To READ the current state of a cell, a small current that does not perturb the cell state is applied to measure the resistance. At normal temperatures (&lt; 120 • C ≪ Tcryst), PCM offers many years of data retention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using PCM in the Memory Hierarchy</head><p>To see where PCM may fit in the memory hierarchy, we need to know its properties. <ref type="table" target="#tab_0">Table 1</ref>  (technology for today's solid state drives), and HDD (hard disk drives), showing the following points:</p><p>• Compared to DRAM, PCM's read latency is close to that of DRAM, while its write latency is about an order of magnitude slower. PCM offers a density advantage over DRAM. This means more memory capacity for the same chip area, or potentially lower price per capacity. PCM is also more energy-efficient than DRAM in idle mode.</p><p>• Compared to NAND Flash, PCM can be programmed in place regardless of the initial cell states (i.e., without Flash's expensive "erase" operation). Therefore, its sequential and random accesses show similar (far superior) performance. Moreover, PCM has orders of magnitude higher write endurance than Flash.</p><p>Because of these attractive properties, PCM is being incorporated in mobile handsets <ref type="bibr" target="#b23">[24]</ref>, and recent computer architecture and systems studies have argued that PCM is a promising candidate to be used in main memory in future mainstream computer systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows three alternative proposals in recent studies for using PCM in the main memory system <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>. Proposal (a) replaces DRAM with PCM to achieve larger main memory capacity. Even though PCM is slower than DRAM, clever optimizations have been shown to reduce application execution time on PCM to within a factor of 1.2 of that on DRAM <ref type="bibr" target="#b14">[15]</ref>. Both proposals (b) and (c) include a small amount of DRAM in addition to PCM so that frequently accessed data can be kept in the DRAM buffer to improve performance and reduce PCM wear. Their difference is that proposal (b) gives software explicit control of the DRAM buffer <ref type="bibr" target="#b8">[9]</ref>, while proposal (c) manages the DRAM buffer as another level of transparent hardware cache <ref type="bibr" target="#b21">[22]</ref>. It has been shown that a relatively small DRAM buffer (3% size of the PCM storage) can bridge most of the latency gap between DRAM and PCM <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenge: Writes to PCM Main Memory</head><p>One major challenge in effectively using PCM is overcoming the relative limitations of its write operations. Compared to its read operations, PCM writes incur higher energy consumption, higher latency, lower bandwidth, and limited endurance, as discussed next.</p><p>• High energy consumption: Compared to reading a PCM cell, a write operation that SETs or RESETs a PCM cell draws higher current, uses higher voltage, and takes longer time <ref type="figure" target="#fig_0">(Figure 1)</ref>. A PCM write often consumes 6-10X more energy than a read <ref type="bibr" target="#b14">[15]</ref>.</p><p>• High latency and low bandwidth: In a PCM device, the write latency of a PCM cell is determined by the (longer) SET time, which is about 3X slower than a read operation <ref type="bibr" target="#b14">[15]</ref>. Moreover, many PCM prototypes support "iterative writing" of a limited number of bits per iteration in order to limit the instantaneous current level. Several prototypes support ×2, ×4, and ×8 write modes in addition to the fastest ×16 mode <ref type="bibr" target="#b2">[3]</ref>. This limitation is likely to hold in the future as well, especially for PCM designed for power-constrained platforms. Because of the limited write bandwidth, writing 64B of data often requires several rounds of writing, leading to the ∼ 1 µs write latency in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>• Limited endurance: Existing PCM prototypes have a write endurance ranging from 10 6 to 10 8 writes per cell. With a good wear-leveling algorithm, a PCM main memory can last for several years under realistic workloads <ref type="bibr" target="#b20">[21]</ref>. However, because such wear-leveling must be done at the memory controller, the wearleveling algorithms need to have small memory footprints and be very fast. Therefore, practical algorithms are simple and in many cases, their effectiveness significantly decreases in the presence of extreme hot spots in the memory. For example, even with the wear-leveling algorithms in <ref type="bibr" target="#b20">[21]</ref>, continuously updating a counter in PCM in a 4GHz machine with 16GB PCM could wear a PCM cell out in about 4 months (without wear-leveling, the cell could wear out in less than a minute).</p><p>Recent studies proposed various hardware optimizations to reduce the number of PCM bits written <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. For example, the PCM controller can perform data comparison writes, where a write operation is replaced with a read-modify-write operation in order to skip programming unchanged bits <ref type="bibr" target="#b30">[31]</ref>. Another proposal is partial writes for only dirty words <ref type="bibr" target="#b14">[15]</ref>. In both optimizations, when writing a chunk of data in multiple iterations, the set of bits to write in every iteration is often hard-wired for simplicity; if all the hard-wired bits of an iteration are unchanged, the iteration can be skipped <ref type="bibr" target="#b6">[7]</ref>. However, these architectural optimizations reduce the volume of writes by only a factor ∼ 3. We believe that applications (such as databases) can play an important role in complementing such architectural optimizations by carefully choosing their algorithms and data structures in order to reduce the number of writes, even at the expense of additional reads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PCM-FRIENDLY DB ALGORITHMS</head><p>In this section, we consider the impact of PCM on database algorithm design. Specifically, reducing PCM writes becomes an important design goal. We discuss general considerations in Section 3.1. Then we re-examine two core database techniques, B + -tree index and hash joins, in Sections 3.2 and 3.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm Design Considerations</head><p>Section 2 described three candidate organizations of future PCM main memory, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Their main difference is whether or not to include a transparent or software-controlled DRAM cache. For algorithm design purposes, we consider an abstract framework that captures all three candidate organizations. Namely, we focus on a PCM main memory, and view any additional DRAM as just another (transparent or software-controlled) cache in the hierarchy. This enables us to focus on PCM-specific issues.</p><p>Because PCM is the primary main memory, we consider algorithm designs in main memory. There are two traditional design goals for main memory algorithms: (i) low computation complexity, and (ii) good CPU cache performance. Power efficiency has recently emerged as a third design goal. Compared to DRAM, one major challenge in designing PCM-friendly algorithms is to cope with the asymmetry between PCM reads and PCM writes: PCM writes consume much higher energy, take much longer time to complete, and wear out PCM cells (recall Section 2). Therefore, one important design goal of PCM-friendly algorithms is to minimize PCM writes.</p><p>What granularity of writes should we use in algorithm analysis with PCM main memory: (a) bits, (b) words, or (c) cache lines? All three granularities are important for computing PCM metrics. Choice (a) impacts PCM endurance. Choices (a) and (c) affect PCM energy consumption. Choice (b) influences PCM write latency. The drawback of choice (a) is that the relevant metric is the number of modified bits (recall that unmodified bits are skipped); this is difficult to estimate because it is often affected not only by the structure of an algorithm, but also by the input data. Fortunately, there is often a simple relationship between (a) and (b). Denote γ as the average number of modified bits per modified word. γ can be estimated for a given input. Therefore, we focus on choices (b) and (c).</p><p>Let N l (T l ) be the number (latency, resp.) of cache line fetches (a.k.a. cache misses) from PCM, N lw be the number of cache line write backs to PCM, and Nw (Tw) be the number (latency, resp.) of modified words written. Let E rb (E wb ) be the energy for reading (writing, resp.) a PCM bit. Let L be the number of bytes in a cache line. <ref type="table" target="#tab_1">(Table 2</ref> summarizes the notation used in this paper.) We model the key PCM metrics as follows:</p><p>• TotalWear: N umBitsM odif ied = γNw</p><formula xml:id="formula_0">• Energy = 8L(N l + N lw )E rb + γNwE wb • T otalP CM AccessLatency = N l T l + NwTw</formula><p>The total wear and energy computations are straightforward. The latency computation requires explanations. The first part (N l T l ) is the total latency of cache line fetches from PCM. The second part (NwTw) is the estimated impact of cache line write backs to PCM on the total time. In a traditional system, the cache line write backs are performed asynchronously in the background and often completely hidden. Therefore, algorithm analysis typically ignores the write backs. However, we find that because of the asymmetry of writes and reads, PCM write latency can keep PCM busy for a sufficiently long time to stall front-end cache line fetches significantly. A PCM write consists of (i) a read of the cache line from PCM to identify modified words then (ii) writing modified words in possibly multiple rounds. The above computation includes (ii) as NwTw, while the latency of (i) (N lw T l ) is ignored because it is similar to a traditional cache line write back and thus likely to be hidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">B + -Tree Index</head><p>As case studies, we consider two core database techniques for memory-resident data, B + -trees (in the current subsection) and hash joins (in the next subsection), where the main memory is PCM instead of DRAM.</p><p>B + -trees are preferred index structures for memory-resident data because they optimize for CPU cache performance. Previous studies recommend that B + -tree nodes be one or a few cache lines large and aligned at cache line boundaries <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. For DRAM-based main memory, the costs of search/insertion/deletion are similar except in those cases where insertions/deletions incur node splits/merges in the tree. In contrast, for PCM-based main memory, even a normal insertion/deletion that modifies a single leaf node can be more costly than a search in terms of total wear, energy, and elapsed time, because of the writes involved.</p><p>We would like to preserve the good CPU cache performance of B + -trees while reducing the number of writes. A cache-friendly B + -tree node is typically implemented as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), where all the keys in the node are sorted and packed, and a counter keeps track of the number of valid keys in the array. The sorted key array is maintained upon insertions and deletions. In this way, binary search can be applied to locate a search key. However, on average, half of the array must be moved to make space for insertions and deletions, resulting in a large number of writes. Suppose that there are K keys and K pointers in the node, and every key, every pointer, and the counter have size equal to the word size W used in PCM writes. Then an insertion/deletion in the sorted node incurs 2(K/2) + 1 = K + 1 word writes on average.</p><p>To reduce writes, we propose two simple unsorted node organizations as shown in Figures 3(b) and 3(c):</p><p>• Unsorted: As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b), the key array is still packed but can be out of order. A search has to scan the array sequentially in order to look for a match or the next smaller/bigger key. On the other hand, an insertion can simply append the new entry to  the end of the array, then increment the counter. For a deletion, one can overwrite the entry to delete with the last entry in the array, then decrement the counter. Therefore, an insertion/deletion incurs 3 word writes.</p><p>• Unsorted with bitmap: We further improve the unsorted organization by allowing the key array to have holes. The counter field is replaced with a bitmap recording valid locations. An insertion writes the new entry to an empty location and updates the bitmap, using 3 word writes, while a deletion updates only the bit in the bitmap, using 1 word write. A search incurs the instruction overhead of a more complicated search process. For 8-byte keys and pointers, a 64-bit bitmap can support nodes up to 1024 bytes large, which is more than enough for supporting typical cache-friendly B + -tree nodes. Given the pros and cons of the three node organizations, we study the following four variants of B + -trees:</p><p>• Sorted: a normal cache-friendly B + -tree. All the nonleaf and leaf nodes are sorted.</p><p>• Unsorted: a cache-friendly B + -tree with all the nonleaf and leaf nodes unsorted.</p><p>• Unsorted leaf: a cache-friendly B + -tree with sorted non-leaf nodes but unsorted leaf nodes. Because most insertions/deletions do not modify non-leaf nodes, the unsorted leaf nodes may capture most of the benefits.</p><p>• Unsorted leaf with bitmap: This variant is the same as unsorted leaf except that leaf nodes are organized as unsorted nodes with bitmaps. Our experimental results in Section 4 show that the unsorted schemes can significantly improve total wear, energy consumption, and run time for insertions and deletions. Among the three unsorted schemes, unsorted leaf is the best for index insertions and it incurs negligible index search overhead, while unsorted leaf with bitmap achieves the best index deletion performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hash Joins</head><p>One of the most efficient join algorithms, hash joins are widely used in data management systems. Several cachefriendly variants of hash joins are proposed in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref>. Most of these algorithms are based on the Algorithm 1 Existing algorithm: simple hash join. Build phase:</p><p>1: for (i = 0; i &lt; MR; i++) do 2: r= record i in Relation R; 3: insert r into hash table; Probe phase:</p><p>1: for (j = 0; j &lt; MS; j++) do 2: s= record j in Relation S; 3: probe s in the hash </p><formula xml:id="formula_1">P = ⌈(MRLR + MSLS + htsize)/C⌉; 3: for (i = 0; i &lt; MR; i++) do {partition R} 4: r= record i in Relation R; 5:</formula><p>p= hash(r) modulo P ; 6: copy r to partition Rp; 7: for (j = 0; j &lt; MS; j++) do {partition S} 8:</p><p>s= record j in Relation S; 9: p= hash(s) modulo P ; 10: copy s to partition Sp; Join phase:</p><p>1: for (p = 0; p &lt; P ; p++) do 2: join Rp and Sp using simple hash join;</p><p>following two representative algorithms. <ref type="table" target="#tab_2">(Table 3</ref> defines the terms used in describing and analyzing the algorithms.)</p><p>Simple Hash Join. As shown in Algorithm 1, in the build phase, the algorithm scans the smaller build relation R. For every build record, it computes a hash code from the join key, and inserts the record into the hash table. In the probe phase, the algorithm scans the larger probe relation S. For every probe record, it computes the hash code, and probes the hash table. If there are matching build records, the algorithm computes the join results, and sends them to upper level query operators. The cost of this algorithm can be analyzed as in <ref type="table" target="#tab_4">Table 4</ref> with the terms defined in <ref type="table" target="#tab_2">Table 3</ref>. Here, we assume that the hash table does not fit into CPU cache, which is usually the case. We do not include PCM costs for the join results as they are often consumed in the CPU cache by higher-level operators in the query plan tree.</p><p>The cache misses of the build phase are caused by reading all the join keys (min( M R L R L , MR)) and accessing the hash table (MRN hR ). When the record size is small, the first term is similar to reading the entire build relation. When the record size is large, it incurs roughly one cache miss per record. Note that because multiple entries may share a single hash bucket, the lines written back can be a subset of the lines accessed for a hash table visit. For the probe phase, the cache misses are caused by scanning the probe relation ( M S L S L ), accessing the hash table (MSN hS ), and accessing matching build records in a random fashion. The latter can be computed as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. The other computations are straightforward.  Cache Partitioning. When both input relation sizes are fixed, if we reduce the record sizes (LR, LS), then the numbers of records (MR, MS) increase. Therefore, simple hash join incurs a large number of cache misses when record sizes are small. The cache partitioning algorithm solves this problem. As shown in Algorithm 2, in the partition phase, the two input relations (R and S) are hash partitioned so that every pair of partitions (Rp and Sp) can fit into the CPU cache. Then in the join phase, every pair of Rp and Sp are joined using the simple hash join algorithm.</p><note type="other">5: r= record i in Relation R; 6: p= hash(r) modulo P ; 7: append ID i into RList[p]; 8: for (j = 0; j &lt; MS; j++) do {virtually partition S} 9: s= record j in Relation S; 10: p= hash(s) modulo P ; 11: append ID j into SList[p]; Join phase: 1: for (p = 0; p &lt; P ; p++) do {join Rp and Sp} 2: for each i in RList[p] do 3: r= record i in Relation R; 4: insert r into hash table; 5: for each j in SList[p] do 6: s= record j in Relation S; 7: probe s in the hash table; 8: if there</note><p>The cost analysis of cache partitioning is straightforward as shown in <ref type="table" target="#tab_4">Table 4</ref>. Note that we assume that modified cache lines during the partition phase are not prematurely evicted because of cache conflicts. Observe that the number of cache misses using cache partitioning is constant if the relation sizes are fixed. This addresses the above problem of simple hash join. Simple Hash  However, cache partitioning introduces a large number of writes compared to simple hash join: it is writing the amount of data equivalent to the size of the entire input relations. As writes are bad for PCM, we would like to design an algorithm that reduces the writes while still achieving similar benefits of cache partitioning. We propose the following variant of cache partitioning.</p><formula xml:id="formula_2">Build min( M R L R L , M R ) + M R N hR M R HashT able lw M R HashT ablew Probe M S L S L + M S N hS + M S M atchP erS( L R −1 L + 1) 0 0 Cache Partition Partition 2( M R L R L + M S L S L ) M R L R L + M S L S L M R L R W + M S L S W Join M R L R L + M S L S L 0 0 Virtual Partition Partition M R L R L + M S L S L + (M R + M S ) 2 L (M R + M S ) 2 L (M R + M S ) 2 W Join (M R + M S ) 2 L + M R ( L R −1 L + 1) + M S ( L S −1 L + 1) 0 0 (a) Cache accesses (N l ) (b) Total</formula><p>New: Virtual Partitioning. Instead of physically copying input records into partitions, we perform the partitioning virtually. As shown in Algorithm 3, in the partition phase, for every partition, we compute and remember the record IDs that belong to the partition for both R and S. 3 Then in the join phase, we can use the record ID lists to join the records of a pair of partitions in place, thus avoiding the large number of writes in cache partitioning.</p><p>We optimize the record ID list implementation by storing the deltas of two subsequent record IDs to further reduce the writes. As the number of cache partitions is often smaller than a thousand, we find using two-byte integers can encode most deltas. For rare cases with larger deltas, we reserve 0xF F F F to indicate that a full record ID is recorded next.</p><p>The costs of the virtual partitioning algorithm is analyzed in <ref type="table" target="#tab_4">Table 4</ref>. The costs for the partition phase include scanning the two relations as well as generating the record ID lists. The latter writes two bytes per record. In the join <ref type="bibr" target="#b2">3</ref> We assume that there is a simple mapping between a record ID and the record location in memory. For example, if fixed length records are stored consecutively in an array, then the array index can be used as the record ID. If records always start at 8B boundaries, then the record ID of a record can be the record starting address divided by 8. phase, the records are accessed in place. They are essentially scattered in the two input relations. Therefore, we use the formula for unaligned records in <ref type="figure" target="#fig_4">Figure 4</ref> to compute the number of cache misses for accessing the build and probe records. Note that the computation of the number of partitions P in Algorithm 3 guarantees that the total cache lines accessed per pair of Rp and Sp fit into the CPU cache capacity C. Comparisons of the Three Algorithms. <ref type="figure" target="#fig_5">Figure 5</ref> compares the three algorithms analytically using the formulas in <ref type="table" target="#tab_4">Table 4</ref>. We assume R and S have the same record size, and it is a primary-foreign key join (thus M atchP erS = 1). From left to right, <ref type="figure" target="#fig_5">Figures 5(a) to (d)</ref> show the comparison results for four metrics: (a) cache accesses (N l ), (b) total wear, (c) energy, and (d) total PCM access latency. In each figure, we vary the record size from 1 to 500 bytes, and the number of matches per R record (M atchP erR) from 1 to 50. Every point represents a configuration for the hash join. The color of a point shows the best scheme for the corresponding configuration: blue for simple hash join, red for cache partitioning, and green for virtual partitioning. For configurations where virtual partitioning is the best scheme, the contour lines show the relative benefits of virtual partitioning compared to the second best scheme. <ref type="figure" target="#fig_5">Figure 5</ref>(a) focuses on CPU cache performance, which is the main consideration for previous cache-friendly hash join designs. We see that as expected, simple hash join is the best scheme when record size is very large and cache partitioning is the best scheme when record size is small. Compared to simple hash join, virtual partitioning avoids the many cache misses caused by hash table accesses. Compared to cache partitioning, virtual partitioning reduces the number of cache misses during the partition phase, while paying extra cache misses for accessing scattered records in the join phase. As a result, virtual partitioning achieves the smallest number of cache misses for a large number of configurations in the middle between the red and blue points.</p><p>Figures 5(b) to (d) show the comparison results for the three PCM metrics. First of all, we see that the figures are significantly different from <ref type="figure" target="#fig_5">Figure 5(a)</ref>. This means that introducing PCM main memory can significantly impact the relative benefits of the algorithms. Second, very few configurations benefit from cache partitioning because it incurs a large number of PCM writes in the partition phase, adversely impacting its PCM performance. Third, in <ref type="figure" target="#fig_5">Figure 5</ref>(b), virtual partitioning achieves the smallest number of writes when M atchP erR ≤ 18. Virtual partitioning avoids many of the expensive PCM writes in the partition phase of the cache partitioning algorithm. Interestingly, simple hash join achieves the smallest number of writes when M atchP erR ≥ 19. This is because as M atchP erR increases, the number of S records (MS) increases proportionally, leading to a larger number of PCM writes for virtual partitioning, while the number of PCM writes in simple hash join is not affected. The cross-over point is 19 here. Finally, virtual partitioning presents a good balance between cache line accesses and PCM writes, and it excels in energy and total PCM access latency in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>We evaluate our proposed B + -tree and hash join algorithms through cycle-accurate simulations in this section. We start by describing the simulator used in the experiments. Then we present the experimental results for B + -trees and hash joins. Finally, we perform sensitivity analysis for PCM parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Platform</head><p>We extended a cycle-accurate out-of-order X86-64 simulator, PTLsim <ref type="bibr" target="#b19">[20]</ref>, with PCM support. PTLsim is used extensively in computer architecture studies and is currently the only publicly available cycle-accurate simulator for outof-order x86 micro-architectures. The simulator models the details of a superscalar out-of-order processor, including instruction decoding, micro-code, branch prediction, function units, speculation, and a three-level cache hierarchy. PTLsim has multiple use modes; we use PTLsim to simulate single 64-bit user-space applications in our experiments.</p><p>We extended PTLsim in the following ways to model PCM. First, we model data comparison writes for PCM writes. When writing a cache line to PCM, we compare the new line with the original line to compute the number of modified bits and the number of modified words. The former is used to compute PCM energy consumption, while the latter impacts PCM write latency. Second, we model four parallel PCM memory ranks. Accesses to different ranks can be carried out in parallel. Third, we model the details of cache line write back operations carefully. Previously, PTLsim assumes that cache line write backs can be hidden completely, and does not model the details of this operation. Because PCM write latency is significantly longer than its read latency, cache line write backs may actually keep the PCM busy for a sufficiently long time to stall front-end cache line fetches. Therefore, we implemented a 32-entry FIFO write queue in the on-chip memory controller, which keeps track of dirty cache line evictions and performs the PCM writes asynchronously in the background. <ref type="table" target="#tab_6">Table 5</ref> describes the simulation parameters. The cache hierarchy is modeled after the recent Intel Nehalem processors <ref type="bibr" target="#b13">[14]</ref>. The PCM latency and energy parameters are based on a previous computer architecture study <ref type="bibr" target="#b14">[15]</ref>. We adjusted the latency in cycles according to the 3GHz processor frequency and the DDR3 bus latency. The word size of 8 bytes per iteration of write operations is based on <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">B + -Tree Index</head><p>We implemented four variants of B + -trees as described in Section 3.2: sorted, unsorted, unsorted leaf, and unsorted leaf with bitmap. <ref type="figure" target="#fig_7">Figure 6</ref> compares the four schemes for common index operations. In every experiment, we populate the trees with 50 million entries. An entry consists of an 8-byte integer key and an 8-byte pointer. We populate the nodes 75% full initially. We randomly shuffle the entries in all unsorted nodes so that the nodes represent the stable situations after updates. Note that the total tree size is over 1GB, much larger than the largest CPU cache (8MB). For the insertion experiments, we insert 500 thousand random new entries into the trees back to back, and report total wear in number of PCM bits modified, PCM energy consumption in millijoules, and execution time in cycles for the entire operation. Similarly, we measure the performance of 500 thousand back-to-back deletions for the deletion experiments, and 500 thousand back-to-back searches for the search experiments. We vary the node size of the trees. As suggested by previous studies, the best tree node sizes are a few cache lines large <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. Since a one-line (64B) node can contain only 3 entries, which makes the tree very deep, we show results for node sizes of 2, 4, and 8 cache lines.</p><p>The sub-figures in <ref type="figure" target="#fig_7">Figure 6</ref> are arranged as a 3x3 matrix. Every row corresponds to a node size. Every column corresponds to a performance metric. In every sub-figure, there are three groups of bars, corresponding to the insertion, deletion, and search experiments. The bars in each group show the performance of the four schemes. (Note that search does not incur any wear.) We observe the following points in <ref type="figure" target="#fig_7">Figure 6</ref>.</p><p>First, compared to the conventional sorted trees, all the three unsorted schemes achieve better total wear, energy consumption, and execution time for insertions and deletions, the two index operations that incur PCM writes. The sorted trees pay the cost of moving the sorted array of entries in a node to accommodate insertions and deletions. In contrast, the unsorted schemes all save PCM writes by allowing entries to be unsorted upon insertions and deletions. This saving increases as the node size increases. Therefore, 5.0E+7</p><p>1.0E+8</p><p>1.5E+8 m񮽙bits񮽙modified 0.0E+0</p><p>5.0E+7</p><p>1.0E+8</p><p>1.5E+8 insert delete search num񮽙bits񮽙modified 0.0E+0</p><p>5.0E+7</p><p>1.0E+8</p><p>1.5E+8 insert delete search num񮽙bits񮽙modified 0.0E+0</p><p>5.0E+7</p><p>1.0E+8</p><p>1.5E+8 insert delete search (50 million entries in trees; 75% full; "insert": inserting 500 thousand random keys; "delete": randomly deleting 500 thousand existing keys; "search": searching for 500 thousand random keys) the performance gaps widen as the node size grows from 2 cache lines to 8 cache lines. Second, compared to the conventional sorted trees, the scheme with all nodes unsorted suffers from slower search time by a factor of 1.13-1.46X because the hot, top tree nodes stay in CPU cache, and a search incurs a lot of instruction overhead in the unsorted non-leaf nodes. In contrast, the two schemes with only unsorted leaf nodes achieve similar search time as the sorted scheme.</p><p>Third, comparing the two unsorted leaf schemes, we see that unsorted leaf with bitmap achieves better total wear, energy, and time for deletions. This is because unsorted leaf with bitmap often only needs to mark one bit in a leaf bitmap for a deletion (and the total wear is about 5E5 bits modified), while unsorted leaf has to overwrite the deleted entry with the last entry in a leaf node and update the counter in the node. On the other hand, the unsorted leaf with bitmap suffers from slightly higher insertion time because of the instruction overhead of handling the bitmap and the holes in a leaf node.</p><p>Overall, we find that the two unsorted leaf schemes achieve the best performance. Compared to the conventional sorted B + -tree, the unsorted leaf schemes improve total wear by a factor of 7.7-436X, energy consumption by a factor of 1.7-2.5X, and execution time by a factor of 2.0-2.5X for insertions and deletions, while achieving similar search performance. If the index workload consists of mainly insertions and searches (with the tree size growing), we recommend the normal unsorted leaf. If the index workload contains a lot of insertions and a lot of deletions (e.g., the tree size stays roughly the same), we recommend the unsorted leaf scheme with bitmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hash Joins</head><p>We implemented the three hash join algorithms as discussed in Section 3.3: simple hash join, cache partitioning, and virtual partitioning. We model in-memory join operations, where the input relations R and S are in main memory. The algorithms build in-memory hash tables on the R relation. To hash a R record, we compute an integer hash code from its join key field, and modulo this hash code by the size of the hash table to obtain the hash slot. Then we insert (hash code, pointer to the R record) into the hash slot. Conflicts are resolved through chained hashing. To probe an S record, we compute the hash code from its join key field, and use the hash code to look up the hash ta- ble. When there is an entry with the matching hash code, we check the associated R record to make sure that the join keys actually match. The join results are sent to a high-level operator that consumes the results. In our implementation, the high-level operator simply increments a counter. <ref type="figure" target="#fig_8">Figure 7</ref> compares the three hash join algorithms. The R relation is 50MB large. Both relations have the same record size. We vary the record size from 20B to 100B in <ref type="figure" target="#fig_8">Figures 7(a)</ref>-(c). We vary the number of matches per R record (M atchP erR) from 1 to 8 in <ref type="figure" target="#fig_8">Figures 7(d)</ref>-(f); in other words, the size of S varies from 50MB to 400MB. We report total wear, energy consumption, and execution times for every set of experiments.</p><p>The results in <ref type="figure" target="#fig_8">Figure 7</ref> confirm our analytical comparison in Section 3.3. First, cache partitioning performs poorly in almost all cases because it performs a large number of PCM writes in its partition phase. This results in much higher total wear, higher energy consumption, and longer execution time compared to the other two schemes.</p><p>Second, compared to simple hash join, when varying record size from 20B to 100B, virtual partitioning improves total wear by a factor of 4.7-5.2X, energy consumption by a factor of 2.3-1.4X, and execution time by a factor of 1.24-1.12X. When varying M atchP erR from 1 to 8, virtual partitioning improves total wear by a factor of 8.6-1.5X, energy consumption by a factor of 1.61-1.59X, and execution time by a factor of 1.19-1.11X.</p><p>Overall, virtual partitioning achieves the best performance among the three schemes in all the experiments. Compared to cache partitioning, virtual partitioning avoids copying data in the partition phase by remembering record IDs per partition. Compared to simple hash join, virtual partitioning avoids excessive cache misses due to hash table accesses. Therefore, virtual partitioning achieves good behaviors for both PCM writes and cache accesses. Note that the record size and M atchP erR settings in the experiments fall in the region where virtual partitioning wins in <ref type="figure" target="#fig_5">Figure 5</ref>. Therefore, the experimental results confirm our analytical comparison in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">PCM Parameter Sensitivity Analysis</head><p>In this section, we vary the energy and latency parameters of PCM in the simulator, and study the impact of the parameter changes on the performance of the B + -tree and hash join algorithms. Note that we still assume data comparison writes for PCM write. <ref type="figure">Figure 8</ref> varies the energy consumed by writing a PCM bit (E wb ) from 2pJ to 64pJ. The default value of E wb is 16pJ, and 2pJ is the same as the energy consumed by reading a PCM bit. From left to right, <ref type="figure">Figures 8(a)-(c)</ref> show the impact of varying E wb on the energy consumptions of B + -tree insertions, B + -tree deletions, and hash joins. First, we see that as E wb gets smaller, the curves become flat; the energy consumption is more and more dominated by the cache line fetches for reads and for data comparison writes. Second, as E wb gets larger, the curves increase upwards because the larger E wb contributes significantly to the overall energy consumption. Third, changing E wb does not qualitatively change our previous conclusions. For B + -trees, the two unsorted leaf schemes are still better than sorted B + -trees. Among the three hash join algorithms, virtual partitioning is still the best.   join and virtual partitioning is 6% when Tw is 230 cycles.) We find that previous conclusions still hold for B + -trees and hash joins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>PCM Architecture. As discussed in previous sections, several recent studies from the computer architecture community have proposed solutions to make PCM a replacement for or an addition to DRAM main memory. These studies address various issues including improving endurance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>, improving write latency by reducing the number of PCM bits written <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, preventing malicious wear-outs <ref type="bibr" target="#b25">[26]</ref>, and supporting error corrections <ref type="bibr" target="#b24">[25]</ref>. However, these studies focus on hardware design issues that are orthogonal to our focus on designing efficient algorithms for software running on PCM.</p><p>PCM-Based File Systems. BPFS [9], a file system designed for byte-addressable persistent memory, exploits both the byte-addressability and non-volatility of PCM. In addition to being significantly faster than disk-based file systems (even when they are run on DRAM), BPFS provides strong safety and consistency guarantees by using a new technique called short-circuit shadow paging. Unlike traditional shadow paging file systems, BPFS uses copy-on-write at fine granularity to atomically commit small changes at any level of the file system tree. This avoids updates to the file system triggering a cascade of copy-on-write operations from the modified location up to the root of the file system tree. BPFS is a file system, and hence it does not consider the database algorithms we consider. Moreover, BPFS has been designed for the general class of byte-addressable persistent memory, and it does not consider PCM-specific issues such as read-write asymmetry or limited endurance.</p><p>Battery-Backed DRAM. Battery-backed DRAM (BB-DRAM) has been studied as a byte-addressable, persistent memory. The Rio file cache <ref type="bibr" target="#b15">[16]</ref> uses BBDRAM as the buffer cache, eliminating any need to flush dirty data to disk. The Rio cache has also been integrated into databases as a persistent database buffer cache <ref type="bibr" target="#b17">[18]</ref>. The Conquest file system <ref type="bibr" target="#b28">[29]</ref> uses BBDRAM to store small files and metadata. eNVy <ref type="bibr" target="#b29">[30]</ref> placed flash memory on the memory bus by using a special controller equipped with a BBDRAM buffer to hide the block-addressable nature of flash. WAFL <ref type="bibr" target="#b12">[13]</ref> keeps file system changes in a log in BBDRAM and only occasionally flushes them to disk. While BBDRAM may be an alternative to PCM, PCM has two main advantages over BBDRAM. First, BBDRAM is vulnerable to correlated failures; for example, the UPS battery will often fail either before or along with primary power, leaving no time to copy data out of DRAM. Second, PCM is expected to scale much better that DRAM, making it a better long-term option for persistent storage <ref type="bibr" target="#b2">[3]</ref>. On the other hand, using PCM requires dealing with expensive writes and limited endurance, a challenge not present with BBDRAM. Therefore, BBDRAM-based algorithms do not require addressing the challenges studied in this paper. Main Memory Database Systems and Cache-Friendly Algorithms. Main memory database systems <ref type="bibr" target="#b10">[11]</ref> maintain necessary data structures in DRAM and hence can exploit DRAM's byte-addressable property. As discussed in Section 3.1, the traditional design goals of main memory algorithms are low computation complexity and good CPU cache performance. Like BBDRAM-based systems, main memory database systems do not need to address PCM-specific challenges. In this paper, we found that for PCM-friendly algorithms, one important design goal is to minimize PCM writes. Compared to previous cache-friendly B + -trees and hash joins, our new algorithms achieve significantly better performance in terms of PCM total wear, energy consumption, and execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>A promising non-volatile memory technology, PCM is expected to play an important role in the memory hierarchy in the near future. This paper focuses on exploiting PCM as main memory for database systems. Based on the unique characteristics of PCM (as opposed to DRAM and NAND flash), we identified the importance of reducing PCM writes for optimizing PCM endurance, energy, and performance. Specifically, we applied this observation to database algorithm design, and proposed new B + -tree and hash join designs that significantly improve the state-of-the-art.</p><p>As future work in the PCM-DB project, we are interested in optimizing PCM writes for different aspects of database system designs, including important data structures, query processing algorithms, and transaction logging and recovery. The latter is important for achieving transaction atomicity and durability. BPFS proposed a different solution based on shadow copying and atomic writes <ref type="bibr" target="#b8">[9]</ref>. It is interesting to compare this proposal with conventional database transaction logging, given the goal of reducing PCM writes.</p><p>Moreover, another interesting aspect to study is the finegrain non-volatility of PCM. Challenges may arise in hierarchies where DRAM is explicitly controlled by software. Because DRAM contents are lost upon restart, the relationship between DRAM and PCM must be managed carefully; for example, pointers to DRAM objects should not be stored in PCM. On the other hand, the fine-grain non-volatility may enable new features, such as "instant-reboot" that resumes the execution states of long-running queries upon crash recovery so that useful work is not lost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Currents and timings (not to scale) for SET, RESET, and READ operations on a PCM cell. For phase change material Ge2Sb2T e5, T melt ≈ 610 • C and Tcryst ≈ 350 • C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Candidate main memory organizations with PCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: B + -tree node organizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 3</head><label>3</label><figDesc>Our proposal: virtual partitioning. 2 Partition phase: 1: htsize = MR * hash table per entry metadata size; 2: P = ⌈((MR + MS)2 + MR(LR − 1 + L) + MS(LS − 1 + L) + htsize)/C⌉; 3: initiate ID lists RList[0..P − 1] and SList[0..P − 1]; 4: for (i = 0; i &lt; MR; i++) do {virtually partition R}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Computing average number of cache misses for unaligned records. A record of size = yL + x bytes, y ≥ 0, L &gt; x ≥ 0, has L possible locations relative to cache line boundaries. Accessing the record incurs on average x−1 L 2 + L−x+1 L + y = size−1 L + 1 cache misses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing three hash join algorithms analytically. (LS = LR, M atchP erS = 1, γ = 0.5; the hash table in simple hash join does not fit into cache; hash table access parameters are based on experimental measurements: N hR ≃ N hS = 1.8, HashT able lw = 1.5, HashT ablew = 5.0.) For configurations where virtual partitioning is the best scheme, contour lines show the relative benefits of virtual partitioning compared to the second best scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: B + -tree performance. (50 million entries in trees; 75% full; "insert": inserting 500 thousand random keys; "delete": randomly deleting 500 thousand existing keys; "search": searching for 500 thousand random keys)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Hash join performance. (50MB R table joins S table, varying the record size from 20B to 100B and varying the number of matches per R record from 1 to 8.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 varies</head><label>9</label><figDesc>the latency of writing a word to PCM (Tw) from 230 cycles to 690 cycles. The default Tw is 450 cycles, and 230 is the same latency as reading a cache line from PCM. From left to right, Figures 8(a)-(c) show the impact of varying Tw on the execution times of B + -tree in- sertions, B + -tree deletions, and hash joins. We see that as Tw increases, the performance gaps among different schemes become larger. (The performance gap between simple hash</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Sensitivity analysis: varying latency of writing a word to PCM (Tw).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of memory technologies. 
DRAM 
PCM 
NAND Flash 
HDD 
Read energy 
0.8 J/GB 
1 J/GB 
1.5 J/GB [28] 
65 J/GB 
Write energy 
1.2 J/GB 
6 J/GB 
17.5 J/GB [28] 
65 J/GB 
Idle power 
∼100 mW/GB 
∼1 mW/GB 
1-10 mW/GB 
∼10 W/TB 
Endurance 
∞ 
10 6 − 10 8 
10 4 − 10 5 
∞ 
Page size 
64B 
64B 
4KB 
512B 
Page read latency 
20-50ns 
∼ 50ns 
∼ 25 µs 
∼ 5 ms 
Page write latency 
20-50ns 
∼ 1 µs 
∼ 500 µs 
∼ 5 ms 
Write bandwidth 
∼GB/s per die 50-100 MB/s per die 5-40 MB/s per die ∼200MB/s per drive 
Erase latency 
N/A 
N/A 
∼ 2 ms 
N/A 
Density 
1× 
2 − 4× 
4× 
N/A 
Note: The table contents are based mainly on </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : General Terms.</head><label>2</label><figDesc></figDesc><table>Term 
Description 
Example 
E rb Energy consumed for reading a PCM bit 
2 pJ 
E wb Energy consumed for writing a PCM bit 
16 pJ 
T l 
Latency of accessing a cache line from PCM 
230 cycles 
Tw Additional latency of writing a word to PCM 450 cycles 
C 
size in bytes of the largest CPU cache 
8 MB 
L 
Cache line size in bytes 
64B 
W 
Word size used in PCM writes 
8B 
N l 
Number of cache line fetches from PCM 
-
N lw Number of cache line write backs to PCM 
-
Nw Number of words written to PCM 
-
γ 
Avg number of modified bits per modified word 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Terms used in analyzing hash joins.</head><label>3</label><figDesc></figDesc><table>Term 
Description 
M R , M S 
Number of records in relation R and S, respectively 
L R , L S 
Record sizes in relation R and S, respectively 

N hR 
Number of cache line accesses per hash table visit 
when building the hash table on R records 

N hS 
Number of cache line accesses per hash table visit 
when probing the hash table for S records 
HashT able lw Number of line write backs per hash table insertion 
HashT ablew Number of words modified per hash table insertion 
M atchP erR Number of matches per R record 
M atchP erS Number of matches per S record 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Cost analysis for three hash join algorithms.</head><label>4</label><figDesc></figDesc><table>Algorithm 
Cache Line Accesses from PCM (N l ) 
Cache Line Write Backs (N lw ) Words Written (Nw) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Simulation Setup. Simulator PTLsim enhanced with PCM support Processor Out-of-order X86-64 core, 3GHz CPU cache Private L1D (32KB, 8-way, 4-cycle latency), private L2 (256KB, 8-way, 11-cycle latency), shared L3 (8MB, 16-way, 39-cycle latency), all caches with 64B lines, 64-entry DTLB, 32-entry write back queue</head><label>5</label><figDesc></figDesc><table>PCM 

4 ranks, read latency for a cache line: 230 cycles, 
write latency per 8B modified word: 450 cycles, 
E rb = 2 pJ, E wb = 16 pJ 

</table></figure>

			<note place="foot" n="1"> (Write) endurance is the maximum number of writes for each memory cell. This article is published under a Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits distribution and reproduction in any medium as well allowing derivative works, provided that you attribute the original work to the author(s) and CIDR 2011. 5th Biennial Conference on Innovative Data Systems Research (CIDR &apos;11) January 9-12, 2011, Asilomar, California, USA.</note>

			<note place="foot" n="2"> For simplicity, Algorithm 2 and Algorithm 3 assume perfect partitioning when generating cache-sized partitions. To cope with data skews, one can increase the number of partitions P so that even the largest partition can fit into the CPU cache. Note that using a larger P does not change the algorithm analysis.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database architecture optimized for the new bottleneck: Memory access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manegold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">uFLIP: Understanding flash IO patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bouganim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jónsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Phase change memory technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Breitwisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franceschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kurdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Lastras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vacuum Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving hash join performance through prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving index performance through prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fractal prefetching B + -trees: Optimizing both cache and disk performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valentin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Flip-N-Write: A simple deterministic technique to improve PRAM write performance, energy and endurance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better I/O through byte-addressable, persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coetzee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Phase change memory and its impacts on memory hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Doller</surname></persName>
		</author>
		<ptr target="http://www.pdl.cmu.edu/SDI/2009/slides/Numonyx.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Main memory database systems: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effect of node size on the performance of cache-conscious B + -trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hankins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">File system design for an NFS file server appliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Winter Technical Conference</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">First the tick, now the tock: Intel micro-architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="http://www.intel.com/technology/architecture-silicon/next-gen/319724.pdf" />
		<imprint>
			<pubPlace>Nehalem</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Architecting phase change memory as a scalable DRAM alternative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Free transactions with Rio Vista</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operating Systems Review</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online maintenance of very large random samples on flash storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Integrating reliable memory in databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pcm-Db</forename></persName>
		</author>
		<ptr target="http://www.pittsburgh.intel-research.net/projects/hi-spade/pcm-db/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ptlsim</surname></persName>
		</author>
		<ptr target="http://www.ptlsim.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enhancing lifetime and security of PCM-based main memory with start-gap wear leveling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Karidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franceschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lastras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable high performance main memory system using phase-change memory technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making B + -trees cache conscious in main memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Samsung ships industry&apos;s first multi-chip package with a PRAM chip for handsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samsung</surname></persName>
		</author>
		<ptr target="http://www.samsung.com/us/business/semiconductor/newsView.do?newsid=1149" />
		<imprint>
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Use ECP, not ECC, for hard failures in resistive memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Schechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Straus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Security refresh: Prevent malicious wear-out and increase durability for phase-change memory with dynamically randomized address mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cache conscious algorithms for relational query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shatdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An energy-efficient virtual memory system with flash memory as the secondary storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symp. on Low Power Electronics and Design (ISPLED)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conquest: Better performance through a disk/persistent-RAM hybrid file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Reiher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Popek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Kuenning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">eNVy: a non-volatile, main memory storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A low power phase-change random access memory using a data-comparison write scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISCAS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A durable and energy efficient main memory using phase change memory technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
