<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OASIS: Online Active SemI-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
							<email>goldberg@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department Madison</orgName>
								<orgName type="institution" key="instit1">Arcode Corporation Bethesda</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>20814, 53706</postCode>
									<region>MD, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department Madison</orgName>
								<orgName type="institution" key="instit1">Arcode Corporation Bethesda</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>20814, 53706</postCode>
									<region>MD, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Furger</surname></persName>
							<email>furger@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department Madison</orgName>
								<orgName type="institution" key="instit1">Arcode Corporation Bethesda</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>20814, 53706</postCode>
									<region>MD, WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ming</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department Madison</orgName>
								<orgName type="institution" key="instit1">Arcode Corporation Bethesda</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
								<address>
									<postCode>20814, 53706</postCode>
									<region>MD, WI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OASIS: Online Active SemI-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider a learning setting of importance to large scale machine learning: potentially unlimited data arrives sequentially , but only a small fraction of it is labeled. The learner cannot store the data; it should learn from both labeled and unlabeled data, and it may also request labels for some of the unlabeled items. This setting is frequently encountered in real-world applications and has the characteristics of on-line, semi-supervised, and active learning. Yet previous learning models fail to consider these characteristics jointly. We present OASIS, a Bayesian model for this learning setting. The main contributions of the model include the novel integration of a semi-supervised likelihood function, a sequential Monte Carlo scheme for efficient online Bayesian updating , and a posterior-reduction criterion for active learning. Encouraging results on both synthetic and real-world optical character recognition data demonstrate the synergy of these characteristics in OASIS.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Real-world applications of supervised machine learning are becoming increasingly widespread yet face a number of practical challenges, such as scalability and limited labeled training data. Our goal is to extend supervised learning in three ways concurrently: (i) online learning: we learn sequentially to handle infinite data streams, (ii) active learning: we strategically select the data to be manually labeled, and (iii) semi-supervised learning (SSL): we exploit the remaining unlabeled data, too. SSL has become an increasingly popular learning paradigm in recent years due to rapidly increasing amounts of unlabeled data (e.g., in online image collections, Web pages, etc). Most SSL algorithms, however, operate in batch mode, potentially requiring a large amount of memory and heavy computation to handle massive data sets.</p><p>We advocate online or incremental SSL instead; labeled and unlabeled data are processed one at a time, predictions can be made at any time, and only a bounded amount of storage is needed to handle an unlimited stream of data. Furthermore, it is natural to try to optimize which data items are labeled by a human annotator (choosing a small number to minimize costs). Therefore, we combine active learning with an online algorithm that updates its decision function based on incoming unlabeled data. Many real-world learning tasks fit nicely into this framework, such as classifying images collected by a surveillance camera, or categorizing blog posts and tweets in real-time as they emerge on the social Web. We present a novel, fully Bayesian algorithm capable of online active semi-supervised learning (OASIS).</p><p>The learning setting we consider is similar to that in Goldberg, <ref type="bibr" target="#b9">Li, and Zhu (2008)</ref>, but with an optional active learning component. For simplicity, we present the binary class version, though multiclass extension is straightforward:</p><p>1. At time t, the world picks x t ∈ R d and y t ∈ {−1, 1} and presents x t to the learner.</p><p>2. The learner makes a prediction y t using its current model.</p><p>3. With a small probability p l , the world reveals the label y t .</p><p>4. If y t is not revealed, the learner may choose to ask for it.</p><p>Otherwise, x t remains unlabeled.</p><p>5. The learner updates its model based on x t and, if available, the label y t .</p><p>This setting differs dramatically from traditional online learning where all data is labeled. It also differs from the typical batch SSL setting where methods must wait to collect all the labeled and unlabeled data before beginning to learn and make semi-supervised predictions. Further, it differs from standard active learning in that the unqueried unlabeled items are used for updating the model, too. We make the iid rather than adversarial assumption about the world. Though more restrictive, the iid assumption is still useful in modeling many real-world problems. The goal is then for the model's predictions y t to be accurate; performance will be measured by the cumulative number of mistakes made.</p><p>Classic supervised learning methods cannot learn from unlabeled data. Semi-supervised learning is often possible through specific assumptions about the interaction between the data marginal p(x) and the label conditional p(y | x). Unlike Goldberg, Li, and  and <ref type="bibr">Valko et al. (2010)</ref>, which invoke the manifold assumption in an online semisupervised setting, we focus on the so-called gap (or cluster) assumption, which states that the decision boundary ought to lie in a region of low data density <ref type="bibr" target="#b1">(Chapelle and Zien 2005)</ref>. This assumption is at the heart of Semi-Supervised Support Vector Machines (S3VMs)-see Chapelle, Sindhwani, and <ref type="bibr" target="#b2">Keerthi (2008)</ref> for a review-as well as the null category noise model Gaussian Processes <ref type="bibr">(Lawrence and Jor- dan 2004</ref>). However, from a Bayesian perspective, these existing approaches fail to capture the full complexity of the posterior distribution: the S3VM solution corresponds to a point estimate in the widest gap, and the null category implementation ( <ref type="bibr" target="#b12">Lawrence and Jordan 2004</ref>) makes an inaccurate unimodal approximation (similar to Assumed Density Filtering) to the intrinsically multimodal posterior (see the next section). Furthermore, they are often batch algorithms and not suitable for life-long online learning with potentially unlimited amounts of data.</p><p>Our main contribution is the OASIS algorithm which integrates online, active, and semi-supervised learning. OASIS is a general online Bayesian learning framework and implements the gap assumption through a likelihood function sensitive to unlabeled data. We employ sequential Monte Carlo to efficiently track the entire posterior distribution over the hypothesis space. OASIS is scalable and easily parallelizable, achieves constant time and space complexity per time step, and performs theoretically motivated active learning.</p><p>The idea of combining semi-supervised, active, and online learning can be traced back at least to <ref type="bibr" target="#b6">Furao et al. (2007)</ref>. Their approach employs a self-organizing incremental neural network and combines these learning paradigms heuristically. In contrast, OASIS adopts a principled Bayesian framework and makes the underlying largegap assumption explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The OASIS Model A Bayesian Model for the Gap Assumption</head><p>Recall that we observe a partially labeled sequence of feature vectors x 1 , x 2 , . . ., where each x t ∈ R d . Let D t = {(x 1 , y 1 ), . . . , (x t , y t )} be all the data observed through time t, where we use y t = 0 for unlabeled data. Our goal is to learn a classifier to predict the class label of each incoming unlabeled data point, and then update the classifier based on the information we obtain (x t alone or (x t , y t )). For now, we assume a linear classifier, but the approach can be kernelized using the randomization trick in <ref type="bibr" target="#b15">Rahimi and Recht (2007)</ref>. Let the classifier be parametrized by weight vector w ∈ R d , which interacts with the data through a linear function f (x) = w x. In practice, we can handle a bias term by adding a dummy feature to all feature vectors. Throughout, we use the terms classifier and weight vector interchangeably. To define our Bayesian model, we begin by introducing a likelihood function that is sensitive to unlabeled data and inspired 1 by the null category noise model in <ref type="bibr" target="#b12">Lawrence and Jordan (2004)</ref>. In addition to the positive and negative classes, we model a third "null category" which is never actually observed, but occupies some region of probability <ref type="bibr">1</ref> Despite the similar appearance of the likelihood functions, the current work is actually quite different from <ref type="bibr" target="#b12">Lawrence and Jordan (2004)</ref>; we are concerned with the strictly online setting, and we maintain the posterior over weight vectors through particle filtering, rather than making a Gaussian approximation which loses the critical ability to track multiple modes in the posterior. Such posterior is typical of semi-supervised learning. </p><formula xml:id="formula_0">p(y | x, w) = min(1 − γ, max(γα, c exp(−yc w x))) if y ∈ {−1, 1} 1 − [p(y = +1|x, w) + p(y = −1|x, w)] if y = ∅ , where γ = 0.2, α = 0.5, c = (1 − γ)γα, c = log( γα c )</formula><p>, though other function forms leading to the same general shape would serve our purpose, too.</p><p>This likelihood has several interesting properties, which implement the gap assumption. It is flat beyond a margin value (w x) of 1 or −1 to ensure that weight vectors placing data outside the margin are treated equally. Furthermore, due to the convex curvature of the positive and negative class likelihoods within <ref type="bibr">[−1, 1]</ref>, there is a concave null category probability between −1 and 1. We never receive data from the null category; rather, unlabeled data will be considered to be in either the positive or the negative class:</p><formula xml:id="formula_1">p(y unlabeled | x, w) = p(y ∈ {−1, 1} | x, w) = p(y = +1 | x, w) + p(y = −1 | x, w)</formula><p>. Therefore, a weight vector w that places unlabeled data x in the "null category region" w x ∈ [−1, 1] has a lower likelihood. As a result, this likelihood favors decision boundaries that fall in a low-density region of the input space.</p><p>To complete the model, we need to specify a prior on the parameter w. As discussed in the experiments, we used independent Cauchy priors on each dimension p(w) = d i=1 Cauchy(w i ; 0, ν) and standardized data. With the likelihood and prior defined, we can apply Bayes rule to derive the posterior over weight vectors (after observing past data D t−1 ), and the predictive distribution:</p><formula xml:id="formula_2">p(w | D t−1 ) = t−1 i=1 p(y i | x i , w)p(w) t−1 i=1 p(y i | x i , w )p(w )dw (1) p(y | x t , D t−1 ) = p(y | x t , w )p(w | D t−1 )dw . (2)</formula><p>In general, it is not possible to compute this probability in closed form. The next section describes a tractable solution.</p><p>Since we never actually predict the null category, we are ultimately interested in the following conditional probability when y ∈ {−1, 1} but is unobserved:</p><formula xml:id="formula_3">p(y | x t , D t−1 , y ∈ {−1, 1}) = p(y | x t , D t−1 ) p(y = −1 | x t , D t−1 ) + p(y = 1 | x t , D t−1 )</formula><p>.</p><p>To appreciate why maintaining a multimodal posterior is desirable, consider the example data set in <ref type="figure" target="#fig_1">Figure 1</ref>(b) containing two gaps (of equal width) within the unit circle. With a large amount of unlabeled data (black dots) and only two labeled points (large colored symbols) in opposite wedges, a decision boundary in either gap is feasible, and thus the posterior <ref type="figure" target="#fig_1">(Figure 1(c)</ref>) is bimodal, as indicated by the contours. The w's in the cone-shaped regions classify the labeled data correctly while placing the unlabeled data outside the null category region. Close to the origin, the w's have magnitude too small to place all data outside the margin (since x ≤ 1), hence the cones do not extend to the origin. The S3VM solution corresponds to a point estimate near 2 one of the modes of the posterior. <ref type="bibr" target="#b12">Lawrence and Jordan (2004)</ref> will use a unimodal Gaussian approximation and miss the posterior structure. The key to OASIS is to maintain and update an estimate of the multimodal posterior as labeled and unlabeled data arrives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online SSL via Particle Filtering</head><p>Given the Bayesian model defined in the preceding section, our goal is to track the posterior. In theory, this is done by repeatedly applying Bayes rule. The integrals involved in using the full posterior are intractable, though, so we must resort to approximate methods. In particular, we use particle filtering with resample-move to reduce particle degeneracy ( <ref type="bibr" target="#b8">Gilks and Berzuini 2001)</ref>. The complete OASIS algorithm is summarized in Algorithm 1 and explained below. See Doucet, De Freitas, and Gordon (2001) for standard particle filtering terminology such as effective sample size and systematic resampling.</p><p>Particle filtering is a sequential Monte Carlo technique designed for tracking and approximating distributions that are not amenable to analytical representation <ref type="bibr">(Doucet, De Fre- itas, and Gordon 2001)</ref>. It relies on maintaining a sample of so-called particles to approximate the true distribution in question. We approximate the posterior distribution p(w | D t−1 ) by the empirical distribution over m weighted particles:</p><formula xml:id="formula_5">p(w | D t−1 ) ≈ m i=1 w i δ(w − w (i) ). Each par- ticle w (i) , i = 1 . .</formula><p>. m is a sample from the posterior and has an associated importance weight w i . At time t, the predictive distribution can be approximated by particles as</p><formula xml:id="formula_6">P (y | x t , D t−1 ) ≈ m i=1 w i p(y | x t , w (i) ).</formula><p>Recall, however, that the conditional probability</p><formula xml:id="formula_7">p(y | x t , D t−1 , y ∈ {−1, 1}) ≈ m i=1 w i p(y | x t , w (i) ) p(y = −1 | x t , w (i) ) + p(y = 1 | x t , w (i) )<label>(4)</label></formula><p>is used to make predictions for incoming data. </p><note type="other">). else update w i = w i p(y ∈ {−1, 1}|x t , w (i) t−1 ). if effective sample size ( w i ) 2 / w 2 i &lt; m 2 then Resample-Move particle filtering: { w (i) } m i=1 ← Systematic resampling {w (i) t } m i=1 ← Metropolis-Hastings for each w (i) (using proposal distribution q). Reset particle weights w i = 1 m . else Keep existing particles: w (i) t = w (i) t−1 . Renormalize particle weights {w i } to sum to 1. end end Algorithm 1: The OASIS algorithm</note><p>For online learning, we begin by sampling m particles from the prior and assign uniform initial weights 1/m. Then, we repeatedly update the posterior based on the likelihood and the previous estimate of the posterior (which now acts as the prior). The new posterior distribution after observing x t , y t is proportional to m i=1 w i p(y t | x t , w (i) )δ(w − w (i) ). We represent the new posterior by reweighting the particles. From the above equation, we see that the new weight for w (i) is obtained as the current weight multiplied by the likelihood p(y t | x t , w (i) ). If y t is unlabeled, the weight is multiplied by p(y t ∈ {−1, 1} | x t , w (i) ) = 1 − p(y t = ∅ | x t , w (i) ).</p><p>So far we have a basic method for incrementally updating an approximate posterior after observing new data. Classic particle methods, such as sampling importance resampling (SIR) <ref type="bibr" target="#b4">(Doucet and Johansen 2009)</ref>, use the particle weights for resampling (with replacement). While theoretically justified, repeating this process many times is known to cause particle degeneracy-the number of distinct particles is non-increasing, so eventually few will remain. To minimize particle degeneracy, we apply the resamplemove algorithm ( <ref type="bibr" target="#b8">Gilks and Berzuini 2001)</ref>, which provides a principled way to "jitter" particles and introduce diversity into the pool. Following a resampling step, particles are potentially relocated by applying one step of MetropolisHastings ( <ref type="bibr" target="#b13">Metropolis et al. 1953;</ref><ref type="bibr" target="#b11">Hastings 1970)</ref>. Using a symmetric proposal distribution q( w | w) allows us to compute the acceptance probability for each move using only the unnormalized posterior f (w | D t ):</p><formula xml:id="formula_8">α( w (i) , w (i) ) = min 1, f ( w (i) | D t ) f (w (i) | D t ) ,<label>(5)</label></formula><p>where w (i) is a proposed move, and</p><formula xml:id="formula_9">f (w | D t ) = p(w) t k=1 p(y k | x k , w)</formula><p>, where y k is understood to be {−1, 1} for unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant Time and Space Complexity Per Iteration</head><p>Computing (5) in the "move" step requires access to the entire history of data D t , which is infeasible for learning on an unlimited stream of data. Thus, we propose using an approximate Metropolis-Hastings step in which the acceptance probability is computed using only a fixed-length buffer of size τ . That is, we replace f (w | D t ) with f (w | D t , τ ) = p(w) t k=t−τ +1 p(y k | x k , w). While this approximation is different from the true posterior, it has constant time and space complexity and will be shown to be effective in practice. In addition, though not explored in the current work, using a τ -buffer can allow the method to handle concept drift by only relying on the most recent sample of data.</p><p>Even with a τ -buffer, computing the Metropolis-Hastings acceptance probability for each particle can be computationally intensive. Therefore, we only perform resample-move when the so-called Effective Sample Size-estimated by ( w i ) 2 / w 2 i -drops below m/2, as is customary in the literature ( <ref type="bibr" target="#b4">Doucet and Johansen 2009;</ref><ref type="bibr">Ridgeway and Madi- gan 2003)</ref>. Otherwise, the algorithm proceeds to the next time step with the same particles reweighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Active Learning</head><p>It is quite natural to incorporate online active learning into the algorithm described thus far. The posterior can be viewed as a soft version space, and like many active learning algorithms, we select queries that will maximally pare down the version space. In our case, we query items that will lead to downweighting and effectively killing off many particles.</p><p>To determine whether to query the label of an incoming unlabeled item x, we compute its disagreement score (Nowak 2009) using the current particles:</p><formula xml:id="formula_10">score(x) = m i=1 w i argmax y∈{−1,1} p(y | x, w (i) ) .<label>(6)</label></formula><p>This score is close to zero if roughly half of the particles predict positive and half negative. Querying the label of such an item is beneficial, as roughly half of the particles (whose predictions disagree with the oracle label) will get downweighted. While many schemes are possible to balance the trade-off between the cost of acquiring a label and the benefit of refining the model, we use a simple thresholding approach in the current work. Actively querying points that minimize the score criterion in (6) is theoretically justified in a pool-based active learning setting <ref type="bibr" target="#b14">(Nowak 2009</ref>). The same theory can also be applied to the online active setting to justify a constant threshold. However, that analysis assumes unqueried points are ignored (no SSL). Adapting the theory to account for the semi-supervised updates between active queries remains an open issue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Evaluation</head><p>We conducted a series of experiments on OASIS. We carefully tease apart OASIS's different elements and show that active online querying leads to better performance than random online labeling, in the context of online semi-supervised learning. Furthermore, the use of semisupervised learning in the online setting (even without active querying) often outperforms the identical learner that ignores unlabeled data, as well as a state-of-the-art online supervised learner.</p><p>For all experiments, to avoid difficult parameter tuning under online semi-supervised conditions, we use the same prior and proposal distributions with a fixed set of hyperparameters. Following <ref type="bibr" target="#b7">Gelman et al. (2008)</ref>, we standardize the data so each feature has mean 0 and standard deviation 0.5, and place independent Cauchy(0, 2.5) priors on each dimension. For the proposal distribution, we use a more peaked version of this distribution: Cauchy(0, 0.025). All experiments use m = 1000 particles with buffer size τ = 100.</p><p>The experiments consider five algorithms: 1.</p><p>[OASIS]: Algorithm 1 2.</p><p>[OSIS]: Online and semi-supervised; no active learning, but otherwise the same as OASIS. 3.</p><p>[OS]: Online and supervised; no active learning or SSL. 4.</p><p>[AROW (C = 1)]: State-of-the-art supervised "Adaptive Regularization of Weight Vectors" online learner <ref type="bibr">(Cram- mer, Kulesza, and Dredze 2009)</ref>, run using code provided by the original authors with a default regularization parameter C = 1. This is a passive-aggressive, confidenceweighted classifier that maintains a diagonal-covariance Gaussian distribution over weight vectors. 5.</p><p>[AROW (C * )]: We also report results for AROW using the per-trial clairvoyant C in terms of total number of mistakes ("test-set tuned") to approximate supervised learning's mistake lower bound. We use the following experimental procedure to compare active and passive algorithms, with and without the help of unlabeled data. Each experiment is based on 20 random trials of randomized sequences of T points. Each trial begins with l = 2 labeled examples (one per class, in random order), as we assume this is practical. While OASIS is the only algorithm under consideration that can actively query labels, we take care to ensure that each algorithm receives exactly the same total number of labels. Let a be the number of active queries OASIS makes on a given trial, determined by the active querying threshold s 0 and the data set. For each trial, we do the following: (i) Run OASIS with l = 2 initial labels and record a (number of queries); (ii) Run each other algorithm with l = 2 + a labeled examples (first two, plus a randomly selected others). Note the same exact sequence of {x t } vectors is used across the same trial for different algorithms. In this way, all algorithms always see the same data and the same total number of labels; the algorithms differ in exactly which labels and how they deal with unlabeled data.</p><p>All experiments were implemented in MATLAB and ran quickly on a modern processor. The average per-iteration run times for each method (over 20 trials and all data sets) were: OS 2.3ms, OSIS 3.8ms, OASIS 4.0ms, AROW C = 1 and C * 0.1ms. Note the OS, OSIS, and OASIS code was not optimized for speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Data</head><p>We begin by considering two families of synthetic data sets:</p><p>• sliced-cube-d: Uniformly distributed unit cube in [−0.5, 0.5] d , with an -width slab removed from the first dimension to create two hyper-rectangles with a gap around the true decision boundary x 1 = 0 ( <ref type="figure" target="#fig_3">Figure 2(a)</ref>).</p><p>• diced-cube-d: Same as sliced-cube-d (with true decision boundary x 1 = 0), except -width slabs are removed from all dimensions to create 2 d hypercubes separated by potentially misleading low-density gaps <ref type="figure" target="#fig_3">(Figure 2(b)</ref>).</p><p>For both data sets, = (T /10) −1/d , such that the gaps should be large enough (relative to the average spacing between points) to be detectable after T /10 points (Singh, Nowak, and Zhu 2008). <ref type="figure">Figure 3</ref> plots the 20-trial average cumulative number of mistakes made by each algorithm when predicting the label of each incoming data point (regardless of whether the label ends up being revealed naturally or actively queried). The captions indicate the mean and standard deviation of a, the number of additional labels used in learning (via active selection for OASIS and random selection for the baselines). We observe that OASIS very quickly learns the true decision boundary and stops making mistakes across both data sets for all dimensionalities considered (d ∈ {2, 4, 8, 16, 32}, though only d = 2 and d = 32 are shown here). As expected, active querying allows OASIS to resolve ambiguities between the multiple gaps in the diced-cube-d data, though learning the decision boundary in this more confusing case takes longer on average. Comparing OSIS to OS and both versions of AROW, we see that SSL provides a large advantage, even when the few labeled data points are randomly selected. This example provides a proof of concept for the particle filtering approach to tracking the posterior, both in cases where the data distribution satisfies the gap assumption and when it contains misleading gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-World Data</head><p>We next demonstrate that OASIS and its passive counterpart OSIS significantly outperform supervised baselines on realworld optical character recognition (OCR) tasks through their use of active sampling and online updates based on unlabeled data. We used the MNIST dataset <ref type="bibr">3</ref> and two UCI datasets, letter and pendigits.</p><p>On all three data sets, <ref type="figure">Figure 4</ref> shows a clear ordering of performance: active SSL is better than passive SSL is better than passive supervised learning. We can measure statistically significant performance differences by applying two-sample t-tests to the total numbers of mistakes made by pairs of algorithms across the 20 trials. <ref type="bibr">4</ref> On letter, OASIS significantly outperforms all the supervised algorithms (p &lt; 0.05), and makes fewer mistakes than the passive semi-supervised OSIS. OSIS fails to achieve statistical significance at the 0.05 level over the supervised baselines, suggesting that for this task, OASIS's active learning (rather than the use of unlabeled data) gives it the advantage. On pendigits and MNIST, though, both OASIS and OSIS make significantly fewer mistakes overall than each of the three supervised learners. Furthermore, on MNIST, OASIS significantly beats OSIS, demonstrating that actively queried labels can be more useful than randomly sampled labels in the context of online semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>We have presented a Bayesian learning model, OASIS, which combines online learning, semi-supervised learning, and active learning. OASIS exploits unlabeled data through the low-density gap assumption and is able to avoid the non-convex optimization typically associated with similar SSL algorithms by maintaining an approximation of the posterior over weight vectors via particle filtering. Outside of some special-purpose classifiers for computer vision tracking applications <ref type="bibr" target="#b10">(Grabner, Leistner, and Bischof 2008;</ref><ref type="bibr" target="#b18">Tang et al. 2007</ref>), few authors have examined the task of online semi-supervised learning. The OASIS model presented here also integrates active learning and shows significant improvements over passive supervised baselines on both synthetic and real-world data.</p><p>Our experiments focused on relatively low dimensional data sets. It is well-known that particle filtering and sequential Monte Carlo techniques can be less efficient in high dimensions. One way to adapt OASIS to better handle much higher dimensional data sets, including those resulting from random-feature-based kernelization, is to improve the proposal distribution in the MCMC step of resample-move. The current approach uses a symmetric random walk proposal distribution q, which limits the ability of particles to jump between modes in the posterior. It is possible that we can achieve better mixing by adapting the proposal distribution based on the previous set of particles.</p><p>Future work will also examine alternative active learning strategies, especially ones that strictly limit the total number of active queries or consider costs associated with certain labels. The current fixed threshold strategy may not be suit- able under all real-world constraints. While we could simply impose a hard limit on the number of queries, more sophisticated approaches may be possible that delicately balance the trade-off between asking for another label versus receiving another unlabeled data point. In both cases, we learn something, but it may be advantageous to delay querying until a more valuable point comes along. Many adaptive thresholding and selection criteria are possible for online active learning (see <ref type="bibr" target="#b0">Beygelzimer, Dasgupta, and Langford (2009)</ref> and the references therein), and careful modification to account for the role and impact of unlabeled data could lead to improved learning rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Copyright c 2011 ,</head><label>2011</label><figDesc>Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) "Null category" likelihood function to encourage low-density separation. (b) A dataset with two gaps in unlabeled data (black dots). (c) Its bimodal posterior. S3VM point-estimate marked by ×.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>They do not overlap because their loss functions differ. Input: Number of particles m, prior distribution p(w), proposal distribution q( w | w), threshold s 0 for active learning score function (see (6)) Sample initial particles wAssign initial particle weights w i = 1 m , i = 1, . . . , m. for t = 1, . . . do Receive x t and possibly y t . Active: If unlabeled, query for y t if score(x t ) &lt; s 0 if y t is available then update ∀i = 1 . . . m: w i = w i p(y = y t |x t , w (i) t−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sliced-cube and diced-cube synthetic data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Sliced-cube-d and diced-cube-d synthetic data results for T = 1000, l = 2, s 0 = 0.01.</figDesc></figure>

			<note place="foot" n="3"> For the MNIST data, we reduced the dimensionality down to 10 via &quot;online PCA.&quot; To roughly simulate the online setting, principal components were found based on x1, . . . , x1000, and x1001, . . . , x10000 were simply projected into the resulting space. 4 The specific labeled examples differ between OASIS and the passive algorithms, so the samples are not paired.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Importance weighted active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTAT 2005</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization techniques for semi-supervised support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive regularization of weight vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 22</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tutorial on particle filtering and smoothing: Fifteen years later</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Nonlinear Filtering</title>
		<editor>Crisan, D., and Rozovsky, B.</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sequential Monte Carlo methods in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An online semi-supervised active learning algorithm with self-organizing incremental neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Furao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A weakly informative default prior distribution for logistic and other regression models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Pittau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1360" to="1383" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Following a moving targetMonte Carlo inference for dynamic Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berzuini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Of The Royal Statistical Society Series B</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="146" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online manifold regularization: A new learning setting and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monte Carlo sampling methods using Markov chains and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning via Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 17</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noisy generalized binary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 22</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Recht</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sequential Monte Carlo method for Bayesian analysis of massive datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="319" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unlabeled data: Now it helps, now it doesn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 21</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-tracking using semi-supervised support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Online semi-supervised learning on quantized graphs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
