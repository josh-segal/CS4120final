<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Time Series Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><forename type="middle">Eamonn</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Time Series Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised Learning</term>
					<term>Time Series</term>
					<term>Classification</term>
					<term>Data Mining</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The problem of time series classification has attracted great interest in the last decade. However current research assumes the existence of large amounts of labeled training data. In reality, such data may be very difficult or expensive to obtain. For example, it may require the time and expertise of cardiologists, space launch technicians, or other domain specialists. As in many other domains, there are often copious amounts of unlabeled data available. For example, the PhysioBank archive contains gigabytes of ECG data. In this work we propose a semi-supervised technique for building time series classifiers. While such algorithms are well known in text domains, we will show that special considerations must be made to make them both efficient and effective for the time series domain. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, handwritten documents, manufacturing, and video datasets. The experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Time series data are ubiquitous and are of interest to many communities. Such data can be found in virtually all avenues of human endeavor including medicine, aerospace, finance, business, meteorology, and entertainment <ref type="bibr" target="#b11">[18]</ref> <ref type="bibr" target="#b20">[26]</ref>[36] <ref type="bibr" target="#b37">[41]</ref>. The problem of time series classification has been the subject of active research for decades <ref type="bibr" target="#b4">[11]</ref>[12] <ref type="bibr" target="#b7">[14]</ref> <ref type="bibr" target="#b16">[22]</ref>[24] <ref type="bibr" target="#b24">[30]</ref>. However current methods are limited by the need for large amounts of labeled training data. In reality, such data may be very difficult or expensive to collect. For example, it may require the time and expertise of cardiologists <ref type="bibr" target="#b11">[18]</ref>, space launch technicians <ref type="bibr" target="#b20">[26]</ref>, entomologists <ref type="bibr" target="#b37">[41]</ref>, or other domain experts to manually label the data. As in many other applications, copious amounts of unlabeled data are often readily available. For example, the PhysioBank archive <ref type="bibr" target="#b11">[18]</ref> contains more than 40 gigabytes of ECG data freely available over the web, and hospitals often archive even larger amounts of ECG data for legal reasons. Recent advances in sensor technology have made it possible to collect enormous amounts of data in real time. In this work we propose a semi-supervised technique for building time series classifiers that takes advantage of the large collections of unlabeled data. As we will demonstrate, our approach requires only a handful of labeled examples to construct accurate classifiers. Furthermore, we are able to leverage off recent advances in time series query filtering to use these classifiers very efficiently, particularly for streaming problems <ref type="bibr" target="#b37">[41]</ref>. To enhance the readers' appreciation of the diversity of domains which can benefit from a semi-supervised technique for building time series classifiers, we begin by considering some applications that we will later address experimentally. Indexing of handwritten documents: There has been a recent explosion of interest in indexing handwritten documents <ref type="bibr" target="#b22">[28]</ref>, driven in large part by Google and Yahoo's stated interest of making large archives of handwritten text searchable <ref type="bibr" target="#b21">[27]</ref>. It has recently been shown that simply treating the words as "time series" (see <ref type="figure" target="#fig_0">Figure 1</ref>) is an extremely competitive approach <ref type="bibr" target="#b22">[28]</ref> for classifying (and thus indexing) handwritten documents. The fundamental problem in creating highly accurate handwriting classifiers is that they must be trained on each individual's particular handwriting; a classifier built for George Washington will not generalize to Isaac Newton. However the cost of obtaining labeled data for each word, for every individual is very expensive as measured in human time. A semi-supervised approach where a user annotates just a few training examples would have great utility <ref type="bibr" target="#b22">[28]</ref>. Heartbeat Classification: As noted earlier, the PhysioBank archive <ref type="bibr" target="#b11">[18]</ref> contains more than 40 gigabytes of freely available medical data, including EEG, gait, and ECG data. Such large datasets are potential goldmines for a researcher wishing to build a classifier. However, only a tiny subset of this data has been annotated. Furthermore, as with handwriting, some level of personalization can be useful here. Once again, a semi- supervised approach where a cardiologist annotates just a few training examples, could be of great utility <ref type="bibr" target="#b37">[41]</ref>. The rest of this paper is organized as follows. In Section 2 we review background material. We introduce our semi-supervised time series classification algorithm in Section 3. Section 4 sees a comprehensive empirical evaluation. Finally in Section 5 we offer some conclusions and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND MATERIAL</head><p>In order to frame our contribution in the proper context, we begin with a review of the necessary background material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Value of Unlabeled Data</head><p>The idea of using unlabeled data to help classification may sound initially unintuitive. However, several studies in the literature have indicated the utility of unlabeled data for classification <ref type="bibr" target="#b6">[13]</ref>. For example, early studies <ref type="bibr" target="#b10">[17]</ref>[19] <ref type="bibr" target="#b30">[34]</ref> asserted that unlabeled data should be used whenever available. Castelli <ref type="bibr" target="#b1">[8]</ref> and Ratsaby et. al. <ref type="bibr" target="#b33">[37]</ref> showed that "unlabeled data are always asymptotically useful for classification". Although unlabeled data alone are generally insufficient to yield better-than-random-guess classification, they do contain information which can help classification. We can see this with the simple contrived example in <ref type="figure">Figure 2</ref> (the reader may find it useful to look at <ref type="figure" target="#fig_0">Figure 12</ref> to see why this is a "time series" problem). Here we have a dataset of just three labeled instances, although eight unlabeled instances (U) also exist. We need to classify the instance marked with "?", which clearly belongs to the F (female) class. However this particular image happens to show the actor in a pose which is very similar to one of the M (male) instances, M 1 , and is thus misclassified 1 .</p><p>Figure 2: A simple example to motivate semi-supervised classification. The instance to be classified (marked with "?") is actually a F (female) but happens to be closer to a M (male) in this small dataset of labeled instances Note that while F 1 happens not to be a close match to the instance awaiting classification, it is a close match to the unlabeled instance U 4 . Because it is such a good match to this instance, we could simply change the label from U 4 to F 2 , and add it to our <ref type="bibr">1</ref> If viewing this graphic on a monochrome printout, it may be helpful to note that the male actor has a knee length leotard. dataset of labeled instances. In fact, the basic tenet of semisupervised learning is that we can do this repeatedly, and thus end up with the situation shown in <ref type="figure">Figure 3</ref>. <ref type="figure">Figure 3</ref>: The small dataset of labeled instances shown in <ref type="figure">Figure  2</ref> has been augmented by incorporating the previously unlabeled examples. Now the instance to be classified (marked with "?") is closest to F 5 , and is correctly classified</p><p>It is important to note that the usefulness of unlabeled data depends on the critical assumption that the underlying models / features / kernels / similarity functions match well with the problem at hand <ref type="bibr" target="#b39">[43]</ref>. Otherwise the addition of unlabeled data may degrade the performance of the classifier <ref type="bibr">[1]</ref>[7] <ref type="bibr" target="#b28">[32]</ref>[40].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Learning</head><p>Learning from both labeled and unlabeled data is called semisupervised learning (SSL). Because semi-supervised learning requires less human effort and generally achieves higher accuracy, it is of great interest both in theory and in practice. There are many semi-supervised learning methods proposed in the literature. Based on their underlying assumptions, they can be organized into five classes: SSL with generative models, SSL with low density separation, graph-based methods, co-training methods, and self-training methods <ref type="bibr" target="#b2">[9]</ref> <ref type="bibr" target="#b39">[43]</ref>. Generative models are the oldest semi-supervised learning methods. They assume that the data are drawn from a mixture distribution which can be identified by large amounts of unlabeled data. The strength of the generative approach is that knowledge of the structure of the data can be naturally incorporated into the model. It has been applied to diverse domains including text classification <ref type="bibr" target="#b28">[32]</ref> and face orientation discrimination <ref type="bibr">[1]</ref>. However, to our knowledge, there has been no discussion of the mixture distribution assumption for time series data in the literature. Low density separation approaches try to leverage off the assumption "the decision boundary should lie in a low density region" by pushing the decision boundary away from the unlabeled data. The most common approach to achieve this goal is to use a margin maximization algorithm such as Transductive Support Vector Machines (TSVM). Since finding the exact TSVM solution is NP-hard, several approximation algorithms have been proposed <ref type="bibr">[4]</ref>[10] <ref type="bibr" target="#b8">[15]</ref>[16] <ref type="bibr" target="#b15">[21]</ref>. However, the unique structure of time series makes the density measure less meaningful. For example, in <ref type="bibr">[</ref> Recently graph-based semi-supervised learning methods have received a lot of attention. Based on the assumption that "the (high-dimensional) data lie (roughly) on a low-dimensional manifold", these methods represent the data by nodes in a graph, whose edges are the distances between the nodes. After the graph is constructed, several approaches can be used, such as graph mincut <ref type="bibr">[5]</ref>, Tikhonov Regularization <ref type="bibr">[2]</ref>, Manifold Regularization <ref type="bibr">[3]</ref>, etc.. The key problem of this method is that graph construction needs to be hand crafted for each domain, because it encodes prior knowledge. In this paper, we are looking for a general semi-supervised classification framework for time series, so we do not consider graph-based methods. The idea of co-training was first proposed by Blum and Mitchell <ref type="bibr">[6]</ref>. It divides the features of the data into two disjoint sets, with each set being sufficient to train a good classifier. Two classifiers are trained separately on each feature subset, and the predictions of one classifier are used to enlarge the training set of the other. For example, in our contrived problem in <ref type="figure">Figure 2</ref>, one classifier could use the shape features, and the other classifier could use only color features. The underlying assumption of the co-training approach is that features of data are independent and can be divided. However, time series is known to have very high feature correlation <ref type="bibr" target="#b16">[22]</ref>, which makes the co-training approach infeasible for this type of data. One of the least studied semi-supervised learning methods is selftraining <ref type="bibr" target="#b39">[43]</ref>. In self-training, a classifier is first trained by the small amount of labeled data. It then classifies the unlabeled data, and adds the most confidently classified examples (along with their predicted labels) into the training set. The procedure repeats and the classifier is gradually refined. The classifier is actually using its own predictions to teach itself. Because of its generality and very few assumptions, we use self-training as a starting point for our work. Note that this review of semi-supervised learning is necessarily brief. We refer the interested reader to <ref type="bibr" target="#b2">[9]</ref> and <ref type="bibr" target="#b39">[43]</ref> for a more detailed treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Time Series Classification</head><p>Although we believe that this is the first paper to formally address semi-supervised classification of time series, a thorough literature search and personal experience suggest that people working on real world time series problems have already done this informally. For example in the context of motion capture indexing, Kovar and Gleicher <ref type="bibr" target="#b19">[25]</ref> noted that they "…add robustness to the search by concentrating on finding these closer motions and then using them as new queries in order to find more distant motions". Likewise in our own experience of building insect classifiers <ref type="bibr" target="#b37">[41]</ref>, faced with enormous amounts of sound data which contain relatively few labeled examples, we found this a useful technique. Below we place these ideas in a more formal footing. For concreteness, we begin with a definition of our data type of interest, time series. Definition 1. Time Series: A time series T = t 1 ,…,t m is an ordered set of m real-valued variables. Time series data usually come in two formats: as a long time series (for example, eight hours recording of a patient's heartbeat) or as a set of short time series (for example, a set of individual abnormal heartbeats). Data miners are typically not interested in any of the global properties of a time series. For example, if we are given eight hours ECG data, we are not interested in classifying the whole time series; rather, we are interested in deciding whether each subsection is normal or abnormal. Therefore if we are given a long time series, we convert it into a set of short time series, where each time series in the set is a subsequence of the long time series. Definition 2. Subsequence: Given a time series T of length m, a subsequence C p of T is a sampling of length w &lt; m of contiguous positions from T, that is,</p><formula xml:id="formula_0">C p = t p ,…,t p+w-1 for 1 ≤ p ≤ m -w + 1.</formula><p>The extraction of subsequences from a time series is achieved by use of a sliding window. Definition 3. Sliding Window: Given a time series T of length m, and a user-defined subsequence length of w, all possible subsequences can be extracted by sliding a window of size w across T and extracting each subsequence C p . The most common distance measure for time series is the Euclidean distance. Definition 4. Euclidean Distance: Given two time series (or time series subsequences) Q and C both of length n, the Euclidean distance between them is the square root of the sum of the squared differences between each pair of corresponding data points:</p><formula xml:id="formula_1">( ) ( ) ∑ − ≡ = n i i i c q C Q D 1 2 ,</formula><p>Before calling the distance function, each time series subsequence is normalized to have mean zero and a standard deviation of one, because it is well understood that in virtually all settings, it is meaningless to compare time series with different offsets and amplitudes <ref type="bibr" target="#b4">[11]</ref></p><formula xml:id="formula_2">[12][22][23][28][41].</formula><p>Definition 5. Time Series Classification: Given a set of unlabeled time series, the task of time series classification is to map each time series to one of the predefined classes. Time series classification has typically been treated like a classic discrimination problem, for example, the famous problem of distinguishing between "Democrat" and "Republication" in the UCI Vote dataset <ref type="bibr" target="#b26">[31]</ref>. However we argue that realistic instances of the problem are much more like text filtering problems <ref type="bibr" target="#b28">[32]</ref> in two important ways:</p><p>• It is typically not the case that we have two or more well defined classes. Rather we often have a positive class with some structure, say Premature Ventricular Event (from cardiology <ref type="bibr" target="#b11">[18]</ref>) or Stuck Poppet Anomaly (from space telemetry launch monitoring <ref type="bibr" target="#b20">[26]</ref>), and negative examples that have little or no common structure. The reason why negative examples have no well-defined structure is because every subsequence extracted from a sliding window must be either classified as positive or negative. We cannot in general assume that subsequences not belonging to the positive class look similar to each other. As a consequence, usually there is only one (or some small number of) way(s) to be in the positive class, while there are an essentially infinite number of ways to be in the negative class.</p><p>• Like text filtering, it is typically the case that positive labeled examples are rare, but unlabeled data is abundant. For example, ECG data is often collected continuously overnight when patients are sleeping, which makes real-time annotation almost impossible. Usually cardiologists annotate at most the first five minutes of the ECG data.</p><p>Based on these two observations, we focus on building binary time series classifiers for extremely imbalanced class distributions, with only a small number of labeled examples from the positive class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEMI-SUPERVISED TIME SERIES CLASSIFICATION</head><p>In this section, we begin by introducing the one-nearest-neighbor classifier. Later we show the special considerations necessary to convert it to a semi-supervised framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-nearest-neighbor with Euclidean Distance Classifier</head><p>The problem of time series classification has attracted great interest recently. Although many algorithms have been proposed, it has been shown that one-nearest-neighbor with Euclidean distance is very difficult to beat <ref type="bibr" target="#b16">[22]</ref>. For example, many different classification techniques have been tried on the famous ControlChart problem <ref type="bibr" target="#b26">[31]</ref>, see <ref type="table" target="#tab_1">Table 1</ref>. However, to our knowledge, none of them can beat the simple one-nearest-neighbor with Euclidean distance approach. Furthermore, most approaches listed in <ref type="table" target="#tab_1">Table 1</ref> are quite complicated and require many parameters to be set, whereas one-nearest-neighbor with Euclidean distance is parameterless. In addition, Keogh et. al. <ref type="bibr" target="#b16">[22]</ref> have conducted an extensive set of experiments to show that one-nearest-neighbor with many other similarity measures can not beat the simple strawman.  <ref type="bibr" target="#b16">[22]</ref> 1.3% First order logic rules with boosting <ref type="bibr" target="#b34">[38]</ref> 3.6% Multi layer perceptron neural network <ref type="bibr" target="#b24">[30]</ref> 1.9% Multiple classifier system <ref type="bibr" target="#b4">[11]</ref> 7.2% Multi-scale histogram approach <ref type="bibr" target="#b5">[12]</ref> 6.0%</p><p>Note that the works in <ref type="table" target="#tab_1">Table 1</ref> do make contributions in telling us something about boosting, neural network, or other classification methods. In addition, the authors are to be commended for experimenting on datasets that are in the public domain. Our point is simply that if you want accurate classification of time series, one-nearest-neighbor with Euclidean distance is very hard to beat. For this reason, we only consider one-nearest-neighbor with Euclidean distance in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Classifier</head><p>In the previous section, we have shown that one-nearest-neighbor with Euclidean distance is very competitive for time series classification. Therefore we have adopted it as our base classifier. Note that it also needs a large labeled training set to work well. Below we show how to apply semi-supervised learning to make it feasible for the situation where only a small set of labeled data is available. The idea is simple. We let the classifier train itself through the following steps:</p><p>Step 1. The classifier is trained on the initial training set, where all labeled instances are positive and all unlabeled instances are regarded as negative (recall that an instance must be either positive or negative, see Section 2.3).</p><p>Note that the size of the training set never changes during the training process, but the labeled set is augmented gradually.</p><p>Step 2. The classifier is used to classify the unlabeled data in the training set. For each unlabeled instance, we find its nearest neighbor in the training set. If its nearest neighbor is labeled (as positive of course), the instance will be classified as positive. Otherwise, its nearest neighbor has not been labeled (and thus is negative) and we classify the instance as negative.</p><p>Step 3. Among all the unlabeled instances, the one we can most confidently classify as positive is the instance which is closest to the labeled positive examples. This instance, along with its newly acquired positive label, will be added into the positive set. With the training set being adjusted, we go back to Step 1 to refine the classifier. The procedure repeats until some stopping criterion is reached (we will discuss the stopping criterion in more detail later). The intuition of the idea is straightforward. The labeled positive examples serve as a model which describes what a positive example "looks like". If an unlabeled instance is very similar to a positive example, the probability of it being positive is very high. For example, in our contrived problem in <ref type="figure">Figure 2</ref>, the unlabeled instance U 4 is a very close match to the labeled instance F 1 . Therefore we can label it as F (female) with high confidence. By adding such an example into the positive set, we are refining the description of the positive class, which in turn will help in classifying the unlabeled data. The hope is that the modeling process and the classification process can reinforce each other iteratively and correctly label as many positive examples as possible.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we formalize this idea. Given a set P of positively labeled examples and a set U of unlabeled examples, the algorithm iterates the following procedure. First, use P and U to train the one-nearest-neighbor classifier C (note again we regard instances in P as positive examples and instances in U as negative examples). Second, use classifier C to classify the unlabeled set U. Third, select one unlabeled example which is nearest to any instance in set P (breaking ties randomly), and add it to P. Until (some stopping criterion)</p><p>use P and U to train the one-nearest-neighbor classifier C use classifier C to classify unlabeled set U select the example that C most confidently labels as positive add this example into P delete this example from U End To be concrete, in <ref type="figure" target="#fig_2">Figure 4</ref> we demonstrate our algorithm with a simple two-class toy problem, where initially only one example is known as positive (the solid square in <ref type="figure" target="#fig_2">Figure 4)</ref>. Using our approach, we can correctly classify almost all the examples in the positive class after seventeen iterations, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. In contrast, if we simply put the seventeen nearest neighbors of the single labeled example to the positive class, we will get very poor accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Stopping Criterion</head><p>As we will show in the empirical evaluation, a self-training classifier can achieve high accuracy with only a handful of labeled examples. In this section, we will discuss the stopping criterion for training the classifier, an issue we have deliberately ignored to this point. Ideally we would like the training procedure to stop when the performance (accuracy or precision-recall etc.) of the classifier begins to deteriorate. However, it is very hard (if not impossible) to know the true performance of the classifier, because we do not know the ground truth of the data. In our case, we are using a distance-based classifier. So the distance statistics may give us some hint about how well the classifier is doing. To develop our intuition, we perform selftraining classification on several datasets and look at the minimal distance between two instances in the labeled positive set. For each iteration in the training procedure, we record the precisionrecall breakeven point (explained in greater detail in Section 4) and the distance between the closest pair in the labeled positive set. <ref type="figure" target="#fig_3">Figure 5</ref> shows the results obtained on the ECG dataset (a detailed description of the ECG dataset can be found in Section 4.1). We can see that the minimal nearest neighbor distance decreases dramatically in the first few iterations, stabilizes for a relatively long time, and drops again. Interestingly, the precisionrecall breakeven point achieved by the classifier has a corresponding trend of increasing, stabilizing, and decreasing. In hindsight, this phenomenon is not surprising. In the first few iterations, the labeled positive set is relatively small. In other words, the known positive space is relatively sparse. By adding more positive examples into it, the space gets denser, and as a result, the minimal nearest neighbor distance decreases gradually. At some point, the closest pair of the positive examples is incorporated in the labeled set. The minimal nearest neighbor distance will be the distance between them. Adding more positive examples will not change the minimal distance (this corresponds to the stabilizing phase). However if a negative example is being labeled as positive, chances are high that we will keep adding negative examples because the negative space is much denser than the positive space. And the closest pair in the labeled positive set will be the pair of two negative examples. Thus we will see a drop of the minimal nearest neighbor distance of the positive set. <ref type="figure" target="#fig_4">Figure 6</ref> illustrates the process on a small sample dataset. Similar observations were made on other datasets. These indicate that, even though the question of when to stop the self-training procedure remains unsolved and is an open problem, we can use the change in the minimal nearest neighbor distance in the labeled positive set as a good heuristic in most cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Using the Classifier</head><p>By the end of the training, much more data in the training set are labeled, and we can use the classifier to classify other datasets. At first glance, this is easy. For each instance to be classified, check whether its nearest neighbor in the training set is labeled or not, and assign it the corresponding class label. However recall that the training set is huge (because of the enormous amount of negative examples). Comparing each instance in the testing set to each example in the training set is untenable in practice.</p><p>To make the classification tractable, we modify the classification scheme of the one-nearest-neighbor classifier, using only the labeled positive examples in the training set. If an instance to be classified is within r distance to any of the labeled positive examples, it will be classified as positive. Otherwise it is negative.</p><p>Recently we have successfully applied this scheme to the problem of monitoring streaming time series for a set of predefined patterns <ref type="bibr" target="#b37">[41]</ref>. A natural value for r would be the average distance from a positive example to its nearest neighbor. The intuition is that if the positive examples we have seen before tended to be about r apart, then a future positive object will probably also be within r of one (or more) positive example(s) in the training set. Paradoxically, we may be victims of our own success. By greatly enlarging the size of the labeled positive set with our semisupervised algorithm, it appears that we will greatly increase the time taken to classify new instances. Fortunately this is not the case. We can leverage off an envelope-based lower-bounding technique <ref type="bibr" target="#b37">[41]</ref> to speed up the classification procedure. For example, in <ref type="bibr" target="#b37">[41]</ref> we applied this technique on an ECG dataset and the speedup achieved is more than 100 times. Because we are focusing on the effectiveness of the semi-supervised learning classifier in this paper, we will not discuss the speedup technique any more. We refer interested readers to <ref type="bibr" target="#b37">[41]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EVALUATION</head><p>In this section, we test our semi-supervised learning classifier with a comprehensive set of experiments on diverse domains. We compare the semi-supervised approach to a naive k-nearestneighbor approach, where the k nearest neighbors of the labeled positive set are classified as positive and others as negative (see <ref type="figure" target="#fig_2">Figure 4</ref>.C for an example). As the reader may already appreciate, the setting of k is a non-trivial problem, since the classifier does not know in advance how many positive examples there are in the testing set. To help the strawman achieve the best performance, we allow it to search over all possible values of k and only report the best result. The performance of the classifier at each iteration is reported using precision-recall breakeven point. Since the class distribution is highly skewed, accuracy is not a good performance metric. The classifier can simply classify everything as negative to ensure high accuracy. Note that precision-recall breakeven point is a standard information retrieval measure for binary classification <ref type="bibr" target="#b14">[20]</ref>[32]. Precision and recall are defined as: The precision-recall breakeven point is the value at which precision and recall are equal <ref type="bibr" target="#b14">[20]</ref>. It is a single performance value over all binary classification tasks and it is insensitive to the distribution of the classes. For simplicity in the experiments we did not evaluate the stopping heuristic described in Section 3.2.1. We just keep training the classifier until it achieves its highest precision-recall and allow a few more iterations after that. For most of the experiments, we use distinct training set and testing set. The Word Spotting dataset and Yoga dataset are too small to be split, so for them we train and test on the same dataset. However we note that it is still nontrivial to classify the training set because most data in the training set are unlabeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ECG Dataset</head><p>As noted earlier, heartbeat classification has received a lot of attention because of the large amounts of freely available data and the potential applications in medical field. Our first experiment is on an ECG dataset obtained from the MIT-BIH Arrhythmia Database <ref type="bibr" target="#b11">[18]</ref>. Each data record in the ECG dataset is a time series of the measurements recorded by one electrode during one heartbeat. The data has been annotated by cardiologists and a label of normal or abnormal is assigned to each data record. Of the 2,026 data records in the dataset, 520 were identified as abnormal and 1,506 were identified as normal. All the data records have been normalized and rescaled to have length 85 (recent results suggest that we lose nothing by rescaling <ref type="bibr" target="#b31">[35]</ref>). We randomly split the data, using half for training and half for testing, A negative instance is added into labeled positive set as summarized in <ref type="table" target="#tab_3">Table 3</ref>. Because usually cardiologists are more interested in the occurrences of the abnormal heartbeats, here abnormal heartbeats are our target (positive class). For the semi-supervised approach, we randomly choose 10 positive examples in the training set as the initial labeled positive set P. In each iteration, the semi-supervised algorithm adds one example to the positive set P and uses the adjusted training set to classify the testing set. The precision-recall breakeven point achieved is recorded. Note that the initial labeled set P has an effect on the performance of the classifier (a good initial set P may give the classifier a high precision-recall in the beginning while a bad initial set P may take the classifier more iterations to achieve good performance). To avoid the bias introduced by the initial set, we ran the experiments 200 times and report the results in <ref type="figure" target="#fig_7">Figure 7</ref>. The bold line is the average performance over the 200 runs. The gray lines bounding it from above and below are one standard deviation intervals. Note that the precision-recall breakeven value increases dramatically in the beginning and stabilizes after about ten iterations. On average, the maximal precision-recall breakeven value achieved by the semi-supervised approach is 94.97%. The shaded area in <ref type="figure" target="#fig_7">Figure 7</ref> is where the performance of the semi-supervised classifier deteriorates because it begins to ingest negative examples. We then ran another 200 experiments (each time with the same initial labeled set P as used in the semi-supervised experiment) for the naive k-nearest-neighbor approach (with k = 312). However, even with the optimal k value, the k-nearest-neighbor approach only achieves an average precision-recall breakeven value of 81.29%, which is much lower than that of the semisupervised approach. This shows that with the help of the unlabeled data, the semi-supervised approach can greatly increase the performance of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Spotting Dataset</head><p>In second experiment, we consider classification of handwritten documents. We test on the Word Spotting dataset, which was created by Rath and Manmatha for word image matching <ref type="bibr" target="#b32">[36]</ref>. It contains 2,381 word images from 10 handwritten pages. We take the images of 50 common words such as "the", "and", etc. and obtain 905 instances in total. Each word image is represented by a four dimensional time series which describes the profile of the image. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, we have shown the upper profile of the word "Alexandria". For simplicity we only consider the first dimension of each image, which is of an average length of 270. Here we focus on the two-class problem of differentiating the word "the" from others. In total, there are 109 images for word "the" and 796 images for other words. In this experiment, we use the same 905 images both for training and testing, as summarized in <ref type="table" target="#tab_4">Table 4</ref>. As before, for the semi-supervised approach, each time we randomly choose 10 positive examples in the training set as the initial labeled positive set P and record the precision-recall breakeven point for each iteration. We repeated the experiment 25 times and the results are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. The bold line is the average performance over the 25 runs, and the gray lines are one standard deviation intervals. We can see that the performance increases steadily at the beginning, reaches its maximal value 86.2% at about fifty iterations, and then begins to decrease (the shaded area in <ref type="figure" target="#fig_8">Figure 8</ref>). We then ran the same 25 experiments for the naive k-nearest-neighbor approach (with k = 109). On average, the precision-recall breakeven value obtained by the knearest-neighbor approach is only 79.52%. Handwritten text is an intuitive domain so we spend more time analyzing its results. For example, it is instructive to take a closer look at what happened during the training procedure. <ref type="figure" target="#fig_10">Figure 9</ref> shows the changes of the rankings of two instances during the training process, where Image 19 is a positive example and Image 585 is a negative example. The ranking of an instance is determined by its distance to the labeled positive set -the larger the distance, the higher the ranking. So an instance with higher ranking has lower probability to eventually be classified as positive. In <ref type="figure" target="#fig_10">Figure 9</ref>, as training begins, Image 19 has a relatively high ranking, while Image 595 has a relatively low ranking. This represents a bad initial labeled set, where Image 19 happened to be similar to none of the examples in the initial labeled positive set, while Image 595 is similar to one or more of them. Fortunately, even with a bad start, the semi-supervised learning classifier is able to correctly label more positive examples, which in turn helps it model the positive examples better. As a result, the ranking of Image 19 decreases and the ranking of Image 585 increases after several iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gun Dataset</head><p>The gun dataset contains two-dimensional time series extracted from video of two actors either aiming a gun or simply pointing at a target. The two dimensions correspond to the X and Y coordinates of the actors' right hand. For simplicity we only consider the Y-axes here. The dataset contains four classes: Class A: Actor 1 with gun Class B: Actor 1 without gun (point) Class C: Actor 2 with gun Class D: Actor 2 without gun (point) Here we focus on the two-class problem of differentiating Actor 1 with gun from others -(A) vs. (B+C+D). In total, there are 57 instances in Class A, and 190 instances in other classes. Each instance has the same length of 150. Again, we randomly split the data, using half as the training set and half as the testing set, as summarized in <ref type="table" target="#tab_5">Table 5</ref>. In this experiment, we start with one labeled positive example and train the classifier. We ran the experiment 27 times (once for each positive example) and show the results in <ref type="figure" target="#fig_0">Figure 10</ref>. Starting with only one labeled example, the classifier is able to identify other positive examples and achieves a maximal precision-recall breakeven point of 65.19% on average. One may notice that the variance of this experiment is higher than that of the previous ones (in <ref type="figure" target="#fig_0">Figure 10</ref> the two gray lines are farther away from the bold line). This is because we start with a single labeled example, which increases the bias of the initial labeled set. We ran the same experiments using the k-nearest-neighbor classifier (with k = 27), the average precision-recall breakeven point achieved is 55.93%. This again shows the superior of our semi-supervised approach: it only needs a small number of labeled examples (as few as one in this case) to build accurate classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Wafer Dataset</head><p>The wafer dataset is a collection of time series containing a sequence of measurements recorded by one vacuum-chamber sensor during the etch process of silicon wafers for semiconductor fabrication <ref type="bibr" target="#b29">[33]</ref>. Each wafer has an assigned classification of normal or abnormal. The abnormal wafers are representative of a range of problems commonly encountered during semiconductor manufacturing. Of the 7,164 time series in wafer dataset, 762 were identified as abnormal and 6,402 were identified as normal.</p><p>We randomly picked half the dataset as the training set and used the other half as the testing set. <ref type="table" target="#tab_6">Table 6</ref> summarizes the contents of the training and testing set. As in the ECG experiment, the abnormal data are our target. For the semi-supervised approach, we ran the experiment 50 times, each time starting with one randomly chosen labeled positive example. The average performance is shown as the bold line in <ref type="figure" target="#fig_0">Figure 11</ref>. As we can see, the performance increases dramatically during the first few iterations and achieves a maximal precision-recall breakeven point of 73.17% on average. We ran the same experiments using the k-nearest neighbor classifier (with k = 381), and the average precision-recall achieved is only 46.87%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Yoga Dataset</head><p>For our last experiment, we revisit the classification problem in <ref type="figure">Figure 2</ref> on a realistic dataset. The dataset was obtained by capturing two actors transiting between yoga poses in front of a green screen. It has been shown recently that in many domains it can be useful to convert images into pseudo time series. Therefore we have converted the motion capture data into time series by a well-known technique as in <ref type="figure" target="#fig_0">Figure 12</ref>. In total, we have 316 time series with an average length of 426. Among them, 156 time series came from the female actor and 150 time series came from the male actor. We use the same dataset both for training and testing, as shown in <ref type="table" target="#tab_7">Table 7</ref>. We ran the experiment 10 times, each time randomly choosing one positive example as labeled. The results are shown in <ref type="figure" target="#fig_0">Figure  13</ref>. As we can see, the precision-recall breakeven point increases steadily with the number of iterations, and gets to a maximum of 89.04% on average. While the same experiments on the naive knearest-neighbor approach (with k = 156) only achieves an average precision-recall of 82.95%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>It is well known that building accurate classifiers requires large quantities of labeled data and such labeled data is often difficult to obtain. To mitigate this discrepancy, we propose a semisupervised learning framework to build accurate time series classifiers when only a small set of labeled examples is available. While there are many semi-supervised algorithms in other domains, their underlying assumptions rarely hold for time series data. Special considerations have been taken to make the semisupervised classification both efficient and effective for the time series domain. The experimental results show that the reduction in the number of labeled examples needed can be dramatic: our selftraining classifiers require only a handful of labeled examples to achieve high precision-recall. This suggests that the self-training method of using unlabeled data has a potential for significant benefits in time series classification. There are many directions in which this work may be extended. We intend to perform a thorough investigation on the stopping criterion for the training process. In addition, we plan to extend our framework to other distance measures which have been shown to be effective, for example, Dynamic Time Warping (DTW) <ref type="bibr" target="#b31">[35]</ref>. Finally, we are conducting a field study of insect classification using the semi-supervised approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>We gratefully acknowledge the datasets donors. We also acknowledge insightful comments from Dr. Christian Shelton. Thanks also to Helga Van Herle M.D. for her expertise in cardiology, Dr. Raghavan Manmatha for help with the Word Spotting dataset, and Xiaopeng Xi for help with the Yoga dataset. Reproducible Research Statement: In the interests of competitive scientific inquiry, all datasets used in this work are freely available at the following URL <ref type="bibr" target="#b38">[42]</ref>. This research was partly funded by the National Science Foundation under grant IIS-0237918.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">REFERENCES</head><p>[1] Baluja, S. (1998). Probabilistic modeling for face orientation discrimination: learning from labeled and unlabeled data. in Neural Information and Processing Systems, pp. 854-860, 1998. <ref type="bibr">[2]</ref> Belkin, M., <ref type="bibr">Matveeva, I., &amp; Niyogi, P. (2004)</ref>. Regularization and semi-supervised learning on large graphs. COLT, 2004. <ref type="bibr">[3]</ref> Belkin, M., <ref type="bibr">Niyogi, P., &amp; Sindhwani, V. (2004)</ref>. Manifold regularization: a geometric framework for learning from examples. Technical Report TR-2004-06, University of Chicago. <ref type="bibr">[4]</ref> Bennett, <ref type="bibr">K. &amp; Demiriz, A. (1999</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A) A sample of text written by George Washington. B) The word "Alexandria" after having its slant removed. C) A time series created by tracing the upper profile of the word (Image courtesy of Raghavan Manmatha, used with permission)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A) A simple two-class dataset. B) The chaining effect of semi-supervised learning: a positive example is labeled which helps labeling other positive examples and so on. Eventually all positive examples are correctly classified. C) If we simply put the seventeen nearest neighbors of the single labeled example to the positive class, we would wrongly include many negative examples into the positive class</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Statistics on ECG dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A sample dataset shown in two-dimensional space. A) Initially the two solid (red) squares are labeled as positive. B) At some point the closest pair in the positive set is added into labeled positive set. C) A negative instance is being added into labeled positive set. D) The closest pair in labeled positive set changes to two negative instances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Classification performance on ECG Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Classification performance on Word Spotting Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Ranking changes of two instances in Word Spotting dataset during semi-supervised training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Classification performance on Gun Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Classification performance on Wafer Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Shapes can be converted to time series. The distance from every point on the profile to the center is measured and treated as the Y-axis of a time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Classification performance on Yoga Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The error rates for various classification techniques on 
Control-Chart Dataset 

Approach 
Error Rate 
One-nearest-neighbor with Euclidean distance </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Semi-supervised time series classification algorithm</head><label>2</label><figDesc></figDesc><table>Function [P] = Semi_Supervised_Classification(P, U) 
1 
2 
3 
4 
5 
6 
7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Number of positive and negative instances in the training set and the testing set for ECG Dataset</head><label>3</label><figDesc></figDesc><table>Training Set Testing Set 
Positive (Abnormal) 
208 
312 

Negative (Normal) 
602 
904 

Total 
810 
1,216 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Number of positive and negative instances in the training 
set and the testing set for Word Spotting Dataset 

Training Set Testing Set 
Positive (Word "the") 
109 
109 
Negative (Other words) 
796 
796 
Total 
905 
905 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Number of positive and negative instances in the training set and the testing set for Gun Dataset</head><label>5</label><figDesc></figDesc><table>Training Set Testing Set 
Positive (Class A) 
27 
30 

Negative (Class B,C,D) 
95 
95 

Total 
122 
125 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Number of positive and negative instances in the training 
set and the testing set for Wafer Dataset 

Training Set Testing Set 
Positive (Abnormal) 
381 
381 

Negative (Normal) 
3,201 
3,201 

Total 
3,582 
3,582 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Number of positive and negative instances in the training 
set and the testing set for Yoga Dataset 

Training Set Testing Set 
Positive (Female) 
156 
156 

Negative (Male) 
150 
150 

Total 
306 
306 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">AI Workshop on Text Learning: Beyond Supervision</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The relative value of labeled and unlabeled samples in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Castelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Stanford University, CA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning. In press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised classification by low density separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005)</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Design of Multiple Classifier Systems for Time Series Data. Multiple Classifier Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using Multi-Scale Histograms to Answer Pattern Existence and Shape Match Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Oria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of 17 th International Conference on Scientific and Statistical Database Management</title>
		<meeting>17 th International Conference on Scientific and Statistical Database Management</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised learning of classifiers: theory, algorithms, and their application to human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Cozman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Cirelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1553" to="1567" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient pruning methods for separate-andconquer rule learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 13 th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 13 th International Joint Conference on Artificial Intelligence<address><addrLine>Chambery, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="988" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Optimization approaches to semisupervised learning. Applications and algorithms of complementarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised support vector machines for unlabeled data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mangasarian</surname></persName>
		</author>
		<idno>99-05</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Data Mining Institute, University of Wisconsin Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The efficiency of a linear discriminant function based on unclassified initial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="658" to="662" />
			<date type="published" when="1978-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Components of a New Research Resource for Complex Physiologic Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Physiotoolkit</forename><surname>Physiobank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Physionet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="220" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of iterative maximum likelihood estimates of the parameters of a mixture of two normal distributions under three different types of sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Hosmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="761" to="770" />
			<date type="published" when="1973-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of 10 th European Conference on Machine Learning</title>
		<meeting>10 th European Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of 16 th International Conference on Machine Learning</title>
		<meeting>16 th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the need for time series data mining benchmarks: A survey and empirical demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 8 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 8 th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HOT SAX: Efficient finding the most unusual time series subsequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 5 th IEEE International Conference on Data Mining (ICDM 2005)</title>
		<meeting>the 5 th IEEE International Conference on Data Mining (ICDM 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine learning as an experimental science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 3rd European Working Session on Learning</title>
		<meeting>the 3rd European Working Session on Learning</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated extraction and parameterization of motions in large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of SIGGRAPH &apos;04</title>
		<meeting>SIGGRAPH &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evolution of knowledgebased applications for launch support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Landford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Ground System Architecture Workshop</title>
		<meeting>Ground System Architecture Workshop<address><addrLine>El Segundo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Google&apos;s Two Revolutions. Newsweek. Dec. 27 / Jan. 3 issue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="Availableatwww.msnbc.msn.com/id/6733225/site/newsweek" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indexing of Handwritten Historical Documents -Recent Progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2003 Symposium on Document Image Understanding Technology (SDIUT)</title>
		<meeting>of the 2003 Symposium on Document Image Understanding Technology (SDIUT)<address><addrLine>Greenbelt, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04-09" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An experimental study of EM-based algorithms for semi-supervised learning in audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the ICML 2003 Workshop on the Continuum from Labeled to Unlabeled Data</title>
		<meeting>the ICML 2003 Workshop on the Continuum from Labeled to Unlabeled Data<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature-based Classification of Time-series Data</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Research</title>
		<imprint>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">UCI Repository of machine learning databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hettich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Irvine</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of California, Department of Information and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generalized feature extraction for structural pattern recognition in time-series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Olszewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normal discrimination with unclassified observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>O&amp;apos;neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">364</biblScope>
			<biblScope unit="page" from="821" to="826" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Everything you know about Dynamic Time Warping is wrong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Third Workshop on Mining Temporal and Sequential Data, in conjunction with the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Third Workshop on Mining Temporal and Sequential Data, in conjunction with the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004-08-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word image matching using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from a mixture of labeled and unlabeled examples with parametric side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ratsaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the Eighth Annual Conference on Computational Learning Theory</title>
		<meeting>the Eighth Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="412" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning First Order Logic Time Series Classifiers: Rules and Boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4 th European Conference on Principles of Data Mining and Knowledge Discovery (PKDD2000)</title>
		<meeting>4 th European Conference on Principles of Data Mining and Knowledge Discovery (PKDD2000)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training object detection models with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the electronic proceedings of the 13 th British Machine Vision Conference</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahshahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Atomic Wedgie: Efficient Query Filtering for Streaming Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Herle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mafra-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 5 th IEEE International Conference on Data Mining (ICDM 2005)</title>
		<meeting>the 5 th IEEE International Conference on Data Mining (ICDM 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="490" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="http://www.cs.ucr.edu/~wli/selfTraining/" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
