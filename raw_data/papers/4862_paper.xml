<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Low Power Front-End for Embedded Processors Using a Block-Aware Instruction Set</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Zmily</surname></persName>
							<email>zmily@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<email>kozyraki@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Low Power Front-End for Embedded Processors Using a Block-Aware Instruction Set</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C3 [Special-Purpose and Application-Based Systems]: Real- time and embedded systems; C0 [General]: Hardware/software interface General Terms Design</term>
					<term>Performance Keywords low power front-end</term>
					<term>instruction re-ordering</term>
					<term>software hints</term>
					<term>in- struction prefetching</term>
					<term>tagless instruction cache</term>
					<term>unified instruction cache and BTB</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Energy, power, and area efficiency are critical design concerns for embedded processors. Much of the energy of a typical embedded processor is consumed in the front-end since instruction fetching happens on nearly every cycle and involves accesses to large memory arrays such as instruction and branch target caches. The use of small front-end arrays leads to significant power and area savings, but typically results in significant performance degradation. This paper evaluates and compares optimizations that improve the performance of embedded processors with small front-end caches. We examine both software techniques, such as instruction reordering and selective caching, and hardware techniques, such as instruction prefetching, tagless instruction cache, and unified caches for instruction and branch targets. We demonstrate that, building on top of a block-aware instruction set, these optimizations can eliminate the performance degradation due to small front-end caches. Moreover , selective combinations of these optimizations lead to an embedded processor that performs significantly better than the large cache design while maintaining the area and energy efficiency of the small cache design.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Energy, power, and area efficiency are important metrics for embedded processors. Die area and power consumption determine the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CASES'07, September 30-October 3, 2007, Salzburg, Austria. Copyright 2007 ACM 978-1-59593-826-8/07/0009 ...$5.00. cost to manufacture, package, and cool the chip. Energy consumption determines if the processor can be used in portable systems. Moreover, embedded processors must also meet the increasing performance requirements of demanding applications such as image, voice, and video processing that are increasingly common in consumer products <ref type="bibr" target="#b24">[25]</ref>. Hence, area, power, and energy efficiency must be achieved without compromising performance. Embedded processors consume a large fraction of their energy in the front-end of their pipeline. The front-end contains several large SRAM structures such as the instruction cache, the branch target buffer (BTB), and the branch predictor, that are accessed on nearly every clock cycle. Such memory arrays are sized to hold a large amount of data in order to obtain good overall performance. For example, the Intel XScale PXA270 processor uses a 32-KByte, 32-way instruction cache and a 128-entry BTB <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, different programs exhibit different locality and memory access patterns and even a single program may not need all the available storage at all times. If the processor is executing a tight loop, for example, most of the instruction cache is underutilized as smaller cache could provide the same performance but with lower area, power, and energy requirements. <ref type="figure">Figure 1</ref> quantifies the total energy and power wasted in the PXA270 processor due to sub-optimal instruction cache and BTB sizing for MediaBench and SpecCPU2000 applications. The optimal configuration is found using a method similar to <ref type="bibr" target="#b25">[26]</ref> where a continuum of cache sizes and configurations are simulated. During each cycle, the cache with the lowest power from among those that hit is selected. On average 16% total power 71.5% <ref type="table">Table 1</ref>: Normalized power dissipation, area, and access time for different instruction cache configurations over the XScale 32-KByte instruction cache configuration.</p><p>and 17% total energy are wasted if the processor uses larger than needed instruction cache and BTB.</p><p>Reducing the instruction cache and BTB capacity of embedded processors by a factor of 4 or 8 leads to direct die area and power savings. <ref type="table">Table 1</ref> presents the normalized power dissipation, area, and access time for different smaller instruction cache configurations over the 32-KByte, 32-way instruction cache of the PXA270 processor using Cacti <ref type="bibr" target="#b26">[27]</ref>. A 2-KByte instruction cache dissipates only 8.4% of the power dissipated by the 32-KByte cache and uses only 4.6% of its area. While the use of smaller arrays reduces die area and power dissipation, several applications will now experience additional instruction cache and BTB misses that will degrade performance and increase energy consumption. <ref type="figure">Figure 1</ref> quantifies the performance penalty with the smaller instruction cache and BTB sizes (13% on average). Furthermore, the energy savings from accessing smaller arrays are nearly canceled from the cost of operating the processor longer due to the performance degradation.</p><p>This paper studies optimization techniques that improve the performance of embedded processors with small front-end arrays. Our goal is to reach or exceed the performance of embedded processors with large caches, while maintaining energy and power consumption close to the optimal design. We evaluate both hardware and software based techniques such as instruction prefetching and re-ordering, unified instruction cache and BTB structures, tagless instruction caches, and various forms of software hints. Instruction prefetching hides the latency of extra cache misses by fetching instructions ahead of time. Instruction re-ordering attempts to densely pack frequently used instruction sequences in order to improve the locality in instruction cache and BTB accesses. Unifying the instruction cache and the BTB allows a program to flexibly use the available storage as needed without the limitations of a fixed partitioning. Alternatively, the BTB and instruction cache could be organized in such away that the instruction cache tags are no longer required; hence, their area and power overhead can be saved. Finally, compiler generated hints can improve the instruction cache performance by guiding the hardware to wisely use the limited resources.</p><p>We explore these front-end optimizations using a block-aware instruction set architecture (BLISS). Previous work <ref type="bibr" target="#b32">[33]</ref> has shown that BLISS leads to significant performance and code size advantages for processors with conventionally sized front-end caches. BLISS defines basic block descriptors in addition to and separately from the actual instructions in each program. A descriptor provides the type of the control-flow operation that terminates the basic block, its potential target, the number of instructions in the block, and a pointer to the actual instructions.</p><p>In this paper, we explore the front-end optimizations that improve the performance of embedded processors with small frontend caches using the BLISS ISA. BLISS provides a flexible substrate to implement the optimizations efficiently because the descriptors are directly visible to software, provide accurate information for prefetching, and can carry software hints. Hence, BLISS allows significant reorganization of the front-end without modifying the software model. While some of the optimizations can also be implemented with a conventional instruction set, they lead to lower performance benefits and are typically more complex.</p><p>Overall, this paper provides the insights and analysis necessary to design the front-end for efficient embedded designs. The specific contributions are:</p><p>• we demonstrate that a block-aware architecture allows the implementation of a wide-range of front-end optimizations in a simple and efficient manner.</p><p>• we evaluate the front-end optimizations and analyze how they allow an embedded processor with small front-end caches to perform similarly to one with larger structures.</p><p>• we demonstrate that combinations of these optimizations further improve the performance and allow the front-end of the processor to achieve power and energy consumption levels close to the optimal design. The best performing configuration allows an embedded processor with small front-end caches to be 9% faster and consume 14% less power and 19% less energy than a similar pipeline with large front-end structures.</p><p>• While some optimizations can be implemented using a conventional instruction set, we demonstrate that they are typically more complex and may lead to lower energy and performance benefits compared to BLISS. We compare BLISS with the front-end optimizations to the Filter cache design with similar optimizations. We show that BLISS provides similar power reduction and at the same time provides significant performance and energy improvements.</p><p>The rest of the paper is organized as follows. Section 2 provides a brief overview of the BLISS architecture. In Section 3, we present the different front-end optimizations. Section 4 explains the methodology used for evaluation. In Section 5, we evaluate the performance, cost, and total energy benefits of the different front-end optimizations for a configuration similar to the XScale processor. In Section 6, we discuss the related research that this work is based on. Section 7 provides a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BLISS OVERVIEW</head><p>Before we describe the front-end optimizations, we provide a brief overview of the BLISS architecture. For further details, we refer readers to <ref type="bibr" target="#b32">[33]</ref>.</p><p>The BLISS instruction set provides explicit basic block descriptors (BBD) in addition to and separately from the ordinary instructions they include. The code segment for a program is divided in two distinct sections. The first section contains descriptors that define the type and boundaries of basic blocks, while the second section lists the actual instructions in each block. <ref type="figure" target="#fig_1">Figure 2</ref> shows the descriptor format. Each block descriptor defines the type of the control-flow operation that terminates the block. The BBD also includes an offset field to be used for blocks ending with a branch or a jump with PC-relative addressing. The actual instructions in the basic block are identified by the pointer to the first instruction and the length field. The last BBD field contains optional compilergenerated hints. <ref type="figure" target="#fig_1">Figure 2</ref> also shows an embedded processor with a BLISS-based front-end that uses a cache for basic block descriptors (BB-cache) <ref type="bibr">Back</ref> Type: Basic Block type (type of terminating branch): -FT, B, J, JAL, JR, JALR, RET, LOOP Offset: displacement for PC-relative branches and jumps. Length: number of instructions in the basic block (0..15) Instruction pointer: address of the 1st instruction in the block bits <ref type="bibr">[15:2]</ref>. bits <ref type="bibr">[31:16]</ref> in TLB Size:</p><p>optional compiler-generated hints used for cache hints in this study BB Descriptor Format: as a replacement for the BTB. The front-end operates in a decoupled manner. On every cycle, the BB-cache is accessed using the PC. On a miss, the front-end stalls until the missing descriptor is retrieved from the memory hierarchy (L2-cache). On a hit, the BBD and its predicted direction/target are pushed in the basic block queue (BBQ). The predicted PC is used to access the BB-cache in the following cycle. Instruction cache accesses use the instruction pointer and length fields of the descriptors available in the BBQ to retrieve the instructions in the block. If all instructions are in a single cache line, a single cache access per block is sufficient. Previous work has shown that BLISS leads to significant improvements in performance, energy consumption, and code size <ref type="bibr" target="#b32">[33]</ref>. Performance is improved because BLISS tolerates instruction cache latency and improves control-flow prediction <ref type="bibr" target="#b30">[31]</ref>. The BBQ decouples control-flow prediction from instruction fetching. Multi-cycle latency for a large instruction cache no longer affects prediction accuracy, as the vital information for speculation is included in basic-block descriptors available through the BB-cache. Since the PC in the BLISS ISA always points to basic block descriptors (i.e. a control-flow instruction), the predictor is only used and trained for PCs that correspond to branches which reduces interference and accelerates training in the predictor.</p><p>The improved control-flow prediction accuracy reduces the energy wasted by mispredicted instructions. In addition, energy consumption is further reduced because BLISS allows for energy optimizations in the processor front-end <ref type="bibr" target="#b31">[32]</ref>. Each basic block defines exactly the number of instructions needed from the instruction cache. Using segmented word lines for the data portion of the cache, we can fetch the necessary words while activating only the necessary sense-amplifiers in each case. Front-end decoupling tolerates higher instruction cache latency without loss in speculation accuracy. Hence, we can access first the tags for a set associative instruction cache, and in subsequent cycles, access the data only in the way that hits We can also merge the instruction accesses for sequential blocks in the BBQ that hit in the same cache line, in order to save decoding and tag access energy. Finally, the branch predictor is only accessed after the block descriptor is decoded; hence, predictor accesses for fall-through or jump blocks can be eliminated.</p><p>BLISS improves code density by removing redundant sequences of instructions across basic blocks and flexible interleaving of 16-bit and 32-bit instructions at basic block granularity <ref type="bibr" target="#b33">[34]</ref>. All instructions in a basic block can be eliminated if the same sequence is present elsewhere in the code. Correct execution is facilitated by adjusting the instruction pointer in the basic block descriptor to point to the unique location in the binary for that instruction sequence. We can also aggressively interleave 16-bit and 32-bit instructions at basic-block boundaries without the overhead of additional instructions for switching between 16-bit and 32-bit modes. The block descriptors identify if the associated instructions use the short or long instruction format.</p><p>In this paper, we go beyond previous BLISS studies by introducing hardware and software optimizations that improve efficiency with small front-end structures. While the base BLISS approach is not sufficient to address the performance challenges in such systems, its instruction set and front-end organization provide significant benefits for hardware and software optimizations that can bridge the gap. Many of the optimization techniques covered in this study have been already proposed for conventional instruction sets. In this paper, we demonstrate that BLISS provides a flexible substrate to implement the optimizations efficiently which translates to higher performance and energy improvement compared to implementing them using conventional instruction sets. We explain the synergy in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FRONT-END OPTIMIZATIONS</head><p>Reducing the instruction cache and BTB capacity of embedded processors by a factor of 4 or 8 leads to direct die area and power savings. However, several applications will now experience additional instruction cache and BTB misses that will degrade performance and increase energy consumption (see <ref type="figure">Figure 1</ref>). This section discusses hardware and software techniques that can reduce the performance degradation. The hardware-based techniques include instruction prefetching, unified instruction cache and BTB structures, and tagless instruction caches. Instruction prefetching hides the latency of extra cache misses by fetching instructions ahead of time. Unifying the instruction cache and the BTB allows a program to flexibly use the available storage as needed without the limitations of a fixed partitioning. Alternatively, the BTB and the instruction cache could be organized in such away that the instruction cache tags are no longer required; hence, their area and power overhead can be saved. The software-based techniques include instruction re-ordering and various forms of software hints. Instruction re-ordering attempts to densely pack frequently used instruction sequences in order to improve the locality in instruction cache and BTB accesses. Finally, compiler-generated hints can improve the instruction cache performance by guiding the hardware to wisely use the limited resources. The following sections will explain each optimization technique and how it can be easily supported by BLISS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Instruction Prefetching (Hardware)</head><p>Instruction cache misses have a severe impact on the processor performance and energy efficiency as they cause the front-end to stall until the missing instructions are available. If an instruction cache is smaller than the working set, misses are inversely proportional to the cache size. Hence, a smaller instruction cache will typically cause additional performance loss. Instruction prefetching can reduce the performance impact of these misses. Instruction prefetching speculatively initiates a memory access for an instruction cache line, bringing the line into the cache (or a prefetching buffer) before the processor requests the instructions. Prefetching from the second level cache or even the main memory can hide the instruction cache miss penalties, but only if initiated sufficiently far ahead in advance of the current program counter.</p><p>Most modern processors only support very basic hardware sequential prefetchers. With a sequential or stream-based prefetcher, one or more sequential cache lines after the currently requested one are prefetched <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19]</ref>. Stream prefetching only helps with misses on sequential instructions. An alternative approach is to initiate prefetches for cache lines on the predicted path of execution <ref type="bibr" target="#b5">[6]</ref>. The advantage of such a scheme is that it can prefetch potentially useful instructions even for non-sequential access patterns as long as branch prediction is sufficiently accurate. Prefetched instructions are typically stored in a separate buffer until the data is used at least once to avoid cache pollution. Most prefetching methods filter out useless prefetches by probing the cache to save bandwidth and power. To avoid adding an additional port, probing is performed only when the instruction cache port is idle.</p><p>BLISS supports efficient execution-based prefetching using the contents of the BBQ. The BBQ decouples basic block descriptor accesses from fetching the associated instructions. The predictor typically runs ahead, even when the instruction cache experiences temporary stalls due to a cache miss or when the instruction queue is full. The contents of the BBQ provide an early, yet accurate view into the instruction address stream and are used to lookup further instructions in the instruction cache. Prefetches are initiated when a potential miss is identified. BLISS also improves prediction accuracy since the PC always points to basic block descriptors and the predictor is only used and trained for PCs that correspond to branches which reduces interference and accelerates training in the predictor <ref type="bibr" target="#b30">[31]</ref>. The improved prediction accuracy makes the execution-based prefetching scheme even more effective. Prefetching, if not accurate, leads to additional L2-cache accesses that can increase the L2-cache power dissipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified I-Cache and BTB (Hardware)</head><p>Programs exhibit different behaviors with respect to the instruction cache and BTB utilization. While some programs stress the instruction cache and are susceptible to its size (e.g., rasta from MediaBench), other programs depend more on the BTB capacity (e.g., adpcm from MediaBench). Even in a single program, different phases may exhibit different instruction cache and BTB access patterns. Being able to flexibly share the instruction cache and BTB resources could be valuable for those types of programs, especially when the hardware resources are limited.</p><p>The BLISS front-end can be configured with a unified instruction cache and BB-cache storage as both instructions and descriptors are part of the architecturally-visible binary code. Each line in the unified cache holds either a few basic block descriptors or a few regular instructions. The unified cache can be accessed by both the descriptor fetch and the instruction fetch units using a single access port. Instruction fetch returns multiple instructions per access (up to a full basic block) to the back-end pipeline and does not need to happen on every cycle. On the remaining cycles, we perform descriptor fetches. For the embedded processors we studied in Section 5, sharing a single port for instruction and descriptor fetches had a negligible impact on performance.</p><p>With a conventional architecture, on the other hand, storing BTB and instruction cache entries in a single structure is more challenging as the same program counter is used to access both structures. This implies that extra information is required to be stored in the unified cache to differentiate between BTB and instruction entries. In addition, the two entries map to the same cache set, causing more conflicts. The BTB and instruction cache are also accessed more frequently as basic block boundaries are not known until instruction decoding. Hence, sharing a single port is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tagless I-Cache (Hardware)</head><p>In previous work <ref type="bibr" target="#b31">[32]</ref>, we showed that we could eliminate the data access for all but the way that hits by accessing the tag arrays first then the data array in a subsequent cycle. Now we will focus on eliminating the instruction cache tags altogether (storage and access). BLISS provides an efficient way to build an instruction cache with no tag accesses by exploiting the tags checks performed on descriptor accesses. This improves instruction cache access time, reduces its energy consumption significantly, and eliminates the area overhead of tags. The new tagless instruction cache is organized as a direct mapped cache, with only the data component. <ref type="figure" target="#fig_2">Figure  3</ref> illustrates the organization of this cache. For each basic block descriptor in the BB-cache, there is only one entry in the tagless instruction cache which can hold a certain number of instructions, 4 in our experiments. A flag bit is used in each descriptor in the BB-cache entry to indicate if the corresponding entry in the tagless instruction cache has valid instructions or not. This flag is initialized during BB-cache refill from L2-cache and is set after the instructions are fetched from the L2-cache and placed in the tagless instruction cache. Moreover, the flag that indicates if the entry in the tagless cache is valid or not can be used by the prefetching logic. This eliminates the need to probe the cache and improves the overall performance of the prefetcher. The operation of the BLISS front-end with the tagless cache is very similar to what we explained in Section 2 except the way the instruction cache is accessed. On a BB-cache miss, the missing descriptors are retrieved from the L2-cache. At that stage, the instruction valid bits (IV) are initialized for those descriptors indicating that their associated instruction cache entries are invalid. The instruction fetch unit uses the valid bit to determine how to access the instruction cache. If the instruction valid bit is not set, the instructions are retrieved from the L2-cache using the instruction pointer available from the descriptor. Once the instructions are retrieved and placed in the instruction cache, the valid bit for the corresponding descriptor is set. If the instruction valid bit is set, the instructions are retrieved from the instruction cache using the index field of the PC and the index of the matching BB-cache way. For basic blocks larger than 4 instructions, only the first four instructions are stored in the instruction cache. In the applications studied in Section 5, 68% of the executed basic blocks include 4 instructions or less. Similar to the victim cache, we use a 4-entry fully associative cache to store the remaining instructions. This victim cache is accessed in a subsequent cycle and is tagged using the PC. In a case of a miss, the instructions are brought from the L2-cache.</p><p>Nevertheless, the tagless instruction cache has two limitations. First, once a BB-cache entry is evicted, the corresponding instruction cache entries become invalid. In addition, the virtual associativity and size of the instruction cache are now linked with that of the BB-cache and cannot be independently set. We can use an alternative approach for indexing the tagless cache to solve this limitation. We can determine the location in the instruction cache independently by an additional pointer field in the BB-cache format. This is similar to having a fully associative instruction cache, but with additional complexity in its management (keep track of LRU, etc).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Instruction Re-ordering (Software)</head><p>Code re-ordering at the basic block level is a mature method that tunes a binary to a specific instruction cache organization and improves hit rate and utilization. Re-ordering uses profiling information to guide placement of basic blocks within the code. The goal is to arrange closely executed blocks into chains that are laid out sequentially, hence increasing the number of instructions executed per cache line. The improved spatial locality reduces the miss rate for the instruction cache of a specific size. This implies that we can afford using a smaller cache without negatively impacting the performance.</p><p>Pettis and Hansen suggested a bottom-up block-level positioning algorithm <ref type="bibr" target="#b19">[20]</ref>. In their approach, they split each procedure into two procedures, one with the commonly used basic blocks and one with the rarely used basic blocks ("fluff"). The infrequently executed code is replaced with a jump to the relocated code. Additionally, a jump is inserted at the end of the relocated code to transfer control back to the commonly executed code. Within each of the two procedures, a control-flow graph is used to from chains of basic blocks based on usage counts. The chains are then placed making fall through the likely case after a branch.</p><p>Basic block re-ordering is easily supported by BLISS using the explicit block descriptors. Blocks of instructions can be freely reordered in the code segment in any desired way as long as we update the instruction pointers in the corresponding block descriptors. <ref type="figure" target="#fig_3">Figure 4</ref> presents an example to illustrate this optimization. The two instructions in the second basic block in the original code are rarely executed. Therefore, they can be moved to the end of the code as long as the instruction pointer for BBD2 is updated. Compared to re-ordering with conventional architectures, this provides two major benefits. First, there is no need to split the procedure or introduce additional jump instructions for control transfers between the commonly and the less commonly used code (fewer static and dynamic instructions). The pointers in the block descriptors handle control transfers automatically. Second, re-ordering basic blocks does not affect branch prediction accuracy for BLISS, as the vital information for speculation is included in the basic block descriptors available through the BB-cache (block type, target offset). On a conventional architecture, re-ordering blocks may change the number of BTB entries needed and the conflicts observed on BTB accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cache Placement Hints (Software)</head><p>Conventional caches are designed to be managed purely by hardware. Hardware must decide where to place the data and which data to evict during cache replacement. A consequence is that the cache resources may not be optimally utilized for a specific benchmark, leading to poor cache hit rate. Compilers and profile-based tools can help the processor with selecting the optimal policies in order to achieve the highest possible performance using the minimal amount of hardware. Hints can indicate at which cache levels it is profitable to retain data based on their access frequency, excluding infrequent data from the first level cache. Hints can also guide the hardware placing data in the cache to avoid conflicts, or improve the cache replacement decisions by keeping data with higher chance of reuse.</p><p>A compiler can attach hints to executable code at various granularities, with every instruction, basic block, loop, function call, etc. BLISS provides a flexible mechanism for passing compilergenerated hints at the granularity of basic blocks. The last field of the basic block descriptor contains optional compiler-generated hints. Specifying hints at the basic block granularity allows for fine-grain information without increasing the length of all instruction encodings or requiring additional, out-of-band, instructions that carry the hints. Hence, hints can be communicated without modifying the conventional instruction stream or affecting static   or dynamic instruction counts. Furthermore, since descriptors are fetched early in the pipeline, the hints can be useful with decisions with most pipeline stages, even before instructions are decoded.</p><p>We evaluate two types of software hints for the L1 instruction cache management. The first type indicates if a basic block should be excluded from the L1 instruction cache. We rely on prefetching, if enabled, to bring excluded blocks from the L2-cache when needed. Note that the hints are visible to the prefetcher; therefore, cache probing is not required for those blocks. A very simple heuristic based on profiling information is used to select which cache lines are cache-able. We exclude blocks with infrequently executed code and blocks that exhibit high miss rates. The second type of hints redistributes the cache accesses over the cache sets to minimize conflict misses. The hints are used as part of the address that indexes the cache. The 3 hint bits are concatenated with the index field of the address to form the new cache index field. <ref type="table" target="#tab_3">Table 2</ref> summarizes the key architectural parameters for the base and BLISS processor configurations used for evaluation. Both are modeled after the Intel XScale PXA270 <ref type="bibr" target="#b9">[10]</ref>. We fully model all contention for the L2-cache bandwidth between BB-cache misses and instruction cache or data cache misses. For fair energy comparison, the base design is modeled with serial instruction tag and data accesses to eliminate the data access for all but the way that hits. We have also performed experiments with a high-end embedded core comparable to the IBM PowerPC 750GX and the achieved results are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY</head><p>Our simulation framework is based on the Simplescalar/PISA 3.0 toolset <ref type="bibr" target="#b4">[5]</ref>, which we modified to add the BLISS front-end model. All front-end optimizations explained in Section 3 are fully modeled in the simulations. For energy measurements, we use the Wattch framework at the cc3 power model <ref type="bibr" target="#b3">[4]</ref>, which we also modified to accurately capture all of the optimizations. Energy consumption was calculated for a 0.10µm process with a 1.1V power supply. The reported Total Energy includes all the processor components (front-end, execution core, and all caches). For performance, we report IPC, ignoring the fact that processors with smaller caches may be able to run at higher clock frequencies than processors with larger caches. We study 10 benchmarks form MediaBench and SpecCPU2000 suites. The selected benchmarks have relatively high instruction cache or BTB miss rates. The benchmarks are compiled at the -O2 optimization level using gcc. We did not include the code size optimizations in <ref type="bibr" target="#b33">[34]</ref>. MediaBench programs are simulated to completion and for the SpecCPU2000 programs we skipped 1 billion instructions and simulated 1 billion instructions for detailed analysis. For benchmarks with multiple datasets, we run all of them and calculate the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>This section presents the performance, total energy, and cost evaluation results for the different front-end optimizations using BLISS. <ref type="figure">Figure 5</ref> compares the IPC of BLISS with small caches and the various optimizations to that of the base design with large caches (IPC of 1.0). We only present a single combination of optimizations, the best performing one (prefetching + instruction re-ordering + unified cache + redistribute cache hints). For reference, the average normalized IPC for various other configurations is: 0.87 for the base design with small caches, 0.91 for the base design with small  caches and prefetching, and 0.99 for BLISS with small caches and no prefetching. It is important to notice that for all but one benchmark (gcc), all optimizations allow BLISS with small caches to reach the IPC of the base design with large caches. The design with the combined optimizations consistently outperforms the base with an average IPC improvement of 9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Analysis</head><note type="other">1-Prefetching 2-Instruction Reordering 3-Unified Cache 4-Tagless Cache + Prefetching 5a-Exclude</note><p>The analysis for the individual optimizations is the following. The advantages of instruction prefetching and re-ordering are consistent across all benchmarks. When combined, re-ordering reduces significantly the prefetching traffic. The unified cache is most beneficial for benchmarks that put pressure on the BTB (e.g., jpeg), but may also lead to additional conflicts (e.g., crafty). With the tagless cache, the performance greatly depends on the size of the basic blocks executed. For large basic blocks (vortex and apsi), performance degrades as the instruction cache cannot fit all the instructions in the block (limit of 4). Similarly, for programs with many small blocks (2 or less instructions as in g721), the instruction cache capacity is underutilized. The tagless cache performs best for programs with basic blocks of size 4 instructions like pegwit. It is also best to combine the tagless instruction cache with prefetching to deal with conflict misses. Software hints tend to provide a consistent improvement for all of the benchmarks. The redistribute cache hints achieve slightly better performance than the exclude cache hints.</p><p>To understand the effectiveness of each technique in reducing the performance impact of the small instruction cache, we look at the instruction cache miss rates for the different optimizations. <ref type="figure">Figure  6</ref> presents the normalized number of instruction cache misses for BLISS with the different front-end optimizations over the base design with the small instruction cache. The reduction in instruction cache misses with prefetching, instruction re-ordering, and unified cache is consistent across most benchmarks with a 20% average. For the tagless instruction cache + prefetching, the decrease varies and largely depends on the basic block average size. Both of the software cache placement hints with prefetching significantly reduce the number of cache misses with an average of 58%. Finally, the best combination of the optimizations (prefetching + instruction re-ordering + unified cache + redistribute cache hints) achieves 66% reduction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cost Analysis</head><p>Power and die area determine the cost to manufacture and package the chip. The front-end consumes a large fraction of the power budget as it includes large memory structures that are accessed nearly every cycle. For example, the Intel XScale PXA270 processor consumes 20% of it power budget in the front-end itself. <ref type="table">Table  3</ref> quantifies the potential cost reduction of using small front-end structures for the XScale PXA270 processor in <ref type="table" target="#tab_3">Table 2</ref>. It presents the normalized power and area of the small front-end structures over the large structures for the XScale configuration. We also report the normalized access times for the small front-end structures. However, we ignore the fact that the processor with the small caches can run at higher clock frequency. The small instruction cache only dissipates 8.4% of the power dissipated by the large cache and uses only 4.6% of its area. The small predictor tables dissipate 75.4% of the power dissipated by the larger structures and use only 47.5% of the area. The small instruction cache access time is also half of the access time for the large cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Area Access Time Instruction Cache</head><p>8.4% 4.6% 50.7% <ref type="table">Predictor Tables  75.4%</ref> 47.5% 94.7% <ref type="table">Table 3</ref>: Normalized power dissipation, area, and access time for the small instruction cache and predictor tables over the large structures of the XScale configuration. <ref type="figure">Figure 7</ref> compares the total energy of BLISS with small caches and the various optimizations to that of the base design with large caches (energy of 1.0). Lower energy is better. For reference, the average total energy for other configurations is: 0.95 for the base design with small caches, 0.93 for the base design with small caches and prefetching, and 0.88 for BLISS with large caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Total Energy Analysis</head><p>With all optimizations, BLISS with small caches consumes less energy than the base with small or large caches. The combined optimizations lead to an energy consumption of 81%. The tagless instruction cache configuration provides significant energy benefits for several benchmarks (adpcm, jpeg, mesa, pegwit) as it eliminates redundant tag accesses. However, for vortex, the tagless instruction cache has the highest energy consumption. This is due to the fact that vortex has large basic blocks that will require to be prefetched and placed in the small victim cache. For the remaining optimizations, energy consumption tracks the IPC behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison Analysis to Hardware-based Techniques</head><p>Many techniques have been proposed to save the front-end power without the need for a new ISA. One such example is the Filter cache design proposed by <ref type="bibr">Kin et al. [13]</ref>. A Filter cache is a tiny cache introduced as the first level of memory in the instruction memory hierarchy.</p><p>Many of the front-end optimizations that are presented in Section 3 can also be implemented with a conventional instruction set using the Filter cache. <ref type="figure" target="#fig_6">Figure 8</ref> summarizes the comparison between BLISS with the combined optimizations (unified cache + prefetching + instruction re-ordering + redistribute hints) to the base design with (Filter cache + prefetching + instruction re-ordering + selective caching hints). Note that similar front-end optimizations and cache sizes are used with both designs. The base XScale configuration with the full-sized instruction cache and BTB is shown as a reference. We also show the results for the base design with optimally-sized caches. We use a method similar to <ref type="bibr" target="#b25">[26]</ref> to quantify the amount of energy wasted due to sub-optimal cache sizes. A continuum of cache sizes and configurations are simulated. During each cycle, the cache with the lowest power from among those that hit is selected.</p><p>BLISS with the front-end optimizations provides similar total power reduction to the Filter cache design and the base design with optimally-sized caches (14% savings). It also provides similar total energy savings to the optimally-sized design (19% reduction). The small advantage is due to the more efficient access of instruction cache in the BLISS base model <ref type="bibr" target="#b30">[31]</ref>. More important, the power and energy savings do not lead to performance losses as it is the case for the base design with the Filter cache. BLISS provides a 9% performance improvement over the base design with large caches and a 12% performance improvement over the base design with Filter cache and the combined front-end optimizations. The performance advantage is due to two reasons. First, the efficient imple- mentation of front-end optimizations mitigates the negative effects of the small instruction cache and BTB. Second, the block-aware architecture allows for higher prediction accuracy that provides the additional performance gains <ref type="bibr" target="#b30">[31]</ref>. In addition, BLISS provides 7% energy improvement over the base design with Filter cache and the combined front-end optimizations. Overall, BLISS with small caches and front-end optimizations improves upon the Filter cache with comparable front-end optimizations by offering similar power reduction at superior performance and energy consumption (12% performance and 7% total energy improvements). We only report IPC for the BLISS and the Filter cache designs, ignoring the opportunity for performance gains if we exploit the faster access time of the small caches. By reducing the clock period, the BLISS and Filter cache designs can run at higher clock frequencies than processors with larger caches which will result in additional performance and energy improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Significant amount of front-end research focused on instruction cache optimizations of microprocessor-based systems because of the cache's high impact on system performance, cost, and power. The use of a tiny (Filter) cache to reduce power dissipation was proposed by <ref type="bibr">Kin et al. [13]</ref>. Bellas et al. <ref type="bibr" target="#b1">[2]</ref> proposed using a profile-aware compiler to map frequent loops into the Filter cache to reduce the performance overhead. BLISS provides similar power reduction as the Filter cache design and at the same time improves performance and energy consumption. <ref type="bibr">Lee et al. [14]</ref> suggested using a tiny tagless loop cache with a controller that dynamically detect loops and fill the cache. The loop cache is an alternative to the first level of memory which is only accessed when a hit is guaranteed. Since the loop cache is not replacing the instruction cache, their approach improves the energy consumption with small performance, area, and total power overhead. Rose et al. <ref type="bibr" target="#b7">[8]</ref> evaluated different small cache designs.</p><p>Many techniques have been proposed to reduce the instruction cache energy. Some of the techniques include way prediction <ref type="bibr" target="#b21">[22]</ref>, selective cache way access <ref type="bibr" target="#b0">[1]</ref>, sub-banking <ref type="bibr" target="#b6">[7]</ref>, and tag comparison elimination <ref type="bibr" target="#b17">[18]</ref>, Other research has focused on reconfigurable caches <ref type="bibr" target="#b22">[23]</ref> where a subset of the ways in a set-associative cache or a subset of the cache banks are disabled during periods of modest cache activity to reduce power. Using a unified reconfigurable cache has also shown to be effective in providing greater levels of hardware flexibility <ref type="bibr" target="#b15">[16]</ref>. Even though reconfigurable caches are effective in reducing energy consumption, they have negligible effect on reducing the peak power or the processor die area.</p><p>Many prefetching techniques have been suggested to hide the latency of cache misses. The simplest technique is the sequential prefetching <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19]</ref>. In this scheme, one or more sequential cache lines that follow the current fetched line are prefetched. Historybased schemes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> use the patterns of previous accesses to initiate the new prefetches. The execution-based scheme has been proposed as an alternative approach <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6]</ref>. In this scheme, the prefetcher uses the predicted execution path to initiate accesses. Other types of prefetching schemes include wrong path prefetching <ref type="bibr" target="#b20">[21]</ref> and software cooperative approach <ref type="bibr" target="#b14">[15]</ref>. In the later scheme, the compiler inserts software prefetches for non-sequential misses. BLISS enables highly accurate execution-based prefetching using the contents of the BBQ.</p><p>Much research exists at exploring the benefit of code re-ordering <ref type="bibr" target="#b29">[30]</ref>. Most of the techniques use a variation of the code positioning algorithm suggested by Pettis and Hansen <ref type="bibr" target="#b19">[20]</ref>. Several researchers have also worked on using software-generated hints to improve the performance and power of caches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. BLISS efficiently enables instruction re-ordering with no extra overhead and no impact on speculation accuracy. Moreover, the architecturally visible basic block descriptors allow communicating software hints without modifying the conventional instruction stream or affecting its instruction code footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>This paper evaluated several front-end optimizations that improve the performance of embedded processors with small frontend caches. Small caches allow for an area and power efficient design but typically lead to performance challenges. The optimizations included instruction prefetching and re-ordering, selective caching, tagless instruction cache, and unified instruction and branch target caches. We built these techniques on top of the blockaware instruction set (BLISS) architecture that provides a flexible platform for both software and hardware front-end optimizations. The best performing combined optimizations (prefetching + instruction re-ordering + unified caches + redistribute cache hints) allow an embedded processor with small front-end caches to be 9% faster and consume 14% less power and 19% less energy than a similar pipeline with large front-end structures. While some of the optimizations can also be implemented with a conventional instruction set, they lead to lower performance benefits and are typically more complex. The BLISS ISA-supported front-end outperforms (12% IPC and 7% energy) a front-end with a conventional ISA with Filter cache and similar front-end optimizations. Overall, BLISS allows for low power and low cost embedded designs in addition to performance, energy, and code size advantages. Therefore, it can be a significant design option for embedded systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : Normalized execution time, total power, and total energy consumption for the base design ( 32 -KByte, 32 -way I- Cache, 64 -entry BTB), the base design with optimal I-Cache and BTB, and the base design with small front-end arrays ( 2 - KByte, 2 -way I-Cache, 16 -entry BTB). The processor core is similar to Intel's XScale PXA270 and is running benchmarks from the MediaBench and SpecCPU2000 suites. Lower bars present better results.</head><label>13232642216</label><figDesc>Figure 1: Normalized execution time, total power, and total energy consumption for the base design (32-KByte, 32-way ICache, 64-entry BTB), the base design with optimal I-Cache and BTB, and the base design with small front-end arrays (2-KByte, 2-way I-Cache, 16-entry BTB). The processor core is similar to Intel's XScale PXA270 and is running benchmarks from the MediaBench and SpecCPU2000 suites. Lower bars present better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 : The BLISS architecture. The top graph presents an embedded processor based on the BLISS ISA. The bottom graph shows the 32-bit basic block descriptor format.</head><label>2</label><figDesc>Figure 2: The BLISS architecture. The top graph presents an embedded processor based on the BLISS ISA. The bottom graph shows the 32-bit basic block descriptor format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : The organization of the tagless instruction cache with BLISS.</head><label>3</label><figDesc>Figure 3: The organization of the tagless instruction cache with BLISS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : Example to illustrate the instruction re-ordering optimization with BLISS. (a) Original BLISS code. (b) Re-ordered BLISS code. For illustration purposes, the instruction pointers in basic block descriptors are represented with arrows.</head><label>4</label><figDesc>Figure 4: Example to illustrate the instruction re-ordering optimization with BLISS. (a) Original BLISS code. (b) Re-ordered BLISS code. For illustration purposes, the instruction pointers in basic block descriptors are represented with arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : Normalized IPC for BLISS with the different front-end optimizations over the base. The BLISS design uses the small I-cache and BB-cache. The base design uses the regular I-cache and BTB. The 1. 0 line presents the base design. Higher bars present better performance.Figure 6 : Normalized number of instruction cache misses for BLISS with the different front-end optimizations over the base. The BLISS design uses the small I-cache and BB-cache. The base design uses the small I-cache and BTB. Lower bars present better results.</head><label>506</label><figDesc>Figure 5: Normalized IPC for BLISS with the different front-end optimizations over the base. The BLISS design uses the small I-cache and BB-cache. The base design uses the regular I-cache and BTB. The 1.0 line presents the base design. Higher bars present better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 : Normalized total energy comparison for BLISS with the different front-end optimizations over the base. The BLISS design uses the small I-cache and BB-cache. The base design uses the regular I-cache and BTB. The 1. 0 line presents the base design. Lower bars present better results.</head><label>70</label><figDesc>Figure 7: Normalized total energy comparison for BLISS with the different front-end optimizations over the base. The BLISS design uses the small I-cache and BB-cache. The base design uses the regular I-cache and BTB. The 1.0 line presents the base design. Lower bars present better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Base-Filter with optimizations BLISS-small with optimizationsFigure 8 : Average execution time, total power, and total energy consumption for base design (with large caches), base design (with optimal caches), base design (with Filter cache and a com- bination of front-end optimizations), and BLISS (with small caches and a combination of front-end optimizations). Lower bars present better results.</head><label>8</label><figDesc>Figure 8: Average execution time, total power, and total energy consumption for base design (with large caches), base design (with optimal caches), base design (with Filter cache and a combination of front-end optimizations), and BLISS (with small caches and a combination of front-end optimizations). Lower bars present better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The microarchitecture parameters for base and BLISS 
processor configurations used for power and area optimization 
experiments. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selective Cache Ways: On-Demand Cache Resource Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11" />
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy and Performance Improvements in Microprocessor Design using a Loop Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Conference on Computer Design</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-10" />
			<biblScope unit="page" from="378" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating Cache Hints for Improved Program Efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hollander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Architecture</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="223" to="250" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wattch: A Framework for Architectural-Level Power Analysis and Optimizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Computer Architecture</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06" />
			<biblScope unit="page" from="83" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simplescalar Tool Set, Version 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno>CS-TR-97-1342</idno>
		<imprint>
			<date type="published" when="1997-06" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instruction Prefetching Using Branch Prediction Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Conference on Computer Design</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10" />
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing Power in Superscalar Processor Caches Using Subbanking, Multiple Line Buffers and Bit-Line Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Kamble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Low Power Electronics and Design</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tiny Instruction Caches for Low Power Embedded Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon-Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="481" />
			<date type="published" when="2003-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Intel Itanium Architecture Software Developers Manual. Revision 2.0</title>
		<imprint>
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Intel PXA27x Processor Family Developer&apos;s Manual</title>
		<imprint>
			<date type="published" when="2004-10" />
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software-Assisted Cache Replacement Mechanisms for Embedded Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Conference on Computer-Aided Design</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prefetching using Markov Predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Computer Architecture</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Filter Cache: An Energy Efficient Memory Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-12" />
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instruction Fetch Energy Reduction Using Loop Caches for Embedded Applications with Small Tight Loops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arends</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Low Power Electronics and Design</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="267" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Architectural and Compiler Support for Effective Instruction Prefetching: a Cooperative Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="109" />
			<date type="published" when="2001-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Low Power Unified Cache Architecture Providing Power and Performance Flexibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cermak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Low Power Electronics and Design</title>
		<meeting><address><addrLine>Rapallo, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07" />
			<biblScope unit="page" from="241" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Program Optimization for Instruction Caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-04" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reducing the Frequency of Tag Compares for Low Power I-Cache Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rennels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Low Power Design</title>
		<meeting><address><addrLine>Dana Point, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-04" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Improved Lookahead Instruction Prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of High-Performance Computing on the Information Superhighway</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-05" />
			<biblScope unit="page" from="712" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Profile Guided Code Positioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pettis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Conference on Programming Language Design and Implementation</title>
		<meeting><address><addrLine>White Plains, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06" />
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wrong-Path Instruction Prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-12" />
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reducing Set-Associative Cache Energy via Way Prediction and Selective Direct-Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="page" from="54" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reconfigurable Caches and their Application to Media Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Computer Architecture</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fetch Directed Instruction Prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11" />
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Engineering the Complex SOC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rowen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Architecture-Level Power Optimization -What Are the Limits?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cacti 3.0: An Integrated Cache Timing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>2001/02</idno>
		<imprint>
			<date type="published" when="2001-08" />
			<pubPlace>Power, Area Model</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Compaq Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prefetching in Supercomputer Instruction Caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Conference on Supercomputing</title>
		<meeting><address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-11" />
			<biblScope unit="page" from="588" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Branch History Guided Instruction Prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on High-Performance Computer Architecture</title>
		<meeting><address><addrLine>Nuevo Leone, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-01" />
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Code Placement Techniques for Cache Miss Rate Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tomiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasuura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Design Automation of Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="429" />
			<date type="published" when="1997-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving Instruction Delivery with a Block-Aware ISA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zmily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of EuroPar Conference</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Energy-Efficient and High-Performance Instruction Fetch using a Block-Aware ISA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zmily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Intl. Symposium on Low Power Electronics and Design</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Block-Aware Instruction Set Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zmily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="357" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simultaneously Improving Code Size, Performance, and Energy in Embedded Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zmily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of Conference on Design, Automation and Test in Europe</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-03" />
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
