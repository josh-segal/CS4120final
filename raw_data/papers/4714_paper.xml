<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Phrase-Based Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
							<email>koehn@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
							<email>och@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
							<email>marcu@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Phrase-Based Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models out-perform word-based models. Our empirical results , which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly , learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Various researchers have improved the quality of statistical machine translation system with the use of phrase translation. Och et al. <ref type="bibr">[1999]</ref>'s alignment template model can be reframed as a phrase translation system; Yamada and Knight <ref type="bibr">[2001]</ref> use phrase translation in a syntaxbased translation system; Marcu and Wong <ref type="bibr">[2002]</ref> introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems 1 are augmented with phrase translation capability.</p><p>Phrase translation clearly helps, as we will also show with the experiments in this paper. But what is the best method to extract phrase translation pairs? In order to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table.</p><p>Our experiments show that high levels of performance can be achieved with fairly simple means. In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for researchers in the field. More sophisticated approaches that make use of syntax do not lead to better performance. In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models <ref type="bibr">[Ya- mada and Knight, 2001]</ref>, proves to be harmful. Our experiments also show, that small phrases of up to three words are sufficient for obtaining high levels of accuracy.</p><p>Performance differs widely depending on the methods used to build the phrase translation table. We found extraction heuristics based on word alignments to be better than a more principled phrase-based alignment method. However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Evaluation Framework</head><p>In order to compare different phrase extraction methods, we designed a uniform framework. We present a phrase translation model and decoder that works with any phrase translation table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>The phrase translation model is based on the noisy channel model. We use Bayes rule to reformulate the translation probability for translating a foreign sentence . We assume a uniform probability distribution over all possible segmentations.</p><p>Each foreign phrase</p><formula xml:id="formula_0">¡ ¢ § ¦ in ¡ ¢ ¤ £ ¥ is translated into an En- glish phrase ¦ ¥</formula><p>For all our experiments we use the same training data, trigram language model <ref type="bibr" target="#b11">[Seymore and Rosenfeld, 1997]</ref>, and a specialized decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder</head><p>The phrase-based decoder we developed for purpose of comparing different phrase-based translation models employs a beam search algorithm, similar to the one by Jelinek <ref type="bibr">[1998]</ref>. The English output sentence is generated left to right in form of partial translations (or hypotheses).</p><p>We start with an initial empty hypothesis. A new hypothesis is expanded from an existing hypothesis by the translation of a phrase as follows: A sequence of untranslated foreign words and a possible English phrase translation for them is selected. The English phrase is attached to the existing English output sequence. The foreign words are marked as translated and the probability cost of the hypothesis is updated.</p><p>The cheapest (highest probability) final hypothesis with no untranslated foreign words is the output of the search.</p><p>The hypotheses are stored in stacks. The stack</p><formula xml:id="formula_1">C § D</formula><p>contains all hypotheses in which E foreign words have been translated. We recombine search hypotheses as done by <ref type="bibr" target="#b8">Och et al. [2001]</ref>. While this reduces the number of hypotheses stored in each stack somewhat, stack size is exponential with respect to input sentence length. This makes an exhaustive search impractical.</p><p>Thus, we prune out weak hypotheses based on the cost they incurred so far and a future cost estimate. For each stack, we only keep a beam of the best F hypotheses. Since the future cost estimate is not perfect, this leads to search errors. Our future cost estimate takes into account the estimated phrase translation cost, but not the expected distortion cost.</p><p>We compute this estimate as follows: For each possible phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase. As language model probability we use the unigram probability for the first word, the bigram probability for the second, and the trigram probability for all following words.</p><p>Given the costs for the translation options, we can compute the estimated future cost for any sequence of consecutive foreign words by dynamic programming. Note that this is only possible, since we ignore distortion costs. Since there are only</p><formula xml:id="formula_2">F ¥ G F I H Q P S R</formula><p>such sequences for a foreign input sentence of length F , we can pre-compute these cost estimates beforehand and store them in a table.</p><p>During translation, future costs for uncovered foreign words can be quickly computed by consulting this table. If a hypothesis has broken sequences of untranslated foreign words, we look up the cost for each sequence and take the product of their costs.</p><p>The beam size, e.g. the maximum number of hypotheses in each stack, is fixed to a certain number. The number of translation options is linear with the sentence length. Hence, the time complexity of the beam search is quadratic with sentence length, and linear with the beam size.</p><p>Since the beam size limits the search space and therefore search quality, we have to find the proper trade-off between speed (low beam size) and performance (high beam size). For our experiments, a beam size of only 100 proved to be sufficient. With larger beams sizes, only few sentences are translated differently. With our decoder, translating 1755 sentence of length 5-15 words takes about 10 minutes on a 2 GHz Linux system. In other words, we achieved fast decoding, while ensuring high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods for Learning Phrase Translation</head><p>We carried out experiments to compare the performance of three different methods to build phrase translation probability tables. We also investigate a number of variations. We report most experimental results on a GermanEnglish translation task, since we had sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ <ref type="bibr" target="#b6">[Och and Ney, 2000</ref>] toolkit for the IBM models <ref type="bibr" target="#b0">[Brown et al., 1993]</ref>. The extraction heuristic is similar to the one used in the alignment template work by Och et al. <ref type="bibr">[1999]</ref>.</p><p>A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation <ref type="bibr" target="#b3">Imamura, 2002]</ref>. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers <ref type="bibr" target="#b1">[Collins, 1997;</ref><ref type="bibr" target="#b10">Schmidt and Schulte im Walde, 2000]</ref>.</p><p>The third method for comparison is the joint phrase model proposed by <ref type="bibr" target="#b5">Marcu and Wong [2002]</ref>. This model learns directly a phrase-level alignment of the parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrases from Word-Based Alignments</head><p>The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described in more detail in Section 4.5.</p><p>We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside <ref type="bibr" target="#b7">[Och et al., 1999]</ref>.</p><p>Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency:</p><formula xml:id="formula_3">© ¥ ¡ ¢ ¨ ¡ ¨ count¥ ¡ ¢ ¡ ¡ ¨ ¢ ¤ £ ¥ count¥ ¡ ¢ ¡ ¡ ¨</formula><p>No smoothing is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Syntactic Phrases</head><p>If we collect all phrase pairs that are consistent with word alignments, this includes many non-intuitive phrases. For instance, translations for phrases such as "house the" may be learned. Intuitively we would be inclined to believe that such phrases do not help: Restricting possible phrases to syntactically motivated phrases could filter out such non-intuitive pairs.</p><p>Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models <ref type="bibr" target="#b12">Wu, 1997]</ref>. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typically only translation of phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction.</p><p>Consistent with Imamura <ref type="bibr">[2002]</ref>, we define a syntactic phrase as a word sequence that is covered by a single subtree in a syntactic parse tree.</p><p>We collect syntactic phrase pairs as follows: We wordalign a parallel corpus, as described in Section 3.1. We then parse both sides of the corpus with syntactic parsers <ref type="bibr" target="#b1">[Collins, 1997;</ref><ref type="bibr" target="#b10">Schmidt and Schulte im Walde, 2000]</ref>. For all phrase pairs that are consistent with the word alignment, we additionally check if both phrases are subtrees in the parse trees. Only these phrases are included in the model. Hence, the syntactically motivated phrase pairs learned are a subset of the phrase pairs learned without knowledge of syntax (Section 3.1).</p><p>As in Section 3.1, the phrase translation probability distribution is estimated by relative frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Phrases from Phrase Alignments</head><p>Marcu and Wong <ref type="bibr">[2002]</ref> proposed a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well. To learn such correspondences, they introduced a phrase-based joint probability model that simultaneously generates both the Source and Target sentences in a parallel corpus. Expectation Maximization learning in Marcu and Wong's framework yields both (i) a joint probability distribution , which reflects the probability that a phrase at position is translated into a phrase at position ¦ . To use this model in the context of our framework, we simply marginalize to conditional probabilities the joint probabilities estimated by Marcu and Wong <ref type="bibr">[2002]</ref>. Note that this approach is consistent with the approach taken by Marcu and Wong themselves, who use conditional models during decoding. </p><note type="other">Training corpus size Method 10k 20k 40k 80k 160k 320k AP 84k 176k 370k 736k 1536k 3152k Joint 125k 220k 400k 707k 1254k 2214k Syn 19k 24k 67k 105k 217k 373k</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We used the freely available Europarl corpus 2 to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing.</p><p>In all experiments in Section 4.1-4.6 we translate from German to English. We measure performance using the BLEU score <ref type="bibr" target="#b9">[Papineni et al., 2001]</ref>, which estimates the accuracy of translation output with respect to a reference translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of Core Methods</head><p>First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model. <ref type="figure" target="#fig_1">Fig- ure 1</ref> displays the results.</p><p>In direct comparison, learning all phrases consistent with the word alignment (AP) is superior to the joint model (Joint), although not by much. The restriction to only syntactic phrases (Syn) is harmful. We also included in the figure the performance of an IBM Model 4 wordbased translation system (M4), which uses a greedy decoder <ref type="bibr" target="#b2">[Germann et al., 2001]</ref>. Its performance is worse than both AP and Joint. These results are consistent over training corpus sizes from 10,000 sentence pairs to 320,000 sentence pairs. All systems improve with more data. <ref type="table" target="#tab_1">Table 1</ref> lists the number of distinct phrase translation pairs learned by each method and each corpus. The number grows almost linearly with the training corpus size, due to the large number of singletons. The syntactic restriction eliminates over 80% of all phrase pairs.</p><p>Note that the millions of phrase pairs learned fit easily into the working memory of modern computers. Even the largest models take up only a few hundred megabyte of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weighting Syntactic Phrases</head><p>The restriction on syntactic phrases is harmful, because too many phrases are eliminated. But still, we might suspect, that these lead to more reliable phrase pairs. <ref type="bibr">2</ref> The  One way to check this is to use all phrase pairs and give more weight to syntactic phrase translations. This can be done either during the data collection -say, by counting syntactic phrase pairs twice -or during translation -each time the decoder uses a syntactic phrase pair, it credits a bonus factor to the hypothesis score.</p><p>We found that neither of these methods result in significant improvement of translation performance. Even penalizing the use of syntactic phrase pairs does not harm performance significantly. These results suggest that requiring phrases to be syntactically motivated does not lead to better phrase pairs, but only to fewer phrase pairs, with the loss of a good amount of valuable knowledge.</p><p>One illustration for this is the common German "es gibt", which literally translates as "it gives", but really means "there is". "Es gibt" and "there is" are not syntactic constituents. Note that also constructions such as "with regard to" and "note that" have fairly complex syntactic representations, but often simple one word translations. Allowing to learn phrase translations over such sentence fragments is important for achieving high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Maximum Phrase Length</head><p>How long do phrases have to be to achieve high performance? <ref type="figure" target="#fig_2">Figure 2</ref>     <ref type="table" target="#tab_4">Table 2</ref>). The increase is almost linear with the maximum length limit. Still, none of these model sizes cause memory problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Lexical Weighting</head><p>One way to validate the quality of a phrase translation pair is to check, how well its words translate to each other.   <ref type="figure" target="#fig_5">Figure 4</ref> shows the impact of lexical weighting on machine translation performance. In our experiments, we achieved improvements of up to 0.01 on the BLEU score scale. Again, all phrases consistent with the word alignment are used (Section 3.1).</p><formula xml:id="formula_4">( ¥ ¢ ¦ ¨ ¨ $ f1 f2 f3 NULL ----## e1 ## ---- e2 --## -- e3 --## -- ' ) ( 1 0 ) 2 3 ¤ 4 2 5 7 6 9 8 A @ C B ' D ( E 0 3 7 F G 3 I H P 3 I Q R 4 5 F 5 H 5 Q 6 9 8 A @ B S T 0 3 R F U 4 5 F @ V X W Y 0 ` S T 0 3 H 4 5 H @ b a c S d 0 3 H 4 5 Q @ 9 @ V S T</formula><p>Note that phrase translation with a lexical weight is a special case of the alignment template model <ref type="bibr" target="#b7">[Och et al., 1999]</ref> with one word class for each word. Our simplification has the advantage that the lexical weights can be factored into the phrase translation table beforehand, speeding up decoding. In contrast to the beam search decoder for the alignment template model, our decoder is able to search all possible phrase segmentations of the input sentence, instead of choosing one segmentation before decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Phrase Extraction Heuristic</head><p>Recall from Section 3.1 that we learn phrase pairs from word alignments generated by Giza++. The IBM Models that this toolkit implements only allow at most one English word to be aligned with a foreign word. We remedy this problem with a heuristic approach. First, we align a parallel corpus bidirectionally -foreign to English and English to foreign. This gives us two word alignments that we try to reconcile. If we intersect the two alignments, we get a high-precision alignment of high-confidence alignment points. If we take the union of the two alignments, we get a high-recall alignment with additional alignment points.</p><p>We explore the space between intersection and union with expansion heuristics that start with the intersection and add additional alignment points. The decision which points to add may depend on a number of criteria:</p><p>In which alignment does the potential alignment point exist? Foreign-English or English-foreign?</p><p>Does the potential point neighbor already established points?</p><p>Does "neighboring" mean directly adjacent (blockdistance), or also diagonally adjacent?</p><p>Is the English or the foreign word that the potential point connects unaligned so far? Are both unaligned?</p><p>What is the lexical probability for the potential point?</p><p>The base heuristic <ref type="bibr" target="#b7">[Och et al., 1999]</ref> proceeds as follows: We start with intersection of the two word alignments. We only add new alignment points that exist in the union of two word alignments. We also always require that a new alignment point connects at least one previously unaligned word.</p><p>First, we expand to only directly adjacent alignment points. We check for potential points starting from the top right corner of the alignment matrix, checking for alignment points for the first English word, then continue with alignment points for the second English word, and so on. This is done iteratively until no alignment point can be added anymore. In a final step, we add non-adjacent alignment points, with otherwise the same requirements.  <ref type="figure" target="#fig_6">Figure 5</ref> shows the performance of this heuristic (base) compared against the two mono-directional alignments (e2f, f2e) and their union (union). The figure also contains two modifications of the base heuristic: In the first (diag) we also permit diagonal neighborhood in the iterative expansion stage. In a variation of this (diag-and), we require in the final step that both words are unaligned.</p><p>The ranking of these different methods varies for different training corpus sizes. For instance, the alignment f2e starts out second to worst for the 10,000 sentence pair corpus, but ultimately is competitive with the best method at 320,000 sentence pairs. The base heuristic is initially the best, but then drops off.</p><p>The discrepancy between the best and the worst method is quite large, about 0.02 BLEU. For almost all training corpus sizes, the heuristic diag-and performs best, albeit not always significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Simpler Underlying Word-Based Models</head><p>The initial word alignment for collecting phrase pairs is generated by symmetrizing IBM Model 4 alignments. Model 4 is computationally expensive, and only approximate solutions exist to estimate its parameters. The IBM Models 1-3 are faster and easier to implement. For IBM Model 1 and 2 word alignments can be computed efficiently without relying on approximations. For more information on these models, please refer to Brown et al. <ref type="bibr">[1993]</ref>. Again, we use the heuristics from the Section 4.5 to reconcile the mono-directional alignments obtained through training parameters using models of increasing complexity.</p><p>How much is performance affected, if we base word alignments on these simpler methods? As <ref type="figure">Figure 6</ref>   cates, not much. While Model 1 clearly results in worse performance, the difference is less striking for Model 2 and 3. Using different expansion heuristics during symmetrizing the word alignments has a bigger effect. We can conclude from this, that high quality phrase alignments can be learned with fairly simple means. The simpler and faster Model 2 provides similar performance to the complex Model 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Other Language Pairs</head><p>We validated our findings for additional language pairs. <ref type="table" target="#tab_5">Table 3</ref> displays some of the results. For all language pairs the phrase model (based on word alignments, Section 3.1) outperforms IBM Model 4. Lexicalization (Lex) always helps as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We created a framework (translation model and decoder) that enables us to evaluate and compare various phrase translation methods. Our results show that phrase translation gives better performance than traditional word-based methods. We obtain the best results even with small phrases of up to three words. Lexical weighting of phrase translation helps.</p><p>Straight-forward syntactic models that map constituents into constituents fail to account for important phrase alignments. As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings. This is a challenge for syntactic translation models.</p><p>It matters how phrases are extracted. The results suggest that choosing the right alignment heuristic is more important than which model is used to create the initial word alignments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the core methods: all phrase pairs consistent with a word alignment (AP), phrase pairs from the joint model (Joint), IBM Model 4 (M4), and only syntactic phrases (Syn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different limits for maximum phrase length show that length 3 is enough</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For this, we need a lexical translation probability distribu- tion ¥ ¢ ¨ ¨ . We estimated it by relative frequency from the same word alignments as the phrase model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 3: Lexical weight</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Lexical weighting (lex) improves performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Different heuristics to symmetrize word alignments from bidirectional Giza++ alignments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 6: Using simpler IBM models for word alignment does not reduce performance much</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Size of the phrase translation table in terms of distinct phrase pairs (maximum phrase length 4)</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Size of the phrase translation table with varying maximum phrase length limits per phrase already achieves top performance. Learning longer phrases does not yield much improvement, and occasionally leads to worse results. Reducing the limit to only two, however, is clearly detrimental. Allowing for longer phrases increases the phrase trans- lation table size (see</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Confirmation of our findings for additional lan-
guage pairs (measured with BLEU) 

</table></figure>

			<note place="foot" n="1"> Presentations at DARPA IAO Machine Translation Workshop, July 22-23, 2002, Santa Monica, CA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="313" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three generative, lexicalized models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 35</title>
		<meeting>ACL 35</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast decoding and optimal decoding for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 39</title>
		<meeting>ACL 39</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Application of translation knowledge acquired by hierarchical phrase alignment for pattern-based mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TMI</title>
		<meeting>TMI</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical Methods for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A phrase-based, joint probability model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 38</title>
		<meeting>ACL 38</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>of the Joint Conf. of Empirical Methods in Natural Language essing and Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient A* search algorithm for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DataDriven MT Workshop</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>RC22176(W0109-022</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
	<note>IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust German noun chunking with a probabilistic context-free grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical language modeling using the CMU-Cambridge toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A syntax-based statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 39</title>
		<meeting>ACL 39</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
