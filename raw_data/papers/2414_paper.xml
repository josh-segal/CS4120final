<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dolly: Virtualization-driven Database Provisioning for the Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Cecchet</surname></persName>
							<email>cecchet@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sharma</surname></persName>
							<email>upendra@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Shenoy</surname></persName>
							<email>shenoy@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dolly: Virtualization-driven Database Provisioning for the Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D29 [Software Engineering]: Management General Terms Algorithms</term>
					<term>Management</term>
					<term>Measurement</term>
					<term>Performance</term>
					<term>Design</term>
					<term>Experimentation Keywords Database</term>
					<term>Autonomic Provisioning</term>
					<term>Virtualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Cloud computing platforms are becoming increasingly popular for e-commerce applications that can be scaled on-demand in a very cost effective way. Dynamic provisioning is used to autonomously add capacity in multi-tier cloud-based applications that see workload increases. While many solutions exist to provision tiers with little or no state in applications, the database tier remains problematic for dynamic provisioning due to the need to replicate its large disk state. In this paper, we explore virtual machine (VM) cloning techniques to spawn database replicas and address the challenges of provisioning shared-nothing replicated databases in the cloud. We argue that being able to determine state replication time is crucial for provisioning databases and show that VM cloning provides this property. We propose Dolly, a database provisioning system based on VM cloning and cost models to adapt the provisioning policy to the cloud infrastructure specifics and application requirements. We present an implementation of Dolly in a commercial-grade replication middleware and evaluate database provisioning strategies for a TPC-W workload on a private cloud and on Amazon EC2. By being aware of VM-based state replication cost, Dolly can solve the challenge of automated provisioning for replicated databases on cloud platforms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Online applications have become popular in a variety of domains such as e-retail, banking, finance, news, and social networking. Typically such web-based applications are hosted in data-centers or on cloud computing platforms, which provide storage and computing resources to these applications. Numerous studies have shown that the workloads seen by these web-based cloud applications are highly dynamic and exhibit variations at different time-scales <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>. For instance, an application may see a rapid increase in its popularity, causing its workload to grow sharply over a period of days or weeks. At shorter time-scales, a flash crowd can cause the application workload to surge within minutes. Applications can also see seasonal trends such as higher workloads during particular periods, e.g., during Black Friday, marketing campaigns, or a new product launch.</p><p>One possible approach for handling workload fluctuations is to employ dynamic provisioning of server capacity. Dynamic provisioning involves increasing or decreasing the number of servers (and server capacity) allocated to an application in response to workload changes. Dynamic provisioning is especially well-suited to web-based cloud applications for two reasons. First, it is often difficult to estimate the peak workload of an Internet application, making it challenging to a priori provision for the peak demand. Second, today's cloud platforms support ondemand allocation of servers and employ a pay-as-you-go service model. These features are attractive from an application provider's perspective, since servers can be requested only when a workload spike arrives or is anticipated, and charging is based only on the duration of the workload surge. Cloud platforms employ virtualization to support these features-upon a customer request for a new (virtual) server, a new virtual machine (VM) is created on a physical server with idle capacity, and the specified virtual disk image is copied to the server, upon which the server is ready for use. In fact, cloud platforms such as Amazon's EC2 platform already support dynamic provisioning (aka "auto scaling") where such VMs are automatically started when a threshold on a userspecified metric such as CPU utilization is exceeded in the current application <ref type="bibr" target="#b0">[1]</ref>.</p><p>Much of the prior work on dynamic provisioning <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b3">[4]</ref> has assumed that web applications have a multi-tier architecture and focus on dynamic provisioning of the front web tier or the middle application tier. Provisioning of these front and middle tiers is simple since these tiers have little or no application state and provisioning merely involves dynamic startup (or shutdown) of VMs in response to workload fluctuations. This prior work assumes that the backend database tier, where much of the application state is stored, is over-provisioned and thus does not require dynamic provisioning. However, in scenarios where the database tier is the bottleneck (e.g., due to compute-intensive query workloads), this simplifying assumption has meant that our inability to a priori estimate the peak workload for Internet applications will cause the database tier to become overloaded and drop user requests. Further it prevents the web application from fully exploiting the benefits of the pay-as-you-go and on-demand server allocation in the cloud for the backend tier. Dynamic provisioning of the back-end database tier has not been considered in the prior literature since it is harder to implementreplication and synchronization of the associated disk state of the database needs to be handled, in addition to the 'simpler' problem of starting up new database VM replicas.</p><p>In this paper, we consider the problem of dynamic provisioning of the database tier of online web applications. We use virtualization as a key building block of our dynamic provisioning system, in particular by leveraging VM snapshots and cloning as the basis for replicating database state in a platform-independent manner. In addition, we devise intelligent state replication strategies to reduce the latency of starting up new database replicas in virtualized public and private clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Why is database provisioning hard?</head><p>Dynamic provisioning of server capacity typically involves two problems: when to trigger a capacity increase (or decrease), and how to achieve the desired capacity addition or reduction. Both the "when" and the "how" questions are simpler in case of the web and application tiers than the database tier.</p><p>Typically the decision of when to trigger provisioning is made in a lazy fashion for the web and the application tiers-upon an actual significant workload change, or an anticipation of one in the near future. Such lazy triggers are appropriate for these tiers since front-end provisioning schemes assume that new capacity can be added immediately whenever needed and that the only latency is that incurred for VM startup. In contrast, provisioning of a new database replica involves (i) extracting database content from an existing replica, if not already available, and (ii) copying and restoring that content on a new replica. These operations can take minutes or hours depending on the database size.</p><p>In fact, traditional "just-in-time" cloud provisioning techniques, including Amazon Auto Scaling <ref type="bibr" target="#b0">[1]</ref>, are similarly based on lazy triggers and/or thresholds and do not take into account the time to replicate the database state. If this state replication and synchronization overhead is ignored, the newly provisioned capacity comes online far too late to handle the workload increase and the capacity requirements will not be met in a timely fashion.</p><p>Similarly the "how" to achieve the desired capacity increase must be handled differently in case of dynamic database provisioning. Typically this part involves (i) a capacity determination model to estimate how many replicas to provision for a given workload, and (ii) the actual system steps necessary in starting up and configuring those replicas for use. Capacity determination models predict the future workload using historical data or dynamic predictors <ref type="bibr" target="#b10">[11]</ref> and then use queuing techniques to estimate the number of replicas needed to service the predicted workload <ref type="bibr" target="#b8">[9]</ref>. This aspect of provisioning is similar for both the front-end web and the back-end database tier. In fact, one of the few papers to address dynamic provisioning of the database tier <ref type="bibr" target="#b7">[8]</ref> proposed an analytical model for databases to determine capacity needed to service a given workload. However, this work did not address the important systems issues of "when" to trigger provisioning based on state replication overheads, nor did it address the many system challenges involved in dynamically starting up database replicas. Specifically, in database provisioning, even after a VM replica starts up, there is an additional overhead of synchronizing the state of the new replica with the current state of all other replicas to preserve data integrity. No such overheads are incurred when provisioning "stateless" web and application tier replicas.</p><p>Thus, database provisioning differs significantly from traditional web server provisioning because databases are stateful and their state can be very large (and this state must be replicated before a new database replica can be spawned). To provision database replicas in a timely fashion, it is necessary to know how much time will be required to replicate/synchronize this disk state and bring the replicas online. These times vary greatly with the database size, schema complexity, backup/restore tool options, database artifacts (e.g., storage engine configuration). Moreover, there are many tradeoffs on how and when to snapshot the database state to minimize replica resynchronization time. It is therefore non-trivial to estimate the exact time needed to spawn a new replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Research Contributions</head><p>In this paper, we present Dolly 1 , a system for dynamically provisioning database replicas in cloud platforms. Dolly is database platform-agnostic and uses virtualization-based replication mechanisms for efficiently spawning database replicas.</p><p>The key insight in Dolly is to intelligently use VM snapshots and cloning as the basis for dynamic database provisioning. In Dolly, each database replica runs in a separate virtual machine. Instead of relying on the traditional database mechanisms to create a new replica, Dolly clones the entire virtual machine (VM) of an existing replica, including the operating environment, the database engine with all its configuration, settings and the database itself. The cloned VM is started on a new physical server, resulting in a new replica, which then synchronizes state with other replicas prior to processing user requests.</p><p>Our work on Dolly has led to the following contributions:</p><p>• When to provision: Dolly takes the long latency of spawning database replicas into account when triggering "eager" provisioning decisions. To do so, Dolly incorporates a model to estimate the latency to spawn a replica, based on the VM snapshot size and the database resynchronization latency, and uses this model to trigger the replica spawning process well in advance of the anticipated workload increase.</p><p>• How to provision: Dolly incorporates an intelligent scheduling technique that can determine whether it is cheaper to take a new VM snapshot or use an older snapshot when spawning a new replica. In addition, the technique can proactively trigger VM snapshots to reduce the future latency of spawning database replicas. These mechanisms are implemented in a new provisioning algorithm, with userdefined cost functions to characterize database provisioning policies on cloud platforms. This allows the system administrator to tune the provisioning decisions to optimize resource usage of her cloud infrastructure.</p><p>• Prototype implementation: We have developed a prototype of Dolly using Sequoia <ref type="bibr" target="#b14">[16]</ref>, a commercial-grade open-source database clustering middleware, and have combined it with the OpenNebula <ref type="bibr">[14]</ref> cloud manager to address provisioning in both private and public clouds. We demonstrate the efficacy of Dolly in provisioning Mysqlbased database tiers.</p><p>• Evaluation on public and private clouds: We conduct an experimental evaluation of Dolly on Amazon's EC2 public cloud and on a laboratory-based Xen private cloud. Our experiments with a TPC-W <ref type="bibr" target="#b17">[19]</ref> e-commerce workload show the ability of Dolly to properly schedule provisioning decisions to meet capacity requirements in a timely fashion while optimizing resource usage in private clouds and minimizing cost in public clouds.</p><p>The remainder of this paper is organized as follows. Section 2 introduces the necessary background on database replication and replica spawning. Section 3 discusses the core techniques for database provisioning in the cloud and when to provision, while section 4 addresses how to provision. Section 5 presents Dolly's implementation. We perform an experimental evaluation on private and public clouds in Section 6. Finally, Section 7 discusses related work before concluding in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section, we present background on virtualized cloud platforms and database replication and also formulate the problem of dynamic database provisioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Virtualized Cloud Platforms</head><p>Our work assumes a virtualized cloud platform that runs distributed web-based applications. The cloud platform is essentially a data center that provides compute and storage resource to its applications. Each physical server in the data center is assumed to run a virtual machine monitor (aka hypervisor) and one or more virtual machines that encapsulate application components. The cloud platform, whether public or private, is assumed to support on-demand allocation of virtual machinesapplications can request one or more virtual machines at any time, upon which the requested VMs are created, placed on to physical servers with idle capacity and started up. We assume that application components are preconfigured as virtual disk images that are available to the cloud platform, enabling automated VM allocation and startup. Such on-demand VM allocation is a prerequisite for our dynamic provisioning techniques. Our work targets both public cloud platforms such as Amazon EC2 as well as private Linux-based clouds constructed using Xen/KVM virtualization platforms. In case of public clouds, where servers and storage is charged based on a pay-as-you-use model, we assume that the pricing model is known a priori and can be taken into account when making provisioning decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Formulation</head><p>We assume that cloud platforms run distributed web applications. Each application is assumed to employ a multi-tier architecture consisting of a front-end web tier, a middle application (e.g., J2EE) tier, and a backend database tier. Each tier is assumed to be dynamically replicable. That is, each tier is assumed to comprise one or more VM replicas, and it is assumed that the number of replicas at each tier can be varied based on changing workload demands at that tier. We assume that each tier also assumes a dispatcher/load-balancer that is responsible for distributing requests to various replicas.</p><p>Our work focuses on the database tier. In contrast to prior work that has typically assumed a static number of (over-provisioned) replicas at the database tier, we assume that this tier can also be dynamically provisioned like the other tiers. The dynamic provisioning problem for the database tier can then be stated as two sub-tasks: (i) when to trigger a capacity change based on current and future workload trends, and (ii) how to startup or shutdown the desired number of VM replicas. As discussed above, both tasks raise new challenges when dynamically provisioning databases. Our goal is to design a database provisioning platform that (i) given future workload forecasts, will estimate the time to start up a new database replica and will use this latency to trigger a provisioning change sufficiently in advance of the anticipated change, and (ii) uses an intelligent algorithm that takes user-specified cost functions to optimize the overheads of starting up (or terminating) the desired number of replicas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Database Replication</head><p>Before presenting our provisioning technique, we present brief background on database replication. Dynamic database provisioning assumes that the underlying database platform is replicable and clusterable. In general, there are two primary architectures for implementing database replication: shared-disk and shared-nothing. In the shared-disk architecture, there is a single copy of the data on a shared disk (SAN or NAS) that is accessed by all replicas. Typically shared-disk database platforms such as Oracle RAC <ref type="bibr" target="#b12">[13]</ref> require specific hardware (in the form of shared disk systems) that may not be available in commodity cloud platforms.</p><p>In the shared-nothing architecture, there are multiple database server processes that run on different machines, and each replica has a copy of the database content on its local disk. Consistency is maintained across replicas using LAN communications. Dolly currently assumes a shared-nothing architecture since they are well suited for today's cloud platforms and also commonly used in multi-tier web applications.</p><p>Within shared-nothing systems, there are two main replication strategies: master-slave and multi-master. In master-slave, updates are sent to a single master node and lazily replicated to slave nodes. Data on slave nodes might be stale and it is the responsibility of the application to check for data freshness when accessing a slave node. Multi-master replication enforces a serializable execution order of transactions between all replicas so that each of them applies update transactions in the same order. This way, any replica can serve any read or write request.</p><p>Further, replication can be implemented inside the database engine, also known as in-core replication, or externally to the database, commonly called middleware-based replication. The technique to add a new replica is similar in both environments. In both architectures, transactions are balanced among the replicas and are stored in a transactional log (also called recovery log). The middleware design usually keeps a separate transactional log for replication, whereas the in-core approach stores the information in each database's replica transactional log. <ref type="figure" target="#fig_0">Figure 1</ref> shows the steps to spawn a replica in a middleware-based replication environment. First, a command to add a new replica is issued from the management console to the replication middleware. A checkpoint is then created in the transactional log (step 2) and a replica is temporarily taken out of the cluster to take a snapshot (also called database dump) of the database content (step 3 via DB 2 ). As soon as the snapshot has been taken, this replica is resynchronized by replaying the transactions written in the transactional log since the checkpoint (step 4) and it rejoins the cluster. A new replica is then started on a separate node, and the snapshot is seeded to this new replica using a restore operation (step 5). Finally, the updates that have occurred since the snapshot was taken are replayed from the transactional log (step 6) to resynchronize the new replica and bring it up-to-date with all other replicas in the system. Conceptually, the above steps for replica creation can be classified into three key phases: (i) the backup phase, where database content is extracted from an existing replica and moved to a new node, (ii) the restore phase, where a new replica is seeded with this snapshot, and (iii) the replay phase, where the replica is resynchronized with others by replaying new updates from the transactional log.</p><p>As we will see in the Dolly design, the use of virtualization simplifies these steps. Dolly employs VM snapshots to implement the backup phase and uses VM cloning to restore the snapshot onto a new replica. By doing so, Dolly is database-platform agnostic, since it relies on the virtualization platform, rather than native platform-specific tools, to implement provisioning via backup/restore. VM cloning is independent of the database schema complexity and eliminates common backup issues of database specific extensions and configurations <ref type="bibr" target="#b6">[7]</ref>.</p><p>Further, the Dolly design is general and can accommodate both master-slave and multi-master shared-nothing architectures; our current implementation, however, is based on a multi-master middleware-based replication and is implemented on Sequoia, a commercial-grade database clustering middleware <ref type="bibr" target="#b14">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dolly: When to Provision</head><p>In this section, we first describe the high-level approach used in Dolly to provision database replicas via VM snapshot and cloning and then present a model to estimate the latency of these operations when provisioning a new replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Replica Spawning via VM Cloning</head><p>Dolly uses the ability to make VM snapshots and clone VMs to efficiently replicate database state and start new replicas. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the high-level approach to spawn 2 new replicas in a private cloud. First, the virtual machine (VM) running a database replica is stopped on machine 1 and cloned to be stored on a backup server (machine B). Two new replicas are then spawned by cloning the VM from the backup server and starting these new VMs. Dolly minimizes the downtime of the existing replica that is being cloned by exploiting VM or file-system-level snapshots. A file-system or VM-level snapshot <ref type="bibr" target="#b4">[5]</ref> is a point-in-time copy of the virtual disk image; snapshots can be made efficiently, after which the original VM replica can be resumed immediately and the snapshot image can be copied to the other machine(s) in the background.    to a backup server in a private cloud. When a new VM is created from an EBS snapshot, a clone of that volume is created and dedicated to the newly started instance. In our case, we assume that the database server disk state (configuration file and data within the database) are stored on the EBS volume; thus snapshots and booting a new VM from the snapshot is an effective mechanism to replicate the shared-nothing database content and start up a new database replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Determining replica spawning time</head><p>Dolly must accurately estimate the overheads of the above VM snapshot and cloning operations in order to intelligently trigger the spawning of new replica(s). We now present a simple model to estimate this latency.</p><p>In general, there is a tradeoff between the time to snapshot/clone a database/VM, the size of the transactional log and the amount of update transactions in the workload. For example, a new replica can be seeded with an old snapshot (e.g., a snapshot that was taken to seed a different replica), which eliminates the backup phase overhead. However, use of an older snapshot forces the system to keep a larger transactional log and also increases the time to replay updates from this log during the replay phase. On the other hand, taking a new snapshot for each new replica may incur significant overheads during the backup phase, especially if the database is large. By analyzing the overheads of these operations, Dolly can choose the option with the lower latency.</p><p>The replica spawning overhead can be analyzed using the five variables defined in <ref type="table" target="#tab_1">Table 1</ref>.  When no snapshot is available, it is necessary to perform a new backup and restore, yielding an overhead of (b i +r i ) as shown on <ref type="figure" target="#fig_4">Figure 4</ref>. The replay phase then replays all updates that have occurred during this period. We can estimate the replay time by observing the current rate of update transactions and assume that it will remain a valid approximation during the replay time. The new replica will be able to replay the requests at w max speed since it does not have to execute any other transaction. Therefore, the time to replay the updates that occurred during backup/restore is ∑ If the system is under peak load, w t = w max , the replica will never be able to catch up and it will have a lag of b i +r i . We find the equations in Since replay i can be accurately predicted, having a constant b j and r j , that are independent of the database size or complexity, allows Dolly to decide if b j &lt;replay i in which case it is faster to take a new snapshot than to use an existing one to spawn a new replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dolly: How to Provision</head><p>Our Dolly provisioning system has four main components: capacity provisioning engine, snapshot scheduler, paused pool cleaner and actuator. Typically, the provisioning engine will employ a workload predictor (Section 4.1) that observes the behavior of the system. To provision a certain capacity by a given deadline, it is necessary to schedule capacity provisioning actions according to the time it takes to replicate the database state (Section 4.2). As replicas have to be spawned from a database snapshot, the snapshot scheduler decides when new database snapshots (VM clones) have to be taken (Section 4.3). Some stopped or paused VMs become obsolete over time and need to be purged by the paused pool cleaner (Section 4.4 To adapt provisioning policies to the target cloud platform, Dolly uses cost functions to allow the administrator to define which option is best if multiple strategies are available. The cost can model any metric like time, resource usage or actual resource cost as we will show in the next sections. <ref type="table" target="#tab_6">Table 3</ref> lists the seven cost functions used by Dolly and the definitions for each. cost to backup an active VM at time t <ref type="table" target="#tab_7">Table 4</ref> summarizes the variables used to measure the time used by the different operations used by the algorithms described in this section.  <ref type="figure">Figure 5</ref>. Example of a capacity and write workload prediction over time. Dolly provision replicas based on the forecast available in the prediction window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Capacity and workload predictors</head><p>Previous work has established how to predict replicated database capacity based on a standalone node measurement <ref type="bibr" target="#b8">[9]</ref>. This allows forecasting performance scalability and identifying potential bottlenecks. Many models exist for workload prediction <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[20]</ref>. Dolly does not assume any particular workload predictor or capacity model; it can use any existing approach and can be a platform to test new predictors or improve existing ones. Depending on the capacity and workload predictors used, the forecast will have a limited visibility in the future. Web sites with stable workloads might have accurate static weekly predictions possibly adjusted by administrators for seasonal peaks. More dynamically changing workloads can be less predictable and only sketch the demand for the next hour or so. We call prediction window the time between now and the latest time in the future for which the load and capacity demand can be predicted. <ref type="figure">Figure 5</ref> shows an example of capacity demand and write throughput of a replicated database. The prediction window slides as time goes on. Prediction windows are not necessarily of a fixed size since a predictor can dynamically change the technique it uses to forecast the load thus increasing or decreasing the prediction window size. Dolly has to schedule provisioning decisions for deadlines d 1 , d 2 and d 3 , where the capacity demand changes in the prediction window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Provisioning replicas</head><p>The provisioning algorithm scans the prediction window and looks for deadlines where changes in workload require additional capacity (such as time d 1 and d 3 on <ref type="figure">Figure 5</ref>) or less capacity (such as time d 2 on <ref type="figure">Figure 5</ref>). The algorithm handles all deadlines in sequence. In <ref type="figure">Figure 5</ref>, d 1 is handled first. Once a schedule has been found for d 1 , it moves to d 2 and so on. The algorithm works in two phases for each deadline: 1) list all possible options for replica spawning or releasing and 2) sort these options according to a cost function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Decreasing capacity</head><p>When the capacity requirements decrease, replicas that are no longer needed are paused. The replication engine keeps track of the state of each stopped virtual machine replica so that it knows exactly what has to be replayed when the VM is resumed. A similar state is saved in the slave nodes for master/slave replication. When a VM is stopped in a private cloud, its image still resides on the machine's local disk. As we might want to resume that image later, we do not return the machine to the free server pool but it is put it in a special paused server pool. The machine can be shutdown as long as it is in the paused pool. A machine can be reclaimed from the paused server pool by the private cloud infrastructure if the free pool is empty and additional capacity is required for other databases or tiers. In a public cloud like EC2, the computing instance is simply detached from the storage and can be re-attached later to any other instance. On a private cloud, the administrator might prefer to switch off the hottest machines to improve cooling. If the capacity has to be reduced by r replicas at time d, the algorithm schedules the r replicas that have the lowest pause_cost for pausing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Increasing capacity</head><p>When an increase in capacity is predicted at deadline d, the algorithm explores all replica spawning options from snapshots and paused VMs.</p><p>In our system, the replicated database always has at least one snapshot available for creating new replicas. The first snapshot is created when initializing the system as shown on <ref type="figure">Figure 5</ref>, and snapshots are updated regularly when needed, as will be explained in section 4.3. When new replicas are spawned from a snapshot, we can predict the time it takes to bring the replica online using the formula described in section 3.2.</p><p>Dolly looks at all available snapshots that can spawn replicas in time to meet deadline d, as well as all paused VMs that can be resumed and resynchronized in time. Each option has its own cost defined by the spawn_cost function. For example, on a private cloud, options using the latest start times allow unused nodes to remain switched off longer and save energy. On a public cloud such as EC2, the cost can be defined by the price the user is going to pay for the compute hours of the instance, the IOs on EBS and the monthly cost for data storage.</p><p>The cheapest options are selected to be executed. Note that if there are not enough options to provision all replicas, this means that it is not possible to spawn all replicas in time for the deadline given the current workload. We address this scenario in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Admission control</head><p>If a capacity deadline cannot be met in time with the current forecast, it is possible to perform admission control on the system in multiple ways. Note that this scenario can only happen if the predictor drastically changes its predictions for the current prediction window (such as an unpredicted flash crowd).</p><p>First we assume that no writes will update the system from now on and compute the time it takes to restore and replay from the latest snapshot (rr), to take a new snapshot and spawn a replica from it (br=backup+restore) or resume from paused VMs (  Note that doing admission control on writes (write throttling), means that update transactions are going to be delayed. Depending on timeout settings, this might translate into transactions being aborted. The minimum acceptable write throughput can be set by the administrator.</p><p>If replicas cannot be spawned in time even with write throttling, it is necessary to perform admission control on the incoming workload to prevent the system from crashing due to overload. Admission control can be performed by the replication engine by allowing only a fixed number of transactions in the system at any given time. It can also be achieved at another tier in front of the database (e.g. web tier admission control </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scheduling new database snapshots</head><p>In addition to provisioning new replicas or pausing existing ones, Dolly must deal with the problem of periodically creating new database snapshots by cloning VMs. A newer snapshot reduces the cost of spawning a new replica in the future (since it has a more recent version of the database and will incur a lower synchronization overhead). However, creating a snapshot incurs an overhead, and Dolly must intelligently schedule their creation to balance the cost and the benefit. Two problems have to be solved to schedule new database snapshots: how and when. How can either be from an already paused VM or by pausing an active VM for the time of the snapshot (see section 4.3.1). A new snapshot must be ready when the time to restore and replay from the previously available snapshot is greater than the prediction window (see section 4.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">How to snapshot?</head><p>An opportunistic method to create a new snapshot is to clone VMs that have been paused. While a paused VM only captures the database state until the time it was paused, it might still be a significant improvement over the last snapshot available.</p><p>The only other option requires taking an existing replica offline for the time of the pause/snapshot/resume (psr) operation and replaying of updates that happened since the VM was paused. This means that the capacity of the system is going to be reduced by 1 replica from t backup to .</p><p>If the workload prediction does not allow a replica to be temporarily disabled during that time interval, an additional replica has to be provisioned at time t backup to allow taking a new snapshot. This new deadline can be added to the current capacity prediction and the capacity provisioning algorithm described in section 4.2 has to be re-executed to provision this additional replica in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">When to snapshot?</head><p>If we want to provision additional replicas in time, the time to restore and replay from the latest available snapshot should never exceed the prediction window. Otherwise, when the predictor forecasts a new capacity demand increase at the end of the prediction window, there would not be enough time to spawn new replicas. This means that a new snapshot must be ready to be fully restored at time t switch defined by:</p><formula xml:id="formula_0">, i backup switch i backup r replay pw + =</formula><p>where pw is the prediction window and</p><p>To make sure that additional replicas can be provisioned at t switch using the new snapshot, the backup operation must be started prior to time 1) The cost to spawn replicas from a snapshot given by spawn_cost (defined in section 4.2.2) for all snapshots that can be restored and replayed by the deadline.</p><p>2) For each paused VM (step 3) that can be snapshotted, restored and replayed by the deadline, the cost to take the backup from the paused VM is given by the cost function backup_paused_cost to which we add the cost of spawning replicas from this backup.</p><p>3) The cost of creating a backup from a live replica is given by the backup_live_cost function to which we add the cost of spawning replicas from this backup and the eventual cost of bringing a replica online if no idle replica is available.</p><p>Next, the algorithm keeps the option that has the minimal cost for each deadline and schedules the operations accordingly. If no option is available to spawn a replica in time for a given deadline, the algorithm computes at what time a snapshot should be taken and modifies the capacity requirements to ask for one replica to be ready by that time. The capacity provisioning is then invoked to provision that replica, eventually using admission control if needed.</p><p>The capacity provisioning algorithm is re-run every time new snapshots have been scheduled to check if a better replica spawning schedule is available. If this is the case, the old schedule is replaced by the new schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relinquishing resources</head><p>Over time, some paused VMs become obsolete and are not cost effective to be resumed. The same applies to old VM snapshots that need to be erased. The paused pool cleaner has the responsibility of releasing these resources. It is invoked at regular time intervals that can be set by the administrator (from every hour, to every day or every week). It scans each paused VM and checks the cost of resuming that VM (spawn_cost(VM, now, pw end )) and compares it to the cost of spawning a replica from the latest available snapshot (spawn_cost(b i , now, pw end )). If the cost of resuming the VM is higher, it means that this VM will not be used anymore and it can be released.</p><p>A similar approach can be used for snapshots. All snapshots that are older than the current latest available snapshot can be released. However, the administrator might want to keep multiple older backups for recovery purposes. On a public cloud like EC2, since storage is paid for on a monthly basis, a better policy may be to retain old volumes until the end of the billing cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Current limitations</head><p>Dolly assumes that all the components of the cloning operation (backup, restore, snapshot…) have a constant time which is correct for homogeneous setups with LAN interconnections. This might not be the case with heterogeneous resources or resources in different EC2 regions or clouds using WAN interconnections. The worst case scenario measurement could be taken to ensure safe scheduling, but specific optimizations for such environments are left to future work. Additional optimizations such as virtual machine migration can also be considered in these environments.</p><p>When synchronizing slave nodes in a master/slave setup, the synchronization process uses master node resources and potentially impacts its performance. We have not currently modeled this performance impact but we did not find it noticeable in our early experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dolly Implementation</head><p>We have implemented the concepts of Dolly in the Sequoia 4.0 <ref type="bibr" target="#b14">[16]</ref> database clustering middleware and integrated it with the OpenNebula cloud infrastructure manager v1.4 <ref type="bibr">[14]</ref>. OpenNebula works with both private and public cloud resources and offers a single API to manipulate VMs independently of the target platform. <ref type="figure" target="#fig_6">Figure 6</ref> shows an overview of the integration of Dolly with Sequoia and OpenNebula in the context of the TPC-W benchmark. Client applications send SQL requests to the Sequoia controller that forwards them to the underlying databases to perform replication. The SQL commands of update transactions are recorded with their execution time in a transactional log called recovery log. The log itself is stored in an embedded database running within the Sequoia controller. The recovery log can be replayed to synchronize new or failed replicas. Additionally, Sequoia has a replica spawning infrastructure with a pluggable backuper interface that interacts with the recovery log and allows for database specific implementations of backup and restore operations. We have implemented a Dolly/OpenNebula backuper that interacts with OpenNebula to start/stop and clone/snapshot  Actuator virtual machines to implement the backup and restore functionality. When a new backup is triggered, a pointer to the current state of the recovery log is stored with the dump metadata. When a restore operation is launched, the dump is first restored and dedicated threads then replay the recovery log (i.e. re-execute the SQL commands) from the point that was saved in the metadata. Updates are applied in a serializable order to bring the new replica in a consistent state with other replicas. The time to replay is computed by summing the recorded execution time of all queries to replay. More information about Sequoia internals and its recovery log can be found in the Sequoia documentation <ref type="bibr" target="#b14">[16]</ref>. Dolly takes predictions directly from the TPC-W load injectors that act as oracles with perfect information. A tunable prediction window can be used from 1 minute to the entire length of the benchmark run. The provisioning actions are directly sent to the Sequoia controller through its administration interface. Dolly performs admission control directly on the load injectors but it would typically do this at the web tier level in a multi-tier setup.</p><p>The write throttling is achieved by interacting with the Sequoia scheduler. We have implemented different cost functions to model our private cloud platform and the Amazon EC2 public cloud. The private cloud cost functions detailed in pseudo-code in <ref type="table">Table  5</ref> optimize the time the resources are used. The longer the resources are used, the more power they use and the higher the cost. When the algorithm has to decide which VM to pause, it selects the hottest machine at that time. <ref type="table">Table 5</ref>. Cost function implementation for our private cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost function name Implementation</head><formula xml:id="formula_1">pause_cost(VM, t) return 1/VM-&gt;machine-&gt;temp spawn_cost(s, t, d) return d-t spawn_cost(VM, t, d) return d-t running_cost(VM,t1,t2) return 1 pause_resume_cost(VM, t1, t2)</formula><p>if (t2-t1&gt;VM-&gt;pause+VM-&gt;resume) return 0 else return 2 backup_paused_cost(VM) return backup_time backup_live_cost(VM, t) return VM-&gt;pause + backup_time + VM-&gt;resume <ref type="table">Table 6</ref> models the cost functions as the real cost the user would pay for EC2 resource usage. It includes both the compute time for server instances (charged by the hour at the hour$ rate) and the IO cost (charged monthly per GB of storage (EBS_storage$) and IOs are charged per million (EBS_io$)). EBS snapshots are stored on S3 and are charged monthly per GB of storage (S3_storage$). <ref type="table">Table 6</ref>. Cost function implementation for Amazon EC2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost function name Implementation</head><formula xml:id="formula_2">pause_cost(VM, t) return 60-((t-VM-&gt;start)%60) spawn_cost(s, t, d) comp$=(d-t)/60*hour$ io$=EBS_storage$*s-&gt;size + EBS_io$* (s-&gt;restore_io+s-&gt;replay_io) return comp$+io$ spawn_cost(VM, t, d) comp$=(d-t)/60*hour$ io$= EBS_io$* (s-&gt;resume_io+s-&gt;replay_io) return comp$+io$ running_cost(VM,t1,t2) (t2-t1)/60*hour$; pause_resume_cost(VM, t1, t2) io$= EBS_io$* (VM-&gt;pause_io+VM-&gt;resume_io) comp$=(60-(VM-&gt;stop-VM-&gt;start) %60)/60*hour$ return io$+ comp$ backup_paused_cost(VM) return S3_storage$*s-&gt;size backup_live_cost(VM, t) return pause_cost(VM, t)$+ S3_storage$*s-&gt;size + (VM-&gt;stop_io+VM-&gt;start_io)* EBS_io$</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Evaluation</head><p>This section first introduces the cloud platforms used for our experiments. We then present our performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cloud Platforms</head><p>We  We build a 4GB VM image of the TPC-W benchmark for both cloud platforms. We report our measurements of the various VM management and cloning operations in <ref type="table" target="#tab_16">Table 7</ref>. We measure the maximum write throughput of a single replica (w max ) obtained by running only write transactions of the TPC-W workload on a standalone database. The average number of IOs per write transaction is calculated by running iostat before and after the w max run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">VM Cloning vs Database Backup/Restore</head><p>VM cloning is an alternative mechanism for replicating content when compared to the traditional database-specific backup-restore mechanism. In this section, we compare the copy overheads of the two approaches. <ref type="table" target="#tab_17">Table 8</ref> shows the time to copy various databases using the database native backup/restore tool (e.g. mysqldump, pg_dump) versus VM cloning. The RUBiS benchmark database <ref type="bibr" target="#b2">[3]</ref> is tested with 3 configurations on MySQL using the InnoDB engine: without constraint or index (-c-i), with integrity constraints and basic indexes (+c+bi) and with constraints and full text indexes (+c+fi). TPC-W and TPC-H <ref type="bibr" target="#b17">[19]</ref> databases are stored in a PostgreSQL RDBMS. We also experiment with two virtual machine image sizes (4 and 16GB) where we store both the operating system and the database within its content.</p><p>Indexes significantly increase the database footprint on disk. We observe from the RUBiS results that integrity constraints checks as well as index building can increase database backup/restore time by a factor of more than 7 for the exact same database content. Not only do the database schema and backup tool configurations affect timings, different database engines yield very different results for databases with a similar size on disk as shown on <ref type="figure" target="#fig_8">Figure 7</ref>. We observe that large or complex databases can take more than 1 hour to replicate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Provisioning Evaluation</head><p>We experiment with TPC-W, an eCommerce benchmark from the Transaction Processing Council <ref type="bibr" target="#b17">[19]</ref> that emulates an online bookstore. We use the ObjectWeb implementation of the TPC-W benchmark <ref type="bibr" target="#b15">[17]</ref>. The setup is similar to the one depicted in <ref type="figure" target="#fig_6">Figure  6</ref> with load injectors providing a 2 hour prediction window. The web tier (not shown on <ref type="figure" target="#fig_6">Figure 6</ref>) is statically provisioned with enough servers for the length of the experiment.</p><p>We compare the provisioning decisions of Dolly for the private and public clouds with traditional provisioning techniques given the workload and initial conditions defined in section 6.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Workload Description</head><p>We have generated a custom mix of interactions to create the workload depicted at the top of <ref type="figure">Figure 8</ref>. We generate a read-only request mix by using the TPC-W browsing mix workload and removing its few write interactions. We use httperf to create the desired number of clients that send these read-only interactions. The write interactions are generated using the customer registration servlet of TPCW. Another set of httperf clients generate these write-only interactions.</p><p>We use the model described in <ref type="bibr" target="#b8">[9]</ref> to determine the capacity requirements shown in <ref type="figure">Figure 8</ref>. The initial capacity demand at t=0 is 4 replicas (middle graph) and the write throughput is 20% of the maximum write throughput (bottom graph). A snapshot s 0 is also available at time t 0 . After 10 minutes the number of replicas needed decreases from 4 to 3. We denote this deadline by d 1 . The number of replicas needed decreases further from 3 to 2 at d 2 =20 minutes. The capacity demand increases sharply from 2 to 5 replicas at d 3 =80 minutes, then drops to 2 at d 4 =90 minutes and increases up to 6 replicas at d 5 =100 minutes. The number of writes remains constant to 0.2 times the maximum write throughput for one hour with a 10 minute read-only workload starting at d 2 . After that hour, the write throughput is 0 until d 3 with a write surge at 50% of the maximum write throughput. The write peak continues for 10 minutes and the write throughput drops to 0 at d 4 . <ref type="figure">Figure 8</ref>. TPC-W workload, predicted capacity requirements and write workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Provisioning results</head><p>We compare Dolly's performance with two traditional provisioning techniques: reactive provisioning and overprovisioning. These techniques behave similarly on the private and public clouds.</p><p>Reactive provisioning does not use any prediction and just reacts to the current capacity demand. When reactive provisioning is used, database snapshots are generated at fixed time intervals. We use intervals of 15 minutes (Reactive15m), 1 hour (Reactive1h) and 2 hours (Reactive2h), generating 7, 1 and 0 snapshots respectively during the experiment.</p><p>The overprovisioning configuration (Overpro6) uses a constant set of 6 nodes. As for reactive provisioning, snapshots are generated periodically. We choose to only generate 1 snapshot during the experiment. We run Dolly with three prediction windows of 10 minutes, 30 minutes and 2 hours. Dolly uses the cost functions presented in section 5 for the private and public clouds. The performance of the different algorithms is summarized in <ref type="table" target="#tab_18">Table 9</ref>. The cost for the private cloud represents the cumulative machine uptime (6 machines up for 5 minutes accounts for 30 minutes). The cost for the public cloud (Amazon EC2) is the real cost in $USD. The second metric used is missing replica minute (MRM) that measures capacity underprovisioning (i.e. SLA violations). 1 MRM corresponds to a missing capacity of 1 replica for 1 minute (5 replicas missing for 2 minutes accounts for 10MRM). The results show that reactive provisioning is not able to properly provision the system with missing capacity ranging from 23.2 to 44.2 missing replica minute. Snapshotting more often reduces the time to spawn new replicas by restore and replay but capacity is missing during the spawning operations.</p><p>Overprovisioning (Overpro6) always provides an adequate capacity but at a significantly larger cost on each cloud platform. In contrast, Dolly uses much less resources while still providing the required capacity. A 10 minute prediction window (Dolly10m) requires more snapshots to be able to react to any new capacity demand at the end of the short prediction window. A 30 minute prediction window (Dolly30m) is enough to provide an optimal provisioning using less than half of the resources of the overprovisioned configuration. <ref type="figure" target="#fig_9">Figure 9</ref> shows in more detail the behavior of each algorithm. When reactive provisioning is used, additional capacity is used to spawn a new replica from the latest snapshot so that a new snapshot can be generated. When capacity needs to be increased, the system remains underprovisioned during the time replicas are spawned. The older the snapshot the longer it takes to spawn new replicas. In the Reactive2h case, replicas spawning starting at t=80 completes only 17 minutes later, leaving the system with only 2 available replicas to serve requests during the first peak period.</p><p>The Overpro6 configuration constantly provides 6 replicas except for when the snapshot is generated where a node is briefly paused. The large shaded area shows the amount of wasted resources.</p><p>Dolly with a 10 minute prediction window (Dolly10m) behaves similarly on both cloud platforms. As the prevision window slides the time to restore and replay from the latest snapshot exceeds the prediction window size. This is why Dolly spawns new replicas to generate new snapshots at deadlines s 1 and s 2 . While new replicas are spawned from s 1 during the first capacity increase, the write spike quickly triggers an additional replica to generate s 2 . Four replicas are paused at the end of the first peak and resumed for the second peak (no replay time since no write occurred during that paused time). An additional replica is quickly spawned from s 2 .</p><p>With a 30 minute or longer prevision window (Dolly30m and Dolly2h), decisions change between the private and the public cloud according to the cost functions. While less machine time is used on the private cloud by generating new snapshots from an additional replica online (s 1 ) or from a paused replica (s 2 ), the storage cost of a new snapshot dominates the IO cost of replay for EC2. Therefore all replicas are always spawned from the original s 0 snapshot in the public cloud. Instances are also not stopped between the two peaks as instances are paid for a full hour, pausing and restarting them 10 minutes later costs more than letting them run.</p><p>In summary, we have shown that Dolly with a prediction window as short as 30 minutes is able to provide optimal resource utilization (according to administrator defined cost functions) while always providing the required capacity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>Much of the prior work on dynamic provisioning <ref type="bibr" target="#b18">[20]</ref>, <ref type="bibr" target="#b19">[21]</ref>, <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b3">[4]</ref> has focused on dynamic provisioning of the front tiers of web applications. In this work we focus on the database tier that differs from other tiers due to its large dynamic state. Commercial solutions such as Oracle RAC <ref type="bibr" target="#b12">[13]</ref> use a shared disk approach to avoid the state replication problem. The use of in-memory databases on top of a shared storage has also been considered <ref type="bibr" target="#b11">[12]</ref>. Our work focuses on cloud environments where a shared disk approach cannot typically be deployed.</p><p>Amazon Relational Database Service (RDS) <ref type="bibr" target="#b1">[2]</ref> works with Amazon Auto Scaling <ref type="bibr" target="#b0">[1]</ref> to provide reactive provisioning of asynchronously replicated (i.e. master/slave) MySQL databases based on static thresholds. Microsoft in its Azure PaaS (Platform as a Service) cloud offering provides built-in replication in the lower layer of its platform but hides it to the user <ref type="bibr" target="#b13">[15]</ref>. Provisioning could be enhanced on both platform using Dolly.</p><p>The few papers related to dynamic provisioning of databases usually focus on workload prediction without modeling the time to spawn new replicas <ref type="bibr" target="#b7">[8]</ref>. Dolly can work with any load predictor and provisions database replicas accordingly by predicting VM cloning and replica resynchronization time. The problem of resynchronizing database replicas in a shared nothing environment has been described in <ref type="bibr" target="#b15">[17]</ref>. However, the proposed technique only relies on log replay and does not exploit snapshotting as a way to bring up new replicas. Even in a more recent work <ref type="bibr" target="#b9">[10]</ref>, state synchronization time is based on fixed estimates for replay. We have shown that using virtualization, we are able to snapshot databases via VM cloning and predict state replication time accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Database provisioning is a challenging problem due to the need to replicate and synchronize disk state. Since modern data centers and cloud platforms employ a virtualized architecture, we proposed a new database replica spawning technique that leverages virtual machine cloning. We argued that VM cloning offers a replication time that depends solely on the VM disk size and is independent of the database size, schema complexity and database engine. We proposed models to accurately estimate replica spawning time and analyzed the tradeoffs between capacity provisioning and database state snapshotting. To the best of our knowledge, Dolly is the first database provisioning system that can be adapted to the specifics of various cloud platforms via administrator-defined cost functions.</p><p>We implemented Dolly and integrated it with a commercial-grade open source database clustering middleware. We proposed different cost functions to optimize resource usage in a private cloud and to minimize cost for the Amazon EC2 public cloud. We evaluated our prototype with a TPC-W e-commerce workload and demonstrated the benefits of an automated database provisioning system for the cloud, with optimized solutions adapted to different cloud platform specifics. We plan to release Dolly as open source software and hope that it will facilitate replicated database deployments in virtualized environments such as clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Procedure to spawn a replica.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Replica spawning in a private cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 shows</head><label>3</label><figDesc>Figure 3 shows how spawning 2 replicas works in a public cloud such as Amazon EC2 that provides a Network Attached Storage (NAS) service called Amazon Elastic Block Storage (or EBS). Note that EBS volumes cannot be shared by multiple instances and are therefore different from a SAN or shared disk approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Replica spawning in a public cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Decomposition of the replica spawning time with a new snapshot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Overview of Dolly integration in Sequoia and OpenNebula running the TPC-W benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Time breakdown for cloning a database with Dolly and MySQL backup/restore tools with the MyISAM and InnoDB engines using the RUBiS benchmark database with various constraints and indices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Capacity made available by each provisioning algorithm compared to the required capacity and the total capacity actually used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Replica spawning time variables. 

b i 
backup time to generate VM snapshot i 

r i 
time to restore/clone snapshot i on a new replica 

replay i 
time to replay update transactions logged since 
snapshot i 

w t 
average update transaction throughput observed at the 
time the new replica spawning command is issued 

w max 
maximum update transaction throughput of the replica 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Replica spawning time formulas 

Replica spawning time when 
no snapshot is available 

( 
) max 

max 

i 
i 
t 

w 
b r 
w 
w 
+ 
− 

Replica spawning time from an 
existing snapshot i 

( 
) max 

max 

i 
i 
t 

w 
r replay 
w 
w 
+ 
− 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 and</head><label>2</label><figDesc></figDesc><table>conclude that: it is faster to 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>). The actuator orchestrates and executes the orders of all the other components.</head><label></label><figDesc></figDesc><table>Whenever new workload predictions become available, the 
capacity provisioning algorithm is invoked to compute a new 
schedule to meet capacity demands. Then the snapshot scheduler 
runs to check if new snapshots could be generated (possibly from 
paused VMs) to make future spawning operations cheaper. If new 
VM snapshots are generated, we re-run the capacity provisioning 
algorithm to generate a new schedule. In the end, we obtain a 
schedule of snapshot and capacity provisioning actions (adding, 
pausing, resuming replicas) that are executed by the actuator. 
Dolly also regularly triggers the paused pool cleaner to free old 
paused VMs and snapshots that are no longer needed. A more 
detailed description of the algorithms is available in [6]. 

time 
backup restore 
replay 

b i 
r i 

updates 

( 
) 

max 
max 
max 

. 
. 
. 
w 

w 
w 

w 
w 

w 
r 
b 

t 
t 
t 
i 
i 

 
 
 

 
 
 
 

 
  
 

 
  
 

 
+ 

replica spawning time </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Cloud platform specific cost functions used by Dolly. 

Cost function name 
Definition 

pause_cost(VM, t) 

cost of pausing VM at time t 

spawn_cost(s, t, d) 

cost to spawn a replica from snapshot s 
at time t to meet deadline d 

spawn_cost(VM, t, d) 

cost to spawn a replica from a paused 
VM at time t to meet deadline d 

running_cost(VM,t1,t2) 

cost to run a VM from time t1 to time t2 

pause_resume_cost(VM, 
t1, t2) 

cost to pause a VM at time t1 and 
resume it at time t2 

backup_paused_cost(VM) 

cost to backup a paused VM 

backup_live_cost(VM, t) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 . Variables used to measure replica spawning operations. rr Time to restore and replay from the latest snapshot br Time to spawn from a new snapshot (backup+restore) i VM rs Time to resume paused VM i psr Time to pause/snapshot/resume a VM pw Prediction window</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>time # of replicas needed prediction window backup restore now init time past write txput</head><label></label><figDesc></figDesc><table>w max 

d 1 
d 2 d 3 

unpredicted 
future 

snapshot 1 
snapshot 2 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>If we find that 
( , , 
, 
) 

i 
j 

VM 
VM 

now min rr br rs 
rs 
d 
+ 
≤ , this implies that 

there is enough time to create replicas but the write throughput is 
too high or too close to w max for replicas to catch up in time. 
Doing admission control on the write throughput w t can be used 
to meet the deadline as long as: 

max 

max 

( , , 
, 
). 

i 
j 

VM 
VM 

t 

min rr br rs 
rs 
w 
w w 
d now 
≤ 
− 
− 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>1 i backup t + so that there is enough time to backup, restore and replay a new replica at time t switch . This translates to:</head><label>1</label><figDesc></figDesc><table>1 
1 
, 
1 
1 
i 
i 
backup 
switch 
i 
i 

backup 
backup 
switch 
backup 

b 
r 
replay 
t 
t 

+ 
+ 
+ 
+ 

+ 
+ 
≤ 
− 

To guarantee that a new snapshot can be ready in time, the 
prediction window must be long enough so that: 

1 
1 
1 
, 
1 
i 
i 
i 
backup 
switch 
i 

switch 
backup 
backup 
backup 

pw t 
t 
b 
r 
replay 

+ 
+ 
+ 
+ 

≥ 
− 
≥ 
+ 
+ 

If the prediction window is too short or write throughput is too 
high, admission control can be used to make sure that new 
snapshots can be prepared in time within the prediction window. 

The algorithm then scans the prediction window and look at each 
deadline where new replicas have to spawned (adding capacity 

, 

max 

switch 

backup switch 
i 
backupi 

t 
t 

t t 

w 
replay 
w 

= 

= ∑ only). For each deadline, it calculates the cost to spawn new 
replicas for 3 strategies: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>table Dump table</head><label>Dump</label><figDesc></figDesc><table>JMX Management API 

Backupers 

Dolly 
OpenNebula 

DB 1 
DB 2 
DB 3 

add/remove replica 
snapshot/pause/… 

VM 1 

OS 
VM 2 

OS 
VM 3 

OS 

New 
replica 

VM 5 

OS 

DB3 

snapshot 

VM clone 

OS 

Load 
balancer 

New 
replica 

VM 4 

OS 

Sequoia controller 

predictions 

Sequoia driver 

admission control 

Backup server 
or NAS 

start/stop/ 
clone VM 

clone 
clone 

Dolly 

Capacity Provisioning 
Private 
EC2 

write throttling 

SQL 

SQL 

SQL 

SQL 

Snapshot scheduler 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table>Operation timings in seconds for a TPC-W benchmark 
virtual machine on our private cloud and EC2. 

Operation 
Private Cloud 
Public Cloud (EC2) 
start VM 
42s 
220s 
pause VM 
26s 
30s 
resume VM 
42s 
30s 
backup (stop/clone) 
150s 
320s 
restore (clone/start) 
165s 
220s 
w max 
149 writes/sec 
197 writes/sec 
Avg IOs per write 
15 
13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 8 .</head><label>8</label><figDesc>, VM cloning performs a filesystem level copy without interpreting database objects, thus it offers a constant time regardless of the database complexity or engine used. The time only depends on the VM image size on disk (280s for a 4GB image and about 900s for a 16GB image). Consequently, since the VM disk size is fixed a priori, VM cloning makes it easy to predict database backup/restore time incurred when spawning a new replica-a crucial pre-requisite for database provisioning. Additionally VM cloning captures the entire OS/database configuration and settings preventing any error in reproducing these settings on the new replica machine.</figDesc><table>Backup/restore and VM cloning time in seconds for 
various standard benchmark databases. 

Database 
DB size 
on disk 

DB 
Backup 
Restore 

Dolly 
4GB VM 
cloning 

Dolly 
16GB VM 
cloning 
RUBiS -c-i 
1022MB 
843s 
281s 
899s 
RUBiS +c+bi 
1.4GB 
5761s 
282s 
900s 
RUBiS +c+fi 
1.5GB 
6017s 
280s 
900s 
TPC-W 
684MB 
288s 
275s 
905s 
TPC-H 1GB 
1.8GB 
1477s 
271s 
918s 
TPC-H 10GB 
12GB 
5573s 
n/a 
911s 
In contrast</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18" validated="false"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table>Provisioning algorithm performance for private and 
public clouds in terms of cost and missing replica minute (MRM). 

Provisioning 
algorithm 

Private Cloud 
Public Cloud (EC2) 

Cost (time) 
MRM 
Cost ($) 
MRM 

Reactive15m 
381m42s 
17.5 
18.29 
27.2 

Reactive1h 
360m30s 
25.8 
5.00 
33.7 

Reactive2h 
410m 
42.1 
4.61 
41.5 

Overpro6 
720m 
0 
8.39 
0 

Dolly10m 
381m54s 
0 
7.16 
0 

Dolly30m 
352m 
0 
3.73 
0 

Dolly2h 
352m 
0 
3.73 
0 

</table></figure>

			<note place="foot" n="1"> Inspired by the sheep Dolly, the first mammal to be cloned successfully.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Steve Dropsho for early contributions to this work. This research was supported in part by NSF grants CNS-0834243, CNS-0720616, CNS-0916972, CNS-0855128, and a gift from NEC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://aws.amazon.com/autoscaling/" />
	</analytic>
	<monogr>
		<title level="j">Amazon Auto Scaling</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rds</forename><surname>Amazon</surname></persName>
		</author>
		<ptr target="http://aws.amazon.com/rds/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ZwaenepoelSpecification and implementation of dynamic Web site benchmarks -WWC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cecchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marguerite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Menasce -Resource allocation for autonomic data centers using analytic performance models -ICAC &apos;05</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Bennani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Washington, DC, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://zagnut.storeitoffsite.com/home/jim.blancet/FAQ/Snapshots%20in%20xen" />
	</analytic>
	<monogr>
		<title level="j">J. Blancet -Snapshots in Xen -Online FAQ</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cecchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shenoy -Dolly</surname></persName>
		</author>
		<idno>UM-CS-2010-006</idno>
		<title level="m">Virtualization-driven Database Provisioning for the CloudUMass</title>
		<imprint/>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Candea and A. Ailamaki -Middleware-based Database Replication: The Gaps between Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cecchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Amza -Autonomic Provisioning of Backend Databases in Dynamic Content Web Servers -ICAC &apos;06</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dropsho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cecchet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
		<title level="m">Predicting Replicated Database Scalability from Standalone Database Profiling -EuroSys</title>
		<imprint>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Amza -Adaptive Learning of Metric Correlations for Temperature-Aware Database Provisioning -ICAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shahabuddin -An Approach to Predictive Detection for Service Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12 th Conference on Systems and Network Management</title>
		<meeting>the 12 th Conference on Systems and Network Management</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Amza -Scaling and Continuous Availability in Database Server Clusters through Multiversion Replication -DSN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manassiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Oracle -Oracle Real Application Clusters 11g -Oracle Technical White Paper</title>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otey -Sql Server Vs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sql Azure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Where SQL Azure is Limited -SQL Server Magazine</title>
		<imprint>
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sequoia</forename><surname>Project</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/sequoiadb/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Online data migration for autonomic provisioning of databases in dynamic content web servers -2005 Conference of the Centre For Advanced Studies on Collaborative Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-10" />
			<pubPlace>Toronto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tpc-W</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://jmob.objectweb.org/tpcw.html" />
	</analytic>
	<monogr>
		<title level="j">ObjectWeb implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<ptr target="http://www.tpc.org/" />
	</analytic>
	<monogr>
		<title level="j">Transaction Processing Council</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<title level="m">Dynamic Provisioning for Multi-tier Internet Applications -ICAC-05</title>
		<meeting><address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rubenstein -Provisioning Servers in the Application Tier for E-commerce Systems -IWQOS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Villela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Smirni -A regression based analytic model for dynamic resource provisioning of multi-tier applications -ICAC &apos;07</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
