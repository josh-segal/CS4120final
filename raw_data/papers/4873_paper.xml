<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-03-26T17:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Autocannibalistic and Anyspace Indexing Algorithms with Applications to Sensor Data Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyue</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
							<email>eamonn@cs.ucr.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agenor</forename><surname>Mafra-Neto</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ISCA Technologies</orgName>
								<address>
									<postCode>92517</postCode>
									<settlement>Riverside</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science &amp; Eng</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Autocannibalistic and Anyspace Indexing Algorithms with Applications to Sensor Data Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Indexing</term>
					<term>Anyspace Algorithms</term>
					<term>Data Mining</term>
					<term>Sensors</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Efficient indexing is at the heart of many data mining algorithms. A simple and extremely effective algorithm for indexing under any metric space was introduced in 1991 by Orchard. Orchard&apos;s algorithm has not received much attention in the data mining and database community because of a fatal flaw; it requires quadratic space. In this work we show that we can produce a reduced version of Orchard&apos;s algorithm that requires much less space, but produces nearly identical speedup. We achieve this by casting the algorithm in an anyspace framework, allowing deployed applications to take as much of an index as their main memory/sensor can afford. As we shall demonstrate, this ability to create an anyspace algorithm also allows us to create auto-cannibalistic algorithms. Auto-cannibalistic algorithms are algorithms which initially require a certain amount of space to index or classify data, but if unexpected circumstances require them to store additional information, they can dynamically delete parts of themselves to make room for the new data. We demonstrate the utility of auto-cannibalistic algorithms in a fielded project on insect monitoring with low power sensors, and a simple autonomous robot application.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Efficient indexing is at the heart of many data mining algorithms. A simple and extremely effective algorithm for indexing under any metric space was introduced in 1991 by Orchard <ref type="bibr" target="#b10">[11]</ref>. The algorithm is commonly known as Orchard's algorithm, however Charles Elkan points out that a nearly identical algorithm was proposed by Hodgson in 1988 <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b4">[5]</ref>. While Orchard's algorithm is currently used in some specialized domains such as vector quantization <ref type="bibr" target="#b5">[6]</ref> and compression <ref type="bibr" target="#b20">[20]</ref>, it has not received much attention in the data mining and database community because of a fatal flaw; it requires quadratic space. In this work we show that we can produce a reduced version of Orchard's algorithm which requires much less space, but produces nearly identical speedup. We further show that we can cast our ideas in an anyspace framework <ref type="bibr" target="#b21">[21]</ref>, allowing deployed applications to take as much of an index as their main memory/sensor can afford. It is important to note that our reduced space algorithms produce exactly the same results as the full algorithm, they simply trade freeing up (a lot of) space for (very little) reduction in speed.</p><p>As we shall demonstrate, the ability to cast indexing as an anyspace algorithm allows us to create auto-cannibalistic algorithms. Auto-cannibalistic algorithms are algorithms which initially require a certain amount of space to index or classify data, but if unexpected circumstances require them to store additional information, they can dynamically delete ("eat") parts of themselves to make room for the new data. This allows the algorithm to be extremely efficient for a given memory allocation at the beginning of its life, and then gracefully degrade as it encounters outliers which it must store. We demonstrate the utility of auto-cannibalistic algorithms in a fielded project on insect monitoring with low power sensors and show that it can greatly extend the battery life of field deployed sensors.</p><p>The rest of the paper is organized as follows. Section 2 reviews the classic Orchard's algorithm and Section 3 introduces our extensions and modifications. We conduct a detailed empirical evaluation in Section 4, and in Section 5 we consider two concrete applications, including one which is already being tested in the field. We conclude with a discussion of related and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CLASSIC ORCHARD'S ALGORIHM</head><p>In order to help the reader understand Orchard's algorithm, and our extensions and modifications to it, we will introduce a simple example dataset in <ref type="figure">Figure 1</ref>  6 10 a 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">10</head><p>Figure 1: A small dataset A containing 7 items, used as a running example in this work</p><p>The preprocessing for Orchard's algorithm requires that we build for each item a i in our dataset A, a sorted list of its neighbors, annotated with the actual distances to a i in ascending order. We denote a sorted list for instance a i as P[a i ], and the full set of these lists for the entire dataset A as P <ref type="bibr">[A]</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows this data structure for our running example. Note that even for our small running example, the size of the ranked lists data structure is considerable, and would clearly be untenable for real world problems. The basic intuition behind Orchard's algorithm is to prune non-nearest neighbors based on the triangular inequality <ref type="bibr" target="#b4">[5]</ref>. Suppose we have a dataset A = {a 1 , a 2 , …, a |A| }, in which we want to find the nearest neighbor of query q.</p><p>Further suppose a i is the nearest neighbor found in A thus far. For any unseen element a j , which will not be the nearest neighbor if:</p><formula xml:id="formula_0">) , ( ) , ( q a dist q a dist i j ≥ (2.1)</formula><p>Given that we are dealing with a metric space, the principle of triangular inequality <ref type="bibr" target="#b4">[5]</ref> illustrated in <ref type="figure">Figure 2</ref> applies here:</p><formula xml:id="formula_1">) , ( ) , ( ) , ( q a dist q a dist a a dist j i j i + ≤ (2.2)</formula><p>Combining (2.1) and (2.2), we can derive the fact that if:</p><formula xml:id="formula_2">) , ( 2 ) , ( q a dist a a dist i j i × ≥ (2.3)</formula><p>is satisfied, a j can be pruned, since it could not be the nearest neighbor. This is how a combination of the triangular inequality and the information stored in the ranked lists data structure P <ref type="bibr">[A]</ref>, can allow us to prune some items a j without the expense of calculating the actual distance between q and a j .</p><p>Figure 2: (above, left) Assume we know the pairwise distances between a i , a j and a j' . A newly arrived query q must be answered. (above, right) After calculating the distance dist(q,a i ) we can conclude that items with a distance to a i less than or equal to 2 × dist(q,a i ) (i.e. the gray area) might be the nearest neighbor of q, but everything else, including a j' in this example, can be excluded from consideration</p><p>The algorithm, as outlined in <ref type="table" target="#tab_2">Table 2</ref>, begins by choosing some random element in A as the tentative nearest neighbor. It records the index of the element in A in nn.loc and calculates the distance nn.dist between q and the element a nn.loc in lines 1 and 2. Thereafter, the algorithm inspects the items in list P[a nn.loc ] in ascending order until one of three things happen. If either the end of the list is reached, or the next item on the list has value that is more than twice the current nn.dist (line 4), the algorithm terminates and returns a nn.loc as the nearest neighbor and the corresponding distance dist(a nn.loc , q). The third possibility is that the item in the list is closer to the query than the current tentative nearest neighbor (line 9). In that case the algorithm simply jumps to the head of the list associated with this new nearest neighbor to the query and continues from there (lines 10 to 12). Note that as the algorithm is traversing down the lists, it may encounter an item more than once. To avoid redundant calculations, a record is kept of all items encountered thus far, and an O(n) hash is sufficient to check if we have already calculated the distance to that item (line 7).</p><p>As the reader can appreciate, the algorithm has the advantages of simplicity, and being completely parameter free. Furthermore as shown both here (cf. Section 4) and elsewhere <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[20]</ref>, it is a very efficient indexing algorithm. However, while we typically would be willing to spare the quadratic time complexity to build the ranked lists dataset, the quadratic space complexity has all but killed interest in this method. In the next section we show how we can achieve the same efficiency in a fraction of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ANYSPACE ORCHARD'S ALGORIHM</head><p>In this section we begin by giving the intuition behind our idea for an anyspace Orchard's algorithm, and then show concrete approaches to allow us to create such an algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Truncated Orchard's Algorithm</head><p>We motivate our ideas with a simple observation. Note that in <ref type="figure">Figure 1</ref> the two data items a 2 and a 3 are very close together. As a result, their lists of nearest neighbors in <ref type="table" target="#tab_1">Table 1</ref> are almost identical. This is a redundancy; most queries that are efficiently pruned by a 2 would also be pruned efficiently by a 3 , and vice-versa. We can exploit this redundancy by deleting one entire list, thus saving some space. For the moment let us assume we have randomly chosen to delete the list of a 3 , as shown in <ref type="table" target="#tab_3">Table  3</ref>. </p><formula xml:id="formula_3">a 4 5 {4.2} 3 {5.0} 6 {5.0} 2 {5.8} 7 {5.8} 1 {7.1} a 5 7 {2.0} 6 {3.6} 4 {4.2} 1 {8.2} 3 {9.2} 2 {10.0} a 6 7 {3.0} 5 {3.6} 1 {5.0} 4 {5.0} 3 {8.9} 2 {9.4} a 7 5 {2.0} 6 {3.0} 4 {5.8} 1 {8.0} 3 {10.6} 2 {11.3}</formula><p>Note that we cannot just delete the entire row. We have a small amount of bookkeeping to do. It is possible that as we are using the index and traversing down the one of the other lists, we will encounter a 3 and find that it is the bestso-far. We should therefore jump to the list for a 3 and continue searching, however the list was deleted. To solve this problem we need to place a special "goto" pointer which tells the search algorithm that it should continue searching from a 2 's list instead.</p><p>As the reader will have already guessed, we can iteratively use this idea to delete additional lists, thus saving more space. In the limit, we will be left with a single list as shown in <ref type="table" target="#tab_4">Table 4</ref>. Note that whatever algorithm we use to delete lists, we must make sure that we don't end up with cycles. For example if a 2 's row says "goto a 3 " and if a 3 's row says "goto a 2 " we have an infinite loop. Another important observation is that although we should not expect this highly truncated Orchard's algorithm to perform as well as the quadratic space version, we still have not (necessarily) degraded to sequential search. Queries that happen to land near a 4 will be answered quickly, no matter which random starting position we choose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Anyspace Orchard's Algorithm</head><p>As framed in the previous section, we appear to have a solution to the quadratic complexity of Orchard's algorithm. We can simply work out how much memory is available for our application, and delete the necessary number of lists to make our Truncated Orchard's Algorithm fit. However, there may be situations where the amount of memory is variable (we discuss such applications in more detail in Section 5). In these applications we may find it useful to delete additional lists on-the-fly, as memory becomes more precious. For example, an autonomous robot could use a 90% Truncated Orchard's Algorithm to efficiently classify the items it sees as friend, foe or unknown. For both the friend and foe categories, it suffices to count how many it encountered. However, for the unknown category we may want the robot to store a picture of the unidentified item for later analysis. In this case, it would be useful to throw out additional lists of the Truncated Orchard's Algorithm in order to make space for the new image. An obvious question is, which lists should we toss out? A random selection would be easy, but this may decrease indexing efficiency greatly. Can we do better than random?</p><p>Our solution is to frame the Truncated Orchard's Algorithm as an anyspace algorithm <ref type="bibr" target="#b21">[21]</ref>. Anyspace algorithms are algorithms that trade space for quality of results. In general, an anyspace algorithm is able to solve the problem at hand with any amount of memory, and the speed at which it can solve the problem improves if more space is made available. In our particular case, we assume we start with all the memory of full data structure for the Truncated Orchard's Algorithm, and if we need space to store information about an unexpected event, we "cannibalize" a part of the Truncated Orchard's Algorithm's space to store it. We call such an approach an Auto-Cannibalistic algorithm. <ref type="figure">Figure 3</ref> shows an idealized anyspace algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: An idealized Anyspace Indexing Algorithm</head><p>Note that in this hypothetical case we get the best indexing performance if we use all the memory, however we can throw away 80% of the data and the performance only gets twice as bad. We could instead have thrown away 50% of the data with no significant difference in performance.</p><p>Note that all anyspace algorithms have some absolute minimum amount of memory which they require. In our case this is the O(|A|) space for the list of items in the dataset.</p><p>Assume the size of the full data structure is denoted as unity. Then we can denote the size of an anyspace algorithm as S, where minimum_space ≤ S ≤ 1. In our particular problem we have</p><formula xml:id="formula_4">O(|A|) ≤ S ≤ O(|A| 2 ).</formula><p>The basic framework for using an anyspace algorithm is as follows. We precompute the full space truncated Orchard's Algorithm table and store it in main memory. At some point in the future we expect to get requests for the index which are space limited. For example, sensor A may need the index, but only have 2MB available. We simply pull off the best 2MB version and give it to sensor A. If sensor B requests a 3 MB version, we pull off the best 3MB version and give it to B.</p><p>Note that for any memory size S of the anyspace algorithm, the data structure S is a proper subset of the data structure S+e. That is to say that a larger data structure is always the same as a smaller one, plus some additional data. This is a useful property. First of all it ensures that the size of the full data structure (S = 1) is no greater than the original (non anyspace) version (plus a tiny overhead). Thus we have no space overhead for keeping the data in an anyspace format. Second, it allows progressive transmission of the data structure. For example, in our scenario above, if sensor A manages to free up some addition memory, and can now devote 2.5MB to indexing the data, we only need to send it the 0.5MB difference.</p><p>Anyspace algorithms are rare in machine learning/data mining applications <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, however anytime algorithms, which are exact analogues that substitute time instead of space as the critical quality, have seen several data mining applications <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Zilberstein and Russell give a number of desirable properties of anytime algorithms, which we can adapt for anyspace algorithms.</p><p>Below we consider the desirable properties of anyspace algorithms, placing the desirable properties for anytime algorithms in parentheses:</p><p>• Interruptability: After some small amount of minimum space (setup time), the algorithm returns an answer using any additional amount of space <ref type="formula">(time)</ref> given.</p><p>• Monotonicity: the quality of the result is a nondecreasing function of space (computation time) used (cf. <ref type="figure">Figure 3</ref>).</p><p>• Measurable quality: the quality of an approximate result can be determined. In our case, this quality is the indexing efficiency, which can be measured by the number of distance calculations required to answer a query.</p><p>• Diminishing returns: the improvement in solution efficiency (quality) is largest at the early stages of computation, and diminishes as more space (time) is given (cf. <ref type="figure">Figure 3</ref>).</p><p>• Preemptability: The algorithm can use the space given, or additional space if it become available (the algorithm can be suspended and resumed) with minimal overhead.</p><p>As we shall see, our anyspace indexing algorithm meets all these properties. <ref type="table" target="#tab_5">Table 5</ref> shows our proposed Anyspace Orchard's algorithm data structure. It differs from the classic data structure (as shown in <ref type="table" target="#tab_1">Table 1</ref>) in just two ways. always delete the lists from the bottom, and replace the entire list by the corresponding goto pointer.</p><p>As a concrete example, suppose that we must free up approximately 40% of the space used. As shown in <ref type="table" target="#tab_6">Table  6</ref>, we can achieve this by deleting the last three lists and replacing them with their respective goto pointers.  <ref type="table" target="#tab_5">Table 5</ref>, the lists were added in this order: a 4 , a 7 , a 3 , a 1 , a 6 , a 2 and a 5 .</p><p>The formal algorithm shown in <ref type="table" target="#tab_7">Table 7</ref> is divided into two phases; selection and mapping. The first phase is a simple, greedy-forward selection search. T is initialized as an empty stack in line 1. For each outer iteration, the algorithm tests every item (that was not previously selected) with the utility function (line 7), picks the item with highest utility and pushes it onto T (lines 8 to 10). This procedure continues until all the items are pushed onto T. In essence, this phase sorts all the items by their expected utility for indexing. For instance, in the running example shown in <ref type="table" target="#tab_5">Table 5</ref>, it first selects a 4 , then a 7 , then a 3 , etc.</p><p>The second phase is to create the goto list. Our strategy is to start from the bottom of the table, repeatedly selecting the candidate with the lowest utility, and change its goto pointer to point at its nearest neighbor with a higher utility.   We have yet to explain how our evaluate_addition function is defined. We propose a simple approach that takes both density and overlap of each item into consideration. Intuitively, we assume the density distribution of the queries to be at least somewhat similar to the density distribution of the data in the index, so we want to reward items for being in a dense part of the space. At the same time, if we have one item from the center of a dense region, then there is little utility in having another item from the same region (overlap), so we want to penalize for this.</p><note type="other">j EndIf EndIf EndFor push(T, additem) EndFor P' = sort P best first according to T For i = |A| to 2 For j =</note><p>Concretely, our algorithm works as follows: the candidate's pool is initialized to include all the items. Given the parameter nearest neighbors number n, we set the item a i maximum utility in the candidate pool with smallest distance between a i and its n th valid nearest neighbor. We then delete a i from candidate pool. In addition, for all the items that on the a i 's nearest neighbor list, ranked from 1 to n, we assign them the minimum utility value (overlap penalty) and they are never again considered to be neighbors of any other items. Suppose we have m items initially, in our approach, we first pick ⎣ ⎦ ) 1 /( + n m pivot items according its radius to cover n valid nearest neighbors. After that, for those m%(n+1) items that are uncovered by any pivot item, we pick them in random order. Finally, we pick remaining nearest neighbors items in random order. We did consider several other possibilities, such as leaving-one-out evaluation, measuring the rank correlation, mutual information, or entropy gain between two lists as a measure of redundancy. However either these ideas did not work empirically, or required several parameters to be tuned. As a simple sanity check, we will include empirical comparisons to random, a variant of evaluate_addition which simply chooses a random list to add.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Using Anyspace Orchard's Algorithm</head><p>After constructing the sorted Orchard's algorithm table, it is easy to adapt the Orchard's algorithm search technique <ref type="table" target="#tab_2">(Table 2)</ref> to allow it to search in the truncated version. The main task is to decide what we should do if the algorithm indicates a jump to the list of a certain item a i while that list has already been deleted.</p><p>Simply jumping to the list of a j if goto[a i ] = a j is not possible, as the list of a j may also have been deleted. Consider the running example in <ref type="table" target="#tab_6">Table 6</ref>. If two more lists are deleted, the Orchard's Algorithm table shown in <ref type="table" target="#tab_9">Table  8</ref> is produced.</p><p>Suppose some query arrives, and the algorithm finds itself needing to jump to the list of a 2 . Since the list of a 2 is deleted, it wants to jump to a 3 which the goto entry of a 2 indicates. However, it cannot do so, because the list of a 3 is also deleted. The algorithm should continue the jump action, and see whether the list of a 4 = goto[a 3 ] is deleted. The general approach to find a valid item to jump to is described in <ref type="figure" target="#fig_8">Figure 9</ref>.  There are two additional things we need to check. One is if the list of a j has not been visited, and the other is if the distance between q and a j is smaller than the distance between query q and a nn , which is the item whose list we are currently visiting.</p><p>The first test is performed to avoid an infinite loop which makes the algorithm jump back and forth between the ranked lists. The second item is because the spirit of Orchard's algorithm tells us to attempt to jump to the item that is nearer the query than the item being visited. After confirmation of the two questions above, we can safely jump to the list of a j. <ref type="table" target="#tab_1">Table 10</ref> shows the entire Anyspace Orchard's search algorithm. First, the algorithm checks the available space and builds the truncated sorted Orchard's algorithm table in lines 1 to 3. One modification is that we add some bookkeeping to the item that best matches query, and the corresponding distance in lines 7 and 8, lines 13 to 15, and lines 21 to 23. The reason is the list of best-match item may have been deleted, thus the search does not necessarily end at the best-match item. Therefore the information of the bestmatch item should be stored at the time we compare the item to the query. Another modification is that we find the valid item to jump to in line 16, and test if we should jump to that item in line 17 as discussed above. Apart from these minor changes, the rest of the algorithm is exactly the same as in the classical Orchard's algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">An Optimization of Anyspace Orchard's</head><p>As described above, the mechanism used to create the Anyspace Orchard's algorithm may be quite slow. In some sense this is not a big issue, since we expect to perform this step offline. Nevertheless, it is reasonable to ask if we can speed up this process.</p><p>Note that one cause of its lethargy is the redundant calculations spent in finding the valid goto entries. The time will increase as the more lists being deleted. Here we show a simple one-scan strategy to update the entire goto list and avoid this overhead, which is described in <ref type="table" target="#tab_1">Table  11</ref>. The parameter cut means the number of sorted rank lists can accommodate in the memory.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>] EndFor</head><p>We initialize the goto entry of those items which have not been truncated to point to themselves in lines 1 and 4. This initialization does not affect these items, as they never use goto pointers, and makes finding valid goto pointers easier for the remaining items. In lines 5 and 8, we consider these truncated items from top to bottom. For each item a i we are considering, we are sure all the items whose position above it in the table already point to a valid item. Suppose a k is the item that a i 's original goto pointer points to. In that case, the option is either making the valid a i 's goto pointer the same as a k 's goto pointer when a k is truncated, or when a k is not truncated, the pointer should point to a k . Once the valid goto list is built, we can avoid all the redundant goto searches.</p><p>Another optimization is to narrow down the pruning criteria. We discovered an extra inequality we can exploit using the distance between the query and the best-so-far item. As in <ref type="figure" target="#fig_2">Figure 4</ref>.A, suppose a j is the best-so-far item while its rank list has been truncated, and a i is a valid item which a j 's goto pointer points to. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>.B, we only need to compute the actual distance between q and any a k∈ A only if the following inequality holds: dist(a i , a k ) &lt; 2 × dist(a j , query) + dist(a i , a j ). Recall that, as shown in Section 2, a k can be pruned if item a k can be admissibly pruned. In our implementation, we can simply replace line 8 of <ref type="table" target="#tab_1">Table 10</ref> with the line below, which is In general, on most datasets, this optimization improves indexing efficiency by 10% to 30%, so we use it in all experiments that follow.  <ref type="figure">(q, a j )</ref>. In this example, everything outside the large gray circle can be pruned</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We begin by stating our experimental philosophy. In order to ensure easy replication of our work we have placed all data and code at a publicly available website <ref type="bibr" target="#b18">[18]</ref>. Recall that our algorithm for constructing truncated Orchard's algorithm has a parameter n. One objective of our experiments is to see how sensitive our algorithm is to the choice of this parameter. A further objective is test the utility of our evaluate_addition function. It might be that any function would work well in this context. As a simple baseline comparison we compare against a function that randomly orders the lists for truncation. To understand the algorithm's efficiency we measure the average number of distance calculations needed to answer a one nearest neighbor query. In this, and all subsequent experiments we normalize the range to be between zero and one when creating figures, so a perfect algorithm would have a value near zero, and a sequential scan would have a value of one.</p><p>We begin with a simple experiment on a synthetic dataset.</p><p>We created a dataset of 5,000 random items from a 2D Gaussian distribution. We created a further 50,000 test examples. We begin by testing the indexing efficiency of the full Orchard's algorithm (i.e with zero truncation) with the test set, and then we truncate a single item and test again. We repeat the process until there is only a single list available to the algorithm, <ref type="figure" target="#fig_3">Figure 5</ref> shows the experiment on the synthetic data. The results appear very promising (compare to the idealized case in <ref type="figure">Figure 3)</ref>. First, it is clear that the choice of parameter n has very little impact on the results. Note that we can truncate 80% of the data without making a significant difference to the efficiency. Thereafter, the efficiency does degrade, but gracefully. Further notice that in contrast to our algorithm, the random approach has linear relationship between size and efficiency. This tells us our evaluate_addition function is finding redundancies in the data to exploit.</p><p>We next consider a problem of indexing data from a road sensor. This sensor data was collected for the Glendale onamp at the 5-North freeway in Los Angeles. The observations were taken over 25 weeks, at 5 minute count aggregates. As the location is close to the Dodgers stadium, it has bursty behavior on days in which a game is played. There are a total of 47497 observations, we randomly choose 1,000 to build our index, and used the rest for testing. <ref type="figure" target="#fig_4">Figure 6</ref> shows the results. As before, the performance of our algorithm is exactly we would like to see in an idealized anyspace algorithm, and once again, and our algorithms performance is almost invariant to the choice of parameter n.</p><p>Having shown that truncated Orchard's algorithm passes some simple sanity checks, in the next section we consider detailed case studies of problems we can solve using our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL CASE STUDIES</head><p>We conclude the experimental section with two detailed case studies of uses for our algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Insect Monitoring</head><p>ISCA Technologies is a Southern California based company that produces devices to monitor and control insect populations in order to mitigate harm to agricultural and human health. They have produced a "smart-trap" device that can be mass produced, and left unattended in the field for long periods to monitor a particular insect of interest.</p><p>The system under consideration here is primarily designed to track Aedes aegypti (yellow fever mosquito), a mosquito that can spread the dengue fever, yellow fever viruses, and a host of other diseases. In particular, the system needs to classify the sex of the insects and keep a running total of how many of each sex are encountered 1 . In order to only capture Aedes aegypti, the trap can be designed specifically for them. For example, carbon dioxide can be used as an attractant (this eliminates most non-mosquito insects). The trap can be placed at a certain height which eliminates low flying insects, and the entrance can be made small enough to prevent larger insects from entering. Nevertheless, as we shall see, non Aedes aegypti insects can occasionally enter the traps. <ref type="bibr" target="#b0">1</ref> Recall that only female mosquitoes suck blood from humans and other animals. While we have attempted classification of the insects with Bayesian Classifiers, SVMs and decision trees, our current best results come from using 1-Nearest Neighbor classification with a 4-dimensional feature vector extracted from the audio signal. A further advantage of using 1NN is that it allows us to come up with a simple definition of outlier. We empirically noted that on average both males and females tend to be a distance of m to their nearest neighbors. This number has a relatively small standard deviation. We therefore have defined an outlier as a data sample that is more than m + 4 standard deviations from its nearest neighbor. <ref type="figure" target="#fig_6">Figure 7</ref> shows a visual intuition of this. The distribution of distances to nearest neighbor for 1,000 insects in our training set. We consider exemplars whose distance to their nearest neighbor is more than the mean plus 4 standard deviations to be suspicious and worthy of follow-up investigation There are two obvious sources of outliers; non-insect sounds from outside the trap-including helicopters and farming equipment-and non-Aedes aegypti insects that enter the trap. Knowing the true identity of the outliers can be very useful. In the former case, it may be possible to change the traps location to reduce the number of outlier events caused by the sound of farm machinery. In the latter case, it may be useful know which unexpected insects we have caught. For example, we may have been subject to an unexpected invasion, as in the famous invasion of Glassywinged Sharpshooters (Homalodisca coagulata) which almost devastated the wine industry in Temecula southern California's Temecula Valley 1997 <ref type="bibr" target="#b0">[1]</ref>. However, the low power/low memory requirements of the traps prohibit recording the entire audio stream. Our solution therefore is to use an auto-cannibalistic algorithm which allows efficient indexing to support the one-nearest neighbor classification, and to record a one second snippet of outlier sounds. Each snippet requires the auto-cannibalistic to delete 3 lists from its table.</p><p>A classifier was built using 1,000 lab reared insects, with 500 of each sex. The training error suggests that we can achieve over 99% accuracy, however we have yet to confirm this by hand annotation of insects captured in the field. In <ref type="figure" target="#fig_7">Figure 8</ref> we see a plot showing the indexing efficiency for various levels of truncation. Note that the application lends itself well to any anyspace framework. Even when we have deleted 25% of the data we can barely detected any change in the indexing efficiency. Having demonstrated the concept in the lab, we deployed an auto-cannibalistic algorithm in the field. As a baseline comparison we compared to hard-coded truncated Orchard's algorithms where just 5%, 1% and a single list remains. <ref type="figure" target="#fig_8">Figure 9</ref> shows the results.</p><p>The figure shows that the more memory an indexing algorithm has, the more efficiently it can process incoming data. The auto-cannibalistic algorithm starts out with a full Orchard's algorithm in memory and is consequently much more effective than its smaller rivals. Over time, outliers are encountered and must be stored, so the amount of memory available to auto-cannibalistic algorithm decreases, causing it to become less efficient. However it is difficult to detect this for the first 50,000 or so events. As the power required is almost perfectly correlated with the cumulative number of Euclidean distance calculations, which in turn is simply the area under the curves, the autocannibalistic algorithm requires less than one tenth the energy of the 5% Orchard's algorithms, and is able to handle 4.99% more outlier events before its memory fills up. Algorithms terminate when there is no more free space for outliers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Robotic Sensors</head><p>In this section we consider the utility of auto-cannibalistic algorithms in a robotic domain. Note that unlike the insect example in the previous section, this is not a mature fielded product, it is simply a demonstration on a toy problem. In particular, we are not claiming that the approach below for finding unexpected tactical sensations is the best possible approach; it is merely an interesting test bed for a demonstration of our ideas.</p><p>The Sony AIBO, shown in <ref type="figure" target="#fig_10">Figure 10</ref>, is a small quadruped robot that comes equipped with a tri-axial accelerometer. This accelerometer measures data at a rate of 125 Hz. By examining the sensor traces, we can (perhaps imperfectly) learn about the surface the robot is walking on. For example, in <ref type="figure" target="#fig_10">Figure 10</ref> we show a two dimensional mapping of the Z-axis time series for both normal unobstructed walking and walking when one leg is obstructed. While the overlap of the two distributions in this figure suggests a high error rate, in three dimensions the separation is better, and we can achieve about 96% accuracy. Naturally it is useful to distinguish between these two situations, as the robot can attempt to free itself or change direction. In addition to merely classifying current state, it may be useful to detect unusual states and photograph them for later analysis. As the AIBO has only 4 megabytes of flash memory on board, memory must be used sparingly. A single compressed image with its 416x320 pixel camera requires about 100k of space. The distribution of distances to nearest neighbor for 700 tactile events in our training set. We consider exemplars whose distance to their nearest neighbor is more that the mean plus 5 standard deviations to be suspicious, and worthy the memory required to take a photograph We use the same basic framework as in the previous section here. We took 700 training instances and use them to build a 1-nearest neighbor classifier indexed by the truncated Orchard's algorithm. Every time an outlier is detected, and an image must be stored, we must delete an average of 18 lists from our index to make room for it. <ref type="figure" target="#fig_12">Figure 12</ref> shows the result of the experiment. As before, we compare to hard-coded truncated Orchard's algorithms where just 5%, 1% and a single list remains. As with the insect example, the truncated Orchard's algorithm requires only a fraction of the energy of the fixed-size indices, and is able to process data until just a single list remains available after dealing with event 1,522.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>We have introduced a novel indexing method especially for sensor data mining. In this section, we discuss the related work and provide future extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Related Work</head><p>Indexing is important for similarity search because it will reduce a large amount of searching time since it can Algorithms terminate when there is no more free space for outliers eliminate expensive distance calculations. The problem of indexing under metric distance has been studied intensively in the last decade and many efficient algorithms have been proposed. There are basically two alternative categories:</p><p>• Embedding method: for the objects in the data set of N dimensions, it created a k-dimension feature vector to represent each object. The distance calculated in the k-dimension feature space provides a lower bound of the actual distance between the objects. If k is considerably smaller than N, and the lower bound is reasonably tight, it can prune a lot of objects with much less distance calculation effort. Examples of this method are in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[16]</ref>.</p><p>• Distance-based method: typical distance-based method is based on partition. All or some distance between the objects in the data set are precomputed.</p><p>When a query comes in, we can estimate the majority of distances between the objects and query based on a small fraction of the actual distance we have computed between the objects and the query, and thus prune a lot of non-qualified objects. The vantage-point tree method <ref type="bibr" target="#b19">[19]</ref> is an example in this category.</p><p>The method we proposed in this paper falls into the second category. The obvious drawback of method in the second category is that the index data structure is fixed, which means, the indexing has a rigid memory requirement. However, under the scenario where main memory is bottleneck, e.g. in the sensor or robot, the algorithms with fixed memory requirement may fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Conclusion</head><p>In this paper, our major contribution is that:</p><p>• We have shown the Orchard's algorithm may be rescued from its relative obscurity by considering it as an anyspace algorithm and leveraging off of its unique properties to produce efficient sensor mining algorithms.</p><p>• We have further shown what we believe is the first example of an auto-cannibalistic algorithm.</p><p>Future work includes a large scale deployment and testing of the insect sensors, and a more general exploration of the notation of auto-cannibalism for other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Function</head><label></label><figDesc>valid_goto_list = find_gotolist(P', goto_list, cut) For i = 1 to cut a i = the item on the ith row of P' valid_goto_list[a i ] = a i EndFor For i = cut+ 1 to n a i = the item on the ith row of P' valid_goto_list[a i ] = valid_goto_list[goto_list[a i ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 : Assume a j is the current nearest neighbor of query q, and that P[a j ] was deleted and replaced with the goto entry a i , (A) A newly arrived query q must be answered. (B) An admissible pruning rule is to exclude items whose distance to a i</head><label>4</label><figDesc>Figure 4: Assume a j is the current nearest neighbor of query q, and that P[a j ] was deleted and replaced with the goto entry a i , (A) A newly arrived query q must be answered. (B) An admissible pruning rule is to exclude items whose distance to a i is greater than or equal to dist(a i , a j ) + 2 × dist(q, a j ). In this example, everything outside the large gray circle can be pruned</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The indexing efficiency vs. level of truncation for a synthetic dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 : The indexing efficiency vs. level of truncation for the Dodgers dataset</head><label>6</label><figDesc>Figure 6: The indexing efficiency vs. level of truncation for the Dodgers dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The distribution of distances to nearest neighbor for 1,000 insects in our training set. We consider exemplars whose distance to their nearest neighbor is more than the mean plus 4 standard deviations to be suspicious and worthy of follow-up investigation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : Indexing efficiency vs. space on the insect monitoring problem</head><label>8</label><figDesc>Figure 8: Indexing efficiency vs. space on the insect monitoring problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 : top) The cumulative number of Euclidean distance calculations performed vs. the number of sound events processed. bottom) The size of auto- cannibalistic algorithm vs. the number of sound events processed</head><label>9</label><figDesc>Figure 9: top) The cumulative number of Euclidean distance calculations performed vs. the number of sound events processed. bottom) The size of autocannibalistic algorithm vs. the number of sound events processed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 : clockwise from top right. A Sony Aibo robot. An on-board sensor can measure acceleration at 125 Hz. The accelerometer data projected into two dimensions</head><label>10</label><figDesc>Figure 10: clockwise from top right. A Sony Aibo robot. An on-board sensor can measure acceleration at 125 Hz. The accelerometer data projected into two dimensions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The distribution of distances to nearest neighbor for 700 tactile events in our training set. We consider exemplars whose distance to their nearest neighbor is more that the mean plus 5 standard deviations to be suspicious, and worthy the memory required to take a photograph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 : top) The cumulative number of Euclidean distance calculations performed vs. the number of tactile events processed. bottom) The size of auto- cannibalistic algorithm vs. the number of tactile events processed</head><label>12</label><figDesc>Figure 12: top) The cumulative number of Euclidean distance calculations performed vs. the number of tactile events processed. bottom) The size of autocannibalistic algorithm vs. the number of tactile events processed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Orchard's Algorithm Ranked Lists P[A] 

Item 
1 st NN 
{dist} 

2 nd NN 
{dist} 

3 rd NN 
{dist} 

4 th NN 
{dist} 

5 th NN 
{dist} 

6 th NN 
{dist} 

a 1 
6 {5.0} 
4 {7.1} 
2 {8.0} 
7 {8.0} 
3 {8.1} 
5 {8.2} 

a 2 
3 {1.0} 
4 {5.8} 
1 {8.0} 
6 {9.4} 
5 {10.0} 
7 {11.3} 

a 3 
2 {1.0} 
4 {5.0} 
1 {8.1} 
6 {8.9} 
5 {9.2} 
7 {10.6} 

a 4 
5 {4.2} 
3 {5.0} 
6 {5.0} 
2 {5.8} 
7 {5.8} 
1 {7.1} 

a 5 
7 {2.0} 
6 {3.6} 
4 {4.2} 
1 {8.2} 
3 {9.2} 
2 {10.0} 

a 6 
7 {3.0} 
5 {3.6} 
1 {5.0} 
4 {5.0} 
3 {8.9} 
2 {9.4} 

a 7 
5 {2.0} 
6 {3.0} 
4 {5.8} 
1 {8.0} 
3 {10.6} 
2 {11.3} 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Orchard's Algorithm Search 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 

Function [nn.loc, nn.dist] = Orchards(P[A], q ) 
nn.loc = random_interger_in_range_of(1, |A|) 
nn.dist = dist(a nn.loc , q) 
index = 1 
While P[a nn.loc ].dist[index] &lt; 2 * nn.dist AND index &lt; |A| do 
node = P[a nn.loc ].node[index] 
If node is not yet tested then 
d = dist(a node , q) 
If d &lt; nn.dist then 
nn.dist = d 
nn.loc = node 
index = 1 
Else 
index = index + 1 
EndIf 
Else 
index = index + 1 
EndIf 
EndWhile 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Truncated Orchard's Algorithm 

T 
1 st NN 
2 nd NN 
3 rd NN 
4 th NN 
5 th NN 
6 th NN 

a 1 
6 {5.0} 
4 {7.1} 
2 {8.0} 
7 {8.0} 
3 {8.1} 
5 {8.2} 

a 2 
3 {1.0} 
4 {5.8} 
1 {8.0} 
6 {9.4} 
5 {10.0} 
7 {11.3} 

a 3 
goto a 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Highly Truncated Orchard's Algorithm4 a 6 goto a 7 a 7 goto a 5</head><label>4</label><figDesc></figDesc><table>Item 
1 st NN 
2 nd NN 
3 rd NN 
4 th NN 
5 th NN 
6 th NN 

a 1 
goto a 6 

a 2 
goto a 4 

a 3 
goto a 2 

a 4 
5 {4.2} 
3 {5.0} 
6 {5.0} 
2 {5.8} 
7 {5.8} 
1 {7.1} 

a 5 
goto a </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Anyspace Orchard's Ranked Lists (I) 

goto 

list 

Item 
1 st NN 

{dist} 

2 nd NN 

{dist} 

3 rd NN 

{dist} 

4 th NN 

{dist} 

5 th NN 

{dist} 

6 th NN 

{dist} 

Linear 
a 4 
5 {4.2} 
3 {5.0} 
6 {5.0} 
2 {5.8} 
7 {5.8} 
1 {7.1} 

goto a 4 
a 7 
5 {2.0} 
6 {3.0} 
4 {5.8} 
1 {8.0} 
3 {10.6} 
2 {11.3} 

goto a 4 
a 3 
2 {1.0} 
4 {5.0} 
1 {8.1} 
6 {8.9} 
5 {9.2} 
7 {10.6} 

goto a 4 
a 1 
6 {5.0} 
4 {7.1} 
2 {8.0} 
7 {8.0} 
3 {8.1} 
5 {8.2} 

goto a 7 
a 6 
7 {3.0} 
5 {3.6} 
1 {5.0} 
4 {5.0} 
3 {8.9} 
2 {9.4} 

goto a 3 
a 2 
3 {1.0} 
4 {5.8} 
1 {8.0} 
6 {9.4} 
5 {10.0} 
7 {11.3} 

goto a 7 
a 5 
7 {2.0} 
6 {3.6} 
4 {4.2} 
1 {8.2} 
3 {9.2} 
2 {10.0} 

First, the rows are no longer in the original order, but 
sorted in a best first order. Second, note that in the leftmost 
column there is a small amount of additional information in 
the form of a goto list. This list tells us what to do if we 
need to free up some space by deleting lists. We will 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Anyspace Orchard's Ranked Lists (II) 

goto 

list 
Item 
1 st NN 

{dist} 

2 nd NN 

{dist} 

3 rd NN 

{dist} 

4 th NN 

{dist} 

5 th NN 

{dist} 

6 th NN 

{dist} 

Linear 
a 4 
5 {4.2} 
3 {5.0} 
6 {5.0} 
2 {5.8} 
7 {5.8} 
1 {7.1} 

goto a 4 
a 7 
5 {2.0} 
6 {3.0} 
4 {5.8} 
1 {8.0} 
3 {10.6} 
2 {11.3} 

goto a 4 
a 3 
2 {1.0} 
4 {5.0} 
1 {8.1} 
6 {8.9} 
5 {9.2} 
7 {10.6} 

goto a 4 
a 1 
6 {5.0} 
4 {7.1} 
2 {8.0} 
7 {8.0} 
3 {8.1} 
5 {8.2} 

a 6 
goto a 7 

a 2 
goto a 3 

a 5 
goto a 7 

3.3 Constructing Anyspace Orchard's 

Assume we have truncated Orchard's data structure, T. At 
one extreme, T has all lists P[a i ] for 1 ≤ i ≤ |A|, and is thus 
the "classic" Orchard's data structure. At the other extreme, 
it has only a single list, and we can only efficiently answer 
queries that happen to be near the untruncated item. 

Assume that we have a black box function 
evaluate_addition(T,i) which given T returns the estimated 
utility of adding list P[a i ]. This function estimates the 
expected number of items that an arbitrary query must be 
compared to in order to find its nearest neighbor by adding 
list P[a i ] to the existing table, and returns the highest utility 
with the smallest comparison number. For the moment we 
will gloss over the details of this function, except to note 
that it allows us to create an Anyspace Orchard's 
Algorithm. The basic idea of the algorithm is to start with 
an empty set T, and iteratively use function 
evaluate_addition(T,i) to decide which list to add next. For 
example, we can see from column 2 of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Build Anyspace Orchard's Construction 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>].pointer in lines 18 and 19. If we find one nearest neighbor a index that ranks above a i in sorted Orchard's algorithm table, we assign a index to the entry of a i in the goto list goto[a i ] in line 20 to 22. In the running example, we first consider the item a 5 . Following a 5 's nearest neighbor list, we check the item a 7 , which is on the second line of the sorted Orchard's algorithm table, and thus above a 5 . We therefore make goto[a 5 ] point to a 7 . We next consider the item a 2 , and so on.</head><label></label><figDesc></figDesc><table>1 to |A| -1 
index = P[a i ].pointer[j] 
If a index appears above a i in P' 
goto_list[a i ] = a index 
break; 
EndIf 
EndFor 
EndFor 

To achieve this we scan the sorted Orchard's algorithm 
table bottom up (as in line 17). For each item a i under 
consideration, we scan down its nearest neighbor list 
P[a i </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :4 a 1 goto a 4 a 6 goto a 7 a 2 goto a 3 a 5 goto a 7</head><label>8</label><figDesc></figDesc><table>Anyspace Orchard's Ranked Lists (III) 

goto 

list 
Item 
1 st NN 

{dist} 

2 nd NN 

{dist} 

3 rd NN 

{dist} 

4 th NN 

{dist} 

5 th NN 

{dist} 

6 th NN 

{dist} 

Linear 
a 4 
5 {4.2} 
3 {5.0} 
6 {5.0} 
2 {5.8} 
7 {5.8} 
1 {7.1} 

goto a 4 
a 7 
5 {2.0} 
6 {3.0} 
4 {5.8} 
1 {8.0} 
3 {10.6} 
2 {11.3} 

a 3 
goto a </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 9 : Find Valid goto for Single Item</head><label>9</label><figDesc></figDesc><table>1 
2 
3 
4 
5 
6 
7 
8 

Function valid_goto = Find_goto(goto_list, a i ) 
item = a i 
While 1 
item = goto_list[item] 
If the list of item is not deleted 
break 
EndIf 
EndWhile 
Return item 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>AnySpace Orchard's Algorithm Search 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 

Function nn = AnyspaceOrchards(A, q) 
Build P 
[goto, P'] = BuildAnyspaceOrchards(A, P) 
truncate P' from bottom to fit it into memory 
nn.loc = random_interger_in_range_of(1,|A|) 
nn.dist = dist(a nn.loc , q) 
index = 1 
bestpos = nn.loc 
bestdist = nn.dist 
While P[a nn.loc ].dist[index] &lt; 2 * nn.dist AND index &lt; |A| 
item = P[a nn.loc ].pointer[index] 
d = dist(a item , q) 
If d &lt; nn.dist 
If d &lt; bestdist 
bestdist= d 
bestpos = item 
EndIf 
a goto = valid_goto(goto_list, a index ) 
If list of a goto not visited AND dist(a goto ,q) &lt; nn.dist 
nn.loc = goto 
nn.dist = dist(a goto , q) 
If nn.dist &lt; bestdist 
bestdist = nn.dist 
bestpos = nn.loc 
EndIf 
Else 
index = index + 1 
EndIf 
Else 
index = index + 1 
EndIf 
EndWhile 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 11 : Find Entire Valid goto List</head><label>11</label><figDesc></figDesc><table>1 
2 
3 
4 
5 
6 
7 
8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>k ) is not available because P[a j ] was truncated. There is an additional inequality between the three items where we can have the lower bound of the value of dist(a j , a k ):</head><label></label><figDesc></figDesc><table>) 
, 
( 
2 
) 
, 
( 
query 
a 
dist 
a 
a 
dist 

j 
k 
j 

× 
≥ 
(3.1) 

However, dist(a j , a ) 
, 
( 
) 
, 
( 
) 
, 
( 

j 
i 
k 
i 
k 
j 

a 
a 
dist 
a 
a 
dist 
a 
a 
dist 
− 
≥ 
(3.2) 

Combining (3.1) and (3.2), if we have 
) 
, 
( 
2 
) 
, 
( 
) 
, 
( 
query 
a 
dist 
a 
a 
dist 
a 
a 
dist 

j 
j 
i 
k 
i 

× 
≥ 
− 
(3.3) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Phenology and demography of Homalodisca coagulata (Hemiptera: Cicadellidae) in southern California citrus and implications for management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Castle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Naranjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Toscano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of Entomological Research</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="621" to="634" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Any-space Algorithm for Distributed Constraint Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chechetka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">K</forename><surname>Sycara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Spring Symposium on Distributed Plan and Schedule Management</title>
		<meeting>AAAI Spring Symposium on Distributed Plan and Schedule Management</meeting>
		<imprint>
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nearest-neighbor searching and metric space dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nearest-Neighbor Methods for Learning and Vision: Theory and Practice</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Any-space probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using the triangle inequality to accelerate kMeans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning</title>
		<meeting>the Twentieth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="147" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast codeword search algorithm for real-time codebook generation inadaptive VQ. Vision, Image and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Shivaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="278" to="284" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anytime algorithm development tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGART Artificial Intelligence. volumn</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1996" />
			<publisher>ACM Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FastMap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGKDD Conference</title>
		<meeting>the 1st ACM SIGKDD Conference</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing computational requirements of the minimum-distance classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environments</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive event detection with time-varying Poisson processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD Conference (KDD&apos;06)</title>
		<meeting>the 12th ACM SIGKDD Conference (KDD&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast nearest-neighbor search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="2297" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A fast search algorithm for vector quantization using L2-norm pyramid of codewords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C ; J B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Transactions</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anytime Exploratory Data Analysis for Massive Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 3rd International Conference on Knowledge Discovery and Data mining (KDD&apos;97)</title>
		<meeting>eeding of the 3rd International Conference on Knowledge Discovery and Data mining (KDD&apos;97)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="54" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anytime Classification Using the Nearest Neighbor Algorithm with Applications to Stream Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th International Conference on Data Mining (ICDM&apos;06)</title>
		<meeting>of the 6th International Conference on Data Mining (ICDM&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Wavelet-Based Anytime Algorithm for K-Means Clustering of Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp; D</forename><surname>Gunopulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Clustering High Dimensionality Data and Its Applications. In the 3 rd SIAM Int&apos;l Conference on Data Mining</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating a class of distancemapping algorithms for data mining and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp; K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM SIGKDD Conference</title>
		<meeting>the 4th ACM SIGKDD Conference</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Classifying under computational resource constraints: Anytime classification using probabilistic estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Korb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">185</biblScope>
		</imprint>
		<respStmt>
			<orgName>Clayton School of Information Technology, Monash University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Supporting URL for this paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="www.cs.ucr.edu/~lexiangy/Anyspace/Dataset.html" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data structures and algorithms for nearest neighbor search in general metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the 4th Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nearest neighbor search for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Structures, Nearest Neighbor Searches, and Methodology: Fifth and Sixth DIMACS Implementation Challenges</title>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximate reasoning using anytime algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Imprecise and Approximate Computation</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
