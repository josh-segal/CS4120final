<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TOWARDS TAMING THE RESOURCE AND DATA HETEROGENEITY IN FEDERATED LEARNING</p>
    <p>Zheng Chai!, Hannan Fayyaz&quot;, Zeshan Fayyaz#, Ali Anwar$,</p>
    <p>Yi Zhou4$, Nathalie Baracaldo$, Heiko Ludwig$, Yue Cheng!</p>
  </div>
  <div class="page">
    <p>Federated learning - learning in distributed data environments</p>
    <p>In a federated learning system, multiple data sources collaborate to learn a predictive model</p>
    <p>Limited trust is likely to exist between each client, so clients do not want other participants to</p>
    <p>learn their private data in the process</p>
    <p>Limited communication  Data/Resource Heterogeneity</p>
    <p>Data heterogeneity some clients will have more data</p>
    <p>Resource heterogeneity some clients may have lesser compute/connectivity</p>
    <p>Clients can drop out during the training</p>
    <p>Federated Learning - Background</p>
  </div>
  <div class="page">
    <p>Federated Learning  Problem Statement</p>
    <p>!&quot; !# !$</p>
    <p>%&quot; %# %$</p>
    <p>Aggregator &amp; ':)**+,*-.,/ 0,1*2.3 R: Gradients C: Client D: Data on client</p>
    <p>' '</p>
    <p>Issue: Aggregation happens after each client has replied</p>
  </div>
  <div class="page">
    <p>Federated Learning  Example Problem: Stragglers</p>
    <p>!&quot; #$ #%</p>
    <p>Aggregator &amp;</p>
    <p>'$ '%</p>
    <p>'(</p>
    <p>#) #* #&quot;</p>
    <p>'+ ',</p>
    <p>Fast clients Moderate clients Slow clients</p>
  </div>
  <div class="page">
    <p>Federated Learning  Example Problem: Dropouts</p>
    <p>!&quot; #$ #%</p>
    <p>Aggregator &amp;</p>
    <p>'$ '%</p>
    <p>'(</p>
    <p>#) #* #&quot;</p>
    <p>'+ ',</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Dataset: MNIST data</p>
    <p>Non-IID: each client sample data from randomly selected 5 digit categories</p>
    <p>No. of Total Clients: 100</p>
    <p>Model: CNN (consists of two CNN layers and one Max Pooling layer)</p>
  </div>
  <div class="page">
    <p>Setup and Hyperparameters</p>
    <p>Platform: AWS EC2 (m4.10xlarge with 40 vCPUs and 160 GB memory )</p>
    <p>Library: TensorFlow</p>
    <p>Hyperparameters:</p>
    <p>Optimizer: Adadelta</p>
    <p>Epochs: 8 (each client per round)</p>
    <p>Drop out rate: 0.25 &amp; 0.5 for each CNN layer</p>
  </div>
  <div class="page">
    <p>Impact of resource heterogeneity on training time</p>
    <p>Per-round training time different CPU resources and different dataset sizes</p>
    <p>Test1 Test2 Test3 Test4 Test5</p>
  </div>
  <div class="page">
    <p>Impact of data heterogeneity on training time</p>
    <p>Per-epoch training time with different dataset sizes</p>
    <p>Training time gets linearly increased as the dataset size gets bigger</p>
    <p>Data heterogeneity can significantly impact the FL systems training time</p>
  </div>
  <div class="page">
    <p>Current ongoing work</p>
    <p>How to classify devices based on their response time and then use this</p>
    <p>information for our advantage without affecting the FL process?</p>
    <p>How to incorporate data of each device in the FL process without</p>
    <p>worrying about stragglers?</p>
    <p>How to identify drop-out devices and mitigate the effect of</p>
    <p>drop-out without affecting the ML process?</p>
    <p>Q1</p>
    <p>Q2</p>
    <p>Q3</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
</Presentation>
