<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Software Wear Management for Persistent Memories</p>
    <p>Vaibhav Gogte, William Wang1, Stephan Diestelhorst1, Aasheesh Kolli2,3, Peter M. Chen, Satish Narayanasamy, Thomas F. Wenisch</p>
    <p>FAST19 02/26/2019</p>
  </div>
  <div class="page">
    <p>Promise of persistent memory (PM)</p>
    <p>Non-volatility</p>
    <p>Performance</p>
    <p>Density</p>
  </div>
  <div class="page">
    <p>Promise of persistent memory (PM)</p>
    <p>PM for capacity expansion</p>
    <p>PM cheaper and denser than DRAM</p>
    <p>Non-volatility</p>
    <p>Performance</p>
    <p>Density</p>
  </div>
  <div class="page">
    <p>Promise of persistent memory (PM)</p>
    <p>PM for capacity expansion</p>
    <p>PM as storage</p>
    <p>PM cheaper and denser than DRAM</p>
    <p>PM enables faster storage via load-store interface Non-volatility</p>
    <p>Performance</p>
    <p>Density</p>
  </div>
  <div class="page">
    <p>PMs have low write endurance</p>
    <p>CPU</p>
    <p>PM</p>
    <p>Wear-leveling mechanisms  PM cells wear out after 107  109 writes [Lee 09]</p>
  </div>
  <div class="page">
    <p>PMs have low write endurance</p>
    <p>CPU</p>
    <p>PM</p>
    <p>Wear-leveling mechanisms</p>
    <p>Remap locations to uniformly distribute writes</p>
    <p>PM cells wear out after 107  109 writes [Lee 09]</p>
  </div>
  <div class="page">
    <p>DRAM</p>
    <p>PMs have low write endurance Wear-leveling mechanisms</p>
    <p>CPU</p>
    <p>PM</p>
    <p>Wear-reduction mechanisms</p>
    <p>Remap locations to uniformly distribute writes</p>
    <p>PM cells wear out after 107  109 writes [Lee 09]</p>
    <p>CPU</p>
    <p>PM</p>
  </div>
  <div class="page">
    <p>DRAM</p>
    <p>PMs have low write endurance Wear-leveling mechanisms</p>
    <p>CPU</p>
    <p>PM</p>
    <p>Wear-reduction mechanisms</p>
    <p>Remap locations to uniformly distribute writes</p>
    <p>PM cells wear out after 107  109 writes [Lee 09]</p>
    <p>Map heavily written locations to DRAM</p>
    <p>CPU</p>
    <p>PM</p>
  </div>
  <div class="page">
    <p>Wear management in software  Prior proposals measure PM wear in hardware [Qureshi 09, Ramos 11, ]</p>
    <p>Wear leveling: Add latency of additional translation layer  Wear reduction: Require specialized memory controllers</p>
    <p>Our proposal: Wear-aware virtual memory system  Employ virtual-to-physical page mappings to manage wear  Eliminates need for another translation layer  Require no special hardware to measure PM wear</p>
    <p>Challenge: Measurement of PM writes at a page granularity in software</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Kevlar achieves PM lifetime target of 4 years with 1.2% performance overhead</p>
    <p>Wear leveling</p>
    <p>Wear reduction</p>
    <p>Periodically remaps virtual-to-physical mappings</p>
    <p>Migrates heavily written pages to high endurance mem.</p>
    <p>Estimates per page wear in software</p>
    <p>Analytical framework Simple remapping achieves near-uniform wear</p>
    <p>Wear estimation</p>
  </div>
  <div class="page">
    <p>Wear leveling uniformly wears out PM pages  Periodically shuffle memory footprint to spread writes uniformly in PM</p>
    <p>PM</p>
    <p>Shuffle 1 Virtual pages Physical pages</p>
    <p>Reassign each virtual page to a randomly chosen physical page</p>
  </div>
  <div class="page">
    <p>Wear leveling uniformly wears out PM pages  Periodically shuffle memory footprint to spread writes uniformly in PM</p>
    <p>PM</p>
    <p>Shuffle 1 Virtual pages Physical pages</p>
    <p>Swap contents in physical pages during the shuffle</p>
    <p>Reassign each virtual page to a randomly chosen physical page</p>
  </div>
  <div class="page">
    <p>Wear leveling uniformly wears out PM pages  Periodically shuffle memory footprint to spread writes uniformly in PM</p>
    <p>PM</p>
    <p>Shuffle 1 Virtual pages Physical pages</p>
    <p>Swap contents in physical pages during the shuffle</p>
    <p>Reassign each virtual page to a randomly chosen physical page</p>
  </div>
  <div class="page">
    <p>Wear leveling uniformly wears out PM pages  Periodically shuffle memory footprint to spread writes uniformly in PM</p>
    <p>PM</p>
    <p>After N shuffles Virtual pages Physical pages</p>
    <p>Disparity in page wear shrinks as shuffles increase</p>
    <p>Are random shuffles enough to achieve near-uniform wear?</p>
    <p>Does not require measurement of per page wear</p>
    <p>Depends on average PM write bandwidth</p>
  </div>
  <div class="page">
    <p>PM lifetime due to random shuffles  Using analytical framework to determine no. of shuffles</p>
    <p>Get write traces of applications using instrumentation  Evaluate wear to pages as number of shuffles increase</p>
    <p>More details in the paper!</p>
  </div>
  <div class="page">
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>li fe</p>
    <p>tim e</p>
    <p>Number of Shuffles</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Ideal</p>
    <p>PM lifetime due to random shuffles  Using analytical framework to determine no. of shuffles</p>
    <p>Get write traces of applications using instrumentation  Evaluate wear to pages as number of shuffles increase</p>
    <p>More details in the paper!</p>
  </div>
  <div class="page">
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>li fe</p>
    <p>tim e</p>
    <p>Number of Shuffles</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Ideal</p>
    <p>PM lifetime due to random shuffles  Using analytical framework to determine no. of shuffles</p>
    <p>Get write traces of applications using instrumentation  Evaluate wear to pages as number of shuffles increase</p>
    <p>More details in the paper!</p>
    <p>Lifetime improves with the increasing number of shuffles &lt; 8192</p>
  </div>
  <div class="page">
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>li fe</p>
    <p>tim e</p>
    <p>Number of Shuffles</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Ideal</p>
    <p>PM lifetime due to random shuffles  Using analytical framework to determine no. of shuffles</p>
    <p>Get write traces of applications using instrumentation  Evaluate wear to pages as number of shuffles increase</p>
    <p>More details in the paper!</p>
    <p>Writes due to shuffles dwarf application writes for &gt; 8192 shuffles</p>
  </div>
  <div class="page">
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>li fe</p>
    <p>tim e</p>
    <p>Number of Shuffles</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Ideal</p>
    <p>PM lifetime due to random shuffles  Using analytical framework to determine no. of shuffles</p>
    <p>Get write traces of applications using instrumentation  Evaluate wear to pages as number of shuffles increase</p>
    <p>More details in the paper!</p>
    <p>Kevlar achieves 94% ideal-wear with 8192 shuffles over PM lifetime</p>
  </div>
  <div class="page">
    <p>Wear leveling alone is not enough</p>
    <p>L if</p>
    <p>et im</p>
    <p>e (i</p>
    <p>n ye</p>
    <p>ar s)</p>
    <p>Number of Shuffles</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo</p>
    <p>Lifetime achieved due to wear leveling alone is limited by PM write bandwidth</p>
    <p>Wear leveling improves PM lifetime to 2.0  2.8 years  Insufficient to meet system lifetime targets (eg. 4 or 6 years)</p>
  </div>
  <div class="page">
    <p>Wear reduction in Kevlar  Improves PM lifetime to a configurable target  Limits PM write bandwidth to meet lifetime target</p>
    <p>Performs page migrations to high endurance memory</p>
    <p>Kevlar requires per page writeback rate to perform page migrations</p>
    <p>PM_bandwidth = -./012.34  ._62748 9:;4&lt;:=4</p>
    <p>= 20K writes/sec/GB</p>
    <p>Eg. For desired lifetime = 4yrs, PM endurance = 107:</p>
  </div>
  <div class="page">
    <p>Measuring PM page writes is challenging  PM writes are a result of cache writebacks</p>
    <p>Existing systems provide no mechanisms to measure per-page writebacks</p>
    <p>Core</p>
    <p>PM</p>
    <p>Cache</p>
    <p>STR</p>
    <p>Stores update cache lines</p>
    <p>Writebacks happen due to cache line conflicts WB</p>
  </div>
  <div class="page">
    <p>Modeling caches to measure PM writebacks</p>
    <p>Precise modeling of caches in software expensive</p>
    <p>Kevlar builds an approximate cache model</p>
    <p>Observe stores using hardware performance counters</p>
    <p>Approximately track set of dirty cache blocks using bloom filter</p>
    <p>Estimate PM writebacks using cache blocks in bloom filter</p>
  </div>
  <div class="page">
    <p>Using PEBS to sample stores</p>
    <p>Employs Intels Precise-Event-Based-Sampling (PEBS) counters  Configures PEBS to record arch. state for retiring stores</p>
    <p>Core</p>
    <p>Triggers a PEBS interrupt PEBS Record 0 Store addr</p>
    <p>PEBS record maintains the virtual address of a sampled store</p>
    <p>STR</p>
    <p>Optimization: Samples one every 17th stores to reduce monitoring overhead</p>
  </div>
  <div class="page">
    <p>Kevlar approximates caches to estimate wear</p>
    <p>Estimates temporal locality in applications access pattern  Uses bloom filter to track dirty blocks in hardware cache</p>
    <p>Bloom Filter</p>
  </div>
  <div class="page">
    <p>Kevlar approximates caches to estimate wear</p>
    <p>Estimates temporal locality in applications access pattern  Uses bloom filter to track dirty blocks in hardware cache</p>
    <p>Bloom FilterStore addr</p>
  </div>
  <div class="page">
    <p>Kevlar approximates caches to estimate wear</p>
    <p>Estimates temporal locality in applications access pattern  Uses bloom filter to track dirty blocks in hardware cache</p>
    <p>Bloom FilterStore addr</p>
    <p>Write_back[page]++</p>
    <p>Estimated writeback count</p>
  </div>
  <div class="page">
    <p>Bloom filters cleared when they are full</p>
    <p>Maintains number of cache blocks equal to size of last-level cache  Clearing bloom filter causes false spike in measured writebacks</p>
    <p>BloomFilter0</p>
    <p>N = 0 N = CacheSize/2 N = CacheSize</p>
    <p>BloomFilter1</p>
    <p>BloomFilter0</p>
    <p>BloomFilter1</p>
    <p>BloomFilter0</p>
    <p>BloomFilter1</p>
    <p>Kevlar uses estimated writebacks per page to perform page migrations</p>
  </div>
  <div class="page">
    <p>Kevlar migrates heavily written pages to DRAM  Limits PM write bandwidth to 20K writes/sec for 4 year lifetime target  Migrates top 10% freq. written pages to DRAM</p>
    <p>PMDRAM</p>
  </div>
  <div class="page">
    <p>Kevlar migrates heavily written pages to DRAM  Limits PM write bandwidth to 20K writes/sec for 4 year lifetime target  Migrates top 10% freq. written pages to DRAM</p>
    <p>PMDRAM</p>
    <p>Pages sorted by per page write rate</p>
  </div>
  <div class="page">
    <p>Kevlar migrates heavily written pages to DRAM  Limits PM write bandwidth to 20K writes/sec for 4 year lifetime target  Migrates top 10% freq. written pages to DRAM</p>
    <p>PMDRAM</p>
    <p>Migrates pages to DRAM Pages sorted by per</p>
    <p>page write rate</p>
    <p>Optimization: Kevlar disables PEBS counters when write rate is &lt; 20K writes/sec</p>
  </div>
  <div class="page">
    <p>Kevlar detects changes in access pattern  Detects PM write rate below 20K writes/sec for 5 consecutive intervals  Re-enables PEBS monitoring to migrate least 10% written pages to PM</p>
    <p>PMDRAM</p>
    <p>Pages sorted by per page write rate</p>
  </div>
  <div class="page">
    <p>Kevlar detects changes in access pattern  Detects PM write rate below 20K writes/sec for 5 consecutive intervals  Re-enables PEBS monitoring to migrate least 10% written pages to PM</p>
    <p>PMDRAM</p>
    <p>Migrates pages to PMPages sorted by</p>
    <p>per page write rate</p>
  </div>
  <div class="page">
    <p>Methodology  Prototyped in Linux 4.5  Intel Xeon E5-2699 v3, 72 hardware threads  Caches: 32KB L1 D&amp;I, 256KB L2, 45MB LLC  Linux cgroups to isolate cores and memory for server threads  PM fails after 1% pages suffer 107 writes</p>
    <p>Server CPU</p>
    <p>DRAM 256GB</p>
    <p>PM 256GB</p>
    <p>Socket 0 Socket 1</p>
    <p>QPI</p>
  </div>
  <div class="page">
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Ac cu</p>
    <p>ra cy</p>
    <p>(% )</p>
    <p>Accuracy of wear estimation</p>
    <p>Kevlar can correctly detect 80% of top 10% heavily written pages in PM</p>
    <p>Capacity workloads Persistency workloads</p>
  </div>
  <div class="page">
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Li fe</p>
    <p>tim e</p>
    <p>(in y</p>
    <p>ea rs</p>
    <p>)</p>
    <p>No wear-mgmt. Ideal WL WL WL+WR</p>
    <p>PM device lifetime</p>
    <p>PM wears out in 1.1 months in absence of wear-management mechanisms</p>
  </div>
  <div class="page">
    <p>Ideal wear leveling shows lifetime for an oracle design that achieves uniform wear</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Li fe</p>
    <p>tim e</p>
    <p>(in y</p>
    <p>ea rs</p>
    <p>)</p>
    <p>No wear-mgmt. Ideal WL WL WL+WR</p>
    <p>PM device lifetime</p>
  </div>
  <div class="page">
    <p>Kevlar improves PM lifetime by 9.8x as compared to the design without wear-mgmt.</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Li fe</p>
    <p>tim e</p>
    <p>(in y</p>
    <p>ea rs</p>
    <p>)</p>
    <p>No wear-mgmt. Ideal WL WL WL+WR</p>
    <p>PM device lifetime</p>
  </div>
  <div class="page">
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Li fe</p>
    <p>tim e</p>
    <p>(in y</p>
    <p>ea rs</p>
    <p>)</p>
    <p>No wear-mgmt. Ideal WL WL WL+WR</p>
    <p>PM device lifetime</p>
    <p>Kevlar limits PM write bandwidth to achieve lifetime target of 4 years</p>
  </div>
  <div class="page">
    <p>Kevlar performance overhead</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Sl ow</p>
    <p>do w</p>
    <p>n (%</p>
    <p>)</p>
    <p>WL WL+Monitoring WL+WR</p>
    <p>Wear leveling alone incurs a negligible performance overhead of 0.04%</p>
  </div>
  <div class="page">
    <p>Kevlars monitoring based on PEBS counters incur a performance overhead of 0.8% (avg.)</p>
    <p>Kevlar performance overhead</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Sl ow</p>
    <p>do w</p>
    <p>n (%</p>
    <p>)</p>
    <p>WL WL+Monitoring WL+WR</p>
  </div>
  <div class="page">
    <p>Kevlar additionally incurs a 1.2% slowdown due to page migrations between DRAM and PM</p>
    <p>Kevlar performance overhead</p>
    <p>Aerospike Memcached TPCC TATP Redis Echo Mean</p>
    <p>Sl ow</p>
    <p>do w</p>
    <p>n (%</p>
    <p>)</p>
    <p>WL WL+Monitoring WL+WR</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Wear leveling</p>
    <p>Wear reduction</p>
    <p>Analytical framework</p>
    <p>Wear estimation</p>
    <p>Remaps pages in PM</p>
    <p>Performs page migrations</p>
    <p>Estimates per page wear</p>
    <p>Simple remaps achieve near-ideal wear</p>
    <p>Simple software mechanisms achieve &gt; 4yr lifetime with 1.2% perf. overhead</p>
  </div>
  <div class="page">
    <p>Software Wear Management for Persistent Memories</p>
    <p>Vaibhav Gogte, William Wang1, Stephan Diestelhorst1, Aasheesh Kolli2,3, Peter M. Chen, Satish Narayanasamy, Thomas F. Wenisch</p>
    <p>FAST19 02/26/2019</p>
  </div>
</Presentation>
