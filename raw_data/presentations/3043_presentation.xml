<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Flare: Optimizing Apache Spark with Native Compilation for Scale-Up Architectures and Medium-Size Data</p>
    <p>Gregory Essertel1, Ruby Y. Tahboub1 , James M. Decker1 , Kevin J. Brown2 , Kunle Olukotun2 , Tiark Rompf1</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>How Fast Is Spark?</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Motivation</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Lets dive into Spark</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Bottlenecks</p>
    <p>override def doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String = { val (broadcastRelation, relationTerm) = prepareBroadcast(ctx) val (keyEv, anyNull) = genStreamSideJoinKey(ctx, input) val (matched, checkCondition, buildVars) = getJoinCondition(ctx, input) val numOutput = metricTerm(ctx, &quot;numOutputRows&quot;) val resultVars = ... ctx.copyResult = true val matches = ctx.freshName(&quot;matches&quot;) val iteratorCls = classOf[Iterator[UnsafeRow]].getName s&quot;&quot;&quot; |// generate join key for stream side |${keyEv.code} |// find matches from HashRelation |$iteratorCls $matches = $anyNull ? null : ($iteratorCls)$relationTerm.get(${keyEv.value}); |if ($matches == null) continue; |while ($matches.hasNext()) { | UnsafeRow $matched = (UnsafeRow) $matches.next(); | $checkCondition | $numOutput.add(1); | ${consume(ctx, resultVars)} |} &quot;&quot;&quot;.stripMargin }</p>
    <p>select *</p>
    <p>from lineitem, orders</p>
    <p>where l_orderkey = o_orderkey</p>
  </div>
  <div class="page">
    <p>Bottlenecks</p>
  </div>
  <div class="page">
    <p>Flare: a New Back-End for Spark</p>
  </div>
  <div class="page">
    <p>Flare design</p>
  </div>
  <div class="page">
    <p>Lightweight Modular Staging (LMS)</p>
    <p>def power(x: Int, n: Int): Int = {</p>
    <p>if (n == 0)</p>
    <p>else</p>
    <p>x * power(x, n - 1)</p>
    <p>}</p>
    <p>def power(x: Rep[Int], n: Int): Rep[Int] = { if (n == 0) 1 else x * power(x, n - 1) }</p>
    <p>val x: Rep[Int] =  val res = power(x, 4)</p>
    <p>val x: Int =</p>
    <p>val x1 = x * x</p>
    <p>val x2 = x * x1</p>
    <p>val x3 = x * x2</p>
    <p>val res = x3</p>
    <p>Stage</p>
    <p>Run</p>
  </div>
  <div class="page">
    <p>Flare implementation</p>
    <p>type Pred = Record =&gt; Rep[Boolean]</p>
    <p>abstract class Op { def exec(callback: Record =&gt; Unit): Unit }</p>
    <p>class Select(op: Op)(pred: Pred) extends Op { def exec(cb: Record =&gt; Unit) = { op.exec { tuple =&gt; if (pred(tuple)) cb(tuple) } } }</p>
    <p>select</p>
    <p>l_returnflag</p>
    <p>from</p>
    <p>lineitem</p>
    <p>where</p>
    <p>l_quantity &lt;= 1</p>
    <p>double* x14 = ... // data loading l_quantity</p>
    <p>char* x22 =  // l_returnflag</p>
    <p>printf(&quot;%s\n&quot;,&quot;begin scan lineitem&quot;);</p>
    <p>for (x744 = 0; x744 &lt; x4; x744++) {</p>
    <p>long x760 = x744;</p>
    <p>double* x769 = x14;</p>
    <p>double x770 = x769[x760];</p>
    <p>char* x777 = x22;</p>
    <p>char x778 = x777[x760];</p>
    <p>bool x804 = x770 &lt;= 1.0;</p>
    <p>if (x804) {</p>
    <p>printf(&quot;%c|\n&quot;,x778);</p>
    <p>}</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>Single-Core Running Time: TPCH</p>
    <p>Absolute running time in milliseconds (ms) for Postgres, Spark, HyPer and Flare in SF10</p>
  </div>
  <div class="page">
    <p>Apache Parquet Format Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 Spark CSV 16762 12244 21730 19836 19316 12278 24484 17726 30050 29533 5224 Spark Parquet 3728 13520 9099 6083 8706 535 13555 5512 19413 21822 3926 Flare CSV 641 168 757 698 758 568 788 875 1417 854 128 Flare Parquet 187 17 125 127 151 99 183 160 698 309 9</p>
    <p>Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 Q21 Q22 Spark CSV 21688 8554 12962 26721 12941 24690 27012 12409 19369 57330 7050 Spark Parquet 5570 7034 719 4506 21834 5176 6757 2681 8562 25089 5295 Flare CSV 701 388 573 551 150 1426 1229 605 792 1868 178 Flare Parquet 133 246 86 88 66 264 181 178 165 324 22</p>
  </div>
  <div class="page">
    <p>What about parallelism?</p>
  </div>
  <div class="page">
    <p>Parallel Scaling Experiment</p>
    <p>Scaling-up Flare and Spark SQL in SF20</p>
    <p>Hardware: Single NUMA machine with 4 sockets, 24 Xeon Platinum 8168 @ 2.70GHz cores per socket, and 256GB RAM per socket (1 TB total).</p>
  </div>
  <div class="page">
    <p>NUMA Optimization</p>
  </div>
  <div class="page">
    <p>NUMA Optimization</p>
    <p>Scaling-up Flare for SF100 with NUMA optimizations on different configurations: threads pinned to one, two and four sockets</p>
    <p>Hardware: Single NUMA machine with 4 sockets, 18 Xeon E5-4657L cores per socket, and 256GB RAM per socket (1 TB total).</p>
  </div>
  <div class="page">
    <p>Heterogeneous Workloads: UDFs and ML Kernels</p>
  </div>
  <div class="page">
    <p>Flare</p>
  </div>
  <div class="page">
    <p>TensorFlare architecture</p>
    <p>Flare</p>
    <p>TensorFlow Model Specialized data loading</p>
    <p>TensorFlow Runtime</p>
    <p>XLA HDD</p>
    <p>SQL Engine</p>
    <p>produces</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p># Define linear classifier using TensorFlow import tensorflow as tf # weights from pre-trained model elided mat = tf.constant([[...]]) bias = tf.constant([...])</p>
    <p>def classifier(c1,c2,c3,c4): # compute distance x = tf.constant([[c1,c2,c3,c4]]) y = tf.matmul(x, mat) + bias y1 = tf.session.run(y1)[0] return max(y1)</p>
    <p># Register classifier as UDF spark.udf.register(&quot;classifier&quot;, classifier) # Use classifier in PySpark: q = spark.sql(&quot; select real_class, sum(case when class = 0 then 1 else 0 end) as class1, sum(case when class = 1 then 1 else 0 end) as class2, ... until 4 ... from (select real_class, classifier(c1,c2,c3,c4) as class from data) group by real_class order by real_class&quot;) q.show()</p>
  </div>
  <div class="page">
    <p>flaredata.github.io</p>
    <p>Identify key impediments to performance for medium-sized workloads running on Spark</p>
    <p>Flare optimizes data loading and generates parallel code NUMA aware</p>
    <p>Flare reduces the gap between Spark SQL and best-of-breed relational query engines</p>
    <p>Thank you!</p>
  </div>
</Presentation>
