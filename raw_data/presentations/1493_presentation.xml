<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MIGRATORY COMPRESSION Coarse-grained Data Reordering to Improve Compressibility</p>
    <p>Xing Lin*, Guanlin Lu, Fred Douglis, Philip Shilane, Grant Wallace</p>
    <p>*University of Utah</p>
    <p>EMC Corporation</p>
    <p>Data Protection and Availability Division</p>
  </div>
  <div class="page">
    <p>Overview  Compression finds redundancy among strings</p>
    <p>within a certain distance (window size)  Problem: window sizes are small, and similarity across a</p>
    <p>larger distance will not be identified</p>
    <p>Migratory compression (MC): coarse-grained reorganization to group similar blocks to improve compressibility</p>
    <p>A generic pre-processing stage for standard compressors  In many cases, improve both compressibility and</p>
    <p>throughput  Effective for improving compression for archival storage</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Compressor MAX Window size Techniques gzip 64 KB LZ; Huffman coding bzip2 900 KB Run-length encoding;</p>
    <p>Burrows-Wheeler Transform; Huffman coding</p>
    <p>Compression Factor (CF) = ratio of original / compressed  Throughput = original size / (de)compression time</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Compressor MAX Window size Techniques gzip 64 KB LZ; Huffman coding bzip2 900 KB Run-length encoding;</p>
    <p>Burrows-Wheeler Transform; Huffman coding</p>
    <p>Compression Factor (CF) = ratio of original / compressed  Throughput = original size / (de)compression time</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Compressor MAX Window size Techniques</p>
    <p>gzip 64 KB LZ; Huffman coding</p>
    <p>bzip2 900 KB Run-length encoding; BurrowsWheeler Transform; Huffman coding</p>
    <p>C o</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/ s)</p>
    <p>Compression Factor (X)</p>
    <p>Example Throughput vs. CF</p>
    <p>gzip</p>
    <p>bzip2</p>
    <p>Compression Factor (CF) = ratio of original / compressed  Throughput = original size / (de)compression time</p>
    <p>The larger the window, the better the compression</p>
    <p>but slower</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>C o</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/ s)</p>
    <p>Compression Factor (X)</p>
    <p>Example Throughput vs. CF</p>
    <p>gzip</p>
    <p>bzip2</p>
    <p>Compression Factor (CF) = ratio of original / compressed  Throughput = original size / (de)compression time</p>
    <p>The larger the window, the better the compression</p>
    <p>but slower</p>
    <p>rzip (from rsync): intra-file deduplication; bzip2 the remainder</p>
    <p>Compressor MAX Window size Techniques</p>
    <p>gzip 64 KB LZ; Huffman coding</p>
    <p>bzip2 900 KB Run-length encoding; BurrowsWheeler Transform; Huffman coding</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Compress a single, large file  Traditional compressors are unable to exploit redundancy</p>
    <p>across a large range of data (e.g., many GB)</p>
    <p>A A B A C B   (GBs)</p>
    <p>compress A A B A C B</p>
    <p>compress</p>
    <p>transform</p>
    <p>mzip Standard compressor</p>
    <p>gz window</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>A B C CR1</p>
    <p>A B C CR2</p>
    <p>Backup tier</p>
    <p>CR: unit of data that is compressed together</p>
    <p>(e.g. 128 KB)</p>
    <p>Better compression for long-term retention  Data migration from backup to archive tier  Observation: similar blocks could be scattered across</p>
    <p>compression regions (CRs) in the backup tier</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>A B C CR1</p>
    <p>A B C CR2</p>
    <p>Archive tier</p>
    <p>A B C CR1</p>
    <p>A B C CR2</p>
    <p>Backup tier</p>
    <p>Migrate</p>
    <p>Current practice</p>
    <p>Better compression for long-term retention  Data migration from backup to archive tier  Observation: similar blocks could be scattered across</p>
    <p>compression regions (CRs) in the backup tier</p>
  </div>
  <div class="page">
    <p>Motivation  Better compression for long-term retention</p>
    <p>Data migration from backup to archive tier  Observation: similar blocks could be scattered across</p>
    <p>compression regions (CRs) in the backup tier  Proposal: fill each CR with similar data</p>
    <p>Archive tier</p>
    <p>A B C CR1</p>
    <p>A B C CR2</p>
    <p>Backup tier</p>
    <p>A A B CR1</p>
    <p>C CR2</p>
    <p>B</p>
    <p>C</p>
    <p>Migrate</p>
    <p>Migratory compression</p>
  </div>
  <div class="page">
    <p>Recap</p>
    <p>Migratory compression  Idea: improve compression by grouping similar blocks  Use cases</p>
    <p>mzip: compress a single, large file  Data migration for archival storage</p>
    <p>Considerations  How to identify similar blocks?  How to reorganize blocks efficiently?  Other tradeoffs (refer to the paper)</p>
  </div>
  <div class="page">
    <p>mzip Workflow</p>
    <p>Segmentation: partition into blocks, calculate similarity features</p>
    <p>Similarity detector: group by content and identify duplicate and similar blocks; output migrate and restore recipe</p>
    <p>Reorganizer: rearrange the input file  Reorganized file may exist only as a pipe</p>
    <p>into compressor</p>
    <p>File</p>
    <p>Segmentation</p>
    <p>Compressor</p>
    <p>Similarity detector</p>
    <p>Reorganizer</p>
    <p>Reorganized file</p>
    <p>Compressed file</p>
  </div>
  <div class="page">
    <p>mzip Workflow</p>
    <p>File</p>
    <p>Segmentation</p>
    <p>Compressor</p>
    <p>Similarity detector</p>
    <p>Reorganizer</p>
    <p>Reorganized file</p>
    <p>Compressed file</p>
    <p>Decompressor</p>
    <p>Restorer</p>
    <p>Compressed file</p>
    <p>File</p>
    <p>Extract restore recipe</p>
    <p>Reorganized file</p>
  </div>
  <div class="page">
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>A</p>
    <p>D</p>
    <p>B</p>
    <p>Block ID</p>
    <p>File</p>
    <p>migrate recipe</p>
    <p>restore recipe</p>
    <p>Similarity detector</p>
    <p>mzip Example</p>
  </div>
  <div class="page">
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>A</p>
    <p>D</p>
    <p>B</p>
    <p>Block ID</p>
    <p>A A</p>
    <p>B B</p>
    <p>C</p>
    <p>D</p>
    <p>File</p>
    <p>R eorg</p>
    <p>an ized</p>
    <p>File</p>
    <p>migrate recipe</p>
    <p>restore recipe</p>
    <p>Reorganize</p>
    <p>Restore</p>
    <p>mzip Example</p>
  </div>
  <div class="page">
    <p>Similarity Detection</p>
    <p>Similarity feature (based on [Broder 1997])  A strong hash for duplication detection  Features: weak hashes provide hints about similarity among</p>
    <p>blocks  Two blocks are similar when they share values for some</p>
    <p>weak hashes</p>
    <p>Mature technique  REBL: Redundancy Elimination at Block Level [Kulkarni et al.</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A</p>
    <p>A</p>
    <p>B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A</p>
    <p>A</p>
    <p>B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B C</p>
    <p>D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B C D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B C D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Data Reorganization</p>
    <p>input</p>
    <p>A A B</p>
    <p>B C D</p>
    <p>Block-level</p>
  </div>
  <div class="page">
    <p>Data Reorganization</p>
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Multipass (HDD)</p>
    <p>input</p>
    <p>A</p>
    <p>A</p>
    <p>B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level Multipass</p>
    <p>Buffer 1st scan</p>
  </div>
  <div class="page">
    <p>Data Reorganization</p>
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Multipass (HDD)</p>
    <p>input</p>
    <p>A A B</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Block-level Multipass</p>
    <p>Buffer 1st scan</p>
  </div>
  <div class="page">
    <p>Data Reorganization</p>
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Multipass (HDD)</p>
    <p>input Block-level Multipass</p>
    <p>Buffer</p>
    <p>B</p>
    <p>C</p>
    <p>D 2 n d scan</p>
  </div>
  <div class="page">
    <p>Data Reorganization</p>
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Multipass (HDD)</p>
    <p>input</p>
    <p>B C D</p>
    <p>Block-level Multipass</p>
    <p>Buffer 2n d scan</p>
  </div>
  <div class="page">
    <p>Data Reorganization</p>
    <p>Re-arrange the input file, according to a recipe  Reorganizer &amp; Restorer</p>
    <p>Options  Block-level</p>
    <p>Random IOs  Fine for memory and SSD</p>
    <p>Multipass (HDD)  Convert random IOs into sequential scans</p>
    <p>input</p>
    <p>B C D</p>
    <p>Block-level Multipass</p>
    <p>Buffer 2n d scan</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>mzip:  How much can compression be improved?  What is the complexity (runtime overhead)?  How does SSD or HDD affect data reorganization</p>
    <p>performance? For HDD, does multi-pass help?  How does MC perform, compared with delta compression?</p>
    <p>(refer to our paper)  What are the configuration options for MC? (refer to our</p>
    <p>paper)</p>
    <p>Data migration for archival storage  Compression and performance</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>Type Name Size (GB) Dedupe Factor</p>
    <p>(X)</p>
    <p>Compression Factor (standalone - baseline)</p>
    <p>gzip bzip2 7z rzip</p>
    <p>Workstation backup</p>
    <p>WS1 17.36 1.69 2.70 3.22 4.44 4.46</p>
    <p>Email server backup</p>
    <p>EXCHANGE2 17.32 1.02 2.78 3.13 4.75 4.79</p>
    <p>VM image UbuntuVM 6.98 1.51 3.90 4.26 6.71 6.69</p>
    <p>* Only one dataset for each type is shown here. Refer to our paper for others.</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/ s)</p>
    <p>Compression Factor (X)</p>
    <p>Workstation1</p>
    <p>gzip</p>
    <p>gz(mc)</p>
    <p>bzip2</p>
    <p>bz(mc)</p>
    <p>rzip</p>
    <p>rz(mc)</p>
    <p>Compression Factor vs. Compression Throughput</p>
    <p>gzip</p>
    <p>gzip(mc)</p>
    <p>bzip2 7z</p>
    <p>rzip</p>
    <p>MC improves both CF and compression throughput  Deduplication  Re-organization</p>
  </div>
  <div class="page">
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/ s)</p>
    <p>Compression Factor (X)</p>
    <p>Exchange2</p>
    <p>gzip</p>
    <p>bzip2</p>
    <p>rzip</p>
    <p>gz(mc)</p>
    <p>bz(mc)</p>
    <p>rz(mc)</p>
    <p>Compression Factor vs. Compression Throughput</p>
    <p>MC improves CF but slightly reduces compression throughput</p>
  </div>
  <div class="page">
    <p>Maximal Compression</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/ s)</p>
    <p>Compression Factor (X)</p>
    <p>Workstation1</p>
    <p>Results for other compressors have</p>
    <p>same trends</p>
    <p>MC benefits maximal compression</p>
    <p>Compressors can be run at various compression levels and window sizes, making a tradeoff between compression and runtime</p>
  </div>
  <div class="page">
    <p>Compression Factor Breakdown MC improves compression</p>
    <p>to varying extents beyond dedup and gzip</p>
    <p>EX2 WS1 UBUN</p>
    <p>C o</p>
    <p>m p</p>
    <p>re si</p>
    <p>o n</p>
    <p>F a ct</p>
    <p>o r</p>
    <p>(X )</p>
    <p>Dataset</p>
    <p>reorg gzip dedup</p>
  </div>
  <div class="page">
    <p>EX1 WS1 UBUN</p>
    <p>C o</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>T p</p>
    <p>u t</p>
    <p>(M</p>
    <p>B /</p>
    <p>s) mem</p>
    <p>ssd-block</p>
    <p>hdd-multipass</p>
    <p>hdd-block</p>
    <p>Data Reorganization SSD: reasonably good HDD: multipass helps  Configurations</p>
    <p>mem: input and output stored in tmpfs  SSD/HDD: used to store input; output to HDD; 8 GB mem</p>
    <p>UBUN: 7 G Multipass window: 4.8 G (2 scans)</p>
  </div>
  <div class="page">
    <p>Data Migration for Archival Storage</p>
    <p>DataDomain Filesystem (DDFS)  Backup tier: LZ (fast)  Archive tier: recompresses with GZ (25-44% better CF)</p>
    <p>With MC,  Identify and sort by cluster sizes of similar blocks  Migrate in 3 stages: top third largest clusters, middle third,</p>
    <p>bottom third (including distinct blocks)</p>
    <p>Datasets  WORKSTATIONS: many backups of several workstations  Exchange[123]: many backups of 3 exchange email servers</p>
  </div>
  <div class="page">
    <p>Effect of MC on Archival Migration  MC improves CF  [44% (EX3), 157%</p>
    <p>(WS)]  Top 2/3 compresses</p>
    <p>very well</p>
    <p>WS EX1 EX2 EX3</p>
    <p>C o</p>
    <p>m p</p>
    <p>re ss</p>
    <p>io n</p>
    <p>F a ct</p>
    <p>o r</p>
    <p>(X )</p>
    <p>gzip</p>
    <p>mc_total</p>
    <p>top1/3</p>
    <p>mid1/3</p>
    <p>end1/3</p>
  </div>
  <div class="page">
    <p>Archival Migration  Costs</p>
    <p>Migration Runtime  Exchange1: 3X longer</p>
    <p>Read Performance  As data is scattered in file system, read performance</p>
    <p>suffers [Lillibridge 2013]  entire EXCHANGE1: 1.3 X longer  final backup: 7X longer (1.24 X longer if just the top third</p>
    <p>of similar clusters are reorganized)</p>
    <p>Memory overheads</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Improving traditional compressors  LZMA algorithm with extra-large window (7z)  rzip rolling hash to deduplicate with a large window  Burrows-Wheeler Transform to reorder data (bzip2)</p>
    <p>Delta compression (MC is slightly better in CF and throughput)  Similar to MC when limited to a single file  Bookkeeping complexities in a file system</p>
    <p>Similarity detection  WAN Optimized Replication: delta-compress similar chunks for replication  REBL: Redundancy Elimination at Block Level (Never applied practically at</p>
    <p>scale, or in self-contained file)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Migratory Compression preprocesses data to make it more compressible</p>
    <p>Identify and cluster similar data</p>
    <p>mzip  Improves existing compressors, in both compressibility</p>
    <p>and frequently runtime  Redraw the performance-compression curve!</p>
    <p>Archival storage  MC reduces $/GB further</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t</p>
    <p>CF</p>
  </div>
  <div class="page">
    <p>Thank you! Questions?</p>
  </div>
</Presentation>
