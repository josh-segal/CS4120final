<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Discriminative Projections for Text Similarity Measures</p>
    <p>Scott Wen-tau Yih Joint work with Kristina Toutanova, John Platt, Chris Meek</p>
    <p>Microsoft Research</p>
  </div>
  <div class="page">
    <p>Cross-language Document Retrieval</p>
    <p>English Query Doc</p>
    <p>Spanish Document Set</p>
  </div>
  <div class="page">
    <p>Web Search &amp; Advertising</p>
  </div>
  <div class="page">
    <p>Web Search &amp; Advertising</p>
    <p>Query: ACL in Portland</p>
    <p>ACL Construction LLC (Portland) ACL Construction LLC in Portland, OR -Map, Phone Number, Reviews,  www.superpages.com</p>
    <p>ACL HLT 2011 The 49th Annual Meeting of the Association for Computational Linguistics acl2011.org</p>
  </div>
  <div class="page">
    <p>Web Search &amp; Advertising</p>
    <p>Query: ACL in Portland</p>
    <p>Don't Have ACL Surgery Used By Top Athletes Worldwide Don't Let Them Cut You See Us First www.arpwaveclinic.com</p>
    <p>Expert Knee Surgeons Get the best knee doctor for your torn ACL surgery. EverettBoneAndJoint.com/Knee</p>
    <p>ACL: Anterior Cruciate Ligament injuries</p>
  </div>
  <div class="page">
    <p>Vector Space Model Represent text objects as vectors</p>
    <p>Word/Phrase: term co-occurrences Document: term vectors with TFIDF/BM25 weighting Similarity is determined using functions like cosine of the corresponding vectors</p>
    <p>Weaknesses Different but related terms cannot be matched</p>
    <p>e.g., (buy, used, car) vs. (purchase, pre-owned, vehicle)</p>
    <p>Not suitable for cross-lingual settings</p>
    <p>cos()vq</p>
    <p>vdqv</p>
  </div>
  <div class="page">
    <p>Learning Concept Vector RepresentationAre and relevant or semantically similar?</p>
    <p>Input: high-dimensional, sparse term vectors Output: low-dimensional, dense concept vectors Model requirements</p>
    <p>Transformation is easy to compute Provide good similarity measures</p>
    <p>(,)</p>
  </div>
  <div class="page">
    <p>Ideal Mapping</p>
    <p>High-dimensional space Low-dimensional space</p>
  </div>
  <div class="page">
    <p>Dimensionality Reduction Methods</p>
    <p>ProjectionProbabilistic</p>
    <p>S up</p>
    <p>er vi</p>
    <p>se d</p>
    <p>U ns</p>
    <p>up er</p>
    <p>vi se</p>
    <p>d</p>
    <p>PLSA</p>
    <p>LDA</p>
    <p>PCA</p>
    <p>LSA</p>
    <p>OPCA</p>
    <p>CCA HDLR</p>
    <p>CL-LSIJPLSA</p>
    <p>CPLSA</p>
    <p>PLTM</p>
    <p>S2Net</p>
  </div>
  <div class="page">
    <p>Outline Introduction Problem &amp; Approach Experiments</p>
    <p>Cross-language document retrieval Ad relevance measures Web search ranking</p>
    <p>Discussion &amp; Conclusions</p>
  </div>
  <div class="page">
    <p>Goal  Learn Vector Representation Approach: Siamese neural network architecture</p>
    <p>Train the model using labeled (query, doc) Optimize for pre-selected similarity function (cosine)</p>
    <p>vqry vdoc</p>
    <p>Query Doc</p>
    <p>sim  ,</p>
  </div>
  <div class="page">
    <p>Goal  Learn Vector Representation Approach: Siamese neural network architecture</p>
    <p>Train the model using labeled (query, doc) Optimize for pre-selected similarity function (cosine)</p>
    <p>vqry vdoc</p>
    <p>Query Doc</p>
    <p>Model</p>
    <p>sim  ,</p>
  </div>
  <div class="page">
    <p>S2Net  Similarity via Siamese NN Model form is the same as LSA/PCA Learning the projection matrix discriminatively</p>
    <p>vqry vdoc 1</p>
    <p>1</p>
    <p>=</p>
    <p>sim  ,</p>
  </div>
  <div class="page">
    <p>Pairwise Loss  Motivation In principle, we can use a simple loss function like mean-squared error: . But</p>
  </div>
  <div class="page">
    <p>Pairwise Loss Consider a query and two documents and</p>
    <p>Assume is more related to , compared to : original term vectors of and</p>
    <p>: scaling factor, as in the experiments</p>
    <p>-2 -1 0 1 2 0</p>
  </div>
  <div class="page">
    <p>Model Training Minimizing the loss function can be done using standard gradient-based methods</p>
    <p>Derive batch gradient and apply L-BFGS</p>
    <p>Non-convex loss Starting from a good initial matrix helps reduce training time and converge to a better local minimum</p>
    <p>Regularization Model parameters can be regularized by adding a smoothing term in the loss function Early stopping can be effective in practice</p>
  </div>
  <div class="page">
    <p>Outline Introduction Problem &amp; Approach Experiments</p>
    <p>Cross-language document retrieval Ad relevance measures Web search ranking</p>
    <p>Discussion &amp; Conclusions</p>
  </div>
  <div class="page">
    <p>Cross-language Document Retrieval</p>
    <p>Dataset: pairs of Wiki documents in EN and ES Same setting as in [Platt et al. EMNLP-10] #document in each language</p>
    <p>Training: 43,380, Validation: 8,675, Test: 8,675</p>
    <p>Effectively, billion training examples Positive: EN-ES documents in the same pair Negative: All other pairs</p>
    <p>Evaluation: find the comparable document in the different language for each query document</p>
  </div>
  <div class="page">
    <p>S2Net</p>
    <p>OPCA</p>
    <p>CPLSA</p>
    <p>JPLSA</p>
    <p>CLLSI</p>
    <p>Dimension</p>
    <p>M e</p>
    <p>a n</p>
    <p>R e</p>
    <p>c ip</p>
    <p>ro c</p>
    <p>a l</p>
    <p>R a</p>
    <p>n k</p>
    <p>( M</p>
    <p>R R</p>
    <p>)</p>
    <p>Results on Wikipedia Documents</p>
  </div>
  <div class="page">
    <p>Ad Relevance Measures Task: Decide whether a paid-search ad is relevant to the query</p>
    <p>Filter irrelevant ads to ensure positive search experience : pseudo-document from Web relevance feedback : ad landing page</p>
    <p>Data: query-ad human relevance judgment Training: 226k pairs Validation: 169k pairs</p>
    <p>Testing: 169k pairs</p>
  </div>
  <div class="page">
    <p>The ROC Curves of the Ad Filters</p>
    <p>S2Net (k=1000)</p>
    <p>TFIDF</p>
    <p>HDLR (k=1000)</p>
    <p>CPLSA (k=1000)</p>
    <p>False-Positive Rate (Mistakenly filtered good ads)</p>
    <p>T ru</p>
    <p>e -P</p>
    <p>o s it iv</p>
    <p>e R</p>
    <p>a te</p>
    <p>(C a</p>
    <p>u g</p>
    <p>h t</p>
    <p>b a</p>
    <p>d a</p>
    <p>d s )</p>
    <p>Better</p>
  </div>
  <div class="page">
    <p>Web Search Ranking [Gao et al., SIGIR-11]</p>
    <p>query 1</p>
    <p>query 2</p>
    <p>query 3</p>
    <p>doc 1</p>
    <p>doc 2</p>
    <p>doc 3</p>
    <p>Parallel corpus from clicks 82,834,648 query-doc pairs</p>
    <p>query doc 1 Good</p>
    <p>doc 2 Fair</p>
    <p>doc 3 Bad</p>
    <p>Human relevance judgment 16,510 queries 15 doc per query in average</p>
    <p>Train latent semantic models Evaluate using labeled data</p>
  </div>
  <div class="page">
    <p>Results on Web Search Ranking</p>
    <p>VSM LSA CL-LSA OPCA S2Net 0.2</p>
    <p>NDCG@1 NDCG@3 NDCG@10</p>
    <p>Only S2Net outperforms VSM compared to other projection models</p>
  </div>
  <div class="page">
    <p>Results on Web Search Ranking</p>
    <p>VS M</p>
    <p>LS A</p>
    <p>+V SM</p>
    <p>CL -L</p>
    <p>SA ..</p>
    <p>.</p>
    <p>O PC</p>
    <p>A +V</p>
    <p>SM</p>
    <p>S2 Ne</p>
    <p>t + VS</p>
    <p>M 0.2</p>
    <p>NDCG@1 NDCG@3 NDCG@10</p>
    <p>After combined with VSM, results are all improved More details and interesting results of generative topic models can be found in [SIGIR-11]</p>
  </div>
  <div class="page">
    <p>Outline Introduction Problem &amp; Approach Experiments</p>
    <p>Cross-language document retrieval Ad relevance measures Web search ranking</p>
    <p>Discussion &amp; Conclusions</p>
  </div>
  <div class="page">
    <p>Model Comparisons S2Net vs. generative topic models</p>
    <p>Can handle explicit negative examples No special constraints on input vectors</p>
    <p>S2Net vs. linear projection methods Loss function designed to closely match the true objective Computationally more expensive</p>
    <p>S2Net vs. metric learning Target high-dimensional input space Scale well as the number of examples increases</p>
  </div>
  <div class="page">
    <p>Why Does S2Net Outperform Other Methods?</p>
    <p>Loss function Closer to the true evaluation objective</p>
    <p>Slight nonlinearity Cosine instead of inner-product</p>
    <p>Leverage a large amount of training data Easily parallelizable: distributed gradient computation</p>
  </div>
  <div class="page">
    <p>Conclusions S2Net: Discriminative learning framework for dimensionality reduction</p>
    <p>Learns a good projection matrix that leads to robust text similarity measures Strong empirical results on different tasks</p>
    <p>Future work Model improvement</p>
    <p>Handle Web-scale parallel corpus more efficiently Convex loss function</p>
    <p>Explore more applications e.g., word/phrase similarity</p>
  </div>
</Presentation>
