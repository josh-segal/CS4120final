<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Inferring Origin Flow Pa0erns in Wi-Fi with Deep Learning</p>
    <p>Youngjune Gwon H. T. Kung</p>
    <p>Philadelphia, PA</p>
    <p>June 18, 2014</p>
  </div>
  <div class="page">
    <p>Introduc5on  Background  Origin flow paMern inference in Wi-Fi  Classical approaches  Our approach  Evalua5on  Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Network traffic analysis is classical research topic  Study, measure, and es5mate flow characteris5cs</p>
    <p>E.g., burst size and interarrival 5me distribu5ons, mean values  Network nodes (routers) regularly sample packets</p>
    <p>To provide data used for analysis</p>
    <p>Why?  Traffic monitoring</p>
    <p>Spot anomalies, (D)DoS aMacks, heavy hiMers  Help manage networking resources</p>
    <p>Wireless spectrum among most precious networking resources  Program network nodes (SDN)</p>
    <p>Improve Tx-Rx scheduling, interference mi5ga5on</p>
    <p>What Is Network Traffic Inference?</p>
  </div>
  <div class="page">
    <p>Flow PaMern  Sequence of data bytes (run) with wai5ng 5mes (gap)  Runs-and-gaps model</p>
    <p>Flow paMern  !me series data  Simple, but powerful abstrac5on</p>
    <p>Applicable at any node (src, dst, intermediate)</p>
    <p>similar ideas to force orthonormal dictionary columns as our two-stage algorithm described in 5.1. However, our idea of accompanying sparsity relaxation (5.2) due to sparse coding with an incoherent dictionary is novel.</p>
    <p>Section 2 explains the time-series representation and processing of a flow. In Section 3, we explore supervised learning methods for the origin flow inference. Section 4 describes our baseline semi-supervised learning method. We propose several enhancements to the baseline method in Section 5 and evaluate the learning methods with a custom simulator and OPNET in Section 6. The paper concludes in Section 7.</p>
    <p>The runs-and-gaps model [13] gives a concise way to describe a flow. In Figure 2, characteristic patterns of an example flow are captured by packet runs and gaps measurable over time. As indicated earlier, we perform our flow measurements directly at the MAC layer rather than at the transport or IP layers by sampling and processing Wi-Fi frames.</p>
    <p>!&quot;#$%&amp;</p>
    <p>'()$&amp; *+,&amp;-./&amp;</p>
    <p>-./&amp; *+,&amp; -./&amp; *+,&amp; -./&amp;</p>
    <p>Figure 2: Runs-and-gaps model</p>
    <p>Let w = [w1 w2 ... wt ... wN] be a vector containing the number of packets in a flow measured over N time intervals. Here, an important parameter is the unit interval Ts or sampling period during which each element wt is sampled and recorded. The total measurement time or observation window is N  Ts. Alternatively, we have vector x = [x1 x2 ... xt ... xN] for w where xt is the corresponding byte count of the total payload at time t. Hence, a zero in x (or w) indicates a gap. If w7 = 3 and x7 = 1,492, we have 3 packets for the flow at t = 7, and the sum of the payloads from the 3 packets is 1,492 bytes. We will call either w or x a measurable input vector for inference, which contains extractable features. While both w and x carry unique information, we mainly work with x throughout this paper.</p>
    <p>We designate an origin flow (pattern) with another vector f. Just like x, f is a sequence of byte counts uniformly sampled, but the difference is that f reflects the initial pattern (or signature) originated at its source. Note f  RM whereas x  RN , and M and N are not necessarily equal. We use notation xi,k to refer kth measurement on flow i since there can be many measurements on fi. We also use fi,k to designate the kth instance of origin flow i because there could be many origin patterns, or the pattern can be a stochastic process and changes dynamically over time. In summary, f,w, and x are all finite time-series representations of a flow.</p>
    <p>Consider sampling and processing of three example flows in Fig. 3 at a receiver. The receive buffer first timestamps each arriving data frame and marks with flow ID. At t = 1, the received frame for flow 1 contains 2 packets whose payload sizes are 50 and 50 bytes, denoted in (2, 50/50B). At t = 6, flow 3 has two received frames. The first frame contains 2 packets with sizes 100 and 400 bytes whereas the second frame contains only one packet with 1,000 bytes. The example results in the following:</p>
    <p>measure. Flow 1 has 133.3 packets/sec, Flow 2 with 50 packets/sec, and Flow 3 with 116.7 packets/sec. In bit rates, they are 62.7, 240, and 400 kbps, respectively.</p>
    <p>!&quot;!&quot;!#$!%&amp;'(# )*+,-!,+-'./012!</p>
    <p>!&quot;!&quot;!&quot; !&quot;!&quot;!&quot; #$%&amp;&quot;'&quot;</p>
    <p>()*+,-+,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>#$%&amp;&quot;1&quot; (2*2,,-2,,-&quot; 2,,-1,,./&quot; #$%&amp;&quot;'&quot;</p>
    <p>('*3,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ()*0,-+,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ('*3,./&quot;</p>
    <p>#$%&amp;&quot;1&quot; ('*',,,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ()*+,-+,./&quot;</p>
    <p>#! 3! 4! 5! 6! 7!</p>
    <p>#$%&amp;&quot;1&quot; ()*',,-2,,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>Figure 3: Time-series processing example</p>
    <p>Feature Learning</p>
    <p>The core of an inference system comprises a feature extractor (FE) and a classifier (CL) that need to be trained. Figure 4 describes the supervised learning framework. Supervised learning requires a labeled training dataset that consists of training examples {x1, ... ,xT } with corresponding desired output values (i.e., labels) {l1, ... ,lT }. There are two mappings, FE : x  y that maps an input x to its feature y and CL : y  l that performs classification on extracted features of the input. The inference system learns the mappings FE and CL from training examples and their labels. Once trained,</p>
    <p>(per unit interval)</p>
  </div>
  <div class="page">
    <p>Flow 1  w1 = [2 1 2 0 1 2], x1 = [100 80 110 0 80 100]</p>
    <p>Flow 2  w2 = [1 0 1 0 1 0], x2 = [600 0 600 0 600 0]</p>
    <p>Flow 3  w3 = [4 0 0 0 0 3], x3 = [1500 0 0 0 0 1500]</p>
    <p>Runs-and-gaps Time Series Processing</p>
    <p>similar ideas to force orthonormal dictionary columns as our two-stage algorithm described in 5.1. However, our idea of accompanying sparsity relaxation (5.2) due to sparse coding with an incoherent dictionary is novel.</p>
    <p>Section 2 explains the time-series representation and processing of a flow. In Section 3, we explore supervised learning methods for the origin flow inference. Section 4 describes our baseline semi-supervised learning method. We propose several enhancements to the baseline method in Section 5 and evaluate the learning methods with a custom simulator and OPNET in Section 6. The paper concludes in Section 7.</p>
    <p>The runs-and-gaps model [13] gives a concise way to describe a flow. In Figure 2, characteristic patterns of an example flow are captured by packet runs and gaps measurable over time. As indicated earlier, we perform our flow measurements directly at the MAC layer rather than at the transport or IP layers by sampling and processing Wi-Fi frames.</p>
    <p>!&quot;#$%&amp;</p>
    <p>'()$&amp; *+,&amp;-./&amp;</p>
    <p>-./&amp; *+,&amp; -./&amp; *+,&amp; -./&amp;</p>
    <p>Figure 2: Runs-and-gaps model</p>
    <p>Let w = [w1 w2 ... wt ... wN] be a vector containing the number of packets in a flow measured over N time intervals. Here, an important parameter is the unit interval Ts or sampling period during which each element wt is sampled and recorded. The total measurement time or observation window is N  Ts. Alternatively, we have vector x = [x1 x2 ... xt ... xN] for w where xt is the corresponding byte count of the total payload at time t. Hence, a zero in x (or w) indicates a gap. If w7 = 3 and x7 = 1,492, we have 3 packets for the flow at t = 7, and the sum of the payloads from the 3 packets is 1,492 bytes. We will call either w or x a measurable input vector for inference, which contains extractable features. While both w and x carry unique information, we mainly work with x throughout this paper.</p>
    <p>We designate an origin flow (pattern) with another vector f. Just like x, f is a sequence of byte counts uniformly sampled, but the difference is that f reflects the initial pattern (or signature) originated at its source. Note f  RM whereas x  RN , and M and N are not necessarily equal. We use notation xi,k to refer kth measurement on flow i since there can be many measurements on fi. We also use fi,k to designate the kth instance of origin flow i because there could be many origin patterns, or the pattern can be a stochastic process and changes dynamically over time. In summary, f,w, and x are all finite time-series representations of a flow.</p>
    <p>Consider sampling and processing of three example flows in Fig. 3 at a receiver. The receive buffer first timestamps each arriving data frame and marks with flow ID. At t = 1, the received frame for flow 1 contains 2 packets whose payload sizes are 50 and 50 bytes, denoted in (2, 50/50B). At t = 6, flow 3 has two received frames. The first frame contains 2 packets with sizes 100 and 400 bytes whereas the second frame contains only one packet with 1,000 bytes. The example results in the following:</p>
    <p>measure. Flow 1 has 133.3 packets/sec, Flow 2 with 50 packets/sec, and Flow 3 with 116.7 packets/sec. In bit rates, they are 62.7, 240, and 400 kbps, respectively.</p>
    <p>!&quot;!&quot;!#$!%&amp;'(# )*+,-!,+-'./012!</p>
    <p>!&quot;!&quot;!&quot; !&quot;!&quot;!&quot; #$%&amp;&quot;'&quot;</p>
    <p>()*+,-+,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>#$%&amp;&quot;1&quot; (2*2,,-2,,-&quot; 2,,-1,,./&quot; #$%&amp;&quot;'&quot;</p>
    <p>('*3,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ()*0,-+,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ('*3,./&quot;</p>
    <p>#$%&amp;&quot;1&quot; ('*',,,./&quot;</p>
    <p>#$%&amp;&quot;'&quot; ()*+,-+,./&quot;</p>
    <p>#! 3! 4! 5! 6! 7!</p>
    <p>#$%&amp;&quot;1&quot; ()*',,-2,,./&quot;</p>
    <p>#$%&amp;&quot;)&quot; ('*0,,./&quot;</p>
    <p>Figure 3: Time-series processing example</p>
    <p>Feature Learning</p>
    <p>The core of an inference system comprises a feature extractor (FE) and a classifier (CL) that need to be trained. Figure 4 describes the supervised learning framework. Supervised learning requires a labeled training dataset that consists of training examples {x1, ... ,xT } with corresponding desired output values (i.e., labels) {l1, ... ,lT }. There are two mappings, FE : x  y that maps an input x to its feature y and CL : y  l that performs classification on extracted features of the input. The inference system learns the mappings FE and CL from training examples and their labels. Once trained,</p>
    <p>Ts = unit interval (e.g., 100 msec) Note: each flow marked with (# packets, sizes)</p>
  </div>
  <div class="page">
    <p>Origin flow paMern (f)  Conveys applica5on-level data genera5on context  As entering source Tx buffer</p>
    <p>Measured flow paMern (x)  At best, x = ,me-shi1ed f  Reflects severity of conges5on/mix with other flows  As 5mestamped at receiver Rx buffer</p>
    <p>Origin Flow PaMern Inference in Wi-Fi (1)</p>
  </div>
  <div class="page">
    <p>Problem: how to accurately infer origin flow paMern fA from received paMern xA|B?  Key challenge: CSMA alters origin paMern by introducing</p>
    <p>complex, irregular mixture of compe5ng flows  BoMomline: mul!class classifica!on problem</p>
    <p>Origin Flow PaMern Inference in Wi-Fi (2)</p>
  </div>
  <div class="page">
    <p>Supervised learning  ARMAX</p>
    <p>AR = delayed ground truth paMerns (f)  MA = model error ()  X = delayed received paMerns (x)  Train ft = [ft1 ... ftn xt1 ... xtm ]  with labeled dataset {x(i), &lt;f(i), l(i)&gt;}</p>
    <p>Es5mate  via least squares (recursive LS by Kalman filtering)</p>
    <p>Nave Bayes classifier  Using feature y = [run gap] for given x  Train p(l|y)  p(x| l) from with {x(i), y(i), l(i)}</p>
    <p>Semi-supervised learning  Gaussian mixtures</p>
    <p>Use same feature, bivariate y = [run gap] for given x  Train K-Gaussian sum  {w,(, )} via EM with {x(i), y(i)} (unsupervised)</p>
    <p>w = mixing weights, (, ) = Gaussian parameters  Classifica5on: use SVM (supervised)</p>
    <p>Train with posterior (membership) probabili5es with {x(i), &lt;f(i), l(i)&gt;}</p>
    <p>Approaches (Classical)</p>
  </div>
  <div class="page">
    <p>Semi-supervised learning  Phase I: unsupervised feature learning</p>
    <p>next layers input)</p>
    <p>Phase II: supervised classifier training 1. Do mul5-layer sparse coding and pooling with labeled xs 2. Train SVM classifiers with final feature vector resulted at</p>
    <p>top</p>
    <p>Our Approach</p>
  </div>
  <div class="page">
    <p>Mul5-layer Feature Learning and SVM Classifica5on</p>
    <p>f-ext (OMP &amp; K-SVD)</p>
    <p>subsample (Max pool)</p>
    <p>x(1)</p>
    <p>y(1) x(2) = z(1)</p>
    <p>f-ext (OMP &amp; K-SVD)</p>
    <p>subsample (Max pool)</p>
    <p>y(2) x(3) = z(2)</p>
    <p>f-ext (OMP &amp; K-SVD)</p>
    <p>subsample (Max pool)</p>
    <p>y(L) z(L)</p>
    <p>. . .</p>
    <p>(received runs-and-gaps 5me series)</p>
    <p>Layer 1</p>
    <p>Layer 2</p>
    <p>Layer L CSMA spreads flow invariances (some preserved original run lengths) over long period  feature learning &amp; pooling over mul5ple layers iden5fy such invariances</p>
    <p>z(L)</p>
    <p>SVM classifier</p>
    <p>x(L) = z(L1)</p>
  </div>
  <div class="page">
    <p>Describe input x as M linear combina5on of Ds columns  x = D y</p>
    <p>x = measured flow paMern  y = extracted feature from x  OMP computes y &amp; K-SVD trains D</p>
    <p>min X  DYF2 s.t. yk0  M k  Sparsity: M &lt;&lt; N &lt; K</p>
    <p>Sparse coding, clustering, and mixtures are fundamentally same idea</p>
    <p>What Is Sparse Coding?</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>What Is Max Pooling?</p>
    <p>What do we do when we have too many of same kinds?  Need to summarize over them</p>
    <p>Max pooling  Transla5on-invariant subsampling of mul5ple feature vectors  Popular in CNN for image recogni5on</p>
  </div>
  <div class="page">
    <p>Summarizing Deep Feature Learning</p>
    <p>. . . xk Incoming measurements</p>
  </div>
  <div class="page">
    <p>Incoherent dic5onary atoms  Force: DT D  = I with new constraint</p>
    <p>min X  DYF2 +  DT D  IF2 s.t. yk0  M k</p>
    <p>Relax sparsity due to distor5ons resulted by incoherent dic5onary training  Use M &gt; M for OMP</p>
    <p>Overlapping max pooling  z1 = max_pool(y1, ..., yL), z2 = max_pool(y5, ..., yL+4), ...</p>
    <p>Instead of z2 = max_pool(yL+1, ..., y2L), ...</p>
    <p>Enhancements</p>
  </div>
  <div class="page">
    <p>Evalua5on  Simulated 7 Wi-Fi nodes in OPNET Modeler</p>
    <p>10 dis5nct flow paMerns generated at source  Mixed with various other flows including RTP/UDP/IP, HTTP, {p,</p>
    <p>interac5ve DB transac5ons</p>
    <p>Schemes  ARMAX  Nave Bayes  GMM with K = 10 &amp; linear 1-vs-all SVMs  Proposed baseline</p>
    <p>2 layers &amp; linear 1-vs-all SVMs  Proposed baseline + 3 enhancements  Implemented in MATLAB</p>
    <p>Metrics  Classifica5on recall (true posi5ve rate) and false alarm rate</p>
  </div>
  <div class="page">
    <p>Flow PaMerns and Nodes</p>
  </div>
  <div class="page">
    <p>Classifica5on Performance</p>
  </div>
  <div class="page">
    <p>Burst and Interarrival Predic5on Errors</p>
    <p>Scheme Origin run size predicVon error</p>
    <p>Origin gap size predicVon error</p>
    <p>ARMAX 45.9% 36.7%</p>
    <p>Nave Bayes 37.5% 24.6%</p>
    <p>GMM (K = 10)</p>
    <p>Proposed (baseline)</p>
    <p>Proposed (enhanced)</p>
  </div>
  <div class="page">
    <p>Simply, we have created inverse mapping  Measured paMern  origin paMern (prequalified)  This mapping consists of deep feature learner &amp; classifier</p>
    <p>Deep learning  Start with small features, aggregate up, and broaden</p>
    <p>coverage  Can learn invariances and changes introduced by CSMA</p>
    <p>Arbitrary mix of flows, retransmissions, loss of data</p>
    <p>Future direc5ons  Explore other (dis)similarity metrics (e.g., DTW)  Sparse packet sampling, mul5ple hops  Test on real Wi-Fi data  Other inference applica5ons in networking (e.g., protocols)</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Metrics</p>
    <p>Table 1: Origin flows used for evaluation</p>
    <p>Flow Type Generative triplet tr,sr,tg Flow 1 Constant 2,100,4 Flow 2 Constant 2,500,2 Flow 3 Constant 5,200,5 Flow 4 Constant 10,200,10 Flow 5 Stochastic Exp(1), Pareto(100,2), Exp(0.1) Flow 6 Stochastic Exp(0.5), Pareto(40,1), Exp(0.25) Flow 7 Stochastic U(4,10), Pareto(100,2), Exp(0.5) Flow 8 Stochastic N(10,5), Pareto(40,1), N(10,5) Flow 9 Mixed 1, Pareto(100,2), 1 Flow 10 Mixed 1, Pareto(100,2), Exp(0.25)</p>
    <p>of 500 elements.</p>
    <p>We precompute the mean run and gap lengths from the</p>
    <p>generated origin flow patterns in the training dataset.</p>
    <p>This is convenient because we enable simple lookup</p>
    <p>(of the precomputed values) based on the classifi</p>
    <p>cation result of a measured flow in order to esti</p>
    <p>mate the origin run and gap properties. In Figure 8,</p>
    <p>we have  s1</p>
    <p>, where</p>
    <p>s1 = 2k=1 s 1</p>
    <p>k , s 2 = 1k=1 s</p>
    <p>k , s 3 = 3k=1 s</p>
    <p>k give total bytes</p>
    <p>of the three bursts. We can then compute the mean burst</p>
    <p>size for this pattern. We also compute {t 1r ,t 2r ,t 3r ,...}, {t 1g ,t 2g ,t 3g ,...}, and their mean values.</p>
    <p>!&quot;!&quot;!#$!%&amp;'(# )*+,-!,+-'./012!</p>
    <p>$#</p>
    <p>!&quot;!&quot; !&quot;#&quot;</p>
    <p>!&quot;$&quot;</p>
    <p>!#!&quot; !##&quot; !#$&quot; %&quot;%&quot;%&quot;</p>
    <p>$!&quot; $#&quot; $$&quot;</p>
    <p>Figure 8: Computing generated flow statistics</p>
    <p>We are foremost interested in the accuracy of classifying</p>
    <p>a measured pattern x to its ground-truth origin flow pattern f. We compute two metrics, recall (true positive rate) and false alarm (false positive rate), to evaluate classification performance:</p>
    <p>Recall =  True positives</p>
    <p>True positives +  False negatives</p>
    <p>False alarm =  False positives</p>
    <p>False positives +  True negatives</p>
    <p>Without false alarm rate, we cannot truly assess the</p>
    <p>probability of detection for a classifier using a computed</p>
    <p>recall value because the classifier can be configured to</p>
    <p>declare positive only, automatically achieving to guess</p>
    <p>all positives correctly. Classification leads to inferring</p>
    <p>Table 2: Wi-Fi parameter configuration for Scenario 1</p>
    <p>Parameter Description Value</p>
    <p>aSlotTime Slot time 20  sec aSIF STime Short interframe space (SIFS) 10  sec aDIF STime DCF interframe space (DIFS) 50  sec aCW min Min contention window size 15 slots aCW max Max contention window size 1023 slots tPLCPPreamble PLCP preamble duration 16  sec tPLCP SIG PLCP SIGNAL field duration 4  sec tSymbol OFDM symbol duration 4  sec</p>
    <p>other important properties of a flow from its training</p>
    <p>dataset records. As our secondary evaluation metrics, we</p>
    <p>calculate errors in estimating the original mean burst size</p>
    <p>and mean gap length of the flow.</p>
    <p>infer the origin time series fA sent by source node A, using xA|B measured at receiver node B. Node C, another source, contends with node A by transmitting its own</p>
    <p>flow fC. We carry out cross-validation with all 10 flow datasets by setting fA = fi i  {1,...,10}, flow by flow at once. When fA = fi, we randomly set fC = f j  j = i. Node C can change its flow pattern from f j to fk, while node A still running fi, but fk is chosen such that k = i.</p>
    <p>Figure 9: Scenario 1</p>
    <p>Wi-Fi setup. We have implemented a custom discreteevent simulator in MATLAB, assuming the IEEE</p>
    <p>CSMA implementation is based on an open-source wire</p>
    <p>less simulator [2]. The backoff mechanism works as</p>
    <p>follows. The contention window CW is initialized to aCW min. In case of timeout, CSMA doubles CW, otherwise waits until the channel becomes idle with an ad</p>
    <p>ditional DCF interframe space (DIFS) duration. CSMA</p>
    <p>chooses a uniformly random wait time between [1, CW]. CW can grow up to aCW max of 1,023 slots. CW is decremented only when the media is sensed idle. RTS and</p>
    <p>CTS are disabled. The Wi-Fi configuration is summa</p>
    <p>rized in Table 2.</p>
    <p>Inference schemes. We have implemented all of the inference schemes in MATLAB. We consider ARMAX</p>
    <p>For mul5ple hypothesis tes5ng, false discovery rate (FDR) could be used instead of false alarm rate</p>
    <p>OMP/KSVD 3grams ED/Kmeans DTW/Kmedoids 2/3/4grams 0</p>
    <p>Recall (single layer) FDR (single layer) Recall (128bit padding) FDR (128bit padding)</p>
    <p>Figure 6: Single-layer feature learning. 1-vs-all classification recall and FDR for language identification</p>
    <p>OMP/KSVD 3grams ED/Kmeans DTW/Kmedoids 2/3/4grams 0</p>
    <p>Recall (2 layers) FDR (2 layers) Recall (128bit padding) FDR (128bit padding)</p>
    <p>Figure 7: Two-layer deep feature learning. 1-vs-all classification recall and FDR for language identification</p>
    <p>Each datapoint is a vector of 1,000 elements constituting the encrypted payload-length time series (measured in bytes), acquired from approximately 30 sec speech of one speaker.</p>
    <p>Implementation. We have implemented the proposed deep feature learning and classification system in MATLAB. We use Technions open-source OMP (v10) and K-SVD (v13) implementations [28] and LIBSVM [17]. We have written our own DTW module and K-medoids based on it. We consider the SRTP default, length-preserving AES encryption in counter mode. We will later show the impact of padding to a cipher block size on classification accuracy. We train mainly 1-vs-all SVM classifiers. For comparison to Wright et al. [31], we also train 1-vs-1 classifiers selectively.</p>
    <p>Classification accuracy metrics. To evaluate the accuracy performance of our classifiers, we compute recall (true positive rate) and either false discovery rate (FDR) or false positive rate (FPR): Recall =</p>
    <p>True positives  True positives +  False negatives , FDR =</p>
    <p>False positives  False positives +  True positives , and</p>
    <p>FPR =  False positives</p>
    <p>False positives +  True negatives . We use FDR for 1-vs-all classifiers. Because we have 21 classes for the 22 Language dataset and 24 classes (including American English accent) for FAE, the total number of negatives tends to be much larger than the number of positives when testing each 1-vs-all classifier against all samples in the test dataset. This makes FPR unfairly small for 1-vs-all, thus FDR should be preferred. We compute FPR for 1-vs-1 classifiers.</p>
    <p>Single layer analysis. We compare the performance of numerous L1 f-ext choices in a single layer configuration: 1) OMP sparse coder &amp; K-SVD (4.2); 2) 3-grams (4.2); 3) ED coder &amp; K-means clustering (5.1); 4) DTW coder &amp; K-medoids clustering (5.1); 5) simultaneous 2/3/4-grams (5.2). We do max pooling by m = 10 on the L1 f-ext output vectors before applying to linear SVM classifiers. We input each datapoint ( R1,000) in a training dataset as a stream from which xk  RN are formed as in Figure 4, using relatively short N = 64 (i.e., about 3.2 sec-long speech fragment). There is an overlap  = 0.2  N between consecutive xks. We use K = 100 (dictionary atoms or clusters) for each of 21 classes in the language identification problem, the concatenated dictionary would have 2,100 atoms. We regularize OMP, ED, and DTW coders by setting P = 50 &lt; K.</p>
    <p>n-grams are a great choice for the high-performance L1 f-ext. However, there is a crucial drawback for practical uses. We have observed that 22 Language dataset incurs 137 different voice payload sizes in the Opus VBR coding (for FAE dataset, we find 98 different payload lengths), making the unigram space size |S1| = 137. If we were to generate 2-, 3-, and 4-gram tables exhaustively, we would face |S2| = 18, 769, |S3|  2.5 million, and |S4|  352 million. So we had to reduce the 4-gram table to popular thousands, 3-grams to a few thousands, and so forth. Still, the feature vector with n-gram embedding has a huge dimensionality compared to other L1 f-ext choices.</p>
    <p>Figure 6 shows the average recall and FDR of 1-vs-all classification for language identification (with 22 Language dataset) based on the single layer feature extraction with a specified L1 f-ext over the horizontal axis. For single layer, the accuracy performance of the proposed DTW coder is very close to simultaneous 2/3/4-grams. DTW-based single layer results in a better recall, but induces more false positives by having a higher FDR. As expected, DTW performs superior over ED in clustering and matching time series data.</p>
    <p>Language identification. We have been able to improve the classification performance by adding one more layer. At layer 2, the OMP sparse coder takes in the pooled DTW-based feature vectors of layer 1. We use overlapping max pooling at layer 2. Figure 8 presents the complete confusion matrix</p>
  </div>
  <div class="page">
    <p>Feature Extrac5on and Pooling Details Do long measurement to acquire large mulVples of N packet length sequence</p>
    <p>x1 Size N</p>
    <p>x2 x3 ...</p>
    <p>y1 y2 y3 yM</p>
    <p>... ...</p>
    <p>z1</p>
    <p>Max pooling by M</p>
    <p>z1,i = max(y1,i, ..., yM,i)</p>
    <p>To next layer:</p>
    <p>xj(I+1) = zj(I)</p>
  </div>
</Presentation>
