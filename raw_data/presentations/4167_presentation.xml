<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ACL Tutorial T6: Deep Bayesian Natural Language Processing</p>
    <p>Jen-Tzung Chien</p>
    <p>National Chiao Tung University</p>
    <p>jtchien@nctu.edu.tw</p>
    <p>July 28, 2019</p>
  </div>
  <div class="page">
    <p>Table of Contents</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Speech and language</p>
    <p>Speech is the most natural way for communication</p>
    <p>vocalized-form of communication  syntactic combination of lexicals  drawn from very large vocabularies</p>
    <p>Language is the ability to acquire and use complex systems of communication</p>
    <p>natural language is a language used naturally by humans for communication</p>
  </div>
  <div class="page">
    <p>Speech recognition</p>
    <p>Frond-End Processing</p>
    <p>Input Speech</p>
    <p>Feature vectors Syllable-level</p>
    <p>match Word-level</p>
    <p>match</p>
    <p>Recognized sentence</p>
    <p>XX WW</p>
    <p>Acoustic model</p>
    <p>Language model</p>
    <p>Acoustic model training</p>
    <p>Language model</p>
    <p>construction Speech Corpora</p>
    <p>Text Corpora</p>
    <p>Frond-End Processing</p>
    <p>Bayes decision rule</p>
    <p>W = arg max W</p>
    <p>p(W |X) = arg max W</p>
    <p>p(X|W)p(W)</p>
  </div>
  <div class="page">
    <p>Document representation</p>
    <p>Document representation is developed for text analysis</p>
    <p>Topic-based text model</p>
    <p>each document is treated as a bag of words  each document can exhibit multiple topics</p>
    <p>Symbolic model is required because</p>
    <p>each topic is a multinomial variable  each document is represented by a multinomial</p>
    <p>mixture model</p>
    <p>Latent Dirichlet allocation (Blei et al., 2003) is popular to build the topic model</p>
    <p>the</p>
    <p>theover supper</p>
    <p>brown quick</p>
    <p>dog</p>
    <p>for</p>
    <p>jumped</p>
    <p>lazy hadfox cat</p>
    <p>the</p>
    <p>the</p>
    <p>dog</p>
  </div>
  <div class="page">
    <p>Machine translation</p>
    <p>Machine translation develops the algorithm to translate text or speech from one language to another</p>
    <p>linguistic rules are helpful  statistical or corpus-based approach is popular</p>
  </div>
  <div class="page">
    <p>Information retrieval</p>
    <p>Document retrieval</p>
    <p>ranking problem Document categorization</p>
    <p>classification problem</p>
    <p>Document representation or symbolic learning is a crucial issue</p>
  </div>
  <div class="page">
    <p>Document summarization</p>
    <p>Automatic summarization involves</p>
    <p>a process of reducing a text document  a computer program in order to create a summary  the most important sentences of the original documents</p>
    <p>Selection of representative sentences is performed</p>
  </div>
  <div class="page">
    <p>Reading comprehension</p>
    <p>Reading comprehension is the ability to read text, process it, and understand its meaning</p>
    <p>understanding of a text message  language skills: phonology, syntax, semantics, and pragmatics  affected by prior knowledge, ability to make inference</p>
  </div>
  <div class="page">
    <p>Information extraction</p>
    <p>Information extraction from news article</p>
    <p>(Narasimhan et al., 2016)</p>
  </div>
  <div class="page">
    <p>Question answering</p>
    <p>QA aims to answer the questions posted by humans in a natural language</p>
    <p>takes natural language question as an input rather than keywords  keyword extraction is performed to identify the question type  person or location are retrieved from who or where  candidate answers are further classified  compact and meaningful answer is translated by parsing</p>
  </div>
  <div class="page">
    <p>Dialogue generation</p>
    <p>(Li et al., 2016)</p>
  </div>
  <div class="page">
    <p>Dialogue with question clarification</p>
    <p>(Li et al., 2016)</p>
  </div>
  <div class="page">
    <p>Text understanding and reasoning</p>
    <p>Synthetic tasks in bAbI project (Weston et al., 2015) used to evaluate the learning algorithms for</p>
    <p>text understanding and reasoning  question answering problem  categorization of different kinds of questions</p>
    <p>single, two or three supporting facts  yes/no question  counting  lists/sets  simple negation  indefinite knowledge</p>
    <p>Childrens book test (Hill et al., 2016)</p>
    <p>measure how well a text model can exploit wider linguistic context  in each question, the first 20 sentences form the context, and a word is</p>
    <p>removed from the 21st sentence, which becomes the query</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Probabilistic model</p>
    <p>Most likely words from top topics Topic proportions sequence</p>
    <p>region</p>
    <p>pcr</p>
    <p>identified</p>
    <p>fragments</p>
    <p>two</p>
    <p>genes</p>
    <p>three</p>
    <p>cdna</p>
    <p>analysis</p>
    <p>measured</p>
    <p>average</p>
    <p>range</p>
    <p>values</p>
    <p>different</p>
    <p>size</p>
    <p>three</p>
    <p>calculated</p>
    <p>two</p>
    <p>low</p>
    <p>residues</p>
    <p>binding</p>
    <p>domains</p>
    <p>helix</p>
    <p>cys</p>
    <p>regions</p>
    <p>structure</p>
    <p>terminus</p>
    <p>terminal</p>
    <p>site</p>
    <p>computer</p>
    <p>methods</p>
    <p>number</p>
    <p>two</p>
    <p>principle</p>
    <p>design</p>
    <p>access</p>
    <p>processing</p>
    <p>advantage</p>
    <p>important</p>
    <p>Abstract with the most likely topic assignments Statistical approaches help in the determination of significant configurations in protein and</p>
    <p>nucleic acid sequence data. Three recent statistical methods are discussed: (i) score</p>
    <p>based sequence analysis that provides a means for characterizing anomalies in local</p>
    <p>sequence text and for evaluating sequence comparisons; (ii) quantile distributions of amino</p>
    <p>acid usage that reveal general compositional biases in proteins and evolutionary relations;</p>
    <p>and (iii) r-scan statistics that can be applied to the analysis of spacing of sequence markers.</p>
    <p>p(word) =</p>
    <p>topic p(word | topic)p(topic)</p>
  </div>
  <div class="page">
    <p>Neural network</p>
    <p>Deep structured/hierarchical learning</p>
    <p>Multiple layers of nonlinear processing units</p>
    <p>High-level abstraction is learned</p>
    <p>Run</p>
    <p>Jump</p>
  </div>
  <div class="page">
    <p>Probabilistic Model + Neural Network</p>
  </div>
  <div class="page">
    <p>Modern machine learning</p>
    <p>Probabilistic Models Neural Nets</p>
    <p>Structure Top-down Bottom-up Representation Intuitive Distributed Interpretation Easy Harder</p>
    <p>Semi/unsupervised Easier Harder Incorp. domain knowl. Easy Hard Incorp. constraint Easy Hard Incorp. uncertainty Easy Hard</p>
    <p>Learning Many algorithms Back-propagation Inference/decode Harder Easier Evaluation on int. quantity End performance</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Seq2Seq learning: encoder-decoder network</p>
    <p>Traditional DNN was sensibly encoded with vectors with a fixed dimensionality</p>
    <p>Many important problems are best expressed with sequences whose lengths are unknown a priori</p>
    <p>An input sequence ABC is encoded and decoded to produce WXYZ as the output sequence (Sutskever et al., 2014)</p>
    <p>LSTM architecture is applied to deal with this problem</p>
  </div>
  <div class="page">
    <p>Sequence learning</p>
    <p>RNN can not deal with sequential learning with input and output sequences in different lengths</p>
    <p>Sequence to sequence learning is performed by</p>
    <p>first, map the input sequence to a fixed-sized vector using on RNN  second, map the vector to the target sequence using another RNN</p>
    <p>LSTM is used to estimate p(y1, . . . ,yT |x1, . . . ,xT ) where {x1, . . . ,xT} is an input sequence and {y1, . . . ,yT } is its output sequence whose length T  may differ from T</p>
    <p>LSTM language model is calculated by</p>
    <p>p(y1, . . . ,yT |x1, . . . ,xT ) = T  t=1</p>
    <p>p(yt|v,y1, . . . ,yt1)</p>
    <p>LSTM computes this probability by obtaining the fixed dimensional v of {x1, . . . ,xT} given by the last hidden state of LSTM</p>
  </div>
  <div class="page">
    <p>Learning via LSTM</p>
    <p>Each sentence ends with a symbol &lt;EOS&gt;, which enables the model to define a distribution over sequences of all possible lengths</p>
    <p>Two LSTMs are used (Sutskever et al., 2014)</p>
    <p>one for the input sequence and another for the output sequence  number of parameters is increased  computational cost is negligible  natural to train LSTM on multiple language pairs simultaneously</p>
    <p>Deep LSTM outperformed shallow LSTM. Four-layer LSTM was chosen</p>
    <p>Reverse the order of the words of an input sentence</p>
  </div>
  <div class="page">
    <p>Listen, attend and spell</p>
    <p>Traditional acoustic, pronunciation and language models were trained separately based on different objectives</p>
    <p>This disjoint training issue was tackled by designing models that are trained end-to-end from speech signals directly to word transcripts</p>
    <p>connectionist temporal classification  sequence to sequence model with attention</p>
    <p>Listen, attend and spell are introduced (Chan et al., 2015)</p>
    <p>Encoder is a listener while decoder is a speller</p>
    <p>Bidirectional LSTM is used in encoder and decoder</p>
    <p>Attention model is used to extract the relevant information from a small number of time steps</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Machine translation</p>
    <p>Sequence to sequence translation model (Sutskever et al., 2014)</p>
    <p>compresses all the information into a fixed length vector s0  degrades as the length of input sentence increases</p>
    <p>LSTM LSTM LSTM</p>
    <p>LSTM LSTM LSTM</p>
    <p>LSTM: encoder</p>
    <p>Howareyou</p>
    <p>s0</p>
    <p>Comment allez vous</p>
    <p>&lt; start &gt; Comment allez</p>
    <p>LSTM</p>
    <p>vous</p>
    <p>&lt; end &gt;</p>
    <p>LSTM: decoder</p>
  </div>
  <div class="page">
    <p>Image caption</p>
    <p>It is challenging to describe the content of an image which  captures the objects in an image  expresses the relations between objects</p>
    <p>An end-to-end system (Vinyals et al., 2015) is built with  CNN encoder  LSTM decoder</p>
    <p>LSTM</p>
    <p>h0</p>
    <p>LSTM LSTM LSTM LSTM LSTM</p>
  </div>
  <div class="page">
    <p>Machine translation with attention</p>
    <p>Attention mechanism was merged in a sequence to sequence model (Bahdanau et al., 2015)</p>
    <p>alignment model  translation model</p>
    <p>ci =</p>
    <p>Tx j=1</p>
    <p>ijhj</p>
    <p>Compute attention weights</p>
    <p>ij = expeijTx k=1 expeik</p>
    <p>where eij = Score(si1,hj) LSTM: encoder</p>
    <p>Howareyou</p>
    <p>Comment allez vous</p>
    <p>&lt; start &gt; Comment allez vous</p>
    <p>&lt; end &gt;</p>
    <p>LSTM: decoder</p>
    <p>t;1 t;2 t;3</p>
    <p>c1 c2 c3 c4</p>
    <p>h1 h2 h3</p>
    <p>s1 s2 s3 s4</p>
  </div>
  <div class="page">
    <p>Image caption with attention</p>
    <p>a1 : : :a2 a3 aL</p>
    <p>Attention</p>
    <p>LSTM</p>
    <p>Attention</p>
    <p>LSTM</p>
    <p>Attention</p>
    <p>LSTM</p>
    <p>Attention</p>
    <p>LSTM</p>
    <p>:::</p>
    <p>:::</p>
  </div>
  <div class="page">
    <p>Results on MS COCO dataset</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Convolutional neural network</p>
    <p>Two-dimensional CNN (Krizhevsky et al., 2012)</p>
    <p>Input Output</p>
    <p>Convolution Max-pooling</p>
    <p>yX Z f1g</p>
    <p>Z f1g</p>
    <p>Convolution</p>
    <p>Z f2g</p>
    <p>U V</p>
  </div>
  <div class="page">
    <p>Convolutional LSTM</p>
    <p>Spatiotemporal correlation is captured for weather forecasting (Xingjian et al., 2015)</p>
    <p>it = (Wxi Xt + Whi Ht1 + Wci Ct1 + bi) ft = (Wxf Xt + Whf Ht1 + Wcf Ct1 + bf) Ct = ft Ct1 + it  tanh(Whc Xt + Whc Ht1 + bc) ot = (Wxo Xt + Who Ht1 + Wco Ct + bo) Ht = ot  tanh(Ct)</p>
    <p>where  is the convolution operation and  is the Hadamard product</p>
  </div>
  <div class="page">
    <p>Character CNN for text classification</p>
    <p>Character-based convolutional neural network achieved better text classification than</p>
    <p>word-based convolutional neural network  recurrent neural network</p>
    <p>(Zhang et al., 2015)</p>
  </div>
  <div class="page">
    <p>Convolutional sequence to sequence learning</p>
    <p>Advantages of using convolutional neural network for sequence modeling</p>
    <p>independence on the computations of the previous time step  computational parallelization  hierarchical representation over the input sequence  shorter path to capture long-range dependencies</p>
    <p>CNN - O( n k ) with a kernel of width k</p>
    <p>RNN - O(n) for linear time</p>
    <p>An entirely convolutional sequence to sequence model (Gehring et al., 2017) was proposed for machine translation</p>
    <p>GLU (Gated Linear Unit): a simplified gating mechanism that reduces the gradient vanishing problem</p>
    <p>residual connections  attention mechanism</p>
  </div>
  <div class="page">
    <p>vj</p>
    <p>ai ci</p>
    <p>zj</p>
    <p>CNNa</p>
    <p>CNNc</p>
  </div>
  <div class="page">
    <p>Convolutional encoder</p>
    <p>Encoder consists of two stacked convolutional networks</p>
    <p>CNNa produces the key vector zj</p>
    <p>zj = CNNa(ej)</p>
    <p>CNNc produces the value vector vj</p>
    <p>vj = CNNc(ej)</p>
    <p>Conditional input ci to the decoder is obtained by</p>
    <p>ai = Attention(zj,si)</p>
    <p>ci =</p>
    <p>T j=1</p>
    <p>aijvj</p>
  </div>
  <div class="page">
    <p>Convolutional encoder using gated CNN</p>
    <p>Gated linear unit (Dauphin et al., 2017) is calculated via convolution operation  for hidden layers h0, . . . ,hL as</p>
    <p>hl(E) = (EW + b)(EV + c)</p>
    <p>LSTM style with no forget and input gates required  only possess output gate in which information to be propagated</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Dilated convolutional neural network - WaveNet</p>
    <p>Dilated CNN (Van Den Oord et al., 2016) was proposed to generate a raw audio waveform</p>
    <p>probabilistic and autoregressive  dilated causal convolution  conditioned on speaker identity to generate different voices  generic and flexible framework</p>
    <p>Waveform x = {x1,    ,xT} is factorised as a product of conditional probabilities</p>
    <p>p(x) =</p>
    <p>T t=1</p>
    <p>p(xt|x1,    ,xt1)</p>
    <p>stack of convolutional layers  no pooling layers  optimize to maximize the log-likelihood</p>
  </div>
  <div class="page">
    <p>Causal convolution</p>
    <p>cannot depend on any of the future time steps  shifting the output of a normal convolution by a few time steps  CNN is faster than RNN</p>
  </div>
  <div class="page">
    <p>Dilated convolution</p>
    <p>filter is applied over an area larger than its length by skipping input values with a certain step</p>
    <p>similar to pooling or strided convolutions, but the output has the same size as the input</p>
    <p>dilation 1 yields the standard convolution  receptive field to grow exponentially with depth</p>
  </div>
  <div class="page">
    <p>Dilated recurrent neural network</p>
    <p>Challenges when learning on long sequences with RNNs</p>
    <p>complex dependencies  vanishing and exploding gradients  efficient parallelization</p>
    <p>Multi-resolution with dilated recurrent skip connections (Chang et al., 2017)</p>
    <p>neural connection architecture analogous to the dilated CNN  single-layer dilated RNN</p>
    <p>W</p>
    <p>x0 x1 x2 x3 x4 x5 x6 x7</p>
  </div>
  <div class="page">
    <p>Dilated recurrent skip connection</p>
    <p>Denote h (l) t as the cell in layer l and time t. Dilated recurrent skip</p>
    <p>connection is represented as</p>
    <p>h (l) t = f(x</p>
    <p>(l) t ,h</p>
    <p>(l)</p>
    <p>td(l))</p>
    <p>d(l) is the skip length or dilation of layer l  x(l)t is the input to layer l at time t  f() denotes any output operation for a RNN cell</p>
    <p>Recurrent chains can be computed in parallel</p>
    <p>Degree of parallelization is increased by d(l)</p>
    <p>x1</p>
    <p>x2</p>
    <p>x0</p>
    <p>x3</p>
    <p>x5</p>
    <p>x6</p>
    <p>x4</p>
    <p>x7</p>
    <p>W</p>
  </div>
  <div class="page">
    <p>Multilayer dilated recurrent neural network</p>
    <p>Dilated RNN is constructed by stacking dilated recurrent layers</p>
    <p>dilation increases exponentially across layers  dilated RNN with L = 3 and M = 2</p>
    <p>d(l) = Ml1, l = 1,    ,L</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Neural Turing machine versus memory network</p>
    <p>Most machine learning models lack an easy way to</p>
    <p>read and write to part of a long-term memory component  combine this seamlessly with inference</p>
    <p>Neural Turing machine (Graves et al., 2014)</p>
    <p>learns to read from and write to memory cells without explicit supervision</p>
    <p>allows end-to-end training via content-based soft attention  emulates algorithmic mechanism in a way that allows gradient-based</p>
    <p>optimization</p>
    <p>Memory network (Weston et al., 2015)</p>
    <p>includes memory cells that can be accessed via an addressing mechanism</p>
    <p>combines learning strategies for inference with a memory component that can be read and written to</p>
  </div>
  <div class="page">
    <p>Neural Turing machine (Graves et al., 2014)</p>
    <p>intelligence requires knowledge</p>
    <p>acquiring knowledge can be done via large-scale deep learning</p>
    <p>neural networks excel at storing implicit knowledge, but struggle to memorize facts</p>
    <p>neural networks lack the working memory system that allows human beings to explicitly hold and manipulate pieces of information</p>
  </div>
  <div class="page">
    <p>Memory Key: k Content Addressing Parameter:</p>
    <p>Interpolation Parameter: g Convolutaional Shift Parameter: s Sharpening Parameter:</p>
  </div>
  <div class="page">
    <p>Reading</p>
    <p>Mt is the N M memory matrix at time t where N is the number of memory locations, and M is the vector size at each location</p>
    <p>wt = {wt(i)} is a weight vector over N locations emitted by a read head at time t, and</p>
    <p>i wt(i) = 1, 0  wt(i)  1</p>
    <p>read vector rt of length M, returned by the head, is defined as a rt</p>
    <p>i wt(i)Mt(i)</p>
  </div>
  <div class="page">
    <p>Writing step 1  Erasing Mt(i)  Mt1(i)[1wt(i)et]</p>
    <p>Writing step 2  Adding Mt(i)  Mt(i) + wt(i)at</p>
  </div>
  <div class="page">
    <p>Addressing mechanism</p>
    <p>Content Addressing</p>
    <p>Head Location: wt1</p>
    <p>Memory: M</p>
    <p>Interpolation</p>
    <p>Convolution al Shift</p>
    <p>Sharpening</p>
    <p>Head Location: wt</p>
    <p>Previous State Controller Outputs</p>
    <p>Shift Weighting: s</p>
    <p>= 100</p>
    <p>g = 0:5</p>
    <p>= 5</p>
  </div>
  <div class="page">
    <p>Step 1: content addressing</p>
    <p>wct(i)  exp</p>
    <p>( tK[kt,Mt(i)]</p>
    <p>)</p>
    <p>j exp</p>
    <p>( tK[kt,Mt(j)]</p>
    <p>) where K[u,v] = u v ||u||  ||v||</p>
  </div>
  <div class="page">
    <p>Step 2: interpolation</p>
    <p>facilitate both simple iteration across the locations of the memory and random-access jumps</p>
    <p>prior to rotation, each head emits a scalar interpolation gate gt</p>
    <p>w g t  gtw</p>
    <p>c t + (1gt)wt1</p>
  </div>
  <div class="page">
    <p>Step 3: convolutional shift</p>
    <p>each head emits a shift weighting st that defines a normalised distribution over the allowed integer shifts</p>
    <p>memory locations from 0 to N 1  rotation is performed via the circular convolution</p>
    <p>wt(i)  N1</p>
    <p>j=0 w g t (j)st(i j)</p>
  </div>
  <div class="page">
    <p>Step 4: sharpening</p>
    <p>rotation will transform a weighting focused at a single point into one slightly blurred over three points</p>
    <p>each head accordingly emits one further scalar t to sharpen weight</p>
    <p>wt(i)  wt(i)</p>
    <p>t j wt(j)</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>End-to-end memory network (Sukhbaatar et al., 2015)</p>
    <p>memory network (Weston et al., 2015) was not easy to train via error backpropagation</p>
    <p>continuous form of memory network  it can be trained end-to-end from input-output pairs  supportive attention was introduced (Chien and Lin, 2018)</p>
    <p>Q: Where is the apple?</p>
    <p>Embedding: B</p>
  </div>
  <div class="page">
    <p>End-to-end memory network (Sukhbaatar et al., 2015)</p>
    <p>memory network (Weston et al., 2015) was not easy to train via error backpropagation</p>
    <p>continuous form of memory network  it can be trained end-to-end from input-output pairs  supportive attention was introduced (Chien and Lin, 2018)</p>
    <p>-1 -2 3 4 2</p>
    <p>-2 5 1 2 3</p>
    <p>-2 1 -3 2 1</p>
    <p>Embedding: CEmbedding: A</p>
    <p>Sam walks into the kitchen. Sam picks up an apple.</p>
    <p>Sam walks into the bedroom. Sam drops the apple.</p>
  </div>
  <div class="page">
    <p>Embedding C</p>
    <p>Sam walks into the kitchen. Sam picks up an apple.</p>
    <p>Sam walks into the bedroom. Sam drops the apple.</p>
    <p>Q: Where is the apple?</p>
    <p>Embedding B</p>
    <p>Embedding A</p>
    <p>-1 -2 3 4 2</p>
    <p>-2 5 1 2 3</p>
    <p>Softmax 1.5 2.5 1.5 1.3 1</p>
    <p>Softmax</p>
    <p>-2 1 -3 2 1</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Auto-encoder</p>
    <p>::: :::</p>
    <p>::: :::</p>
    <p>::: :::</p>
    <p>xx xx</p>
    <p>zz</p>
    <p>Encoder Decoder</p>
  </div>
  <div class="page">
    <p>Variational auto-encoder</p>
    <p>::: :::</p>
    <p>::: :::</p>
    <p>::: :::</p>
    <p>Encoder Decoder ::: :::</p>
    <p>q(zjx)q(zjx)</p>
    <p>p(xjz)p(xjz)xx</p>
    <p>zz</p>
    <p>xx</p>
    <p>Sampling</p>
  </div>
  <div class="page">
    <p>Variational auto-encoder</p>
    <p>zz</p>
    <p>x</p>
    <p>Recognition</p>
    <p>model Generative</p>
    <p>model</p>
    <p>q(zjx)q(zjx) p(xjz)p(xjz)</p>
    <p>(Kingma and Welling, 2014)</p>
    <p>Mean-field approach requires analytical solution to maximum likelihood problem, which is intractable in case of neural network</p>
    <p>Use neural network to sample the latent variables z from variational posterior</p>
    <p>VAE was a building block for speaker recognition (Chien and Hsu, 2017)</p>
  </div>
  <div class="page">
    <p>Stochastic gradient variational Bayes</p>
    <p>L = Eq(zjx)[f(x; z)]L = Eq(zjx)[f(x; z)]</p>
    <p>L ' f(xjz(l))L ' f(xjz(l))</p>
    <p>rL ' rf(x; z(l))rL ' rf(x; z(l))</p>
    <p>Objective: Gradient:</p>
    <p>Step1</p>
    <p>Step2</p>
    <p>Step3</p>
    <p>z(l) = z + z  (l)z(l) = z + z  (l)</p>
    <p>sample (l) from N(0; I)sample (l) from N(0; I)</p>
    <p>Step4</p>
    <p>Reduce the variance caused by directly sampling z (Rezende et al., 2014)</p>
  </div>
  <div class="page">
    <p>Neural variational document model</p>
    <p>Continuous semantic latent variable model for a document X (Miao et al., 2016)</p>
    <p>X</p>
    <p>z</p>
    <p>Inference Networkq(zjX)</p>
    <p>p(Xjz)</p>
    <p>X</p>
    <p>Softmax Generative</p>
    <p>Network</p>
  </div>
  <div class="page">
    <p>Neural answer selection model</p>
    <p>LSTM LSTMLSTM LSTM</p>
    <p>LSTM LSTM LSTM LSTM</p>
  </div>
  <div class="page">
    <p>Generating sentences from a continuous space</p>
    <p>Variational recurrent auto-encoder (VRAE) (?) is  composed of two RNNs for both encoder and decoder  developed for unsupervised learning for time series data  constructed to map data into latent representation</p>
    <p>Parameters of variational distribution over latent variable z are function of the last state of RNN hT</p>
    <p>q(z|X) = N(z, diag( 2 z)), where</p>
    <p>[ z,</p>
    <p>] = f</p>
    <p>(q)  (hT )</p>
    <p>Initial state of RNN decoder is computed by a sample z</p>
    <p>h0 = f (i)  (z)</p>
    <p>ht+1 = f dec  (ht,xt)</p>
    <p>xt = f (o)  (ht)</p>
  </div>
  <div class="page">
    <p>Variational recurrent auto-encoder</p>
    <p>z</p>
    <p>x0 x1 x2</p>
    <p>x2</p>
    <p>x1</p>
    <p>x1 x3</p>
    <p>x0</p>
    <p>RNNs work workRNNs&lt; BOS &gt;</p>
    <p>workRNNs &lt; EOS &gt;</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Unsupervised variational recurrent neural network</p>
    <p>VAE and RNN are combined by</p>
    <p>incorporating the hidden state ht at time step t into VAE</p>
    <p>Stochastic or variational recurrent neural network was constructed for unsupervised learning (Chung et al., 2015)</p>
    <p>Hidden state is expressed for</p>
    <p>RNN ht = Fw(xt,ht1)</p>
    <p>variational RNN (VRNN)</p>
    <p>ht = F(xt,z  t,ht1)</p>
    <p>Apply stochastic gradient variational Bayes for optimization</p>
    <p>Characterize the variability by using high-level latent random variable zt</p>
  </div>
  <div class="page">
    <p>Graphical representation: unsupervised VRNN</p>
    <p>xt1</p>
    <p>xt</p>
    <p>xt</p>
    <p>xt+1</p>
    <p>ht1 ht</p>
    <p>zt</p>
  </div>
  <div class="page">
    <p>Feature extractor Prior network</p>
    <p>Feature extractor</p>
    <p>Generative network</p>
    <p>Inference network</p>
    <p>xtxt</p>
    <p>x0tx 0 t</p>
    <p>ht1ht1</p>
    <p>p!(ztjht1)p!(ztjht1)</p>
    <p>zt  q(ztjxt; ht1)zt  q(ztjxt; ht1)</p>
    <p>z0tz 0 t</p>
    <p>htht</p>
    <p>x0t; ht1x 0 t; ht1</p>
    <p>ht1ht1</p>
    <p>xt  p(xtjzt; ht1)xt  p(xtjzt; ht1)</p>
  </div>
  <div class="page">
    <p>Supervised VRNN was proposed for speech separation (Chien and Kuo, 2017) and speech recognition (Chien and Shen, 2017)</p>
    <p>target variable yt is introduced for supervised learning</p>
    <p>xt1</p>
    <p>yt1</p>
    <p>xt</p>
    <p>yt</p>
    <p>ht1 ht</p>
    <p>zt</p>
  </div>
  <div class="page">
    <p>Feature extractor Prior network</p>
    <p>Feature extractor</p>
    <p>Discriminative network</p>
    <p>Inference network</p>
    <p>xtxt</p>
    <p>x0tx 0 t</p>
    <p>ht1ht1</p>
    <p>p!(ztjxt; ht1)p!(ztjxt; ht1)</p>
    <p>zt  q(ztjxt; yt; ht1)zt  q(ztjxt; yt; ht1)</p>
    <p>z0tz 0 t</p>
    <p>htht</p>
    <p>x0t; ht1x 0 t; ht1</p>
    <p>yt  p(ytjht)yt  p(ytjht)</p>
    <p>Feature extractor</p>
    <p>ytyt</p>
    <p>y0ty 0 t</p>
    <p>x0tx 0 t</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Planning long-term future</p>
    <p>RNN is usually trained with teacher forcing where</p>
    <p>model is optimized to predict one-step ahead  local correlation dominates the long-term dependency  generated samples tend to exhibit local coherence but lack meaningful</p>
    <p>global structure</p>
    <p>Regularizing the recurrent neural network based on future information (Serdyuk et al., 2018)</p>
    <p>run twin forward and backward RNNs with no parameter sharing  encourage hidden state of forward RNN to be close to that of backward</p>
    <p>RNN</p>
    <p>allow forward RNN to catch past and future features that are useful in test time</p>
  </div>
  <div class="page">
    <p>Twin network</p>
    <p>Forward RNN  h t =</p>
    <p>f (xt1,</p>
    <p>h t1)</p>
    <p>prediction of xt using past information pf (xt|x&lt;t) =  (  h t)</p>
    <p>Backward RNN  h t =</p>
    <p>f (xt+1,</p>
    <p>h t+1)</p>
    <p>prediction of xt using future information pb(xt|x&gt;t) =  (  h t)</p>
    <p>h t and</p>
    <p>h t contain past and future features for predicting xt,</p>
    <p>respectively</p>
  </div>
  <div class="page">
    <p>Graphical representation</p>
    <p>xt1 xt xt+1</p>
    <p>xt+1xtxt1</p>
    <p>! h t1</p>
    <p>! h t</p>
    <p>! h t+1</p>
    <p>h t1</p>
    <p>h t</p>
    <p>h t+1</p>
    <p>Input</p>
    <p>Forward</p>
    <p>layer</p>
    <p>Backward</p>
    <p>layer</p>
    <p>Output</p>
    <p>Lt1 Lt Lt+1</p>
  </div>
  <div class="page">
    <p>Learning objective</p>
    <p>Penalizing the distance between forward and backward hidden states leading to the same prediction</p>
    <p>Lt = g(  h t)</p>
    <p>h t</p>
    <p>function g() is a parameterized affine transformation  affine transformation gives flexibility for equivalence between</p>
    <p>h t and</p>
    <p>h t</p>
    <p>Training criterion</p>
    <p>F() =  t</p>
    <p>{log pf(xt|x&lt;t) + log pb(xt|x&gt;t)Lt}</p>
    <p>backward network is discarded during inference</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Markov recurrent neural network</p>
    <p>A large-scale RNN is hard to train and prone to be overfitting</p>
    <p>A single path of hidden states ht is insufficient to capture temporal dependencies</p>
    <p>Deterministic hidden state ht in RNN disregards the essence of stochastic process in sequential data</p>
    <p>Markov recurrent neural network (Kuo and Chien, 2018)</p>
    <p>introduces the Markov property to build hidden state of RNN  incorporates the discrete latent variable into RNN  constructs the continuous hidden representation diversely  expresses the highly structured sequential data</p>
  </div>
  <div class="page">
    <p>Graphical representation</p>
    <p>xt1</p>
    <p>ht1;K</p>
    <p>ht1;1 ht;1</p>
    <p>ht1;k zt1</p>
    <p>xt</p>
    <p>ht;K</p>
    <p>htk zt</p>
    <p>ht+1;1</p>
    <p>xt+1</p>
    <p>ht+1;K</p>
    <p>ht+1;k</p>
  </div>
  <div class="page">
    <p>Markov recurrent neural network</p>
    <p>MRNN is developed to combine recurrent neural networks with probabilistic interpretation</p>
    <p>introduces a Markov chain in latent representation  constructs multiple hidden state representation  conducts the stochastic state-to-state transitions</p>
    <p>Hidden state ht is selected from {htk}Kk=1 according to zt</p>
    <p>ht = S&gt;t zt</p>
    <p>Transition of a stochastic state zt complies with the property of Markov chain</p>
    <p>p(zt|z1:t1,x1:t) = p(zt|zt1,xt)</p>
  </div>
  <div class="page">
    <p>State space</p>
    <p>St  RKd at each time t consists of all deterministic states {ht1, . . . ,htK} as basis vectors given by</p>
    <p>St ,</p>
    <p>h&gt;t1 h&gt;t2</p>
    <p>... h&gt;tK</p>
    <p>=</p>
    <p>LSTM(ht1,xt,1) LSTM(ht1,xt,2)</p>
    <p>... LSTM(ht1,xt,K)</p>
    <p>State encoder</p>
    <p>each LSTM encoder k is calculated by</p>
    <p>itk = (Wik[ht1; xt] + bik)</p>
    <p>ftk = (Wfk[ht1; xt] + bfk)</p>
    <p>utk = tanh(Wuk[ht1; xt] + bgk)</p>
    <p>ctk = ftk ct1 + itk utk otk = (Wok[ht1; xt] + bok)</p>
    <p>htk = otk  tanh(ctk)</p>
  </div>
  <div class="page">
    <p>System implementation</p>
    <p>+ + ++</p>
    <p>tanh</p>
    <p>i t ; 1f t ; 1</p>
    <p>c t</p>
    <p>ot;1</p>
    <p>tanh</p>
    <p>+ + ++</p>
    <p>ut;1</p>
    <p>tanh</p>
    <p>ht1 xt</p>
    <p>St</p>
    <p>zt</p>
    <p>ht;1</p>
    <p>ht;K</p>
    <p>...</p>
    <p>ht;2</p>
    <p>Logit encoder</p>
    <p>log t</p>
    <p>Gumbel-softmax</p>
    <p>ht</p>
    <p>ht;1</p>
    <p>ht;K</p>
    <p>... ht;2</p>
    <p>i t ; 1f t ; 1</p>
    <p>c t</p>
    <p>ot;1</p>
    <p>tanh</p>
    <p>ut;1</p>
    <p>it;1ft;1</p>
    <p>ct</p>
    <p>ot;1</p>
    <p>tanh</p>
    <p>+ + ++</p>
    <p>ut;1</p>
    <p>tanh</p>
  </div>
  <div class="page">
    <p>Learning objective</p>
    <p>Parameters of state encoder and logit encoder {,} are jointly trained by maximizing the likelihood of D = {xt,yt}Tt=1</p>
    <p>p(y1:T |x1:T ) = T t=1</p>
    <p>Ep(z1:t|x1:t)</p>
    <p>[ p(yt|x1:t,z1:t)p(z1:t|x1:t)</p>
    <p>]</p>
    <p>Monte Carlo method for log likelihood is calculated by</p>
    <p>T t=1</p>
    <p>Ep(z1:t|x1:t)</p>
    <p>[ log p(yt|x1:t,z1:t)</p>
    <p>]</p>
    <p>T t=1</p>
    <p>( 1</p>
    <p>L</p>
    <p>L l=1</p>
    <p>log p(yt|x1:t,z (l) 1:t)p(z</p>
    <p>(l) 1:t|x1:t)</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>[1] D. Bahdanau, K. Cho, and Y. Bengio, Neural machine translation by jointly learning to align and translate, in Proc. of International Conference on Learning Representation, 2015.</p>
    <p>[2] D. M. Blei, A. Y. Ng, and M. I. Jordan, Latent Dirichlet allocation, Journal of Machine Learning Research, vol. 3, pp. 9931022, 2003.</p>
    <p>[3] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, Listen, attend and spell, arXiv preprint arXiv:1508.01211, 2015.</p>
    <p>[4] S. Chang, Y. Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui, M. Witbrock, M. A. Hasegawa-Johnson, and T. S. Huang, Dilated recurrent neural networks, in Advances in Neural Information Processing Systems 30, 2017, pp. 7787.</p>
    <p>[5] J.-T. Chien and C.-W. Hsu, Variational manifold learning for speaker recognition, in Proc. of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.</p>
    <p>[6] J.-T. Chien and K.-T. Kuo, Variational recurrent neural networks for speech separation, in Proc. Annual Conference of International Speech Communication Association, 2017, pp. 11931197.</p>
    <p>[7] J.-T. Chien and T.-A. Lin, Supportive attention in end-to-end memory networks, in Proc. of IEEE International Workshop on Machine Learning for Signal Processing, 2018, pp. 16.</p>
    <p>[8] J.-T. Chien and C. Shen, Stochastic recurrent neural network for speech recognition, in Proc. of Annual Conference of International Speech Communication Association, 2017, pp. 13131317.</p>
  </div>
  <div class="page">
    <p>References II</p>
    <p>[9] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio, A recurrent latent variable model for sequential data, in Advances in neural information processing systems, 2015, pp. 29802988.</p>
    <p>[10] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, Language modeling with gated convolutional networks, in Proc. of International Conference on Machine Learning, 2017, pp. 933941.</p>
    <p>[11] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, Convolutional sequence to sequence learning, in Proc. of International Conference on Machine Learning, 2017, pp. 12431252.</p>
    <p>[12] A. Graves, G. Wayne, and I. Danihelka, Neural Turing machines, arXiv preprint arXiv:1410.5401, 2014.</p>
    <p>[13] F. Hill, A. Bordes, S. Chopra, and J. Weston, The Goldilocks principle: Reading childrens books with explicit memory representations, in Proc. of International Conference on Learning Representation, 2016.</p>
    <p>[14] D. P. Kingma and M. Welling, Auto-encoding variational Bayes, arXiv preprint arXiv:1312.6114, 2014.</p>
    <p>[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet classification with deep convolutional neural networks, in Advances in Neural Information Processing Systems, 2012, vol. 25, pp. 10971105.</p>
  </div>
  <div class="page">
    <p>References III</p>
    <p>[16] C.-Y. Kuo and J.-T. Chien, Markov recurrent neural networks, in Proc. of IEEE International Workshop on Machine Learning for Signal Processing, 2018, pp. 16.</p>
    <p>[17] J. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. Weston, Learning through dialogue interactions, arXiv preprint arXiv:1612.04936, 2016.</p>
    <p>[18] J. Li, W. Monroe, A. Ritter, and D. Jurafsky, Deep reinforcement learning for dialogue generation, in Proc. of Conference on Empirical Methods in Natural Language Processing, 2016, pp. 11921202.</p>
    <p>[19] Y. Miao, L. Yu, and P. Blunsom, Neural variational inference for text processing, in Proc. of International Conference on Machine Learning, 2016, pp. 17271736.</p>
    <p>[20] K. Narasimhan, A. Yala, and R. Barzilay, Improving information extraction by acquiring external evidence with reinforcement learning, in Proc. of Conference on Empirical Methods in Natural Language Processing, 2016, pp. 23552365.</p>
    <p>[21] D. J. Rezende, S. Mohamed, and D. Wierstra, Stochastic backpropagation and approximate inference in deep generative models, in Proc. of International Conference on Machine Learning, 2014, pp. 12781286.</p>
    <p>[22] D. Serdyuk, R. N. Ke, A. Sordoni, C. Pal, and Y. Bengio, Twin networks: Using the future as a regularizer, in Proc. of International Conference on Learning Representations, 2018.</p>
    <p>[23] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, End-to-end memory networks, in Advances in Neural Information Processing Systems 28, 2015, pp. 24402448.</p>
  </div>
  <div class="page">
    <p>References IV</p>
    <p>[24] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with neural networks, in Advances in Neural Information Processing Systems 27. ., 2014, pp. 31043112.</p>
    <p>[25] A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, Wavenet: A generative model for raw audio, arXiv preprint arXiv:1609.03499, 2016.</p>
    <p>[26] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: A neural image caption generator, in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 31563164.</p>
    <p>[27] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van Merrienboer, A. Joulin, and T. Mikolov, Towards AI-complete question answering: A set of prerequisite toy tasks, in Proc. of International Conference on Learning Representation, 2015.</p>
    <p>[28] J. Weston, S. Chopra, and A. Bordes, Memory networks, in Proc. of International Conference on Learning Representation, 2015.</p>
    <p>[29] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-C. Woo, Convolutional LSTM network: A machine learning approach for precipitation nowcasting, in Advances in Neural Information Processing Systems, 2015, pp. 802810.</p>
    <p>[30] X. Zhang, J. Zhao, and Y. LeCun, Character-level convolutional networks for text classification, in Advances in Neural Information Processing Systems 28, 2015, pp. 649657.</p>
  </div>
</Presentation>
