<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cooperative Learning of Disjoint Syntax and Semantics</p>
    <p>Serhii Havrylov</p>
    <p>Germn Kruszewski Armand Joulin</p>
  </div>
  <div class="page">
    <p>Is using linguistic structures for sentence modelling useful?</p>
    <p>(e.g. syntactic trees)</p>
  </div>
  <div class="page">
    <p>Is using linguistic structures for sentence modelling useful?</p>
    <p>(e.g. syntactic trees) Yes, it is! Lets create</p>
    <p>more treebanks!</p>
  </div>
  <div class="page">
    <p>Is using linguistic structures for sentence modelling useful?</p>
    <p>(e.g. syntactic trees) Yes, it is! Lets create</p>
    <p>more treebanks!</p>
    <p>No! Annotations are expensive to make. Parse trees is just a linguists social construct.</p>
    <p>Just stack more layers and you will be fine!</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
    <p>neutral</p>
  </div>
  <div class="page">
    <p>Recursive neural network</p>
    <p>neutral</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
    <p>RL-SPINN: Yogatama et al., 2016  Soft-CYK: Maillard et al., 2017  Gumbel Tree-LSTM: Choi et al., 2018</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
    <p>RL-SPINN: Yogatama et al., 2016  Soft-CYK: Maillard et al., 2017  Gumbel Tree-LSTM: Choi et al., 2018</p>
    <p>Recent work has shown that:  Trees do not resemble any semantic or syntactic formalisms</p>
    <p>(Williams et al. 2018).</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
    <p>RL-SPINN: Yogatama et al., 2016  Soft-CYK: Maillard et al., 2017  Gumbel Tree-LSTM: Choi et al., 2018</p>
    <p>Recent work has shown that:  Trees do not resemble any semantic or syntactic formalism</p>
    <p>(Williams et al. 2018).  Parsing strategies are not consistent across random restarts</p>
    <p>(Williams et al. 2018).</p>
  </div>
  <div class="page">
    <p>Latent tree learning</p>
    <p>RL-SPINN: Yogatama et al., 2016  Soft-CYK: Maillard et al., 2017  Gumbel Tree-LSTM: Choi et al., 2018</p>
    <p>Recent work has shown that:  Trees do not resemble any semantic or syntactic formalisms</p>
    <p>(Williams et al. 2018).  Parsing strategies are not consistent across random restarts</p>
    <p>(Williams et al. 2018).  These models fail to learn the simple context-free grammar</p>
    <p>(Nangia et al. 2018).</p>
  </div>
  <div class="page">
    <p>ListOps (Nangia, &amp; Bowman (2018))</p>
    <p>[MIN 1 [MAX [MIN 9 [MAX 1 0 ] 2 9 [MED 8 4 3 ] ] [MIN 7 5 ] 6 9 3 ] ] [MAX 1 4 0 9 ] [MAX 7 1 [MAX 6 8 1 7 ] [MIN 2 6 ] 3 ]</p>
  </div>
  <div class="page">
    <p>ListOps (Nangia, &amp; Bowman (2018))</p>
    <p>[MIN 1 [MAX [MIN 9 [MAX 1 0 ] 2 9 [MED 8 4 3 ] ] [MIN 7 5 ] 6 9 3 ] ] [MAX 1 4 0 9 ] [MAX 7 1 [MAX 6 8 1 7 ] [MIN 2 6 ] 3 ]</p>
  </div>
  <div class="page">
    <p>ListOps (Nangia, &amp; Bowman (2018))</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Tree-LSTM parser (Choi et al., 2018)</p>
  </div>
  <div class="page">
    <p>Separation of syntax and semantics</p>
    <p>Parser Compositional Function</p>
  </div>
  <div class="page">
    <p>Parsing as a RL problem Parser Compositional Function</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>Size of the search space is</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>Size of the search space is</p>
    <p>For a sentence with 20 words, there are 1_767_263_190 possible trees.</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>Syntax and semantic has to be learnt simultaneously model has to infer from examples that [MIN 0 1] = 0</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>nonstationary environment (i.e the same sequence of actions can receive different rewards)</p>
    <p>Syntax and semantic has to be learnt simultaneously model has to infer from examples that [MIN 0 1] = 0</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>Typically, the compositional function  is learned faster than the parser .</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>Typically, the compositional function  is learned faster than the parser .</p>
    <p>This fast coadaptation limits the exploration of the search space to parsing strategies similar to those found at the beginning of the training.</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>High variance in the estimate of a parsers gradient  has to be addressed.</p>
    <p>Learning paces of a parser  and a compositional function  have to be levelled off.</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>reward</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>reward</p>
    <p>Is this a carrot?</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>the moving average of recent rewards</p>
    <p>new reward</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>[MIN 1 [MAX [MIN 9 [MIN 1 0 ] 2 [MED 8 4 3 ] ] [MAX 7 5 ] 6 9 ] ]  [MAX 1 0 ]</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>[MIN 1 [MAX [MIN 9 [MIN 1 0 ] 2 [MED 8 4 3 ] ] [MAX 7 5 ] 6 9 ] ]  [MAX 1 0 ]</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>[MIN 1 [MAX [MIN 9 [MIN 1 0 ] 2 [MED 8 4 3 ] ] [MAX 7 5 ] 6 9 ] ]  [MAX 1 0 ]</p>
  </div>
  <div class="page">
    <p>Variance reduction</p>
    <p>self-critical training (SCT) baseline Rennie et al. (2017)</p>
    <p>[MIN 1 [MAX [MIN 9 [MIN 1 0 ] 2 [MED 8 4 3 ] ] [MAX 7 5 ] 6 9 ] ]  [MAX 1 0 ]</p>
  </div>
  <div class="page">
    <p>Synchronizing syntax and semantics learning</p>
    <p>Syntax Semantics</p>
  </div>
  <div class="page">
    <p>Synchronizing syntax and semantics learning</p>
  </div>
  <div class="page">
    <p>Synchronizing syntax and semantics learning</p>
  </div>
  <div class="page">
    <p>Synchronizing syntax and semantics learning</p>
    <p>Proximal Policy Optimization (PPO) of Schulman et al. (2017)</p>
  </div>
  <div class="page">
    <p>Optimization challenges</p>
    <p>High variance in the estimate of a parsers gradient  is addressed by using self-critical training (SCT) baseline of Rennie et al. (2017).</p>
    <p>Learning paces of a parser  and a compositional function  is levelled off by controlling parsers updates using Proximal Policy Optimization (PPO) of Schulman et al. (2017).</p>
  </div>
  <div class="page">
    <p>ListOps results</p>
  </div>
  <div class="page">
    <p>ListOps results</p>
  </div>
  <div class="page">
    <p>ListOps results</p>
  </div>
  <div class="page">
    <p>ListOps results</p>
  </div>
  <div class="page">
    <p>ListOps results</p>
  </div>
  <div class="page">
    <p>Extrapolation</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis (SST-2)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis (SST-2)</p>
  </div>
  <div class="page">
    <p>Natural language inference (MultiNLI)</p>
  </div>
  <div class="page">
    <p>Method Time</p>
    <p>complexity Space</p>
    <p>complexity ListOps</p>
    <p>RL-SPINN: Yogatama et al., 2016 O(nd2) O(nd2)</p>
    <p>Soft-CYK: Maillard et al., 2017 O(n3d+n2d2) O(n3d)</p>
    <p>Gumbel Tree-LSTM: Choi et al., 2018 O(n2d+nd2) O(n2d)</p>
    <p>Ours O(Knd2) O(nd2)</p>
    <p>Time and Space complexities</p>
    <p>n  sentence length d  tree-LSTM dimensionality K  number of updates in PPO</p>
  </div>
  <div class="page">
    <p>Conclusions  The separation between syntax and semantics allows</p>
    <p>coordination between optimisation schemes for each module.  Self-critical training mitigates credit assignment problem by</p>
    <p>distinguishing hard and easy to solve datapoints.  The model can recover a simple context-free grammar of</p>
    <p>mathematical expressions.  The model performs competitively on several real natural</p>
    <p>language tasks.</p>
    <p>github.com/facebookresearch/latent-treelstm</p>
  </div>
</Presentation>
