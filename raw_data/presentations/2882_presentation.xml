<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Opportunities And Challenges Of Machine Learning Accelerators In Production 2019 USENIX Conference On Operational Machine Learning</p>
    <p>Rajagopal Ananthanarayanan, Peter Brandt, Manasi Joshi, Maheswaran Sathiamoorthy {ananthr, pbrandt, manasi, nlogn}@google.com</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Intro</p>
    <p>Production Deployment</p>
    <p>Architectural Specialization - TPU Case Study</p>
    <p>Developer Experience</p>
    <p>Opportunities and Future Directions</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Intro</p>
  </div>
  <div class="page">
    <p>Big Trends Shaping the Future of Compute</p>
    <p>Source: John Hennessy and David Patterson, Computer Architecture: A Quantitative Approach, 6/e. 2018. Based on SPECintCPU.</p>
    <p>End of Moores Law</p>
    <p>Source: https://blog.openai.com/ai-and-compute/</p>
    <p>Rise of Deep Learning</p>
  </div>
  <div class="page">
    <p>Demand for ML Accelerators has Exploded</p>
    <p>Google has built and deployed 3 generations of TPUs now widely used across the company</p>
    <p>NVIDIA has built an ML accelerator datacenter business with $2.93B revenue in fiscal year 2019, up 52% YoY</p>
    <p>AI chip startups have proliferated Cloud TPU v3 Pod: 100+ petaflops, 32 TB HBM</p>
  </div>
  <div class="page">
    <p>ML Accelerators drive Product Innovation</p>
    <p>Assistants Wavenet human-sounding voice serves all production traffic on TPUs (article, related Cloud product).</p>
  </div>
  <div class="page">
    <p>Production Deployment</p>
  </div>
  <div class="page">
    <p>System Design for Balanced I/O and Compute</p>
    <p>ML Accelerators are typically connected to a host CPU.</p>
    <p>PCI</p>
    <p>ML Accelerator Host VM</p>
    <p>CPU</p>
    <p>Your VM</p>
    <p>Network</p>
    <p>Runs linear algebra for deep learningRuns data pre-processing</p>
  </div>
  <div class="page">
    <p>System Design for Balanced I/O and Compute</p>
    <p>The ratio between ML Accelerators and their host CPU machines is generally fixed / hard to dynamically change.</p>
    <p>If the CPU host cant perform data processing, shuffling, transformation, etc. at a high throughput, the ML Accelerator may be underutilized.</p>
    <p>ML Accelerators are getting faster at a quicker rate than CPUs! Depending on the ML workload, it is easy for systems to be bottlenecked on CPU or I/O. APIs like tf.data can help.</p>
  </div>
  <div class="page">
    <p>Utilizing Diverse Hardware Pools Efficiently</p>
    <p>Sharing is easy with homogeneous hardware</p>
    <p>x86</p>
    <p>x86x86</p>
    <p>x86</p>
    <p>x86</p>
    <p>x86x86</p>
    <p>x86</p>
    <p>Today, hardware pools are much more heterogeneous</p>
    <p>x86</p>
    <p>GPUx86</p>
    <p>x86</p>
    <p>x86</p>
    <p>?x86</p>
    <p>TPU</p>
    <p>Help me!</p>
  </div>
  <div class="page">
    <p>Utilizing Diverse Hardware Pools Efficiently</p>
    <p>Job scheduling constraints are increasingly complex. A few techniques that work in practice:</p>
    <p>Smart queueing of training workloads with a dominant resource fairness policy1</p>
    <p>Proportional sharing instead of job priority on a large pool of resources</p>
    <p>Bin packing multiple models on a single device</p>
    <p>Trading off inference latency and query cost constraints</p>
  </div>
  <div class="page">
    <p>Resource Planning Timelines for designing new hardware and deploying it to data centers stretch over several years. Demand can be unpredictable.</p>
    <p>Decide what you are optimizing for! There is tension between ensuring flexibility for launches / research and maximizing utilization.</p>
    <p>Utilization can have many definitions e.g. financial impact, examples per second, model convergence, doing useful work.</p>
    <p>Machine-level instrumentation can be a good coarse metric e.g. power usage of individual chips.</p>
    <p>Accurately simulating future hardware performance for computationally intensive models can greatly help planning.</p>
  </div>
  <div class="page">
    <p>Architectural Specialization TPU Case Study</p>
  </div>
  <div class="page">
    <p>Architectural Specializations</p>
    <p>Instruction sets for linear algebra  Large matrix multiply unit  VLIW instructions  Restricted precision data types e.g. bfloat16, quantized integers</p>
    <p>Memory hierarchy  Operate over block-oriented data  Buffers to efficiently feed data</p>
    <p>Specialized networks and data paths  Fast synchronization and AllReduce</p>
    <p>Separate hosts with generalized CPUs  Implement language runtime and operating system services</p>
  </div>
  <div class="page">
    <p>TPU V1 Architecture</p>
    <p>Source: https://cacm.acm.org/magazines/2018/9/230571-a-domain-specific-architecture-for-deep-neural-networks/fulltext</p>
    <p>Designed for high performance linear algebra and deep learning at scale.</p>
  </div>
  <div class="page">
    <p>TPU V2 Pod Interconnect</p>
    <p>All-reduce weights updates on a 2-D</p>
    <p>toroidal mesh network</p>
    <p>As easy to use as a single node</p>
    <p>See animation at tpudemo.com</p>
    <p>Pod configuration can scale ~linearly and act as a unified accelerator on large problems of billions of parameters.</p>
  </div>
  <div class="page">
    <p>Software Stack Computation is defined as a directed acyclic graph (DAG) to optimize an objective function.</p>
    <p>Graphs are defined in high-level language e.g. Python</p>
    <p>Executed (in parts or fully) on available low-level devices e.g. CPU, GPU, TPU</p>
    <p>Tensors of data flow through the graph</p>
    <p>Gradients are computed automatically!</p>
  </div>
  <div class="page">
    <p>Software Stack</p>
    <p>Keras and TF Estimator</p>
    <p>TensorFlow</p>
    <p>XLA Just-in-time Compiler</p>
    <p>TF graphs go in</p>
    <p>Specify a model using common high-level abstractions</p>
    <p>Run compute graph ops partially or completely on ML accelerator hardware</p>
    <p>TFX, TF Hub, TF Serving, TF Transform</p>
    <p>Train, eval, export, and serve in production with template code for common classes of models</p>
    <p>Optimized assembly comes out (illustrated w/ x86 assembly)</p>
  </div>
  <div class="page">
    <p>Developer Experience</p>
  </div>
  <div class="page">
    <p>Tips and tricks</p>
    <p>Smart decompose by dividing ops across host CPU and accelerators - compatibility vs. suitability</p>
    <p>Balance large batch sizes (beneficial for data parallelism) and slow ramp up of learning rate for training (to compensate for potential poor quality)</p>
    <p>Report errors through multiple levels of abstractions</p>
    <p>Development and production workflow</p>
    <p>Continuously track model quality during training to catch regressions, and gate model deployment</p>
    <p>Continuously benchmark inference load tests - optimize batch size for the highest throughput vs. compromising query costs or latency</p>
    <p>Practical Considerations</p>
  </div>
  <div class="page">
    <p>Opportunities and Future Directions</p>
  </div>
  <div class="page">
    <p>Multitask learning</p>
    <p>Transfer learning</p>
    <p>Multimodal models</p>
    <p>Sparse features</p>
    <p>Larger datasets and features</p>
    <p>Modeling Enhancements</p>
  </div>
  <div class="page">
    <p>Infrastructure Improvements</p>
    <p>Language and compilers</p>
    <p>Built-in support for accelerators e.g. JAX, Julia</p>
    <p>Distribution strategies for large-scale systems</p>
    <p>Mirroring variables across workers</p>
    <p>Better support for balanced system throughput</p>
    <p>Input, output, and intermediate checkpoints</p>
  </div>
  <div class="page">
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>ML workloads are computationally intensive and fast growing</p>
    <p>Investment in Domain Specific Architectures is increasing</p>
    <p>Instruction sets, systems architecture</p>
    <p>Diverse ML programming environments are proliferating</p>
    <p>Interpreted, compiled, graph analysis, auto differentiation</p>
    <p>Ensure good programming / developer experience</p>
    <p>Optimizing data center deployment is critical</p>
    <p>Ultimately, there is a lot of innovation enabling new types of ML models and products!</p>
  </div>
</Presentation>
