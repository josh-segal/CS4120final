<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>APUNet: Revitalizing GPU as Packet Processing Accelerator</p>
    <p>Younghwan Go, Muhammad Asim Jamshed, YoungGyoun Moon, Changho Hwang, and KyoungSoo Park</p>
    <p>School of Electrical Engineering, KAIST</p>
  </div>
  <div class="page">
    <p>GPU-accelerated Networked Systems</p>
    <p>Execute same/similar operations on each packet in parallel  High parallelization power  Large memory bandwidth</p>
    <p>Improvements shown in number of research works  PacketShader [SIGCOMM10], SSLShader [NSDI11], Kargus [CCS12], NBA</p>
    <p>[EuroSys15], MIDeA [CCS11], DoubleClick [APSys12],</p>
    <p>CPU PacketPacket</p>
    <p>GPU PacketPacket</p>
    <p>PacketPacket</p>
  </div>
  <div class="page">
    <p>Source of GPU Benefits</p>
    <p>GPU acceleration mainly comes from memory access latency hiding  Memory I/O  switch to other thread for continuous execution</p>
    <p>GPU</p>
    <p>a = b + c;  v = mem[a].val;</p>
    <p>Thread 1 Thread 2</p>
    <p>d = e * f;</p>
    <p>Inactive</p>
    <p>Memory I/O</p>
    <p>GPU</p>
    <p>a = b + c;  v = mem[a].val;</p>
    <p>Thread 1 Thread 2</p>
    <p>d = e * f;</p>
    <p>Inactive</p>
    <p>Prefetch in background</p>
    <p>Quick Context Switch</p>
  </div>
  <div class="page">
    <p>Memory Access Hiding in CPU vs. GPU</p>
    <p>Re-order CPU code to mask memory access (G-Opt)*  Group prefetching, software pipelining</p>
    <p>Anuj Kalia, Dong Zhu, Michael Kaminsky, and David G. Anderson</p>
    <p>*Borrowed from G-Opt slides</p>
    <p>Questions: Can CPU code optimization be generalized to all network applications?</p>
    <p>Which processor is more beneficial in packet processing?</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Demystify processor-level effectiveness on packet processing algorithms  CPU optimization benefits light-weight memory-bound workloads  CPU optimization often does not help large memory workloads  GPU is more beneficial for compute-bound workloads  GPUs data transfer overhead is the main bottleneck, not its capacity</p>
    <p>Packet processing system with integrated GPU w/o DMA overhead  Addresses GPU kernel setup / data sync overhead, and memory contention  Up to 4x performance over CPU-only approaches!</p>
  </div>
  <div class="page">
    <p>Discrete GPU</p>
    <p>CPU</p>
    <p>Peripheral device communicating with CPU via a PCIe lane</p>
    <p>Host DRAM</p>
    <p>PCIe LanesGPU</p>
    <p>GDDR Device Memory</p>
    <p>L2 Cache</p>
    <p>Graphics Processing</p>
    <p>Cluster x N</p>
    <p>Streaming Multiprocessor (SM)</p>
    <p>Instruction Cache</p>
    <p>L1 Cache Shared Memory</p>
    <p>Scheduler Registers High computation power High memory bandwidth Fast inst./data access Fast context switch</p>
    <p>PCIe Lanes</p>
    <p>Require CPU-GPU DMA transfer!</p>
    <p>GDDR Device Memory</p>
    <p>Host DRAM</p>
  </div>
  <div class="page">
    <p>Integrated GPU</p>
    <p>Place GPU into same die as CPU  share DRAM  AMD Accelerated Processing Unit (APU), Intel HD Graphics</p>
    <p>Graphics Northbridge</p>
    <p>Unified Northbridge Host</p>
    <p>DRAM</p>
    <p>Compute Unit</p>
    <p>L1 Cache</p>
    <p>Scheduler Registers</p>
    <p>High computation power Fast inst./data access Fast context switch Low power &amp; cost</p>
    <p>Host DRAM</p>
    <p>Graphics Northbridge</p>
    <p>Unified Northbridge No DMA transfer!CPU</p>
    <p>GPU L2 Cache</p>
    <p>Compute Unit x N</p>
    <p>APU</p>
  </div>
  <div class="page">
    <p>CPU vs. GPU: Cost Efficiency Analysis</p>
    <p>Performance-per-dollar on 8 popular packet processing algorithms  Memory- or compute-intensive  IPv4, IPv6, Aho-Corasick pattern match, ChaCha20, Poly1305, SHA-1, SHA-2, RSA</p>
    <p>Test platform  CPU-baseline, G-Opt (optimized CPU), dGPU w/ copy, dGPU w/o copy, iGPU</p>
    <p>CPU / Discrete GPU</p>
    <p>CPU Intel Xeon E5-2650 v2 (8 @ 2.6 GHz)</p>
    <p>GPU NVIDIA GTX980 (2048 @ 1.2 GHz)</p>
    <p>RAM 64 GB (DIMM DDR3 @ 1333 MHz)</p>
    <p>Cost CPU: $1143.9 dGPU: $840</p>
    <p>APU / Integrated GPU</p>
    <p>CPU AMD RX-421BD (4 @ 3.4 GHz)</p>
    <p>GPU AMD R7 Graphics (512 @ 800 MHz)</p>
    <p>RAM 16 GB (DIMM DDR3 @ 2133 MHz)</p>
    <p>Cost iGPU: $67.5</p>
  </div>
  <div class="page">
    <p>Cost Effectiveness of CPU-based Optimization</p>
    <p>G-Opt helps memory-intensive, but not compute-intensive algorithms  Computation capacity as bottleneck with more computations</p>
    <p>CPU G-Opt</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>IPv6 table lookup</p>
    <p>CPU G-Opt</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>AC pattern matching</p>
    <p>CPU G-Opt dGPU w/ copy</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>SHA-2</p>
    <p>CPU G-Opt dGPU w/ copy</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>Detailed analysis on CPU-based optimization in the paper</p>
  </div>
  <div class="page">
    <p>Cost Effectiveness of Discrete/Integrated GPUs</p>
    <p>Discrete GPU suffers from DMA transfer overhead  Integrated GPU is most cost efficient!</p>
    <p>G-Opt dGPU w/o copy</p>
    <p>iGPUN or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>G-Opt dGPU w/o copy</p>
    <p>iGPUN or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r</p>
    <p>ChaCha20</p>
    <p>G-Opt dGPU w/ copy</p>
    <p>dGPU w/o copy</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r IPv4 table lookup</p>
    <p>G-Opt dGPU w/ copy</p>
    <p>dGPU w/o copy</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>p er</p>
    <p>f p er</p>
    <p>d ol</p>
    <p>la r IPv4 table lookup</p>
    <p>Our approach: Use integrated GPU to accelerate packet processing!</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Introduction and motivation  Background on GPU  CPU vs. GPU: cost efficiency analysis  Research Challenges  APUNet design  Evaluation  Conclusion</p>
  </div>
  <div class="page">
    <p>Research Challenges</p>
    <p>Frequent GPU kernel setup overhead  Overhead exposed w/o DMA transfer</p>
    <p>High data synchronization overhead  CPU-GPU cache coherency</p>
    <p>More contention on shared DRAM  Reduced effective memory bandwidth</p>
    <p>Set input Launch kernel</p>
    <p>Teardown kernel Retrieve result</p>
    <p>Redundant overhead!</p>
    <p>Launch kernel Teardown kernel</p>
    <p>APUNet: a high-performance APU-accelerated network packet processor</p>
    <p>APU</p>
    <p>CPU DRAM</p>
    <p>Bottleneck!</p>
    <p>BW: 10x</p>
    <p>Explicit Sync!GPU</p>
    <p>Cache</p>
  </div>
  <div class="page">
    <p>Persistent Thread Execution Architecture</p>
    <p>Persistently run GPU threads without kernel teardown  Master passes packet pointer addresses to GPU threads</p>
    <p>NIC</p>
    <p>Shared Virtual Memory (SVM)CPU</p>
    <p>Workers</p>
    <p>Master</p>
    <p>Packet I/O</p>
    <p>Packet Pool Packet</p>
    <p>Packet Packet</p>
    <p>Pointer Array</p>
    <p>GPU</p>
    <p>Thread 0</p>
    <p>Thread 1</p>
    <p>Thread 2</p>
    <p>Thread 3</p>
    <p>Persistent Threads</p>
    <p>Packet</p>
  </div>
  <div class="page">
    <p>Data Synchronization Overhead</p>
    <p>Synchronization point for GPU threads: L2 cache  Require explicit synchronization to main memory</p>
    <p>Shared Virtual Memory PP P P P Master</p>
    <p>Thread 0 Thread 1</p>
    <p>Thread 2 Thread 3</p>
    <p>Thread 4 Thread 5</p>
    <p>Thread 6 Thread 7</p>
    <p>L2 Cache</p>
    <p>Graphics Northbridge</p>
    <p>Need explicit sync!Can process one</p>
    <p>request at a time!</p>
    <p>Update result?</p>
    <p>CPU</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>Solution: Group Synchronization</p>
    <p>Implicitly synchronize group of packet memory GPU threads processed  Exploit LRU cache replacement policy</p>
    <p>Shared Virtual Memory PP P P P</p>
    <p>Master</p>
    <p>Group 2</p>
    <p>Group 1</p>
    <p>GPU Thread Group 1 GPU Thread Group 2 Thread 0</p>
    <p>Process Poll</p>
    <p>Barrier</p>
    <p>Thread 31</p>
    <p>Process Poll</p>
    <p>Barrier</p>
    <p>Thread 0</p>
    <p>Dummy Mem I/O Poll</p>
    <p>Barrier</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>GPU L2 Cache P P P P P P D D D D D D</p>
    <p>Verify correctness!</p>
    <p>For more details and tuning/optimizations, please refer to our paper</p>
  </div>
  <div class="page">
    <p>Zero-copy Based Packet Processing</p>
    <p>Integrate memory allocation for NIC, CPU, GPU</p>
    <p>Standard (e.g., mmap) GDDR (e.g., cudaMalloc)</p>
    <p>COPY GPUCPUNIC</p>
    <p>Standard (e.g., mmap) Shared (e.g., clSVMAlloc)</p>
    <p>COPY GPUCPUNIC</p>
    <p>Option 1. Traditional method with discrete GPU Option 2. Zero-copy between CPU-GPU</p>
    <p>High overhead! High overhead!</p>
    <p>Shared (e.g., clSVMAlloc)</p>
    <p>GPUCPUNIC</p>
    <p>No copy overhead!</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>How well does APUNet reduce latency and improve throughputs?  How practical is APUNet in real-world network applications?</p>
    <p>APUNet (AMD Carrizo APU) RX-421BD (4 cores @ 3.4 GHz)</p>
    <p>R7 Graphics (512 cores @ 800 MHz) 16GB DRAM</p>
    <p>Client (packet/flow generator) Xeon E3-1285 v4 (8 cores @ 3.5 GHz)</p>
  </div>
  <div class="page">
    <p>Benefits of APUNet Design</p>
    <p>Workload: IPsec (128-bit AES-CBC + HMAC-SHA1)</p>
    <p>Pa ck</p>
    <p>et la</p>
    <p>te nc</p>
    <p>y (u</p>
    <p>s)</p>
    <p>Packet size (bytes)</p>
    <p>GPU-Copy GPU-ZC GPU-ZC-PERSIST</p>
    <p>Packet Processing Latency</p>
    <p>Synchronization Throughput (64B Packet)</p>
    <p>Atomics Group sync</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>Real-world Network Applications</p>
    <p>5 real-world network applications  IPv4/IPv6 packet forwarding, IPsec gateway, SSL proxy, network IDS</p>
    <p>IPsec Gateway SSL Proxy</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Packet size (bytes)</p>
    <p>CPU Baseline G-Opt APUNet</p>
    <p>H T</p>
    <p>T P</p>
    <p>tr an</p>
    <p>s/ se</p>
    <p>c Number of concurrent connections</p>
    <p>CPU Baseline G-Opt APUNet</p>
  </div>
  <div class="page">
    <p>Real-world Network Applications</p>
    <p>Snort-based Network IDS  Aho-Corasick pattern matching</p>
    <p>No benefit from CPU optimization!  Access many data structures  Eviction of already cached data</p>
    <p>DFC* outperforms AC-APUNet  CPU-based algorithm  Cache-friendly &amp; reduces memory access</p>
    <p>Network IDS</p>
    <p>*DFC: Accelerating String Pattern Matching for Network Applications [NSDI16] Byungkwon Choi, Jongwook Chae, Muhammad Jamshed, KyoungSoo Park, and Dongsu Han</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Packet size (bytes)</p>
    <p>CPU Baseline G-Opt APUNet DFC</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Packet size (bytes)</p>
    <p>CPU Baseline G-Opt APUNet DFC</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>G bp</p>
    <p>s)</p>
    <p>Packet size (bytes)</p>
    <p>CPU Baseline G-Opt APUNet DFC</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Re-examine the efficacy of GPU-based packet processor  GPU is bottlenecked by PCIe data transfer overhead  Integrated GPU is the most cost effective processor</p>
    <p>APUNet: APU-accelerated networked system  Persistent thread execution: eliminate kernel setup overhead  Group synchronization: minimize data synchronization overhead  Zero-copy packet processing: reduce memory contention  Up to 4x performance improvement over CPU baseline &amp; G-Opt</p>
    <p>APUNet High-performance, cost-effective platform for real-world network applications</p>
  </div>
  <div class="page">
    <p>Thank you.</p>
    <p>Q &amp; A</p>
  </div>
</Presentation>
