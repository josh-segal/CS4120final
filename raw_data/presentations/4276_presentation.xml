<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cardinal Virtues: Extracting Relation Cardinalities from Text Paramita Mirza1, Simon Razniewski2, Fariz Darari2 and Gerhard Weikum1</p>
    <p>IE has largely focused on answering Who has won which award?  However, some facts are never fully mentioned and no IE method has perfect recall</p>
    <p>Sentences like John lives with his spouse and 5 children on a farm in Alabama are much more frequent in texts.</p>
    <p>We focus instead on answering How many awards has someone won?  Useful for aggregate query answering, e.g., Who won the most awards?</p>
    <p>Contributions:  We introduce the problem of Relation Cardinality Extraction  We present a distant supervision method using Conditional Random Fields  We discuss specific challenges that set it apart from standard IE</p>
    <p>Relation Cardinality a mention that expresses relation cardinality</p>
    <p>is a cardinal number that states the number of objects that stand in a specific relation with a certain subject</p>
    <p>Barack and Michelle Obama have two children, which are currently .</p>
    <p>Acknowledgment This work has been partially supported by the projects TCFR - The Call for Recall, funded by the Free University of Bozen-Bolzano.</p>
    <p>KB recall is highly variant and mostly unknown</p>
    <p>Barack and Michelle Obama have two children, which are currently .</p>
    <p>Given a well defined relation/predicate p, a subject s and a corresponding text about s, we try to estimate the relation cardinality,</p>
    <p>i.e., the count of &lt;s, p, *&gt; triples</p>
    <p>Methodology  Sequence labelling problem:</p>
    <p>Barack and Michelle Obama have two children , which are currently . Barack and Michelle Obama have _num_ child , which be currently   lemma</p>
    <p>O O O O O CHILD O O O O O</p>
    <p>Conditional Random Fields (CRF) model using CRF++ (Kudo, 2005)  Feature set: lemma of observed token t, context lemmas (windows size = 5),</p>
    <p>bigrams and trigrams containing t</p>
    <p>Distant supervision for generating training data  Given an &lt;s, p&gt; pair we identify:</p>
    <p>the triple count |&lt;s, p, *&gt;| from Wikidata (Vrandei and Krtzsch, 2014); and</p>
    <p>candidate sentences from English Wikipedia article of s  candidate numbers (not labelled as TEMPORAL, MONEY or PERCENT) in each</p>
    <p>sentence (if any)  We generate training examples by labelling a candidate number n with p if</p>
    <p>n = |&lt;s, p, *&gt;|, otherwise, it is labelled as O, like the rest of non-number tokens</p>
    <p>Prediction  Having the annotated sentences by the CRF-based model,  Relation cardinality for a given &lt;s, p&gt; pair is the candidate number labelled with</p>
    <p>p, which has the highest confidence score (i.e., marginal probability of a token labelled as such, resulting from forward-backward inference)</p>
    <p>Experiments</p>
    <p>Evaluation on manually annotated randomly sampled subjects for 4 Wikidata properties: 20 (has part), 100 (contains admin.) and 200 (child and spouse)</p>
    <p>baseline: randomly select a number from a pool of numbers in text  only nummod: consider only candidate numbers that modify a noun</p>
    <p>KB: 0 KB: 1 KB: 2</p>
    <p>Recall: 0% Recall: 50% Recall: 100%</p>
    <p>Despite its frequency</p>
    <p>Open IE (Mausam et al. 2012; Del Corro and Gemulla, 2013)  No way to interpret the numeric expression in the Object slot , e.g., &lt;Obama, has,</p>
    <p>two children&gt;</p>
    <p>KB-population IE, e.g., NELL (Mitchell et al., 2015)  Knows 13 relations about the number of casualties and injuries in disasters, e.g.,</p>
    <p>&lt;Berlin2016attack, hasNumOfVictims, 32&gt;  Contains only seed facts and no learned facts</p>
    <p>Stanford Named Entity (NE) tagger on cardinal numbers in 10K Wikipedia articles</p>
    <p>July 30-August 4, 2017  Vancouver, Canada</p>
    <p>DBpedia contains currently only 6 out of 35 Dijkstra</p>
    <p>Prize winners</p>
    <p>According to YAGO, the average number of children per person is 0.02</p>
    <p>Quality of Training Data</p>
    <p>Distant supervision from highly incomplete KB  e.g., manual annotation on child evaluation set  Wikidata is only 50% accurate.  Unlike in classical IE, missing ground truth may lead to false positives as well.</p>
    <p>Possible approaches:  Filtering ground truth  consider only popular entities for training.  Incompleteness-resilient distant supervision  label all numbers equal or higher</p>
    <p>than the KB count as positive examples.</p>
    <p>Compositionality</p>
    <p>They have two sons and one daughter together; he has four children from his first wife.</p>
    <p>16% of false positives in extracting child cardinalities</p>
    <p>Possible approaches:  Aggregating numbers  in training data generation, label a sequence of numbers</p>
    <p>as correct cardinalities if the sum is equal to the KB count; in prediction step, sum up all consecutive cardinalities.</p>
    <p>Learning composition rules  e.g., children are composed of sons and daughters.</p>
    <p>Linguistic Variance</p>
    <p>Ordinals are quite common to express lower bounds, e.g., Johns first wife, Mary, .  Relation cardinalities are sometimes expressed with non-numerals, e.g., He never married,</p>
    <p>They have a daughter together, The book is a trilogy.</p>
    <p>Possible approaches:  Translation to numbers  translate certain kinds of negation and indefinite articles</p>
    <p>into expressions containing 0 and 1.  Word similarity with cardinals  consider words bear high similarity with cardinal</p>
    <p>numbers, possibly in other language such as Latin or Greek.</p>
    <p>p #s train baseline vanilla only nummod</p>
    <p>P P R F1 P R F1</p>
    <p>has part (creative work series) 261 .050 .333 .316 .324 .353 .316 .333</p>
    <p>contains admin 18,000 .034 .390 .188 .254 .548 .200 .293</p>
    <p>spouse 45,917 0 .014 .011 .013 .028 .017 .021</p>
    <p>child 35,057 .112 .151 .129 .139 .320 .219 .260</p>
    <p>child (manual ground truth) 6,408 .374 .309 .338 .452 .315 .317</p>
    <p>Further Reading  Predicting Completeness in Knowledge Bases, Luis Galrraga, Simon Razniewski, Antoine Amarilli,</p>
    <p>Fabian M. Suchanek, WSDM, Cambridge, UK, 2017  Expanding Wikidatas Parenthood Information by 178%, or How To Mine Relation Cardinalities,</p>
    <p>Paramita Mirza, Simon Razniewski, Werner Nutt, ISWC Poster, Osaka, Japan, 2016  But What Do We Actually Know?, Simon Razniewski, Fabian Suchanek, Werner Nutt, AKBC</p>
    <p>workshop at NAACL, San Diego, USA, 2016  Identifying the Extent of Completeness of Query Answers over Partially Complete Databases, Simon</p>
    <p>Razniewski, Flip Korn, Werner Nutt, Divesh Srivastava, SIGMOD, Melbourne, Australia, 2015  A tool for crowdsourced completeness annotations for Wikidata: http://cool-wd.inf.unibz.it/</p>
  </div>
</Presentation>
