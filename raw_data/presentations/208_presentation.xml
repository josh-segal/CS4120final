<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Natively Supporting True One-sided</p>
    <p>Communication in MPI on Multi-core</p>
    <p>Systems with InfiniBand</p>
    <p>G. Santhanaraman, P. Balaji, K. Gopalakrishnan, R. Thakur,</p>
    <p>W. Gropp D. K. Panda</p>
    <p>Dept. of Computer Science, Ohio State University</p>
    <p>Mathematics and Computer Science, Argonne National Laboratory</p>
    <p>Dept. of Computer Science, University of Illinois, Urbana Champaign</p>
  </div>
  <div class="page">
    <p>Massive Systems &amp; Scalable Applications</p>
    <p>Massive High-end Computing (HEC) Systems available  Systems with few thousand cores are common  Systems with 10s to 100s of thousands of cores available</p>
    <p>Scalable application communication patterns  Clique-based communication</p>
    <p>Nearest neighbor: Ocean/Climate modeling, PDE solvers  Cartesian grids: 3DFFT</p>
    <p>Unsynchronized communication  Minimize need to synchronize with other processes  Non-blocking communication is heavily used  One-sided communication is getting popular</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Process 1 Process 2 Process 3</p>
    <p>Private Virtual</p>
    <p>Address</p>
    <p>Private Virtual</p>
    <p>Address</p>
    <p>Private Virtual</p>
    <p>Address</p>
    <p>Process 0</p>
    <p>Private Virtual</p>
    <p>Address</p>
    <p>One-sided Communication (RMA)</p>
    <p>Popular in many modern programming models  MPI, UPC, Global Arrays</p>
    <p>Idea is to have an easily accessible global address space together with each process virtual address space</p>
    <p>CCGrid (05/21/2009)</p>
    <p>Shared Virtual</p>
    <p>Address</p>
    <p>Shared Virtual</p>
    <p>Address</p>
    <p>Shared Virtual</p>
    <p>Address</p>
    <p>Shared Virtual</p>
    <p>Address</p>
    <p>Global Address Space</p>
  </div>
  <div class="page">
    <p>RMA benefits for Applications</p>
    <p>Access to additional address space  Not limited by per-core memory available  Two sided communication requires data to be explicitly</p>
    <p>moved between virtual address spaces</p>
    <p>Tolerant to Load Imbalance  Work stealing is simple and efficient (unsynchronized)</p>
    <p>Lock global address region  Modify required regions  Unlock global address region</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Need for RMA Communication</p>
    <p>State of Art and Prior Work</p>
    <p>Proposed Design</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>RMA in MPI</p>
    <p>MPI is the dominant programming model for HEC systems  Extremely portable  MPI (v1) Traditionally relied on two-sided communication</p>
    <p>Each process sends data from its virtual address space  Each process receives data into its virtual address space  Did not have a notion of a global address space</p>
    <p>MPI-2 introduced RMA capability  Each process can use a part of its address space to form a</p>
    <p>global address space window  Processes can perform one-sided operations on this window</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Prior Work: Hardware Supported RMA</p>
    <p>Initial implementations internally relied on two-sided communication  Loses all benefits of a one</p>
    <p>sided programming model</p>
    <p>Our prior work utilized InfiniBand RDMA and atomic operations  One-sided communication</p>
    <p>truly one-sided</p>
    <p>CCGrid (05/21/2009)</p>
    <p>MPI_Win_lock</p>
    <p>Lock (unset)</p>
    <p>Compare &amp; Swap</p>
    <p>Lock (set)</p>
    <p>Lock Acquired</p>
    <p>MPI_Get</p>
    <p>MPI_Put</p>
    <p>MPI_Win_unlock</p>
    <p>Compare &amp; Swap</p>
    <p>Lock Released</p>
  </div>
  <div class="page">
    <p>Multicore Systems: Issues and Pitfalls</p>
    <p>Increasing number of cores per node  Network-based lock/unlock operations not ideal</p>
    <p>Network locks go through the network adapter (several microseconds)</p>
    <p>CPU locks useful when on the same node  Uses CPU atomic instructions: few hundred nanoseconds</p>
    <p>Problem:  Network locks and CPU locks are unaware of each other  No atomicity guarantee between the two</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Our Focus in this Paper</p>
    <p>To provide true one-sided communication capabilities within the node as well as outside</p>
    <p>To propose a design that achieves two seemingly contradictory goals:  Take advantage of network hardware atomic operations for</p>
    <p>efficient inter-node RMA  Take advantage of CPU atomic operations for efficient intra</p>
    <p>node RMA</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Need for RMA Communication</p>
    <p>State of Art and Prior Work</p>
    <p>Proposed Design</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Towards a Hybrid Lock Design</p>
    <p>Goal:  Use network locks for inter-node RMA  Use CPU locks for intra-node RMA</p>
    <p>Caveat:  No atomicity guarantee between network and CPU locks</p>
    <p>Hybrid approach:  At any point lock is either CPU based or Network based</p>
    <p>Network mode locks always go through the network  Loopback if needed (inefficient !)</p>
    <p>CPU mode locks always go through the CPU  Using two-sided communication if needed (inefficient !)</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Migrating Locks</p>
    <p>CCGrid (05/21/2009)</p>
    <p>(4) Modify Lock Mechanism</p>
    <p>NETWORK LOCK</p>
    <p>CPU LOCK</p>
    <p>WINDOW</p>
    <p>NODE A P3</p>
    <p>P0</p>
    <p>P2P1</p>
    <p>(5) G rant</p>
    <p>Loc k</p>
    <p>(2) Req</p>
    <p>ues t Lo</p>
    <p>ck</p>
    <p>(3) Acquire CPU and Network Locks</p>
    <p>(1) Compare &amp; Swap returns CPU Lock Mode</p>
    <p>LOCK MODE: CPU P11</p>
    <p>LOCK MODE: Network</p>
    <p>Synchronization only required during migration of lock type</p>
    <p>All other operations are truly one-sided</p>
  </div>
  <div class="page">
    <p>Migration Policy</p>
    <p>Lock Migration Policy:  When should we use a network lock vs. a CPU lock</p>
    <p>Different policies possible:  Communication pattern history  User-specified priority  Native hardware capabilities (performance of network lock</p>
    <p>as compared to CPU lock)</p>
    <p>We use a simple approach:  Migrate lock on first conflicting request</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Need for RMA Communication</p>
    <p>State of Art and Prior Work</p>
    <p>Proposed Design</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Experimental Testbed</p>
    <p>8-node AMD Barcelona Cluster  Each node 4 processors, each with 4 1.95GHz cores  64KB per-core dedicated L1 cache  512KB per-core dedicated L2 cache  2MB per-processor dedicated L3 cache  16GB RAM</p>
    <p>Network configuration:  InfiniBand ConnectX DDR (MT25418) adapters  PCIe Gen1 x8  Mellanox Infiniscale III 24-port fully non-blocking switch</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Overview of MVAPICH and MVAPICH2</p>
    <p>High Performance MPI for InfiniBand and iWARP Clusters  MVAPICH (MPI-1) and MVAPICH2 (MPI-2): Available since 2002</p>
    <p>Derived from the MPICH2 implementation from Argonne  MVAPICH2 shares upper-level code and includes IB/iWARP</p>
    <p>specific enhancements  Used by more than 900 organizations in 48 countries (registered</p>
    <p>with OSU); More than 29,000 downloads from OSU web site  Empowering many TOP500 production clusters (Nov 08 listing)</p>
    <p>62,976-core cluster (Ranger) at TACC (6th rank)  18,176-core cluster (Chinook) at PNNL (20th rank)</p>
    <p>Available with software stacks of many server vendors including Open Fabrics Enterprise Distribution (OFED)</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Intra-node Performance</p>
    <p>Two-Sided One-Sided Hybrid 0</p>
    <p>Baseline Intra-node Performance</p>
    <p>T im</p>
    <p>e (</p>
    <p>u s</p>
    <p>)</p>
    <p>Two-Sided</p>
    <p>One-Sided</p>
    <p>Hybrid</p>
    <p>Compute Units (iterations)</p>
    <p>E x</p>
    <p>e c</p>
    <p>u ti</p>
    <p>o n</p>
    <p>T im</p>
    <p>e (</p>
    <p>u s</p>
    <p>)</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Contention Measurements</p>
    <p>TwoSided OneSided Hybrid</p>
    <p>Number of Cores</p>
    <p>L a</p>
    <p>te n</p>
    <p>c y</p>
    <p>( u</p>
    <p>s )</p>
    <p>Number of Contending Processes</p>
    <p>L a</p>
    <p>te n</p>
    <p>c y</p>
    <p>( u</p>
    <p>s )</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Migration Overhead</p>
    <p>Internode</p>
    <p>Lock Migration Percentage</p>
    <p>L o</p>
    <p>c k</p>
    <p>/U n</p>
    <p>lo c</p>
    <p>k L</p>
    <p>a te</p>
    <p>n c</p>
    <p>y</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Emulated mpiBLAST Communication Kernel</p>
    <p>Two-Sided</p>
    <p>One-Sided</p>
    <p>Hybrid</p>
    <p>System size (cores)</p>
    <p>E x</p>
    <p>e c</p>
    <p>u ti</p>
    <p>o n</p>
    <p>T im</p>
    <p>e (</p>
    <p>u s</p>
    <p>)</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Need for RMA Communication</p>
    <p>State of Art and Prior Work</p>
    <p>Proposed Design</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Concluding Remarks and Future Work</p>
    <p>Proposed a new design for MPI RMA optimized for highspeed networks and multicore architectures  Uses InfiniBand RDMA and atomics for internode RMA  Uses CPU atomic primitives for intranode RMA</p>
    <p>Significantly improved performance in benchmarks as well as real application kernels</p>
    <p>Future Work:  Study on other architectures such as Blue Gene/P  Different migration policies  Evaluation on various other applications</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Email Contacts:</p>
    <p>G. Santhanaraman: santhana@cse.ohio-state.edu P. Balaji: balaji@mcs.anl.gov</p>
    <p>K. Gopalakrishnan: gopalkk@cse.ohio-state.edu</p>
    <p>R. Thakur: thakur@mcs.anl.gov W. Gropp: wgropp@illinois.edu</p>
    <p>D. K. Panda: panda@cse.ohio-state.edu</p>
    <p>Project Websites:</p>
    <p>http://mvapich.cse.ohio-state.edu</p>
    <p>http://www.mcs.anl.gov/research/projects/mpich2</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
  <div class="page">
    <p>Inter-node Performance</p>
    <p>Two-Sided</p>
    <p>One-Sided</p>
    <p>Hybrid</p>
    <p>Number of Nodes</p>
    <p>T im</p>
    <p>e (</p>
    <p>u s</p>
    <p>)</p>
    <p>CCGrid (05/21/2009)</p>
  </div>
</Presentation>
