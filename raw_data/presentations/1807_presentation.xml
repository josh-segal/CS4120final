<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards Fair Sharing of Block Storage in a Multi-tenant Cloud</p>
    <p>Xing Lin, Yun Mao*, Feifei Li, Robert Ricci</p>
    <p>*</p>
  </div>
  <div class="page">
    <p>Cloud Computing Key Idea: Resource Sharing</p>
    <p>Ecomonies of scale  High utilization</p>
    <p>Host machine</p>
    <p>VM1 VM2 ...</p>
    <p>Network</p>
    <p>Disk</p>
    <p>Typical setup</p>
  </div>
  <div class="page">
    <p>Performance Unpredictability Sharing results in interference</p>
    <p>Listed as the Number 5 obstacle for Cloud Computing (Above the Cloud: a Berkeley View of Cloud Computing)  CPU and memory sharing work well in practice  A dedicated session for network performance</p>
    <p>yesterday  Here, we are looking into disk I/O sharing</p>
  </div>
  <div class="page">
    <p>Disk I/O Sharing Disk I/O sharing is problematic</p>
    <p>Interference between random and sequential workloads</p>
    <p>Conflicts between read and write workloads</p>
    <p>Can we build a cloud storage system with</p>
    <p>more predictable performance?</p>
  </div>
  <div class="page">
    <p>Interference Analysis - Workloads</p>
    <p>Use FIO to investigate interference between:  Random Read(RR)  Sequential Read(SR)  Random Write(RW)  Sequential Write(SW)</p>
    <p>Real-world application  TPC-H</p>
  </div>
  <div class="page">
    <p>Interference Analysis - Setup</p>
    <p>Disk: Seagate Cheetah 10,000 RPM 146 GB SCSI disk(pc3000 in Emulab)  FIO benchmarks</p>
    <p>10 GB partitions  Direct IO  Block size: 4 KB  IO depth: 32  Runtime: 120 s  Metrics: IOPS for random workloads and throughput</p>
    <p>for sequential workloads</p>
  </div>
  <div class="page">
    <p>Interference Analysis Result - I</p>
    <p>Observation1: When co-locating the same type of workloads, each workload gets a fair share in performance and system resources.</p>
    <p>Random Read Sequential Write</p>
    <p>Co-locating same type of workloads</p>
  </div>
  <div class="page">
    <p>Interference Analysis Results - II</p>
    <p>Co-locating different types of workloads The performance of a RR workload when it is run</p>
    <p>with a SR workload</p>
    <p>in isolation</p>
  </div>
  <div class="page">
    <p>Interference Analysis Results - II</p>
    <p>Observation2: Random workloads are destructive to sequential workloads.</p>
    <p>Sequential workloads</p>
  </div>
  <div class="page">
    <p>Interference Analysis Results - II</p>
    <p>Observation3: Random write workload is destructive for all other types of workloads.</p>
  </div>
  <div class="page">
    <p>Interference Analysis Result - III</p>
    <p>Real-world application: TPC-H  21 TPC-H queries(random read)  sequential scan of 9 tables(sequential read)</p>
  </div>
  <div class="page">
    <p>Goal: want to build a block storage system, similar to Amazon EBS, with more predictable performance</p>
    <p>Assumptions  Inexpensive commodity components: replication  Exclusive ownership of a virtual volume  No assumption about workloads within VM</p>
    <p>FAST  Fair Assignment for Storage Tenants</p>
  </div>
  <div class="page">
    <p>FAST  System Design</p>
    <p>System Design:  Directs random reads and sequential reads to different</p>
    <p>replicas  Log-structure to convert random writes into sequential</p>
    <p>Computenode ... Namenode Datanode ...</p>
    <p>network</p>
  </div>
  <div class="page">
    <p>FAST  Architecture</p>
    <p>Replication group1</p>
    <p>RR</p>
    <p>Legend: Control messages Data messages</p>
    <p>(te n a</p>
    <p>n tID</p>
    <p>,c h</p>
    <p>u n</p>
    <p>kID )</p>
    <p>IO type(g ro</p>
    <p>u p ,p</p>
    <p>e rm</p>
    <p>,va lid</p>
    <p>)</p>
    <p>Namenode</p>
    <p>Tenant info table</p>
    <p>Replica-group info table</p>
    <p>Chunk mapping table</p>
    <p>...</p>
    <p>...</p>
    <p>Datanode1 (Buffered)</p>
    <p>...</p>
    <p>Datanode2 (Buffered)</p>
    <p>...</p>
    <p>Datanode3 (Direct IO)</p>
    <p>status</p>
    <p>Computenode</p>
    <p>FAST client Mapping cache</p>
    <p>VM VM ...</p>
    <p>R/W(chunkid)</p>
  </div>
  <div class="page">
    <p>FAST  Architecture</p>
    <p>Replication group1</p>
    <p>SR</p>
    <p>Legend: Control messages Data messages</p>
    <p>(te n a</p>
    <p>n tID</p>
    <p>,c h</p>
    <p>u n</p>
    <p>kID )</p>
    <p>IO type(g ro</p>
    <p>u p ,p</p>
    <p>e rm</p>
    <p>,va lid</p>
    <p>)</p>
    <p>Namenode</p>
    <p>Tenant info table</p>
    <p>Replica-group info table</p>
    <p>Chunk mapping table</p>
    <p>...</p>
    <p>...</p>
    <p>Datanode1 (Buffered)</p>
    <p>...</p>
    <p>Datanode2 (Buffered)</p>
    <p>...</p>
    <p>Datanode3 (Direct IO)</p>
    <p>status</p>
    <p>Computenode</p>
    <p>FAST client Mapping cache</p>
    <p>VM VM ...</p>
    <p>R/W(chunkid)</p>
  </div>
  <div class="page">
    <p>FAST  Architecture</p>
    <p>Replication group1</p>
    <p>W</p>
    <p>Legend: Control messages Data messages</p>
    <p>(te n a</p>
    <p>n tID</p>
    <p>,c h</p>
    <p>u n</p>
    <p>kID )</p>
    <p>IO type(g ro</p>
    <p>u p ,p</p>
    <p>e rm</p>
    <p>,va lid</p>
    <p>)</p>
    <p>Namenode</p>
    <p>Tenant info table</p>
    <p>Replica-group info table</p>
    <p>Chunk mapping table</p>
    <p>...</p>
    <p>...</p>
    <p>Datanode1 (Buffered)</p>
    <p>...</p>
    <p>Datanode2 (Buffered)</p>
    <p>...</p>
    <p>Datanode3 (Direct IO)</p>
    <p>status</p>
    <p>Computenode</p>
    <p>FAST client Mapping cache</p>
    <p>VM VM ...</p>
    <p>R/W(chunkid)</p>
  </div>
  <div class="page">
    <p>FAST  Disk Layout and Strategy</p>
    <p>Default-with-steal strategy  By default, random reads go to head node and</p>
    <p>sequential reads go to middle node.  Allows idle or lightly-loaded replicas to steal requests</p>
    <p>from other replicas</p>
    <p>Chain replication</p>
    <p>Chain Replication: Disk Layout and Write Policies</p>
  </div>
  <div class="page">
    <p>Initial Results  Simulation Setup</p>
    <p>Workloads:  One replication group  30 tenants, each running one workload  10 random read of 16 MB each  10 sequential read of 19 MB each  5 random write of 20 MB each  5 sequential write of 20 MB each</p>
    <p>Workload assignment  Baseline: round-robin  FAST: workload type-aware</p>
  </div>
  <div class="page">
    <p>Initial Results - Assignment</p>
    <p>Head Middle Tail</p>
    <p>Replication group</p>
    <p>Baseline: (round-robin) D1 D2 D3</p>
    <p>Replication group</p>
    <p>FAST D1 D2 D3</p>
    <p>Workloads: 10 RRs, 10 SRs, 5 RWs and 5 SWs</p>
  </div>
  <div class="page">
    <p>Initial Results - Evaluations</p>
    <p>D1 D2 D3</p>
    <p>Baseline</p>
    <p>D1 D2 D3</p>
    <p>Replication group</p>
    <p>FAST</p>
    <p>Replication group</p>
    <p>Result1: Write workloads in FAST get much better performance</p>
  </div>
  <div class="page">
    <p>Initial Results - Evaluations</p>
    <p>D1 D2 D3</p>
    <p>Baseline</p>
    <p>D1 D2 D3</p>
    <p>Replication group</p>
    <p>FAST</p>
    <p>Replication group</p>
    <p>Result2: a). All SRs in FAST get similar performance b). SRs in FAST get comparable or better performance than the baseline</p>
  </div>
  <div class="page">
    <p>Initial Results - Evaluations</p>
    <p>D1 D2 D3</p>
    <p>Baseline</p>
    <p>D1 D2 D3</p>
    <p>Replication group</p>
    <p>FAST</p>
    <p>Replication group</p>
    <p>Result3: a). All RRs in FAST get similar performance b). RRs get worse performance in FAST</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Modeling of effects of co-locating same type of workloads but with different I/O request characteristics  Failure handling for datanode and namenode  Load balancing among replication groups  Tradeoff of chunk size  System implementation</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Directs random and sequential reads to different</p>
    <p>replicas  Introduce different write policies and disk layouts</p>
    <p>for chain replication</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Related Works and Contributions  Related works</p>
    <p>QoS-based resource allocation Stonehege, Argon and Aqua</p>
    <p>Support for latency control SMART, BVT and pClock</p>
    <p>Proporitional share + limit and reservation  mClock</p>
    <p>These work typically abstract the storage</p>
    <p>device to a single block device and rely on the lower layer to deal with</p>
    <p>replications.</p>
  </div>
  <div class="page">
    <p>IOPS  1</p>
    <p>From disk specification:  Average (rotational) latency: 3.0 ms</p>
    <p>Average read seek time: 4.7 ms</p>
    <p>Average write seek time: 5.3 ms</p>
    <p>For the whole disk:  Theoretical read IOPS = 1000/(3+4.7) = 129.87</p>
    <p>Theoretical write IOPS = 1000/(3+5.3) = 120.48</p>
    <p>Measured read IOPS = 123</p>
    <p>Measured write IOPS = 222</p>
  </div>
  <div class="page">
    <p>IOPS  2 From disk specification:  Average (rotational) latency: 3.0 ms</p>
    <p>Average read seek time: 4.7 ms</p>
    <p>Average write seek time: 5.3 ms</p>
    <p>For a 10GB partition:  Theoretical read IOPS = 1000/(3+4.7*10G/146.8G) =</p>
    <p>Theoretical write IOPS = 1000/(3+5.3*10G/146.8G) = 297.53</p>
    <p>Measured read IOPS = 198</p>
    <p>Measured write IOPS = 339</p>
  </div>
  <div class="page">
    <p>RR with different think times</p>
  </div>
  <div class="page">
    <p>SR with different block size</p>
    <p>Throughput Isolation: 4k-SR: 60.538 MB/s 256k-SR: 73.755 MB/s</p>
    <p>concurrent: 4k-SR: 31.222 MB/s 256k-SR: 35.651 MB/s</p>
    <p>Throughput Isolation: 4k-SR: 60.538 MB/s 1m-SR: 73.635 MB/s</p>
    <p>concurrent: 4k-SR: 28.037 MB/s 1m-SR: 38.942 MB/s</p>
  </div>
</Presentation>
