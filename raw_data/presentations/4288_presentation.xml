<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</p>
    <p>Wan-Ting Hsu</p>
    <p>National Tsing Hua University</p>
    <p>Chieh-Kai Lin</p>
    <p>National Tsing Hua University</p>
    <p>Project page</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Textual Media</p>
    <p>People spend 12 hours everyday consuming media in 2018.</p>
    <p>eMarketer https://www.emarketer.com/topics/topic/time-spent-with-media</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Textual Media</p>
    <p>People spend 12 hours everyday consuming media in 2018.</p>
    <p>eMarketer https://www.emarketer.com/topics/topic/time-spent-with-media</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Textual Media</p>
    <p>People spend 12 hours everyday consuming media in 2018.</p>
    <p>eMarketer https://www.emarketer.com/topics/topic/time-spent-with-media</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Textual Media</p>
    <p>People spend 12 hours everyday consuming media in 2018.</p>
    <p>eMarketer https://www.emarketer.com/topics/topic/time-spent-with-media</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Text Summarization</p>
    <p>To condense a piece of text to a shorter version while maintaining the important points</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Examples of Text Summarization</p>
    <p>Article headlines</p>
    <p>Meeting minutes</p>
    <p>Movie/book reviews</p>
    <p>Bulletins (weather forecasts/stock market reports)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Examples of Text Summarization</p>
    <p>Article headlines</p>
    <p>Meeting minutes</p>
    <p>Movie/book reviews</p>
    <p>Bulletins (weather forecasts/stock market reports)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Examples of Text Summarization</p>
    <p>Article headlines</p>
    <p>Meeting minutes</p>
    <p>Movie/book reviews</p>
    <p>Bulletins (weather forecasts/stock market reports)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Examples of Text Summarization</p>
    <p>Article headlines</p>
    <p>Meeting minutes</p>
    <p>Movie/book reviews</p>
    <p>Bulletins (weather forecasts/stock market reports)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Examples of Text Summarization</p>
    <p>Article headlines</p>
    <p>Meeting minutes</p>
    <p>Movie/book reviews</p>
    <p>Bulletins (weather forecasts/stock market reports)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Automatic Text Summarization</p>
    <p>To condense a piece of text to a shorter version while maintaining the important points</p>
    <p>Extractive Summarization Abstractive Summarization</p>
    <p>select text from the article generate the summary word-by-word</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Extractive Summarization</p>
    <p>Select phrases or sentences from the source document</p>
    <p>- Shen, D.; Sun, J.-T.; Li, H.; Yang, Q.; and Chen, Z. 2007. Document summarization using conditional random fields. IJCAI 2007. - Kgebck, M., Mogren, O., Tahmasebi, N., &amp; Dubhashi, D. Extractive Summarization using Continuous Vector Space Models. EACL 2014. - Cheng, J., and Lapata, M. Neural summarization by extracting sentences and words. ACL 2016. - Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive</p>
    <p>summarization of documents. AAAI 2017</p>
    <p>Representation</p>
    <p>sentence 1</p>
    <p>sentence 2</p>
    <p>sentence 3</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Abstractive Summarization</p>
    <p>Select phrases or sentences from the source document</p>
    <p>- Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive sentence summarization. EMNLP 2015. - Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Abstractive text summarization using sequence</p>
    <p>tosequence rnns and beyond. CoNLL 2016. - Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointergenerator networks. ACL 2017. - Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. ICLR 2018. - Fan, Angela, David Grangier, and Michael Auli. Controllable abstractive summarization. arXiv preprint arXiv:1711.05217 (2017).</p>
    <p>Encoder Article</p>
    <p>Representations Decoder</p>
  </div>
  <div class="page">
    <p>Extractive summary (select sentences):  important, correct  incoherent or not concise</p>
    <p>Abstractive summary (generate word-by-word):  readable, concise  may lose or mistake some facts</p>
    <p>Unified summary:  important, correct  readable, concise</p>
    <p>Overview</p>
    <p>Motivation</p>
    <p>Italian artist Johannes Stoetter has painted two naked women to look like a chameleon.</p>
    <p>The 37-year-old has previously transformed his models into frogs and parrots but this may be his most intricate and impressive artwork to date.</p>
    <p>not concise</p>
  </div>
  <div class="page">
    <p>Extractive summary (select sentences):  important, correct  incoherent or not concise</p>
    <p>Abstractive summary (generate word-by-word):  readable, concise  may lose or mistake some facts</p>
    <p>Unified summary:  important, correct  readable, concise</p>
    <p>Overview</p>
    <p>Motivation</p>
    <p>Italian artist Johannes Stoetter has painted two naked women to look like a chameleon.</p>
    <p>The 37-year-old has previously transformed his models into frogs and parrots but this may be his most intricate and impressive artwork to date.</p>
    <p>Johannes Stoetter has previously transformed his models into frogs and parrots but this chameleon may be his most impressive artwork to date.</p>
    <p>not concise</p>
    <p>concise</p>
  </div>
  <div class="page">
    <p>Extractive summary (select sentences):  important, correct  incoherent or not concise</p>
    <p>Abstractive summary (generate word-by-word):  readable, concise  may lose or mistake some facts</p>
    <p>Unified summary:  important, correct  readable, concise</p>
    <p>Overview</p>
    <p>Motivation</p>
    <p>Italian artist Johannes Stoetter has painted two naked women to look like a chameleon.</p>
    <p>The 37-year-old has previously transformed his models into frogs and parrots but this may be his most intricate and impressive artwork to date.</p>
    <p>Johannes Stoetter has previously transformed his models into frogs and parrots but this chameleon may be his most impressive artwork to date.</p>
    <p>not concise</p>
    <p>concise</p>
    <p>Justin Bieber</p>
  </div>
  <div class="page">
    <p>Extractive summary (select sentences):  important, correct  incoherent or not concise</p>
    <p>Abstractive summary (generate word-by-word):  readable, concise  may lose or mistake some facts</p>
    <p>Unified summary:  important, correct  readable, concise</p>
    <p>Overview</p>
    <p>Motivation</p>
    <p>Italian artist Johannes Stoetter has painted two naked women to look like a chameleon.</p>
    <p>The 37-year-old has previously transformed his models into frogs and parrots but this may be his most intricate and impressive artwork to date.</p>
    <p>Johannes Stoetter has previously transformed his models into frogs and parrots but this chameleon may be his most impressive artwork to date.</p>
    <p>not concise</p>
    <p>concise</p>
    <p>Justin Bieber</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Models Extractor</p>
    <p>Method</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. AAAI 2017</p>
  </div>
  <div class="page">
    <p>Models Extractor</p>
    <p>Method</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. AAAI 2017</p>
    <p>static sentence attention</p>
  </div>
  <div class="page">
    <p>Models Extractor Abstracter</p>
    <p>Method</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. AAAI 2017</p>
    <p>Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-generator networks. ACL 2017</p>
    <p>static sentence attention</p>
  </div>
  <div class="page">
    <p>Models Extractor Abstracter</p>
    <p>Method</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. AAAI 2017</p>
    <p>Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-generator networks. ACL 2017</p>
    <p>static sentence attention</p>
    <p>dynamic word attention</p>
  </div>
  <div class="page">
    <p>Combined Attention Extractor Abstracter</p>
    <p>Method</p>
    <p>static sentence attention</p>
    <p>dynamic word attention</p>
    <p>: word index : sentence index : generated word index</p>
  </div>
  <div class="page">
    <p>Combined Attention Extractor Abstracter</p>
    <p>Method</p>
    <p>static sentence attention</p>
    <p>dynamic word attention</p>
    <p>1</p>
    <p>1</p>
    <p>: word index : sentence index : generated word index</p>
    <p>2 3</p>
    <p>Cindy is lucky. She won $1000. She is going to</p>
  </div>
  <div class="page">
    <p>Combined Attention Extractor Abstracter</p>
    <p>Method</p>
    <p>static sentence attention</p>
    <p>dynamic word attention</p>
    <p>1</p>
    <p>1</p>
    <p>: word index : sentence index : generated word index</p>
    <p>2</p>
    <p>2 3</p>
    <p>Cindy is lucky. She won $1000. She is going to</p>
    <p>4 5 6</p>
  </div>
  <div class="page">
    <p>Combined Attention Extractor Abstracter</p>
    <p>Method</p>
    <p>static sentence attention</p>
    <p>dynamic word attention</p>
    <p>1</p>
    <p>1</p>
    <p>: word index : sentence index : generated word index</p>
    <p>2 3</p>
    <p>2 3</p>
    <p>Cindy is lucky. She won $1000. She is going to</p>
    <p>4 5 6 7 8 9</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Combined Attention</p>
    <p>Our unified model combines sentence-level and word-level attentions to take advantage of both extractive and abstractive summarization approaches.</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Combined Attention</p>
    <p>Updated word attention is used for calculating the context vector and final word distribution</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Encourage Consistency</p>
    <p>We propose a novel inconsistency loss function to ensure our unified model to be mutually beneficial to both extractive and abstractive summarization.</p>
    <p>multiplied attention of top K attended words</p>
    <p>maximize</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Encourage Consistency</p>
    <p>encourage consistency of the top K attended words at each decoder time step.</p>
    <p>Sentence 1 Sentence 2 Sentence 3</p>
    <p>inconsistent</p>
    <p>consistent</p>
    <p>inconsistency loss: consistent &lt; inconsistent</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Abstractive Summarization</p>
    <p>select sentences from the article generate the summary word-by-word</p>
    <p>Training Procedures</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>3 types of loss functions:</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>3 types of loss functions:</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>3 types of loss functions:</p>
    <p>Ground Truth 1 0 1</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>3 types of loss functions:</p>
    <p>Ground Truth 1 0 1</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>Extractor Target</p>
    <p>To extract sentences with high informativity: the extracted sentences should contain information that is needed to generate an abstractive summary as much as possible.</p>
    <p>Ground-truth labels: 1. Measure the informativity of each sentence in the article by computing the</p>
    <p>ROUGE-L recall score between the sentence and the reference abstractive summary.</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. AAAI 2017</p>
  </div>
  <div class="page">
    <p>Combined Attention</p>
    <p>Extractor Abstracterstatic sentence attention dynamic word</p>
    <p>attention</p>
    <p>: word index : sentence index : generated word index</p>
    <p>Training Procedures</p>
  </div>
  <div class="page">
    <p>3 types of loss functions:</p>
    <p>Training Procedures</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>3 types of loss functions:</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>and output only those sentences. = Hard attention on the original article.</p>
    <p>simply combine the extractor and abstracter by feeding the extracted sentences to the abstracter.</p>
    <p>Extractor extracted sentences</p>
    <p>Abstracter summaryarticle</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>word-level attention</p>
    <p>minimize extractor loss and abstracter loss</p>
    <p>Extractor Abstracter summaryarticle +</p>
  </div>
  <div class="page">
    <p>Training Procedures</p>
    <p>word-level attention</p>
    <p>minimize extractor loss, abstracter loss and inconsistency loss:</p>
    <p>Extractor Abstracter summaryarticle +</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Dataset  CNN/DailyMail Dataset</p>
    <p>Train Validation Test</p>
    <p>Article-summary pairs 287,113 13,368 11,490</p>
    <p>()</p>
    <p>Article  766 words Summary  53 words</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Dataset  CNN/DailyMail Dataset</p>
    <p>Train Validation Test</p>
    <p>Article-summary pairs 287,113 13,368 11,490</p>
    <p>()</p>
    <p>Article  766 words Summary  53 words</p>
    <p>Highlight 50 words</p>
    <p>Article 700 words</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Abstractive Summarization</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Abstractive Summarization</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Abstractive Summarization</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Inconsistency Rate</p>
    <p>sentence attention and word attention in time step</p>
    <p>inconsistency step : inconsistency rate:</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Inconsistency Rate</p>
    <p>sentence attention and word attention in time step</p>
    <p>inconsistency step : inconsistency rate:</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Inconsistency Rate</p>
    <p>sentence attention and word attention in time step</p>
    <p>inconsistency step : inconsistency rate:</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Inconsistency Rate</p>
    <p>sentence attention and word attention in time step</p>
    <p>inconsistency step : inconsistency rate:</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Human Evaluation on MTurk</p>
    <p>Informativity: how well does the summary capture the important parts of the article?</p>
    <p>Conciseness: is the summary clear enough to explain everything without being redundant?</p>
    <p>Readability: how well-written (fluent and grammatical) the summary is?</p>
    <p>trap</p>
  </div>
  <div class="page">
    <p>Experiment</p>
    <p>Results  Human Evaluation</p>
    <p>Informativity: how well does the summary capture the important parts of the article?</p>
    <p>Conciseness: is the summary clear enough to explain everything without being redundant?</p>
    <p>Readability: how well-written (fluent and grammatical) the summary is?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Our Method</p>
    <p>Training Procedures</p>
    <p>Experiments and Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion and Future work</p>
    <p>Conclusion</p>
    <p>We propose a unified model combining the strength of extractive and abstractive summarization.</p>
    <p>A novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. The inconsistency loss enables extractive and abstractive summarization to be mutually beneficial.</p>
    <p>By end-to-end training of our model, we achieve the best ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>Min Sun</p>
    <p>Wen-Ting Tsu</p>
    <p>Chieh-Kai Lin</p>
    <p>Ming-Ying Lee</p>
    <p>Kerui Min</p>
    <p>Jing Tang</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
    <p>Project page</p>
    <p>Code  Test output  Supplementary material</p>
    <p>https://hsuwanting.github.io/unified_summ/</p>
  </div>
</Presentation>
