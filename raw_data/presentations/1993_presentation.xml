<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Virtualized Infrastructures, Systems, &amp; Applications</p>
    <p>Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources</p>
    <p>Yitao Chen, Kaiqi Zhao, Baoxin Li, Ming Zhao</p>
    <p>Arizona State University</p>
    <p>http://visa.lab.asu.edu</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Cloud-only approach: training models only in the cloud NOT SUFFICIENT!</p>
  </div>
  <div class="page">
    <p>Limitations of cloud-only approach</p>
    <p>Cloud is insufficient o Not enough resources when facing explosive growth of data o No fast enough to provide responsive training on real-time data</p>
    <p>Unable to exploit the increasingly powerful edge devices in training o Multi-core, GPUs, AI chips o Edge devices are not contributing to the deep learning training</p>
  </div>
  <div class="page">
    <p>Objectives</p>
    <p>Distributed learning across cloud and edge resources o Utilize potentially weak resources o Across edge devices and cloud</p>
    <p>resources</p>
    <p>Limitations of existing solutions o Data parallelism does not work</p>
    <p>for edge resources o Conventional backpropagation</p>
    <p>based training makes model parallelism difficult</p>
  </div>
  <div class="page">
    <p>Proposed approach</p>
    <p>Using synthetic gradients to achieve model parallelism o Split a large DNN model into many</p>
    <p>small models</p>
    <p>o Deploy small models on distributed and potentially weak resources</p>
    <p>o Use synthetic gradients to decouple the training of the small models</p>
    <p>Decouple</p>
  </div>
  <div class="page">
    <p>Conventional backpropagation training</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>h1 h2 X</p>
    <p>h3</p>
    <p>Forward</p>
    <p>321 Loss</p>
    <p>Label</p>
    <p>= (  )  = (  )  = (  )</p>
  </div>
  <div class="page">
    <p>Conventional backpropagation training</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>21</p>
    <p>h1 h2 X</p>
    <p>h3</p>
    <p>3</p>
    <p>Backward</p>
    <p>321 Loss</p>
    <p>=</p>
    <p>3</p>
    <p>Loss = L</p>
    <p>=</p>
    <p>=</p>
    <p>2</p>
    <p>3</p>
    <p>2 Tightly coupled</p>
    <p>Backpropagation tightly couples the layers!</p>
    <p>Decouple the layers in backpropagation by predicting the gradients!</p>
  </div>
  <div class="page">
    <p>Decouple layers using synthetic gradients</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>X Loss</p>
  </div>
  <div class="page">
    <p>Decouple layers using synthetic gradients</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>h1 X</p>
    <p>1</p>
    <p>1 Synthetic gradients</p>
    <p>Synthetic gradient Model</p>
    <p>M1</p>
    <p>Loss</p>
  </div>
  <div class="page">
    <p>Decouple layers using synthetic gradients</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>h1 X</p>
    <p>1</p>
    <p>1 Synthetic gradients</p>
    <p>Synthetic gradient Model</p>
    <p>M1</p>
    <p>Loss</p>
    <p>h2</p>
    <p>2M2</p>
    <p>2</p>
    <p>Device 1 Device 2</p>
    <p>3</p>
    <p>Device 3</p>
  </div>
  <div class="page">
    <p>Gradient prediction</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>h1</p>
    <p>1</p>
    <p>1M1 1</p>
    <p>?</p>
    <p>Find the target -&gt; 1</p>
    <p>Calculate the loss between 1 and 1</p>
    <p>Update parameters of M1</p>
    <p>Repeat Step 1 to 3 until the loss is minimized</p>
    <p>Loss X</p>
  </div>
  <div class="page">
    <p>Gradient prediction</p>
    <p>Layer1 Layer2 Layer3</p>
    <p>h1</p>
    <p>1</p>
    <p>1M1 1</p>
    <p>2</p>
    <p>h2</p>
    <p>2M2</p>
    <p>Loss X</p>
    <p>Find the target -&gt; 1</p>
    <p>Calculate the loss between 1 and 1</p>
    <p>Update parameters of M1</p>
    <p>Repeat Step 1 to 3 until the loss is minimized</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Accuracy o How accurate is the synthetic gradients approach?</p>
    <p>Convergence speed o How fast can the model reach satisfactory accuracy?</p>
    <p>Training speed o How much speedup in training when deployed?</p>
  </div>
  <div class="page">
    <p>Accuracy evaluation</p>
    <p>Using synthetic gradients to achieve model parallelism o 4-layered model  One convolutional layer and</p>
    <p>three fully-connected</p>
    <p>o 8-layered model  Five convolutional layers and</p>
    <p>three fully-connected</p>
    <p>o VGG16 o MNIST Dataset o 500K Iteration of training</p>
    <p>Effective when the model is shallow o 4-layered model: 1% o 8-layered model: 5%</p>
    <p>Less effective when the model is complex o VGG16: 15%</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Accuracy comparison</p>
    <p>Back propagation Synthet ic gradients</p>
  </div>
  <div class="page">
    <p>Convergence speed evaluation</p>
    <p>Synthetic gradients convergence speed o 4-layered model  One convolution layer and three fully</p>
    <p>connected</p>
    <p>o 8-layered model  Five convolution layer and three fully</p>
    <p>connected</p>
    <p>o MNIST Dataset o 500K Iteration of training</p>
    <p>A c c u ra</p>
    <p>c y</p>
    <p>Iterations</p>
    <p>sg validation bp validation</p>
    <p>c c u ra</p>
    <p>c y</p>
    <p>Iterations</p>
    <p>sg validation bp validation</p>
    <p>Model complexity slows down the convergence</p>
    <p>Reach 90% accuracy</p>
    <p>Reach 90% accuracy</p>
  </div>
  <div class="page">
    <p>Training speed evaluation</p>
    <p>Achieve 11% training speedup</p>
    <p>Each iteration is faster if each layer is trained in parallel</p>
    <p>L</p>
    <p>M</p>
    <p>LL</p>
    <p>M</p>
    <p>L1Data</p>
    <p>M2</p>
    <p>Container 1 Container 2 Container 3 Container 4</p>
    <p>Deploy our 4-layered model layer-wise on four Docker container instances</p>
    <p>Use gRPC for communication between instances</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Need to rethink DNN platforms to handle future learning needs o Enable model parallelism without modifying the learning models</p>
    <p>Model parallelism is critical for utilizing cloud and edge resources o Synthetic gradients is a feasible solution</p>
    <p>The use of synthetic gradients improves training speed o At the cost of some loss in accuracy and convergence speed</p>
  </div>
  <div class="page">
    <p>Open questions and future work</p>
    <p>How to improve the accuracy when using SG? o Explore different gradient prediction models</p>
    <p>How to appropriately partition a model to different devices? o Investigate performance difference between model configurations (type, width) and</p>
    <p>partition based on the available device resources</p>
    <p>How to design a parameter server suited for the SG approach? o Investigate compression techniques to minimize communication and provide fast</p>
    <p>access for SG models</p>
  </div>
  <div class="page">
    <p>Acknowledgement</p>
    <p>National Science Foundation o GEARS project: CNS-1629888 o CNS-1619653, CNS-1562837, CMMI</p>
    <p>VISA Lab @ ASU o Saman, Qirui, Runyu and other lab</p>
    <p>mates</p>
    <p>Thank you! o Questions and suggestions? o Come to our poster!</p>
    <p>http://gears.asu.edu/</p>
  </div>
</Presentation>
