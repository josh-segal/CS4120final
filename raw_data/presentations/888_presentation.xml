<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>GPUvm: Why Not Virtualizing GPUs at the Hypervisor?</p>
    <p>Yusuke Suzuki* in collaboraBon with</p>
    <p>Shinpei Kato**, Hiroshi Yamada***, Kenji Kono*</p>
    <p>* Keio University ** Nagoya University</p>
    <p>*** Tokyo University of Agriculture and Technology</p>
  </div>
  <div class="page">
    <p>Graphic Processing Unit (GPU)  GPUs are used for data-parallel computaBons  Composed of thousands of cores  Peak double-precision performance exceeds 1 TFLOPS  Performance-per-waT of GPUs outperforms CPUs</p>
    <p>GPGPU is widely accepted for various uses  Network Systems [Jang et al. 11], FS [Silberstein et al. 13]</p>
    <p>[Sun et al. 12], DBMS [He et al. 08] etc.</p>
    <p>L2 Cache L1 L1 L1 L1 L1 L1 L1</p>
    <p>Video Memory Main Memory CPU</p>
    <p>NVIDIA/GPU</p>
  </div>
  <div class="page">
    <p>MoBvaBon  GPU is not the first-class ciBzen of cloud compuBng environment  Can not mulBplex GPGPU among virtual machines (VM)  Can not consolidate VMs that run GPGPU applicaBons</p>
    <p>GPU virtualizaBon is necessary  VirtualizaBon is the norms in the clouds</p>
    <p>VM</p>
    <p>GPU</p>
    <p>VM VM</p>
    <p>Hypervisor Share a single GPU among VMs Physical</p>
    <p>Machine</p>
  </div>
  <div class="page">
    <p>VirtualizaBon Approaches</p>
    <p>Categorized into three approaches 1. I/O pass-through 2. API remoBng 3. Para-virtualizaBon</p>
  </div>
  <div class="page">
    <p>I/O pass-through</p>
    <p>Amazon EC2 GPU instance, Intel VT-d  Assign physical GPUs to VMs directly  MulBplexing is impossible</p>
    <p>VM</p>
    <p>GPU</p>
    <p>VM</p>
    <p>GPU</p>
    <p>VM</p>
    <p>Hypervisor</p>
    <p>Assign GPUs</p>
    <p>to VMs directly</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>API remoBng  GViM [Gupta et al. 09], rCUDA [Duato et al 10], VMGL [Largar-Cavilla et al. 07] etc.  Forward API calls from VMs to the hosts GPUs  API and its version compaBbility problem  Enlarge the trusted compuBng base (TCB)</p>
    <p>Wrapper Library v4</p>
    <p>VM</p>
    <p>Hypervisor</p>
    <p>GPU Forwarding API calls</p>
    <p>Wrapper Library v5</p>
    <p>VM</p>
    <p>Driver</p>
    <p>Host Library v4 Library v4</p>
  </div>
  <div class="page">
    <p>Para-virtualizaBon</p>
    <p>VMWare SVGA2 [Dowty 09] LoGV [GoEschalk et al. 10]  Expose an ideal GPU device model to VMs  Guest device driver must be modified or rewriTen</p>
    <p>Driver</p>
    <p>Host</p>
    <p>Hypervisor</p>
    <p>GPU Hypercalls</p>
    <p>Library VM</p>
    <p>PV Driver</p>
    <p>Library VM</p>
    <p>PV Driver</p>
  </div>
  <div class="page">
    <p>Goals</p>
    <p>Fully virtualize GPUs  allow mulJple VMs to share a single GPU  without any driver modificaJon</p>
    <p>Vanilla driver can be used as is in VMs  GPU runBme can be used as is in VMs</p>
    <p>IdenBfy performance boTlenecks of full virtualizaBon  GPU details are not open</p>
    <p>Library VM</p>
    <p>Driver</p>
    <p>Virtual GPU</p>
    <p>GPU</p>
    <p>Virtual GPU</p>
    <p>Library VM</p>
    <p>Driver</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>MoBvaBon &amp; Goals  GPU Internals  Proposal: GPUvm  Experiments  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>GPU Internals  PCIe connected discrete GPU (NVIDIA, AMD GPU)  Driver accesses to GPU w/ MMIO through PCIe BARs  Three major components  GPU compuJng cores, GPU channel and GPU memory</p>
    <p>MMIO</p>
    <p>GPU CompuBng Cores</p>
    <p>GPU</p>
    <p>Driver, Apps (CPU)</p>
    <p>PCIe BARs GPU</p>
    <p>Channel  GPU</p>
    <p>Channel GPU Channels</p>
    <p>GPU Memory</p>
  </div>
  <div class="page">
    <p>GPU Channel &amp; CompuBng Cores  GPU channel is a hardware unit to submit commands to GPU compuBng cores</p>
    <p>The number of GPU channels is fixed  MulBple channels can be acBve at a Bme</p>
    <p>App App</p>
    <p>GPU Channel</p>
    <p>GPU Channel</p>
    <p>GPU Commands</p>
    <p>GPU CompuBng Cores</p>
    <p>GPU</p>
    <p>Commands are executed on compuBng cores</p>
    <p>CompuBng</p>
  </div>
  <div class="page">
    <p>GPU Memory</p>
    <p>Memory accesses from compuBng cores are confined by GPU page tables App App</p>
    <p>GPU Channel</p>
    <p>GPU Channel</p>
    <p>GPU Commands</p>
    <p>GPU CompuBng Cores</p>
    <p>GPU</p>
    <p>GPU Memory</p>
    <p>Pointer to GPU Page Table</p>
    <p>GPU Virtual Address</p>
    <p>GPU Physical Address</p>
    <p>GPU Page Table</p>
    <p>GPU Page Table</p>
  </div>
  <div class="page">
    <p>Unified Address Space</p>
    <p>GPU and CPU memory spaces are unified  GPU virtual address (GVA) is translated CPU physical addresses as well as GPU physical addresses (GPA)</p>
    <p>GPU CompuBng Cores</p>
    <p>App</p>
    <p>GPU Channel</p>
    <p>GPU Commands</p>
    <p>GPU</p>
    <p>GPU Memory</p>
    <p>GVA</p>
    <p>GPA CPU Memory</p>
    <p>CPU physical address</p>
    <p>Unified Address Space</p>
    <p>GPU Page Table</p>
  </div>
  <div class="page">
    <p>DMA handling in GPU</p>
    <p>DMAs from compuBng cores are issued with GVA  Confined by GPU Page Tables</p>
    <p>DMAs must be isolated between VMs</p>
    <p>GPU CompuBng Cores</p>
    <p>App</p>
    <p>GPU Channel</p>
    <p>GPU Commands</p>
    <p>GPU</p>
    <p>GPU Memory</p>
    <p>Memcpy(GVA1, GVA2)</p>
    <p>CPU Memory</p>
    <p>GVA1 translated to GPA</p>
    <p>GVA2 translated to CPU physical address</p>
    <p>DMA</p>
    <p>GPU Page Table</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>MoBvaBon &amp; Goals  GPU Internals  Proposal: GPUvm  Experiments  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>GPUvm overview</p>
    <p>Isolate GPU channel, compuBng cores &amp; memory</p>
    <p>GPU GPU Channel</p>
    <p>GPU Channel</p>
    <p>Assigned to VM1</p>
    <p>GPU Channel</p>
    <p>GPU Channel</p>
    <p>Assigned to VM2</p>
    <p>GPU Memory</p>
    <p>Assigned to VM1</p>
    <p>Assigned to VM2</p>
    <p>VM1</p>
    <p>GPU CompuBng Cores</p>
    <p>Time Sharing</p>
    <p>Virtual GPU</p>
    <p>VM2</p>
    <p>Virtual GPU</p>
  </div>
  <div class="page">
    <p>GPUvm Architecture</p>
    <p>Expose the Virtual GPU to each VM and intercept &amp; aggregate MMIO to them</p>
    <p>Maintain Virtual GPU views and arbitrate accesses to physical GPU</p>
    <p>Host</p>
    <p>GPUvm Hypervisor</p>
    <p>GPU</p>
    <p>Library VM</p>
    <p>Driver</p>
    <p>Library VM</p>
    <p>Driver</p>
    <p>Virtual GPU Virtual GPU</p>
    <p>GPUvm</p>
    <p>Intercept MMIOs</p>
  </div>
  <div class="page">
    <p>GPUvm components</p>
  </div>
  <div class="page">
    <p>GPU Shadow Page Table</p>
    <p>Create GPU shadow page tables  Memory accesses from GPU compuBng cores are confined by GPU shadow page tables</p>
    <p>GPU Channel</p>
    <p>GPU Channel</p>
    <p>GPU Virtual GPU</p>
    <p>VM2</p>
    <p>GPU Commands  VM1</p>
    <p>GPU Channel</p>
    <p>GPU Memory</p>
    <p>GPU Shadow Page Table</p>
    <p>GPU Page Table</p>
    <p>Virtual GPU GVA</p>
    <p>Access allowed</p>
    <p>Access not allowed</p>
    <p>Keep consistency</p>
  </div>
  <div class="page">
    <p>GPU Shadow Page Table &amp; DMA</p>
    <p>DMA is also confined by GPU shadow page tables  Since DMA is issued with the GVA</p>
    <p>Other DMAs can be intercepted by MMIO handling</p>
    <p>GPU Channel</p>
    <p>GPU Channel</p>
    <p>GPU Virtual GPU</p>
    <p>VM2</p>
    <p>GPU Commands VM1</p>
    <p>GPU Channel</p>
    <p>GPU Memory</p>
    <p>Virtual GPU DMA with GVAs</p>
    <p>CPU Memory</p>
    <p>VM1 Memory VM2</p>
    <p>Memory DMA allowed</p>
    <p>GPU Shadow Page Table</p>
    <p>DMA not</p>
    <p>allowed</p>
  </div>
  <div class="page">
    <p>Channels are logically parBBoned for VMs  Maintain mappings between virtual &amp; shadow channels</p>
    <p>VM2</p>
    <p>GPU Shadow Channel</p>
    <p>Assigned to VM1 0 1 2</p>
    <p>VM1 Virtual GPU</p>
    <p>GPU Virtual Channels 0 1 2</p>
    <p>VM2 Virtual GPU</p>
    <p>GPU Virtual Channels 0 1 2</p>
    <p>VM1 64 65 66</p>
    <p>Mappings between virtual &amp; shadow channels</p>
    <p>Assigned to VM2 64 65 66</p>
    <p>GPU Shadow Channels</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>GPU Fair-Share Scheduler  Schedules non-preempBve command execuBons  Employs BAND scheduling algorithm [Kato et al. 12]  GPUvm can employ exisBng algorithms</p>
    <p>VGRIS [Yu et al. 13], Pegasus [Gupta et al. 12], TimeGraph [Kato et al. 11], Disengaged Scheduling [Menychtas et al. 14]</p>
    <p>GPU Shadow Channels GPU</p>
    <p>Assigned to VM1</p>
    <p>Assigned to VM2</p>
    <p>Time</p>
    <p>VM1 Virtual</p>
    <p>Channels VGPU</p>
    <p>VM2 Virtual</p>
    <p>Channels VGPU</p>
    <p>GPU fair-share scheduler</p>
    <p>VM1 Queue</p>
    <p>VM2 Queue</p>
  </div>
  <div class="page">
    <p>OpBmizaBon Techniques</p>
    <p>Introduce several opBmizaBon techniques to reduce overhead caused by GPUvm 1. BAR Remap 2. Lazy Shadowing 3. Para-virtualizaBon</p>
  </div>
  <div class="page">
    <p>MMIO through PCIe BARs is intercepted by GPUvm  Allow direct BAR accesses to the non-virtualizaBon-sensiBve areas</p>
    <p>Virtual BAR</p>
    <p>BAR Remap</p>
    <p>SensiBve Non-sensiBve</p>
    <p>Naive w/ BAR Remap Guest Driver</p>
    <p>MMIO</p>
    <p>GPUvm</p>
    <p>Intercepted</p>
    <p>Physical BAR</p>
    <p>Aggregated</p>
    <p>Issued</p>
    <p>Virtual BAR</p>
    <p>SensiBve Non-sensiBve</p>
    <p>Guest Driver MMIO</p>
    <p>GPUvm</p>
    <p>Intercepted</p>
    <p>Physical BAR</p>
    <p>Aggregated</p>
    <p>Issued</p>
    <p>Direct Access</p>
  </div>
  <div class="page">
    <p>TLB flush</p>
    <p>Lazy Shadowing  Page-fault-driven shadowing cannot be applied  When fault occurs, computaBon cannot be resumed</p>
    <p>Scanning enBre page tables incurs high overhead  Delay the reflecBon to the shadow page tables unBl the channel is used</p>
    <p>Naive</p>
    <p>Time</p>
    <p>Lazy Shadowing</p>
    <p>Channel becomes acBve Scan 3 Bmes</p>
    <p>Scan only once</p>
    <p>Scan</p>
    <p>Ignore</p>
    <p>Channel becomes acBve</p>
  </div>
  <div class="page">
    <p>Para-virtualizaBon</p>
    <p>Shadowing is sBll a major source of overhead  Provide para-virtualized driver  Manipulate page table entries through hypercalls (similar to Xen direct-paging)</p>
    <p>Provide a mulBcall interface that can batch several hypercalls into one (borrowed from Xen)</p>
    <p>Eliminate cost of scanning enBre page tables</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>MoBvaBon &amp; Goals  GPU Internals  Proposal: GPUvm  Experiments  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>EvaluaBon Setup</p>
    <p>ImplementaBon  Xen 4.2.0, Linux 3.6.5  Nouveau [hEp://nouveau.freedesktop.org/]</p>
    <p>Open-source device driver for NVIDIA GPUs  Gdev [Kato et al. 12]</p>
    <p>Open-source CUDA runBme</p>
    <p>Xeon E5-24700, NVIDIA Quadro6000 GPU  Schemes  NaJve: non-virtualized  FV Naive: Full-virtualizaBon w/o opBmizaBons  FV OpJmized: FV w/ opBmizaBons  PV: Para-virtualizaBon</p>
  </div>
  <div class="page">
    <p>Overhead</p>
    <p>Significant overhead over NaBve  Can be miBgated by opBmizaBon techniques  PV is faster than FV since it eliminates shadowing  PV is sBll 2-3x slower than NaBve</p>
    <p>Hypercalls, MMIO intercepBon 275.39</p>
    <p>FV Naive FV OpBmized PV NaBve</p>
    <p>madd (short term workload)</p>
    <p>Re la J ve J m e (lo</p>
    <p>g- sc al ed</p>
    <p>)</p>
    <p>Shadowing eliminated</p>
    <p>Shadowing &amp; MMIO handling reduced</p>
  </div>
  <div class="page">
    <p>FV O pB</p>
    <p>m iz ed</p>
    <p>PV</p>
    <p>N aB</p>
    <p>ve</p>
    <p>FV O pB</p>
    <p>m iz ed</p>
    <p>PV</p>
    <p>N aB</p>
    <p>ve</p>
    <p>FV O pB</p>
    <p>m iz ed</p>
    <p>PV</p>
    <p>N aB</p>
    <p>ve</p>
    <p>FV O pB</p>
    <p>m iz ed</p>
    <p>PV</p>
    <p>N aB</p>
    <p>ve</p>
    <p>Ti m e (s ec on</p>
    <p>ds )</p>
    <p>Number of GPU Contexts</p>
    <p>Kernel execuBon Bme VirtualizaBon overhead</p>
    <p>Performance at Scale</p>
    <p>FV incurs large overhead in 4- and 8- VM case  Since page shadowing locks GPU resources</p>
  </div>
  <div class="page">
    <p>Performance IsolaBon  In FIFO and CREDIT a long-running task occupies GPU</p>
    <p>BAND achieves fair-share</p>
    <p>FIFO CREDIT BAND</p>
    <p>G PU</p>
    <p>U Bl iz aB</p>
    <p>on (%</p>
    <p>)</p>
    <p>short&quot; long&quot;</p>
    <p>short&quot; long&quot;</p>
    <p>short&quot; long&quot;</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>MoBvaBon &amp; Goals  GPU Internals  Proposal: GPUvm  Experiments  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>Related Work  I/O pass-through  Amazon EC2</p>
    <p>API remoBng  GViM [Gupta et al. 09], vCUDA [Shi et al. 12], rCUDA [Duato et al 10], VMGL [Largar-Cavilla et al. 07], gVirtuS [Giunta et al. 10]</p>
    <p>Para-virtualizaBon  VMware SVGA2 [Dowty et al. 09], LoGV [GoEschalk et al. 10]</p>
    <p>Full-virtualizaBon  XenGT [Tian et al. 14]  GPU Architecture is different (Integrated Intel GPU)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>GPUvm shows the design of full GPU virtualizaBon  GPU shadow page table  GPU shadow channel  GPU fair-share scheduler</p>
    <p>Full-virtualizaBon exhibits non-trivial overhead  MMIO handling</p>
    <p>Intercept TLB flush and scan page table  OpBmizaBons and para-virtualizaBon reduce this overhead</p>
    <p>However sBll 2-3 Bmes slower</p>
  </div>
</Presentation>
