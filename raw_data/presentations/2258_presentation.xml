<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SplitKV: Splitting IO Paths for Different Sized KeyValue Items with Advanced Storage Devices</p>
    <p>Shukai Han, Dejun Jiang, Jin Xiong Institute of Computing Technology, Chinese Academy of Sciences</p>
    <p>University of Chinese Academy of Sciences</p>
    <p>HotStorage '20 JULY 13-14, 2020</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background &amp; Motivation</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Key-Value Store  Key-Value (KV) stores are widely deployed in data centers</p>
    <p>The sizes of KV items vary from a couple of bytes to hundreds of kilobytes  Facebook's analysis on Memcached's workload found that more than</p>
    <p>over 128KB in size[2] .</p>
    <p>[1] Berk, SIGMETRICS '2012 [2] Lai, MSST '2015</p>
  </div>
  <div class="page">
    <p>Conventional Storage Device based KV Store</p>
    <p>Log Structured Merge Tree is widely adopted in KV stores to convert random writes to sequential writes.</p>
    <p>Write Buffer</p>
    <p>Table Table</p>
    <p>Table Table Table</p>
    <p>Level 0</p>
    <p>Level 1</p>
    <p>Table Table TableLevel n</p>
    <p>...</p>
    <p>... ... ... ...</p>
    <p>DRAM SSD</p>
    <p>Conventional Storage Devices  Block access  Low random access performance</p>
    <p>Log-Structured Merge (LSM) Tree based KV Store</p>
  </div>
  <div class="page">
    <p>Advanced Storage Device based KV Store</p>
    <p>Advanced Storage Devices  PM:Byte access  SSD: Block access  High random access performance</p>
    <p>KVell[3] builds low CPU overhead Key-Value Store Based on Modern SSDs</p>
    <p>Some works[4] based on the low latency characteristics of PM, in which persistent buffers are built to reduce the logging overhead.</p>
    <p>Persistent Write Buffer</p>
    <p>SSD Store</p>
    <p>write</p>
    <p>flush PM SSD</p>
    <p>Optane SSD</p>
    <p>Optane DC PMM</p>
    <p>[3] Lepers, SOSP'19 [4] Kannan, ATC'18</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Random Write 64B 256B 1KB 4KB 16KB 64KB 256KB 1MB 4MB 16MB</p>
    <p>Optane SSD P3700 14.09 14.09 14.09 14.09 21.44 45.79 145.58 532 2091 8223</p>
    <p>Optane DC PMM 0.18 0.20 0.43 1.05 3.90 15.50 61.88 247 1440 6840</p>
    <p>Ratio 79.2 70.5 33.0 13.4 5.5 2.9 2.4 2.2 1.45 1.2</p>
    <p>PM is friendly to small KV items  NVM based SSD is friendly to large KV items without suffering from random access cost</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background &amp; Motivation</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>SplitKV Overview</p>
    <p>small KV items store global index</p>
    <p>Persistent Memory NVM based SSD</p>
    <p>ust_4KB ust_16KB</p>
    <p>small KV items large KV items large KV items</p>
    <p>KV items</p>
    <p>directly write batch write</p>
    <p>Key idea: Splitting IO Path for small/large KV items</p>
  </div>
  <div class="page">
    <p>SplitKV Overview</p>
    <p>small KV items store global index</p>
    <p>Persistent Memory NVM based SSD</p>
    <p>st_3</p>
    <p>st_2</p>
    <p>st_1 ust_4KB ust_16KB</p>
    <p>Reclaim PM space</p>
    <p>sort table (st)</p>
    <p>select &amp; sort flush</p>
    <p>[5] Hwang, FAST'16</p>
  </div>
  <div class="page">
    <p>SplitKV Overview</p>
    <p>small KV items store global index</p>
    <p>Persistent Memory NVM based SSD</p>
    <p>st_1</p>
    <p>st_2</p>
    <p>st_3 ust_4KB ust_16KB</p>
    <p>Global index[5] B+Tree</p>
    <p>(FAST-FAIR)</p>
    <p>index index</p>
    <p>[5] Hwang, FAST'16</p>
  </div>
  <div class="page">
    <p>Design challenges</p>
    <p>Persistent Memory NVMe SSD</p>
    <p>small KV items large KV items</p>
    <p>KV items Challenge 1: How to decide the</p>
    <p>size boundary of KV items?</p>
    <p>Challenge 2: How to handle the migration of small items?</p>
  </div>
  <div class="page">
    <p>Size Boundary of KV Items</p>
    <p>Persistent Memory</p>
    <p>NVMe SSD</p>
    <p>KV items</p>
    <p>Access Size 256B 1KB 4KB 16KB</p>
    <p>IO Path 1 1.5 4.5 15.7 27.6</p>
    <p>IO Path 2 23.4 25.4 14.8 21.3</p>
    <p>Ratio 15.8 5.7 0.9 0.8</p>
    <p>When the KV item size is large, the data is written directly to the SSD for better performance.</p>
    <p>Any KV pair whose size is equal to or greater than 4 KB is considered to be large one.</p>
    <p>IO Path 1: KV is written to PM and then migrated to SSD through a background thread. IO Path 2: KV is directly written to SSD.</p>
    <p>Write latencies (us) of different IO path</p>
  </div>
  <div class="page">
    <p>Hotness-aware KV Migration</p>
    <p>Key2 Weight:5</p>
    <p>Key:4 Weight:2</p>
    <p>Key:5 Weight:3</p>
    <p>Key:3 Weight:4</p>
    <p>Key:6 Weight:3</p>
    <p>Key:1 Weight:1</p>
    <p>Key:4 Weight:2</p>
    <p>Key:1 Weight:1</p>
    <p>Key:5 Weight:3</p>
    <p>Key:6 Weight:3</p>
    <p>batch sort table (st)</p>
    <p>flush</p>
    <p>Weight:1</p>
    <p>Average Weight = 3</p>
    <p>Average Weight = 1.5</p>
    <p>select</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background &amp; Motivation</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Experiment Setup  System and hardware configuration</p>
    <p>Server equipped with two Intel Xeon Gold 5215 CPU (2.5GHZ)  64GB memory, one Intel Optane SSD P4800 and one Intel Optane DC PMM  CentOS Linux release 7.6.1810 with 4.18.8 kernel</p>
    <p>Compared systems  RocksDBNoveLSM[4]KVell[3]</p>
    <p>Workload  YCSB with zipfan and unifrom skew  Each workload handles 128 GB data set  50% of the KV items are 256B/4KB in size 15</p>
    <p>Workload Description</p>
    <p>A 50% reads and 50% updates</p>
    <p>B 95% reads and 5% updates</p>
    <p>C 100% reads</p>
    <p>D 95% reads for latest keys and 5% inserts</p>
    <p>E 95% scan and 5% inserts</p>
    <p>F 50% reads and 50% read-modify-writes</p>
    <p>[3] Lepers, SOSP'19 [4] Kannan, ATC'18</p>
  </div>
  <div class="page">
    <p>Average Latency with Single Thread (Zipfan)</p>
    <p>zipfan A B C D E F</p>
    <p>NoveLSM 48.35 34.89 30.52 32.28 445.83 72.57</p>
    <p>RocksDB 17.47 21.82 21.72 21.13 497.02 35.19</p>
    <p>KVell 11.76 8.60 8.64 9.20 609.38 14.12</p>
    <p>SplitKV 3.81 4.65 4.56 4.56 306.65 5.05</p>
    <p>For workloads A and F, SplitKV reduces latency by 14.4x, 6.9x, and 3.1x compared to NoveLSM, RocksDB and KVell under zipfan workloads.</p>
  </div>
  <div class="page">
    <p>Average Latency with Single Thread (Zipfan)</p>
    <p>For read-intensive workloads B, C and D, SplitKV and KVell achieved better performance than NoveLSM and RocksDB due to the adoption of the global B+-Tree index.</p>
    <p>zipfan A B C D E F</p>
    <p>NoveLSM 48.35 34.89 30.52 32.28 445.83 72.57</p>
    <p>RocksDB 17.47 21.82 21.72 21.13 497.02 35.19</p>
    <p>KVell 11.76 8.60 8.64 9.20 609.38 14.12</p>
    <p>SplitKV 3.81 4.65 4.56 4.56 306.65 5.05</p>
  </div>
  <div class="page">
    <p>Average Latency with Single Thread (Zipfan)</p>
    <p>For workload E, KVell does not sort small KV items in SSD. This introduces read amplification to KVell when serving scan query by reading a plenty of blocks.</p>
    <p>zipfan A B C D E F</p>
    <p>NoveLSM 48.35 34.89 30.52 32.28 445.83 72.57</p>
    <p>RocksDB 17.47 21.82 21.72 21.13 497.02 35.19</p>
    <p>KVell 11.76 8.60 8.64 9.20 609.38 14.12</p>
    <p>SplitKV 3.81 4.65 4.56 4.56 306.65 5.05</p>
  </div>
  <div class="page">
    <p>Average Latency with Single Thread (Zipfan .vs Uniform)</p>
    <p>uniform A B C D E F NoveLSM 96.69 69.77 61.04 64.56 476.19 145.14 RocksDB 21.11 26.13 26.08 25.89 529.10 43.27</p>
    <p>KVell 17.86 14.02 13.31 13.80 670.69 23.09 SplitKV 8.81 12.78 12.77 9.22 346.02 13.87</p>
    <p>zipfan A B C D E F NoveLSM 48.35 34.89 30.52 32.28 445.83 72.57 RocksDB 17.47 21.82 21.72 21.13 497.02 35.19</p>
    <p>KVell 11.76 8.60 8.64 9.20 609.38 14.12 SplitKV 3.81 4.65 4.56 4.56 306.65 5.05</p>
    <p>Note that, the hotnessaware migration policy is difficult to figure out cold items under uniform workloads.</p>
  </div>
  <div class="page">
    <p>Throughput in YCSB with Four Threads</p>
    <p>A B C D E F</p>
    <p>N o</p>
    <p>rm .T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>Workload</p>
    <p>RocksDB KVell SplitKV</p>
    <p>A B C D E FN o</p>
    <p>rm .T</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>Workload</p>
    <p>RocksDB KVell SplitKV</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background &amp; Motivation</p>
    <p>Design</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion  Modern NVMe SSD and persistent memory provide different access</p>
    <p>features when serving small/large data.</p>
    <p>We propose SplitKV to provide different IO paths for different sized KV items for building KV stores with such advanced storage devices.</p>
    <p>The throughput of SplitKV is up to 7.9 times that of other KV stores under zipfan load skew.</p>
  </div>
  <div class="page">
    <p>THANK YOU ! Q &amp; A</p>
    <p>Author Email: hanshukai@ict.ac.cn</p>
  </div>
</Presentation>
