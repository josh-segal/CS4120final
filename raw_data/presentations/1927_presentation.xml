<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Black-Box Approach for Estimating Utilization of Polled IO Network Functions</p>
    <p>Harshit Gupta+, Abhigyan Sharma*, Alex Zelezniak*, Minsung Jang*,</p>
    <p>Umakishore Ramachandran+</p>
    <p>+Georgia Tech, *AT&amp;T Labs Research</p>
  </div>
  <div class="page">
    <p>Userspace</p>
    <p>Polled I/O for efficient Network Functions (NFs)</p>
    <p>DMA</p>
    <p>Ring buffer</p>
    <p>Network function</p>
    <p>application</p>
    <p>Packet DMA from NIC to ring buffer in userspace</p>
    <p>Thriving open-source projects (e.g. DPDK, fd.io, Open vSwitch)</p>
    <p>Polling for incoming packets (no interrupts)  No kernel processing overheads like context switching, packet copying</p>
    <p>Order of magnitude improvement in packet processing throughput</p>
    <p>Poll for packets</p>
  </div>
  <div class="page">
    <p>Polling makes util% estimation of NFs difficult  Tools like top shows cores always at 100% utilization even at zero traffic!</p>
    <p>CPU busy time</p>
    <p>Pkt process Empty poll</p>
    <p>INSIGHT: Instead of ________ use _______________ as utilization metricbusy time total time</p>
    <p>pkt process time total time</p>
  </div>
  <div class="page">
    <p>Existing approaches  Niccolini ATC12 Driver reports NIC queue occupancy</p>
    <p>Driver modification, app recompilation</p>
    <p>Trifonov TR17 NF instrumentation to output empty polls  App modification for all NFs</p>
    <p>Cao HotCloud17 Learn variation of application metrics exposed by NF (e.g. num_http_req for HTTP proxy)</p>
    <p>Application metrics known and exposed to network provider</p>
    <p>Not well suited for network providers like AT&amp;T where NF is provided as a black box by vendors</p>
  </div>
  <div class="page">
    <p>Can we learn how the CPU behaves when it is busy processing packets ?</p>
  </div>
  <div class="page">
    <p>Hardware performance counters  Are programmable, per-core CPU registers for counting CPU events  Can count 100s of events, e.g., branch (mis)predictions, cache hit/miss ...</p>
    <p>... but only few at a time up to the number of hardware registers</p>
    <p>Enable low overhead (tens of cycles) collection of program execution metrics  Are available in any Intel or AMD server CPU</p>
    <p>Number of branch predictor hits</p>
    <p>CPU event for branch predictor hits (0x005300c4)</p>
    <p>write read</p>
  </div>
  <div class="page">
    <p>High correlation of counters with load on NF  270 out of 714 events show &gt; 95% correlation with traffic load</p>
    <p>Branch instructions Branch prediction misses (midpredictions)</p>
    <p>Number of events every 500ms Intel Xeon E5. CPU, for DPDK L3FWD app 7</p>
  </div>
  <div class="page">
    <p>Hardware counter based estimator functions</p>
    <p>fE is an estimator function that maps counter values to a utilization value u</p>
    <p>Input of fE : counter values (c1, c2, , cn) of event set E = {e1, e2, , en } (E  E_ALL)</p>
    <p>Output of fE : Utilization u  [0, 100]</p>
    <p>e1 e2 en</p>
    <p>u = fE (c1, c2, ...., cn)</p>
    <p>CPU core</p>
    <p>Problem 1 : Choosing right set of events E</p>
    <p>Problem 2 : Building accurate and robust mapping function</p>
  </div>
  <div class="page">
    <p>Data-driven estimator selection  Build 3 families of estimator functions fE with |E| = 1, 2, 3 (E  E_ALL)</p>
    <p>Each fE is implemented as a linear regression model</p>
    <p>From each family select fE that minimizes average estimation error |fE(.) - u|  Evaluate error using 3-fold cross-validation</p>
  </div>
  <div class="page">
    <p>Generating the training dataset Example : Generating training data for estimator fE , E = {e1, e2, e3}</p>
    <p>NF{pkt_size : 64B, pkts_per_flow : 1000, max_tput : 10 Gbps}</p>
    <p>Test config CPU events</p>
    <p>c1 c2 . .</p>
    <p>{events: [e1, e2, e3], counters: [c1, c2, c3], util: 10%}</p>
    <p>Training datapoint Traffic : 1 Gbps</p>
    <p>Use multiple test configurations (diff packet sizes, flow lengths, payload size)  Representative of production workloads</p>
    <p>Vary traffic load for each test config (0%, 10%, 20%, ... , 100%) 10</p>
  </div>
  <div class="page">
    <p>Assumptions  Fixed hardware (same set of CPU events, BIOS settings)</p>
    <p>Run-to-completion packet processing model</p>
    <p>CPU as the bottleneck resource</p>
  </div>
  <div class="page">
    <p>Network Functions evaluated</p>
    <p>Ethernet Header</p>
    <p>IP Header</p>
    <p>TCP Header Application Data</p>
    <p>Ethernet Trailer</p>
    <p>Ethernet Header</p>
    <p>IP Header</p>
    <p>TCP Header Application Data</p>
    <p>Ethernet Trailer</p>
    <p>Ethernet Header</p>
    <p>IP Header</p>
    <p>TCP Header Application Data</p>
    <p>Ethernet Trailer</p>
  </div>
  <div class="page">
    <p>Util% estimation error of best estimators</p>
    <p>Num of input events</p>
    <p>Local estimators have lower error than universal estimators.  Estimation error increases with NF complexity (DPI &gt; L4LB &gt; L3FWD)  Using more events as input improves error  Best universal estimator has &lt;10% error</p>
  </div>
  <div class="page">
    <p>Testing on unseen network traffic profiles</p>
    <p>Util% estimation error remains below 10% for best estimators</p>
    <p>Counting events for 10 ms provides the best tradeoff between latency and accuracy</p>
    <p>Result for heterogeneous workload with mix of pkt sizes and flow lengths</p>
  </div>
  <div class="page">
    <p>Summary  Proposed polled IO utilization estimation based on hardware counters</p>
    <p>Works purely at host level for black box NFs</p>
    <p>Shows low errors (&lt; 10%) on stateless, stateful and compute-intensive NFs</p>
    <p>Universal estimators (more input events, neural network-based estimators)</p>
    <p>Additional NFs (virtualized NFs, cross-core packet processing NFs)</p>
    <p>Use cases (power management, load balancing, workload placement)</p>
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>Vision</p>
    <p>$ nf-top --cores=6,7 --events=0x005300c4,0x005300c5</p>
    <p>--mapping-function=./firewall.map</p>
    <p>Network Function utilization</p>
  </div>
  <div class="page">
    <p>Backup slides</p>
  </div>
  <div class="page">
    <p>Counting CPU event  branch prediction hits</p>
  </div>
  <div class="page">
    <p>Why not use all CPU events available ?  &gt; 700 CPU events available  A lot of them are not correlated with input traffic  no useful data  There are limited number of performance monitoring registers on CPUs</p>
    <p>Can only count so many events at once  To count 8 events for 2 sec each on 4 registers , we need (8/4)*2secs</p>
  </div>
  <div class="page">
    <p>Test configurations used  L3FWD (Stateless NF) :</p>
    <p>Diverse packet sizes (64B  512B)</p>
    <p>L4LB (Stateful NF) :</p>
    <p>Diverse flow lengths (10  1000 pkts per flow)</p>
    <p>Smallest size packets (64B for max load on NF)</p>
    <p>DPI (Compute-intensive NF) :</p>
    <p>HTTP connections of varying lengths (1 KB  1024 KB)</p>
    <p>Patterns cover IP, transport and application layer 20</p>
  </div>
  <div class="page">
    <p>Experimental testbed  Setup consists of 2 servers</p>
    <p>Server 1 : Device-under-test</p>
    <p>Server 2 : Traffic Generator</p>
    <p>2x10G network interfaces (Intel 82599ES 10 Gbps SFI/SFP+ NIC)</p>
    <p>2 CPUs (Intel Xeon E5-2680 v3).</p>
    <p>Connected by 10G switch  TRex packet generator used for generating traffic</p>
    <p>L4LB and DPI are tested using TRex stateful mode</p>
  </div>
  <div class="page">
    <p>Input events of best estimators</p>
  </div>
  <div class="page">
    <p>L3FWD L4LB</p>
    <p>DPI Universal</p>
    <p>Distribution of error across all possible estimators</p>
  </div>
</Presentation>
