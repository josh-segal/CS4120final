<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Head-to-TOE Evaluation of High Performance Sockets</p>
    <p>over Protocol Offload Engines</p>
    <p>P. Balaji W. Feng Q. Gao</p>
    <p>R. Noronha W. Yu D. K. Panda</p>
    <p>Network Based Computing Lab,</p>
    <p>Ohio State University</p>
    <p>Advanced Computing Lab,</p>
    <p>Los Alamos National Lab</p>
  </div>
  <div class="page">
    <p>Ethernet Trends</p>
    <p>Ethernet is the most widely used network architecture today</p>
    <p>Traditionally Ethernet has been notorious for performance issues</p>
    <p>Near an order-of-magnitude performance gap compared to InfiniBand, Myrinet</p>
    <p>Cost conscious architecture</p>
    <p>Relied on host-based TCP/IP for network and transport layer support</p>
    <p>Compatibility with existing infrastructure (switch buffering, MTU)</p>
    <p>Used by 42.4% of the Top500 supercomputers</p>
    <p>Key: Extremely high performance per unit cost</p>
    <p>GigE can give about 900Mbps (performance) / 0$ (cost)</p>
    <p>10-Gigabit Ethernet (10GigE) recently introduced</p>
    <p>10-fold (theoretical) increase in performance while retaining existing features</p>
    <p>Can 10GigE bridge the performance between Ethernet and InfiniBand/Myrinet?</p>
  </div>
  <div class="page">
    <p>InfiniBand, Myrinet and 10GigE: Brief Overview</p>
    <p>InfiniBand (IBA)  Industry Standard Network Architecture</p>
    <p>Supports 10Gbps and higher network bandwidths</p>
    <p>Offloaded Protocol Stack</p>
    <p>Rich feature set (one-sided, zero-copy communication, multicast, etc.)</p>
    <p>Myrinet  Proprietary network by Myricom</p>
    <p>Supports up to 4Gbps with dual ports (10G adapter announced !)</p>
    <p>Offloaded Protocol Stack and rich feature set like IBA</p>
    <p>10-Gigabit Ethernet (10GigE)  The next step for the Ethernet family</p>
    <p>Supports up to 10Gbps link bandwidth</p>
    <p>Offloaded Protocol Stack</p>
    <p>Promises a richer feature set too with the upcoming iWARP stack</p>
  </div>
  <div class="page">
    <p>Characterizing the Performance Gap  Each High Performance Interconnect has its own interface</p>
    <p>Characterizing the performance gap is no longer straight forward</p>
    <p>Portability in Application Development</p>
    <p>Portability across various networks is a must</p>
    <p>Message Passing Interface (MPI)</p>
    <p>De facto standard for Scientific Applications</p>
    <p>Sockets Interface</p>
    <p>Legacy Scientific Applications</p>
    <p>Grid-based or Heterogeneous computing applications</p>
    <p>File and Storage Systems</p>
    <p>Other Commercial Applications</p>
    <p>Sockets and MPI are the right choices to characterize the GAP</p>
    <p>In this paper we concentrate only on the Sockets interface</p>
  </div>
  <div class="page">
    <p>Presentation Overview</p>
    <p>Introduction and Motivation</p>
    <p>Protocol Offload Engines</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Interfacing with Protocol Offload Engines</p>
    <p>Application or Library</p>
    <p>Traditional Sockets Interface</p>
    <p>High Performance Sockets</p>
    <p>User-level Protocol</p>
    <p>TCP/IP</p>
    <p>Device Driver</p>
    <p>High Performance Network Adapter</p>
    <p>Network Features (e.g., Offloaded Protocol)</p>
    <p>High Performance Sockets</p>
    <p>toedev</p>
    <p>TOM</p>
    <p>Application or Library</p>
    <p>Traditional Sockets Interface</p>
    <p>TCP/IP</p>
    <p>Device Driver</p>
    <p>High Performance Network Adapter</p>
    <p>Network Features (e.g., Offloaded Protocol)</p>
    <p>TCP Stack Override</p>
  </div>
  <div class="page">
    <p>High Performance Sockets</p>
    <p>Pseudo Sockets-like Interface</p>
    <p>Smooth transition for existing sockets applications</p>
    <p>Existing applications do not have to rewritten or recompiled !</p>
    <p>Improved Performance by using the Offloaded Protocol Stack on networks</p>
    <p>High Performance Sockets exist for many networks</p>
    <p>Initial implementations on VIA</p>
    <p>Follow-up implementations on Myrinet and Gigabit Ethernet</p>
    <p>Sockets Direct Protocol is an industry standard specification for IBA</p>
    <p>Implementations by Voltaire, Mellanox and OSU exist</p>
    <p>[shah98:canpc, kim00:cluster, balaji02:hpdc]</p>
    <p>[balaji02:cluster]</p>
    <p>[balaji03:ispass, mellanox05:hoti]</p>
  </div>
  <div class="page">
    <p>TCP Stack Override</p>
    <p>Similar to the High Performance Sockets approach, but</p>
    <p>Overrides the TCP layer instead of the Sockets layer</p>
    <p>Advantages compared to High Performance Sockets</p>
    <p>Sockets features do not have to be duplicated</p>
    <p>E.g., Buffer management, Memory registration</p>
    <p>Features implemented in the Berkeley sockets implementation can be used</p>
    <p>Disadvantages compared to High Performance Sockets</p>
    <p>A kernel patch is required</p>
    <p>Some TCP functionality has to be duplicated</p>
    <p>This approach is used by Chelsio in their 10GigE adapters</p>
  </div>
  <div class="page">
    <p>Presentation Overview</p>
    <p>Introduction and Motivation</p>
    <p>High Performance Sockets over Protocol Offload Engines</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Test-bed and Evaluation</p>
    <p>4 node cluster: Dual Xeon 3.0GHz; SuperMicro SUPER X5DL8-GG nodes</p>
    <p>512 KB L2 cache; 2GB of 266MHz DDR SDRAM memory; PCI-X 64-bit 133MHz</p>
    <p>InfiniBand</p>
    <p>Mellanox MT23108 Dual Port 4x HCAs (10Gbps link bandwidth); MT43132 24-port switch</p>
    <p>Voltaire IBHost-3.0.0-16 stack</p>
    <p>Myrinet</p>
    <p>Myrinet-2000 dual port adapters (4Gbps link bandwidth)</p>
    <p>SDP/Myrinet v1.7.9 over GM v2.1.9</p>
    <p>10GigE</p>
    <p>Chelsio T110 adapters (10Gbps link bandwidth); Foundry 16-port SuperX switch</p>
    <p>Driver v1.2.0 for the adapters; Firmware v2.2.0 for the switch</p>
    <p>Experimental Results:</p>
    <p>Micro-benchmarks (latency, bandwidth, bi-dir bandwidth, multi-stream, hot-spot, fan-tests)</p>
    <p>Application-level Evaluation</p>
  </div>
  <div class="page">
    <p>Ping-Pong Latency Measurements</p>
    <p>SDP/Myrinet achieves the best small message latency at 11.3us</p>
    <p>10GigE and IBA achieve latencies of 17.7us and 24.4us respectively</p>
    <p>As message size increases, IBA performs the best  Myrinet cards are 4Gbps links right now !</p>
  </div>
  <div class="page">
    <p>Unidirectional Throughput Measurements</p>
    <p>10GigE achieves the highest throughput at 6.4Gbps</p>
    <p>IBA and Myrinet achieve about 5.3Gbps and 3.8Gbps  Myrinet is only a 4Gbps link</p>
  </div>
  <div class="page">
    <p>Snapshot Results (Apples-to-Oranges comparison) 10GigE: Opteron 2.2GHz IBA: Xeon 3.6GHz Myrinet: Xeon 3.0GHz</p>
    <p>Provide a reference point ! Only valid for THIS slide !</p>
    <p>SDP/Myrinet with MX allows polling and achieves about 4.6us latency (event-based is better for SDP/GM ~ 11.3us)</p>
    <p>10GigE achieves the lowest event-based latency of 8.9us on Opteron systems</p>
    <p>IBA achieves a 9Gbps throughput with their DDR cards (link speed of 20Gbps)</p>
  </div>
  <div class="page">
    <p>Bidirectional and Multi-Stream Throughput</p>
  </div>
  <div class="page">
    <p>Hot-Spot Latency</p>
    <p>10GigE and IBA demonstrate similar scalability with increasing number of clients</p>
    <p>Myrinets performance deteriorates faster than the other two</p>
  </div>
  <div class="page">
    <p>Fan-in and Fan-out Throughput Test</p>
    <p>10GigE and IBA achieve a similar performance for the fan-in test</p>
    <p>10GigE performs slightly better for the fan-out test</p>
  </div>
  <div class="page">
    <p>Data-Cutter Run-time Library (Software Support for Data Driven Applications)</p>
    <p>http://www.datacutter.org</p>
    <p>Designed by Univ. of Maryland</p>
    <p>Component framework</p>
    <p>User-defined pipeline of components</p>
    <p>Each component is a filter</p>
    <p>The collection of components is a filter group</p>
    <p>Replicated filters as well as filter groups</p>
    <p>Illusion of a single stream in the filter group</p>
    <p>Stream based communication</p>
    <p>Flow control between components</p>
    <p>Unit of Work (UOW) based flow control</p>
    <p>Each UOW contains about 16 to 64KB of data</p>
    <p>Several applications supported</p>
    <p>Virtual Microscope</p>
    <p>ISO Surface Oil Reservoir Simulator</p>
  </div>
  <div class="page">
    <p>Data-Cutter Performance Evaluation</p>
    <p>InfiniBand performs the best for both the data-cutter applications (especially Virtual Microscope)</p>
    <p>The filter-based approach makes the environment medium message latency sensitive</p>
  </div>
  <div class="page">
    <p>Network</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Meta-Data Manager</p>
    <p>I/O Server Node</p>
    <p>I/O Server Node</p>
    <p>I/O Server Node</p>
    <p>Meta Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Designed by ANL and Clemson University</p>
    <p>Relies on Striping of data across different nodes</p>
    <p>Tries to aggregate I/O bandwidth from multiple nodes</p>
    <p>Utilizes the local file system on the I/O Server nodes</p>
  </div>
  <div class="page">
    <p>PVFS Contiguous I/O</p>
    <p>Performance trends are similar to the throughput test</p>
    <p>Experiment is throughput intensive</p>
  </div>
  <div class="page">
    <p>MPI-Tile I/O (PVFS Non-contiguous I/O)</p>
    <p>10GigE and IBA perform quite equally</p>
    <p>Myrinet is very close behind inspite of being only a 4Gbps network</p>
  </div>
  <div class="page">
    <p>Ganglia Cluster Management Infrastructure</p>
    <p>Cluster Nodes</p>
    <p>Management Node</p>
    <p>connect()Request Information</p>
    <p>Receive Data</p>
    <p>Developed by UC Berkeley</p>
    <p>For each transaction, one connection and one medium sized message (~6KB) transfer is required</p>
    <p>Ganglia Daemon</p>
  </div>
  <div class="page">
    <p>Ganglia Performance Evaluation</p>
    <p>Performance is dominated by connection time  IBA, Myrinet take about a millisecond to establish connections while 10GigE takes about 60us</p>
    <p>Optimizations for SDP/IBA and SDP/Myrinet such as connection caching are possible (being implemented)</p>
  </div>
  <div class="page">
    <p>Presentation Overview</p>
    <p>Introduction and Motivation</p>
    <p>High Performance Sockets over Protocol Offload Engines</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Concluding Remarks</p>
    <p>Ethernet has traditionally been notorious for performance reasons</p>
    <p>Close to an order-of-magnitude performance gap compared to IBA/Myrinet</p>
    <p>10GigE: Recently introduced as a successor in the Ethernet family</p>
    <p>Can 10GigE help bridge the gap for Ethernet with IBA/Myrinet?</p>
    <p>We showed comparisons between Ethernet, IBA and Myrinet</p>
    <p>Sockets Interface was used for the comparison</p>
    <p>Micro-benchmark as well as application-level evaluations have been shown</p>
    <p>10GigE performs quite comparably with the IBA and Myrinet</p>
    <p>Better in some cases and worse in others; but around the same ballpark</p>
    <p>Quite ubiquitous in Grid and WAN environments</p>
    <p>Comparable performance in SAN environments</p>
  </div>
  <div class="page">
    <p>Continuing and Future Work</p>
    <p>Sockets is only one end of the comparison chart  Other middleware are quite widely used too (e.g., MPI)</p>
    <p>IBA/Myrinet might have an advantage due to RDMA/multicast capabilities</p>
    <p>Network interfaces and software stacks change  Myrinet coming out with 10Gig adapters</p>
    <p>10GigE might release a PCI-Express based card</p>
    <p>IBA has a zero-copy sockets interface for improved performance</p>
    <p>IBMs 12x InfiniBand adapters increase the performance of IBA by 3 fold</p>
    <p>These results keep changing with time; more snapshots needed for fairness</p>
    <p>Multi-NIC comparison for Sockets/MPI  Awful lot of work at the host</p>
    <p>Scalability might be bound by the host</p>
    <p>iWARP compatibility and features for 10GigE TOE adapters</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>Our research is supported by the following organizations</p>
    <p>Funding support by</p>
    <p>Equipment donations by</p>
  </div>
  <div class="page">
    <p>Web Pointers</p>
    <p>Network Based Computing</p>
    <p>Laboratory</p>
    <p>Websites:</p>
    <p>http://nowlab.cse.ohio-state.edu</p>
    <p>http://public.lanl.gov/radiant/index.html</p>
    <p>Emails:</p>
    <p>balaji@cse.ohio-state.edu</p>
    <p>feng@lanl.gov</p>
    <p>panda@cse.ohio-state.edu</p>
    <p>NBCL</p>
    <p>LANL</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Regular Ethernet adapters</p>
    <p>Layer-2 adapters</p>
    <p>Rely on host-based TCP/IP to provide network/transport functionality</p>
    <p>Could achieve a high performance with optimizations</p>
    <p>TCP Offload Engines (TOEs)</p>
    <p>Layer-4 adapters</p>
    <p>Have the entire TCP/IP stack offloaded on to hardware</p>
    <p>Sockets layer retained in the host space</p>
    <p>iWARP-aware adapters</p>
    <p>Layer-4 adapters</p>
    <p>Entire TCP/IP stack offloaded on to hardware</p>
    <p>Support more features than TCP Offload Engines</p>
    <p>No sockets ! Richer iWARP interface !</p>
    <p>E.g., Out-of-order placement of data, RDMA semantics</p>
    <p>[feng03:hoti, feng03:sc, balaji03:rait]</p>
    <p>[balaji05:hoti]</p>
  </div>
</Presentation>
