<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>NAVER Machine Translation System for WAT 2015</p>
    <p>Hyoung-Gyu Lee, Jae-Song Lee, Jun-Seok Kim and Chang-Ki Lee</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Introduction</p>
    <p>English-to-Japanese MT Task</p>
    <p>Korean-to-Japanese MT Task</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Introduction</p>
  </div>
  <div class="page">
    <p>Traditional SMT and Neural MT</p>
    <p>Traditional SMT Traditional SMT + Neural Network Neural MT</p>
    <p>a few year ago recently more recently</p>
    <p>Source Sentence</p>
    <p>Traditional SMT (PB/HPB/T2S/F2S)</p>
    <p>Target Sentence</p>
    <p>Source Sentence</p>
    <p>Target Sentence</p>
    <p>Traditional SMT</p>
    <p>Neural Net</p>
    <p>Source Sentence</p>
    <p>Target Sentence</p>
    <p>Traditional SMT</p>
    <p>Neural Net</p>
    <p>Source Sentence</p>
    <p>Neural Network</p>
    <p>Target Sentence</p>
  </div>
  <div class="page">
    <p>Neural Machine Translation</p>
    <p>Proposed by Google and Montreal University in 2014  Is called</p>
    <p>Sequence-to-sequence model  End-to-end model</p>
    <p>Input sentence is encoded into fix-length vector, and from the vector translated sentence is produced. Thats all</p>
    <p>Various extensions is emerged  LSTM, GRU, Bidirectional Encoding, Attention Mechanism,</p>
  </div>
  <div class="page">
    <p>Pros and Cons of NMT</p>
    <p>Pros Cons</p>
    <p>no need domain knowledge  no need to store explicit TM and LM  Can jointly train multiple features  Can implement decoder easily</p>
    <p>Is time consuming to train NMT model  Is slow in decoding, if target vocab. is large  Is weak to OOV problem  Is difficult to debug</p>
  </div>
  <div class="page">
    <p>At WAT 2015</p>
    <p>Two tasks</p>
    <p>Methods of MT</p>
    <p>EnglishJapanese</p>
    <p>MT</p>
    <p>KoreanJapanese</p>
    <p>MT</p>
    <p>Traditional SMT +</p>
    <p>Neural MT</p>
    <p>Traditional SMT</p>
    <p>Neural MT</p>
  </div>
  <div class="page">
    <p>English-to-Japanese</p>
    <p>Machine Translation Task</p>
  </div>
  <div class="page">
    <p>Outline of ENG-JPN MT Task</p>
    <p>OOV</p>
    <p>Handling</p>
    <p>T2S Syntax</p>
    <p>based MT</p>
    <p>NMT</p>
    <p>Re-ranking</p>
    <p>n-best</p>
    <p>Training Corpus</p>
    <p>Source Vocab.</p>
    <p>English sentence</p>
    <p>Japanese sentence</p>
  </div>
  <div class="page">
    <p>Tree-to-String Syntax-based MT</p>
    <p>Training Corpus  Translation model :</p>
    <p>1 million sentence pairs (train-1.txt)  Language model :</p>
    <p>3 million Japanese sentences (train-1.txt, train-2.txt)</p>
    <p>Tokenizer</p>
    <p>English: Moses tokenizer  Japanese: In-house tokenizer and POS tagger</p>
    <p>T2S model</p>
    <p>Assign linguistic syntax label to X hole of HPB model  Use Berkeley parser</p>
  </div>
  <div class="page">
    <p>Tree-to-String Syntax-based MT 2/2</p>
    <p>Rule Augmentation  Proposed by CMUs venugopal and Zollmann in 2006  Extract more rules by modifying parse trees  Use relax-parser in Moses toolkit (option: SAMT 2)</p>
    <p>I love you (0) (1) (2)</p>
    <p>PRP VBP PRP</p>
    <p>NP NP</p>
    <p>VP</p>
    <p>S</p>
    <p>Baseline nodes Additional nodes</p>
  </div>
  <div class="page">
    <p>Handling OOV</p>
    <p>Use open source spell checker, Aspell</p>
    <p>VLSI  H2  remrakable</p>
    <p>detection remrakable remarkable</p>
    <p>correction</p>
    <p>[Suggestion by Aspell]</p>
    <p>Detection Phrase  Based on skip rules  Skip the word containing capital, number or symbol</p>
    <p>Correction Phrase</p>
    <p>Based on edit distance  Because large gap causes wrong correction  Select one with shortest distance among top-3</p>
    <p>suggestion</p>
  </div>
  <div class="page">
    <p>Neural Machine Translation (1/2)</p>
    <p>RNN with an attention mechanism [Bahdanau, 2015]</p>
    <p>Tokenization English: word-level Japanese: char-level</p>
    <p># of vocab. English: 245k Japanese: 6k</p>
    <p>BI representation Use Ex)  =&gt; /B /I /I</p>
    <p>Dim. of word-embedding 200</p>
    <p>Size of recurrent unit 1000</p>
    <p>Optimization Stochastic gradient descent(SGD)</p>
    <p>Drop-out Dont use</p>
    <p>Time of training 10 days (4 epoch)</p>
  </div>
  <div class="page">
    <p>Neural Machine Translation (2/2)</p>
    <p>[ Modified RNN ]</p>
    <p>New hidden state of the decoder</p>
    <p>Prob. of the next target word</p>
  </div>
  <div class="page">
    <p>Experimental Results (T2S Syntax-based MT)</p>
    <p>SYS BLEU #Rules</p>
    <p>T2S SB MT 31.34 250M</p>
    <p>+ Rule augmentation 32.48 1950M</p>
    <p>+ Parameter modification 32.63 1950M</p>
    <p>+ OOV handling 32.76 1950M</p>
    <p>Rule augmentation increases both BLEU and #Rules  OOV handling improves the performance</p>
  </div>
  <div class="page">
    <p>Experimental Results (Neural MT)</p>
    <p>NMT Model BLEU</p>
    <p>RNN (target word-level) 29.78</p>
    <p>RNN (target char-level) 31.25</p>
    <p>RNN (target char-level with BI) 32.05</p>
    <p>Modified RNN (target char-level with BI) 33.14</p>
    <p>Char-level of target language is better than word-level  BI representation is helpful  Modified RNN is better than original RNN</p>
  </div>
  <div class="page">
    <p>Experimental Results (/w Human evaluation)</p>
    <p>SYS ENG-JPN</p>
    <p>BLEU Human</p>
    <p>T2S SB MT* only 32.76</p>
    <p>NMT** only 33.14 48.50</p>
    <p>T2S SB MT* + NMT** re-ranking 34.60 53.25</p>
    <p>T2S SB MT* : Rule augmentation + Parameter modification + OOV handling  NMT** : Modified NMT using target char. seg. with B/I</p>
    <p>NMT only outperform T2S SB MT  NMT re-ranking gives the best</p>
  </div>
  <div class="page">
    <p>Korean-to-Japanese</p>
    <p>Machine Translation Task</p>
  </div>
  <div class="page">
    <p>Outline of KOR-JPN MT Task</p>
    <p>Char-level PBMT</p>
    <p>Decoding</p>
    <p>Word-level PBMT</p>
    <p>Decoding</p>
    <p>NMT Re-ranking N-best</p>
    <p>Korean sentence</p>
    <p>Japanese sentence</p>
    <p>Training Corpus</p>
  </div>
  <div class="page">
    <p>Phrase-based MT system</p>
    <p>Training Corpus  Translation model &amp; Language model</p>
    <p>1 million sentence pairs (JPO corpus)</p>
    <p>Word-level PB MT</p>
    <p>use Mecab-ko and Juman for tokenization  5-gram LM</p>
    <p>Char-level PB MT  tokenize Korean and Japanese into char-level  10-gram LM  Max-phrase length : 10</p>
  </div>
  <div class="page">
    <p>Neural Machine Translation</p>
    <p>RNN using attention mechanism [Bahdanau, 2015]</p>
    <p>Tokenization Korean: word-level Japanese: char-level</p>
    <p># of vocab. Korean: 60k Japanese: 5k</p>
    <p>BI representation Use Ex)  =&gt; /B /I /I</p>
    <p>Dim. of word-embedding 200</p>
    <p>Size of recurrent unit 1000</p>
    <p>Optimization Stochastic gradient descent(SGD)</p>
    <p>Drop-out Dont use</p>
    <p>Time of training 10 days (4 epoch)</p>
  </div>
  <div class="page">
    <p>Combination of PBMT+ NMT</p>
    <p>Rule-based  Choose the result of char-based PB if there is OOV in word-level  Choose the result of word-based PB, otherwise</p>
    <p>NMT-based</p>
    <p>Re-rank simply by NMT score</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>SYS BLEU</p>
    <p>Word PB 70.36</p>
    <p>Character PB 70.31</p>
    <p>Word PB + Character PB 70.91</p>
    <p>Character-level PB is comparable to Word-level PB  Combined system has the best result</p>
  </div>
  <div class="page">
    <p>Experimental Results (/w human evaluation)</p>
    <p>SYS KOR-JPN</p>
    <p>BLEU Human</p>
    <p>Word PB + Character PB 70.91 6.75</p>
    <p>NMT only 65.72</p>
    <p>Word PB + Character PB + NMT re-ranking</p>
    <p>NMT only doesnt outperform PBMT  NMT re-ranking gives the best</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>We apply different MT models for each task</p>
    <p>T2S/PB SMT + NMT Re-ranking is best in both tasks</p>
    <p>Char-level tokenization of target language is useful for NMT  Speed up the time of training  Vanish OOV problem  Give the better BLEU score</p>
    <p>BI representation of char-level tokenization is helpful also for NMT</p>
    <p>In the future, we will apply our method to other language-pair;</p>
    <p>CHN-JPN</p>
  </div>
</Presentation>
