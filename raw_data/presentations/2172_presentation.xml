<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ZEA, A Data Management Approach for SMR</p>
    <p>Adam Manzanares</p>
  </div>
  <div class="page">
    <p>Western Digital Research  Cyril Guyot, Damien Le Moal, Zvonimir Bandic</p>
    <p>University of California, Santa Cruz  Noah Watkins, Carlos Maltzahn</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 2</p>
    <p>Co-Authors</p>
  </div>
  <div class="page">
    <p>Why SMR ?</p>
    <p>HDDs are not going away  Exponential growth of data still exists  $/TB vs. Flash is still much lower  We want to continue this trend!</p>
    <p>Traditional Recording (PMR) is reaching scalability limits  SMR is a density enhancing technology being shipped right now.</p>
    <p>Future recording technologies may behave like SMR  Write constraint similarities  HAMR</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 3</p>
  </div>
  <div class="page">
    <p>Flavors of SMR</p>
    <p>SMR Constraint  Writes must be sequential to avoid data loss</p>
    <p>Drive Managed  Transparent to the user  Comes at the cost of predictability and additional drive resources</p>
    <p>Host Aware  Host is aware of SMR working model  If user does something wrong the drive will fix the problem internally</p>
    <p>Host Managed  Host is aware of SMR working model  If user does something wrong the drive will reject the IO request</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 4</p>
  </div>
  <div class="page">
    <p>SMR Drive Device Model</p>
    <p>New SCSI standard Zoned Block Commands (ZBC)  SATA equivalent ZAC</p>
    <p>Drive described by zones and their restrictions</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 5</p>
    <p>Type Write Restriction</p>
    <p>Intended Use Con Abbreviation</p>
    <p>Conventional None In-place updates Increased Resources</p>
    <p>CZ</p>
    <p>Sequential Preferred</p>
    <p>None Mostly sequential writes</p>
    <p>Variable Performance</p>
    <p>SPZ</p>
    <p>Sequential Required</p>
    <p>Sequential Write Only sequential writes</p>
    <p>SRZ</p>
    <p>Our user space library (libzbc) queries zone information from the drive  https://github.com/hgst/libzbc</p>
  </div>
  <div class="page">
    <p>Why Host Managed SMR ?</p>
    <p>We are chasing predictability  Large systems with complex data flows  Demand latency windows that are relatively tight from storage devices  Translates into latency guarantees from the larger system</p>
    <p>Drive managed HDDs  When is GC triggered  How long does GC take</p>
    <p>Host Aware HDDs  Seem ok if you do the right thing  Degrade to drive managed when you dont</p>
    <p>Host Managed  Complexity and variance of drive internalized schemes are now gone</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 6</p>
  </div>
  <div class="page">
    <p>Host Managed SMR seems great, but</p>
    <p>Applications (processes)</p>
    <p>VFS</p>
    <p>Request-based device mapper targets</p>
    <p>dm-multipath</p>
    <p>Physical devices</p>
    <p>HDD SSD DVD drive</p>
    <p>Micron PCIe card</p>
    <p>LSI RAID</p>
    <p>Adaptec RAID</p>
    <p>Qlogic HBA</p>
    <p>Emulex HBA</p>
    <p>malloc</p>
    <p>BIOs (block I/Os)</p>
    <p>sysfs (transport attributes) SCSI upper level drivers</p>
    <p>/dev/sda</p>
    <p>scsi-mq</p>
    <p>.../dev/sd*</p>
    <p>SCSI low level drivers megaraid_sas</p>
    <p>aacraid</p>
    <p>qla2xxx ...libata</p>
    <p>ahci ata_piix ... lpfc</p>
    <p>Transport classes scsi_transport_fc</p>
    <p>scsi_transport_sas</p>
    <p>scsi_transport_...</p>
    <p>/dev/vd*</p>
    <p>virtio_blk mtip32xx</p>
    <p>/dev/rssd*</p>
    <p>The Linux Storage Stack Diagram http://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram</p>
    <p>Created by Werner Fischer and Georg Schnberger License: CC-BY-SA 3.0, see http://creativecommons.org/licenses/by-sa/3.0/</p>
    <p>ext2 ext3</p>
    <p>btrfs</p>
    <p>ext4 xfs</p>
    <p>ifs iso9660</p>
    <p>...</p>
    <p>NFS coda Network FS</p>
    <p>gfs ocfs</p>
    <p>smbfs ...</p>
    <p>Pseudo FS Special purpose FSproc sysfs</p>
    <p>futexfs</p>
    <p>usbfs ...</p>
    <p>tmpfs ramfs</p>
    <p>devtmpfs pipefs</p>
    <p>network</p>
    <p>nvme device</p>
    <p>The Linux Storage Stack Diagram version 4.0, 2015-06-01</p>
    <p>outlines the Linux storage stack as of Kernel version 4.0</p>
    <p>mmap (anonymous pages)</p>
    <p>iscsi_tcp</p>
    <p>network</p>
    <p>/dev/rbd*</p>
    <p>Block-based FS</p>
    <p>re a d (2</p>
    <p>)</p>
    <p>w ri</p>
    <p>te (2</p>
    <p>)</p>
    <p>o p e n (2</p>
    <p>)</p>
    <p>s ta</p>
    <p>t( 2</p>
    <p>)</p>
    <p>c h m</p>
    <p>o d (2</p>
    <p>)</p>
    <p>.. .</p>
    <p>Page cache</p>
    <p>mdraid ...</p>
    <p>stackable</p>
    <p>Devices on top of normal block devices drbd</p>
    <p>(optional)</p>
    <p>LVM BIOs (block I/Os)</p>
    <p>BIOs BIOs</p>
    <p>Block Layer</p>
    <p>multi queue</p>
    <p>blkmq</p>
    <p>Software queues</p>
    <p>Hardware dispatch queues</p>
    <p>...</p>
    <p>...</p>
    <p>hooked in device drivers (they hook in like stacked devices do)</p>
    <p>BIOs</p>
    <p>Maps BIOs to requests</p>
    <p>deadline</p>
    <p>cfq noop</p>
    <p>I/O scheduler</p>
    <p>Hardware dispatch queue</p>
    <p>Request based drivers</p>
    <p>BIO based drivers</p>
    <p>Request based drivers</p>
    <p>ceph</p>
    <p>struct bio - sector on disk</p>
    <p>- bio_vec cnt - bio_vec index - bio_vec list</p>
    <p>- sector cnt</p>
    <p>F ib</p>
    <p>re C</p>
    <p>h a n n e l</p>
    <p>o v e r</p>
    <p>E th</p>
    <p>e rn</p>
    <p>e t</p>
    <p>LIO</p>
    <p>target_core_mod</p>
    <p>tc m</p>
    <p>_ fc</p>
    <p>F ir</p>
    <p>e W</p>
    <p>ir e</p>
    <p>IS C</p>
    <p>S I</p>
    <p>Direct I/O (O_DIRECT)</p>
    <p>device mapper</p>
    <p>network</p>
    <p>is c s i_</p>
    <p>ta rg</p>
    <p>e t_</p>
    <p>m o d</p>
    <p>s b p _ ta</p>
    <p>rg e t</p>
    <p>target_core_file</p>
    <p>target_core_iblock</p>
    <p>target_core_pscsi</p>
    <p>vfs_writev, vfs_readv, ...</p>
    <p>dm-crypt dm-mirror dm-thindm-cache</p>
    <p>tc m</p>
    <p>_ q la</p>
    <p>tc m</p>
    <p>_ u s b _ g a d g e t</p>
    <p>U S B</p>
    <p>F ib</p>
    <p>re C</p>
    <p>h a n n e l</p>
    <p>tc m</p>
    <p>_ v h o s t</p>
    <p>V ir</p>
    <p>tu a l H</p>
    <p>o s t</p>
    <p>/dev/nvme*n*</p>
    <p>SCSI mid layer</p>
    <p>virtio_pci</p>
    <p>LSI 12Gbs SAS HBA</p>
    <p>mpt3sas</p>
    <p>bcache</p>
    <p>/dev/nullb*</p>
    <p>vmw_pvscsi</p>
    <p>/dev/skd*</p>
    <p>skd</p>
    <p>stec device</p>
    <p>virtio_scsi</p>
    <p>para-virtualized SCSI</p>
    <p>VMware's para-virtualized</p>
    <p>SCSI</p>
    <p>target_core_user</p>
    <p>unionfs FUSE</p>
    <p>/dev/mmcblk*p*</p>
    <p>dm-raid</p>
    <p>/dev/sr* /dev/st*</p>
    <p>pm8001</p>
    <p>PMC-Sierra HBA</p>
    <p>SD-/MMC-Card</p>
    <p>/dev/rsxx*</p>
    <p>rsxx</p>
    <p>IBM flash adapter</p>
    <p>/dev/zram*</p>
    <p>memory</p>
    <p>null_blk</p>
    <p>ufs</p>
    <p>userspace</p>
    <p>ecryptfs</p>
    <p>Stackable FS</p>
    <p>mobile device flash memory</p>
    <p>nvme</p>
    <p>overlayfs</p>
    <p>userspace (e.g. sshfs)</p>
    <p>mmcrbdzram</p>
    <p>dm-delay</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 7</p>
    <p>All of these layers must be SMR compatible  Any IO reordering causes a problem  Must not happen at any point between the user and device</p>
    <p>What about my new SMR FS?  What is your GC policy?  Is it tunable?  Does your FS introduce latency at your discretion?</p>
    <p>What about my new KV that is SMR compatible?  See above  In addition, is it built on top of a file system?</p>
  </div>
  <div class="page">
    <p>What Did We Do ?</p>
    <p>Introduce a zone and block based extent allocator [ ZEA ]  Write Logical Extent [ Zone Block Address ]  Return Drive LBA</p>
    <p>Read Extent [ Logical Block Address ]  Return data if extent is valid</p>
    <p>Iterators over extent metadata  Allows one to build ZBA -&gt; LBA mapping</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 8</p>
    <p>ZEA + SMR Drive</p>
    <p>Per Write Metadata</p>
  </div>
  <div class="page">
    <p>ZEA Performance vs. Block Device</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 9</p>
  </div>
  <div class="page">
    <p>ZEA Is Just An Extent Allocator, What Next ?</p>
    <p>ZEA + LevelDB  LevelDB is KV store library that is widely used  LevelDB Backend API is good for SMR  Write File Append Only, Read Randomly,</p>
    <p>Create &amp; Delete Files, Flat Directory  Lets Build a Simple FS compatible</p>
    <p>with LevelDB</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 10</p>
    <p>ZEA + LevelDB Architecture</p>
  </div>
  <div class="page">
    <p>ZEA + LevelDB Performance</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 11</p>
  </div>
  <div class="page">
    <p>Lessons Learned</p>
    <p>ZEA is a lightweight abstraction  Hides sequential write constraint from application  Low overhead vs. a block device when extent size is reasonable  Provides addressing flexibility to application  Useful for GC</p>
    <p>LevelDB integration opens up usability of Host Managed SMR HDD  Unfortunately LevelDB not so great for large objects  Ideal Use case for SMR drive would be large static blobs</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 12</p>
  </div>
  <div class="page">
    <p>What Is Left ?</p>
    <p>What is a good interface above ZEA  Garbage collection policies  When and How</p>
    <p>How to use multiple zones efficiently  Allocation  Garbage collection</p>
    <p>Thanks for listening</p>
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 13</p>
  </div>
  <div class="page">
    <p>2016 Western Digital Corporation or affiliates. All rights reserved. 14</p>
    <p>adam.manzanares@hgst.com</p>
  </div>
</Presentation>
