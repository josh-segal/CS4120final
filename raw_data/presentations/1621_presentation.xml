<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Ram Kesavan, Matthew Curtis-Maury, Vinay Devadas, Kesari Mishra</p>
    <p>Storage Gardening: Using a Virtualization Layer for Efficient Defragmentation in the</p>
    <p>WAFL File System</p>
    <p>NetApp Inc.</p>
  </div>
  <div class="page">
    <p>Outline  Some WAFL Background  Defragmentation techniques in WAFL</p>
    <p>Multi-core systems, multiple storage devices of different media  Trade-offs for enterprise-grade system  Multiple applications and use-cases  All features (eg. snapshots) must be preserved</p>
    <p>Evaluation &amp; some history</p>
  </div>
  <div class="page">
    <p>Forms of Fragmentation  File layout fragmentation</p>
    <p>Impacts sequential read performance  Mitigation: predictive/speculative readahead algorithms  Unavoidable: more reads IOs</p>
    <p>Free space fragmentation  Impacts write throughput of file/storage system  Impacts file layout</p>
    <p>Intra-block fragmentation  WAFL supports sub-block compaction &amp; addressing  Impacts achieved storage efficiency</p>
    <p>Briefly discussed: Objects in an object tier</p>
  </div>
  <div class="page">
    <p>WAFL Background  Runs in the kernel space of proprietary OS: ONTAP</p>
    <p>Multi-core, multi-protocol (NFS, SMB, SCSI, NVMe, etc.), multi-workload</p>
    <p>Multi-media: HDDs, SSDs, object store, cloud, flash devices, and more</p>
    <p>WAFL is feature-rich</p>
    <p>Typical deployment:</p>
    <p>One node: 100s of FlexVols (file systems), 100s of TiB, 1000s of LUNs, with dozens of applications</p>
    <p>Several features enabled: snapshots, file cloning, file system cloning, replication, dedupe, compression, mobility, encryption, etc.</p>
    <p>Copy-On-Write: so has the potential to fragment</p>
    <p>Storage gardening techniques</p>
  </div>
  <div class="page">
    <p>FlexVols &amp; Aggregates  Aggregate: pool of storage  FlexVol: namespace exported to</p>
    <p>clientsfiles, dirs, LUNs  Each FlexVol &amp; Aggr is a WAFL</p>
    <p>file system  tree of blocks rooted at</p>
    <p>superblock, leaves of tree contain data of user files &amp; metafiles</p>
    <p>Each FlexVol stored as a container file</p>
    <p>... ... RG of HDDs RG of SSDs Object store</p>
    <p>aggrA</p>
    <p>FlexVol1 FlexVol2 ... FlexVoln</p>
    <p>Container File 1</p>
    <p>Container File 2</p>
    <p>Container File n</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>FlexVols &amp; Aggregates</p>
    <p>4KB block number spaces:  V in FlexVol  P in aggregate</p>
    <p>FlexVol structures cache P  For performance</p>
    <p>V-&gt;P indirection used for several features in WAFL  Also for storage gardening</p>
    <p>techniques</p>
    <p>P1</p>
    <p>V1</p>
    <p>P2</p>
    <p>V2</p>
    <p>Container file P1</p>
    <p>P2</p>
    <p>File A</p>
    <p>P1V1</p>
    <p>File B</p>
    <p>P2V2</p>
    <p>FlexVol</p>
    <p>Aggr</p>
  </div>
  <div class="page">
    <p>Free Space Fragmentation</p>
    <p>Results in &quot;partial&quot; stripe writes  More reads &amp; compute for xor/parity,</p>
    <p>more writeIOs, poor file layout</p>
    <p>Latency of write-op not directly affected  Acked after logging to fast NVRAM</p>
    <p>WAFL checkpoints ~GBs of dirty content  Should complete in few seconds</p>
    <p>Lower device write throughput =&gt; longer checkpoint</p>
    <p>Impacts op latency &amp; IOPS throughput</p>
    <p>D1 D2 D3 P</p>
    <p>Used Block Free Block</p>
  </div>
  <div class="page">
    <p>WAFL: Segment Cleaning  Each block stored with WAFL context</p>
    <p>For protection against lost writes  Identifies file + offset  FlexVol block =&gt; container file</p>
    <p>Loaded as dirty container leaf block  Rewritten (moved) by next CP</p>
    <p>Preserves snapshots in FlexVol  More efficient</p>
    <p>Lazy fixup of stale cached P  WAFL context used to catch stale P  Read redirected to container file</p>
    <p>CSC: JIT cleaning of segments</p>
    <p>Used Free D1 D4D2 D3</p>
    <p>P1</p>
    <p>P1 P2 P3</p>
    <p>P4</p>
    <p>P4 P5</p>
    <p>P2</p>
    <p>P5</p>
    <p>P3</p>
  </div>
  <div class="page">
    <p>CSC Evaluation: Summary  All-HDD:</p>
    <p>With CSC: Op latency &amp; write chain length stabilize after 35 days  SSD+HDD:</p>
    <p>Hot spots of working set stay in SSD tier; SSDs fragment quickly  HDD write chain length stabilizes after 22 days  CSC in SSD-tier: Ameliorates flash wear-out (early gen. SSDs)</p>
    <p>All-SSD:  Expectation of high &amp; consistent performance: CPU is the bottleneck  Disk bandwidth &amp; wear-out is less of a concern (modern gen. SSDs)  CSC improves write chain lengths, but without op latency improvement  Pure random overwrites: op latency regresses (0.7ms  1.3ms)</p>
  </div>
  <div class="page">
    <p>File Layout Defragmentation</p>
    <p>Re-dirtying file blocks can trivially fix contiguity in P space  But that impacts efficiency of diffing-snapshots of the FlexVol</p>
    <p>Special fake-dirty that retains its V  Diffing of snapshots finds V unchanged  Stale P handled by consulting the container</p>
    <p>Performance: pre-fragmented data  All-HDD: results in lower latency, higher throughput  All-SSD:</p>
    <p>Beneficial for pure read workloads  But, op latency increases with read/write op mix (1.7ms  2.5ms)</p>
  </div>
  <div class="page">
    <p>Sub-block Compaction in WAFL  Tail-end of files &amp; of</p>
    <p>compression groups  Compacted into leaves of</p>
    <p>the container file  Benefits:</p>
    <p>No fixed sizes for subblocks; potential for workload-aware compaction</p>
    <p>Compaction-related metadata overhead proportional to savings</p>
    <p>Crunching/re-compaction of FlexVol snapshots</p>
    <p>P1</p>
    <p>V3</p>
    <p>File A</p>
    <p>Container File for FlexVol</p>
    <p>P1</p>
    <p>Initial:</p>
    <p>Current:</p>
    <p>RefCount Metafile</p>
    <p>V1 len offset</p>
    <p>V2 len offset</p>
    <p>V3 len offset</p>
    <p>(empty space)</p>
    <p>Compacted Block P1</p>
    <p>(block of B)</p>
    <p>(2nd block of A)</p>
    <p>(1st block of A)</p>
    <p>V2P1V1 P1</p>
    <p>File B</p>
    <p>V3</p>
    <p>FlexVol</p>
    <p>P1P1P1</p>
    <p>V1 V2</p>
  </div>
  <div class="page">
    <p>Re-Compaction</p>
    <p>Background scan recovers wasted fragments  Walks container file</p>
    <p>Compares refi with refcurr per P; if worth re-compacting  each fragment loaded as a dirty container leaf  Re-compacted (moved) by next CP</p>
    <p>Same handling of stale cached P  Future work:</p>
    <p>Make re-compaction less intrusive  Make re-compaction autonomous</p>
  </div>
  <div class="page">
    <p>Conclusion  Fourth form of fragmentation</p>
    <p>WAFL aggregate can include (on-prem/remote) object tier</p>
    <p>Cold blocks packaged into large objects</p>
    <p>Objects can get fragmented; defragmentation based on cost-metrics</p>
    <p>Defragmentation techniques help in all-HDD or mixed-media systems  All-SSD systems are sensitive to CPU consumption</p>
    <p>Ideally, defragmentation should be autonomous and based on system load</p>
    <p>Customer data show</p>
    <p>All-SSD systems have sufficient CPU headroom</p>
    <p>Autonomous defragmentation is feasible</p>
    <p>Consistent &amp; predictable performance</p>
  </div>
</Presentation>
