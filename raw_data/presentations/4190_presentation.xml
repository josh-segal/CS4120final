<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Measuring Word Relatedness Using Heterogeneous Vector Space Models</p>
    <p>Scott Wen-tau Yih (Microsoft Research) Joint work with Vahed Qazvinian (University of Michigan)</p>
  </div>
  <div class="page">
    <p>Measuring Semantic Word Relatedness</p>
    <p>How related are words movie and popcorn?</p>
  </div>
  <div class="page">
    <p>Measuring Semantic Word RelatednessSemantic relatedness covers many word relations,</p>
    <p>not just similarity [Budanitsky &amp; Hirst 06] Synonymy (noon vs. midday) Antonymy (hot vs. cold) Hypernymy/Hyponymy (Is-A) (wine vs. gin) Meronymy (Part-Of) (finger vs. hand) Functional relation (pencil vs. paper) Other frequent association (drug vs. abuse)</p>
    <p>Applications Text classification, paraphrase detection/generation, textual entailment,</p>
  </div>
  <div class="page">
    <p>Sentence Completion (Zweig et al. ACL-2012)</p>
    <p>The physics professor designed his lectures to avoid ____ the material: his goal was to clarify difficult topics, not make them confusing.</p>
    <p>(a) theorizing (b) elucidating (c) obfuscating (d) delineating (e) accosting</p>
  </div>
  <div class="page">
    <p>Sentence Completion (Zweig et al. ACL-2012)</p>
    <p>The physics professor designed his lectures to avoid ____ the material: his goal was to clarify difficult topics, not make them confusing.</p>
    <p>(a) theorizing (b) elucidating (c) obfuscating (d) delineating (e) accosting</p>
    <p>The answer word should be semantically related to some keywords in the sentence.</p>
  </div>
  <div class="page">
    <p>Vector Space Model Distributional Hypothesis (Harris 54)</p>
    <p>Words appearing in the same context tend to have similar meaning</p>
    <p>Basic vector space model (Pereira 93; Lin &amp; Pantel 02) For each target word, create a term vector using the neighboring words in a corpus The semantic relatedness of two words is measured by the cosine score of the corresponding vectors</p>
    <p>cos()</p>
  </div>
  <div class="page">
    <p>Need for Multiple VSMs Representing a multi-sense word (e.g., jaguar) with one vector could be problematic</p>
    <p>Violating triangle inequality</p>
    <p>Multi-prototype VSMs (Reisinger &amp; Mooney 10) Sense-specific vectors for each word Discovering senses by clustering contexts</p>
    <p>Two potential issues in practice Quality depends heavily on the clustering algorithm The corpus may not have enough coverage</p>
  </div>
  <div class="page">
    <p>Our Work  Heterogeneous VSMs Novel Insight</p>
    <p>Vectors from different information sources bias differently Jaguar: Wikipedia (cat), Bing (car)</p>
    <p>Heterogeneous vector space models provide complementary coverage of word sense and meaning</p>
    <p>Solution Construct VSMs using general corpus (Wikipedia), Web (Bing) and thesaurus (Encarta &amp; WordNet) Word relatedness measure: Average cosine score</p>
    <p>Strong empirical results Outperform existing methods on 2 benchmark datasets</p>
  </div>
  <div class="page">
    <p>Roadmap Introduction Construct heterogeneous vector space models</p>
    <p>Corpus  Wikipedia Web  Bing search snippets Thesaurus  Encarta &amp; WordNet</p>
    <p>Experimental evaluation Task &amp; datasets Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Corpus-based VSM (Lin &amp; Pantel 02) Construction</p>
    <p>Collect terms within a window of [-10,+10] centered at each occurrence of a target word Create TFIDF term-vector</p>
    <p>Refinement Vocabulary Trimming (removing stop-words)</p>
    <p>Top 1500 high DF terms are removed from vocabulary</p>
    <p>Term Trimming (local feature selection) Top 200 high-weighted terms for each term-vector</p>
    <p>Data Wikipedia (Nov. 2010)  917M words</p>
  </div>
  <div class="page">
    <p>Web-based VSM (Sahami &amp; Heilman 06)</p>
    <p>Construction Issue each target word as a query to Bing Collect terms in the top 30 snippets Create TFIDF term-vector</p>
    <p>Vocabulary trimming: top 1000 high DF terms are removed No term trimming</p>
    <p>Compared to corpus-based VSM Reflects user preference May bias different word sense and meaning</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Thesaurus-based VSM (1/2)</p>
    <p>Addresses two well-known weaknesses of distributional similarity</p>
    <p>Co-occurrence synonymous bread vs. butter  high score because of bread and butter Related, but shouldnt be scored higher than synonyms</p>
    <p>Words in general corpora follow Zipfs law Frequency of any word is inversely proportional to its rank Some words occur very infrequently in the corpus As a result, the term vector contains only few, noisy terms</p>
  </div>
  <div class="page">
    <p>Thesaurus-based VSM (2/2)</p>
    <p>Construction Create a TFIDF document-term matrix</p>
    <p>Each document is a group of synonyms (synset)</p>
    <p>Each word is represented by the corresponding column vector  the synsets it belongs to</p>
    <p>Data WordNet  227,446 synsets, 190,052 words Encarta thesaurus  46,945 synsets, 50,184 words</p>
  </div>
  <div class="page">
    <p>Roadmap Introduction Construct heterogeneous vector space models</p>
    <p>Corpus  Wikipedia Web  Bing search snippets Thesaurus  Encarta &amp; WordNet</p>
    <p>Experimental evaluation Task &amp; datasets Results</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Evaluation Method</p>
    <p>Directly test the correlation of the ranking of word relatedness measures with human judgment</p>
    <p>Spearmans rank correlation coefficient</p>
    <p>Word 1 Word 2 Human Score (mean)</p>
    <p>midday noon 9.3</p>
    <p>tiger jaguar 8.0</p>
    <p>cup food 5.0</p>
    <p>forest graveyard 1.9</p>
    <p>Data: list of word pairs with human judgment</p>
  </div>
  <div class="page">
    <p>Results: WordSim-353 (Finkelstein et al. 01)</p>
    <p>Assessed on a 0-10 scale by 13-16 human judges</p>
  </div>
  <div class="page">
    <p>Results: MTurk-287 (Radinsky et al. 11)</p>
    <p>Assessed on a 1-5 scale by 10 Turkers</p>
  </div>
  <div class="page">
    <p>Conclusion Combining heterogeneous VSMs for measuring word relatedness</p>
    <p>Better coverage on word sense and meaning A simple and yet effective strategy</p>
    <p>Future Work Other combination strategy or model</p>
    <p>Extending to longer text segments (e.g., phrases) More fine-grained word relations</p>
    <p>Polarity Inducing LSA for Synonymy and Antonymy (Yih, Zweig &amp; Platt, EMNLP-2012)</p>
  </div>
</Presentation>
