<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Real-time Scheduling of Skewed MapReduce Jobs in Heterogeneous</p>
    <p>Environments</p>
    <p>Nikos Zacheilas, Vana Kalogeraki</p>
    <p>Department of Informatics</p>
    <p>Athens University of Economics and Business</p>
  </div>
  <div class="page">
    <p>Introduction  Big Data era has arrived!</p>
    <p>Facebook processes daily more than 500 TB of data</p>
    <p>Twitter users generate 500M tweets per day</p>
    <p>Dublins city operational center receives over 100 bus GPS traces per minute</p>
    <p>Wide range of domains  Traffic monitoring</p>
    <p>Inventory management</p>
    <p>Healthcare infrastructures</p>
    <p>More data than we can handle with traditional approaches (e.g. relational databases)</p>
    <p>Novel frameworks were proposed</p>
    <p>Batch processing  Googles MapReduce</p>
    <p>IBMs BigInsights</p>
    <p>Microsofts Dryad</p>
    <p>Stream processing</p>
    <p>Storm</p>
    <p>IBMs Infosphere Streams</p>
    <p>Nikos Zacheilas 2</p>
  </div>
  <div class="page">
    <p>The MapReduce Model  MapReduce [Dean@OSDI2004] was proposed as a powerful and cost-effective</p>
    <p>approach for massive scale batch processing</p>
    <p>Popularized via its open source implementation, Hadoop, is used by some of the major computer companies:  Yahoo!</p>
    <p>Twitter</p>
    <p>Facebook</p>
    <p>Intense processing jobs are broken into smaller tasks</p>
    <p>Two stages of processing map and reduce</p>
    <p>All [2, 2] intermediate pairs assigned to the same reduce task are called a reduce tasks partition</p>
    <p>map(1,1)[2, 2]</p>
    <p>reduce(2, [2])[3, 3]</p>
    <p>Nikos Zacheilas 3</p>
  </div>
  <div class="page">
    <p>Processing Big Data with MapReduce Challenges</p>
    <p>Load imbalances due to skewed data</p>
    <p>Heterogeneous environments with heterogeneous processing capabilities</p>
    <p>Real time response requirements  95% of Facebooks MapReduce jobs have average execution time of 30</p>
    <p>seconds [Chen@MASCOTS2011]</p>
    <p>Youtube social graph application Nikos Zacheilas 4</p>
  </div>
  <div class="page">
    <p>Problem</p>
    <p>Question: How can we efficiently schedule the execution of multiple MapReduce jobs with real-time response requirements?</p>
    <p>Challenges:</p>
    <p>Maximize the probability of meeting end-to-end real-time response requirements</p>
    <p>Effectively handle skewed data</p>
    <p>Identify overloaded nodes</p>
    <p>Deal with heterogeneous environments</p>
    <p>Nikos Zacheilas 5</p>
  </div>
  <div class="page">
    <p>DynamicShare System</p>
    <p>We propose DynamicShare a novel MapReduce framework for heterogeneous environments. Our approach makes the following contributions:</p>
    <p>New jobs execution times estimation model based on nonparametric regression</p>
    <p>Distributed least laxity first scheduling of jobs tasks to meet endto-end demands</p>
    <p>Early identification of overloaded nodes through Local Outlier Factor algorithm</p>
    <p>Handling data skewness with two approaches:  Simple partitions assignment</p>
    <p>Count-Min Sketch assignment</p>
    <p>Nikos Zacheilas 6</p>
  </div>
  <div class="page">
    <p>The MapReduce Model</p>
    <p>M</p>
    <p>M</p>
    <p>M R.1</p>
    <p>P a</p>
    <p>rt it</p>
    <p>io n</p>
    <p>in g</p>
    <p>P.2</p>
    <p>Reduce Phase Map Phase</p>
    <p>P.1</p>
    <p>P.3</p>
    <p>Split File</p>
    <p>(, ) (, )</p>
    <p>(, ) (, )</p>
    <p>(, )</p>
    <p>(, ) (, )</p>
    <p>(, )</p>
    <p>(, [])</p>
    <p>(, , )</p>
    <p>(, [,])</p>
    <p>(, [])</p>
    <p>(, [])</p>
    <p>P.4</p>
    <p>P.5</p>
    <p>(, [])</p>
    <p>(, [])</p>
    <p>(, [])</p>
    <p>R.2</p>
    <p>R.3</p>
    <p>Split File</p>
    <p>Split File</p>
    <p>Output</p>
    <p>Output</p>
    <p>Output</p>
    <p>Nikos Zacheilas 7</p>
  </div>
  <div class="page">
    <p>DynamicShare Architecture</p>
    <p>Master</p>
    <p>Workers</p>
    <p>Reduce Task Slots</p>
    <p>Map Tasks Slots</p>
    <p>Nikos Zacheilas 8</p>
    <p>DynamicShare comprises a single Master and multiple Worker nodes</p>
    <p>Master node  responsible for assigning map</p>
    <p>and reduce tasks to Workers under skewness and real-time criteria</p>
    <p>monitor jobs performance  Worker nodes</p>
    <p>execute map/reduce tasks  report task progress</p>
  </div>
  <div class="page">
    <p>System Model Each submitted job  comprises a sequence of invocations of map and reduce tasks. Each job  is characterized by:</p>
    <p>is the time interval, starting at job initialization, within which job  must be completed</p>
    <p>__: the estimated amount of time required for the job to complete. Estimation is given by the following Equation: __ = max ,,,, + max {,,,,}</p>
    <p>: the difference between  and __, considered a metric of urgency for job</p>
    <p>_: the size of a split file</p>
    <p>Each task  of job  has the following parameters:</p>
    <p>,, ,: estimated execution times of map and reduce tasks in Worker</p>
    <p>,,,: average CPU and memory usage of task  in Worker</p>
    <p>Nikos Zacheilas 9</p>
  </div>
  <div class="page">
    <p>DynamicShare: How it works?</p>
    <p>Master</p>
    <p>Worker</p>
    <p>Split File</p>
    <p>Laxity values</p>
    <p>Partitions Sizes</p>
    <p>Assignment</p>
    <p>Split File</p>
    <p>M M</p>
    <p>R R</p>
    <p>Task Slots</p>
    <p>1 1</p>
    <p>2 2</p>
    <p>3 3</p>
    <p>1 1</p>
    <p>1 1</p>
    <p>1</p>
    <p>1</p>
  </div>
  <div class="page">
    <p>Task Scheduling</p>
    <p>Master</p>
    <p>Split File</p>
    <p>Laxity values</p>
    <p>Partitions Sizes</p>
    <p>Assignment</p>
    <p>Split File</p>
    <p>M M</p>
    <p>R R</p>
    <p>Task Slots</p>
    <p>1 1</p>
    <p>2 2</p>
    <p>3 3</p>
    <p>1 1</p>
    <p>1 1</p>
    <p>1</p>
    <p>1</p>
    <p>Nikos Zacheilas 11</p>
  </div>
  <div class="page">
    <p>Task Scheduling  Given the  and __ for job , we compute</p>
    <p>the  value with the following formula</p>
    <p>=   __</p>
    <p>Least laxity scheduling is a dynamic algorithm that allow us to compensate for queueing delays experienced by the tasks executing at the nodes</p>
    <p>TaskScheduler sorts jobs tasks based on the  values. Tasks</p>
    <p>of jobs with the smaller laxity values will be closer to the head of the queue</p>
    <p>Scheduling decisions are made when:</p>
    <p>Nikos Zacheilas 12</p>
  </div>
  <div class="page">
    <p>Estimating Tasks Execution Time  Current solutions such as building job profiles or using debug runs</p>
    <p>are not adequate</p>
    <p>Works well for homogeneous environments</p>
    <p>What happens though in heterogeneous environments where multiple applications may share the same resources?</p>
    <p>Need to take into account the resource requirements (e.g., CPU, memory usage) of newly submitted tasks</p>
    <p>Approximate   function  Parametric regression considers the functional form known</p>
    <p>Non-parametric regression makes no assumption (data-driven technique)</p>
    <p>= _, ,,,</p>
    <p>Execution Times</p>
    <p>Estimator ,</p>
    <p>Nikos Zacheilas 13</p>
  </div>
  <div class="page">
    <p>Estimating Tasks Execution Time</p>
    <p>Execution Times</p>
    <p>Estimator</p>
    <p>,</p>
    <p>= 1</p>
    <p>=1</p>
    <p>( )</p>
    <p>Vector Execution Time</p>
    <p>1 1</p>
    <p>Non-parametric Regression</p>
    <p>Execution Times</p>
    <p>Estimator ,</p>
    <p>= 1</p>
    <p>=1</p>
    <p>( )</p>
    <p>Vector Execution Time</p>
    <p>1 1</p>
    <p>k-Nearest Neighbor (k-NN) Smoothing</p>
    <p>Use k closest in Euclidean distance past runs</p>
    <p>Past runs Past runs</p>
    <p>Nikos Zacheilas 14</p>
  </div>
  <div class="page">
    <p>Identifying Overloaded Nodes  Due to the dynamic behavior of the jobs Workers performance</p>
    <p>may change rapidly. Need to quickly detect overloaded Workers</p>
    <p>We consider overloaded nodes those that are assigned more tasks than their processing capabilities</p>
    <p>Key Observation: Laxity values of these tasks will be left behind in relation to the tasks running in different nodes</p>
    <p>Solution: Applied Local Outlier Factor algorithm (LOF) on the laxity values of the tasks of the same job that run on different Workers</p>
    <p>(): = ()()</p>
    <p>|()|()</p>
    <p>Compares reachability density of a point with each neighbors</p>
    <p>Nikos Zacheilas 15</p>
  </div>
  <div class="page">
    <p>Handling Skewed Data In our system two types of skew frequently occur:  Skewed Key Frequencies  Skewed Tuple Sizes Idea: Use more partitions than the original MapReduce Problem: How to assign partitions to the reduce tasks in order to minimize the reduce phase execution time? Exploit two approaches:  Simple Partitions Assignment  Count Min Sketch Assignment</p>
    <p>Master</p>
    <p>Worker</p>
    <p>Split File</p>
    <p>Partitions Sizes Assignment</p>
    <p>Split File</p>
    <p>M M</p>
    <p>R R Task Slots</p>
    <p>Laxity values</p>
    <p>Nikos Zacheilas 16</p>
  </div>
  <div class="page">
    <p>Simple Partitions Assignment</p>
    <p>Map Tasks</p>
    <p>Dynamic Partitioning Algorithm</p>
    <p>2</p>
    <p>2</p>
    <p>1 1 1</p>
    <p>1</p>
    <p>2</p>
    <p>+</p>
    <p>2 2 2 +</p>
    <p>1:1</p>
    <p>Master</p>
    <p>.1 . 2</p>
    <p>Estimated via k-NN smoothing</p>
    <p>1</p>
    <p>1</p>
    <p>Reduce Tasks</p>
    <p>1:1</p>
    <p>Nikos Zacheilas 17</p>
    <p>each partition to the available reduce tasks 4. Pick the reduce task () that requires the</p>
    <p>minimum execution time</p>
  </div>
  <div class="page">
    <p>Count-Min Sketch Assignment</p>
    <p>Map Tasks</p>
    <p>Dynamic Partitioning Algorithm</p>
    <p>1:1</p>
    <p>+ Master</p>
    <p>2:1</p>
    <p>1:1 2:1</p>
    <p>1:1 1:1 1:1</p>
    <p>+ 2:1 2:1 2:1</p>
    <p>1: 1:1</p>
    <p>2:1:2</p>
    <p>1:1 : 1</p>
    <p>1:1 : 1</p>
    <p>1:1 2:1</p>
    <p>Reduce Tasks</p>
    <p>Nikos Zacheilas 18</p>
  </div>
  <div class="page">
    <p>Implementation  We implemented and evaluated DynamicShare on Planetlab.</p>
    <p>Fourteen nodes were used with 82 processing cores. One dedicated node was the Master and the others used as Workers</p>
    <p>Two MapReduce jobs were issued:  A Twitter friendship request query on 2GB of available tweets. 59 map and 23</p>
    <p>reduce tasks were used</p>
    <p>A Youtube friends counting application for a 39MB Youtube social graph. Again 59 map and 23 reduce tasks were used</p>
    <p>Compared our scheduling proposal with:  Earliest Deadline First (EDF)</p>
    <p>FIFO</p>
    <p>FAIR</p>
    <p>Our partitioning algorithms were compared to:  Load Balance [Gufler@CLOSER2011]</p>
    <p>Hadoop</p>
    <p>Skewtune [Kwon@SIGMOD2012]</p>
    <p>Nikos Zacheilas 19</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>k-NN Smoothing Performance</p>
    <p>Initially when not enough data are available, the estimated value is larger than the actual</p>
    <p>Better prediction when more past runs are used</p>
    <p>LOF Execution time  LOF depends on the</p>
    <p>number of tasks used by a job</p>
    <p>Even for great number of tasks the algorithm is capable of detecting outliers in respectable amount of time</p>
    <p>Deadline misses comparison</p>
    <p>LLF maintains the percentage of deadline misses at the lowest possible level</p>
    <p>Takes into account the current system conditions for the assignment Nikos Zacheilas 20</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Comparing LB with DP in regards to achieved balance</p>
    <p>LB has better results because it considers a fair distribution of the partitions to the available reduce tasks</p>
    <p>DP does not consider balance in the assignment</p>
    <p>Comparing DP with LB in regards to achieved execution time</p>
    <p>Balance is not the correct approach for heterogeneous environments</p>
    <p>DPs opportunistic assignment exploits high performance nodes by assigning extra partitions</p>
    <p>Nikos Zacheilas 21</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Comparing DP with Skewtune and Hadoop partitioning</p>
    <p>Hadoop leads to the execution of large partitions to slow nodes</p>
    <p>Skewtune repartitioning cost is prohibitive for short jobs</p>
    <p>DP does an appropriate one time assignment</p>
    <p>Similar results were observed in Youtube job</p>
    <p>Comparing DP with and without sketches</p>
    <p>DP with sketches achieves better results than DP without sketches, because more partitions assignments are possible</p>
    <p>However the overhead of the algorithm is not negligible. When sketches are applied DP requires approximately 200 ms while without sketches only 80 ms</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work  We proposed a new framework for handling MapReduce jobs</p>
    <p>with real-time constraints in highly heterogeneous environments using:  non-parametric regression for estimating tasks execution times</p>
    <p>Least Laxity First scheduling of jobs tasks in the available slots</p>
    <p>Local Outlier Factor for detecting overloaded nodes</p>
    <p>Dynamic Partitioning algorithms for handling skewed data</p>
    <p>Evaluated our proposal in Planetlab, and the results point out that our system achieves its goals</p>
    <p>Future work:  Dynamically decide the number of partitions and examine the trade-off</p>
    <p>between the reduce phase execution time and the two partitioning algorithms</p>
    <p>Nikos Zacheilas 23</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>Questions??</p>
    <p>Nikos Zacheilas 24</p>
  </div>
</Presentation>
