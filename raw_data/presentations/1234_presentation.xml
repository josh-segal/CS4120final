<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Kate Keahey</p>
    <p>University of Chicago, Argonne National Laboratory</p>
    <p>Jason Anderson (UC), Zhuo Zhen (UC), Pierre Riteau (StackHPC), Paul Ruth (RENCI), Dan Stanzione (TACC), Mert Cevik (RENCI), Jacob Colleran (UC), Haryadi Gunawi (UC), Cody Hammock (TACC), Joe Mambretti (Northwestern), Alexander Barnes (TACC), Franc ois Halbach (TACC), Alex Rocha (TACC), Joe Stubbs (TACC)</p>
    <p>LESSONS LEARNED FROM THE CHAMELEON TESTBED</p>
  </div>
  <div class="page">
    <p>CHAMELEON IN A NUTSHELL  We like to change: a testbed that adapts itself to your experimental needs</p>
    <p>Deep reconfigurability (bare metal) and isola7on  power on/off, reboot, custom kernel, serial console access, etc.</p>
    <p>Balance: large-scale versus diverse hardware  Large-scale: ~large homogenous par77on (~15,000 cores), ~6 PB of storage distributed over 2</p>
    <p>sites (UC, TACC) connected with 100G network  Diverse: ARMs, Atoms, FPGAs, GPUs, Corsa switches, etc.</p>
    <p>Cloud++: leveraging mainstream cloud technologies  Powered by OpenStack with bare metal reconfigura7on (Ironic) + special sauce  Blazar contribu7on recognized as official OpenStack component</p>
    <p>We live to serve: open, produc@on testbed for Computer Science Research  Started in 10/2014, available since 07/2015, renewed in 10/2017, working on renewal now!  Currently 4,000+ users, 600+ projects, 100+ ins7tu7ons</p>
  </div>
  <div class="page">
    <p>systems experiments you can run ex</p>
    <p>pe rim</p>
    <p>en te</p>
    <p>rs Traditional</p>
    <p>HPC resources Virtual cloud</p>
    <p>resources</p>
    <p>Custom testbed</p>
    <p>Chameleon</p>
    <p>THE MOST EXPERIMENTS FOR THE MOST USERS</p>
    <p>Hardware Expressiveness Configurability and isolation</p>
    <p>Cost (per user/exp) and isolation Usability (user tools)</p>
    <p>Familiarity</p>
    <p>shar ing e</p>
    <p>cosy stem Expressing experiments (cost per exp)</p>
    <p>Publication and discovery (cost of sharing)</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS: HARDWARE</p>
    <p>Largest lease: 120  67% single node, 5% exceed 10 nodes (11% on Haswell)</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS: ALLOCATABLE RESOURCES</p>
    <p>Allocatable: managed in @me (advance reserva@ons, extensions) and space  Advance reserva@ons are cri@cal to provide access to resources in demand  Extensions: 5.4% usage across leases</p>
    <p>Also see: Managing Allocatable Resources , CLOUD19</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS: EXPRESSIVENESS  Resources can be specified at different levels</p>
    <p>Model/constraint-based: none (9.5%), single (89.24%), mul7ple (1.26%)</p>
    <p>Hardware type (single constraint): 90.18%</p>
    <p>Node UID (single constraint): 3.38% (18.45% for leases made 7 days in advance)</p>
    <p>Separa@on of alloca@on and configura@on  20.07% alloca7ons had more than 1 instance deployed (max of 12)</p>
    <p>Network s@tching (ExoGENI): 22 (8%) projects created 920 s@tched links  Bring Your Own Controller (BYOC): 11 (4%) projects  Orchestra@on (Heat): 94 (2017), 155 (2018), and 405 (2019) deployments  Automated deployment: surprisingly liYle use</p>
  </div>
  <div class="page">
    <p>EXPERIMENTERS: COST  Support cost</p>
    <p>Average of 13 help desk 2ckets per week, less than one 2cket per user</p>
    <p>Heavily leveraging smoke tests, live monitoring, and automated remedia2on</p>
    <p>Working with mainstream open source project (OpenStack)  Familiar interfaces: 858 deployments across 441 organiza2ons in 63 countries</p>
    <p>Transferable skills</p>
    <p>Working with large community (~8,400 total contributors, ~6,000 reviewing code)</p>
    <p>New features: whole disk image boot, support for non x86, mul2-tenant networking</p>
    <p>Access to exis2ng documenta2on and support systems</p>
    <p>Opportunity to contribute (though at a cost): Blazar as OpenStack component</p>
    <p>Chameleon expresses capabili1es needed for CS research in terms of a mainstream cloud func1onality (CHI-in-a-Box)</p>
  </div>
  <div class="page">
    <p>EXPERIMENTERS: ACTIVE USERS</p>
  </div>
  <div class="page">
    <p>EXPERIMENTERS: ACTIVE LEASES</p>
  </div>
  <div class="page">
    <p>EXPERIMENTERS: COMMUNITY  Ins$tu$ons: 168 (11 MSI, 19 EPSCOR)  Geography (US): 40 states + Puerto Rico  Funding source: NSF (also DOE, DARPA, others)  Research versus educa$on</p>
    <p>Educa7on: 45/513 projects use ~9% of total 7me  Research: similar average usage</p>
    <p>Publica$ons: 275/75 overall /journal  Field of science</p>
    <p>12% (non CS), 10% (security), 17% (ML), 8% (Edge)</p>
    <p>Renewals: ~75% of eligible projects sought renewal, 33 renewed &gt; 5 $mes</p>
  </div>
  <div class="page">
    <p>SHARING EXPERIMENTS  Testbeds/clouds lead to the crea@on of compa@ble digital ar@facts that</p>
    <p>package an experiment  In Chameleon: ~120,000 images and ~31,000 orchestra2on templates</p>
    <p>Elements of reproducibility support in Chameleon  Testbed versioning</p>
    <p>Image versioning</p>
    <p>Orchestra2on</p>
    <p>Experiment Prcis (Linux history analogue)</p>
    <p>How do we @e them all together?</p>
  </div>
  <div class="page">
    <p>SHARING EXPERIMENTS: PACKAGING</p>
    <p>Repeatability by default: Jupyter notebooks + Chameleon experimental containers  JupyterLab for our users: use jupyter.chameleoncloud.org with Chameleon creden&lt;als</p>
    <p>Interface to the testbed in Python/bash + examples (see LCN18: hGps://vimeo.com/297210055)</p>
    <p>Named containers: your experimental process goes here</p>
    <p>Experimental storytelling: ideas/text, process/code, results Complex Experimental containers</p>
    <p>Jupyter Notebooks</p>
    <p>+</p>
    <p>Also see: A Case for Integra@ng Experimental Containers with Notebooks, CloudCom 2019</p>
  </div>
  <div class="page">
    <p>SHARING EXPERIMENTS: PUBLICATION</p>
    <p>Digital publishing with Zenodo: make your experimental ar$facts citable via Digital Object Iden$fiers (DOIs)</p>
    <p>Integra$on with Zenodo  Export: make your research citable and discoverable  Import: access a wealth of digital research ar7facts already published</p>
    <p>Towards making research findable: the digital sharing pla_orm</p>
    <p>Familiar research sharing ecosystem Digital research sharing ecosystem</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>PARTING THOUGHTS  Chameleon expresses capabili@es needed for CS research in terms of a</p>
    <p>mainstream cloud func@onality -- OpenStack  Our paper discusses the extensions and augmenta2ons to support our use case</p>
    <p>Prac2cal delivery: CHI-in-a-Box  packaging of the CHameleon Infrastructure</p>
    <p>Experimental testbeds: opportunity for sharing  The most experiments for the most experimenters</p>
    <p>Opportunity for the support of efficient sharing of experiments</p>
    <p>Chasing the research fron@er: the func@onality of any scien@fic instrument has to follow the emergent opportuni@es in the science they serve  development-driven opera@ons</p>
  </div>
  <div class="page">
    <p>Were here to change www.chameleoncloud.org</p>
    <p>keahey@anl.gov</p>
  </div>
</Presentation>
