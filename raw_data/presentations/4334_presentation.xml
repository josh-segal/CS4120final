<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Context-Aware Neural Machine</p>
    <p>Translation Learns Anaphora</p>
    <p>Resolution</p>
    <p>Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
    <p>What does it refer to?</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
    <p>Possible translations into Russian:</p>
    <p>48 . (masculine or neuter)</p>
    <p>48 . (feminine)</p>
    <p>48 . (plural)</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
    <p>What do columns mean?</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
    <p>Possible translations into Russian:</p>
    <p>// 48 .</p>
    <p>// 48 .</p>
  </div>
  <div class="page">
    <p>Do we really need context?</p>
    <p>Source:</p>
    <p>It has 48 columns.</p>
    <p>Translation:</p>
    <p>48 .</p>
    <p>Under the cathedral lies the antique chapel. Context:</p>
  </div>
  <div class="page">
    <p>Recap: antecedent and anaphora resolution</p>
    <p>Under the cathedral lies the antique chapel. It has 48 columns.</p>
    <p>Wikipedia:</p>
    <p>An antecedent is an expression that gives its meaning to</p>
    <p>a proform (pronoun, pro-verb, pro-adverb, etc.)</p>
    <p>Anaphora resolution is the problem of resolving references to earlier</p>
    <p>or later items in the discourse.</p>
    <p>antecedent anaphoric</p>
    <p>pronoun</p>
  </div>
  <div class="page">
    <p>Context in Machine Translation</p>
    <p>SMT</p>
    <p>focused on handling specific phenomena</p>
    <p>used special-purpose features ([Le Nagard and Koehn, 2010]; [Hardmeier and Federico, 2010]; [Hardmeier et al., 2015], [Meyer</p>
    <p>et al., 2012], [Gong et al., 2012], [Carpuat, 2009]; [Tiedemann, 2010]; [Gong et al., 2011])</p>
  </div>
  <div class="page">
    <p>Context in Machine Translation</p>
    <p>SMT</p>
    <p>focused on handling specific phenomena</p>
    <p>used special-purpose features ([Le Nagard and Koehn, 2010]; [Hardmeier and Federico, 2010]; [Hardmeier et al., 2015], [Meyer</p>
    <p>et al., 2012], [Gong et al., 2012], [Carpuat, 2009]; [Tiedemann, 2010]; [Gong et al., 2011])</p>
    <p>NMT</p>
    <p>directly provide context to an NMT system at training time ([Jean et al., 2017]; [Wang et al., 2017]; [Tiedemann and Scherrer, 2017]; [Bawden et al., 2018])</p>
  </div>
  <div class="page">
    <p>Context in Machine Translation</p>
    <p>SMT</p>
    <p>focused on handling specific phenomena</p>
    <p>used special-purpose features ([Le Nagard and Koehn, 2010]; [Hardmeier and Federico, 2010]; [Hardmeier et al., 2015], [Meyer</p>
    <p>et al., 2012], [Gong et al., 2012], [Carpuat, 2009]; [Tiedemann, 2010]; [Gong et al., 2011])</p>
    <p>NMT</p>
    <p>directly provide context to an NMT system at training time ([Jean et al., 2017]; [Wang et al., 2017]; [Tiedemann and Scherrer, 2017]; [Bawden et al., 2018])</p>
    <p>not clear: what kinds of discourse phenomena are successfully handled</p>
    <p>how they are modeled</p>
  </div>
  <div class="page">
    <p>Our work</p>
    <p>we introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between</p>
    <p>the context and the rest of the translation model</p>
    <p>we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the</p>
    <p>model</p>
    <p>by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly</p>
    <p>captures anaphora</p>
    <p>Plan</p>
  </div>
  <div class="page">
    <p>Context-Aware Model Architecture</p>
  </div>
  <div class="page">
    <p>Transformer model architecture</p>
    <p>start with the Transformer [Vaswani et al, 2018]</p>
  </div>
  <div class="page">
    <p>Context-aware model architecture</p>
    <p>start with the Transformer [Vaswani et al, 2018]</p>
    <p>incorporate context information on the encoder side</p>
  </div>
  <div class="page">
    <p>Context-aware model architecture</p>
    <p>start with the Transformer [Vaswani et al, 2018]</p>
    <p>incorporate context information on the encoder side</p>
    <p>use a separate encoder for context</p>
    <p>share first N-1 layers of source and context encoders</p>
  </div>
  <div class="page">
    <p>Context-aware model architecture</p>
    <p>start with the Transformer [Vaswani et al, 2018]</p>
    <p>incorporate context information on the encoder side</p>
    <p>use a separate encoder for context</p>
    <p>share first N-1 layers of source and context encoders</p>
    <p>the last layer incorporates contextual information</p>
  </div>
  <div class="page">
    <p>Overall performance Dataset: OpenSubtitles2018 (Lison et al., 2018) for English and Russian</p>
  </div>
  <div class="page">
    <p>Overall performance: models comparison</p>
    <p>(context is the previous sentence)</p>
    <p>baseline</p>
    <p>concatenation</p>
    <p>context encoder (our work)</p>
    <p>baseline: context-agnostic Transformer</p>
    <p>concatenation: modification of the approach by [Tiedemann and</p>
    <p>Scherrer, 2017]</p>
  </div>
  <div class="page">
    <p>Our model: different types of context</p>
    <p>baseline</p>
    <p>next sentence</p>
    <p>random sentence</p>
    <p>previous sentence</p>
    <p>Next sentence does not appear beneficial</p>
    <p>Performance drops for a random context sentence</p>
    <p>Model is robust towards being shown a random context</p>
    <p>sentence</p>
    <p>(the only significant at p&lt;0.01 difference is with the best model;</p>
    <p>differences between other results are not significant)</p>
  </div>
  <div class="page">
    <p>Analysis</p>
  </div>
  <div class="page">
    <p>Our work</p>
    <p>we introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between</p>
    <p>the context and the rest of the translation model</p>
    <p>we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the</p>
    <p>model</p>
    <p>by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly</p>
    <p>captures anaphora</p>
    <p>to context</p>
    <p>Analysis</p>
  </div>
  <div class="page">
    <p>What do we mean by attention to context?</p>
    <p>attention from source to context</p>
    <p>mean over heads of per-head attention weights</p>
  </div>
  <div class="page">
    <p>What do we mean by attention to context?</p>
    <p>attention from source to context</p>
    <p>mean over heads of per-head attention weights</p>
    <p>take sum over context words (excluding &lt;bos&gt;, &lt;eos&gt; and punctuation)</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
    <p>Third person</p>
    <p>singular masculine</p>
    <p>singular feminine</p>
    <p>singular neuter</p>
    <p>plural</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
    <p>Second person</p>
    <p>singular impolite</p>
    <p>singular polite</p>
    <p>plural</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
    <p>Need to know gender, because</p>
    <p>verbs must agree in gender with I</p>
    <p>(in past tense)</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
    <p>Many of these words appear at</p>
    <p>sentence initial position.</p>
    <p>Maybe this is all that matters?</p>
  </div>
  <div class="page">
    <p>Top words influenced by context</p>
    <p>word pos</p>
    <p>it 5.5</p>
    <p>yours 8.4</p>
    <p>yes 2.5</p>
    <p>i 3.3</p>
    <p>yeah 1.4</p>
    <p>you 4.8</p>
    <p>ones 8.3</p>
    <p>m 5.1</p>
    <p>wait 3.8</p>
    <p>well 2.1</p>
    <p>word pos</p>
    <p>it 6.8</p>
    <p>yours 8.3</p>
    <p>ones 7.5</p>
    <p>m 4.8</p>
    <p>you 5.6</p>
    <p>am 4.4</p>
    <p>i 5.2</p>
    <p>s 5.6</p>
    <p>one 6.5</p>
    <p>won 4.6</p>
    <p>Only positions</p>
    <p>after the first</p>
  </div>
  <div class="page">
    <p>Does the amount of attention to</p>
    <p>context depend on factors such</p>
    <p>as sentence length and position?</p>
  </div>
  <div class="page">
    <p>Dependence on sentence length</p>
  </div>
  <div class="page">
    <p>Dependence on sentence length</p>
    <p>short source</p>
    <p>long context</p>
    <p>high attention to context</p>
  </div>
  <div class="page">
    <p>Dependence on sentence length</p>
    <p>long source</p>
    <p>short context</p>
    <p>low attention to context</p>
  </div>
  <div class="page">
    <p>Is context especially helpful for short sentences?</p>
  </div>
  <div class="page">
    <p>Dependence on token position</p>
  </div>
  <div class="page">
    <p>Analysis of pronoun translation</p>
  </div>
  <div class="page">
    <p>Ambiguous pronouns and translation quality:</p>
    <p>how to evaluate</p>
    <p>feed CoreNLP (Manning et al., 2014) with pairs of sentences</p>
    <p>pick examples with a link between the pronoun and a noun group in a context</p>
    <p>gather a test set for each pronoun</p>
    <p>use the test sets to evaluate the context-aware NMT system</p>
    <p>Metric: BLEU (standard metric for MT)</p>
    <p>Specific test sets:</p>
  </div>
  <div class="page">
    <p>Ambiguous pronouns and translation quality:</p>
    <p>noun antecedent</p>
    <p>it you I</p>
    <p>B L</p>
    <p>E U</p>
    <p>baseline</p>
    <p>context-aware</p>
  </div>
  <div class="page">
    <p>Ambiguous it: noun antecedent</p>
    <p>masculine feminine neuter plural</p>
    <p>B L</p>
    <p>E U</p>
    <p>baseline</p>
    <p>context-aware +4.8</p>
  </div>
  <div class="page">
    <p>It with noun antecedent: example</p>
    <p>Source:</p>
    <p>It was locked up in the hold with 20 other boxes of supplies.</p>
    <p>Possible translations into Russian:</p>
    <p>20    . (masculine)</p>
    <p>20    . (neuter)</p>
    <p>20    . (feminine)</p>
    <p>20    . (plural)</p>
  </div>
  <div class="page">
    <p>It with noun antecedent: example</p>
    <p>Source:</p>
    <p>You left money unattended?</p>
    <p>Possible translations into Russian:</p>
    <p>20    . (plural)</p>
    <p>Context:</p>
    <p>It was locked up in the hold with 20 other boxes of supplies.</p>
  </div>
  <div class="page">
    <p>Latent anaphora resolution</p>
  </div>
  <div class="page">
    <p>Hypothesis</p>
    <p>Observation:</p>
    <p>Large improvements in BLEU on test sets with pronouns co-referent with an expression in context</p>
    <p>Attention mechanism Latent anaphora resolution</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>How to test the hypothesis: agreement with CoreNLP</p>
    <p>Test set:</p>
    <p>Find an antecedent noun phrase (using CoreNLP)</p>
    <p>Pick examples where the noun phrase contains a single noun</p>
    <p>Pick examples with several nouns in context</p>
  </div>
  <div class="page">
    <p>How to test the hypothesis: agreement with CoreNLP</p>
    <p>Test set:</p>
    <p>Find an antecedent noun phrase (using CoreNLP)</p>
    <p>Pick examples where the noun phrase contains a single noun</p>
    <p>Pick examples with several nouns in context</p>
    <p>Calculate an agreement:</p>
    <p>Identify the token with the largest attention weight (excluding punctuation, &lt;bos&gt; and &lt;eos&gt;)</p>
    <p>If the token falls within the antecedent span, then its an agreement</p>
  </div>
  <div class="page">
    <p>Does the model learn anaphora,</p>
    <p>or just some simple heuristic?</p>
    <p>Use several baselines:</p>
    <p>random noun</p>
    <p>first noun</p>
    <p>last noun</p>
  </div>
  <div class="page">
    <p>Agreement with CoreNLP predictions</p>
    <p>it</p>
    <p>random first</p>
    <p>last attention  agreement of attention is the highest</p>
    <p>last noun is the best heuristic</p>
  </div>
  <div class="page">
    <p>Agreement with CoreNLP predictions</p>
    <p>you I</p>
    <p>random first last attention  agreement of attention is the</p>
    <p>highest</p>
    <p>first noun is the best heuristic</p>
  </div>
  <div class="page">
    <p>Compared to human annotations for it</p>
    <p>last noun CoreNLP</p>
    <p>attention  pick 500 examples from the</p>
    <p>previous experiment</p>
    <p>ask human annotators to mark an antecedent</p>
    <p>pick examples where an antecedent is a noun phrase</p>
    <p>calculate the agreement with human antecedents</p>
  </div>
  <div class="page">
    <p>Attention map examples</p>
    <p>Source:</p>
    <p>And you, no doubt, would have broken it.</p>
    <p>There was a time I would have lost my heart to a</p>
    <p>face like yours.</p>
    <p>Context:</p>
  </div>
  <div class="page">
    <p>Attention map examples</p>
    <p>Source:</p>
    <p>And you, no doubt, would have broken it.</p>
    <p>There was a time I would have lost my heart to a</p>
    <p>face like yours.</p>
    <p>Context:</p>
  </div>
  <div class="page">
    <p>Attention map examples</p>
    <p>Source:</p>
    <p>And you, no doubt, would have broken it.</p>
    <p>There was a time I would have lost my heart to a</p>
    <p>face like yours.</p>
    <p>Context:</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>introduce a context-aware NMT system based on the Transformer</p>
    <p>the model outperforms both the context-agnostic baseline and a simple context-aware baseline (on an En-Ru corpus)</p>
    <p>pronoun translation is the key phenomenon captured by the model</p>
    <p>the model induces anaphora relations</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating Discourse</p>
    <p>Phenomena in Neural Machine Translation. In Proceedings of the 16th Annual Conference of the</p>
    <p>North American Chapter of the Association for Computational Linguistics: Human Language</p>
    <p>Technologies. New Orleans, USA.</p>
    <p>Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun Cho. 2017. Does Neural Machine</p>
    <p>Translation Benefit from Larger Context? In arXiv:1704.05135. ArXiv: 1704.05135.</p>
    <p>Pierre Lison, Jo rg Tiedemann, and Milen Kouylekov. 2018. Opensubtitles2018: Statistical</p>
    <p>rescoring of sentence alignments in large, noisy parallel corpora. In Proceedings of the Eleventh</p>
    <p>International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and</p>
    <p>David McClosky. 2014b. The Stanford CoreNLP natural language processing toolkit. In</p>
    <p>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System</p>
    <p>Demonstrations. Association for Computational Linguistics, Baltimore, Maryland, pages 5560.</p>
    <p>https://doi.org/10.3115/v1/P14-5010.</p>
    <p>Jo rg Tiedemann and Yves Scherrer. 2017. Neural Machine Translation with Extended Context. In</p>
    <p>Proceedings of the Third Workshop on Discourse in Machine Translation. Association for</p>
    <p>Computational Linguistics, Copenhagen, Denmark, DISCOMT17, pages 8292.</p>
    <p>https://doi.org/10.18653/v1/W17- 4811.</p>
    <p>Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017. Exploiting Cross-Sentence Con</p>
    <p>text for Neural Machine Translation. In Proceedings of the 2017 Conference on Empirical Methods</p>
    <p>in Natural Language Processing. Association for Computational Linguistics, Den- mark,</p>
    <p>Copenhagen, EMNLP17, pages 28162821. https://doi.org/10.18653/v1/D17-1301.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Ronan Le Nagard and Philipp Koehn. 2010. Aiding pronoun translation with coreference</p>
    <p>resolution. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and</p>
    <p>MetricsMATR. Association for Computational Linguistics, Uppsala, Sweden, pages 252261.</p>
    <p>http://www.aclweb.org/anthology/W10-1737.</p>
    <p>Christian Hardmeier and Marcello Federico. 2010. Modelling Pronominal Anaphora in Statistical</p>
    <p>Machine Translation. In Proceedings of the seventh International Workshop on Spoken Language</p>
    <p>Translation (IWSLT). pages 283289.</p>
    <p>Christian Hardmeier, Preslav Nakov, Sara Stymne, Jo rg Tiedemann, Yannick Versley, and Mauro</p>
    <p>Cettolo. 2015. Pronoun-Focused MT and Cross-Lingual Pronoun Prediction: Findings of the 2015</p>
    <p>DiscoMT Shared Task on Pronoun Translation. In Proceedings of the Second Workshop on</p>
    <p>Discourse in Machine Translation. Association for Computational Linguistics, Lisbon, Portugal,</p>
    <p>pages 116. https://doi.org/10.18653/v1/W15-2501.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui, and Andrea Gesmundo. 2012. Machine</p>
    <p>Translation of Labeled Discourse Connectives. In Proceedings of the Tenth Conference of the</p>
    <p>Association for Machine Translation in the Americas (AMTA). http://www.mt-archive.info/AMTA</p>
    <p>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based Document-level Statistical</p>
    <p>Machine Translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural</p>
    <p>Language Processing. Association for Computational Linguistics, Edinburgh, Scotland, UK.,</p>
    <p>pages 909919. http://www.aclweb.org/anthology/D11-1084.</p>
    <p>Marine Carpuat. 2009. One Translation Per Discourse. In Proceedings of the Workshop on</p>
    <p>Semantic Evaluations: Recent Achievements and Future Directions. Association for</p>
    <p>Computational Linguistics, Boulder, Colorado, pages 1927.</p>
    <p>http://www.aclweb.org/anthology/W09-2404.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based Document-level Statistical</p>
    <p>Machine Translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural</p>
    <p>Language Processing. Association for Computational Linguistics, Edinburgh, Scotland, UK.,</p>
    <p>pages 909919. http://www.aclweb.org/anthology/D11-1084.</p>
    <p>Jo rg Tiedemann. 2010. Context Adaptation in Statistical Machine Translation Using Models with</p>
    <p>Exponentially Decaying Cache. In Proceedings of the 2010 Workshop on Domain Adaptation for</p>
    <p>Natural Language Processing. Association for Computational Linguistics, Uppsala, Sweden,</p>
    <p>pages 815. http://www.aclweb.org/anthology/W10-2602.</p>
    <p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,</p>
    <p>Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS. Los Angeles.</p>
    <p>http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.</p>
  </div>
</Presentation>
