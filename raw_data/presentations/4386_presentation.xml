<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>EfficientLow-rank Multimodal Fusion With Modality-specific Factors</p>
    <p>Zhun Liu, Ying Shen, Varun Bharadwaj, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency</p>
  </div>
  <div class="page">
    <p>Artificial Intelligence</p>
  </div>
  <div class="page">
    <p>This movie is sick</p>
    <p>Speakers behaviors Sentiment Intensity</p>
    <p>time</p>
    <p>Sentiment and Emotion Analysis</p>
    <p>?</p>
    <p>Smile</p>
    <p>Loud</p>
  </div>
  <div class="page">
    <p>This movie is sick</p>
    <p>Speakers behaviors Sentiment Intensity</p>
    <p>time</p>
    <p>Multimodal Sentiment and Emotion Analysis</p>
    <p>? U ni m od</p>
    <p>al</p>
    <p>Multimodal Representation (Multimodal Fusion)</p>
    <p>Bi m od</p>
    <p>al</p>
    <p>Tr im</p>
    <p>od al</p>
    <p>Intra-modal Interactions</p>
    <p>Cross-modal Interactions</p>
    <p>Computational Efficiency</p>
    <p>Smile</p>
    <p>Loud</p>
  </div>
  <div class="page">
    <p>Multimodal Fusion using Tensor Representation</p>
    <p>This movie is sick</p>
    <p>Visual</p>
    <p>Language</p>
    <p>Bimodal</p>
    <p>Unimodal</p>
    <p>Tensor Fusion Network for Multimodal Sentiment Analysis by Zadeh, A., et, al. (2017)</p>
    <p>= # #  % 1 %</p>
    <p>= # 1</p>
    <p>% 1</p>
    <p>Intra-modal interactions</p>
    <p>Cross-modal interactions</p>
    <p>Computational efficiency</p>
    <p>Multimodal</p>
    <p>Representation</p>
    <p>||</p>
  </div>
  <div class="page">
    <p>Computational Complexity  Tensor Product</p>
    <p>()</p>
    <p>()</p>
    <p>3</p>
    <p>6</p>
    <p>M=2 M=3</p>
  </div>
  <div class="page">
    <p>CORE CONTRIBUTIONS</p>
    <p>Low-rank Multimodal Fusion (LMF)</p>
  </div>
  <div class="page">
    <p>From Tensor Representation to Low-rank Fusion</p>
    <p>Visual</p>
    <p>Language</p>
    <p>Decomposition of weight .</p>
    <p>Decomposition of input tensor .</p>
    <p>Rearrange the computation of .</p>
    <p>Visual</p>
    <p>Language</p>
    <p>Low-rank Multimodal Fusion</p>
    <p>Tensor Fusion Networks</p>
  </div>
  <div class="page">
    <p>Canonical Polyadic (CP) Decomposition of tensors</p>
  </div>
  <div class="page">
    <p>Canonical Polyadic (CP) Decomposition of 3D tensors</p>
    <p>||</p>
    <p>+</p>
    <p>|| ||</p>
  </div>
  <div class="page">
    <p>Modality-specific Decomposition</p>
    <p>||</p>
    <p>Retain the dimension for the multimodal representation  during decomposition</p>
    <p>|| ||</p>
  </div>
  <div class="page">
    <p>= ;</p>
    <p>%</p>
    <p>#</p>
    <p>Decomposition of weight tensor W</p>
  </div>
  <div class="page">
    <p>% (&gt;)</p>
    <p>+ +</p>
    <p># (&gt;)</p>
    <p>% (@)</p>
    <p># (@)</p>
    <p>;</p>
    <p>%</p>
    <p>#</p>
    <p>Decomposition of weight tensor W</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>% (&gt;)</p>
    <p>+ +</p>
    <p>% (@)</p>
    <p># (&gt;) #</p>
    <p>(@)</p>
    <p>;</p>
    <p>%</p>
    <p>#</p>
    <p>Decomposition of Z</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Rearranging computation</p>
  </div>
  <div class="page">
    <p>Low-rank Multimodal Fusion</p>
  </div>
  <div class="page">
    <p>Easily scales to more modalities</p>
    <p>Intra-modal interactions</p>
    <p>Cross-modal interactions</p>
    <p>Computational complexity</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS AND RESULTS</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>Emotion Recognition</p>
    <p>Segment level annotations  10 classes of emotions  Categorical annotations</p>
    <p>Sentiment Analysis</p>
    <p>Segment level annotations  Sentiment  Real-valued</p>
    <p>Speaker Trait Recognition</p>
    <p>Video level annotations  16 types of speaker traits  Categorical annotations</p>
    <p>CMU-MOSI POM IEMOCAP</p>
  </div>
  <div class="page">
    <p>Low-rank Multimodal Fusion (Our Model)</p>
    <p>Tensor Fusion Networks (Zadeh, et al., 2017)</p>
    <p>Compare to full rank tensor fusion</p>
    <p>Acc-7</p>
    <p>LMF</p>
    <p>TFN</p>
    <p>Correlation Acc-2MAE F1</p>
    <p>CMU-MOSI</p>
  </div>
  <div class="page">
    <p>Compare to full rank tensor fusion</p>
    <p>CorrelationMAE</p>
    <p>MAE Correlation F1-Happy F1-Sad 21</p>
    <p>CMU-MOSI POM IEMOCAP</p>
  </div>
  <div class="page">
    <p>Compare with State-of-the-Art Approaches</p>
    <p>Deep Fusion</p>
    <p>Mean Average Error (MAE)</p>
    <p>CMU-MOSI</p>
    <p>TFN</p>
    <p>MV-LSTM</p>
    <p>MARN</p>
    <p>LMF</p>
    <p>MFN</p>
    <p>Low-rank Multimodal Fusion (our model)</p>
    <p>Memory Fusion Networks (Zadeh, et al., 2018)</p>
    <p>Multi-attention Recurrent Networks (Zadeh, et al., 2018)</p>
    <p>Tensor Fusion Networks (Zadeh, et al., 2017)</p>
    <p>Multi-view LSTM (Rajagopalan, et al., 2016)</p>
    <p>Deep Fusion (Nojavanasghari, et al., 2016)</p>
  </div>
  <div class="page">
    <p>Compare with Top 2 State-of-the-Art Approaches</p>
    <p>CorrelationMAE</p>
    <p>MAE Correlation F1-Angry F1-Sad 23</p>
    <p>CMU-MOSI POM IEMOCAP</p>
    <p>TFN</p>
    <p>MV-LSTM</p>
    <p>MARN</p>
    <p>LMF</p>
    <p>MFN</p>
  </div>
  <div class="page">
    <p>Efficiency Improvement</p>
    <p>Training - samples/s Testing - samples/s</p>
    <p>LMF (Ours) TFN (Zadeh, et al., 2017)CMU-MOSI</p>
    <p>Efficiency Metric: Number of data samples processed per second</p>
    <p>Training Efficiency  Testing Efficiency</p>
  </div>
  <div class="page">
    <p>Conclusions Intra-modal interactions</p>
    <p>Cross-modal interactions</p>
    <p>Computational complexity</p>
    <p>State-of-the-art results</p>
  </div>
  <div class="page">
    <p>Code: https://github.com/Justin1904/Low-rank-Multimodal-Fusion</p>
    <p>Thank you!</p>
    <p>http://multicomp.cs.cmu.edu/</p>
  </div>
</Presentation>
