<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Fishy Faces: Crafting Adversarial Images to Poison Face Authentication Giuseppe Garofalo, Vera Rimmer, Tim Van hamme, Davy Preuveneers and Wouter Joosen</p>
    <p>WOOT 2018, August 13-14 (Baltimore, MD, USA)</p>
  </div>
  <div class="page">
    <p>Face authentication</p>
  </div>
  <div class="page">
    <p>Face authentication</p>
    <p>Wide adoption of face recognition in mobile devices</p>
    <p>Face authentication is a highly security-sensitive application</p>
    <p>Several attacks have been proposed (e.g replay attacks1, Bkavs mask2 etc.)</p>
    <p>[1] Face Anti-spoofing, Face Presentation Attack Detection [2] Bkavs new mask beats Face ID in &quot;twin way&quot;: Severity level raised, do not use Face ID in business transactions.</p>
  </div>
  <div class="page">
    <p>Face authentication - Machine Learning</p>
    <p>Authentication relies on Machine Learning (ML) algorithms</p>
    <p>they learn how to recognise the user through time and changes</p>
    <p>ML algorithms are not security-oriented per se Adversarial ML arms-race investigates the existing vulnerabilities, models active attacks and seeks for proactive countermeasures</p>
  </div>
  <div class="page">
    <p>Why poison face authentication?</p>
    <p>Adversarial ML has been applied to face recognition1, but not face authentication</p>
    <p>Face authentication systems are adaptive ML model is periodically re-trained</p>
    <p>Gives an attacker access prior to training</p>
    <p>Feasibility and efficacy of poisoning attacks against face authentication is yet unknown</p>
    <p>[1] Biggio, B., Didaci, L., Fumera, G., and Roli, F. Poisoning attacks to compromise face templates. In 2013 International Conference on Biometrics (ICB) (June 2013), pp. 17.</p>
  </div>
  <div class="page">
    <p>Background</p>
  </div>
  <div class="page">
    <p>Background - Machine learning</p>
    <p>Machine learning algorithms as a tool for learning patterns</p>
    <p>Patterns comprise biometric traits used for authenticating a person</p>
    <p>The classification task is divided into two phases: Training on a set of labelled points, i.e. the training set</p>
    <p>Testing the model by predicting the label of new points, i.e. the test set</p>
    <p>Each point is a feature vector</p>
    <p>Training minimizes a loss function 5</p>
  </div>
  <div class="page">
    <p>Background - Adversarial Machine Learning</p>
    <p>Adversarial ML investigates the ML algorithms in the adversarial environment</p>
    <p>The two main scenarios are:</p>
    <p>the evasion of the classification rule (post-training)</p>
    <p>the poisoning of the training set</p>
  </div>
  <div class="page">
    <p>Background - Poisoning SVM</p>
    <p>Poisoning requires the attackers to inject / control a malicious sample into the training set</p>
  </div>
  <div class="page">
    <p>Background - Poisoning SVM</p>
    <p>Poisoning requires the attackers to inject / control a malicious sample into the training set</p>
  </div>
  <div class="page">
    <p>Background - Poisoning SVM</p>
    <p>The attack point is moved towards a desired direction to maximize a loss function (instead of minimizing it)</p>
  </div>
  <div class="page">
    <p>Background - Poisoning SVM</p>
    <p>The re-training phase triggers the poisoning effects</p>
  </div>
  <div class="page">
    <p>Background - Attack point search</p>
    <p>The best attack point is the one that maximizes the loss function the most</p>
    <p>In this work, we apply an existing theoretical algorithm1</p>
    <p>Poisoning attack against SVM</p>
    <p>Focus on the hinge loss as a classification error estimate</p>
    <p>Gradient Ascent strategy to search the attack point</p>
    <p>[1] Biggio, B., Nelson, B., and Laskov, P., Poisoning attacks against SVM. (2012).</p>
  </div>
  <div class="page">
    <p>System under attack</p>
  </div>
  <div class="page">
    <p>System design</p>
    <p>Our target authenticator is composed of two parts:</p>
    <p>Feature extractor</p>
    <p>Classification model</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>System design</p>
    <p>Feature Extractor</p>
    <p>OpenFace Library</p>
    <p>Based on Googles FaceNet1 (Convolutional Neural Network)</p>
    <p>face detection  pre-processing  feature extraction</p>
    <p>Feature Extraction</p>
    <p>Input image</p>
    <p>[1] Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (2015), pp. 815823.</p>
  </div>
  <div class="page">
    <p>System design</p>
    <p>One-Class SVM for classification1 Trained only on images of the user</p>
    <p>Takes a hyper-parameter which defines the upperbound to the percentage of training errors</p>
    <p>One-Class SVM</p>
    <p>Feature Extraction</p>
    <p>[1] Inspired by: Gadaleta, M., and Rossi, M. Idnet: Smartphone-based gait recognition with convolutional neural networks. Pattern Recognition 74 (2018), 25  37.</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>System design</p>
    <p>Once trained, the model is used to authenticate the user</p>
    <p>One-Class SVM</p>
    <p>Feature Extraction</p>
    <p>Authentication</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>Attack methodology</p>
  </div>
  <div class="page">
    <p>Methodology - Threat Model</p>
    <p>One-Class SVM</p>
    <p>Feature Extraction</p>
    <p>Authentication</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>Methodology - Threat Model</p>
    <p>One-Class SVM</p>
    <p>Feature Extraction</p>
    <p>Authentication</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>Methodology - Threat Model</p>
    <p>Attackers goals:</p>
    <p>Denial-of-Service: to increase the false negative rate of the target authenticator</p>
    <p>Impersonation: to allow other identities to be authenticated as the rightful user</p>
    <p>A. Attackers resources:</p>
    <p>A. to poison the training set by injecting malicious images</p>
    <p>B. Has the knowledge of the models detail (including training images and model parameters</p>
  </div>
  <div class="page">
    <p>Methodology - Threat Model</p>
    <p>Attackers goals:</p>
    <p>Denial-of-Service: to increase the false negative rate of the target authenticator</p>
    <p>Impersonation: to allow other identities to be authenticated as the rightful user</p>
    <p>Attackers resources:</p>
    <p>Able to poison the training set by injecting malicious images</p>
    <p>Has the knowledge of the models detail (including training images and model parameters)</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>The attack methodology is divided into two parts:</p>
    <p>Obtain the attack point by using the gradient ascent strategy</p>
    <p>Reverse the feature extraction process to inject a real-world image</p>
  </div>
  <div class="page">
    <p>Methodology - Step-by-step</p>
    <p>Obtain the images used for training the model to train an exact copy of our target</p>
    <p>Target SVM</p>
    <p>Copy SVM</p>
    <p>Authentication</p>
    <p>Feature Extraction</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>Methodology - Step-by-step</p>
    <p>Find the best attack point using the gradient ascent strategy the best attack point is the one which maximizes the classification error</p>
    <p>It is found by modifying the feature vector of a validation set image</p>
    <p>Attack point Copy SVM</p>
    <p>Target SVM</p>
    <p>Feature Extraction</p>
    <p>Authentication</p>
    <p>Input image</p>
  </div>
  <div class="page">
    <p>Methodology - Step-by-step</p>
    <p>Find a face image corresponding to the best attack point</p>
    <p>A best-first search strategy to reverse the CNN function is exploited</p>
    <p>Attack point Copy SVM</p>
    <p>Target SVM</p>
    <p>Feature Extraction</p>
    <p>Adversarial Image</p>
    <p>Authentication</p>
  </div>
  <div class="page">
    <p>Methodology - Step-by-step</p>
    <p>Present the image to the system which will be re-trained over the new sample, affecting the authentication procedure</p>
    <p>Attack point Copy SVM</p>
    <p>Target SVM</p>
    <p>Feature Extraction</p>
    <p>Adversarial Image</p>
    <p>Authentication</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>The target One-Class SVM is trained to recognize one identity Data is collected from the FaceScrub celebrity dataset</p>
    <p>Training set is composed by 30 images</p>
    <p>Authenticated user 14</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>Raw attack point</p>
    <p>Attack point</p>
    <p>The attack point is computed by using the gradient ascent technique, starting from the feature vector of a randomlychosen validation image</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>Raw attack point Pre-processing Reverse-CNN</p>
    <p>A sliding window is used to apply modifications to the image so that its feature vector becomes very similar to the attack point</p>
    <p>Attack point</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>Raw attack point Pre-processing Reverse-CNN</p>
    <p>Feature VectorAttack point Feature Extraction</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>Injected image</p>
    <p>After the injection, the classification accuracy drops from 4% to 44% (by 40%!)</p>
    <p>False positive Unauthorised User</p>
    <p>False negative Authorised User</p>
  </div>
  <div class="page">
    <p>Methodology - Attack example</p>
    <p>Injected image</p>
    <p>Using just a random image, the classification accuracy drops by 2%</p>
    <p>True negative Unauthorised User</p>
    <p>True positive Authorised User</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Evaluation C</p>
    <p>la ss</p>
    <p>ifi ca</p>
    <p>tio n</p>
    <p>er ro</p>
    <p>r</p>
    <p>Training set size</p>
    <p>Test set Validation set</p>
  </div>
  <div class="page">
    <p>Evaluation C</p>
    <p>la ss</p>
    <p>ifi ca</p>
    <p>tio n</p>
    <p>er ro</p>
    <p>r</p>
    <p>Training set size</p>
    <p>Test set Validation set  The effectiveness is greatly</p>
    <p>reduced as bigger training sets are used to train the model</p>
    <p>However, a huge number of images is not always available</p>
  </div>
  <div class="page">
    <p>Evaluation C</p>
    <p>la ss</p>
    <p>ifi ca</p>
    <p>tio n</p>
    <p>er ro</p>
    <p>r</p>
    <p>Percentage of training errors</p>
    <p>Test set Validation set</p>
  </div>
  <div class="page">
    <p>Evaluation C</p>
    <p>la ss</p>
    <p>ifi ca</p>
    <p>tio n</p>
    <p>er ro</p>
    <p>r</p>
    <p>Percentage of training errors</p>
    <p>Test set Validation set As before, the effectiveness of the attack can be reduced during the tuning phase</p>
    <p>Increasing this value leads to a higher false negative rate  usability is lower</p>
  </div>
  <div class="page">
    <p>Limitations</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>The poisoning attack relies on two assumptions on the attackers capabilities</p>
    <p>Knowledge of the training images of the target user</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>The poisoning attack relies on two assumptions on the attackers capabilities</p>
    <p>Knowledge of the training images of the target user</p>
    <p>Transferability property can be exploited to train a model without knowing training images</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>The poisoning attack relies on two assumptions on the attackers capabilities</p>
    <p>Knowledge of the training images of the target user</p>
    <p>Ability to inject an image into the training set</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>The poisoning attack relies on two assumptions on the attackers capabilities</p>
    <p>Knowledge of the training images of the target user</p>
    <p>Ability to inject an image into the training set</p>
    <p>Continuously-adapted injection strategies may be useful to break the authentication step</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>In this work we:</p>
    <p>Apply a poisoning attack against a state-of-the-art face authentication model  obtain classification error of over 50% with one injected image</p>
    <p>Demonstrate how to defend against such attacks through careful design choices</p>
    <p>Show the feasibility to attack a multi-stage authentication process involving face recognition with a reverse-mapping strategy</p>
    <p>This work urges to integrate awareness of adversarial ML attacks into all stages of the authentication system design</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>In this work we:</p>
    <p>Apply a poisoning attack against a state-of-the-art face authentication model  obtain classification error of over 50% with one injected image</p>
    <p>Demonstrate how to defend against such attacks through careful design choices</p>
    <p>Show the feasibility to attack a multi-stage authentication process involving face recognition with a reverse-mapping strategy</p>
    <p>This work urges to integrate awareness of adversarial ML attacks into all stages of the authentication system design</p>
  </div>
  <div class="page">
    <p>Thank you! https://distrinet.cs.kuleuven.be/</p>
  </div>
</Presentation>
