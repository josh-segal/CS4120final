<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mingyu Wu1, Ziming Zhao1, Yanfei Yang1, Haoyu Li1, Haibo Chen1, Binyu Zang1, Haibing Guan1, Sanhong Li2, Chuansheng Lu2, Tongbao Zhang2</p>
  </div>
  <div class="page">
    <p>A page is multiple services</p>
    <p>checkout</p>
    <p>check-out page</p>
    <p>Coupon Cart Recommendation</p>
    <p>Cache Cache</p>
    <p>A user request relies on multiple services  Small, single-purposed, and interactive</p>
    <p>interactive services</p>
    <p>user</p>
  </div>
  <div class="page">
    <p>A page is multiple services</p>
    <p>checkout</p>
    <p>check-out page</p>
    <p>Recommendation</p>
    <p>Cache Cache</p>
    <p>A user request relies on multiple services  Small, single-purposed, and interactive</p>
    <p>Most online services in Alibaba are written in Java</p>
    <p>interactive services</p>
    <p>user</p>
    <p>Coupon Cart</p>
  </div>
  <div class="page">
    <p>A page is multiple services</p>
    <p>checkout</p>
    <p>check-out page</p>
    <p>Recommendation</p>
    <p>Cache Cache</p>
    <p>A user request relies on multiple services  Small, single-purposed, and interactive</p>
    <p>Most online services in Alibaba are written in Java  Services might be paused by the modules in Java runtime</p>
    <p>interactive services</p>
    <p>user</p>
    <p>Coupon Cart</p>
  </div>
  <div class="page">
    <p>( )</p>
    <p>Language runtimes (like JVM) leverages GC for automatic memory management</p>
    <p>Prior GC in JVM is Stop-The-World (STW) GC  Pause application threads (mutators) for memory reclamation  Pros: High GC throughput, satisfying CPU efficiency  Cons: High application latency</p>
    <p>Mutators</p>
    <p>GC Threads</p>
    <p>Idle</p>
    <p>GC pause</p>
  </div>
  <div class="page">
    <p>- -</p>
    <p>Running the Coupon service from Alibaba for 30 seconds  Each GC cycle follows stragglers  GC is the killer factor for the tail latency</p>
    <p>R eq</p>
    <p>ue st</p>
    <p>L at</p>
    <p>en cy</p>
    <p>(m s)</p>
    <p>GC-induced long tail</p>
  </div>
  <div class="page">
    <p>Allowing mutators to execute even during collection  Effective on reducing GC pauses  Two categories: partially-concurrent / mostly-concurrent</p>
    <p>Partially-concurrent collectors (example: G1)  Mutators can execute in some GC phases (other phases are still STW)  Proposing running arguments to reduce the duration of STW phases</p>
    <p>Mutators</p>
    <p>GC Threads</p>
    <p>Tuned pause</p>
  </div>
  <div class="page">
    <p>Allowing mutators to execute even during collection  Effective on reducing GC pauses  Two categories: partially-concurrent / mostly-concurrent</p>
    <p>Mostly-concurrent collectors (example: Shenandoah)  Mutators can execute in nearly all GC phases  Only introducing short pauses in some synchronization points</p>
    <p>Mutators</p>
    <p>GC Threads</p>
    <p>Short pauses</p>
  </div>
  <div class="page">
    <p>Partially concurrent GC: Tuning may lead to worse tail latency  Evaluation on G1: tuning the MaxGCPauseMillis argument to restrict pause time  Decreasing MaxGCPauseMillis can reduce per-GC pauses</p>
    <p>MaxGCPauseMillis 30ms 40ms 60ms</p>
    <p>Min. GC pause (ms) 21.815 21.459 39.856</p>
    <p>Avg. GC pause (ms) 34.441 40.724 48.491</p>
    <p>The number of GC 550 392 111</p>
    <p>Avg. CPU util. 51.45% 50.81% 36.17%</p>
    <p>p99 latency (ms) 1942.09 1389.99 148.85</p>
  </div>
  <div class="page">
    <p>Partially concurrent GC: Tuning may lead to worse tail latency  Evaluation on G1: tuning the MaxGCPauseMillis argument to restrict pause time  Decreasing MaxGCPauseMillis can reduce per-GC pauses  But the GC frequency increase and consume more CPU resource</p>
    <p>MaxGCPauseMillis 30ms 40ms 60ms</p>
    <p>Min. GC pause (ms) 21.815 21.459 39.856</p>
    <p>Avg. GC pause (ms) 34.441 40.724 48.491</p>
    <p>The number of GC 550 392 111</p>
    <p>Avg. CPU util. 51.45% 50.81% 36.17%</p>
    <p>p99 latency (ms) 1942.09 1389.99 148.85</p>
  </div>
  <div class="page">
    <p>Partially concurrent GC: Tuning may lead to worse tail latency  Evaluation on G1: tuning the MaxGCPauseMillis argument to restrict pause time  Decreasing MaxGCPauseMillis can reduce per-GC pauses  But the GC frequency increase and consume more CPU resource  Result: worse application tail latency</p>
    <p>MaxGCPauseMillis 30ms 40ms 60ms</p>
    <p>Min. GC pause (ms) 21.815 21.459 39.856</p>
    <p>Avg. GC pause (ms) 34.441 40.724 48.491</p>
    <p>The number of GC 550 392 111</p>
    <p>Avg. CPU util. 51.45% 50.81% 36.17%</p>
    <p>p99 latency (ms) 1942.09 1389.99 148.85</p>
  </div>
  <div class="page">
    <p>Mostly-concurrent GC consumes even more CPU resource  Evaluation on Shenandoah: the same application throughput as G1  The GC pauses in Shenandoah become quite short</p>
    <p>GC Type G1-30ms Shenandoah</p>
    <p>Min. GC pause (ms) 21.815 5.860</p>
    <p>Avg. GC pause (ms) 34.441 18.764</p>
    <p>GC Duration (s) 18.94 53.05</p>
    <p>Avg. CPU util. 51.45% 83.05%</p>
    <p>p99 latency (ms) 1942.09 3614.58</p>
  </div>
  <div class="page">
    <p>Mostly-concurrent GC consumes even more CPU resource  Evaluation on Shenandoah: the same application throughput as G1  The GC pauses in Shenandoah become quite short  But: the overall GC time is longer and consumes more CPU resource</p>
    <p>GC Type G1-30ms Shenandoah</p>
    <p>Min. GC pause (ms) 21.815 5.860</p>
    <p>Avg. GC pause (ms) 34.441 18.764</p>
    <p>GC Duration (s) 18.94 53.05</p>
    <p>Avg. CPU util. 51.45% 83.05%</p>
    <p>p99 latency (ms) 1942.09 3614.58</p>
  </div>
  <div class="page">
    <p>Mostly-concurrent GC consumes even more CPU resource  Evaluation on Shenandoah: the same application throughput as G1  The GC pauses in Shenandoah become quite short  But: the overall GC time is longer and consumes more CPU resource  Result: worse application tail latency</p>
    <p>GC Type G1-30ms Shenandoah</p>
    <p>Min. GC pause (ms) 21.815 5.860</p>
    <p>Avg. GC pause (ms) 34.441 18.764</p>
    <p>GC Duration (s) 18.94 53.05</p>
    <p>Avg. CPU util. 51.45% 83.05%</p>
    <p>p99 latency (ms) 1942.09 3614.58</p>
  </div>
  <div class="page">
    <p>Contentions on CPU resources  GC threads and mutators may run on the same CPU</p>
    <p>Synchronizations  GC threads have to synchronize with mutators when accessing the same object</p>
    <p>Software barrier code  Mutators should check invariants before every read/write operations</p>
    <p>GC Threads copy Object Mutators</p>
    <p>write</p>
    <p>y.x = z;</p>
    <p>if (is_being_collected(y)) { slow_path(y.x, z);</p>
    <p>} else { y.x = z;</p>
    <p>}</p>
    <p>Barrier code</p>
  </div>
  <div class="page">
    <p>Contentions on CPU resources  GC threads and mutators may run on the same CPU</p>
    <p>Synchronizations  GC threads have to synchronize with mutators when accessing the same object</p>
    <p>Software barrier code  Mutators should check invariants before every read/write operations</p>
    <p>GC Thread copy Object Mutators</p>
    <p>write</p>
    <p>y.x = z;</p>
    <p>if (is_being_collected(y)) { slow_path(y.x, z);</p>
    <p>} else { y.x = z;</p>
    <p>}</p>
    <p>Barrier code</p>
    <p>Can we design a collector with both low latency and high CPU-efficiency?</p>
  </div>
  <div class="page">
    <p>It is hard to reach perfect scalability for GC  Workload for each GC threads is unknown before processing -&gt; imbalance</p>
    <p>Evaluation on Specjbb2015 with 80 logic cores  Even for STW GC, performance remains stable when reaching 30 cores</p>
    <p>A cc</p>
    <p>um ul</p>
    <p>at ed</p>
    <p>G C</p>
    <p>ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>PSGC</p>
  </div>
  <div class="page">
    <p>It is hard to reach perfect scalability for GC  Workload for each GC threads is unknown before processing -&gt; imbalance</p>
    <p>Evaluation on Specjbb2015 with 80 logic cores  Even for STW GC, performance remains stable when reaching 30 cores</p>
    <p>The default setting in OpenJDK: using less GC threads  The default GC thread number with 80 cores: 53</p>
  </div>
  <div class="page">
    <p>Interactive services process requests in sessions  A service creates a session for each request</p>
    <p>Skewed write behaviors: most writes fall in the newly-created objects (named working-set)</p>
    <p>request</p>
    <p>response</p>
    <p>write operations</p>
  </div>
  <div class="page">
    <p>Interactive services process requests in sessions  A service creates a session for each request</p>
    <p>Skewed write behaviors: most writes fall in the newly-created objects (named working-set)</p>
    <p>Different services show similar behaviors  Other applications (Spark) are different</p>
    <p>0.00</p>
    <p>ra tio</p>
    <p>o f s</p>
    <p>es si</p>
    <p>on s</p>
    <p>Cassandra Coupon ShopCenter Spark SpecJBB</p>
    <p>request</p>
    <p>response</p>
    <p>write operations</p>
  </div>
  <div class="page">
    <p>MPK (Memory Protection Keys) allows intra-process page permission configuration</p>
    <p>Each page in a process belongs to a domain  Each thread can gain different permissions on different domains</p>
    <p>MPK is mainly exploited for security consideration  Is it possible to use MPK for performance improvement?</p>
    <p>Dom 0</p>
    <p>Dom 2</p>
    <p>Dom 1 Thread 1 R/W</p>
    <p>RThread 2</p>
    <p>R</p>
    <p>R/W</p>
  </div>
  <div class="page">
    <p>A mostly-concurrent collector with moderate CPU consumption  Low latency: allowing mutators to execute with GC threads  CPU-efficient: removing synchronizations and barriers</p>
    <p>Execution flow: a three-phase collection algorithm  Init pause : initializing the collection (&lt; 1ms)  Concurrent scavenge: mutators can execute (but with restrictions)</p>
    <p>Final pause: updating stale references (~10ms)</p>
    <p>Mutators</p>
    <p>GC Threads</p>
    <p>Init FinalConcurrent</p>
  </div>
  <div class="page">
    <p>Idle core collection  Collecting idle cores (not used by GC threads) and give them to mutators</p>
    <p>Heap partition</p>
    <p>Barrier elimination Collection Area Pinned Allocation</p>
    <p>Eden Space</p>
    <p>Idle cores</p>
    <p>GC thread Mutators</p>
    <p>From To Old Space</p>
    <p>Collection Area</p>
    <p>MPK domain 0</p>
    <p>MPK domain 1</p>
  </div>
  <div class="page">
    <p>Idle core collection</p>
    <p>Heap partition  Isolating the memory between threads  Minimizing inter-thread synchronizations</p>
    <p>Barrier elimination</p>
    <p>Collection Area Pinned Allocation</p>
    <p>Eden Space</p>
    <p>Idle cores</p>
    <p>GC thread Mutators</p>
    <p>From To Old Space</p>
    <p>Collection Area</p>
    <p>MPK domain 0</p>
    <p>MPK domain 1</p>
  </div>
  <div class="page">
    <p>Idle core collection</p>
    <p>Heap partition</p>
    <p>Barrier elimination  Using MPK to divide heap into domains  Removing the needs for barriers</p>
    <p>Collection Area Pinned Allocation</p>
    <p>Eden Space</p>
    <p>Idle cores</p>
    <p>GC thread Mutators</p>
    <p>From To Old Space</p>
    <p>Collection Area</p>
    <p>MPK domain 0</p>
    <p>MPK domain 1</p>
  </div>
  <div class="page">
    <p>Binding GC threads to separated cores (with sched_setaffinity)  Each GC thread can monopoly its core during GC  Other unused cores are identified as idle cores</p>
    <p>Changing the affinity of mutators with/without GC  During GC: mutators can only run on idle cores  Out of GC: mutators can run on all cores</p>
    <p>Avoiding CPU contention between GC threads and mutators</p>
  </div>
  <div class="page">
    <p>Partition the heap into three areas according to the skewed write behavior</p>
    <p>Collection area: containing objects under collection  Pinned area: containing active objects being used by mutators (estimated)  Allocation area: reserved for memory allocation during GC</p>
    <p>Collection Pinned Allocation</p>
    <p>Bump pointer</p>
  </div>
  <div class="page">
    <p>Partition the heap into three areas according to the skewed write behavior  Collection area: containing objects under collection  Pinned area: containing active objects being used by mutators (estimated)  Allocation area: reserved for memory allocation during GC</p>
    <p>Enforcing isolation for GC threads and mutators  GC threads only collect objects in the collection area  Mutators will mostly access the other two areas</p>
    <p>Collection Pinned Allocation</p>
    <p>GC Threads Mutators</p>
  </div>
  <div class="page">
    <p>It is still possible for mutators to access the collection area  Although it is rare (due to the skewed write behavior)</p>
    <p>Traditional solution: write barriers  Checking the address before every write -&gt; runtime overhead</p>
    <p>if (is_in_collection_area(y)) { slow_path(y.x, z);</p>
    <p>} else { y.x = z;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>It is still possible for mutators to access the collection area  Although it is rare (due to the skewed write behavior)</p>
    <p>Our solution: MPK-based hardware protection  Mutators only has read-only permission on the collection area  Mutator-writes to the collection area will trigger page faults and thus be corrected</p>
    <p>Collection Pinned Allocation</p>
    <p>Domain 1 Domain 2</p>
    <p>GC Threads RW</p>
    <p>RW Mutators</p>
    <p>RW R</p>
  </div>
  <div class="page">
    <p>Hardware: Intel Gold 6138 CPU (80 cores)</p>
    <p>Software: OpenJDK 8u141, 16GB Java heap</p>
    <p>Baseline:  Concurrent-Mark-Sweep (CMS): a classic partially concurrent collector  Garbage-First (G1): a tunable partially concurrent collector (default in OpenJDK 9)  Shenandoah: a mostly-concurrent collector</p>
    <p>Applications:  Specjbb2015: Asimulated online supermarket (web services)  Cassandra: Akey-value store (storage service)  Coupon: A real service inAlibaba</p>
  </div>
  <div class="page">
    <p>Evaluating its performance with different throughput settings  Better than CMS (up to 79.3%), comparable with G1  The best maximum throughput among all</p>
    <p>p9 9</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>CMS G1tuned Platinum Shenandoah</p>
    <p>p9 9</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(m s)</p>
    <p>CMS G1tuned Platinum Shenandoah</p>
  </div>
  <div class="page">
    <p>Evaluation with two YCSB workload settings:  Read-intensive (76000 reads and 4000 updates per second)  Write-intensive (40000 reads and 40000 updates per second)  Result:</p>
    <p>Comparable with Shenandoah in read-intensive  Less improvement in write-intensive but still better than CMS</p>
    <p>0.97</p>
    <p>Pe rc</p>
    <p>en til</p>
    <p>e</p>
    <p>CMS G1 Platinum Shenandoah</p>
    <p>Pe rc</p>
    <p>en til</p>
    <p>e</p>
    <p>CMS G1 Platinum Shenandoah Read Intensive Write Intensive</p>
  </div>
  <div class="page">
    <p>Evaluation with production traces in Alibaba  96-core machine, 16GB Java heap  66.8% and 23.5% improvement on p99 latency compared with CMS and G1</p>
    <p>P er</p>
    <p>ce nt</p>
    <p>ile</p>
    <p>CMS G1 Platinum</p>
  </div>
  <div class="page">
    <p>Collecting the average CPU utilization with Linux sar  CMS induces moderate CPU consumption (but poor latency)  Platinum has better CPU utilization compared with G1 and Shenandoah</p>
    <p>Application CMS G1 Shenandoah Platinum Specjbb2015 48.79% 77.66% 77.80% 50.56% Cassandra 12.10% 15.97% 14.93% 13.79%</p>
    <p>Coupon 38.47% 36.17% 83.05% 34.50%</p>
  </div>
  <div class="page">
    <p>Conclusion  Prior GC makes a trade-off between latency and CPU efficiency</p>
    <p>Platinum: a mostly-concurrent GC with satisfying CPU efficiency  Idle core collection -&gt; mitigate CPU contention between threads  Heap partition -&gt; minimize synchronizations  MPK-based barrier elimination -&gt; reduce runtime overhead</p>
    <p>Achieving both low latency and moderate CPU consumption for interactive services</p>
    <p>Thanks! Q&amp;A: mingyuwu93@gmail.com</p>
  </div>
</Presentation>
