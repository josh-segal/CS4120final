<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>2006 Hewlett-Packard Development Company, L.P. The information contained herein is subject to change without notice</p>
    <p>Profiling and Modeling Resource Usage of Virtualized Applications Timothy Wood1, Lucy Cherkasova2, Kivanc Ozonat2, and Prashant Shenoy1 1University of Massachusetts, Amherst 2HPLabs, Palo Alto</p>
  </div>
  <div class="page">
    <p>Virtualized Data Centers</p>
    <p>Benefits Lower hardware and energy costs through server</p>
    <p>consolidation Capacity on demand, agile and dynamic IT</p>
    <p>Challenges Apps are characterized by a collection of resource</p>
    <p>usage traces in native environment Virtualization overheads Effects of consolidating multiple VMs to one</p>
    <p>host  Important for capacity planning and efficient</p>
    <p>server consolidation</p>
  </div>
  <div class="page">
    <p>Application Virtualization Overhead</p>
    <p>Many research papers measure virtualization overhead but do not predict it in a general way: A particular hardware platform</p>
    <p>A particular app/benchmark, e.g., netperf, Spec or SpecWeb, disk benchmarks</p>
    <p>Max throughput/latency/performance is X% worse</p>
    <p>Showing Y% increase in CPU resources</p>
    <p>How do we translate these measurements in what is a virtualization overhead for a given application?</p>
    <p>New performance models are needed</p>
  </div>
  <div class="page">
    <p>Predicting Resource Requirements</p>
    <p>Most overhead caused by I/O Network and Disk activity</p>
    <p>Xen I/O Model  2 components</p>
    <p>Dom0 handles I/O</p>
    <p>Must predict CPU needs of: 1. Virtual machine running the application</p>
    <p>Requires several prediction models based</p>
    <p>on multiple resources</p>
    <p>VM</p>
    <p>Domain0</p>
  </div>
  <div class="page">
    <p>Problem Definition</p>
    <p>T1</p>
    <p>C P U</p>
    <p>T1</p>
    <p>N e tw</p>
    <p>o rk</p>
    <p>T1</p>
    <p>D is</p>
    <p>k</p>
    <p>Native Application Trace</p>
    <p>T1</p>
    <p>V M</p>
    <p>C P U</p>
    <p>T1</p>
    <p>D o m</p>
    <p>P U</p>
    <p>Virtualized Application Trace</p>
    <p>? ?</p>
  </div>
  <div class="page">
    <p>Why Bother?</p>
    <p>More accurate cost/benefit analysis Capacity planning and VM placement</p>
    <p>Impossible to pre-test some critical services  Hypervisor comparisons</p>
    <p>Different platforms or versions</p>
    <p>App 1</p>
    <p>App 2 VM 1 VM 2 Dom 0</p>
    <p>+</p>
    <p>Native Virtual</p>
    <p>CPU Util</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>Automated robust model generation  Run benchmark set on native and virtual</p>
    <p>platforms Performs a range of I/O and CPU intensive tasks Gather resource traces</p>
    <p>Build model of Native --&gt; Virtual relationship Use linear regression techniques</p>
    <p>Model is specific to platform, but not applications</p>
    <p>Automate all the steps in the process Can apply this general model to any</p>
    <p>applications traces to predict its requirements</p>
    <p>Native system usage profile</p>
    <p>Virtual system usage profile</p>
    <p>model ?</p>
  </div>
  <div class="page">
    <p>Microbenchmark Suite</p>
    <p>Focus on CPU-intensive and different types of I/O-intensive client-server apps</p>
    <p>Benchmark activities: Network-intensive: download and upload files Disk-intensive: read and write files CPU-intensive</p>
    <p>Need to break correlations between resources High correlation between packets/sec and CPU</p>
    <p>time  Simplicity of implementation</p>
    <p>based on httperf, Apache Jmeter, Apache Web Server and PHP</p>
    <p>Microbenchmarks are easy to run in a traditional data center environment</p>
  </div>
  <div class="page">
    <p>Model Generation</p>
    <p>nativ e</p>
    <p>virtual</p>
    <p>Model VM:</p>
    <p>Model Dom0:</p>
    <p>Set of equations</p>
    <p>to solve:</p>
    <p>Set of equations</p>
    <p>to solve:</p>
    <p>model ?</p>
  </div>
  <div class="page">
    <p>Building Robust Models</p>
    <p>Outliers can considerably impact regression models Creates model that minimizes absolute error</p>
    <p>Must use robust regression techniques to eliminate outliers</p>
    <p>Not all metrics are equally significant Starts with 11 metrics: 3 CPU, 4 Network, and 4 Disk</p>
    <p>Use stepwise regression to find most significant metrics</p>
    <p>Evaluate outcome of microbenchmark runs and eliminate erroneous and corrupted data</p>
    <p>Correct data set is a prerequisite for building an accurate model</p>
  </div>
  <div class="page">
    <p>Performance Evaluation: Testbed Details</p>
    <p>Two hardware platforms HP ProLiant DL385, 2-way AMD Opteron, 2.6GHz, 64-bit HP ProLiant DL580, 4-way Intel Xeon, 1.6GHz, 32-bit</p>
    <p>Two applications: RUBiS (auction site, modeled after e-Bay) TPC-W (e-commerce site, modeled after Amazon.com)</p>
    <p>Monitoring Native: sysstat Virtual: xenmon and xentop Measurements: 30 sec intervals</p>
  </div>
  <div class="page">
    <p>Questions</p>
    <p>Why this set of metrics?  Why these benchmarks?  Why this process of model creation?  Model accuracy</p>
  </div>
  <div class="page">
    <p>Importance of Modeling I/O</p>
    <p>Is it necessary to look at resources other than just total CPU?</p>
    <p>How accurate such a simplified model for predicting the CPU requirement of VM ?</p>
    <p>Prediction Error</p>
    <p>C u</p>
    <p>m u</p>
    <p>la tiv</p>
    <p>e P</p>
    <p>ro b</p>
    <p>a b</p>
    <p>ili ty</p>
    <p>VM Error CDF</p>
    <p>CPU Scaling Only Multi-resource</p>
    <p>Definitely need multiple resources!</p>
  </div>
  <div class="page">
    <p>Benchmark Coverage</p>
    <p>Using a subset of benchmarks leads to a poor accuracy model</p>
    <p>Why these benchmarks?</p>
  </div>
  <div class="page">
    <p>Automated Benchmark Error Detection</p>
    <p>Some benchmarks run incorrectly Rates too high</p>
    <p>Background activity</p>
    <p>Remove benchmarks with abnormally high error rates</p>
    <p>Automatically remove bad benchmarks without eliminating useful data</p>
  </div>
  <div class="page">
    <p>Model Accuracy  Intel hardware platform  Train the model using simple benchmarks  Apply to RUBiS web application</p>
  </div>
  <div class="page">
    <p>Second Hardware Platform</p>
    <p>AMD, 64bit dual CPU, 2.6Ghz</p>
    <p>Produces different model parameters Predictions are just as accurate</p>
  </div>
  <div class="page">
    <p>Different Platforms Virtualization Overhead</p>
    <p>Different platforms exhibit different amount of CPU overhead</p>
    <p>To predict virtualization overhead for different hardware platforms require building their own models</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed approach builds a model for each hardware and virtualization platform.</p>
    <p>It enables comparison of application resource requirements on different hardware platforms.</p>
    <p>Interesting additional application: helps to assess and compare performance overhead of different virtualization software releases.</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Refine a set of microbenchmarks and related measurements (what is a practical minimal set?)</p>
    <p>Repeat the experiments for VMware platform  Linear models  are they enough?</p>
    <p>Create multiple models for resources with different overheads at different rates</p>
    <p>Evaluation of virtual device capacity  Define composition rules for estimating resource</p>
    <p>requirements of collocated virtualized applications</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
