<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Framework for Representing Language Acquisition in a Population Setting Jordan Kodner Christopher Cerezo Falco University of Pennsylvania</p>
    <p>ACL - July 16, 2018 Melbourne</p>
  </div>
  <div class="page">
    <p>Language Change</p>
    <p>Languages change over time  Both an internal and external process  Fundamentally social  Individuals acquire language and transmit it to future generations  New variants propagate through populations</p>
    <p>Modelling Change  Must model how the individual reacts to linguistic input and to the</p>
    <p>community</p>
  </div>
  <div class="page">
    <p>Example - The Cot-Caught Merger  // cot is pronounced the same</p>
    <p>as // caught  Minimal pairs distinguished by</p>
    <p>//~// become homophones</p>
    <p>// // cot caught Don Dawn collar caller knotty naughty odd awed pond pawned</p>
    <p>Merged Unmerged</p>
  </div>
  <div class="page">
    <p>Example - The Cot-Caught Merger  // cot is pronounced the same</p>
    <p>as // caught  Present in many dialects of North</p>
    <p>American English  Eastern New England  Western Pennsylvania  Lower Midwest  West  Canada (all)</p>
    <p>Merged Unmerged</p>
  </div>
  <div class="page">
    <p>Example - The Cot-Caught Merger  // cot is pronounced the same</p>
    <p>as // caught  Present in many dialects of North</p>
    <p>American English  Eastern New England  Western Pennsylvania  Lower Midwest  West  Canada (all)</p>
    <p>Spreading into Rhode Island (Johnson 2007)</p>
    <p>Merged Unmerged</p>
  </div>
  <div class="page">
    <p>Example - The Cot-Caught Merger  // cot is pronounced the same</p>
    <p>as // caught  Present in many dialects of North</p>
    <p>American English  Eastern New England  Western Pennsylvania  Lower Midwest  West  Canada (all)</p>
    <p>Spreading into Rhode Island  Rapid! Families with Non-merged</p>
    <p>parents and older siblings but merged younger siblings 6</p>
    <p>Merged Unmerged</p>
  </div>
  <div class="page">
    <p>Existing Frameworks</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Kenny 2013</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Kenny 2013 + Bloomfield (1933)s Principle of Density for free + Diffusion is straightforward - Not a lot of control over the network - Thousands of degrees of freedom</p>
    <p>-&gt; should run many many times -&gt; slow</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Speakers are nodes in a graph, edges are possibility of interaction  e.g., Baxter et al. 2006, Baxter et al. 2009, Blythe &amp; Croft 2012, Fagyal et</p>
    <p>al. 2010, Minett &amp; Wang 2008, Kauhanen 2016</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Speakers are nodes in a graph, edges are possibility of interaction  e.g., Baxter et al. 2006, Baxter et al. 2009, Blythe &amp; Croft 2012, Fagyal et</p>
    <p>al. 2010, Minett &amp; Wang 2008, Kauhanen 2016 + Much more control over network structure + Easy to model concepts from the sociolinguistic lit. (e.g., Milroy &amp; Milroy)</p>
    <p>- Nodes only interact with immediate neighbours -&gt; slow and less realistic? - Practically implemented as random interactions between neighbours -&gt;</p>
    <p>same problem as #1 12</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Expected outcome of interactions is calculated analytically  e.g., Abrams &amp; Stroganz 2003, Baxter et al. 2006, Minett &amp; Wang 2008,</p>
    <p>Niyogi &amp; Berwick 1997, Yang 2000, Niyogi &amp; Berwick 2009</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>Expected outcome of interactions is calculated analytically  e.g., Abrams &amp; Stroganz 2003, Baxter et al. 2006, Minett &amp; Wang 2008,</p>
    <p>Niyogi &amp; Berwick 1997, Yang 2000, Niyogi &amp; Berwick 2009 + Closed-form solution rather than simulation -&gt; faster and more direct - No network structure! Always implemented over perfectly mixed</p>
    <p>populations</p>
  </div>
  <div class="page">
    <p>Three Classes of Framework</p>
    <p>This proliferation of boutique frameworks is a problem  An ad hoc framework risks overfitting the pattern  Comparison between frameworks is challenging</p>
  </div>
  <div class="page">
    <p>Our Framework</p>
  </div>
  <div class="page">
    <p>Best of All Worlds</p>
    <p>Impose density effects on a network structure and calculate the outcome of each iteration analytically</p>
  </div>
  <div class="page">
    <p>Best of All Worlds</p>
    <p>Impose density effects on a network structure and calculate the outcome of each iteration analytically</p>
    <p>Swarm + Captures the Principle of Density</p>
    <p>Network + Models key facts about social networks</p>
    <p>Algebraic + No random process in the core algorithm 18</p>
  </div>
  <div class="page">
    <p>The Model</p>
    <p>Language change as a two-step loop 1. Propagation: Variants distribute through the network 2. Acquisition: Individuals internalize them</p>
  </div>
  <div class="page">
    <p>Vocabulary</p>
    <p>L: That which is transmitted Language  Variant  Sample</p>
    <p>G: That which generates/describes/distinguishes L That which is learned/influenced by L Grammar  Variety  Latent Variable</p>
  </div>
  <div class="page">
    <p>Binary G Examples</p>
    <p>G: {Merged grammar, Non-merged grammar} L: Merged or non-merged instances of cot and caught words</p>
    <p>G: {Dived-generating grammar, Dove-generating grammar} L: Instances of the past tense of dive as dived or dove</p>
    <p>G: {have+NEG = havent got grammar, have+NEG = dont have grammar} L: Instances of havent got and instances of dont have</p>
  </div>
  <div class="page">
    <p>The Model</p>
    <p>Language change as a two-step loop 1. Propagation: L distributes through the network 2. Acquisition: Individuals react to L to create G</p>
    <p>If this were a linear chain,</p>
    <p>L0  G1  L1  G2  L2    Ln  Gn+1  ...</p>
  </div>
  <div class="page">
    <p>The Model</p>
    <p>Language change as a two-step loop 1. Propagation: L distributes through the network 2. Acquisition: Individuals react to L to create G</p>
    <p>Generic. Not problem-specific.</p>
  </div>
  <div class="page">
    <p>Intuition behind Propagation Algorithm For T iterations,</p>
    <p>For the individual at each node Begin travelling; While travelling</p>
    <p>Randomly select outgoing edge by weight and follow it OR stop;</p>
    <p>Increase chance of stopping next time; End Interact with the individual at the current</p>
    <p>Node; End</p>
    <p>End 24</p>
  </div>
  <div class="page">
    <p>Intuition behind Propagation Algorithm For T iterations,</p>
    <p>For the individual at each node Begin travelling; While travelling</p>
    <p>Randomly select outgoing edge by weight and follow it OR stop;</p>
    <p>Increase chance of stopping next time; End Interact with the individual at the current</p>
    <p>node; End</p>
    <p>End 25</p>
    <p>Nodes are not individuals. Individuals stand on nodes</p>
  </div>
  <div class="page">
    <p>Intuition behind Propagation Algorithm For T iterations,</p>
    <p>For the individual at each node Begin travelling; While travelling</p>
    <p>Randomly select outgoing edge by weight and follow it OR stop;</p>
    <p>Increase chance of stopping next time; End Interact with the individual at the current</p>
    <p>node; End</p>
    <p>End 26</p>
    <p>Weighted or unweighted, Directed or undirected</p>
    <p>Individuals travel along edges and find someone to interact with</p>
  </div>
  <div class="page">
    <p>Intuition behind Propagation Algorithm For T iterations,</p>
    <p>For the individual at each node Begin travelling; While travelling</p>
    <p>Randomly select outgoing edge by weight and follow it OR stop;</p>
    <p>Increase chance of stopping next time; End Interact with the individual at the current</p>
    <p>node; End</p>
    <p>End 27</p>
    <p>Weighted or unweighted, Directed or undirected Determine who this node</p>
    <p>Individuals connected by shorter or higher weighted paths are more likely to interact.</p>
  </div>
  <div class="page">
    <p>Intuition behind Propagation Algorithm For T iterations,</p>
    <p>For the individual at each node Begin travelling; While travelling</p>
    <p>Randomly select outgoing edge by weight and follow it OR stop;</p>
    <p>Increase chance of stopping next time; End Interact with the individual at the current</p>
    <p>node; End</p>
    <p>End 28</p>
    <p>Weighted or unweighted, Directed or undirected</p>
    <p>Rather than simulating interactions in a loop, calculate a closed-form solution</p>
  </div>
  <div class="page">
    <p>The Propagation Function</p>
    <p>E = G T (I - (1 - ) A)-1</p>
  </div>
  <div class="page">
    <p>The Propagation Function</p>
    <p>E = G T (I - (1 - ) A)-1</p>
    <p>The Linguistic Environment  E is a g x n matrix: n individuals, g possible grammars  For each individual, the proportion of input drawn from each grammar</p>
  </div>
  <div class="page">
    <p>The Propagation Function</p>
    <p>E = G T (I - (1 - ) A)-1</p>
    <p>The Linguistic Environment Distribution of Grammars</p>
    <p>Of the previous generation  G is an n x g matrix  Proportions by which each individual produces L</p>
  </div>
  <div class="page">
    <p>The Propagation Function</p>
    <p>E = G T (I - (1 - ) A)-1</p>
    <p>The Linguistic Environment Distribution of Grammars Interaction Probabilities</p>
    <p>A is an n x n adjacency matrix  The probabilities that nodes i, j interact given that the number of</p>
    <p>steps travelled declines by a geometric distribution   parameter from that distribution [0,1] 32</p>
  </div>
  <div class="page">
    <p>The Acquisition Function</p>
    <p>Problem-specific  Should take Et as input and produce Gt+1 as output  In the simplest case (neutral change), Gt+1 = Et</p>
    <p>T</p>
    <p>The following case study uses a variational learner</p>
  </div>
  <div class="page">
    <p>Case Study Spread of the Cot-Caught Merger</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Learners will acquire the merged grammar iff more than ~17% of their environment is merged</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Learners will acquire the merged grammar iff more than ~17% of their environment is merged</p>
    <p>+ Accounts for mergers tendency to spread (Labov 1994) + 17% is close to the merged rate estimated in Johnson 2007</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Learners will acquire the merged grammar iff more than ~17% of their environment is merged</p>
    <p>+ Accounts for mergers tendency to spread (Labov 1994) + 17% is close to the merged rate estimated in Johnson 2007 - In a perfectly-mixed model, population will immediately fix at 100% g+ or g</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage Claim: Merged listeners have a lower rate of initial misinterpretation</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage Claim: Merged listeners have a lower rate of initial misinterpretation Claim: Only minimal pairs are relevant</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage Claim: Merged listeners have a lower rate of initial misinterpretation Claim: Only minimal pairs are relevant  If speaker A- and listener B- are both non-merged, B- misunderstands A- at the</p>
    <p>rate of mishearing one vowel for the other (A- said // but B- heard //)</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage Claim: Merged listeners have a lower rate of initial misinterpretation Claim: Only minimal pairs are relevant  If speaker A- and listener B- are both non-merged, B- misunderstands A- at the</p>
    <p>rate of mishearing one vowel for the other (A- said // but B- heard //)  If A+ speaks to B-, B- initially misunderstands whenever A+ says // when B</p>
    <p>expects // and visa-versa</p>
  </div>
  <div class="page">
    <p>Model for Merger Acquisition (Yang 2009)</p>
    <p>Claim: The merged grammar has a processing advantage Claim: Merged listeners have a lower rate of initial misinterpretation Claim: Only minimal pairs are relevant  If speaker A- and listener B- are both non-merged, B- misunderstands A- at the</p>
    <p>rate of mishearing one vowel for the other (A- said // but B- heard //)  If A+ speaks to B-, B- initially misunderstands whenever A+ says // when B</p>
    <p>expects // and visa-versa  If A- or A+ speaks to B+, B+ cannot hear A-s distinctions. Initial</p>
    <p>misunderstandings come down to lexical access - if the intended meaning is not the most frequent meaning (Carmazza et al 2001)</p>
  </div>
  <div class="page">
    <p>Variational Model for Merger Acquisition</p>
    <p>Probability of initial misunderstanding depends on  minimal pair frequencies  mix merged (+) and non-merged (-) speakers in the environment</p>
  </div>
  <div class="page">
    <p>Variational Model for Merger Acquisition</p>
    <p>Probability of initial misunderstanding depends on  minimal pair frequencies  mix merged (+) and non-merged (-) speakers in the environment</p>
    <p>Using minimal pair frequencies estimated from SUBTLEXus and a variational learner, learners will acquire the merged grammar iff more than ~17% of their environment is merged (Yang 2009)</p>
  </div>
  <div class="page">
    <p>Acquisition Function</p>
    <p>Two Grammars: Merged grammar g+ Non-merged grammar g</p>
    <p>Precomputed Acquisition Function An individual acquires 100% g+ if &gt;17% environment is generated by the g+, else acquire 100% g</p>
  </div>
  <div class="page">
    <p>Network Model  100 clusters of 75 individuals each  Each cluster is centralised randomly such that</p>
    <p>some community members are better connected than others</p>
    <p>MA (Merged)</p>
    <p>RI (Non-Merged)</p>
  </div>
  <div class="page">
    <p>Network Model  100 clusters of 75 individuals each  Each cluster is centralised randomly such that</p>
    <p>some community members are better connected than others</p>
    <p>One cluster begins 100% merged (Massachusetts)</p>
    <p>The rest start 100% non-merged (Rhode Island)</p>
    <p>MA (Merged)</p>
    <p>RI (Non-Merged)</p>
  </div>
  <div class="page">
    <p>Network Model  100 clusters of 75 individuals each  Each cluster is centralised randomly such that</p>
    <p>some community members are better connected than others</p>
    <p>One cluster begins 100% merged (Massachusetts)</p>
    <p>The rest start 100% non-merged (Rhode Island)</p>
    <p>Half the RI clusters are connected to the MA cluster (the Frontier)</p>
    <p>MA (Merged)</p>
    <p>RI (Non-Merged)</p>
  </div>
  <div class="page">
    <p>Network Model  100 clusters of 75 individuals each  Each cluster is centralised randomly such that</p>
    <p>some community members are better connected than others</p>
    <p>One cluster begins 100% merged (Massachusetts)</p>
    <p>The rest start 100% non-merged (Rhode Island)</p>
    <p>Half the RI clusters are connected to the MA cluster (the Frontier)</p>
    <p>Two members of each RI cluster are randomly connected to other clusters</p>
    <p>MA (Merged)</p>
    <p>RI (Non-Merged)</p>
  </div>
  <div class="page">
    <p>Merger Rate in Rhode Island over Time  The average merger rate across all</p>
    <p>Rhode Island clusters follows an S-shape</p>
    <p>The 99 RI community cluster curves are also S-shaped  Staggered in time  Steep slopes = rapid change</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The Propagation Function  Removes the need to simulate interactions  Is widely applicable rather than made-to-order</p>
    <p>The Cot-Caught Application  Predicts behaviour consistent with the empirical data  And with principles of language change</p>
  </div>
  <div class="page">
    <p>End</p>
    <p>Acknowledgements: Implementation:  Charles Yang github.com/jkodner05/NetworksAndLangChange  Mitch Marcus  NDSEG Fellowship (US ARO)</p>
  </div>
  <div class="page">
    <p>Variational Learner (Yang 2000)  Learners consider multiple</p>
    <p>grammars g1, g2 simultaneously</p>
    <p>P(g1) = p, P(g2) = q, p+q = 1</p>
  </div>
  <div class="page">
    <p>Variational Learner (Yang 2000)  Learners consider multiple</p>
    <p>grammars g1, g2 simultaneously  Each g is penalised when it</p>
    <p>cannot parse an input</p>
    <p>P(g1) = p, P(g2) = q, p+q = 1</p>
    <p>p = p + q, if g1 parses input (1-)p, if g1 fails</p>
  </div>
  <div class="page">
    <p>Variational Learner (Yang 2000)  Learners consider multiple</p>
    <p>grammars g1, g2 simultaneously  Each g is penalised when it</p>
    <p>cannot parse an input  The g with lower penalty</p>
    <p>probability has the advantage</p>
    <p>P(g1) = p, P(g2) = q, p+q = 1</p>
    <p>p =</p>
    <p>limt pt = C2 / (C1 + C2)  limt qt = C1 / (C2 + C1)</p>
    <p>p + q, if g1 parses input (1-)p, if g1 fails</p>
  </div>
  <div class="page">
    <p>Variational Learner (Yang 2000)  Learners consider multiple</p>
    <p>grammars g1, g2 simultaneously  Each g is penalised when it</p>
    <p>cannot parse an input  The g with lower penalty</p>
    <p>probability has the advantage  If mature speakers adopt one</p>
    <p>grammar categorically, the one with smaller C wins</p>
    <p>P(g1) = p, P(g2) = q, p+q = 1</p>
    <p>p =</p>
    <p>limt pt = C2 / (C1 + C2)  limt qt = C1 / (C2 + C1)</p>
    <p>limt pt =</p>
    <p>p + q, if g1 parses input (1-)p, if g1 fails</p>
  </div>
  <div class="page">
    <p>Variational Model for Merger Acquisition</p>
    <p>Penalty probabilities depend on  minimal pair frequencies  mix merged (+) and non-merged (-) speakers in the environment</p>
  </div>
  <div class="page">
    <p>Variational Model for Merger Acquisition</p>
    <p>Penalty probabilities depend on  minimal pair frequencies  mix merged (+) and non-merged (-) speakers in the environment</p>
    <p>mi, ni = frequencies of each member of a minimal pair H = i mi + ni  = probability of mishearing one vowel for the other</p>
    <p>C+ = (1/H) i min(mi, ni) hearing the less freq word C- = (1/H) i [p+((1-m)mi + nni) mishearing + input</p>
    <p>+ p-(mmi + nni)] misinterpreting - input 59</p>
  </div>
  <div class="page">
    <p>Results - Updating Connections  Social connections change constantly  Rewire the edges (recalculate A) at every</p>
    <p>iteration</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Results - Updating Connections  Social connections change constantly  Rewire the edges (recalculate A) at every</p>
    <p>iteration</p>
    <p>The outcome is similar, but clusters tipping points are temporally closer</p>
    <p>No cluster remains particularly well or poorly connected for long</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Fractional Updating  The merger spreads rapidly enough to</p>
    <p>distinguish older and younger siblings  Only a fraction of the population is of the</p>
    <p>correct age at any moment  Update only 10% of random nodes at every</p>
    <p>iteration</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Fractional Updating  The merger spreads rapidly enough to</p>
    <p>distinguish older and younger siblings  Only a fraction of the population is of the</p>
    <p>correct age at any moment  Update only 10% of random nodes at every</p>
    <p>iteration</p>
    <p>Similar outcome with wider spread between cluster tipping points</p>
    <p>Simulation took about 5x as long because</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Results - Network Size  Tested our network size assumptions  Repeat the experiment with 40 clusters of 18</p>
    <p>individuals each</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Results - Network Size  Tested our network size assumptions  Repeat the experiment with 40 clusters of 18</p>
    <p>individuals each</p>
    <p>Qualitatively similar  The S-shape is less S-shaped  Individual clusters shows step pattern</p>
    <p>Cluster Merger Rates Rhode Island Avg</p>
  </div>
  <div class="page">
    <p>Results - Community Averages  At small network sizes, the community average</p>
    <p>is more sensitive to random connections  Repeat the small-scale experiment 10 times</p>
    <p>Trial Avgs</p>
  </div>
  <div class="page">
    <p>Results - Community Averages  At small network sizes, the community average</p>
    <p>is more sensitive to random connections  Repeat the small-scale experiment 10 times</p>
    <p>The slope is ~consistent in most simulations  A few simulations show aberrant behaviour</p>
    <p>Trial Avgs</p>
  </div>
</Presentation>
