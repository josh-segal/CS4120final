<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Principal Component Analysis (PCA) for Sparse High-Dimensional Data</p>
    <p>Tapani Raiko, Alexander Ilin, and Juha Karhunen</p>
    <p>Helsinki University of Technology, Finland Adaptive Informatics Research Center</p>
    <p>AB</p>
  </div>
  <div class="page">
    <p>Principal Component Analysis  Data X consists of n d-dimensional vectors</p>
    <p>Matrix X is decomposed in to a product of smaller matrices such that the square reconstruction error is minimized</p>
    <p>inadequate in this case, and we thus propose a new algorithm. The problem of overfitting is also studied and solutions based on regularization and variational Bayesian learning are given.</p>
    <p>Principal subspace and components Assume that we have n d-dimensional data vectors x1, x2, . . . , xn, which form the d!n data matrix X = [x1, x2, . . . , xn]. The matrix X is decomposed into</p>
    <p>X &quot; AS, (1)</p>
    <p>where A is a d ! c matrix, S is a c ! n matrix and c # d # n. Principal subspace methods [6, 4] find A and S such that the reconstruction error</p>
    <p>C = $X % AS$2F = d</p>
    <p>!</p>
    <p>i=1</p>
    <p>n !</p>
    <p>j=1</p>
    <p>(xij % c</p>
    <p>!</p>
    <p>k=1</p>
    <p>aikskj ) 2 , (2)</p>
    <p>is minimized. There F denotes the Frobenius norm, and xij , aik, and skj elements of the matrices X, A, and S, respectively. Typically the row-wise mean is removed from X as a preprocessing step.</p>
    <p>Without any further constraints, there exist infinitely many ways to perform such a decomposition. However, the subspace spanned by the column vectors of the matrix A, called the principal subspace, is unique. In PCA, these vectors are mutually orthogonal and have unit length. Further, for each k = 1, . . . , c, the first k vectors form the k dimensional principal subspace. This makes the solution practically unique, see [4, 2, 5] for details.</p>
    <p>There are many ways to determine the principal subspace and components [6, 4, 2]. We will discuss three common methods that can be adapted for the case of missing values.</p>
    <p>Singular Value Decomposition PCA can be determined by using the singular value decomposition (SVD) [5]</p>
    <p>X = U!VT , (3)</p>
    <p>where U is a d ! d orthogonal matrix, V is an n ! n orthogonal matrix and ! is a d ! n pseudodiagonal matrix (diagonal if d = n) with the singular values on the main diagonal [5]. The PCA solution is obtained by selecting the c largest singular values from !, by forming A from the corresponding c columns of U, and S from the corresponding c rows of !VT .</p>
    <p>Note that PCA can equivalently be defined using the eigendecomposition of the d ! d covariance matrix C of the column vectors of the data matrix X:</p>
    <p>C = 1</p>
    <p>n XXT = UDUT , (4)</p>
    <p>inadequate in this case, and we thus propose a new algorithm. The problem of overfitting is also studied and solutions based on regularization and variational Bayesian learning are given.</p>
    <p>Principal subspace and components Assume that we have n d-dimensional data vectors x1, x2, . . . , xn, which form the d!n data matrix X = [x1, x2, . . . , xn]. The matrix X is decomposed into</p>
    <p>X &quot; AS, (1)</p>
    <p>where A is a d ! c matrix, S is a c ! n matrix and c # d # n. Principal subspace methods [6, 4] find A and S such that the reconstruction error</p>
    <p>C = $X % AS$2F = d</p>
    <p>!</p>
    <p>i=1</p>
    <p>n !</p>
    <p>j=1</p>
    <p>(xij % c</p>
    <p>!</p>
    <p>k=1</p>
    <p>aikskj ) 2 , (2)</p>
    <p>is minimized. There F denotes the Frobenius norm, and xij , aik, and skj elements of the matrices X, A, and S, respectively. Typically the row-wise mean is removed from X as a preprocessing step.</p>
    <p>Without any further constraints, there exist infinitely many ways to perform such a decomposition. However, the subspace spanned by the column vectors of the matrix A, called the principal subspace, is unique. In PCA, these vectors are mutually orthogonal and have unit length. Further, for each k = 1, . . . , c, the first k vectors form the k dimensional principal subspace. This makes the solution practically unique, see [4, 2, 5] for details.</p>
    <p>There are many ways to determine the principal subspace and components [6, 4, 2]. We will discuss three common methods that can be adapted for the case of missing values.</p>
    <p>Singular Value Decomposition PCA can be determined by using the singular value decomposition (SVD) [5]</p>
    <p>X = U!VT , (3)</p>
    <p>where U is a d ! d orthogonal matrix, V is an n ! n orthogonal matrix and ! is a d ! n pseudodiagonal matrix (diagonal if d = n) with the singular values on the main diagonal [5]. The PCA solution is obtained by selecting the c largest singular values from !, by forming A from the corresponding c columns of U, and S from the corresponding c rows of !VT .</p>
    <p>Note that PCA can equivalently be defined using the eigendecomposition of the d ! d covariance matrix C of the column vectors of the data matrix X:</p>
    <p>C = 1</p>
    <p>n XXT = UDUT , (4)</p>
  </div>
  <div class="page">
    <p>Algorithms for PCA  Eigenvalue decomposition (standard approach)</p>
    <p>Compute the covariance matrix and its eigenvectors</p>
  </div>
  <div class="page">
    <p>Algorithms for PCA  Eigenvalue decomposition (standard approach)</p>
    <p>Compute the covariance matrix and its eigenvectors</p>
    <p>EM algorithm</p>
    <p>Iterates between:</p>
    <p>PCA can be seen as a special case of the probabilistic PCA model [7] in the zero-noise limit. The model can be identified using the EM algorithm which in the zero-noise case iterates updating A and S alternately [8]. When either of these matrices is fixed, the other one can be obtained from an ordinary least-squares problem:</p>
    <p>A ! XST(SST)!1 , S ! (ATA)!1ATX . (7)</p>
    <p>This approach is especially e!cient when only a few principal components are needed, that is c &quot; d [8].</p>
    <p>Juha: Please give more details here in the longer report version.</p>
    <p>Grung and Manne [9] studied the same algorithm in the case of missing values. The learning algorithm alternates between updating A and S using the update rules shown in Table 1. There,</p>
    <p>! X is the matrix X</p>
    <p>in which all the missing values are replaces with zeros, ! X:j is the j-th column and</p>
    <p>! X</p>
    <p>T</p>
    <p>i: is the i-th row of ! X. Aj is matrix A in which each i-th row is replaced with zeros if xij is not observed, and Si is matrix S in which each j-th column is replaced with zeros if xij is not observed. Note the similarity of the update rules to (7). However, now there are more computations involved because each column of S and each row of A has to be computed separately. This becomes too heavy in high dimensions.</p>
    <p>Table 1: Least-squares algorithm for PCA with missing values</p>
    <p>S A</p>
    <p>s:j = (ATj Aj ) !1ATj</p>
    <p>! X:j ATi: =</p>
    <p>! XTi: S</p>
    <p>T i (SiS</p>
    <p>T i )</p>
    <p>!1</p>
    <p>j = 1, . . . , n i = 1, . . . , d</p>
    <p>Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity in the general case is O(N c2 +nc3), where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. The computational complexity can be smaller in the cases when, for example, several rows of the data matrix X have missing values exactly at the same columns. In that case, the matrix (SiSTi )</p>
    <p>!1 is same for all such rows and can be computed once. Similarly, one matrix of form (ATj Aj )</p>
    <p>!1 can be computed once for all columns of X which contain missing values in the same rows.</p>
    <p>The same reconstruction error (2) can also be minimized using the gradient descent procedure yielding the update rules</p>
    <p>A ! A + !(X # AS)ST , S ! S + !AT(X # AS) . (8)</p>
    <p>The above learning rules are batch versions of Ojas subspace rule [10, 11]. In the classical Ojas algorithm, A is updated using (8) while S can be computed using the least squares solution in (7) or simply as S = ATX. The presented approach is not very practical for processing fully observed data in the batch mode. However, we show experimentally that a similar approach can be useful for PCA learning in high-dimensional problems with lots of missing data. Alex: Please check.</p>
    <p>If needed, the end result can be transformed into the PCA solution, for instance, by computing the eigenvalue decomposition SST = USDSUTS and the singular value decomposition AUSD</p>
    <p>T A.</p>
    <p>The transformed A is formed from the first c columns of UA!A and the transformed S from the first c rows of VTAD</p>
    <p>!1/2 S U</p>
    <p>T S S. Note that the required decompositions are computationally lighter than the</p>
    <p>ones done to the data matrix directly.</p>
  </div>
  <div class="page">
    <p>Algorithms for PCA  Eigenvalue decomposition (standard approach)</p>
    <p>Compute the covariance matrix and its eigenvectors</p>
    <p>EM algorithm</p>
    <p>Iterates between:</p>
    <p>Minimization of cost C (Ojas subspace rule)</p>
    <p>PCA can be seen as a special case of the probabilistic PCA model [7] in the zero-noise limit. The model can be identified using the EM algorithm which in the zero-noise case iterates updating A and S alternately [8]. When either of these matrices is fixed, the other one can be obtained from an ordinary least-squares problem:</p>
    <p>A ! XST(SST)!1 , S ! (ATA)!1ATX . (7)</p>
    <p>This approach is especially e!cient when only a few principal components are needed, that is c &quot; d [8].</p>
    <p>Juha: Please give more details here in the longer report version.</p>
    <p>Grung and Manne [9] studied the same algorithm in the case of missing values. The learning algorithm alternates between updating A and S using the update rules shown in Table 1. There,</p>
    <p>! X is the matrix X</p>
    <p>in which all the missing values are replaces with zeros, ! X:j is the j-th column and</p>
    <p>! X</p>
    <p>T</p>
    <p>i: is the i-th row of ! X. Aj is matrix A in which each i-th row is replaced with zeros if xij is not observed, and Si is matrix S in which each j-th column is replaced with zeros if xij is not observed. Note the similarity of the update rules to (7). However, now there are more computations involved because each column of S and each row of A has to be computed separately. This becomes too heavy in high dimensions.</p>
    <p>Table 1: Least-squares algorithm for PCA with missing values</p>
    <p>S A</p>
    <p>s:j = (ATj Aj ) !1ATj</p>
    <p>! X:j ATi: =</p>
    <p>! XTi: S</p>
    <p>T i (SiS</p>
    <p>T i )</p>
    <p>!1</p>
    <p>j = 1, . . . , n i = 1, . . . , d</p>
    <p>Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity in the general case is O(N c2 +nc3), where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. The computational complexity can be smaller in the cases when, for example, several rows of the data matrix X have missing values exactly at the same columns. In that case, the matrix (SiSTi )</p>
    <p>!1 is same for all such rows and can be computed once. Similarly, one matrix of form (ATj Aj )</p>
    <p>!1 can be computed once for all columns of X which contain missing values in the same rows.</p>
    <p>The same reconstruction error (2) can also be minimized using the gradient descent procedure yielding the update rules</p>
    <p>A ! A + !(X # AS)ST , S ! S + !AT(X # AS) . (8)</p>
    <p>The above learning rules are batch versions of Ojas subspace rule [10, 11]. In the classical Ojas algorithm, A is updated using (8) while S can be computed using the least squares solution in (7) or simply as S = ATX. The presented approach is not very practical for processing fully observed data in the batch mode. However, we show experimentally that a similar approach can be useful for PCA learning in high-dimensional problems with lots of missing data. Alex: Please check.</p>
    <p>If needed, the end result can be transformed into the PCA solution, for instance, by computing the eigenvalue decomposition SST = USDSUTS and the singular value decomposition AUSD</p>
    <p>T A.</p>
    <p>The transformed A is formed from the first c columns of UA!A and the transformed S from the first c rows of VTAD</p>
    <p>!1/2 S U</p>
    <p>T S S. Note that the required decompositions are computationally lighter than the</p>
    <p>ones done to the data matrix directly.</p>
    <p>PCA can be seen as a special case of the probabilistic PCA model [7] in the zero-noise limit. The model can be identified using the EM algorithm which in the zero-noise case iterates updating A and S alternately [8]. When either of these matrices is fixed, the other one can be obtained from an ordinary least-squares problem:</p>
    <p>A ! XST(SST)!1 , S ! (ATA)!1ATX . (7)</p>
    <p>This approach is especially e!cient when only a few principal components are needed, that is c &quot; d [8].</p>
    <p>Juha: Please give more details here in the longer report version.</p>
    <p>Grung and Manne [9] studied the same algorithm in the case of missing values. The learning algorithm alternates between updating A and S using the update rules shown in Table 1. There,</p>
    <p>! X is the matrix X</p>
    <p>in which all the missing values are replaces with zeros, ! X:j is the j-th column and</p>
    <p>! X</p>
    <p>T</p>
    <p>i: is the i-th row of ! X. Aj is matrix A in which each i-th row is replaced with zeros if xij is not observed, and Si is matrix S in which each j-th column is replaced with zeros if xij is not observed. Note the similarity of the update rules to (7). However, now there are more computations involved because each column of S and each row of A has to be computed separately. This becomes too heavy in high dimensions.</p>
    <p>Table 1: Least-squares algorithm for PCA with missing values</p>
    <p>S A</p>
    <p>s:j = (ATj Aj ) !1ATj</p>
    <p>! X:j ATi: =</p>
    <p>! XTi: S</p>
    <p>T i (SiS</p>
    <p>T i )</p>
    <p>!1</p>
    <p>j = 1, . . . , n i = 1, . . . , d</p>
    <p>Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity in the general case is O(N c2 +nc3), where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. The computational complexity can be smaller in the cases when, for example, several rows of the data matrix X have missing values exactly at the same columns. In that case, the matrix (SiSTi )</p>
    <p>!1 is same for all such rows and can be computed once. Similarly, one matrix of form (ATj Aj )</p>
    <p>!1 can be computed once for all columns of X which contain missing values in the same rows.</p>
    <p>The same reconstruction error (2) can also be minimized using the gradient descent procedure yielding the update rules</p>
    <p>A ! A + !(X # AS)ST , S ! S + !AT(X # AS) . (8)</p>
    <p>The above learning rules are batch versions of Ojas subspace rule [10, 11]. In the classical Ojas algorithm, A is updated using (8) while S can be computed using the least squares solution in (7) or simply as S = ATX. The presented approach is not very practical for processing fully observed data in the batch mode. However, we show experimentally that a similar approach can be useful for PCA learning in high-dimensional problems with lots of missing data. Alex: Please check.</p>
    <p>If needed, the end result can be transformed into the PCA solution, for instance, by computing the eigenvalue decomposition SST = USDSUTS and the singular value decomposition AUSD</p>
    <p>T A.</p>
    <p>The transformed A is formed from the first c columns of UA!A and the transformed S from the first c rows of VTAD</p>
    <p>!1/2 S U</p>
    <p>T S S. Note that the required decompositions are computationally lighter than the</p>
    <p>ones done to the data matrix directly.</p>
  </div>
  <div class="page">
    <p>PCA with Missing Values</p>
    <p>Red and blue data points are reconstructed based on only one of the two dimensions</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
  </div>
  <div class="page">
    <p>Adapting the Algorithms for Missing Values</p>
    <p>Iterative imputation</p>
    <p>Alternately 1) fill in missing values and 2) solve normal PCA with the standard approach</p>
    <p>EM algorithm becomes computationally heavier</p>
    <p>Minimization of cost C</p>
    <p>Easy to adapt: Take error over observed values only</p>
    <p>PCA can be seen as a special case of the probabilistic PCA model [7] in the zero-noise limit. The model can be identified using the EM algorithm which in the zero-noise case iterates updating A and S alternately [8]. When either of these matrices is fixed, the other one can be obtained from an ordinary least-squares problem:</p>
    <p>A ! XST(SST)!1 , S ! (ATA)!1ATX . (7)</p>
    <p>This approach is especially e!cient when only a few principal components are needed, that is c &quot; d [8].</p>
    <p>Juha: Please give more details here in the longer report version.</p>
    <p>Grung and Manne [9] studied the same algorithm in the case of missing values. The learning algorithm alternates between updating A and S using the update rules shown in Table 1. There,</p>
    <p>! X is the matrix X</p>
    <p>in which all the missing values are replaces with zeros, ! X:j is the j-th column and</p>
    <p>! X</p>
    <p>T</p>
    <p>i: is the i-th row of ! X. Aj is matrix A in which each i-th row is replaced with zeros if xij is not observed, and Si is matrix S in which each j-th column is replaced with zeros if xij is not observed. Note the similarity of the update rules to (7). However, now there are more computations involved because each column of S and each row of A has to be computed separately. This becomes too heavy in high dimensions.</p>
    <p>Table 1: Least-squares algorithm for PCA with missing values</p>
    <p>S A</p>
    <p>s:j = (ATj Aj ) !1ATj</p>
    <p>! X:j ATi: =</p>
    <p>! XTi: S</p>
    <p>T i (SiS</p>
    <p>T i )</p>
    <p>!1</p>
    <p>j = 1, . . . , n i = 1, . . . , d</p>
    <p>Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity in the general case is O(N c2 +nc3), where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. The computational complexity can be smaller in the cases when, for example, several rows of the data matrix X have missing values exactly at the same columns. In that case, the matrix (SiSTi )</p>
    <p>!1 is same for all such rows and can be computed once. Similarly, one matrix of form (ATj Aj )</p>
    <p>!1 can be computed once for all columns of X which contain missing values in the same rows.</p>
    <p>The same reconstruction error (2) can also be minimized using the gradient descent procedure yielding the update rules</p>
    <p>A ! A + !(X # AS)ST , S ! S + !AT(X # AS) . (8)</p>
    <p>The above learning rules are batch versions of Ojas subspace rule [10, 11]. In the classical Ojas algorithm, A is updated using (8) while S can be computed using the least squares solution in (7) or simply as S = ATX. The presented approach is not very practical for processing fully observed data in the batch mode. However, we show experimentally that a similar approach can be useful for PCA learning in high-dimensional problems with lots of missing data. Alex: Please check.</p>
    <p>If needed, the end result can be transformed into the PCA solution, for instance, by computing the eigenvalue decomposition SST = USDSUTS and the singular value decomposition AUSD</p>
    <p>T A.</p>
    <p>The transformed A is formed from the first c columns of UA!A and the transformed S from the first c rows of VTAD</p>
    <p>!1/2 S U</p>
    <p>T S S. Note that the required decompositions are computationally lighter than the</p>
    <p>ones done to the data matrix directly.</p>
  </div>
  <div class="page">
    <p>Speeding up Gradient Descent</p>
    <p>Newtons method is known to converge fast, but</p>
    <p>It requires computing the Hessian matrix which is computationally too demanding in highdimensional problems</p>
    <p>We propose using only the diagonal part of the Hessian</p>
    <p>We also include a control parameter to interpolate between standard gradient descent (0) and the diagonal Newtons method (1)</p>
  </div>
  <div class="page">
    <p>Adapting SVD: Imputation Algorithm One can use the SVD approach (4) in order to find an approximate solution to the PCA problem. However, estimating the covariance matrix C becomes very di!cult when there are lots of missing values. If we estimate C leaving out terms with missing values from the average, we get for the estimate of the covariance matrix</p>
    <p>C = 1</p>
    <p>n XXT =</p>
    <p>!</p>
    <p>&quot; 0.5 1 0 1 0.667 ? 0 ? 1</p>
    <p>#</p>
    <p>$ . (9)</p>
    <p>There are at least two problems. First, the estimated covariance 1 between the first and second components is larger than their estimated variances 0.5 and 0.667. This is clearly wrong, and leads to the situation where the covariance matrix is not positive (semi)definite even though it theoretically should be, with some of its eigenvalues being negative. Secondly, the covariance between the second and the third component could not be estimated at all1.</p>
    <p>Another option is to complete the data matrix by iteratively imputing the missing values (see, e.g., [10]). Initially, the missing values can be replaced by zeroes. The covariance matrix of the complete data can be estimated without the problems mentioned above. Now, the product AS can be used as a better estimate for the missing values, and this process can be iterated until convergence. This approach requires the use of the complete data matrix, and therefore it is computationally very expensive if a large part of the data matrix is missing. The time complexity of computing the sample covariance matrix explicitly is O(nd2). We will further refer to this approach as the imputation algorithm.</p>
    <p>Adapting the EM Algorithm Grung and Manne [11] studied the EM algorithm in the case of missing values. Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity is O(N c2 + nc3) per iteration, where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. This is quite a bit heavier than EM with complete data, whose complexity is O(ndc) [7] per iteration.</p>
    <p>Adapting the Subspace Learning Algorithm The subspace learning algorithm works in a straightforward manner also in the presence of missing values. We just take the sum over only those indices i and j for which the data entry xij (the ijth element of X) is observed, in short (i, j) ! O. The cost function is</p>
    <p>C = %</p>
    <p>(i,j)!O</p>
    <p>e2ij , with eij = xij &quot; c%</p>
    <p>k=1</p>
    <p>aikskj . (10)</p>
    <p>The cost function:</p>
  </div>
  <div class="page">
    <p>Principal Component Analysis for Sparse High-Dimensional Data 5</p>
    <p>and its partial derivatives are</p>
    <p>!C</p>
    <p>!ail = !2</p>
    <p>!</p>
    <p>j|(i,j)!O</p>
    <p>eij slj , !C</p>
    <p>!slj = !2</p>
    <p>!</p>
    <p>i|(i,j)!O</p>
    <p>eij ail . (11)</p>
    <p>We propose a novel speed-up to the original simple gradient descent algorithm. In Newtons method for optimization, the gradient is multiplied by the inverse of the Hessian matrix. Newtons method is known to converge fast especially in the vicinity of the optimum, but using the full Hessian is computationally too demanding in truly high-dimensional problems. Here we use only the diagonal part of the Hessian matrix. We also include a control parameter &quot; that allows the learning algorithm to interpolate between the standard gradient descent (&quot; = 0) and the diagonal Newtons method (&quot; = 1), much like the well known Levenberg-Marquardt algorithm. The learning rules then take the form</p>
    <p>ail &quot; ail ! # &quot;</p>
    <p>&quot; !2C</p>
    <p>!a2il</p>
    <p>##! !C</p>
    <p>!ail = ail + #</p>
    <p>$ j|(i,j)!O eij slj%$ j|(i,j)!O s</p>
    <p>&amp;! , (12)</p>
    <p>slj &quot; slj ! # &quot;</p>
    <p>' !2C</p>
    <p>!s2lj</p>
    <p>(#! !C</p>
    <p>!slj = slj + #</p>
    <p>$ i|(i,j)!O eij ail%$ i|(i,j)!O a</p>
    <p>&amp;! . (13)</p>
    <p>The computational complexity is O(N c + nc) per iteration.</p>
    <p>A trained PCA model can be used for reconstructing missing values:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /# O . (14)</p>
    <p>Although PCA performs a linear transformation of data, overfitting is a serious problem for large-scale problems with lots of missing values. This happens when the value of the cost function C in Eq. (10) is small for training data, but the quality of prediction (14) is poor for new data. This e!ect is illustrated using a toy problem in the longer version of this paper [12].</p>
    <p>Another way to examine overfitting is to compare the number of model parameters to the number of observed values in data. A coarse rule of thumb in estimation is that the latter should be at least tenfold to avoid overfitting. Consider the subproblem of finding the jth column vector of S given jth column vector of X while regarding A a constant. Here, c parameters are determined by the observed values of the jth column vector of X. If the column has fewer than 10c observations, it is likely to su!er from at least some overfitting, and if it has fewer than c observations, the subproblem becomes underdetermined.</p>
    <p>Adapting SVD: Imputation Algorithm One can use the SVD approach (4) in order to find an approximate solution to the PCA problem. However, estimating the covariance matrix C becomes very di!cult when there are lots of missing values. If we estimate C leaving out terms with missing values from the average, we get for the estimate of the covariance matrix</p>
    <p>C = 1</p>
    <p>n XXT =</p>
    <p>!</p>
    <p>&quot; 0.5 1 0 1 0.667 ? 0 ? 1</p>
    <p>#</p>
    <p>$ . (9)</p>
    <p>There are at least two problems. First, the estimated covariance 1 between the first and second components is larger than their estimated variances 0.5 and 0.667. This is clearly wrong, and leads to the situation where the covariance matrix is not positive (semi)definite even though it theoretically should be, with some of its eigenvalues being negative. Secondly, the covariance between the second and the third component could not be estimated at all1.</p>
    <p>Another option is to complete the data matrix by iteratively imputing the missing values (see, e.g., [10]). Initially, the missing values can be replaced by zeroes. The covariance matrix of the complete data can be estimated without the problems mentioned above. Now, the product AS can be used as a better estimate for the missing values, and this process can be iterated until convergence. This approach requires the use of the complete data matrix, and therefore it is computationally very expensive if a large part of the data matrix is missing. The time complexity of computing the sample covariance matrix explicitly is O(nd2). We will further refer to this approach as the imputation algorithm.</p>
    <p>Adapting the EM Algorithm Grung and Manne [11] studied the EM algorithm in the case of missing values. Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity is O(N c2 + nc3) per iteration, where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. This is quite a bit heavier than EM with complete data, whose complexity is O(ndc) [7] per iteration.</p>
    <p>Adapting the Subspace Learning Algorithm The subspace learning algorithm works in a straightforward manner also in the presence of missing values. We just take the sum over only those indices i and j for which the data entry xij (the ijth element of X) is observed, in short (i, j) ! O. The cost function is</p>
    <p>C = %</p>
    <p>(i,j)!O</p>
    <p>e2ij , with eij = xij &quot; c%</p>
    <p>k=1</p>
    <p>aikskj . (10)</p>
    <p>The cost function:</p>
    <p>Its partial derivatives:</p>
  </div>
  <div class="page">
    <p>Principal Component Analysis for Sparse High-Dimensional Data 5</p>
    <p>and its partial derivatives are</p>
    <p>!C</p>
    <p>!ail = !2</p>
    <p>!</p>
    <p>j|(i,j)!O</p>
    <p>eij slj , !C</p>
    <p>!slj = !2</p>
    <p>!</p>
    <p>i|(i,j)!O</p>
    <p>eij ail . (11)</p>
    <p>We propose a novel speed-up to the original simple gradient descent algorithm. In Newtons method for optimization, the gradient is multiplied by the inverse of the Hessian matrix. Newtons method is known to converge fast especially in the vicinity of the optimum, but using the full Hessian is computationally too demanding in truly high-dimensional problems. Here we use only the diagonal part of the Hessian matrix. We also include a control parameter &quot; that allows the learning algorithm to interpolate between the standard gradient descent (&quot; = 0) and the diagonal Newtons method (&quot; = 1), much like the well known Levenberg-Marquardt algorithm. The learning rules then take the form</p>
    <p>ail &quot; ail ! # &quot;</p>
    <p>&quot; !2C</p>
    <p>!a2il</p>
    <p>##! !C</p>
    <p>!ail = ail + #</p>
    <p>$ j|(i,j)!O eij slj%$ j|(i,j)!O s</p>
    <p>&amp;! , (12)</p>
    <p>slj &quot; slj ! # &quot;</p>
    <p>' !2C</p>
    <p>!s2lj</p>
    <p>(#! !C</p>
    <p>!slj = slj + #</p>
    <p>$ i|(i,j)!O eij ail%$ i|(i,j)!O a</p>
    <p>&amp;! . (13)</p>
    <p>The computational complexity is O(N c + nc) per iteration.</p>
    <p>A trained PCA model can be used for reconstructing missing values:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /# O . (14)</p>
    <p>Although PCA performs a linear transformation of data, overfitting is a serious problem for large-scale problems with lots of missing values. This happens when the value of the cost function C in Eq. (10) is small for training data, but the quality of prediction (14) is poor for new data. This e!ect is illustrated using a toy problem in the longer version of this paper [12].</p>
    <p>Another way to examine overfitting is to compare the number of model parameters to the number of observed values in data. A coarse rule of thumb in estimation is that the latter should be at least tenfold to avoid overfitting. Consider the subproblem of finding the jth column vector of S given jth column vector of X while regarding A a constant. Here, c parameters are determined by the observed values of the jth column vector of X. If the column has fewer than 10c observations, it is likely to su!er from at least some overfitting, and if it has fewer than c observations, the subproblem becomes underdetermined.</p>
    <p>Principal Component Analysis for Sparse High-Dimensional Data 5</p>
    <p>and its partial derivatives are</p>
    <p>!C</p>
    <p>!ail = !2</p>
    <p>!</p>
    <p>j|(i,j)!O</p>
    <p>eij slj , !C</p>
    <p>!slj = !2</p>
    <p>!</p>
    <p>i|(i,j)!O</p>
    <p>eij ail . (11)</p>
    <p>We propose a novel speed-up to the original simple gradient descent algorithm. In Newtons method for optimization, the gradient is multiplied by the inverse of the Hessian matrix. Newtons method is known to converge fast especially in the vicinity of the optimum, but using the full Hessian is computationally too demanding in truly high-dimensional problems. Here we use only the diagonal part of the Hessian matrix. We also include a control parameter &quot; that allows the learning algorithm to interpolate between the standard gradient descent (&quot; = 0) and the diagonal Newtons method (&quot; = 1), much like the well known Levenberg-Marquardt algorithm. The learning rules then take the form</p>
    <p>ail &quot; ail ! # &quot;</p>
    <p>&quot; !2C</p>
    <p>!a2il</p>
    <p>##! !C</p>
    <p>!ail = ail + #</p>
    <p>$ j|(i,j)!O eij slj%$ j|(i,j)!O s</p>
    <p>&amp;! , (12)</p>
    <p>slj &quot; slj ! # &quot;</p>
    <p>' !2C</p>
    <p>!s2lj</p>
    <p>(#! !C</p>
    <p>!slj = slj + #</p>
    <p>$ i|(i,j)!O eij ail%$ i|(i,j)!O a</p>
    <p>&amp;! . (13)</p>
    <p>The computational complexity is O(N c + nc) per iteration.</p>
    <p>A trained PCA model can be used for reconstructing missing values:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /# O . (14)</p>
    <p>Although PCA performs a linear transformation of data, overfitting is a serious problem for large-scale problems with lots of missing values. This happens when the value of the cost function C in Eq. (10) is small for training data, but the quality of prediction (14) is poor for new data. This e!ect is illustrated using a toy problem in the longer version of this paper [12].</p>
    <p>Another way to examine overfitting is to compare the number of model parameters to the number of observed values in data. A coarse rule of thumb in estimation is that the latter should be at least tenfold to avoid overfitting. Consider the subproblem of finding the jth column vector of S given jth column vector of X while regarding A a constant. Here, c parameters are determined by the observed values of the jth column vector of X. If the column has fewer than 10c observations, it is likely to su!er from at least some overfitting, and if it has fewer than c observations, the subproblem becomes underdetermined.</p>
    <p>Adapting SVD: Imputation Algorithm One can use the SVD approach (4) in order to find an approximate solution to the PCA problem. However, estimating the covariance matrix C becomes very di!cult when there are lots of missing values. If we estimate C leaving out terms with missing values from the average, we get for the estimate of the covariance matrix</p>
    <p>C = 1</p>
    <p>n XXT =</p>
    <p>!</p>
    <p>&quot; 0.5 1 0 1 0.667 ? 0 ? 1</p>
    <p>#</p>
    <p>$ . (9)</p>
    <p>There are at least two problems. First, the estimated covariance 1 between the first and second components is larger than their estimated variances 0.5 and 0.667. This is clearly wrong, and leads to the situation where the covariance matrix is not positive (semi)definite even though it theoretically should be, with some of its eigenvalues being negative. Secondly, the covariance between the second and the third component could not be estimated at all1.</p>
    <p>Another option is to complete the data matrix by iteratively imputing the missing values (see, e.g., [10]). Initially, the missing values can be replaced by zeroes. The covariance matrix of the complete data can be estimated without the problems mentioned above. Now, the product AS can be used as a better estimate for the missing values, and this process can be iterated until convergence. This approach requires the use of the complete data matrix, and therefore it is computationally very expensive if a large part of the data matrix is missing. The time complexity of computing the sample covariance matrix explicitly is O(nd2). We will further refer to this approach as the imputation algorithm.</p>
    <p>Adapting the EM Algorithm Grung and Manne [11] studied the EM algorithm in the case of missing values. Experiments showed a faster convergence compared to the iterative imputation algorithm. The computational complexity is O(N c2 + nc3) per iteration, where N is the number of observed values, assuming nave matrix multiplications and inversions but exploiting sparsity. This is quite a bit heavier than EM with complete data, whose complexity is O(ndc) [7] per iteration.</p>
    <p>Adapting the Subspace Learning Algorithm The subspace learning algorithm works in a straightforward manner also in the presence of missing values. We just take the sum over only those indices i and j for which the data entry xij (the ijth element of X) is observed, in short (i, j) ! O. The cost function is</p>
    <p>C = %</p>
    <p>(i,j)!O</p>
    <p>e2ij , with eij = xij &quot; c%</p>
    <p>k=1</p>
    <p>aikskj . (10)</p>
    <p>The cost function:</p>
    <p>Its partial derivatives:</p>
    <p>Update rules:</p>
  </div>
  <div class="page">
    <p>Overfitting in Case of Sparse Data</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
    <p>Overfitted solution Regularized solution</p>
  </div>
  <div class="page">
    <p>Regularization against Overfitting</p>
    <p>Penalizing the use of large parameter values</p>
    <p>Estimating the distribution of unknown parameters (Variational Bayesian learning)</p>
    <p>Principal Component Analysis for Large Scale Problems with Lots of Missing Values</p>
    <p>Tapani.Raiko, Alexander.Ilin, Juha.Karhunen @hut.fi</p>
    <p>Helsinki University of Technology, Adaptive Informatics Research Center, Finland</p>
    <p>Abstract Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also di!er in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.</p>
    <p>Model: X d!n</p>
    <p>&quot; A d!c</p>
    <p>S c!n</p>
    <p>, c # d # n</p>
    <p>Minimized cost function:</p>
    <p>Ce = !</p>
    <p>(i,j)$O</p>
    <p>&quot; xij%</p>
    <p>c!</p>
    <p>k=1</p>
    <p>aikskj</p>
    <p>#2</p>
    <p>Reconstruction:</p>
    <p>xij = c!</p>
    <p>k=1</p>
    <p>aikskj , (i, j) /$ O</p>
    <p>Imputation algorithm: 1) Replace missing values with zeros, 2) Compute PCA, 3) Update mising values with AS, 4) Go to step 2).</p>
    <p>EM-like learning:</p>
    <p>A &amp; arg max A</p>
    <p>Ce , S &amp; arg max S</p>
    <p>Ce</p>
    <p>Subspace Learning (Ojas subspace rule):</p>
    <p>A &amp; A % ! &quot;Ce &quot;A</p>
    <p>, S &amp; S % ! &quot;Ce &quot;S</p>
    <p>.</p>
    <p>(Bayesian regularization)</p>
    <p>Minimized cost function: Cbr = v</p>
    <p>%1 x Ce + 'A'</p>
    <p>+ c!</p>
    <p>k=1</p>
    <p>v%1sk 'Sk:' 2 F + C(vx, vsk)</p>
    <p>Point (MAP) estimates for A and S</p>
    <p>A and S are modeled a posteriori using simple distributions q(aik) = N (aik; aik, $aik), q(skj) = N</p>
    <p>% skj; skj, $skj</p>
    <p>&amp;</p>
    <p>Minimized cost function</p>
    <p>Cvb = Eq</p>
    <p>' ln</p>
    <p>q(A, S)</p>
    <p>p(X, A, S)</p>
    <p>( = Cbr + C($aik, $skj)</p>
    <p>Extra term C($aik, $skj) accounts for posterior uncertainty  aik, $aik, skj, $skj are found by minimizing Cvb</p>
    <p>Left: Unregularized PCA solution is based on the only pair of fully observed vectors. Right: In regularized solution, the correlations are not trusted that much</p>
    <p>Most existing implementations of PCA are ine&quot;cient  We propose a gradient-based implementation which does not recon</p>
    <p>struct the missing values during learning (memory e&quot;cient)</p>
    <p>Faster convergence can be achieved using proposed approxiate Newtons iteration</p>
    <p>##i ( % )</p>
    <p>&quot;2C</p>
    <p>&quot;#2i</p>
    <p>*%$ &quot;C</p>
    <p>&quot;#i</p>
    <p>$ = 0: gradient descent, $ = 1: Newtons method  The update rules are modifications of Ojas subspace rule</p>
    <p>Collaborative filtering task: predict peoples preferences based on other people preferences</p>
    <p>d = 1.8  103 movies, n = 5  105 customers, given N = 108 movie ratings from 1 to 5, 98.8% of the values are missing</p>
    <p>Left fig.: Training rms error against computation time in hours  The learning speed of the proposed optimization scheme is</p>
    <p>demonstrated using unregularized PCA with $ = 0 (gradient) and with $ = 5/8 (speed-up)</p>
    <p>Right fig.: The rms errors on test data against computation time in hours</p>
    <p>Regularization helps avoid severe overfitting ! &quot; # $ % &quot;&amp; '# &amp;$!()&amp;</p>
    <p>!(%</p>
    <p>!(%$</p>
    <p>!(%%</p>
    <p>!(*#</p>
    <p>!(*&amp;</p>
    <p>&quot;</p>
    <p>+</p>
    <p>+</p>
    <p>,-./0123</p>
    <p>! &quot; # $ % &quot;&amp; '#</p>
    <p>!(*#</p>
    <p>!(*$</p>
    <p>!(*&amp;</p>
    <p>!(*%</p>
    <p>&quot;</p>
    <p>&quot;(!#</p>
    <p>&quot;(!$</p>
    <p>&quot;(!&amp;</p>
    <p>&quot;(!%</p>
    <p>&quot;(&quot;</p>
    <p>+</p>
    <p>+ ,-./0123</p>
    <p>-1,6;.-0&lt;1/</p>
    <p>=&gt;&quot;</p>
    <p>=&gt;#</p>
  </div>
  <div class="page">
    <p>Experiments with Netflix Data www.netflixprize.com</p>
    <p>Collaborative filtering task: predict peoples preferences based on other peoples preferences</p>
    <p>d = 18 000 movies, n = 500 000 customers, N = 100 000 000 movie ratings from 1 to 5</p>
    <p>98.8% of the values are missing</p>
    <p>Find c=15 principal components</p>
  </div>
  <div class="page">
    <p>Computational Performance Principal Component Analysis for Sparse High-Dimensional Data 9</p>
    <p>Method Complexity Seconds/Iter Hours to EO = 0.85 Gradient O(N c + nc) 58 1.9 Speed-up O(N c + nc) 110 0.22 Natural Grad. O(N c + nc2) 75 3.5 Imputation O(nd2) 110000 ! 64 EM O(N c2 + nc3) 45000 58 Table 1. Summary of the computational performance of di!erent methods on the Netflix problem. Computational complexities (per iteration) assume nave computation of products and inverses of matrices and ignores the computation of SVD in the imputation algorithm. While the proposed speed-up makes each iteration slower than the basic gradient update, the time to reach the error level 0.85 is greatly diminished.</p>
    <p>adapting them (marked as VB2). We initialized regularized PCA and VB1 using normal PCA learned with ! = 0.625 and orthogonalized A, and VB2 using VB1. The parameter ! was set to 2/3.</p>
    <p>Fig. 1 (right) shows the results. The performance of basic PCA starts to degrade during learning, especially using the proposed speed-up. Natural gradient diminishes this phenomenon known as overlearning, but it is even more e!ective to use regularization. The best results were obtained using VB2: The final validation error EV was 0.9180 and the training rms error EO was 0.7826 which is naturally larger than the unregularized EO = 0.7657.</p>
    <p>We studied a number of di!erent methods for PCA with sparse data and it turned out that a simple gradient descent approach worked best due to its minimal computational complexity per iteration. We also gave it a more than tenfold speed-up by using an approximated Newtons method. We found out empirically that setting the parameter ! = 2/3 seems to work well for our problem. It is left for future work to find out whether this generalizes to other problem settings. There are also many other ways to speed-up the gradient descent algorithm. The natural gradient did not help here, but we expect that the conjugate gradient method would. The modification to the gradient proposed in this paper, could be used together with the conjugate gradient speed-up. This will be another future research topic.</p>
    <p>There are also other benefits in solving the PCA problem by gradient descent. Algorithms that minimize an explicit cost function are rather easy to extend. The case of variational Bayesian learning applied to PCA was considered in Section 4, but there are many other extensions of PCA, such as using non-Gaussianity, non-linearity, mixture models, and dynamics.</p>
    <p>The developed algorithms can prove useful in many applications such as bioinformatics, speech processing, and meteorology, in which large-scale datasets with missing values are very common. The required computational burden is linearly proportional to the number of measured values. Note also that the proposed techniques provide an analogue of confidence regions showing the reliability of</p>
    <p>N=100 000 000, # of ratings</p>
    <p>c=15, # of components</p>
    <p>n=500 000, # of people</p>
    <p>d=18 000, # of movies</p>
  </div>
  <div class="page">
    <p>Error on Training Data against computation time in hours8 Tapani Raiko et al.</p>
    <p>Gradient</p>
    <p>Speed!up</p>
    <p>Natural Grad.</p>
    <p>Imputation</p>
    <p>EM</p>
    <p>Gradient</p>
    <p>Speed!up</p>
    <p>Natural Grad.</p>
    <p>Regularized</p>
    <p>VB1</p>
    <p>VB2</p>
    <p>Fig. 1. Left: Learning curves for unregularized PCA (Section 3) applied to the Netflix data: Root mean-square error on the training data EO is plotted against computation time in hours. Right: The root mean square error on the validation data EV from the Netflix problem during runs of several algorithms: basic PCA (Section 3), regularized PCA (Section 4) and VB (Section 4). VB1 fixes variances vsk to large values while VB2 updates all the parameters. The time scales are linear below 1 and logarithmic above 1.</p>
    <p>the observed part of the data led to problems as explained in Section 3. The covariance matrix had both missing values and values out of range.</p>
    <p>Using the EM algorithm by [12], the E-step (updating S) takes 7 hours and the M-step (updating A) takes 18 hours. (There is some room for optimization since we used a straightforward Matlab implementation.) Each iteration gives a much larger improvement compared to the imputation algorithm, but starting from a random initialization, EM could not reach a good solution in reasonable time.</p>
    <p>We also tested the subspace learning algorithm described in Section 3 with and without the proposed speed-up. Each run of the algorithm with di!erent values of the speed-up parameter ! was initialized in the same starting point (generated randomly from a normal distribution). The learning rate &quot; was adapted such that if an update decreased the cost function, &quot; was multiplied by 1.1. Each time an update would increase the cost, the update was canceled and &quot; was divided by 2. Figure 1 (left) shows the learning curves for basic gradient descent, natural gradient descent, and the proposed speed-up with the best found parameter value ! = 0.625. The proposed speed-up gave about a tenfold speed-up compared to the gradient descent algorithm even if each iteration took longer. Natural gradient was slower than the basic gradient. Table 1 gives a summary of the computational complexities.</p>
    <p>Overfitting We compared PCA (Section 3), regularized PCA (Section 4) and VB-PCA (Section 4) by computing the rms reconstruction error for the validation set V , that is, testing how the models generalize to new data: EV = !</p>
    <p>&quot;</p>
    <p>(i,j)!V e 2 ij . We tested VB-PCA by firstly fixing some of the parameter</p>
    <p>values (this run is marked as VB1 in Fig. 1, see [13] for details) and secondly by</p>
  </div>
  <div class="page">
    <p>Error on Validation Data against computation time in hours8 Tapani Raiko et al.</p>
    <p>Gradient</p>
    <p>Speed!up</p>
    <p>Natural Grad.</p>
    <p>Imputation</p>
    <p>EM</p>
    <p>Gradient</p>
    <p>Speed!up</p>
    <p>Natural Grad.</p>
    <p>Regularized</p>
    <p>VB1</p>
    <p>VB2</p>
    <p>Fig. 1. Left: Learning curves for unregularized PCA (Section 3) applied to the Netflix data: Root mean-square error on the training data EO is plotted against computation time in hours. Right: The root mean square error on the validation data EV from the Netflix problem during runs of several algorithms: basic PCA (Section 3), regularized PCA (Section 4) and VB (Section 4). VB1 fixes variances vsk to large values while VB2 updates all the parameters. The time scales are linear below 1 and logarithmic above 1.</p>
    <p>the observed part of the data led to problems as explained in Section 3. The covariance matrix had both missing values and values out of range.</p>
    <p>Using the EM algorithm by [12], the E-step (updating S) takes 7 hours and the M-step (updating A) takes 18 hours. (There is some room for optimization since we used a straightforward Matlab implementation.) Each iteration gives a much larger improvement compared to the imputation algorithm, but starting from a random initialization, EM could not reach a good solution in reasonable time.</p>
    <p>We also tested the subspace learning algorithm described in Section 3 with and without the proposed speed-up. Each run of the algorithm with di!erent values of the speed-up parameter ! was initialized in the same starting point (generated randomly from a normal distribution). The learning rate &quot; was adapted such that if an update decreased the cost function, &quot; was multiplied by 1.1. Each time an update would increase the cost, the update was canceled and &quot; was divided by 2. Figure 1 (left) shows the learning curves for basic gradient descent, natural gradient descent, and the proposed speed-up with the best found parameter value ! = 0.625. The proposed speed-up gave about a tenfold speed-up compared to the gradient descent algorithm even if each iteration took longer. Natural gradient was slower than the basic gradient. Table 1 gives a summary of the computational complexities.</p>
    <p>Overfitting We compared PCA (Section 3), regularized PCA (Section 4) and VB-PCA (Section 4) by computing the rms reconstruction error for the validation set V , that is, testing how the models generalize to new data: EV = !</p>
    <p>&quot;</p>
    <p>(i,j)!V e 2 ij . We tested VB-PCA by firstly fixing some of the parameter</p>
    <p>values (this run is marked as VB1 in Fig. 1, see [13] for details) and secondly by</p>
  </div>
  <div class="page">
    <p>Summary  PCA with sparse data and high dimensionality has two</p>
    <p>problems that require attention:</p>
    <p>Standard algorithms are computationally inefficient</p>
    <p>Overfitted model does not generalize well to new data</p>
    <p>We proposed solutions to both problems:</p>
    <p>Using gradient descent to minimize the cost, and speeding it up by an approximated Newtons method</p>
    <p>Regularization of the model by Variational Bayesian learning</p>
  </div>
</Presentation>
