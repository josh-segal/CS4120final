<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>mTCP: A Highly Scalable User-level TCP</p>
    <p>Stack for Multicore Systems</p>
    <p>EunYoung Jeong, Shinae Woo, Muhammad Jamshed, Haewon Jeong</p>
    <p>Sunghwan Ihm*, Dongsu Han, and KyoungSoo Park</p>
    <p>KAIST * Princeton University</p>
  </div>
  <div class="page">
    <p>Needs for Handling Many Short Flows</p>
    <p>End systems - Web servers</p>
    <p>C D</p>
    <p>F</p>
    <p>FLOW SIZE (BYTES)</p>
    <p>Flow count</p>
    <p>* Commercial cellular traffic for 7 days Comparison of Caching Strategies in Modern Cellular Backhaul Networks, MOBISYS 2013</p>
    <p>Middleboxes - SSL proxies - Network caches</p>
  </div>
  <div class="page">
    <p>Unsatisfactory Performance of Linux TCP</p>
    <p>Large flows: Easy to fill up 10 Gbps</p>
    <p>Small flows: Hard to fill up 10 Gbps regardless of # cores  Too many packets:</p>
    <p>Kernel is not designed well for multicore systems</p>
    <p>C o</p>
    <p>n n</p>
    <p>e ct</p>
    <p>io n</p>
    <p>s/ se</p>
    <p>c (x</p>
    <p>Number of CPU Cores</p>
    <p>TCP Connection Setup Performance</p>
    <p>Linux: 3.10.16 Intel Xeon E5-2690 Intel 10Gbps NIC</p>
    <p>Performance meltdown</p>
  </div>
  <div class="page">
    <p>Kernel Uses the Most CPU Cycles</p>
    <p>Performance bottlenecks 1. Shared resources 2. Broken locality 3. Per packet processing</p>
    <p>Bottleneck removed by mTCPKernel</p>
    <p>(without TCP/IP) 45%</p>
    <p>Packet I/O 4%</p>
    <p>TCP/IP 34%</p>
    <p>Application 17%</p>
    <p>CPU Usage Breakdown of Web Server Web server (Lighttpd) Serving a 64 byte file Linux-3.10</p>
  </div>
  <div class="page">
    <p>Inefficiencies in Kernel from Shared FD</p>
    <p>Shared listening queue</p>
    <p>Shared file descriptor space</p>
    <p>Per-core packet queue</p>
    <p>Receive-Side Scaling (H/W)</p>
    <p>Core 0 Core 1 Core 3Core 2</p>
    <p>Listening queue</p>
    <p>Lock</p>
    <p>File descriptor space</p>
    <p>Linear search for finding empty slot</p>
  </div>
  <div class="page">
    <p>Inefficiencies in Kernel from Broken Locality</p>
    <p>Per-core packet queue</p>
    <p>Receive-Side Scaling (H/W)</p>
    <p>Core 0 Core 1 Core 3Core 2</p>
    <p>Interrupt handle</p>
    <p>accept() read() write()</p>
    <p>Interrupt handling core != accepting core</p>
  </div>
  <div class="page">
    <p>Inefficiencies in Kernel from Lack of Support for Batching</p>
    <p>Inefficient per packet processing</p>
    <p>Frequent mode switching Cache pollution</p>
    <p>Per packet memory allocation</p>
    <p>Inefficient per system call processing</p>
    <p>accept(), read(), write()</p>
    <p>Packet I/O</p>
    <p>Kernel TCP</p>
    <p>Application thread</p>
    <p>BSD socket LInux epoll Kernel</p>
    <p>User</p>
  </div>
  <div class="page">
    <p>Previous Works on Solving Kernel Complexity</p>
    <p>Listening queue</p>
    <p>Connection locality</p>
    <p>App &lt;-&gt; TCP comm.</p>
    <p>Packet I/O API</p>
    <p>Linux-2.6 Shared No Per system call Per packet BSD</p>
    <p>Linux-3.9 SO_REUSEPORT</p>
    <p>Per-core No Per system call Per packet BSD</p>
    <p>Affinity-Accept Per-core Yes Per system call Per packet BSD</p>
    <p>MegaPipe Per-core Yes Batched system call</p>
    <p>Per packet custom</p>
    <p>How much performance improvement can we get if we implement a user-level TCP stack with all optimizations?</p>
    <p>Still, 78% of CPU cycles are used in kernel!</p>
  </div>
  <div class="page">
    <p>Clean-slate Design Principles of mTCP</p>
    <p>mTCP: A high-performance user-level TCP designed for multicore systems</p>
    <p>Clean-slate approach to divorce kernels complexity</p>
    <p>Each core works independently</p>
    <p>No shared resources</p>
    <p>Resources affinity</p>
    <p>Problems Our contributions</p>
    <p>Batching from flow processing from packet I/O to user API</p>
    <p>Easily portable APIs for compatibility</p>
  </div>
  <div class="page">
    <p>Overview of mTCP Architecture</p>
    <p>User-level packet I/O library (PSIO)</p>
    <p>mTCP thread 0 mTCP thread 1</p>
    <p>Application Thread 0</p>
    <p>Application Thread 1</p>
    <p>mTCP socket mTCP epoll</p>
    <p>NIC device driver Kernel-level</p>
    <p>User-level</p>
    <p>Core 0 Core 1</p>
    <p>[SIGCOMM10] PacketShader: A GPU-accelerated software router, http://shader.kaist.edu/packetshader/io_engine/index.html</p>
  </div>
  <div class="page">
    <p>User-level packet I/O library (PSIO)</p>
    <p>mTCP thread 0 mTCP thread 1</p>
    <p>Application Thread 0</p>
    <p>Application Thread 1</p>
    <p>mTCP socket mTCP epoll</p>
    <p>Device driver</p>
    <p>Core 0 Core 1</p>
    <p>Symmetric Receive-Side Scaling (H/W)</p>
    <p>Per-core packet queue</p>
    <p>Per-core listening queue</p>
    <p>Per-core file descriptor</p>
    <p>Kernel-level</p>
    <p>User-level</p>
  </div>
  <div class="page">
    <p>From System Call to Context Switching</p>
    <p>Packet I/O</p>
    <p>Kernel TCP</p>
    <p>Application thread</p>
    <p>BSD socket LInux epoll</p>
    <p>User-level packet I/O library</p>
    <p>mTCP thread</p>
    <p>Application Thread</p>
    <p>NIC device driver</p>
    <p>mTCP socket mTCP epoll Kernel</p>
    <p>User</p>
    <p>Linux TCP mTCP</p>
    <p>System call Context switching</p>
  </div>
  <div class="page">
    <p>From System Call to Context Switching</p>
    <p>Packet I/O</p>
    <p>Kernel TCP</p>
    <p>Application thread</p>
    <p>BSD socket LInux epoll</p>
    <p>User-level packet I/O library</p>
    <p>mTCP thread</p>
    <p>Application Thread</p>
    <p>NIC device driver</p>
    <p>mTCP socket mTCP epoll Kernel</p>
    <p>User</p>
    <p>Linux TCP mTCP</p>
    <p>System call Context switching</p>
    <p>higher overhead</p>
    <p>&lt; Batching to amortize context switch overhead</p>
  </div>
  <div class="page">
    <p>Accept queue</p>
    <p>S S/A F/A</p>
    <p>Data list</p>
    <p>ACK list</p>
    <p>Control list</p>
    <p>TX manager</p>
    <p>Connect queue</p>
    <p>Application thread</p>
    <p>SYN RST FIN</p>
    <p>Data list</p>
    <p>Write queue</p>
    <p>Close queue</p>
    <p>write()</p>
    <p>Rx manager</p>
    <p>Payload handler</p>
    <p>Socket API</p>
    <p>Internal event queue</p>
    <p>Event queue</p>
    <p>ACK list</p>
    <p>Control list</p>
    <p>TX manager</p>
    <p>connect() close()epoll_wait()accept()</p>
    <p>SYN ACK</p>
    <p>Data</p>
    <p>mTCP thread</p>
  </div>
  <div class="page">
    <p>Two goals: Easy porting + keeping popular event model</p>
    <p>Ease of porting</p>
    <p>Just attach mtcp_ to BSD socket API</p>
    <p>socket()  mtcp_socket(), accept()  mtcp_accept(), etc.</p>
    <p>Event notification: Readiness model using epoll()</p>
    <p>Porting existing applications</p>
    <p>Mostly less than 100 lines of code change</p>
    <p>Application Description Modified lines / Total lines</p>
    <p>Lighttpd An event-driven web server 65 / 40K</p>
    <p>ApacheBench A webserver performance benchmark tool 29 / 66K</p>
    <p>SSLShader A GPU-accelerated SSL proxy [NSDI 11] 43 / 6,618</p>
    <p>WebReplay A web log replayer 81 / 3,366</p>
  </div>
  <div class="page">
    <p>Optimizations for Performance</p>
    <p>Lock-free data structures</p>
    <p>Cache-friendly data structure</p>
    <p>Hugepages for preventing TLB missing</p>
    <p>Efficient TCP timer management</p>
    <p>Priority-based packet queuing</p>
    <p>Lightweight connection setup</p>
    <p>Please refer to our paper</p>
  </div>
  <div class="page">
    <p>mTCP Implementation</p>
    <p>11,473 lines (C code)</p>
    <p>Packet I/O, TCP flow management, User-level socket API, Event system library</p>
    <p>552 lines to patch the PSIO library</p>
    <p>Support event-driven packet I/O: ps_select()</p>
    <p>TCP implementation</p>
    <p>Follows RFC793</p>
    <p>Congestion control algorithm: NewReno</p>
    <p>Passing correctness test and stress test with Linux TCP stack</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Scalability with multicore</p>
    <p>Comparison of performance of multicore with previous solutions</p>
    <p>Performance improvement on ported applications</p>
    <p>Web Server (Lighttpd)  Performance under the real workload</p>
    <p>SSL proxy (SSL Shader, NSDI 11)  TCP bottlenecked application</p>
  </div>
  <div class="page">
    <p>T ra</p>
    <p>n sa</p>
    <p>ct io</p>
    <p>n s/</p>
    <p>se c</p>
    <p>(x 1</p>
    <p>Number of CPU Cores</p>
    <p>Linux REUSEPORT MegaPipe mTCP</p>
    <p>Multicore Scalability</p>
    <p>64B ping/pong messages per connection  Heavy connection overhead, small packet processing overhead  25x Linux, 5x SO_REUSEPORT*[LINUX3.9], 3x MegaPipe*[OSDI12]</p>
    <p>Shared fd in process</p>
    <p>Shared listen socket</p>
    <p>* [LINUX3.9] https://lwn.net/Articles/542629/ * [OSDI12] MegaPipe: A New Programming Interface for Scalable Network I/O, Berkeley</p>
    <p>Inefficient small packet processing in Kernel</p>
  </div>
  <div class="page">
    <p>Performance Improvement on Ported Applications</p>
    <p>Web Server (Lighttpd)  Real traffic workload: Static file</p>
    <p>workload from SpecWeb2009 set</p>
    <p>3.2x faster than Linux</p>
    <p>1.5x faster than MegaPipe</p>
    <p>SSL Proxy (SSLShader)  Performance Bottleneck in TCP</p>
    <p>Cipher suite 1024-bit RSA, 128-bit AES, HMACSHA1</p>
    <p>Download 1-byte object via HTTPS</p>
    <p>18% ~ 33% better on SSL handshake</p>
    <p>Linux REUSEPORT MegaPipe mTCP</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (G</p>
    <p>b p</p>
    <p>s) 26,762 28,208 27,725 31,710</p>
    <p>Tr a</p>
    <p>n sa</p>
    <p>ct io</p>
    <p>n s/</p>
    <p>se c</p>
    <p>(x 1</p>
    <p># Concurrent Flows</p>
    <p>Linux</p>
    <p>mTCP</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>mTCP: A high-performing user-level TCP stack for multicore systems  Clean-slate user-level design to overcome inefficiency in kernel</p>
    <p>Make full use of extreme parallelism &amp; batch processing  Per-core resource management</p>
    <p>Lock-free data structures &amp; cache-aware threading</p>
    <p>Eliminate system call overhead</p>
    <p>Reduce context switch cost by event batching</p>
    <p>Achieve high performance scalability  Small message transactions: 3x to 25x better</p>
    <p>Existing applications: 33% (SSLShader) to 320% (lighttpd)</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Source code is available at http://shader.kaist.edu/mtcp/</p>
    <p>https://github.com/eunyoung14/mtcp</p>
  </div>
</Presentation>
