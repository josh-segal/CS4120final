<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Consensus in a Box Inexpensive Coordination in Hardware</p>
    <p>Zsolt Istvn, David Sidler, Gustavo Alonso, Marko Vukolic*</p>
    <p>Systems Group, Department of Computer Science, ETH Zurich *IBM Research, Zurich 3/18/2016 1</p>
  </div>
  <div class="page">
    <p>Consensus is an essential function in datacenters</p>
    <p>How can consensus be made inexpensive?</p>
    <p>Motivation: Cost of consensus</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (c</p>
    <p>o n</p>
    <p>s e n</p>
    <p>s u</p>
    <p>s</p>
    <p>ro u</p>
    <p>n d</p>
    <p>s /s</p>
    <p>)</p>
    <p>Consensus latency (us)</p>
    <p>General</p>
    <p>purpose</p>
    <p>systems</p>
    <p>This talk:</p>
  </div>
  <div class="page">
    <p>Related work</p>
    <p>Speeding up consensus is an important problem</p>
    <p>Related work in networking, systems, HPC, etc.</p>
    <p>Specialized hardware can remove traditional limitations</p>
    <p>[1] Zhang et al. Smartswitch: Blurring the line between network infrastructure</p>
    <p>&amp; cloud applications. In HotCloud14</p>
    <p>[2] Mai et al. NetAgg: Using Middleboxes for Application-Specific On-path</p>
    <p>Aggregation in Data Centres. In CoNEXT14</p>
    <p>[3] Dang et al. NetPaxos: Consensus at Network Speed. In SOSR15</p>
    <p>[4] Poke et al. DARE: High -Performance State Machine Replication on</p>
    <p>RDMA Networks. In HPDC15.</p>
  </div>
  <div class="page">
    <p>Why? Consensus is expensive, but desired</p>
    <p>What? Atomic broadcast  Zookeepers ZAB protocol</p>
    <p>How? Specialized processor on FPGA  Tight integration with 10Gbps networking + deep pipelining</p>
    <p>Evaluation? Drop-in replacement for Memcached with Zookeepers replication</p>
    <p>Consensus in a Box</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 5</p>
    <p>Zookeepers Atomic Broadcast</p>
    <p>Ordered, reliable channels</p>
    <p>Write</p>
    <p>Propose</p>
    <p>Propose</p>
    <p>Ack.</p>
    <p>Ack.</p>
    <p>Commit</p>
    <p>Commit</p>
    <p>Follower</p>
    <p>Leader</p>
    <p>Follower</p>
  </div>
  <div class="page">
    <p>ZookeeperTCP</p>
    <p>Atomic</p>
    <p>Broadcast Replicated</p>
    <p>Data Store</p>
    <p>Writes</p>
    <p>ReadsClients</p>
    <p>Zookeeper from 10000ft</p>
    <p>Other Zookeeper</p>
    <p>instances</p>
  </div>
  <div class="page">
    <p>Inherent parallelism</p>
    <p>Tight integration</p>
    <p>Very fast on-chip memory</p>
    <p>Specialized processor architecture</p>
    <p>DDR Memory Controller</p>
    <p>Atomic Broadcast (State</p>
    <p>Machine)</p>
    <p>FPGA</p>
    <p>Main Memory Key-value Store</p>
    <p>DDR3</p>
  </div>
  <div class="page">
    <p>Networking optimizations  Low-latency on-chip buffers for RX path</p>
    <p>Datacenter and application-specific knowledge</p>
    <p>Predictable behavior  Fast local caches for common case behavior</p>
    <p>Pipelined execution</p>
    <p>What makes it go fast</p>
    <p>Networking Consensus Key-value</p>
    <p>store</p>
    <p>Latency</p>
    <p>Throughput</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 9</p>
    <p>Deployment and Evaluation</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 10</p>
    <p>Hardware platform</p>
    <p>Xilinx VC709 Evaluation Board</p>
    <p>SFP+</p>
    <p>SFP+</p>
    <p>SFP+</p>
    <p>SFP+</p>
    <p>DRAM (8GB)</p>
    <p>FPGA</p>
    <p>Networking Atomic</p>
    <p>Broadcast</p>
    <p>Replicated</p>
    <p>key-value</p>
    <p>store</p>
    <p>Reads</p>
    <p>Writes</p>
    <p>SW Clients /</p>
    <p>Other nodes</p>
    <p>Other nodes</p>
    <p>Other nodes</p>
    <p>TCP</p>
    <p>Direct</p>
    <p>Direct</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 11</p>
    <p>Evaluation setup</p>
    <p>X 12</p>
    <p>Clients</p>
    <p>Drop-in replacement for Memcached with Zookeepers replication</p>
    <p>Standard tools for benchmarking (libmemcached)  Simulating 100s of clients</p>
    <p>Comm. over TCP/IP</p>
    <p>Comm. over direct</p>
    <p>connections + Leader election</p>
    <p>+ Recovery</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 12</p>
    <p>Latency of KVS writes (consensus)</p>
    <p>Consensus</p>
    <p>Memaslap</p>
    <p>(ixgbe)</p>
    <p>TCP / 10Gbps Ethernet</p>
    <p>~3s</p>
    <p>Direct connections</p>
  </div>
  <div class="page">
    <p>T h</p>
    <p>ro u</p>
    <p>g p</p>
    <p>u t (c</p>
    <p>o n</p>
    <p>s e n</p>
    <p>s u</p>
    <p>s</p>
    <p>ro u</p>
    <p>n d</p>
    <p>s /s</p>
    <p>)</p>
    <p>Consensus latency (us)</p>
    <p>FPGA (Direct)</p>
    <p>FPGA (TCP)</p>
    <p>DARE* (Infiniband)</p>
    <p>Libpaxos (TCP)</p>
    <p>Etcd (TCP)</p>
    <p>Zookeeper (TCP)</p>
    <p>Specialized</p>
    <p>solutions</p>
    <p>The benefit of specialization</p>
    <p>General</p>
    <p>purpose</p>
    <p>solutions</p>
    <p>[1] Dragojevic et al. FaRM: Fast Remote Memory. In NSDI14. [2] Poke et al. DARE: High-Performance State Machine Replication on RDMA Networks. In HPDC15.</p>
    <p>*=We extrapolated from the 5 node setup for a 3 node setup and removed estimated client overhead.</p>
  </div>
  <div class="page">
    <p>Software*</p>
    <p>Predictable hardware performance</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g p</p>
    <p>u t (c</p>
    <p>o n</p>
    <p>s e n</p>
    <p>s u</p>
    <p>s</p>
    <p>ro u</p>
    <p>n d</p>
    <p>s /s</p>
    <p>)</p>
    <p>Consensus latency (us)</p>
    <p>FPGA (Direct) + 99th</p>
    <p>FPGA (TCP) + 99th</p>
  </div>
  <div class="page">
    <p>||Consensus in a Box 3/18/2016 15</p>
    <p>Throughput overview M</p>
    <p>il li o</p>
    <p>n r e</p>
    <p>p li c a</p>
    <p>te d</p>
    <p>K V</p>
    <p>S w</p>
    <p>ri te</p>
    <p>s /s</p>
  </div>
  <div class="page">
    <p>Consensus is expensive but often necessary  Solution: specialization and tight integration with networking</p>
    <p>We built high-throughput low-latency consensus in hardware</p>
    <p>Specialized hardware opens</p>
    <p>up new opportunities for</p>
    <p>smarter networks</p>
    <p>Conclusion</p>
    <p>{zsolt.istvan},{david.sidler}@inf.ethz.ch</p>
  </div>
</Presentation>
