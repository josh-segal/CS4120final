<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box</p>
    <p>Adversarial Sensor Attack and Countermeasures</p>
    <p>Jiachen Sun1Yulong Cao1, Qi Alfred Chen2, and Z. Morley Mao1</p>
  </div>
  <div class="page">
    <p>Autonomous Vehicle (AV) Perception</p>
    <p>Sensors Perception</p>
    <p>Prediction</p>
    <p>Planning</p>
    <p>Control</p>
    <p>Position Speed Object Detection</p>
    <p>Object Future Path</p>
    <p>AV Future Path</p>
    <p>Breaking Steering .</p>
    <p>LiDAR: Light Detection And Ranging Picture ref: https://softwareengineeringdaily.com/2017/07/28/self-driving-deep-learning-with-lex-fridman/</p>
  </div>
  <div class="page">
    <p>Autonomous Vehicle (AV) Perception</p>
    <p>Machine learning, especially deep learning, is heavily adopted in stateof-the-art AV perception pipelines.</p>
    <p>Camera</p>
    <p>LiDAR</p>
    <p>Camera-based Perception</p>
    <p>Model</p>
    <p>LiDAR-based Perception</p>
    <p>Model</p>
    <p>Detected Obstacles</p>
    <p>Detected Obstacles</p>
  </div>
  <div class="page">
    <p>Related Work: Security of AV Perception</p>
    <p>Security of camera-based perception is well studied  Found to be vulnerable to adversarial machine learning (AML) attacks in the</p>
    <p>physical world.</p>
    <p>Camera</p>
    <p>LiDAR</p>
    <p>Camera-based Perception</p>
    <p>Model</p>
    <p>LiDAR-based Perception</p>
    <p>Model</p>
    <p>Fake Obstacles</p>
    <p>Detected Obstacles</p>
  </div>
  <div class="page">
    <p>Related Work: Security of LiDAR-based AV Perception</p>
    <p>Adv-LiDAR [1] demonstrated LiDAR-based perception is vulnerable to sensor attack with the help of AML.</p>
    <p>Formulation of the sensor attack capability.  Strategically injecting points.</p>
  </div>
  <div class="page">
    <p>Related Work: Security of LiDAR-based AV Perception</p>
    <p>Adv-LiDAR [1] demonstrated LiDAR-based perception is vulnerable to sensor attack with the help of adversarial machine learning.</p>
    <p>LiDAR LiDAR-based Perception</p>
    <p>Model</p>
    <p>Fake Obstacles</p>
    <p>[1] Cao, Yulong, et al. &quot;Adversarial sensor attack on lidar-based perception in autonomous driving.&quot; Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2019.</p>
    <p>Strategically Injecting Points</p>
    <p>Optimization Solving</p>
    <p>fake vehicle Detected Obstacles</p>
  </div>
  <div class="page">
    <p>Motivation: Limitations of Existing Work</p>
    <p>White-box attack limitation  Adv-LiDAR assumes that attackers have full knowledge of LiDAR-based perception model</p>
    <p>along with its pre- and post-processing modules.</p>
    <p>LiDAR Fake Obstacles</p>
    <p>Preprocessing</p>
    <p>DNN Model</p>
    <p>Postprocessing</p>
  </div>
  <div class="page">
    <p>Motivation: Limitations of Existing Work</p>
    <p>White-box attack limitation  Attack generality limitation</p>
    <p>Adv-LiDAR only targets Apollo 2.5 model. The designed differentiable approximation function cannot generalize to other models.</p>
    <p>Optimized adversarial examples generated by Adv-LiDAR cannot attack other models.</p>
    <p>LiDAR Fake Obstacles</p>
    <p>Preprocessing</p>
    <p>Apollo 2.5 Model</p>
    <p>Postprocessing</p>
    <p>differentiable approximation function</p>
  </div>
  <div class="page">
    <p>Motivation: Limitations of Existing Work</p>
    <p>White-box attack limitation  Attack generality limitation  No practical defense solution</p>
    <p>There is no countermeasure proposed, making AVs still open to LiDAR spoofing attacks.</p>
    <p>LiDAR Fake Obstacles</p>
    <p>Preprocessing</p>
    <p>Apollo 2.5 Model</p>
    <p>Postprocessing</p>
    <p>differentiable approximation function</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Explore a general vulnerability of current LiDAR-based perception architectures.</p>
    <p>Construct the first black-box attacks and achieve ~80% mean attack success rates on all target models .</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Explore a general vulnerability of current LiDAR-based perception architectures and construct the firstblack-box spoofing attack.</p>
    <p>Perform the first defense study, proposing CARLO as an anomaly detection module that can be stacked on LiDAR-based perception models.</p>
    <p>Reduce the mean attack success rate to ~5.5% without sacrificing the detection accuracy.</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Explore a general vulnerability of current LiDAR-based perception architectures and construct the firstblack-box spoofing attack.</p>
    <p>Perform the first defense study, proposing CARLO as an anomaly detection module that can be stacked on LiDAR-based perception models.</p>
    <p>Design the first end-to-end general architecture for robust LiDAR-based perception.</p>
    <p>Reduce the mean attack success rate to ~2.3% with similar detection accuracy to the original model.</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>Physical sensor attack capability[1]  Number of points. Attackers can spoof at most 200 points into the</p>
    <p>LiDAR point clouds.  Location of points. Attackers can modify the distance, altitude, and</p>
    <p>azimuth of a spoofed point. Azimuth is within 10.</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>Physical sensor attack capability[1]  Number of points: 200 points.  Location of points: distance, altitude, and azimuth (10).</p>
    <p>Attack model  Goal: spoofing fake vehicles right in front of the victim AV [1] .  Attackers can control the spoofed points within the described sensor</p>
    <p>attack capability.  Attackers are not required to have access to the perception systems.</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>Physical sensor attack capability[1]  Number of points: 200 points.  Location of points: distance, altitude, and azimuth (10).</p>
    <p>Attack model  Goal: spoofing fake vehicles right in front of the victim AV [1] .  Within the described sensor attack capability.  Black-box access assumption.</p>
    <p>Defense model  We consider defending LiDAR spoofing attacks under both white</p>
    <p>and black-box settings.  We focus on software-level countermeasures due to cost concerns.</p>
  </div>
  <div class="page">
    <p>State-of-the-art LiDAR-based Perception Models  Birds-eye view (BEV)-based Model</p>
    <p>Baidu Apollo 5.0[1] (latest version)  Baidu Apollo 2.5 (model attacked in [2])</p>
    <p>Voxel-based Model  PointPillars[3] (CVPR19, used by AutoWare [4])  VoxelNet[5] (CVPR18)</p>
    <p>Point-wise Model  PointRCNN[6] (CVPR19)  Fast PointRCNN[7] (ICCV19)</p>
    <p>[1] Baidu Apollo. https://apollo.auto, 2020. [2] Cao, Yulong, et al. &quot;Adversarial sensor attack on lidar-based perception in autonomous driving.&quot; Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. 2019. [3] Lang, Alex H., et al. &quot;Pointpillars: Fast encoders for object detection from point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. [4] AutoWare.ai. https://gitlab.com/autowarefoundation/autoware.ai, 2020. [5] Zhou, Yin, and Oncel Tuzel. &quot;Voxelnet: End-to-end learning for point cloud based 3d object detection.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. [6] Shi, Shaoshuai, Xiaogang Wang, and Hongsheng Li. &quot;Pointrcnn: 3d object proposal generation and detection from point cloud.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. [7] Chen, Yilun, et al. &quot;Fast point r-cnn.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.</p>
  </div>
  <div class="page">
    <p>A General Vulnerability &amp; Black-box Adversarial Sensor Attack</p>
  </div>
  <div class="page">
    <p>Behind the Scenes of Adv-LiDAR</p>
    <p>A valid front-near vehicle (located 5-8 meters right in front of the AV) should contain ~2000 reflected points and occupy 15 in azimuth[1] .</p>
    <p>However, Adv-LiDAR was able to spoof a fake front-near vehicle by injecting much fewer amount of points (80 points).</p>
    <p>A valid front-near vehicle</p>
    <p>An attack trace generated by Adv-LiDAR</p>
    <p>[1] Statistical study on KITTI dataset (64-beam LiDAR) KITTI Vision Benchmark: 3D Object Detection. http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d, 2020.</p>
  </div>
  <div class="page">
    <p>Behind the Scenes of Adv-LiDAR</p>
    <p>Two situations that a valid vehicle contains much fewer points in a LiDAR point cloud:</p>
    <p>An occluded vehicle  A distant vehicle</p>
  </div>
  <div class="page">
    <p>False Positives</p>
    <p>Based on these observations, we find and validate two false positive (FP) conditions for the models</p>
  </div>
  <div class="page">
    <p>Vulnerability Identification</p>
    <p>Attackers can directly exploit such two FP conditions to fool the LiDAR-based perception models and spoof a fake vehicle with much fewer points.</p>
  </div>
  <div class="page">
    <p>Vulnerability Identification</p>
    <p>Attackers can directly exploit such two FP conditions to fool the LiDAR-based perception models and spoof a fake vehicle with much fewer points.</p>
    <p>FP1 State-of-the-art models perform detection in the 3D space where the occluder and occludee stands apart with each other. However, DNN models prefer local features.</p>
    <p>FP2 Object detection models are designed to be insensitive to the locations of objects.</p>
  </div>
  <div class="page">
    <p>Attack Evaluation</p>
    <p>Evaluation setup  Environments: KITTI[1] point clouds.  Combination of digital spoofing and physical spoofing.</p>
    <p>Black-box attacks universally achieve ~80% mean attack success rate (ASR) on all target models.</p>
  </div>
  <div class="page">
    <p>CARLO: oCclusion-Aware hieRarchy anomaLy detectiOn</p>
  </div>
  <div class="page">
    <p>Free Space Detection</p>
    <p>Free space: the frustum (the straight-line path) from the LiDAR sensor and any point in the point cloud.</p>
    <p>Inter-occlusion</p>
    <p>Intra-occlusion</p>
    <p>Due to intra-occlusion and inter-occlusion, there is limited free space inside a valid</p>
    <p>vehicles bounding box.</p>
  </div>
  <div class="page">
    <p>Free Space Detection</p>
    <p>Free space: the frustum (the straight-line path) from the LiDAR sensor and any point in the point cloud.</p>
    <p>Due to the limited sensor attack</p>
    <p>capability, there is a large portion of free space inside a fake</p>
    <p>vehicles bounding box.</p>
    <p>Lasers can penetrate the spoofed vehicle so that points are located behind the bounding box.</p>
  </div>
  <div class="page">
    <p>CARLO</p>
    <p>CARLO serves as a post-processing module leveraging free space as a physical invariant to detect spoofed vehicles.</p>
    <p>CARLO can be efficiently stacked onto existing LiDAR-based perception architectures.</p>
    <p>No need for model re-training.  Consists of another GPU-friendly submodule to achieve around 8.5ms per-vehicle</p>
    <p>processing time.</p>
    <p>LiDAR LiDAR-based Perception</p>
    <p>Model</p>
    <p>Fake Obstacles</p>
    <p>CARLO Fake</p>
    <p>Obstacles</p>
    <p>Valid Obstacles</p>
    <p>Please refer to our paper for more details of CARLO.</p>
  </div>
  <div class="page">
    <p>CARLO Evaluation</p>
    <p>CARLO overall reduces the mean attack success rate from ~80% to 5.5%.</p>
    <p>The accuracy of CARLO achieves at least 99.5%.</p>
    <p>The 0.5% detection errors comes from some faraway vehicles.</p>
    <p>No immediate impacts on AVs current driving decisions.</p>
    <p>CARLO can also defend the white-box attack, Adv-LiDAR, and its adaptive attack.</p>
  </div>
  <div class="page">
    <p>SVF: Sequential View Fusion A Robust LiDAR-based Perception Architecture</p>
  </div>
  <div class="page">
    <p>Existing Architectures Revisit</p>
  </div>
  <div class="page">
    <p>Existing Architectures Revisit</p>
  </div>
  <div class="page">
    <p>Front View (FV) Should Help!</p>
    <p>The occluder and occludee neighbor with each other in the FV, making it possible for DNN models to learn the local correlations. FP1</p>
    <p>A valid vehicles points are clustered in the FV. However, due to the limited sensor attack capability, attack traces will scatter in the FV. FP2</p>
  </div>
  <div class="page">
    <p>Front View (FV) Should Help!</p>
  </div>
  <div class="page">
    <p>Sequential View Fusion (SVF)</p>
    <p>Attach a semantic segmentation network to the FV representation.</p>
    <p>Output the probability score of each point that it belongs to a vehicle.</p>
    <p>An easier task as it does not need to estimate object-level output.</p>
    <p>Achieve much more satisfactory results than the 3D object detection task over FV[1,2].</p>
  </div>
  <div class="page">
    <p>Sequential View Fusion (SVF)</p>
    <p>Attach a semantic segmentation network to the FV representation.</p>
    <p>The original point cloud is augmented with the scores from the FV.</p>
    <p>The final 3D object detection module takes the augmented point cloud as input.</p>
    <p>Reserve the advantages of detection on 3D representations with useful information from FV.</p>
  </div>
  <div class="page">
    <p>SVF Evaluation</p>
    <p>SVF models are shown to be robust against LiDAR spoofing attacks, where the mean success rates are merely ~2.3%.</p>
    <p>Similar detection accuracy with the original models.</p>
    <p>SVF models are also resilient to the state-of-the-art white-box attack, AdvLiDAR, and its adaptive attack.</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>Attack Practicality  Large-scale evaluations are based on digital LiDAR spoofing.  Physical LiDAR spoofing is performed in in-lab environments.  No real road test due to cost concerns.</p>
    <p>Vulnerability Completeness  The identified vulnerability only partially explains the success of LiDAR spoofing attacks.</p>
    <p>Defenses Guarantees  Both defense solutions cannot provide strong guarantees.  Defenses may fail when the sensor attack capability improves dramatically (e.g., injecting</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Explore a general vulnerability of current LiDAR-based perception architectures and construct the firstblack-box spoofing attack.</p>
    <p>Perform the first defense study, proposing CARLO as an anomaly detection module that can be stacked on LiDAR-based perception models.</p>
    <p>Design the first end-to-end general architecture for robust LiDAR-based perception.</p>
    <p>Thank you !</p>
    <p>Q&amp;A Contact us!</p>
    <p>jiachens@umich.edu</p>
  </div>
</Presentation>
