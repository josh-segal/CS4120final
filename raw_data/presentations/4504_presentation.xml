<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Applying Classification Techniques to Remotely-Collected</p>
    <p>Program Execution Data</p>
    <p>This work was supported in part by NSF awards CCF-0205118 to NISS, CCR-0098158 and CCR-0205265 to Univers ity of Maryland, and CCR-0205422, CCR-0306372, and CCR-0209322 to Georgia Tech.</p>
    <p>Murali Haran Penn State University</p>
    <p>Alan Karr, Ashish Sanil National Institute of Statistical Sciences</p>
    <p>Adam Porter University of</p>
    <p>Maryland</p>
    <p>Alessandro Orso Georgia Institute</p>
    <p>of Technology</p>
  </div>
  <div class="page">
    <p>Testing &amp; Analysis after Deployment</p>
    <p>Program P</p>
    <p>User User</p>
    <p>User</p>
    <p>User UserUserUserUser</p>
    <p>UserUserUser</p>
    <p>User</p>
    <p>User UserUser</p>
    <p>User</p>
    <p>Field DataSE Tasks</p>
    <p>[Pavlopoulou99] Test adequacy Residual coverage data [Hilbert00] Usability testing GUI interactions [Dickinson01] Failure classification Caller/callee profiles [Bowring02] Coverage analysis Partial coverage data [Orso03] Impact analysis Dynamic slices [Liblit05] Fault localization Various profiles (returns, )</p>
  </div>
  <div class="page">
    <p>Tradeoffs of T&amp;A after Deployment</p>
    <p>In-house (+) Complete control (measurements, reruns, ) (-) Small fraction of behaviors</p>
    <p>In the field (+) All (exercised) behaviors (-) Little control</p>
    <p>Only partial measures, no reruns,   In particular, no oracles  Currently, mostly crashes</p>
  </div>
  <div class="page">
    <p>Our Goal</p>
    <p>Provide a technique for automatically identifying failures  Mainly, in the field  Useful in-house too</p>
    <p>Automatically generated test cases</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation and Goal  General Approach  Empirical Studies  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation and Goal  General Approach  Empirical Studies  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Background: Classification Techniques</p>
    <p>Classification -&gt; Supervised learning -&gt; Machine learning</p>
    <p>Many existing techniques (logistic regression, neural networks, tree-based classifiers, SVM, )</p>
    <p>obj 1</p>
    <p>obj 2</p>
    <p>obj n</p>
    <p>Learning Algorithm Model</p>
    <p>label x</p>
    <p>label y label z</p>
    <p>obj i</p>
    <p>predicted label</p>
    <p>Training</p>
    <p>Classification Classifier</p>
    <p>Model</p>
    <p>Pass/Fail Random Forests</p>
    <p>Executions</p>
    <p>Execution Data</p>
  </div>
  <div class="page">
    <p>Background: Random Forests Classifiers</p>
    <p>Tree-based classifiers  Partition predictor space in</p>
    <p>hyper-rectangular regions  Regions are assigned a label (+) Easy to interpret (-) Unstable</p>
    <p>Random forests [Breiman01]  Integrate many (500) tree classifiers  Classification via a voting scheme (+) Easy to interpret (+) Stable</p>
    <p>fail</p>
    <p>size  14.5</p>
    <p>size  8.5</p>
    <p>time  111</p>
    <p>time &gt; 55</p>
    <p>passfail</p>
    <p>pass</p>
    <p>fail</p>
    <p>(size=10, time=80)</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>Some critical open issues  What data should we collect?  What tradeoffs exist between different types of data?  How reliable/generalizable are the statistical analyses?</p>
    <p>InstrumentorP P</p>
    <p>inst</p>
    <p>Test Cases</p>
    <p>R un</p>
    <p>ti m</p>
    <p>e</p>
    <p>Execution Data</p>
    <p>Labels (pass/fail)</p>
    <p>Learning Algorithm</p>
    <p>Model (random forest)</p>
    <p>Training Set</p>
    <p>Training (In-House)</p>
    <p>Classification (In the Field)</p>
    <p>P inst</p>
    <p>Users R un</p>
    <p>ti m</p>
    <p>e</p>
    <p>Execution Data</p>
    <p>Classifier</p>
    <p>Model</p>
    <p>Predicted Labels</p>
    <p>(pass/fail)</p>
  </div>
  <div class="page">
    <p>Specific Research Questions</p>
    <p>RQ1: Can we reliably classify program outcomes using execution data?</p>
    <p>RQ2: If so, what type of execution data should we collect?</p>
    <p>RQ3: How can we reduce runtime data collection overhead while still producing accurate and reliable classifications?</p>
    <p>Set of exploratory studies</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation and Goal  General Approach  Empirical Studies  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Setup (I)</p>
    <p>Subject program  JABA bytecode analysis library  60 KLOC, 400 classes, 3000 methods  19 single-fault versions (golden version + 1 real fault)</p>
    <p>Training set  707 test cases (7 drivers applied to 101 input programs)  Collected various kinds of execution data (e.g., counts</p>
    <p>for throws, catch blocks, basic blocks, branches, methods, call edges, )</p>
    <p>Golden version to label passing/failing runs</p>
  </div>
  <div class="page">
    <p>Experimental Setup (II)</p>
    <p>Users Runs Classifier Predicted</p>
    <p>Outcome (pass/fail)</p>
    <p>Model</p>
    <p>In-House In the Field</p>
    <p>Training Set Learning</p>
    <p>Algorithm</p>
    <p>Model (random forest)</p>
    <p>Ideal setting, but  Expensive  Difficult to get enough data points  Oracle problem</p>
    <p>=&gt; Simulate users runs</p>
    <p>Users Runs</p>
    <p>Training Set 1/3</p>
    <p>Training Set 2/3</p>
    <p>Training Set 1/3</p>
    <p>Training Set 2/3</p>
    <p>classification error (misclassification rate)</p>
  </div>
  <div class="page">
    <p>RQ1 &amp; RQ2: Can We Classify at All? How?</p>
    <p>RQ1: Can we reliably classify program outcomes using execution data?</p>
    <p>RQ2: Assuming we can classify program outcomes, what type of execution data should we collect?</p>
    <p>We first considered a specific kind of execution data: basic-block counts (~20K) (simple measure, intuitively related to faults)</p>
    <p>Results: classification error estimates always almost 0!  But, time overheard ~15% and data volume not negligible =&gt; Other kinds of execution data</p>
    <p>exec i</p>
    <p>pass/fail</p>
    <p>Basic-block counts</p>
  </div>
  <div class="page">
    <p>RQ1 &amp; RQ2: Can We Classify at All? How?</p>
    <p>We considered other kinds of execution data:  Basic-block counts yielded almost perfect predictors</p>
    <p>=&gt; richer data not considered  Counts for: throws, catch-blocks, methods, and call-edges</p>
    <p>Results  Throw and catch-block counts are poor predictors  Method counts produced nearly perfect models</p>
    <p>As accurate as block counts, but much cheaper to collect  3000 methods vs. 20000 blocks (overhead &lt; 5%)</p>
    <p>Branch and call-edge counts equally accurate, but more costly than method counts</p>
    <p>Preliminary conclusion (1): Possible to classify program runs; method counts provided high accuracy at low cost</p>
  </div>
  <div class="page">
    <p>RQ3: Can We Collect Less Information?</p>
    <p>Method-count models used between 2 and 7 method counts. Great for instrumentation, but</p>
    <p>Two alternative hypotheses  Few methods are relevant -&gt; must choose specific methods well  Many, redundant methods -&gt; method selection less important</p>
    <p>To investigate, performed 100 random samplings  Took 10% random samples of method counts and rebuilt models  Models were excellent 90% of the times  Evidence that many method counts are good predictors</p>
    <p>Preliminary conclusion (2): failure signal spread, rather than localized to single entities =&gt; estimates can be based on a few data, collected with negligible overhead</p>
  </div>
  <div class="page">
    <p>Validity of the Analysis</p>
    <p>Two main issues to consider  Multiplicity  Generality</p>
  </div>
  <div class="page">
    <p>Statistical Issues -- Multiplicity</p>
    <p>When # of predictors far exceeds # of data points, the likelihood of finding spurious relationship increases  i.e., random relationships confused for real ones</p>
    <p>We took two steps to address the problem  Consider method counts</p>
    <p>(least number of predictors)  Conducted study in which we</p>
    <p>Randomly permuted method counts  Took a 10% random sample of method</p>
    <p>counts and rebuilt models (100 times) =&gt; Never found good models based on this data</p>
    <p>Preliminary conclusion (3): Results are unlikely to be due to random chance</p>
    <p>Executions</p>
    <p>M et</p>
    <p>ho ds 3 7 11 2</p>
    <p>Executions</p>
    <p>M et</p>
    <p>ho ds 3 7 11 2</p>
  </div>
  <div class="page">
    <p>Statistical Issues -- Generality</p>
    <p>Classifiers for 1 specific bug are useful, but  We would like to have models that encode correct</p>
    <p>behavior for the application in general  Looked for predictors that worked in general Found 11 excellent predictors for all versions</p>
    <p>Programs typically contain more than 1 bug  Applied our approach to 6 multi-bug versions  Models had error rates less than 2% in most cases</p>
    <p>Preliminary conclusion (4): Results promising w.r.t. generality (but need to investigate further)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Motivation and Goal  General Approach  Empirical Studies  Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Possible to classify program outcomes using execution data</p>
    <p>Method counts gave high accuracy at low cost  Estimates can be computed based on very few</p>
    <p>data, collected with negligible overhead  Our results are unlikely to depend on random</p>
    <p>chance and are promising in terms of generality  But, these are still preliminary results, and we</p>
    <p>need to investigate further</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Multiple faults  Investigate relationship between</p>
    <p>predictors and failures  Investigate relationship between</p>
    <p>predictors and faults  Conduct further experiments with</p>
    <p>system(s) in actual use</p>
  </div>
</Presentation>
