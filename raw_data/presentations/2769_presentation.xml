<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Comparison of Internet Traffic Classification Tools</p>
    <p>IMRG Workshop on Application Classification and Identification October 3, 2007</p>
    <p>BBN Technologies</p>
    <p>Hyunchul Kim CAIDA, UC San Diego</p>
    <p>Joint work with</p>
    <p>Marina Fomenkov, kc claffy, Nevil Brownlee (CAIDA, UC San Diego) Dhiman Barman, Michalis Faloutsos (UC Riverside)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>We evaluated the performance of  CoralReef [CoralReef 07]  BLINC [Karagiannis 05]  Six machine learning algorithms [WEKA 07]</p>
    <p>Data used : 7 payload traces  Three backbone and four edge traces  From Japan, Korea, Trans-pacific, and US</p>
    <p>Performance metrics  Per-whole trace : accuracy  Per-application : precision, recall, and F-measure  Running time</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>Trace (Country)</p>
    <p>Link type Date (Local time)</p>
    <p>Start time &amp; duration</p>
    <p>(Local time)</p>
    <p>Average Utilization</p>
    <p>(Mbps)</p>
    <p>Payload bytes per each packet</p>
    <p>PAIX-I (US)</p>
    <p>OC48 Backbone, unidirectional</p>
    <p>PAIX-II (US)</p>
    <p>OC48 Backbone</p>
    <p>WIDE (US-JP)</p>
    <p>KEIO-I (JP)</p>
    <p>KEIO-II (JP)</p>
    <p>KAIST-I (KR)</p>
    <p>KAIST-II (KR)</p>
  </div>
  <div class="page">
    <p>Payload-based classification</p>
    <p>Classification unit  5-tuple flow</p>
    <p>&lt;srcip, dstip, protocol, srcport, dstport&gt;</p>
    <p>With 64 seconds timeout</p>
    <p>5 minute interval</p>
    <p>Payload signatures of 33+ applications from  The BLINC work [Karagiannis 05]</p>
    <p>Jeff Erman et al.s work [Erman 06]</p>
    <p>Korean P2P/File sharing applications [Won 06]</p>
    <p>Manual payload inspection</p>
  </div>
  <div class="page">
    <p>Application breakdown</p>
    <p>Percentage of flows Percentage of bytes News, Streaming</p>
    <p>DNS</p>
    <p>PlanetLab</p>
    <p>P2P, FTP</p>
  </div>
  <div class="page">
    <p>Tools used</p>
    <p>CoralReef</p>
    <p>Port number based classification</p>
    <p>Version 3.8 (or later)</p>
    <p>BLINC</p>
    <p>Host behavior-based classification</p>
    <p>28 configurable threshold parameters</p>
    <p>WEKA</p>
    <p>A collection of machine learning algorithms</p>
    <p>6 most often used / well-known algorithms</p>
    <p>Key attributes, training set size, and the best algo?</p>
  </div>
  <div class="page">
    <p>Machine learning algorithms</p>
    <p>Supervised machine learning algorithms</p>
    <p>Bayesian Decision Trees Rules Functions Lazy</p>
    <p>Nave Bayesian, Support Vector Machine, [Moore 05, Williams 05] C4.5 Neural Net [Auld 07, k-Nearest Neighbors</p>
    <p>Bayesian Network [Zander 06] Nogueira 06]</p>
    <p>[Williams 06]</p>
  </div>
  <div class="page">
    <p>Key attributes by CFS</p>
    <p>Protocol srcport dstport Payloaded or not</p>
    <p>Min pkt size</p>
    <p>TCP flags</p>
    <p>Size of n-th pkt</p>
    <p>Keio-I O O O PUSH 2, 8</p>
    <p>Keio-II O O O PUSH 1, 4</p>
    <p>WIDE O O O O SYN, PUSH</p>
    <p>KAIST-I O O O O SYN,RST, PUSH, ECN</p>
    <p>KAIST-II O O O O SYN, PUSH</p>
    <p>PAIX-I O O SYN, ECN 2, 9</p>
    <p>PAIX-II O O O O SYN, CWR</p>
    <p>* CFS : Correlation-based Feature Selection [Williams 06] 7/16</p>
  </div>
  <div class="page">
    <p>Training set size vs. accuracy</p>
    <p>Support Vector Machine achieves over 97.5 and 99%% of accuracy when only 0.1% and 1% of a trace is used to train it, respectively.</p>
  </div>
  <div class="page">
    <p>Accuracy = # of correctly classified flows</p>
    <p>total number of flows in a trace</p>
    <p>Accuracy</p>
    <p>Trans-pacific BackboneHighest</p>
    <p>proportion of P2P flows</p>
    <p>Lowest proportion of P2P flows</p>
    <p>* Only 0.1% of each trace is used to train machine learning algorithms 9/16</p>
  </div>
  <div class="page">
    <p>Per-application performance metrics</p>
    <p>Precision : How precise is an application fingerprint?</p>
    <p>True Positives</p>
    <p>True Positives + False Positives</p>
    <p>Recall : How complete is an application fingerprint? True Positives</p>
    <p>True Positives + False Negatives</p>
    <p>F-Measure : Combination of precision and recall 2 x Precision x Recall</p>
    <p>Precision + Recall 10/16</p>
  </div>
  <div class="page">
    <p>F-Measure of CoralReef</p>
    <p>Most applications (WWW, DNS, Mail, News, NTP, SNMP, Spam Assassin, SSL, Chat, Game, SSH, and Streaming) use their default ports in most cases.</p>
    <p>High precision Low recall Low precision</p>
    <p>High recall</p>
  </div>
  <div class="page">
    <p>F-Measure of BLINC</p>
    <p>Incomplete fingerprints for the behavior of FTP, Streaming, and Game.</p>
    <p>Backbone traces</p>
    <p>&lt; 1% of P2P flows</p>
    <p>Threshold-based mechanism mandates enough behavior information of hosts.</p>
    <p>High precision low recall</p>
    <p>Often misclassifies DNS and Mail flows on backbone traces.</p>
  </div>
  <div class="page">
    <p>F-Measure vs training set size (SVM vs C4.5)</p>
    <p>For all applications, Support Vector Machine requires the smallest # of training sets 13/16</p>
  </div>
  <div class="page">
    <p>Running time of machine learning algos</p>
    <p>Training time Testing time</p>
    <p>WEKA is very slow on large data sets. [Dimov 07] 14/16</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Understand contributions and limitations of each method</p>
    <p>Still discerning attributes for many applications</p>
    <p>Highly depends on the link characteristics</p>
    <p>Parameter tuning is too</p>
    <p>Requires the smallest number of training set</p>
  </div>
  <div class="page">
    <p>Futurework</p>
    <p>7 existing + 3 new traces [DITL 07]  So far, &gt;= 94~96% of accuracy on all of them</p>
    <p>Automatically tuning 28 parameters of BLINC</p>
    <p>For realistic Internet traffic modeling and regeneration 16/16</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>This research was supported in part by IT Scholarship program of Institute for Information Technology Advancement &amp; Ministry of Information and Communication, South Korea.</p>
    <p>http://www.mic.go.kr</p>
    <p>This research was supported in part by the National Science Foundation through TeraGrid resources provided by SDSC.</p>
    <p>http://www.nsf.gov</p>
    <p>This research was supported in part by the San Diego Supercomputer Center under SDS104 and utilized the Datastar system.</p>
    <p>http://www.sdsc.edu</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[Auld 07] Auld et al., Bayesian Neural Networks For Internet Traffic Classification, IEEE Transaction on Neural Networks, 18(1):223-239, January 2007.</p>
    <p>[Bennet 00] Bennet, Support Vector Machines: Hype or Halleujah?, ACM SIGKDD Explorations, 2(2):1-13, October 2000.</p>
    <p>[CoralReef 07] CoralReef. http://www.caida.org/tools/measurement/coralreef [Erman 06] Erman et al., Traffic Classification Using Clustering Algorithms, ACM SIGCOMM Workshop on</p>
    <p>Mining Network Data (MineNet), Pisa, Italy, September 2006. [Dimov 07] Dimov, Weka: Practical machine learning tools and techniques with Java implementations, AI</p>
    <p>Tools Seminar, University of Saarland, April 2007. [DITL 07] Day in the Life of the Internet. http://www.caida.org/projects/ditl [Karagiannis 05] Karagiannis et al., BLINC: Multi-level Traffic Classification in the Dark, ACM SIGCOMM</p>
    <p>Conference on Networking and Services, July 2006. [WEKA 07] WEKA: Data Mining Software in Java. http://www.cs.waikato.ac.nz/ml/weka [Williams 06] Williams et al., A Preliminary Performance Comparison of Five Machine Learning Algorithms for</p>
    <p>Practical IP Traffic Flow Classification, ACM SIGCOMM Computer Communication Review, 36(5):7-15, October 2006.</p>
    <p>[Won 06] Won et al., A Hybrid Approach for Accurate Application Traffic Identification, IEEE/IFIP E2EMON, April 2006.</p>
    <p>[Zander 06] Zander et al., Internet Archeology: Estimating Individual Application Trends in Incomplete Historic Traffic Traces, CAIA Technical Report 060313A, March 2006.</p>
  </div>
  <div class="page">
    <p>Backup slides</p>
  </div>
  <div class="page">
    <p>SVM Design Objective [Bennet 00]</p>
    <p>Find the hyperplane that Maximizes the margin</p>
  </div>
  <div class="page">
    <p>Why maximum margin Hyperplane? [Bennet 00]</p>
    <p>Intuitively, this feels safest</p>
    <p>A hyperplane is really simple</p>
    <p>It is robust to outliers since non-support vectors do not affect the solution at all.</p>
    <p>If weve made a small variation near the boundary this gives us least chance of causing a misclassification.</p>
    <p>There is Structural Risk Minimization theory (using VC D.) that gives the upper bound of generalization error.</p>
    <p>Empirically it works very well</p>
    <p>Optimal Hyperplane</p>
    <p>Non-optimal</p>
  </div>
  <div class="page">
    <p>F-Measure vs training set size (SVM vs Nave Bayes)</p>
  </div>
  <div class="page">
    <p>F-Measure vs training set size (SVM vs Bayesian Net.)</p>
  </div>
  <div class="page">
    <p>F-Measure vs training set size (SVM vs k-Nearest.)</p>
  </div>
</Presentation>
