<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Extracting Commonsense Properties from Embeddings with Limited Human Guidance Property Comparison from Embeddings (PCE model)</p>
    <p>Yiben Yang, Larry Birnbaum, Ji-Ping Wang and Doug Downey July 18, 2018</p>
    <p>Northwestern University</p>
  </div>
  <div class="page">
    <p>Table of contents</p>
  </div>
  <div class="page">
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>Commonsense Property Comparison Task</p>
    <p>Is an elephant bigger or smaller than a mouse? Is Ferrari more expensive or cheaper than beer?</p>
  </div>
  <div class="page">
    <p>Problem Definition</p>
    <p>Three-way task:</p>
    <p>P(L|O1,O2,Property), L  { &lt; , &gt; ,  }.</p>
    <p>Four-way task:</p>
    <p>P(L|O1,O2,Property), L  { &lt; , &gt; ,  , N/A }.</p>
  </div>
  <div class="page">
    <p>Learning Commonsense Knowledge from Text?</p>
    <p>Challenges:</p>
    <p>Reporting bias [Gordon and Van Durme 2013]: Commonsense knowledge is rarely explicitly stated.</p>
    <p>Large knowledge dimensions: Property specified by adjectives: large, heavy, fast, rigid, etc. Creating training examples and building separate models on each type of property requires expensive labeling efforts. Handling unseen properties during the test phase (zero-shot prediction)?</p>
    <p>Language variation: An ideal model should be able to take flexible natural language inputs.</p>
  </div>
  <div class="page">
    <p>Learning Commonsense Knowledge from Text?</p>
    <p>Can we build an efficient commonsense comparison model with word embedding inputs only ?</p>
  </div>
  <div class="page">
    <p>Method</p>
  </div>
  <div class="page">
    <p>Categorical Linear Regressions</p>
    <p>Figure 1: Creating a softmax regression model for each property.</p>
  </div>
  <div class="page">
    <p>Our PCE model</p>
    <p>Figure 2: PCE model</p>
  </div>
  <div class="page">
    <p>Experiment</p>
  </div>
  <div class="page">
    <p>Data</p>
    <p>VERB PHYSICS ( 5 physical properties) [Forbes and Choi 2017]  PROPERTY COMMON SENSE ( 32 commonsense properties)</p>
  </div>
  <div class="page">
    <p>Results: Supervised Performance</p>
    <p>Model Test</p>
    <p>size weight stren rigid speed overall Majority 0.51 0.55 0.52 0.49 0.50 0.51 F&amp;C 0.75 0.76 0.72 0.65 0.61 0.70</p>
    <p>PCE(LSTM) 0.80 0.79 0.76 0.71 0.71 0.76 PCE(GloVe) 0.76 0.75 0.71 0.68 0.68 0.72</p>
    <p>PCE(Word2vec) 0.76 0.76 0.73 0.68 0.66 0.72</p>
    <p>Table 1: Supervised accuracy on the VERB PHYSICS data set. PCE outperforms the F&amp;C model from previous work.</p>
  </div>
  <div class="page">
    <p>Results: Zero-shot Prediction</p>
    <p>Model Test</p>
    <p>size weight stren rigid speed Random 0.33 0.33 0.33 0.33 0.33</p>
    <p>Emb-Similarity 0.37 0.53 0.48 0.43 0.35 PCE 0.74 0.73 0.70 0.62 0.58</p>
    <p>Table 2: Accuracy of zero-shot learning on the VERB PHYSICS data set(using LSTM embeddings).</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Model Test Random 0.25</p>
    <p>Majority Class 0.51 PCE(GloVe) 0.63</p>
    <p>PCE(Word2vec) 0.67 PCE(LSTM) 0.67</p>
    <p>Table 3: Accuracy on the four-way task on the PROPERTY COMMON SENSE data.</p>
  </div>
  <div class="page">
    <p>Synthesis Active Learning</p>
    <p>Want further reduce labeling effort?</p>
    <p>Figure 3: Test accuracy as a function of the number of queried training examples. The synthesis approach performs best.</p>
  </div>
  <div class="page">
    <p>Active Learning</p>
    <p>Figure 4: The uncertainty measure of each queried training example. As training proceeds, the synthesis approach continues to select more uncertain examples.</p>
  </div>
  <div class="page">
    <p>Demo</p>
  </div>
  <div class="page">
    <p>Demo</p>
    <p>http://thor.cs.northwestern.edu:1959/</p>
  </div>
</Presentation>
