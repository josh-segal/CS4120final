<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Color check if this is unreadable, were in trouble if this is unreadable, whatever</p>
    <p>+ +</p>
    <p>+ +</p>
    <p>+ +</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Adversarial Example Defenses:</p>
    <p>Ensembles of Weak Defenses are not Strong</p>
    <p>Warren He James Wei</p>
    <p>Xinyun Chen Nicholas Carlini</p>
    <p>Dawn Song</p>
    <p>UC Berkeley</p>
  </div>
  <div class="page">
    <p>AlphaGo: Winning over World Champion</p>
    <p>Source: David Silver</p>
  </div>
  <div class="page">
    <p>Source: Kaiming He</p>
    <p>Achieving Human-Level Performance on ImageNet Classification</p>
  </div>
  <div class="page">
    <p>Deep Learning Powering Everyday Products</p>
    <p>pcmag.com theverge.com</p>
  </div>
  <div class="page">
    <p>Deep Learning Systems Are Easily Fooled</p>
    <p>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., &amp; Fergus, R. Intriguing properties of neural networks. ICLR 2014.</p>
    <p>ostrich</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Input: a vector of numbers, e.g., image pixels</p>
    <p>input</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Linear combination (matrix multiply) and add bias</p>
    <p>input</p>
    <p>W b</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Nonlinearity, e.g., ReLU(x) = max(0, x)</p>
    <p>input</p>
    <p>W b</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Many layers of these (deep)</p>
    <p>input</p>
    <p>W b</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Background: Neural networks In image classification, softmax function converts output to probabilities</p>
    <p>input</p>
    <p>W b</p>
    <p>... % output</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Overall, a great big function that takes an input x and parameters .</p>
    <p>xinput</p>
    <p>outputF(x, )</p>
  </div>
  <div class="page">
    <p>Background: Neural networks Overall, a great big function that takes an input x and parameters .</p>
    <p>Some training data in x, know what output should be, use gradient descent to figure out best .</p>
    <p>xinput</p>
    <p>outputF(x, )</p>
    <p>gradients</p>
  </div>
  <div class="page">
    <p>Background: Adversarial examples Small change in input, wrong output.</p>
    <p>Smallness referred to as distortion.</p>
    <p>Measured in L2 distance: Euclidean distance if image were a vector of pixel values</p>
  </div>
  <div class="page">
    <p>Background: Adversarial examples State of the art: Vulnerable</p>
    <p>Image classification  Caption generation  Speech recognition  Natural language processing  Policies, reinforcement learning  Self-driving cars</p>
    <p>Stop Sign</p>
    <p>Yield Sign</p>
  </div>
  <div class="page">
    <p>Background: Generating adversarial examples How? Gradients again.</p>
    <p>Differentiate with respect to inputs, rather than parameters Get: how to change each pixel to make output a little more wrong</p>
    <p>xinput</p>
    <p>outputF(x, )</p>
  </div>
  <div class="page">
    <p>Background: Generating adversarial examples We have gradient  We optimize</p>
    <p>Given original input x and correct output y:</p>
    <p>close to original some other input</p>
    <p>where output is wrong</p>
  </div>
  <div class="page">
    <p>Background: Other threat models These were white-box attacks, where attacker knows the model parameters.</p>
    <p>Black-box scenarios have less information available. There are techniques to use white-box attacks in black-box scenarios.</p>
    <p>We focus on white-box attacks in this work.</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Background: Defenses Ensemble</p>
    <p>Normalization</p>
    <p>Distributional detection</p>
    <p>PCA detection</p>
    <p>Secondary classification</p>
    <p>Stochastic</p>
    <p>Generative</p>
    <p>Training process</p>
    <p>Architecture</p>
    <p>Retrain</p>
    <p>Pre-process input</p>
    <p>Detection</p>
    <p>Prevention</p>
  </div>
  <div class="page">
    <p>Background: Defenses We evaluate defenses:</p>
    <p>Can we still algorithmically find adversarial examples?  Do we need higher distortion?</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Data sets MNIST</p>
    <p>CIFAR-10</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing Address two kinds of perturbations</p>
    <p>Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Ensemble defense: Feature squeezing Run prediction on multiple versions of an input image</p>
    <p>Use squeezing algorithms to produce different versions of input</p>
    <p>If predictions differ too much, input is adversarial</p>
    <p>Xu, W., Evans, D., &amp; Qi, Y. (2017). Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155.</p>
    <p>model</p>
    <p>model</p>
    <p>squeeze</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>result</p>
    <p>prediction or adversarial</p>
  </div>
  <div class="page">
    <p>Ensemble defense: Feature squeezing Squeezing an image removes some of its information</p>
    <p>Maps many images to the same image: Ideally maps adversarial examples to something easier to classify</p>
  </div>
  <div class="page">
    <p>Feature squeezing algorithms and attacks Two specific squeezing algorithms</p>
    <p>Color depth reduction  Spatial smoothing</p>
    <p>Effectiveness when used in isolation</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Convert image colors to low bit-depth</p>
    <p>Eliminates small changes on many pixels</p>
    <p>&gt;&lt; ?</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Works well against fast gradient sign method (FGSM)</p>
    <p>Instead of optimizing, do one quick step</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Works well against fast gradient sign method (FGSM)</p>
    <p>Gradient in direction of wrong prediction, as usual</p>
    <p>pixels</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Works well against fast gradient sign method (FGSM)</p>
    <p>Sign of that gradient: only increase or decrease</p>
    <p>pixels</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Works well against fast gradient sign method (FGSM)</p>
    <p>Increase or decrease each pixel by</p>
    <p>pixels</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Not fully differentiable</p>
    <p>model depth</p>
    <p>input result</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Can be attacked using a substitute that excludes the non-differentiable part</p>
    <p>model depth</p>
    <p>input result</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Can be attacked using a substitute that excludes the non-differentiable part</p>
    <p>modelinput result</p>
    <p>gradients</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Can be attacked using a substitute that excludes the non-differentiable part</p>
    <p>modelinput result</p>
    <p>model depth</p>
    <p>input result</p>
    <p>try on complete system</p>
    <p>keep if still misclassified</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Color depth reduction</p>
    <p>Can be attacked using a substitute that excludes the non-differentiable part</p>
    <p>modelinput result</p>
    <p>model depth</p>
    <p>input result</p>
    <p>Attacker uses two versions.</p>
    <p>One differentiable for generating candidates.</p>
    <p>One for testing candidates.</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms</p>
    <p>Color depth reduction: untargeted optimization attack</p>
    <p>Can be attacked using a substitute that excludes the non-differentiable part</p>
    <p>original</p>
    <p>adversarial</p>
    <p>MNIST, reduction to 1 bit CIFAR-10, reduction to 3 bits</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms</p>
    <p>Color depth reduction</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Spatial smoothing</p>
    <p>Median filter: replace each pixel with median around its neighborhood</p>
    <p>Eliminates strong changes on a few pixels</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Spatial smoothing</p>
    <p>Can be attacked directly using existing techniques</p>
    <p>model smooth</p>
    <p>input result</p>
    <p>gradients</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Spatial smoothing: untargeted optimization attack</p>
    <p>Can be attacked directly using existing techniques</p>
    <p>original</p>
    <p>adversarial</p>
    <p>MNIST, 33 smoothing CIFAR-10, 22 smoothing</p>
  </div>
  <div class="page">
    <p>Squeezing algorithms Spatial smoothing</p>
  </div>
  <div class="page">
    <p>Feature squeezing Full defense combines these squeezing algorithms in an ensemble.</p>
    <p>If predictions differ by too much (L1 distance), input is adversarial.</p>
    <p>model</p>
    <p>model depth</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>adv.?</p>
    <p>model smooth</p>
    <p>pred.</p>
  </div>
  <div class="page">
    <p>Feature squeezing: Attack Loss function  Make prediction wrong  Make all predictions have low L1 distance  Stay close to original image</p>
    <p>model</p>
    <p>model depth</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>adv.?</p>
    <p>model smooth</p>
    <p>pred.</p>
  </div>
  <div class="page">
    <p>Feature squeezing: Attack Wrong prediction is fully differentiable</p>
    <p>model</p>
    <p>model depth</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>adv.?</p>
    <p>model smooth</p>
    <p>pred.</p>
    <p>gradients</p>
  </div>
  <div class="page">
    <p>Feature squeezing: Attack L1 distance only gets gradients from two branches.</p>
    <p>Attacker tests candidates on complete system.</p>
    <p>model</p>
    <p>model depth</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>adv.?</p>
    <p>model smooth</p>
    <p>pred.</p>
    <p>approx. gradients</p>
  </div>
  <div class="page">
    <p>Feature squeezing: Attack Ensemble defense</p>
    <p>Can be attacked using gradients from differentiable branches and random initialization</p>
    <p>original</p>
    <p>adversarial</p>
    <p>MNIST, 1-bit, 33 CIFAR-10, 3-bit, 22</p>
  </div>
  <div class="page">
    <p>Feature squeezing: Attack Ensemble defense</p>
  </div>
  <div class="page">
    <p>Feature squeezing Dont have to completely fool the strongest component defense</p>
    <p>model</p>
    <p>model depth</p>
    <p>input compare</p>
    <p>predictions (threshold)</p>
    <p>benign</p>
    <p>model smooth</p>
    <p>pred.</p>
    <p>Be careful about how defenses are put together.</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger</p>
    <p>Feature squeezing (MNIST) +23%</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger Weaker?</p>
    <p>Feature squeezing (MNIST) +23%</p>
    <p>Feature squeezing (CIFAR-10) 36%</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1</p>
    <p>Multiple models, to cover common errors  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Ensemble defense: Specialists+1 Combine specialist classifiers that classify among sets of confusing classes.</p>
    <p>Example: automobiles are more often confused with trucks than with dogs.</p>
    <p>Automobile includes sedans, SUVs, things of that sort. Truck includes only big trucks. Neither includes pickup trucks.</p>
    <p>Abbasi, M., &amp; Gagn, C. (2017). Robustness to Adversarial Examples through an Ensemble of Specialists. ICLR 2017 Workshop Track.</p>
  </div>
  <div class="page">
    <p>Ensemble defense: Specialists+1 Two sets corresponding to each class:</p>
    <p>The most common confused classes (top 80%)  The rest of the classes</p>
    <p>For auto: truck, ship, frog and airplane, auto, bird, cat, deer, dog, horse</p>
    <p>Additionally, a generalist set with all classes</p>
  </div>
  <div class="page">
    <p>Ensemble defense: Specialists+1 For each set, train a classifier to classify between those classes</p>
    <p>If all classifiers that can predict a class do predict that class, then only those classifiers vote; otherwise, all classifiers vote</p>
    <p>Class with most votes is the prediction</p>
    <p>If average confidence among voting classifiers is low, then input is adversarial</p>
    <p>bird</p>
    <p>frog</p>
    <p>cat</p>
    <p>bird</p>
    <p>bird</p>
    <p>cat</p>
    <p>cat</p>
    <p>bird</p>
    <p>bird</p>
    <p>auto</p>
    <p>cat</p>
    <p>cat</p>
    <p>dog</p>
    <p>cat</p>
    <p>cat</p>
    <p>dog</p>
    <p>truck</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
  </div>
  <div class="page">
    <p>Specialists+1: attack Targeted attack: figure out which classifiers would be needed to win with a unanimous vote</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
  </div>
  <div class="page">
    <p>Specialists+1: attack Targeted attack: figure out which classifiers would be needed to win with a unanimous vote</p>
    <p>Optimize loss function made from those classifiers outputs: add up loss functions that we would use for individual ones</p>
    <p>Favor high confidence, not just misclassification</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat</p>
    <p>cat 62</p>
    <p>loss =</p>
  </div>
  <div class="page">
    <p>Specialists+1: attack Targeted optimization attack</p>
    <p>original</p>
    <p>adversarial</p>
    <p>MNIST, high confidence</p>
  </div>
  <div class="page">
    <p>Specialists+1: attack Targeted optimization attack</p>
    <p>Randomly chosen targets, 99% success rate, average confidence higher than average of benign images</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger Weaker?</p>
    <p>Feature squeezing (MNIST) +23%</p>
    <p>Feature squeezing (CIFAR-10) 36%</p>
    <p>Specialists+1 (MNIST) +6%</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Does it matter if defenses are designed to work well together?</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Three unrelated detectors 1. A separate network that distinguishes benign and adversarial images.</p>
    <p>GONG, Z., WANG, W., AND KU, W.-S. Adversarial and clean data are not twins. arXiv preprint arXiv:1704.04960 (2017).</p>
    <p>input conv conv FC FC output</p>
    <p>adv.?...</p>
  </div>
  <div class="page">
    <p>Three unrelated detectors 1. A separate network that distinguishes benign and adversarial images.</p>
    <p>GONG, Z., WANG, W., AND KU, W.-S. Adversarial and clean data are not twins. arXiv preprint arXiv:1704.04960 (2017).</p>
    <p>input conv conv FC FC output</p>
    <p>adv.?...</p>
  </div>
  <div class="page">
    <p>Three unrelated detectors 1. A separate network that distinguishes benign and adversarial images.</p>
    <p>GONG, Z., WANG, W., AND KU, W.-S. Adversarial and clean data are not twins. arXiv preprint arXiv:1704.04960 (2017).</p>
    <p>input conv conv FC FC output</p>
    <p>adv.?...</p>
  </div>
  <div class="page">
    <p>Ensemble: three unrelated detectors If any of the three detect adversarial, system outputs adversarial.</p>
    <p>input conv conv FC FC output</p>
    <p>adv.?</p>
    <p>detector 3</p>
    <p>detector 2</p>
    <p>detector 1</p>
    <p>or</p>
  </div>
  <div class="page">
    <p>Unrelated detectors: attack Fully differentiable system. Again, previous approaches are directly applicable.</p>
    <p>input conv conv FC FC output</p>
    <p>adv.?</p>
    <p>detector 3</p>
    <p>detector 2</p>
    <p>detector 1</p>
    <p>or</p>
    <p>gradients</p>
  </div>
  <div class="page">
    <p>Unrelated detectors: attack 100% success rate, imperceptible perturbations on CIFAR-10</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger Weaker?</p>
    <p>Feature squeezing (MNIST) +23%</p>
    <p>Feature squeezing (CIFAR-10) 36%</p>
    <p>Specialists+1 (MNIST) +6%</p>
    <p>Unrelated detectors (CIFAR-10) +60%</p>
  </div>
  <div class="page">
    <p>Outline Background: neural networks and adversarial examples</p>
    <p>Defenses against adversarial examples</p>
    <p>Ensemble defenses case studies</p>
    <p>Feature squeezing  Specialists+1  Unrelated detectors</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Stronger!</p>
    <p>Not much stronger Weaker?</p>
    <p>Feature squeezing (MNIST) +23%</p>
    <p>Feature squeezing (CIFAR-10) 36%</p>
    <p>Specialists+1 (MNIST) +6%</p>
    <p>Unrelated detectors (CIFAR-10) +60%</p>
  </div>
  <div class="page">
    <p>Are ensemble defenses stronger? Not these ones:</p>
    <p>Ensembles with parts designed to work together  Feature squeezing  Specialists+1</p>
    <p>Unrelated detectors  Gong et al., Metzen et al., and Feinman et al.</p>
    <p>Combining defenses does not guarantee that the ensemble will be a stronger defense.</p>
  </div>
  <div class="page">
    <p>Conclusions Combining defenses does not guarantee that the ensemble will be a stronger defense.</p>
    <p>Lessons:</p>
  </div>
</Presentation>
