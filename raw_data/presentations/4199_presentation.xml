<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence</p>
    <p>Compression</p>
    <p>Christos Baziotis, Ion Androutsopoulos, Ioannis Konstas, Alexandros Potamianos</p>
    <p>Ed nburgh NLPUniversity of Edinburgh Natural Language Processing</p>
    <p>NAACL-HLT 2019, Minneapolis, USA</p>
    <p>Baziotis et al. SEQ3 Autoencoder 1 / 12</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Sentence Compression</p>
    <p>the big black cat</p>
    <p>Machine Translation</p>
    <p>the big black cat</p>
    <p>Text to Tree</p>
    <p>A: What do you want to do tonight?</p>
    <p>B: Lets go for a movie!</p>
    <p>Dialogue</p>
    <p>sort a list of numbers</p>
    <p>Text to Code</p>
    <p>for i in range(len(A)): min_idx = i for j in range(i+1, len(A)):</p>
    <p>if A[min_idx] &gt; A[j]: min_idx = j</p>
    <p>A[i], A[min_idx] = A[min_idx], A[i]</p>
    <p>SEQ3: Sequence-to-Sequence-to-Sequence Autoencoder</p>
    <p>Input Sentence ReconstructionCompression</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 12</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Sentence Compression</p>
    <p>the big black cat</p>
    <p>Machine Translation</p>
    <p>the big black cat</p>
    <p>Text to Tree</p>
    <p>A: What do you want to do tonight?</p>
    <p>B: Lets go for a movie!</p>
    <p>Dialogue</p>
    <p>sort a list of numbers</p>
    <p>Text to Code</p>
    <p>for i in range(len(A)): min_idx = i for j in range(i+1, len(A)):</p>
    <p>if A[min_idx] &gt; A[j]: min_idx = j</p>
    <p>A[i], A[min_idx] = A[min_idx], A[i]</p>
    <p>SEQ3: Sequence-to-Sequence-to-Sequence Autoencoder</p>
    <p>Input Sentence ReconstructionCompression</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 12</p>
  </div>
  <div class="page">
    <p>Unsupervised Models for Language</p>
    <p>Vanilla Autoencoders</p>
    <p>,,, , ,,</p>
    <p>Discrete Latent Variable Autoencoders</p>
    <p>,,, , ,,</p>
    <p>+ Model the discreteness of language</p>
    <p>Sampling is not differentiable  REINFORCE: sample inefficient and unstable</p>
    <p>Baziotis et al. SEQ3 Autoencoder 3 / 12</p>
  </div>
  <div class="page">
    <p>Unsupervised Models for Language</p>
    <p>Vanilla Autoencoders</p>
    <p>,,, , ,,</p>
    <p>Discrete Latent Variable Autoencoders</p>
    <p>,,, , ,,</p>
    <p>+ Model the discreteness of language</p>
    <p>Sampling is not differentiable  REINFORCE: sample inefficient and unstable</p>
    <p>Baziotis et al. SEQ3 Autoencoder 3 / 12</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Model Supervision Abstractive Differentiable Latent</p>
    <p>Miao &amp; Blunsom (2016) semi</p>
    <p>Wang &amp; Lee (2018) weak</p>
    <p>Fevry &amp; Phang (2018) none</p>
    <p>seq3 none</p>
    <p>seq3 Features (+ contributions) + Fully unsupervised and abstractive</p>
    <p>+ Fully differentiable (continuous approximations)</p>
    <p>+ Topic-grounded compressions</p>
    <p>Human-readable compressions via LM prior</p>
    <p>User-defined flexible compression ratio</p>
    <p>SOTA in unsupervised sentence compression</p>
    <p>Baziotis et al. SEQ3 Autoencoder 4 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>Compressor</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence</p>
    <p>LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>Reconstruction Loss</p>
    <p>Minimize input reconstruction error:</p>
    <p>LR(x, x) =  N</p>
    <p>i=1 log pR(xi = xi)</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions</p>
    <p>Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Reconstructor</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions</p>
    <p>Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Reconstructor</p>
    <p>Compressor</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>LM Prior Loss</p>
    <p>Minimize DKL between Compressor and LM:</p>
    <p>LP = 1 M</p>
    <p>M t=1</p>
    <p>DKL(pC(yt|y&lt;t, x)</p>
    <p>pLM(yt|y&lt;t)</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions</p>
    <p>Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Reconstructor</p>
    <p>Compressor</p>
    <p>LM</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>LM Prior Loss</p>
    <p>Minimize DKL between Compressor and LM:</p>
    <p>LP = 1 M</p>
    <p>M t=1</p>
    <p>DKL(pC(yt|y&lt;t, x)  pLM(yt|y&lt;t))</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input</p>
    <p>Length constraints: user-defined shorter length</p>
    <p>1 1</p>
    <p>1  2</p>
    <p>Compressor</p>
    <p>LM</p>
    <p>1  1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>2</p>
    <p>1</p>
    <p>input</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>Topic Loss</p>
    <p>vx: IDF-weighted average of esi</p>
    <p>vy: average of eci</p>
    <p>LT = 1 cos(vx,vy)</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input</p>
    <p>Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>LM</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>input compression</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>Topic Loss</p>
    <p>vx: IDF-weighted average of esi vy: average of eci</p>
    <p>LT = 1 cos(vx,vy)</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input</p>
    <p>Length constraints: user-defined shorter length</p>
    <p>Compressor</p>
    <p>LM</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1</p>
    <p>1  2</p>
    <p>2</p>
    <p>1</p>
    <p>input compression</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>Topic Loss</p>
    <p>vx: IDF-weighted average of esi vy: average of eci</p>
    <p>LT = 1cos(vx,vy)</p>
  </div>
  <div class="page">
    <p>seq3 Overview</p>
    <p>Reconstruction loss: distill input into the latent sequence LM Prior loss: human-readable compressions Topic loss: similar topic as input Length constraints: user-defined shorter length</p>
    <p>input compression</p>
    <p>Compressor</p>
    <p>LM</p>
    <p>1  1</p>
    <p>1</p>
    <p>Compressor</p>
    <p>Reconstructor</p>
    <p>Encoder Decoder</p>
    <p>Encoder Decoder</p>
    <p>1  2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 12</p>
    <p>Length Constraints</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling</p>
    <p>Straight-Through + Gumbel-softmax (Bengio et al.,2013, Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Forward-pass: Discrete embedding (Gumbel-max trick)</p>
    <p>logits</p>
    <p>+</p>
    <p>~ Gumbel</p>
    <p>(( + )/ )</p>
    <p>Backward-pass: Mixture of embeddings (Gumbel-softmax approx.)</p>
    <p>(( + )/ ) Gradient</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 12</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling</p>
    <p>Straight-Through + Gumbel-softmax (Bengio et al.,2013, Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Forward-pass: Discrete embedding (Gumbel-max trick)</p>
    <p>logits</p>
    <p>+</p>
    <p>~ Gumbel</p>
    <p>(( + )/ )</p>
    <p>Backward-pass: Mixture of embeddings (Gumbel-softmax approx.)</p>
    <p>(( + )/ ) Gradient</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 12</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling</p>
    <p>Straight-Through + Gumbel-softmax (Bengio et al.,2013, Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Forward-pass: Discrete embedding (Gumbel-max trick)</p>
    <p>logits</p>
    <p>+</p>
    <p>~ Gumbel</p>
    <p>(( + )/ )</p>
    <p>Backward-pass: Mixture of embeddings (Gumbel-softmax approx.)</p>
    <p>(( + )/ ) Gradient</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 12</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Dataset Training Evaluation</p>
    <p>Gigaword (English) (source sentences)</p>
    <p>DUC-2003 DUC-2004</p>
    <p>Training</p>
    <p>Train LM (LM prior)  Train seq3</p>
    <p>Never exposed to target sentences (compressions)</p>
    <p>Vocabulary: 15K most frequent words in source sentences</p>
    <p>Metrics</p>
    <p>Average F1 of ROUGE-1, ROUGE-2, ROUGE-L</p>
    <p>Baziotis et al. SEQ3 Autoencoder 7 / 12</p>
  </div>
  <div class="page">
    <p>Results on Gigaword</p>
    <p>Supervision Model R-1 R-2 R-L</p>
    <p>Unsupervised</p>
    <p>Lead-8 (Rush et al., 2015) 21.86 7.66 20.45</p>
    <p>Pretrained Generator (Wang &amp; Lee,2018) 21.26 5.60 18.89</p>
    <p>seq3 25.39 8.21 22.68</p>
    <p>Weak Adv. REINFORCE (Wang &amp; Lee,2018) 28.11 9.97 25.41</p>
    <p>Supervised</p>
    <p>ABS (Rush et al.,2015) 29.55 11.32 26.42</p>
    <p>SEASS (Zhou et al., 2017) 36.15 17.54 33.63</p>
    <p>words-lvt5k-1sent (Nallapati et al.,2016) 36.40 17.70 33.71</p>
    <p>Table: Results on (English) Gigaword for sentence compression.</p>
    <p>Table: Results on (English) Gigaword for sentence compression.</p>
    <p>Baziotis et al. SEQ3 Autoencoder 8 / 12</p>
  </div>
  <div class="page">
    <p>Results on Gigaword</p>
    <p>Supervision Model R-1 R-2 R-L</p>
    <p>Unsupervised</p>
    <p>Lead-8 (Rush et al., 2015) 21.86 7.66 20.45</p>
    <p>Pretrained Generator (Wang &amp; Lee,2018) 21.26 5.60 18.89</p>
    <p>seq3 25.39 8.21 22.68</p>
    <p>Weak Adv. REINFORCE (Wang &amp; Lee,2018) 28.11 9.97 25.41</p>
    <p>Supervised</p>
    <p>ABS (Rush et al.,2015) 29.55 11.32 26.42</p>
    <p>SEASS (Zhou et al., 2017) 36.15 17.54 33.63</p>
    <p>words-lvt5k-1sent (Nallapati et al.,2016) 36.40 17.70 33.71</p>
    <p>Table: Results on (English) Gigaword for sentence compression.</p>
    <p>Table: Results on (English) Gigaword for sentence compression.</p>
    <p>Baziotis et al. SEQ3 Autoencoder 8 / 12</p>
  </div>
  <div class="page">
    <p>Ablation</p>
    <p>Model R-1 R-2 R-L</p>
    <p>seq3 (Full) 25.39 8.21 22.68</p>
    <p>seq3 w/o lm 24.48 (-0.91) 6.68 (-1.53) 21.79 (-0.89)</p>
    <p>seq3 w/o topic 3.89 0.10 3.75</p>
    <p>Table: Ablation results on Gigaword.</p>
    <p>Both topic and LM losses work in synergy</p>
    <p>LM prior loss: how words should be included</p>
    <p>Topic loss: what words to include</p>
    <p>Baziotis et al. SEQ3 Autoencoder 9 / 12</p>
  </div>
  <div class="page">
    <p>Model Outputs</p>
    <p>INPUT the central election commission ( cec ) on monday decided that taiwan will hold another election of national assembly members on may # .</p>
    <p>GOLD national &lt;unk&gt; election scheduled for may</p>
    <p>SEQ3 the central election commission ( cec ) announced elections .</p>
    <p>INPUT dave bassett resigned as manager of struggling english premier league side nottingham forest on saturday after they were knocked out of the f.a. cup in the third round , according to local reports on saturday .</p>
    <p>GOLD forest manager bassett quits</p>
    <p>SEQ3 dave bassett resigned as manager of struggling english premier league side UNK forest on knocked round press</p>
    <p>Baziotis et al. SEQ3 Autoencoder 10 / 12</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Conclusions</p>
    <p>Fully differentiable seq2seq2seq (seq3) autoencoder</p>
    <p>SOTA in unsupervised abstractive sentence compression</p>
    <p>Topic loss is essential for convergence</p>
    <p>LM prior improves readability</p>
    <p>Next Step: unsupervised machine translation</p>
    <p>Sentence Compression</p>
    <p>Machine Translation</p>
    <p>the big black cat</p>
    <p>Text to Tree</p>
    <p>A: What do you want to do tonight?</p>
    <p>B: Lets go for a movie!</p>
    <p>Dialogue</p>
    <p>sort a list of numbers</p>
    <p>Text to Code</p>
    <p>for i in range(len(A)): min_idx = i for j in range(i+1, len(A)):</p>
    <p>if A[min_idx] &gt; A[j]: min_idx = j</p>
    <p>A[i], A[min_idx] = A[min_idx], A[i]</p>
    <p>the big black cat</p>
    <p>Baziotis et al. SEQ3 Autoencoder 11 / 12</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Conclusions</p>
    <p>Fully differentiable seq2seq2seq (seq3) autoencoder</p>
    <p>SOTA in unsupervised abstractive sentence compression</p>
    <p>Topic loss is essential for convergence</p>
    <p>LM prior improves readability</p>
    <p>Next Step: unsupervised machine translation</p>
    <p>Sentence Compression</p>
    <p>Machine Translation</p>
    <p>the big black cat</p>
    <p>Text to Tree</p>
    <p>A: What do you want to do tonight?</p>
    <p>B: Lets go for a movie!</p>
    <p>Dialogue</p>
    <p>sort a list of numbers</p>
    <p>Text to Code</p>
    <p>for i in range(len(A)): min_idx = i for j in range(i+1, len(A)):</p>
    <p>if A[min_idx] &gt; A[j]: min_idx = j</p>
    <p>A[i], A[min_idx] = A[min_idx], A[i]</p>
    <p>the big black cat</p>
    <p>Baziotis et al. SEQ3 Autoencoder 11 / 12</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Source code</p>
    <p>https://github.com/cbaziotis/seq3</p>
    <p>Contact me</p>
    <p>R christos.baziotis@gmail.com 7 @cbaziotis</p>
    <p>Baziotis et al. SEQ3 Autoencoder 12 / 12</p>
  </div>
  <div class="page">
    <p>Appendix</p>
    <p>Bonus Slides</p>
    <p>Baziotis et al. SEQ3 Autoencoder 1 / 8</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling (Extended)</p>
    <p>Soft-argmax: Weighted sum of embeddings from peaked softmax (Goyal et al.,2017)</p>
    <p>logits</p>
    <p>( / )</p>
    <p>Gumbel-softmax: Differentiable approximation to sampling (Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Straight-Through: forward-pass: one-hot, backward-pass: soft (Bengio et al.,2013)</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 8</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling (Extended)</p>
    <p>Soft-argmax: Weighted sum of embeddings from peaked softmax (Goyal et al.,2017)</p>
    <p>logits</p>
    <p>( / )</p>
    <p>Gumbel-softmax: Differentiable approximation to sampling (Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Straight-Through: forward-pass: one-hot, backward-pass: soft (Bengio et al.,2013)</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 8</p>
    <p>Gumbel-Softmax</p>
    <p>Gumbel-max trick:</p>
    <p>y  softmax(ai) = argmax(ai + i), i  Gumbel</p>
    <p>Gumbel-softmax relaxation:</p>
    <p>y = softmax(ai + i), i  Gumbel</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling (Extended)</p>
    <p>Soft-argmax: Weighted sum of embeddings from peaked softmax (Goyal et al.,2017)</p>
    <p>logits</p>
    <p>( / )</p>
    <p>Gumbel-softmax: Differentiable approximation to sampling (Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Straight-Through: forward-pass: one-hot, backward-pass: soft (Bengio et al.,2013)</p>
    <p>logits</p>
    <p>+</p>
    <p>~ Gumbel</p>
    <p>(( + )/ )</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 8</p>
  </div>
  <div class="page">
    <p>Differentiable Sampling (Extended)</p>
    <p>Soft-argmax: Weighted sum of embeddings from peaked softmax (Goyal et al.,2017)</p>
    <p>logits</p>
    <p>( / )</p>
    <p>Gumbel-softmax: Differentiable approximation to sampling (Maddison et al.,2017; Jang et al.,2017)</p>
    <p>Straight-Through: forward-pass: one-hot, backward-pass: soft (Bengio et al.,2013)</p>
    <p>logits</p>
    <p>+</p>
    <p>~ Gumbel</p>
    <p>(( + )/ )</p>
    <p>Baziotis et al. SEQ3 Autoencoder 2 / 8</p>
  </div>
  <div class="page">
    <p>Out of Vocabulary (OOV) Words</p>
    <p>We copy OOV words using the approach of Fevry and Phang (2018). Simpler alternative to pointer networks (See et al., 2017).</p>
    <p>OOV Handling Example</p>
    <p>RAW John arrived in Rome yesterday. While in Rome, John had fun. INPUT oov1 arrived in oov2 yesterday. While in oov2, oov1 had fun. OOVs John, Rome</p>
    <p>Baziotis et al. SEQ3 Autoencoder 3 / 8</p>
  </div>
  <div class="page">
    <p>Temperature for Gumbel-Softmax</p>
    <p>Temperature  does not affect the forward pass, but it affects gradients.</p>
    <p>(hct) = 1</p>
    <p>log(1 + exp(w   h</p>
    <p>c t)) + 1</p>
    <p>(hct) = 1</p>
    <p>log(1 + exp(w   h</p>
    <p>c t)) + 0</p>
    <p>8 6 4 2 2 4 6 8</p>
    <p>Figure: Values of 0 bound.</p>
    <p>In our experiments the learned temperature lead to instability. We fix  = 0.5 following (Gu et al., 2018).</p>
    <p>Baziotis et al. SEQ3 Autoencoder 4 / 8</p>
  </div>
  <div class="page">
    <p>Implementation Details</p>
    <p>Hyper-Parameters</p>
    <p>Encoders: 2-layer bidirectional LSTM with size 300</p>
    <p>Decoders: 2-layer unidirectional LSTM with size 300</p>
    <p>Embedding: initialize with 100d GloVe (Pennington et al., 2014)</p>
    <p>Parameter Sharing</p>
    <p>Tied encoders of the compressor and reconstructor.</p>
    <p>Shared embedding layer for all encoders and decoders.</p>
    <p>Tied embedding-output layers of both decoders.</p>
    <p>Baziotis et al. SEQ3 Autoencoder 5 / 8</p>
  </div>
  <div class="page">
    <p>Length Control</p>
    <p>M</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 8</p>
  </div>
  <div class="page">
    <p>Length Control</p>
    <p>()</p>
    <p>M</p>
    <p>x1 x2 xN &lt;BOS&gt;</p>
    <p>yM-1 yM yM+1 yM+2y1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 8</p>
  </div>
  <div class="page">
    <p>Length Control</p>
    <p>()</p>
    <p>M</p>
    <p>x1 x2 xN &lt;BOS&gt;</p>
    <p>yM-1 yM yM+1 yM+2</p>
    <p>y1</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 8</p>
  </div>
  <div class="page">
    <p>Length Control</p>
    <p>()</p>
    <p>M</p>
    <p>x1 x2 xN &lt;BOS&gt;</p>
    <p>yM-1 yM yM+1 yM+2</p>
    <p>vs</p>
    <p>&lt;EOS&gt;</p>
    <p>y1</p>
    <p>vs</p>
    <p>&lt;EOS&gt;</p>
    <p>Baziotis et al. SEQ3 Autoencoder 6 / 8</p>
  </div>
  <div class="page">
    <p>Results on DUC Shared Tasks</p>
    <p>Model R-1 R-2 R-L Topiary (Zajic et al., 2007) 25.12 6.46 20.12 (Woodsend et al., 2010) 22.00 6.00 17.00 abs (Rush et al., 2015) 28.18 8.49 23.81 Prefix 20.91 5.52 18.20 seq3 (Full) 22.13 6.18 19.3</p>
    <p>Table: Results on the DUC-2004</p>
    <p>Model R-1 R-2 R-L abs (Rush et al., 2015) 28.48 8.91 23.97 Prefix 21.3 6.38 18.82 seq3 (Full) 20.90 6.08 18.55</p>
    <p>Table: Results on the DUC-2003</p>
    <p>Baziotis et al. SEQ3 Autoencoder 7 / 8</p>
  </div>
  <div class="page">
    <p>Model Output (Extra)</p>
    <p>INPUT the american sailors who thwarted somali pirates flew home to the u.s. on wednesday but without their captain , who was still aboard a navy destroyer after being rescued from the hijackers .</p>
    <p>GOLD us sailors who thwarted pirate hijackers fly home</p>
    <p>SEQ3 the american sailors who foiled somali pirates flew home after crew hijacked .</p>
    <p>Baziotis et al. SEQ3 Autoencoder 8 / 8</p>
  </div>
</Presentation>
