<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Blizzard: Fast, Cloud-scale Block Storage for Cloud-oblivious</p>
    <p>Applica&lt;ons</p>
    <p>James Mickens, Jeremy Elson, Edmund B. Nigh&lt;ngale, Darren Gehring, Krishna</p>
    <p>Nareddy</p>
    <p>Asim Kadav Vijay Chidambaram</p>
    <p>Bin Fan Osama Khan</p>
  </div>
  <div class="page">
    <p>ITS, UH, THE FUTURE</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>WHAT ARE WE GOING TO DO WITH ALL OF THIS CLOUD DATA?</p>
  </div>
  <div class="page">
    <p>OCEAN</p>
  </div>
  <div class="page">
    <p>My Goal  Take unmodified POSIX/Win32 applica&lt;ons . . .  Run those applica&lt;ons in the cloud . . .  On the same hardware used to run big-data apps . . .  . . . and give them cloud-scale IO performance!</p>
  </div>
  <div class="page">
    <p>My Goal</p>
    <p>Map Redu</p>
    <p>ce</p>
    <p>Throughput &gt; 1000 MB/s  Scale-out architecture using commodity parts</p>
    <p>Transparent failure recovery</p>
    <p>Take unmodified POSIX/Win32 applica&lt;ons . . .  Run those applica&lt;ons in the cloud . . .  On the same hardware used to run big-data apps . . .  . . . and give them cloud-scale IO performance!</p>
  </div>
  <div class="page">
    <p>Why Do I Want To Do This?</p>
    <p>BECAUSE THIS WOULD BE AMAZING</p>
    <p>Write POSIX/Win32 app once, automagically have fast cloud version</p>
    <p>Cloud operators dont have to open up their proprietary or sensi&lt;ve protocols</p>
    <p>Admin/hardware efforts that help big data apps help POSIX/Win32 apps (and vice versa)</p>
  </div>
  <div class="page">
    <p>Our Solu&lt;on: Blizzard</p>
    <p>Blizzard virtual drive</p>
    <p>Remote disks</p>
  </div>
  <div class="page">
    <p>Our Solu&lt;on: Blizzard</p>
    <p>Remote disks</p>
    <p>Blizzard virtual drive</p>
    <p>RAID?</p>
    <p>NAS?</p>
    <p>EBS?</p>
  </div>
  <div class="page">
    <p>THE PROBLEM IS YOU</p>
  </div>
  <div class="page">
    <p>The nave approach for implemen&lt;ng virtual disks does not maximize spindle parallelism for POSIX/Win32 applica&lt;ons which frequently issue fsync() operaLons to maintain consistency.</p>
    <p>LISTEN</p>
  </div>
  <div class="page">
    <p>The nave approach for implemen&lt;ng virtual disks does not maximize spindle parallelism for POSIX/Win32 applica&lt;ons which frequently issue fsync() operaLons to maintain consistency.</p>
    <p>LISTEN</p>
    <p>Excellence</p>
    <p>IOp dila&lt;on: Nested striping</p>
    <p>Rack locality: Locality-oblivious</p>
    <p>storage</p>
    <p>fsync() write barriers: Delayed durability</p>
    <p>seman&lt;cs</p>
  </div>
  <div class="page">
    <p>LISTEN</p>
    <p>Excellence</p>
    <p>IOp dila&lt;on: Nested striping</p>
    <p>Rack locality: Locality-oblivious</p>
    <p>storage</p>
    <p>fsync() write barriers: Delayed durability</p>
    <p>seman&lt;cs</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>X Y Virtual disk</p>
    <p>Remote disks</p>
  </div>
  <div class="page">
    <p>X Y</p>
    <p>Y</p>
    <p>Virtual disk</p>
    <p>Remote disks</p>
    <p>Disk arm</p>
  </div>
  <div class="page">
    <p>Y</p>
    <p>Disk arm</p>
    <p>X Y</p>
  </div>
  <div class="page">
    <p>Y</p>
    <p>Client App fwrite(WX) fwrite(WY)</p>
    <p>Client OS send(WX) send(WY)</p>
    <p>Server OS fwrite(WX)</p>
    <p>fwrite(WY)</p>
    <p>Network WX</p>
    <p>WY</p>
    <p>Ti m e</p>
    <p>(WX) (WY) IO queue X Y</p>
  </div>
  <div class="page">
    <p>Y</p>
    <p>Client App fwrite(WX) fwrite(WY)</p>
    <p>Client OS fwrite(WX)</p>
    <p>fwrite(WY)</p>
    <p>Server OS fwrite(WX)</p>
    <p>fwrite(WY)</p>
    <p>Network fwrite(WX)</p>
    <p>fwrite(WY)</p>
    <p>Ti m e</p>
    <p>(WX) (WY) IO queue</p>
  </div>
  <div class="page">
    <p>Fixing IOp Convoy Dila&lt;on</p>
    <p>Virtual drive</p>
    <p>Remote disks</p>
    <p>Segment size = 4</p>
  </div>
  <div class="page">
    <p>Fixing IOp Convoy Dila&lt;on</p>
    <p>Virtual drive</p>
    <p>Remote disks</p>
    <p>Segment size = 4</p>
  </div>
  <div class="page">
    <p>TOTAL VICTORY</p>
  </div>
  <div class="page">
    <p>Rack Locality</p>
  </div>
  <div class="page">
    <p>Rack 1 Rack 2</p>
    <p>Rack Locality</p>
    <p>Remote disks</p>
    <p>Virtual drive</p>
    <p>Segment size = 4</p>
    <p>Blizzard client</p>
  </div>
  <div class="page">
    <p>AINT NOBODY GOT TIME FOR</p>
    <p>THAT</p>
  </div>
  <div class="page">
    <p>FDS To The Rescue (OSDI 2012)</p>
    <p>Hardware architecture  Full bisec&lt;on bandwidth network (no oversubscrip&lt;on)  Allocate each disk enough network bandwidth to drive disk at full sequen&lt;al speed (1 disk  128 MB/s  1Gbps)</p>
    <p>Result: locality-oblivious storage  Any client can access any disk as fast as local  Enables aggressive striping</p>
  </div>
  <div class="page">
    <p>Blizzard as FDS Client</p>
    <p>Blizzard client handles:  Nested striping  Delayed durability seman&lt;cs</p>
    <p>FDS provides:  Locality-oblivious storage hardware</p>
    <p>Server-side failure recovery</p>
    <p>RTS/CTS to avoid edge conges&lt;on</p>
  </div>
  <div class="page">
    <p>Blizzard as FDS Client</p>
    <p>Blizzard client handles:  Nested striping  Delayed durability seman&lt;cs</p>
    <p>FDS provides:  Locality-oblivious storage hardware</p>
    <p>Server-side failure recovery</p>
    <p>RTS/CTS to avoid edge conges&lt;on</p>
  </div>
  <div class="page">
    <p>POSIX/Win32 apps</p>
    <p>TradiLonal big-data apps</p>
  </div>
  <div class="page">
    <p>ARE WE THERE YET?</p>
  </div>
  <div class="page">
    <p>The problem with fsync()</p>
    <p>Used by POSIX/Win32 file systems and applica&lt;ons to implement crash consistency  Disk only returns from fsync() when all prior writes have become durable</p>
    <p>Ex: ensure data is wriken before metadata</p>
    <p>data fsync() Write</p>
    <p>metadata Write</p>
  </div>
  <div class="page">
    <p>WRITE BARRIERS RUIN</p>
    <p>BIRTHDAYS</p>
    <p>Time</p>
    <p>WA WB F WD WC WE WF</p>
    <p>Stalled operaLons limit parallelism!</p>
  </div>
  <div class="page">
    <p>Delayed Durability  Decouple durability from ordering  Acknowledge fsync() immediately . . .  . . . but increment flush epoch  Tag writes with their epoch number, asynchronously re&lt;re writes in epoch order</p>
  </div>
  <div class="page">
    <p>Delayed Durability  Decouple durability from ordering  Acknowledge fsync() immediately . . .  . . . but increment flush epoch  Tag writes with their epoch number, asynchronously re&lt;re writes in epoch order</p>
    <p>App F1 WY 0 WX 1 WY 1 F2 WY 2</p>
    <p>Blizzard</p>
    <p>Remote disk</p>
    <p>WY 0 WY 0 WX 1 WY 1</p>
    <p>WY 1</p>
  </div>
  <div class="page">
    <p>Delayed Durability  Decouple durability from ordering  Acknowledge fsync() immediately . . .  . . . but tag writes with their epoch number  . . . and asynchronously re&lt;re writes in epoch order</p>
    <p>App F1 WY 0 WX 1 WY 1 F2 WY 2</p>
    <p>Blizzard</p>
    <p>Remote disk</p>
    <p>WY 0 WY 0 WY 1</p>
    <p>All writes are acknowledged . . .  . . . but only and are durable!  Sa&lt;sfies prefix consistency  All epochs up to N-1 are durable  Some, all, or no writes from epoch N are durable  No writes from later epochs are durable</p>
    <p>Prefix consistency good enough for most apps, provides much beker performance!</p>
    <p>WY 0 WY 1</p>
    <p>WX 1 WY 1</p>
  </div>
  <div class="page">
    <p>Isnt Blizzard buffering a lot of data?</p>
    <p>Epoch 0 Epoch 1 Epoch 2 Epoch 3</p>
    <p>Cannot issue!</p>
    <p>In flight . . .</p>
  </div>
  <div class="page">
    <p>THERE ARE NO NEW SYSTEMS TECHNIQUES CACHING?</p>
    <p>SPECULATIVE EXECUTION? LOG-BASED WRITES!</p>
  </div>
  <div class="page">
    <p>Log-based Writes  Treat backing storage as a distributed log  Issue writes to log immediately and in order  On failure, roll forward from last checkpoint and stop when you find torn write, unallocated log block with old epoch number</p>
    <p>W0 W1 W3</p>
    <p>W2 W3 W0 W1</p>
    <p>Remote log</p>
    <p>Write stream</p>
  </div>
  <div class="page">
    <p>Summary of Blizzards Design  Problem: IOp Dila&lt;on  Solu&lt;on: Nested striping</p>
    <p>Problem: Rack locality constrains parallelism</p>
    <p>Solu&lt;on: Full-bisec&lt;on networks, match disk and network bandwidth</p>
    <p>Problem: Evil fsync()s  Solu&lt;on: Delayed durability</p>
    <p>FDS</p>
    <p>Virtual drive</p>
    <p>Remote disks</p>
    <p>W0 W1 W2 W3</p>
    <p>Remote log</p>
    <p>Write stream</p>
    <p>W0 W1 W3</p>
  </div>
  <div class="page">
    <p>PROOF OF YOUR EXCELLENCE</p>
    <p>I DO NOT SEE IT</p>
  </div>
  <div class="page">
    <p>Throughput Microbenchmark</p>
  </div>
  <div class="page">
    <p>Applica&lt;on Macrobenchmarks</p>
  </div>
  <div class="page">
    <p>Delayed Durability: Hiding Replica&lt;on Penal&lt;es</p>
  </div>
  <div class="page">
    <p>Blizzard vs EBS: Write IOps and Read IOps</p>
  </div>
  <div class="page">
    <p>iSCSI, AoE  Petal  EBS, Azure Drive</p>
    <p>Salus</p>
    <p>BlueSky, pNFS</p>
    <p>OptFS  BPFS</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Conclusions  Unmodified POSIX/Win32 apps can have cloud-scale IO!  Nested striping  FDS-style hardware substrate  Delayed durability seman&lt;cs</p>
    <p>Raw perf: 1000+ MB/s  2x10x app-level speedups</p>
  </div>
  <div class="page">
    <p>IOp Latency</p>
  </div>
  <div class="page">
    <p>RTT Sensi&lt;vity</p>
  </div>
  <div class="page">
    <p>Blizzard vs. EBS</p>
  </div>
  <div class="page">
    <p>Recovery Log-based Writes</p>
    <p>W0 W1 W2 W3</p>
    <p>Remote log</p>
    <p>Write stream W1 W2 W3</p>
    <p>Checkpoint Log pos: 0 Write #: 0</p>
    <p>W0</p>
  </div>
  <div class="page">
    <p>Recovery</p>
    <p>W2 W3</p>
    <p>Remote log</p>
    <p>W0 W1 W3</p>
    <p>Checkpoint Log pos: 0 Write #: 0</p>
    <p>W0 Write stream</p>
    <p>W1</p>
    <p>Checkpoint Log pos: 2 Write #: 2</p>
  </div>
  <div class="page">
    <p>Addi&lt;onal Details  Blizzard maps each virtual block to backing physical block in the log  Alloca&lt;on map included in checkpoints</p>
    <p>To avoid IOp dila&lt;on, use random permuta&lt;on to determine next log posi&lt;on!</p>
    <p>Prior example: What Blizzard really does:</p>
    <p>Xn+1 = (aXn + c) mod m</p>
    <p>(Checkpoint a, c, m, Xn)</p>
  </div>
</Presentation>
