<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Self-Organizing Flock of Condors</p>
    <p>Ali Raza Butt Rongmei Zhang Y. Charlie Hu</p>
    <p>{butta,rongmei,ychu}@purdue.edu</p>
  </div>
  <div class="page">
    <p>The need for sharing compute-cycles  Scientific applications</p>
    <p>Complex, large data sets</p>
    <p>Specialized hardware  Expensive</p>
    <p>Modern workstation  Powerful resource  Available in large numbers  Underutilized</p>
    <p>Harness idle-cycles of network of workstations</p>
  </div>
  <div class="page">
    <p>Condor: High throughput computing</p>
    <p>Cost-effective idle-cycle sharing</p>
    <p>Job management facilities  Scheduling, checkpointing, migration</p>
    <p>Resource management  Policy specification/enforcement</p>
    <p>Solves real problems world-wide  1200+ machines Condor pools, 100+ researchers</p>
    <p>@Purdue</p>
  </div>
  <div class="page">
    <p>flocking</p>
    <p>Sharing across pools: Flocking</p>
    <p>Pre-configured resource sharing</p>
    <p>Central manager</p>
  </div>
  <div class="page">
    <p>Flocking</p>
    <p>Static flocking requires  Pre-configuration  Apriori knowledge of all remote pools</p>
    <p>Does not support dynamic resources</p>
  </div>
  <div class="page">
    <p>Our contribution: Peer-to-peer based dynamic flocking</p>
    <p>Automated remote Condor pool discovery</p>
    <p>Dynamic resource management  Support dynamic membership  Support changing local policies</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Background: peer-to-peer networks  Proposed scheme  Implementation  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Overlay Networks</p>
    <p>P2P networks are self-organizing overlay networks without central control</p>
    <p>ISP3</p>
    <p>ISP1 ISP2</p>
    <p>Site 1</p>
    <p>Site 4</p>
    <p>Site 3Site 2 N</p>
    <p>N N</p>
    <p>N</p>
    <p>N</p>
    <p>N N</p>
  </div>
  <div class="page">
    <p>Advantages of structured p2p networks</p>
    <p>Scalable  Self-organization  Fault-tolerant  Locality-aware  Simple to deploy</p>
    <p>Many implementations available  E.g. Pastry, Tapestry, Chord, CAN</p>
  </div>
  <div class="page">
    <p>Pastry: locality-aware p2p substrate</p>
    <p>128-bit circular identifier space  Unique random nodeIds  Message keys</p>
    <p>Routing: A message is routed reliably to a node with nodeId numerically closest to the key</p>
    <p>Routing in overlay &lt; 2 * routing in IP</p>
    <p>Identifier space</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Background: peer-to-peer networks  Proposed scheme  Implementation  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Step 1: P2p organization of Condor pools</p>
    <p>Participating central managers join an overlay  Just need to know a single remote pool</p>
    <p>P2p provides self-organization  Pools can reach each other through the overlay  Pools can join/leave at anytime</p>
  </div>
  <div class="page">
    <p>P2p organized central managers</p>
  </div>
  <div class="page">
    <p>Step 2: Disseminating resource information</p>
    <p>Announcements to nearby pools  Contain pool status information  Leverage locality-aware routing table</p>
    <p>Routing table has O(log N) entries matching increasingly long prefix of local nodeId</p>
    <p>Soft state  Periodically refreshed</p>
  </div>
  <div class="page">
    <p>Resource announcements</p>
    <p>are physically close to</p>
  </div>
  <div class="page">
    <p>Step 3: Enable dynamic flocking</p>
    <p>Central managers flock with nearby pools  Use knowledge gained from resource announcements  Implement local policies  Support dynamic reconfiguration</p>
  </div>
  <div class="page">
    <p>Interactions between central managers</p>
    <p>Locality-aware flocking</p>
  </div>
  <div class="page">
    <p>Matchmaking</p>
    <p>Orthogonal to flocking  Condor matchmaking within a pool</p>
    <p>P2p approach affects the flocking decisions only</p>
  </div>
  <div class="page">
    <p>Are we discovering enough pools?</p>
    <p>Only subset of nearby pools reached using the Pastry routing table</p>
    <p>Multi-hop TTL based announcement forwarding</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Background: peer-to-peer networks  Proposed scheme  Implementation  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Software</p>
    <p>Implemented as a daemon: poolD  Leverages FreePastry 1.3 from Rice  Runs on central managers  Manages self-organized Condor pools</p>
    <p>Condor version 6.4.7</p>
    <p>Interfaced to Condor configuration control</p>
  </div>
  <div class="page">
    <p>Software architecture p2p</p>
    <p>network</p>
    <p>C on</p>
    <p>do r</p>
    <p>p2 p</p>
    <p>ex te</p>
    <p>ns io</p>
    <p>n</p>
    <p>Query services Configuration</p>
    <p>p2p module</p>
    <p>Announcement Manager</p>
    <p>Condor module</p>
    <p>Policy Manager</p>
    <p>Flocking Manager</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Background: peer-to-peer networks  Proposed scheme  Implementation  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Measured results  Effect of flocking on job throughput</p>
    <p>Time spent in queue</p>
    <p>Four pools, three compute machines each  Synthetic job trace</p>
  </div>
  <div class="page">
    <p>Job trace</p>
    <p>Sequence  100 (issue time: T, job length: L) pairs  Interval (TnTn-1), L uniform distribution [1,17]  Designed to keep a single machine busy  Random overload/idle periods</p>
    <p>Trace  One or more job sequences merged together</p>
  </div>
  <div class="page">
    <p>B</p>
    <p>PlanetLab experimental setup</p>
    <p>U.C. Berkeley</p>
    <p>Dynamic flocking</p>
    <p>D C</p>
    <p>A</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Interxion, Germany Rice Columbia</p>
  </div>
  <div class="page">
    <p>Time spent in queue</p>
    <p>maxminmeanmaxminmean</p>
    <p>With flockingWithout flockingNo.of sequences in</p>
    <p>traceh Pool</p>
  </div>
  <div class="page">
    <p>Simulations</p>
    <p>1000 Condor pools</p>
    <p>GT-ITM transit-stub model  50 transit domains  1000 stub domains</p>
    <p>Size of pool: uniform distribution [25,225]</p>
    <p>Number of sequences in trace: uniform distribution [25,225]</p>
  </div>
  <div class="page">
    <p>Cumulative distribution of locality</p>
  </div>
  <div class="page">
    <p>Total job completion time: without flocking</p>
  </div>
  <div class="page">
    <p>Total job completion time: with flocking</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>Background: peer-to-peer networks  Proposed scheme  Implementation  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Conclusions  Design and implementation of a self</p>
    <p>organizing flock of Condors  Scalability  Fault-tolerance  Locality-awareness which yields flocking with</p>
    <p>nearby resources  Local sharing policy enforced</p>
    <p>P2p mechanisms provide an effective substrate for discovery and management of dynamic resources over the wide-area network</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>What about security?  Authenticated pools / users</p>
    <p>Enforced by policy manager  Accountability</p>
    <p>Restricted access  Limited privileges e.g. UNIX user nobody  Condor libraries</p>
    <p>Controlled execution environment  Sandboxing  Process cleanups on job completion</p>
    <p>Intrusion detection</p>
  </div>
</Presentation>
