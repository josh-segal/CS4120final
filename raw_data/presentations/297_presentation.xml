<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Centre for Ultra-Broadband Information Networks THE UNIVERSITY OF MELBOURNE</p>
    <p>Virtualize Everything but Time</p>
    <p>Timothy Broomhead ( t.broomhead@ugrad.unimelb.edu.au ) Laurence Cremean ( l.cremean@ugrad.unimelb.edu.au ) Julien Ridoux ( jrid@unimelb.edu.au ) Darryl Veitch ( dveitch@unimelb.edu.au )</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>! Clock synchronization, who cares?  Network monitoring / Traffic analysis  Telecommunications Industry; Finance; Gaming, ...  Distributed `scheduling: timestamps instead of message passing</p>
    <p>! Status quo under Xen  Based on ntpd, amplifies its flaws  Fails under live VM migration</p>
    <p>! We propose a new architecture  Based on RADclock client synchronization solution  Robust, accurate, scalable  Enables dependent clock paradigm  Seamless migration</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>! Each physical host has a single clock which never migrates</p>
    <p>! Only a (stateless) clock read function migrates</p>
  </div>
  <div class="page">
    <p>! Hypervisor  minimal kernel managing physical resources</p>
    <p>! Para-virtualization  Guest OSs have access to hypervisor via hypercalls  Fully-virtualized more complex, not addressed here</p>
    <p>! Focus on Xen  But approach has general applicability !  Focus on Linux OSs ( 2.6.31.13 Xen pvops branch )  Guest OSs:</p>
    <p>Dom0: privileged access to hardware devices  DomU: access managed by Dom0</p>
    <p>Use Hypervisor 4.0 mainly</p>
    <p>Para-Virtualization and Xen</p>
  </div>
  <div class="page">
    <p>! Clocks built on local hardware (oscillators ! counters)  HPET, ACPI, TSC  Counters imperfect, they drift (temperature driven)  Affected by OS</p>
    <p>ticking rate  access latency</p>
    <p>! TSC (counts CPU cycles)  Highest resolution and lowest latency - preferred! but..  May be unreliable</p>
    <p>multi-core ! multiple unsynchronised TSCs  power management ! variable rate, including stopping !</p>
    <p>! HPET  Reliable, but  Lower resolution, higher latency</p>
    <p>Hardware Counters</p>
  </div>
  <div class="page">
    <p>A hardware/software hybrid timer provided by the hypervisor</p>
    <p>! Purpose  Combine reliability of HPET with low latency of TSC  Compensate for TSC unreliability  Provides 1GHz 64-bit counter</p>
    <p>! Performance of XCS versus HPET  XCS performs well: low latency and high stability  HPET not that far behind, and a lot simpler</p>
    <p>Xen Clocksource</p>
  </div>
  <div class="page">
    <p>! Timekeeping and timestamping are distinct ! Raw timestamps and clock timestamps are distinct ! A scaled counter is not a good clock: drift ! ! Purpose of clock sync algo is to correct for drift ! Network based sync is convenient, exchange timing packets:</p>
    <p>Clock Fundamentals</p>
    <p>! Two key problems  Dealing with delay variability (complex, but possible)  Path asymmetry (simple, but impossible)</p>
    <p>Server</p>
    <p>Host time</p>
    <p>Network</p>
    <p>d d</p>
    <p>r</p>
  </div>
  <div class="page">
    <p>! NTP (ntpd)  Status Quo  Feedback based</p>
    <p>Event timestamps are system clock stamps  Feedback controller (PLL,FLL) tries to lock onto rate</p>
    <p>Intimate relationship with system clock (API, dynamics..)  In Xen, ntpd uses Xen Clocksource</p>
    <p>! RADclock (Robust Absolute and Difference Clock)  Algo developed in 2004, extensively tested  Feedforward based</p>
    <p>Event timestamps are raw stamps  Clock error estimates made and removed when clock read</p>
    <p>`System clock has no dynamics, just a function call  Can use any raw counter: here use HPET, Xen Clocksource</p>
    <p>Synchronisation Algorithms</p>
  </div>
  <div class="page">
    <p>Experimental Methodology</p>
    <p>Unix PC NTP Server Stratum 1</p>
    <p>GPS Receiver</p>
    <p>Hub</p>
    <p>Host</p>
    <p>DAG Card</p>
    <p>PPS Sync. NTP flow UDP flow Timestamping</p>
    <p>SW-GPS</p>
    <p>DAG-GPS</p>
    <p>External MonitorInternal Monitor</p>
    <p>UDP Sender &amp; Receiver</p>
    <p>Atomic Clock</p>
    <p>RADclock</p>
    <p>RADclock</p>
    <p>H yp</p>
    <p>er vi</p>
    <p>so r</p>
    <p>ntpd-NTP</p>
    <p>ntpd-NTP</p>
    <p>D om</p>
    <p>U D</p>
    <p>om 0</p>
  </div>
  <div class="page">
    <p>Wots the problem? ntpd can perform well</p>
    <p>! Ideal Setup  Quality Stratum-1 time-server  Client is on the same LAN, lightly loaded, barely any traffic  Constrained and small polling period: 16 sec</p>
    <p>Time [day]</p>
    <p>C lo</p>
    <p>c k e</p>
    <p>rr o r</p>
    <p>[ s ]</p>
    <p>ntpd</p>
  </div>
  <div class="page">
    <p>Or less well...</p>
    <p>! Different configuration (ntpd recommended!)  Multiple servers  Relax constraint on polling period  Still no load, no traffic, high quality servers</p>
    <p>!1000</p>
    <p>!500</p>
    <p>Hours</p>
    <p>C lo</p>
    <p>c k E</p>
    <p>rr o r</p>
    <p>[ s ]</p>
    <p>ntpd!NTP</p>
    <p>When/Why? Loss of stability a complex function of parameters  unreliable</p>
  </div>
  <div class="page">
    <p>The Xen Context</p>
    <p>! Three examples of inadequacy of ntpd based solution 1) Dependent ntpd clock 2) Independent ntpd clock 3) Migrating independent ntpd clock</p>
  </div>
  <div class="page">
    <p>! The Solution  Only Dom0 runs ntpd  Periodically updates a `boot time variable in hypervisor  DomU uses Xen Clocksource to interpolate</p>
    <p>! The Result (2.6.26 kernel)</p>
    <p>!4000</p>
    <p>!2000</p>
    <p>Time [Hours]</p>
    <p>C lo</p>
    <p>c k e</p>
    <p>rr o r</p>
    <p>[ s ]</p>
    <p>ntpd dependent</p>
  </div>
  <div class="page">
    <p>! The Solution  All guests run entirely separate ntpd daemons  Resource hungry</p>
    <p>! The Result  When all is well, works as before but with a bit more noise  When works: (parallel comparison on Dom0, stratum-1 on LAN)</p>
    <p>Time [day]</p>
    <p>C lo</p>
    <p>c k e</p>
    <p>rr o r</p>
    <p>[ s ]</p>
    <p>ntpd RADclock</p>
  </div>
  <div class="page">
    <p>! The Solution  All guests run entirely separate ntpd daemons  Resource hungry</p>
    <p>! The Result  Increased noise makes instability more likely  When fails: (DomU with some load, variable polling period, guest churn)</p>
    <p>C lo</p>
    <p>c k e</p>
    <p>rr o</p>
    <p>r [</p>
    <p>s ]</p>
    <p>Time [Hours]</p>
    <p>ntpd</p>
  </div>
  <div class="page">
    <p>! The Solution  Independent clock as before, migrates  Starts talking to new system clock, new counter</p>
    <p>! The Result</p>
    <p>Migration Shock!</p>
    <p>More Soon</p>
  </div>
  <div class="page">
    <p>RADclock Architecture</p>
    <p>Principles</p>
    <p>! Timestamping:  raw counter reads, not clock reads  independent of the clock algorithm</p>
    <p>! Synchronization Algorithm:  based on raw timestamps and server timestamps (feedforward)  estimates clock parameters and makes available  concentrated in a single module (in userland)</p>
    <p>! Clock Reading  combines a raw timestamp with retrieved clock parameters  stateless</p>
  </div>
  <div class="page">
    <p>More Concretely</p>
    <p>! Timestamping  read chosen counter, say HPET(t)</p>
    <p>! Sync Algorithm maintains:  Period: a long term average (barely changes)  rate stability  K: sets origin to desired timescale (e.g. UTC)  E: estimate of error  updates on each stamp exchange</p>
    <p>! Clock Reading  Absolute clock: Ca(t) = Period *HPET(t) + K - E(t)</p>
    <p>used for absolute, and differences above critical scale</p>
    <p>Difference clock: Cd(t1,t2) = Period * ( HPET(t2) - HPET(t1) )  used for time differences under some critical time scale</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>! Timestamping `feedforward support  create cumulative and wide (64-bit) form of counter  make accessible from both kernel and user context</p>
    <p>under Linux, modify Clocksource abstraction</p>
    <p>! Sync Algorithm  Make clock parameters available via a user thread</p>
    <p>! Clock reading  Read counter, retrieve clock data, compose  Fixed-point code to enable clock to be read from kernel</p>
  </div>
  <div class="page">
    <p>On Xen!</p>
    <p>! Dependent Clock now very natural  Dom0 maintains a RADclock daemon, talks to timeserver  Makes Period, K, E available through Xenstore filesystem  Each DomU can just reads counter, retrieve clockdata, compose</p>
    <p>! All Guest Clocks identically the same, but:  Small delay (~1ms) in Xenstore update</p>
    <p>stale data possible but very unlikely  small impact</p>
    <p>Latency to read counter higher on DomU</p>
    <p>! Support Needed  Expose HPET to Clocksource in guest OSs  Add hypercall to access platform timer (HPET here)  Add read/right functions to access clockdata from Xenstore</p>
    <p>Feedforward paradigm a perfect match to para-virtualisation</p>
  </div>
  <div class="page">
    <p>Independent RADclock on Xen</p>
    <p>Concurrent test on two DomUs, separate NTP streams</p>
    <p>!10 0 10 0</p>
    <p>x 10 !3</p>
    <p>RADclock error [s]</p>
    <p>HPET</p>
    <p>Med: !2.5</p>
    <p>IQR: 9.3</p>
    <p>!10</p>
    <p>R A</p>
    <p>D c lo</p>
    <p>c k E</p>
    <p>rr o</p>
    <p>r [</p>
    <p>s ]</p>
    <p>Time [mn]</p>
    <p>Xen Clocksource HPET</p>
    <p>!10 0 10 0</p>
    <p>x 10 !3</p>
    <p>RADclock error [s]</p>
    <p>XEN</p>
    <p>Med: 3.4</p>
    <p>IQR: 9.5</p>
  </div>
  <div class="page">
    <p>Migration On Xen!</p>
    <p>! Clocks dont migrate, only a clock reading function does!  Each Dom0 has its own RADclock daemon  DomU only ever calls a function, no state is migrated</p>
    <p>! Caveats  Local copy of clockdata used to limit syscalls - needs refreshing  Host asymmetry will change, result in small clock jump</p>
    <p>asymmetry effects different for Dom0 (hence clock itself) and DomU</p>
    <p>Feedforward paradigm a perfect match to migration</p>
  </div>
  <div class="page">
    <p>Migration Comparison!</p>
    <p>Time [Hours]</p>
    <p>C lo</p>
    <p>c k e</p>
    <p>rr o r</p>
    <p>[ s ]</p>
    <p>Dom0 ! Tastiger Dom0 ! Kultarr Migrated Guest RADclock Migrated Guest ntpd</p>
    <p>! Setup  Two machines, each Dom0 running a RADclock  One DomU migrates with a</p>
    <p>dependent RADclock  independent ntpd</p>
  </div>
  <div class="page">
    <p>Noise Overhead of Xen and Guests</p>
    <p>Native Dom0 1 guest 2 guests 3 guests 4 guests</p>
    <p>R T</p>
    <p>T H</p>
    <p>o s t [</p>
    <p>s ]</p>
    <p>R T</p>
    <p>T H</p>
    <p>o s t [</p>
    <p>s ]</p>
    <p>DomU #1 DomU #2 DomU #3 DomU #4</p>
  </div>
  <div class="page">
    <p>Noise Penalty Under C-States</p>
    <p>C0 C1 C2 C3</p>
    <p>T T</p>
    <p>H o s t [</p>
    <p>s ]</p>
    <p>Xen Clocksource HPET Hypervisor</p>
  </div>
  <div class="page">
    <p>Algo Performance Under C-States</p>
    <p>C0 C1 C2 C3</p>
    <p>!20</p>
    <p>!15</p>
    <p>!10</p>
    <p>!5</p>
    <p>R A</p>
    <p>D c lo</p>
    <p>c k E</p>
    <p>rr o</p>
    <p>r: E</p>
    <p>! m</p>
    <p>e d</p>
    <p>ia n (E</p>
    <p>) [</p>
    <p>s ]</p>
    <p>RADclock Xen RADclock HPET</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>! Feed-Forward approach has many advantages  Difference clock defined  Absolute clock can be made much more robust  Time can be replayed  Simpler kernel support</p>
    <p>! Good match to needs of para-virtualisation  Enables clock dependent mode that works  Allows seamless live migration</p>
    <p>! RADclock project  Aims to replace ntpd  Client and Server code  Packages for FreeBSD and Linux (Xen now supported)  http://www.cubinlab.ee.unimelb.edu.au/radclock/</p>
  </div>
</Presentation>
