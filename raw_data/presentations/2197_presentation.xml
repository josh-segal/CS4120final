<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>IO Priority: To The Device and Beyond Adam Manzanares Filip Blagojevic Cyril Guyot</p>
  </div>
  <div class="page">
    <p>The Relationship of Throughput and Latency</p>
    <p>Increasing QD IO Ps In cr ea se W</p>
    <p>ith Q D</p>
    <p>La te nc y In cr ea se s W ith</p>
    <p>Q D</p>
    <p>Increasing IOPs is positive Increase in Latency is not desirable</p>
    <p>Request 1</p>
    <p>Request 2</p>
    <p>Request 3</p>
    <p>Request 4</p>
    <p>Drive Queue</p>
    <p>Queue Depth - # of outstanding requests IOPs - requests served per second Latency  time to service one request p99.99  99.99 per cent of request latencies are lower than this value</p>
    <p>BARS represent IOPs Line represents tail latency</p>
    <p>From Host To Media</p>
    <p>Environment  Lots of HDDs Response Time of a given HDD is critical</p>
    <p>HDD is serial!</p>
    <p>GOOD! BAD!</p>
  </div>
  <div class="page">
    <p>Where To Handle Priority In The IO Stack</p>
    <p>Application</p>
    <p>VFS</p>
    <p>Block Layer</p>
    <p>Device Drivers</p>
    <p>Device</p>
    <p>Application</p>
    <p>VFS</p>
    <p>Block Layer</p>
    <p>Device Drivers</p>
    <p>Device</p>
    <p>Block Queue/s</p>
    <p>Device Queue/s</p>
    <p>Request 1</p>
    <p>Request 2</p>
    <p>Request 3</p>
    <p>Request 4</p>
    <p>Block Queue</p>
    <p>From Application</p>
    <p>To Device</p>
    <p>Request 1</p>
    <p>Request 2</p>
    <p>Request 3</p>
    <p>Request 4</p>
    <p>Device Queue</p>
    <p>From Host</p>
    <p>To Media</p>
    <p>Application to device IO traverses many layers in the OS.</p>
    <p>Priority may be handled in an OS layer, but requests may be reordered in the device queue</p>
    <p>Device queue priorities are a tuning knob for device queue request de-staging.</p>
    <p>Latency should improve, but not as flexible as host level solutions.</p>
    <p>OS = Linux For this work</p>
  </div>
  <div class="page">
    <p>What We Did</p>
    <p>Application</p>
    <p>VFS</p>
    <p>Block Layer</p>
    <p>Device Drivers</p>
    <p>Device</p>
    <p>Request 1</p>
    <p>Request 2</p>
    <p>Request 3</p>
    <p>Request 4</p>
    <p>Block Queue</p>
    <p>From Application</p>
    <p>To Device</p>
    <p>Request 1</p>
    <p>Request 2</p>
    <p>Request 3</p>
    <p>Request 4</p>
    <p>Device Queue</p>
    <p>From Host</p>
    <p>To Media</p>
    <p>Ensure application to device IO preserves priority throughout the layers in the OS IO stack</p>
    <p>Leveraged existing application priority interface. Mapped this interface down to device through block and device driver layers.</p>
    <p>Goal  prioritized application requests are de-staged from device queue faster than non-prioritized IO improving application tail latency.</p>
    <p>Enables priority to be respected at block layer, device layer, or a combination of the two.</p>
  </div>
  <div class="page">
    <p>Benchmark Overview</p>
    <p>FG 1</p>
    <p>FG 2</p>
    <p>FG 3</p>
    <p>FG 4</p>
    <p>Foreground Application Queue</p>
    <p>To Block Queue</p>
    <p>From Block Queue To</p>
    <p>Media</p>
    <p>BG 1</p>
    <p>BG 2</p>
    <p>BG 3</p>
    <p>BG 4</p>
    <p>Background Application Queue</p>
    <p>To Block Queue</p>
    <p>FG 1</p>
    <p>BG 1</p>
    <p>FG 2</p>
    <p>FG 3</p>
    <p>BG 2</p>
    <p>FG 4</p>
    <p>BG 3</p>
    <p>BG 4</p>
    <p>Operating System Block Queue</p>
    <p>FG 1</p>
    <p>FG 2</p>
    <p>FG 3</p>
    <p>FG 4</p>
    <p>BG 1</p>
    <p>BG 2</p>
    <p>BG 3</p>
    <p>BG 4</p>
    <p>Storage Device Queue</p>
    <p>QD = 32</p>
    <p>QD = {1-32}</p>
    <p>All IOs are small (4KiB) random read requests. This approximates infrequently accessed non cacheable requests. This is typical in large scale storage systems where HDDs are used for long term high capacity storage.</p>
  </div>
  <div class="page">
    <p>Benchmark Details</p>
    <p>Background IO  FIO Random Read  4KiB Block Size  Run time = 5m  Queue Depth = 32  ioengine = libaio</p>
    <p>Foreground IO  FIO Random Read  4KiB Block Size  Run time = 5m  ioengine = libaio  Queue Depth = {1,4,16,32}</p>
    <p>Schedulers used  Deadline with/without drive priority  NOOP with/without drive priority  CFQ with/without priority &amp; CFQ with priority and drive</p>
    <p>priority</p>
    <p>Benchmark approximates small random read requests. Large-scale-enterprise customers indicate that tail latency is a key metric. HDD efficient with large requests. Write requests typically buffered in large scale systems. Lots of data spread over many HDDs, most of the data not hot. Cold data must be retrieved in a bounded, predictable manner.</p>
    <p>In the experiments we fix the background at a QD of 32, which fills the drive queue. Foreground is then increased from QD 1-32 while maintaining the background workload of QD 32. We compare the behavior of the application before and after our changes that plumb IO priority to the device.</p>
    <p>FIO is the Flexible IO tester, which we use to approximate the user application. We also vary the Linux Block Scheduler that is used in the experiment. For each of the experiments we have results when using drive level priorities and also block level scheduler priorities when they are respected at their respective layers.</p>
  </div>
  <div class="page">
    <p>Deadline Scheduler Latency Results</p>
    <p>Two experiments. Foreground and Background IO with no prioritization. Foreground IO prioritized to the device with Background IO.</p>
    <p>Drive prioritization dramatically improves latency with a minimal impact to IOPs</p>
  </div>
  <div class="page">
    <p>Deadline Scheduler IOPs Results</p>
    <p>Two experiments. Foreground and Background IO with no prioritization. Foreground IO prioritized to the device with Background IO.</p>
    <p>Drive prioritization dramatically improves latency with a minimal impact to IOPs</p>
  </div>
  <div class="page">
    <p>CFQ Scheduler Latency Results Two experiments Foreground IO with host prioritization and Background IO Foreground IO with host and device prioritization and Background IO</p>
  </div>
  <div class="page">
    <p>CFQ Scheduler IOPs Results Two experiments Foreground IO with host prioritization and Background IO Foreground IO with host and device prioritization and Background IO</p>
  </div>
  <div class="page">
    <p>Conclusion and Future Directions</p>
    <p>Priority at the device matters  Extra knob to influence tail latency  Must be passed through many layers  Mechanism now exists to test device behavior</p>
    <p>Host Queue and Drive Queue  Interaction between the two can cause unexpected behaviors  Working with host scheduler developers</p>
    <p>Work is currently HDD focused  HDD is inherently serial  How does SSD impact this?</p>
    <p>Work is large-scale enterprise focused  What to do with consumer and mobile?</p>
  </div>
  <div class="page">
    <p>Thanks for your attention</p>
    <p>Questions ??? Contact me at : adam.manzanares@wdc.com</p>
  </div>
</Presentation>
