<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>MLB: A Memory-aware Load</p>
    <p>Balancing Method for Mitigating</p>
    <p>Memory Contention</p>
    <p>Dongyou Seo, Hyeonsang Eom and Heon Y. Yeom</p>
    <p>Department of Computer Science and Engineering, Seoul</p>
    <p>National University, Korea</p>
    <p>Broomfiled, Co, USA</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Multicore CPU</p>
    <p>Multicore has becomes the general CPU architecture</p>
    <p>Intel has recently introduced a Xeon E7-8890 v2 on which 15 physical cores are integrated</p>
    <p>1,000 core processor will be possible, be said Intel http://www.neoseeker.com/news/15313-1000-coreprocessor-possible-says-intel/</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Symmetric Multi-Processing (SMP) may be the basic architecture to support parallel computing with shared resources such as the memory controller and last level cache</p>
    <p>Shared resources may be efficiently used for the fastest on-chip communication</p>
    <p>Trend in the CPU architecture</p>
    <p>BUS</p>
    <p>core0 coreN</p>
    <p>Memory Controller</p>
    <p>Last Level Cache(LLC)</p>
    <p>DIMM0 DIMM1 DIMMN</p>
    <p>core1 core2</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>When there are too many requests for the shared resources,</p>
    <p>the contention for memory subsystems occurs</p>
    <p>The more lbms are executed at the same time, the longer</p>
    <p>becomes the runtime of each lbm and the lower gets the</p>
    <p>per-core memory bandwidth</p>
    <p>Performance bottleneck on multicore CPU</p>
    <p>lbm_1 lbm_2 lbm_3 lbm_4 lbm_5 lbm_6 lbm_7 lbm_8</p>
    <p>a v</p>
    <p>g _</p>
    <p>r u</p>
    <p>n ti</p>
    <p>m e (s</p>
    <p>e c )</p>
    <p>p e r _</p>
    <p>c o</p>
    <p>r e _</p>
    <p>m e m</p>
    <p>o r y</p>
    <p>_ b</p>
    <p>a n</p>
    <p>d w</p>
    <p>id</p>
    <p>(b y</p>
    <p>te s/</p>
    <p>se c )</p>
    <p>lbm (SPEC CPU 2006) on an 8cores E5-2690</p>
    <p>per_core_memory_bandwidth(byte/sec) runtime(sec)</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Related work</p>
    <p>Various solutions have been developed to mitigate contention for memory subsystems  Zhuravlev et al. Addressing Shared Resource Contention in Multicore Processors via</p>
    <p>Scheduling. In ASPLOS, 2010.</p>
    <p>Xie et al. Dynamic Classification of Program Memory Behaviors in CMPs. In Proc. of CMP-MSI, held in conjunction with ISCA, 2009.</p>
    <p>Jiang et al. Analysis and Approximation of Optimal Co-Scheduling on Chip Multiprocessors. In PACT, 2008.</p>
    <p>Kim et al. Virtual Machine Consolidation Based on Interference Modeling. In the journal of Supercomputing, April 2013.</p>
    <p>And so on</p>
    <p>However, most of the methodologies have been developed and used on the assumption that a task is the sole owner of a CPU core and thus they are not applicable in the multitasking case except for VBSC [Merkel et al. EuroSys10]</p>
    <p>We will show the results of comparing our method with VBSC</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Cluster imbalance</p>
    <p>Figure. VMware cluster imbalance</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Memory-aware Load Balancing(MLB)</p>
    <p>Core 0 Core 1 Core 2 Core 3</p>
    <p>lbm</p>
    <p>soplex gobmk</p>
    <p>namd</p>
    <p>libquantum</p>
    <p>GemsFDTD gamess</p>
    <p>sjeng</p>
    <p>Memory-aware Load Balancing (MLB) is a solution dealing with the</p>
    <p>multitasking case by mitigating memory contention</p>
    <p>MLB migrates memory intensive tasks in several cores and compels</p>
    <p>the only designated cores to generate high memory traffics, not all</p>
    <p>cores</p>
    <p>MLB can dynamically increase and decrease the number of the</p>
    <p>cores by complying with our memory contention model</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Contributions</p>
    <p>Presenting simple models indicating the level of memory</p>
    <p>contention and predicting the maximum number of the</p>
    <p>memory intensive tasks spreading on multiple cores for</p>
    <p>dynamic MLB operation</p>
    <p>Evaluating MLB on both server-level and desktop-level</p>
    <p>CPUs</p>
    <p>Comparing MLB with the state of the art method considering</p>
    <p>the multitasking case, Vector Balancing &amp; Sorted Co</p>
    <p>scheduling (VBSC)</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Contents</p>
    <p>Memory Contention Modeling</p>
    <p>Design and Implementation of MLB</p>
    <p>Evaluation</p>
    <p>Comparison with VBSC</p>
    <p>Extension of MLB for multi-threaded applications</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Memory Contention Modeling</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Memory Contention Modeling</p>
    <p>r e ti</p>
    <p>r e d</p>
    <p>_ m</p>
    <p>e m</p>
    <p>o r y</p>
    <p>_ tr</p>
    <p>a ff</p>
    <p>ic</p>
    <p>r a</p>
    <p>te (e</p>
    <p>v e n</p>
    <p>ts /s</p>
    <p>e c )</p>
    <p>m e m</p>
    <p>o r y</p>
    <p>_ r e q</p>
    <p>u e st</p>
    <p>_ b</p>
    <p>u ff</p>
    <p>e r _</p>
    <p>fu ll</p>
    <p>r a</p>
    <p>te (e</p>
    <p>v e n</p>
    <p>ts /s</p>
    <p>e c )</p>
    <p>On an 8cores E5-2690</p>
    <p>memory_request_buffer_full rate (events/sec)</p>
    <p>retired_memory_traffic rate (events/sec)</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Memory Contention Modeling (Contd)</p>
    <p>solo +1 +2</p>
    <p>+3 +4</p>
    <p>+5</p>
    <p>+6</p>
    <p>+7</p>
    <p>r e d</p>
    <p>ic te</p>
    <p>d _</p>
    <p>d e g</p>
    <p>r a</p>
    <p>d a</p>
    <p>ti o</p>
    <p>n</p>
    <p>n o r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth (#bytes/sec)</p>
    <p>lbm</p>
    <p>normalized_runtime predicted_degradation</p>
    <p>solo +1 +2</p>
    <p>+3 +4</p>
    <p>+5 +6</p>
    <p>+7</p>
    <p>p r e d</p>
    <p>ic te</p>
    <p>d _</p>
    <p>d e g</p>
    <p>r a</p>
    <p>d a</p>
    <p>ti o</p>
    <p>n</p>
    <p>n o r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth (#bytes/sec)</p>
    <p>GemsFDTD</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Memory Contention Modeling (Contd)</p>
    <p>Correlation between memory bandwidth, memory contention</p>
    <p>level and performance</p>
    <p>solo +1 +2 +3 +4</p>
    <p>+5 +6 +7</p>
    <p>ic te</p>
    <p>d _</p>
    <p>d e g</p>
    <p>r a</p>
    <p>d a</p>
    <p>ti o</p>
    <p>n</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth (#bytes/sec)</p>
    <p>soplex</p>
    <p>solo</p>
    <p>+1 +2 +3</p>
    <p>+4</p>
    <p>+5</p>
    <p>+6</p>
    <p>+7</p>
    <p>ic te</p>
    <p>d _</p>
    <p>d e g</p>
    <p>r a</p>
    <p>d a</p>
    <p>ti o</p>
    <p>n</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth(byte/sec)</p>
    <p>omnetpp</p>
    <p>solo</p>
    <p>+1 +2 +3 +4</p>
    <p>+5 +6 +7</p>
    <p>p r e d</p>
    <p>ic te</p>
    <p>d _</p>
    <p>d e g r a d</p>
    <p>a ti</p>
    <p>o n</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth (#bytes/sec)</p>
    <p>tonto</p>
    <p>solo +1 +2 +3</p>
    <p>+4 +5 +6</p>
    <p>+7</p>
    <p>p r e d</p>
    <p>ic te</p>
    <p>d _ d</p>
    <p>e g r a d</p>
    <p>a ti</p>
    <p>o n</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ r u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>memory_bandwidth(byte/sec)</p>
    <p>namd</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Design and Implementation of MLB</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Design and Implementation of MLB</p>
    <p>Intensive runqueue and non intensive runqueue lists</p>
    <p>MLB mechanism is implemented in Linux kernel 3.2.0</p>
    <p>Memory intensive tasks have the higher intensity than the average of all tasks</p>
    <p>Memory intensive tasks should be executed on the cores whose runqueues are intensive ones</p>
    <p>Non memory intensive tasks should be executed on the cores whose runqueues are non intensive one</p>
    <p>RQ 0 RQ 1 RQ 2 RQ 3</p>
    <p>lbm</p>
    <p>sjeng namd</p>
    <p>GemsFDTD</p>
    <p>libquantum</p>
    <p>gobmk gamess</p>
    <p>soplex</p>
    <p>Intensive_rq</p>
    <p>Nonintensive RQ</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Design and Implementation of MLB (Contd)</p>
    <p>Xeon E5-2690</p>
    <p>Max_mb=51.2</p>
    <p>GB/sec</p>
    <p>Xeon E5-2670</p>
    <p>Max_mb=51.2</p>
    <p>GB/sec</p>
    <p>i7-2600</p>
    <p>Max_mb=21</p>
    <p>GB/sec Xeon E5606</p>
    <p>Max_mb=25.6</p>
    <p>GB/sec</p>
    <p>m e m</p>
    <p>o r y</p>
    <p>_ c o</p>
    <p>n te</p>
    <p>n ti</p>
    <p>o n</p>
    <p>_ le</p>
    <p>v e l</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Design and Implementation of MLB</p>
    <p>(Contd)  Dynamic MLB algorithm</p>
    <p>Src_rq is a non intensive runqueue which includes the highest memory intensive task in non intensive runqueue list</p>
    <p>Dest_rq is an intensive runqueue which has the highest intensity sum for all tasks in a non intensive runqueue selected to minimize the number of task swapping</p>
    <p>RQ 0 RQ 1 RQ 2 RQ 3</p>
    <p>lbm</p>
    <p>libqua soplex</p>
    <p>Gems</p>
    <p>FDTD</p>
    <p>sjeng</p>
    <p>gobmk gamess</p>
    <p>gobmk</p>
    <p>Intensive_rq Nonintensive_rq</p>
    <p>Predicted # = 2 Intensive_rq # = 0</p>
    <p>RQ 0 RQ 1 RQ 2 RQ 3</p>
    <p>lbm</p>
    <p>libqua soplex</p>
    <p>Gems</p>
    <p>FDTD</p>
    <p>sjeng</p>
    <p>gobmk gamess</p>
    <p>gobmk</p>
    <p>Highest_rq Src_rq</p>
    <p>Task swapping</p>
    <p>Mem_task Nonmem_task</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Design and Implementation of MLB (Contd )</p>
    <p>Dynamic MLB algorithm</p>
    <p>Until only non memory intensive tasks reside in the source</p>
    <p>rqs, MLB continues to conduct task swapping</p>
    <p>If needed, a non target rq is inserted to the target rq list</p>
    <p>Predicted # = 2 Intensive_rq # = 1</p>
    <p>Predicted # = 2 Target_rq # = 2</p>
    <p>RQ 0 RQ 1 RQ 2 RQ 3</p>
    <p>lbm</p>
    <p>sjeng soplex</p>
    <p>Gems</p>
    <p>FDTD</p>
    <p>libqua</p>
    <p>gobmk gamess</p>
    <p>gobmk</p>
    <p>Highest_rq Src_rq</p>
    <p>Task swapping</p>
    <p>RQ 0 RQ 1 RQ 2 RQ 3</p>
    <p>lbm</p>
    <p>sjeng gobmk</p>
    <p>Gems</p>
    <p>FDTD</p>
    <p>libqua</p>
    <p>gobmk gamess</p>
    <p>soplex</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>p r e d</p>
    <p>ic te</p>
    <p>d _ n</p>
    <p>u m</p>
    <p>b e r</p>
    <p>timeline(sec)</p>
    <p>Evaluation (Model applicability)</p>
    <p>Some sections are overestimated, but there are unnecessary task migrations due to prevention mechanism</p>
    <p>Src_rq-&gt;highest_task-&gt;intensity &lt;= intensive_rq_list-&gt;lowest_task-&gt;intensity</p>
    <p>overestimated</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Evaluation (System performance)</p>
    <p>lb m</p>
    <p>_ 0</p>
    <p>lb m</p>
    <p>_ 1</p>
    <p>li b</p>
    <p>q u</p>
    <p>a n</p>
    <p>tu m</p>
    <p>_ 0</p>
    <p>li b</p>
    <p>q u</p>
    <p>a n</p>
    <p>tu m</p>
    <p>_ 1</p>
    <p>G e m</p>
    <p>sF D</p>
    <p>T D</p>
    <p>_ 0</p>
    <p>G e m</p>
    <p>sF D</p>
    <p>T D</p>
    <p>_ 1</p>
    <p>so p</p>
    <p>le x</p>
    <p>_ 0</p>
    <p>so p</p>
    <p>le x</p>
    <p>_ 1</p>
    <p>n a</p>
    <p>m d</p>
    <p>_ 0</p>
    <p>n a</p>
    <p>m d</p>
    <p>_ 1</p>
    <p>sj e n</p>
    <p>g _</p>
    <p>sj e n</p>
    <p>g _</p>
    <p>g a</p>
    <p>m e ss</p>
    <p>_ 0</p>
    <p>g a</p>
    <p>m e ss</p>
    <p>_ 1</p>
    <p>g o</p>
    <p>b m</p>
    <p>k _</p>
    <p>g o</p>
    <p>b m</p>
    <p>k _</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>t o</p>
    <p>n a</p>
    <p>iv e Workload mix1 on an E5-2690</p>
    <p>(16tasks on 8cores)</p>
    <p>lb m</p>
    <p>_ 0</p>
    <p>lb m</p>
    <p>_ 1</p>
    <p>li b</p>
    <p>q u</p>
    <p>a n</p>
    <p>tu m</p>
    <p>_ 0</p>
    <p>li b</p>
    <p>q u</p>
    <p>a n</p>
    <p>tu m</p>
    <p>_ 1</p>
    <p>to n</p>
    <p>to _</p>
    <p>to n</p>
    <p>to _</p>
    <p>z e u</p>
    <p>sm p</p>
    <p>_ 0</p>
    <p>z e u</p>
    <p>m sp</p>
    <p>_ 1</p>
    <p>n a</p>
    <p>m d</p>
    <p>_ 0</p>
    <p>n a</p>
    <p>m d</p>
    <p>_ 1</p>
    <p>sj e n</p>
    <p>g _ 0</p>
    <p>sj e n</p>
    <p>g _ 1</p>
    <p>g a m</p>
    <p>e ss</p>
    <p>_ 0</p>
    <p>g a m</p>
    <p>e ss</p>
    <p>_ 1</p>
    <p>g o b</p>
    <p>m k</p>
    <p>_ 0</p>
    <p>g o b</p>
    <p>m k</p>
    <p>_ 1</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>t o</p>
    <p>n a</p>
    <p>iv e Workload mix 2 on an E5-2690</p>
    <p># of workload mixes Workloads (SPEC CPU 2006)</p>
    <p>Workload-mix1 (E5-2690)</p>
    <p>Workload-mix2 (E5-2690)</p>
    <p>Mix 1</p>
    <p>on Xeon</p>
    <p>E5-2690</p>
    <p>Mix 2</p>
    <p>on Xeon</p>
    <p>E5-2690</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ to</p>
    <p>_ n</p>
    <p>a iv</p>
    <p>a</p>
    <p>memory_bandwidth(byte/sec) memory request</p>
    <p>buffer full (events/sec)</p>
    <p>LLC_miss (events/sec) LLC_reference(events/sec)</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Evaluation (Additional benefit)</p>
    <p>(Workload mix 1  a soplex) + a streamcluster (Rodinia benchmark)</p>
    <p>MLB can indirectly lead to improvements in the performance of CPU</p>
    <p>GPU communication via the mitigation of memory contention when</p>
    <p>memory contention is intensive (17.8% on i7-2600 with GTX 580 and</p>
    <p>on i7-2600</p>
    <p>with GTX 580</p>
    <p>on E5-2690</p>
    <p>with Tesla K20C</p>
    <p>n o rm</p>
    <p>a li z e d _t</p>
    <p>o _n</p>
    <p>a iv</p>
    <p>e</p>
    <p>GPU kernel time CPU-GPU communication time</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Comparison with VBSC</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Comparison with VBSC</p>
    <p>Core 0 Core 1 Core 2 Core 3</p>
    <p>Vec:100</p>
    <p>Vec:95 Vec:20</p>
    <p>Vec:25</p>
    <p>Vec:10</p>
    <p>Vec:15 Vec:90</p>
    <p>Vec:85</p>
    <p>First order</p>
    <p>Second order</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Comparison with VBSC</p>
    <p>(CPU-bound application)</p>
    <p>lbm libquantum GemsFDTD soplex namd sjeng gamess gobmk</p>
    <p>n o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>_ to</p>
    <p>_ n</p>
    <p>a iv</p>
    <p>e</p>
    <p>on 4cores i7-2600</p>
    <p>with VBSC with MLB</p>
    <p>mix1 with VBSC mix1 with MLB</p>
    <p>n o rm</p>
    <p>a li z e d _t</p>
    <p>o _n</p>
    <p>a iv</p>
    <p>e</p>
    <p>memory_bandwidth (#bytes/sec) memory_request</p>
    <p>buffer_full_rate (#events/sec)</p>
    <p>LLC_reference_rate (#events/sec) LLC_miss_rate (#events/sec)</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Comparison with VBSC</p>
    <p>(I/O-bound application)</p>
    <p>VBSC cannot avoid modifying the timeslice mechanism for sorting tasks</p>
    <p>in a runqueue, and hence this scheduling scheme can be in conflict with</p>
    <p>I/O-bound tasks</p>
    <p>An I/O boud application, TPC-C benchmark, is seriously degraded by</p>
    <p>about 300% with VBSC</p>
    <p>MLB can allow the I/O boud task to be performed without causing</p>
    <p>performance degradation compared with the nave Linux scheduler</p>
    <p>VBSC</p>
    <p>(epoch_lengthX1)</p>
    <p>VBSC</p>
    <p>(epoch_lengthX2)</p>
    <p>VBSC</p>
    <p>(epoch_lengthX4)</p>
    <p>MLB</p>
    <p>n o rm</p>
    <p>a li z e d _t</p>
    <p>o _n</p>
    <p>a v</p>
    <p>e</p>
    <p>(t p m</p>
    <p>C )</p>
    <p>lbm + libquantum + soplex + GemsFDTD + namd</p>
    <p>+ sjeng + gamess + gobmk + TPC-C</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Comparison with VBSC</p>
    <p>(Worst case)</p>
    <p>Core 0 Core 1 Core 2 Core 3</p>
    <p>Vec:100 Vec:95 Vec:90</p>
    <p>Vec:85</p>
    <p>Vec:15 Vec:20</p>
    <p>m e m</p>
    <p>o ry</p>
    <p>_r e q u e s t</p>
    <p>b u ff</p>
    <p>e r_</p>
    <p>fu ll _r</p>
    <p>a te</p>
    <p>(# e v e n ts</p>
    <p>/s e c )</p>
    <p>timeline(sec)</p>
    <p>VBSC MLB</p>
    <p>Task termination can make the worst case with VBSC</p>
    <p>All memory intensive tasks can be scheduled at the same time after at 300secs</p>
    <p>But, there is no growth of memory request buffer full rate with MLB</p>
    <p>With VBSC</p>
    <p>First order</p>
    <p>Second order</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Extension of MLB for multi-threaded</p>
    <p>application</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>MLB can reduce the level of the parallelism, but the placement by MLB can decrease the memory contention and increase the cache hit ratio [Dey et al. ISPASS11]</p>
    <p>A pair for seven applications (total 21 pairs) and each application creates 8 threads on 8 cores of Xeon E5-2690 (16threads on 8cores E5-2690)</p>
    <p>canneal, ratrace (PARSEC), streamcluster, hotspot, lavaMD, cfd (rodinia), water spatial (SPLASH2x)</p>
    <p>The approach in which the highly intensive tasks are migrated to specific cores as in MLB can be extended to multi-threaded applications</p>
    <p>Extension of MLB for multi-threaded</p>
    <p>applications</p>
  </div>
  <div class="page">
    <p>DCS Lab</p>
    <p>Conclusion</p>
    <p>MLB has three stronger points than VBSC</p>
    <p>MLB mechanism can be more easily implemented in both user and</p>
    <p>kernel levels than VBSC without the modification of timeslice,</p>
    <p>MLB can allow I/O boud applications to be executed without</p>
    <p>causing performance degradation unlike VBSC</p>
    <p>MLB can handle the worst case which can be shown by the VBSC</p>
    <p>mechanism</p>
    <p>We will extend MLB on the bigger system which includes</p>
    <p>those in the NUMA architecture and multiple GPGPUs</p>
  </div>
  <div class="page">
    <p>Seoul National University DCS Lab</p>
    <p>Thanks</p>
    <p>Any questions</p>
  </div>
</Presentation>
