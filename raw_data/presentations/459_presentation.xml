<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Simulation Calibration with Correlated Knowledge-Gradients</p>
    <p>Peter Frazier1</p>
    <p>Warren Powell2</p>
    <p>Hugo Simao2</p>
    <p>Monday December 14, 2009 Winter Simulation Conference, Austin</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 1 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration at Schneider National</p>
    <p>The logistics company Schneider National uses a large simulation-based optimization model to try what if scenarios.</p>
    <p>The model has several input parameters that must be tuned to make its behavior match reality before it can be used.</p>
    <p>The model is tuned by hand once per year on the most recent data. Each tuning effort requires between 1 and 2 weeks.</p>
    <p>2008 Warren B. Powell Slide 113</p>
    <p>Schneider National</p>
    <p>2008 Warren B. Powell Slide 114</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 2 / 19</p>
  </div>
  <div class="page">
    <p>Model Parameters</p>
    <p>Input parameters to the model include:</p>
    <p>time-at-home bonuses. pacing parameters describing how fast and far drivers drive per day. gas prices . . .</p>
    <p>Output parameters from the model include:</p>
    <p>billed miles driver utilization average number of trips home per driver per 4 weeks. proportion of drivers without time at home over 4 weeks. . . .</p>
    <p>Some of these inputs are known (e.g., gas prices), but some are unknown (e.g. time-at-home bonuses).</p>
    <p>Goal: adjust the inputs to make the optimal solution found by the model match current practice.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 3 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration</p>
    <p>Goal: adjust the inputs to make the optimal solution found by the ADP model match current practice.</p>
    <p>Running the simulator to convergence for one set of bonuses takes 3 days, making calibration difficult.</p>
    <p>The model may be run for shorter periods of time, e.g. 12 hours, to obtain noisy output estimates.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 4 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration: Objective Function</p>
    <p>We have a global optimization problem with expensive noisy measurements:</p>
    <p>min x</p>
    <p>f (x ),</p>
    <p>f (x ) is the fitting error with input parameters x  X  Rp ,</p>
    <p>f (x ) = J</p>
    <p>j=1</p>
    <p>(j (x )gj )2.</p>
    <p>j (x ) is the models limiting output for variable j when given input x .</p>
    <p>gj is our goal for output variable j .</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 5 / 19</p>
  </div>
  <div class="page">
    <p>Bayesian Global Optimization</p>
    <p>Bayesian Global Optimization (BGO) [Mockus 1989, Jones et al. 1998] is a general approach for global optimization of functions that are expensive or time-consuming to evaluate.</p>
    <p>We begin with a Gaussian process prior distribution on the unknown function, which is generally a Gaussian process.</p>
    <p>The parameters of the prior were chosen using data from past calibrations and conversations with the calibration expert at Schneider.</p>
    <p>We combine the function evaluations observed so far with the prior to obtain a posterior.</p>
    <p>Then, we use the posterior to choose the next point to evaluate.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 6 / 19</p>
  </div>
  <div class="page">
    <p>Bayesian Global Optimization</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 7 / 19</p>
  </div>
  <div class="page">
    <p>Bayesian Global Optimization</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 7 / 19</p>
  </div>
  <div class="page">
    <p>Bayesian Global Optimization</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 7 / 19</p>
  </div>
  <div class="page">
    <p>Bayesian Global Optimization</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 7 / 19</p>
  </div>
  <div class="page">
    <p>Knowledge-Gradient Policy</p>
    <p>The knowledge-gradient policy is defined to be the policy that chooses its next measurement xm to maximize the KG factor,</p>
    <p>KG (x ) = min</p>
    <p>x</p>
    <p>m(x)Em [</p>
    <p>min x</p>
    <p>m+1(x) | xm = x</p>
    <p>] .</p>
    <p>m(x) := En[f (x)] is the expected loss at x given what we know at</p>
    <p>time m.</p>
    <p>minx  m(x) is the best we can do given what we know at m.</p>
    <p>minx  m+1(x) is the best we will be able to do given what we know</p>
    <p>at m and what we learn from our measurement xm.</p>
    <p>The KG factor is similar to expected improvement [Jones et al. 1998], and is the expected value of sampling information [Howard 1966].</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 8 / 19</p>
  </div>
  <div class="page">
    <p>ADP Output</p>
    <p>Typical ADP Output at one choice of the parameter vector x . The plot shows sampled time-at-home for one particular driver type over 200 ADP training iterations.</p>
    <p>iterations (n)</p>
    <p>so lo</p>
    <p>T A</p>
    <p>H</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 9 / 19</p>
  </div>
  <div class="page">
    <p>Reconciling ADP Output and BGO Formulation</p>
    <p>The classic BGO formulation assumes that an observation at x has distribution Normal(f (x ),  (x )).</p>
    <p>If we run our ADP model to convergence (say, to 200 iterations), then this assumption is met. . .</p>
    <p>. . . but running to convergence at a single x takes 3 days.</p>
    <p>If our x seems bad after a few iterations, we should stop early.</p>
    <p>Human calibrators use early stopping to their advantage.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 10 / 19</p>
  </div>
  <div class="page">
    <p>Statistical Model of ADP Output</p>
    <p>iterations (n) so</p>
    <p>lo TA</p>
    <p>H</p>
    <p>We model the ADP output as</p>
    <p>Y nj (x ) = Bj (x ) + [j (x )Bj (x )] [1exp (nRj (x ))] +  n, n &gt; n0.</p>
    <p>Y nj (x ) are direct observations from the ADP model;</p>
    <p>j (x ) is the limiting value to which this output converges;</p>
    <p>Rj (x ) is the rate at which the output converges to its limiting value;</p>
    <p>n0 = 10 allows us to ignore erratic initial output;</p>
    <p>Bj (x ) is, roughly speaking, the output at the first iteration;</p>
    <p>n is an independent unbiased normal random variable.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 11 / 19</p>
  </div>
  <div class="page">
    <p>Working with Non-stationary Output</p>
    <p>Using the model, we may obtain an estimate of j (x ) from observations Y nj (x ), n = 1, . . . , m, for m less than 200.</p>
    <p>iterations (n)</p>
    <p>so lo</p>
    <p>T A</p>
    <p>H</p>
    <p>iterations (n)</p>
    <p>E st</p>
    <p>im a te</p>
    <p>o f G</p>
    <p>k( )</p>
    <p>Avg of data after n=100</p>
    <p>Avg of all data</p>
    <p>Posterior mean</p>
    <p>Posterion mean  2 std dev</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 12 / 19</p>
  </div>
  <div class="page">
    <p>KG method</p>
    <p>Recall that the KG factor is given by</p>
    <p>KG (x ) = min</p>
    <p>x</p>
    <p>m(x)Em [</p>
    <p>min x</p>
    <p>m+1(x) | xm = x</p>
    <p>] ,</p>
    <p>and that the KG policy measures the x with the largest KG factor.</p>
    <p>This KG factor is well-defined even when the observations are non-stationary.</p>
    <p>To compute the KG factor, we use the predictive distribution of(</p>
    <p>n+1(x) ) xX given that we measure at x .</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 13 / 19</p>
  </div>
  <div class="page">
    <p>Computing the KG policy (Approximately)</p>
    <p>We have  m+1(x ) = Em+1 [ j (j (x )gj )2</p>
    <p>] , which is a function of</p>
    <p>the time m + 1 posterior mean and variance of j (x ).</p>
    <p>We calculate the predictive distributions for the time m + 1 posterior mean and variance of j (x ) from the statistical model.</p>
    <p>We then calculate Em[ m+1(x )] =  m(x ) and</p>
    <p>m(x, xm) =</p>
    <p>Varm[ m+1(x ) | xm]. maxxX</p>
    <p>m+1(x )  maxxX   m(x ) +  m(x, xm)Z where X   X is a finite subset and Z is a one-dimensional standard normal random variable.</p>
    <p>Then, the KG factor is approximated by the expectation of a piecewise linear function of a standard normal random variable. This expectation can be computed analytically.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 14 / 19</p>
  </div>
  <div class="page">
    <p>Time-at-Home</p>
    <p>The most critical input parameters are the time-at-home (TAH) bonuses.</p>
    <p>The optimization model awards a bonus to itself each time it brings a truck driver home. The amount awarded depends on the type of driver.</p>
    <p>The most critical driver types are solo company drivers, and solo independent contractors.</p>
    <p>Current company practice gets solo company drivers home 2 times per month, and independent contractors 1.7 times per month, on average.</p>
    <p>If we tune these so that the average number of time at home events are correct for these two driver types, then the other outputs also tend to match.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 15 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>Mean of Posterior, n</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>log(KG Factor)</p>
    <p>Bonus 1</p>
    <p>B o</p>
    <p>n u</p>
    <p>s 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>n</p>
    <p>lo g</p>
    <p>(B e</p>
    <p>st F</p>
    <p>it)</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 16 / 19</p>
  </div>
  <div class="page">
    <p>Simulation Model Calibration Results</p>
    <p>The KG method calibrates the model in approximately 3 days, compared to 714 days when tuned by hand. The calibration is automatic, freeing the human calibrator to do other work.</p>
    <p>The KG method calibrates as accurately or better than does by-hand calibration.</p>
    <p>Current practice uses the years calibrated bonuses for each new what if scenario, but to enforce the constraint on driver at-home time it would be better to recalibrate the model for each scenario. Automatic calibration with the KG method makes this feasible.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 17 / 19</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Frazier, P., Powell, W., and Simao, H. (2009). Simulation model calibration with correlated knowledge-gradients. Winter Simul. Conf. Proc., 2009.</p>
    <p>Howard, R. (1966). Information Value Theory. Systems Science and Cybernetics, IEEE Transactions on, 2(1):2226.</p>
    <p>Jones, D., Schonlau, M., and Welch, W. (1998). Efficient Global Optimization of Expensive Black-Box Functions. Journal of Global Optimization, 13(4):455492.</p>
    <p>Mockus, J. (1989). Bayesian approach to global optimization: theory and applications. Kluwer Academic, Dordrecht.</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 18 / 19</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Any questions?</p>
    <p>Frazier,Powell,Simao (Cornell,Princeton) WSC 2009 19 / 19</p>
  </div>
</Presentation>
