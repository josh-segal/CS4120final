<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Implementing MPI on Windows: Comparison with Common Approaches on Unix</p>
    <p>Jayesh Krishna,1 Pavan Balaji,1 Ewing Lusk,1 Rajeev Thakur,1 Fabian Tillier2</p>
  </div>
  <div class="page">
    <p>Windows and HPC</p>
    <p>Popular in the desktop world  Increasing presence in HPC world  Windows Server 2008 HPC Edition</p>
    <p>(http://www.microsoft.com/hpc)  Dawning 5000A cluster with 30,720</p>
    <p>cores at Shanghai Supercomputer Center running Windows HPC Server 2008 achieved more than 200 TF/s on LINPACK and ranked 10th in Nov 2008 Top500 list</p>
    <p>Commercial applications in areas such as CFD, Structural analysis, Seismic modeling, finance etc</p>
    <p>Allows scientists to use a familiar desktop interface for HPC</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Historically Unix has been OS of choice for HPC  More research and development experience in HPC in the Unix world  An MPI library suite on Windows allows organizations to tap into idle-time</p>
    <p>computing power of their Windows desktops  Users can use a familiar desktop interface for HPC  Implementing a high-performance, portable MPI library is not trivial.  Performance vs maintainability</p>
    <p>Good performance involves using OS-specific features which usually requires careful design for maintainability</p>
    <p>Portability vs maintainability  A portable MPI library involves several OS-specific components.  More code typically results poor maintenance due to complicated</p>
    <p>dependencies among components.</p>
  </div>
  <div class="page">
    <p>Background (MPICH2)</p>
    <p>A high-performance, widely portable implementation of the MPI standard</p>
    <p>MPI 2.2 compliant  Supports Windows and Unix  Modular design supports largely</p>
    <p>common code base  Framework extensible by plugging</p>
    <p>in custom network devices, Process Managers and other tools</p>
    <p>MPICH2 suite includes a high performance multi-network communication subsystem called Nemesis</p>
  </div>
  <div class="page">
    <p>Background (Nemesis)</p>
    <p>A Scalable, multi-network communication subsystem available as a CH3 channel.</p>
    <p>An efficient implementation available on Windows (MPICH2 1.1x series onward)</p>
    <p>Default channel in MPICH2 1.3x series on Windows; earlier on Unix  Uses efficient shared memory operations, lock-free algorithms and</p>
    <p>optimized memory-copy routines  Low shared memory latency, 240-275ns  Modular Design allows developers to plug-in custom communication</p>
    <p>modules  Large Message Transfer (LMT) modules, Internode communication modules etc</p>
  </div>
  <div class="page">
    <p>Implementing MPI</p>
    <p>An MPI library suite usually includes  a library that implements the MPI standard  the runtime components that support it.</p>
    <p>Typically the following OS services are required to implement an MPI library suite  Thread services  Create threads, mutex locks etc  Intranode communication services - e.g. shared memory services  Internode communication services  e.g. TCP/IP sockets  Process management services  Create/Suspend/Kill processes, interprocess</p>
    <p>communication mechanisms (e.g. pipes)</p>
  </div>
  <div class="page">
    <p>Implementing MPI on Windows Vs Unix</p>
    <p>MPICH2s modular architecture maintains a largely common code base  MPI layer (Algorithms for collectives etc)  CH3 device (Implementing the ADI3 interface)</p>
    <p>Portability across Windows and Unix is achieved using  OS abstraction layer (eg: For shared memory services, TCP/IP services)  Portable libraries (eg: Open Portable Atomics library)  OS specific plug-in modules (eg: Large Message Transfer module in Nemesis)</p>
    <p>A high-performance implementation of MPI should incorporate OSspecific features provided. These features differ on Unix and Windows wrt  Asynchronous progress support  Process Management services  Intranode communication services  User thread support</p>
  </div>
  <div class="page">
    <p>Asynchronous Progress</p>
    <p>Asynchronous progress  User initiates operation  OS performs progress on the operation  OS notifies user when operation completes</p>
    <p>Allows other user operations while OS performs progress on the user initiated operation</p>
    <p>Contrast with non-blocking progress  user polls for availability of resource  user performs non-blocking operation  again polls for availability of the resource, if needed</p>
    <p>No portable support for asynchronous progress on Unix systems for network communication.</p>
    <p>Windows supports asynchronous progress using I/O Completion Ports</p>
  </div>
  <div class="page">
    <p>Asynchronous Progress (I/O Completion Ports)  Supports asynchronous progress of user-initiated OS requests  Provides an efficient threading model for processing multiple</p>
    <p>asynchronous I/O requests on a multiprocessor system  Nemesis uses I/O Completion Ports for internode communication (An</p>
    <p>older version used non-blocking progress engine based on select() )  Asynchronous progress in Nemesis</p>
    <p>Nemesis posts a request to the kernel (e.g.: read on a TCP/IP socket)  When request completes, the kernel queues a completion packet on the</p>
    <p>completion port  Nemesis processes this packet when it waits (MPI_Wait()) on the request</p>
    <p>MPI process can proceed with user computation while kernel processes the request</p>
    <p>Available on some Unix systems (e.g. Solaris). However not portable across Unix systems</p>
    <p>Several advantages over select()/poll()</p>
  </div>
  <div class="page">
    <p>Asynchronous progress (contd)</p>
    <p>MPI application</p>
    <p>MPICH2</p>
    <p>Operating System</p>
    <p>I/O Completion Object</p>
    <p>Completion Queue</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>National Center for Supercomputing Applications (NCSA) Intel 64 Cluster  Abe</p>
    <p>Dell PowerEdge 1955  1200 nodes/9600 cores  2.33 GHz Dual Quad Core Intel 64</p>
    <p>(Clovertown) processors  4x32 KB L1 Cache, 2x4 MB L2 Cache,</p>
  </div>
  <div class="page">
    <p>NCSA Intel 64 Cluster Abe (contd)</p>
    <p>Windows nodes  Windows Server 2008 HPC Edition SP2  Visual Studio 2008 C/C++ compiler suite</p>
    <p>Unix nodes  Red Hat Enterprise Linux 4 (Linux 2.6.18)  Intel C/C++ 10.1 compiler</p>
    <p>Job Scheduler  Unix nodes  Torque  Windows nodes - Windows Server 2008 HPC Job Scheduler</p>
    <p>MPICH2 compiled with aggressive optimization &amp; error checking disabled for user arguments</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation (Asynchronous Progress)  Asynchronous progress (I/O completion ports) Vs Non-blocking progress</p>
    <p>(select) for Nemesis netmod on Windows  Compared overlap of MPI communication with User computation  Nemesis uses Asynchronous progress by default on Windows  Modified NetMPI (NetPIPE benchmark)</p>
    <p>{ MPI_Irecv(); MPI_Send(); MPI_Wait(); }</p>
    <p>{ MPI_Irecv(); MPI_Irecv();  MPI_Isend(); MPI_Isend();  do{ MPI_Testall(); User_computation(250ns); }while(!complete);</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation (Asynchronous Progress)</p>
    <p>Time spent in the MPI library (MPI time) vs Time spent in user computation (Compute time). Asynchronous progress (using iocp - Completion ports) is compared with non-blocking progress (select())</p>
  </div>
  <div class="page">
    <p>Process Management</p>
    <p>Provide a mechanism to launch MPI processes and setting the runtime environment for the processes to communicate with each other.</p>
    <p>Launch process locally on a node  OS services like fork()  Launch process remotely without 3rd party Job schedulers.</p>
    <p>On Unix  ssh (sshd typically runs on Unix nodes)  On Windows - No such mechanism available</p>
    <p>MPICH2 suite includes SMPD (Simple MultiPurpose Daemon)  server daemons are launched on each node  Installed as a Windows service on windows nodes when user installs MPICH2</p>
    <p>SMPD also supports Windows Server 2008 HPC Job Scheduler for launching MPI processes</p>
  </div>
  <div class="page">
    <p>Process Management (contd)</p>
    <p>User authentication  On Unix  ssh protocol manages user credentials  On Windows  No such mechanism available</p>
    <p>SMPD  manages user credentials for Windows users  SMPD also supports user authentication using Active Directory or</p>
    <p>Windows Server 2008 HPC pack.  SMPD communication protocol is node architecture agnostic</p>
    <p>allows users to run MPI jobs across Unix and Windows nodes</p>
  </div>
  <div class="page">
    <p>Intranode communication</p>
    <p>Typically using shared memory  Shared memory performance degrades when CPUs dont share data</p>
    <p>cache.  Windows allows users to directly read from the virtual memory of another</p>
    <p>process  Direct remote memory access performance not good for small messages  Nemesis on Windows uses an OS-specific plug-in module for large</p>
    <p>message transfers (Read Remote Virtual Memory module - RRVM)  Nemesis on Windows uses lock-free shared memory queues for small</p>
    <p>messages (low latency) and direct remote memory access for large messages (high bandwidth)</p>
  </div>
  <div class="page">
    <p>Default LMT vs RRVM LMT module</p>
    <p>RTS</p>
    <p>CTS + cookie</p>
    <p>Rank 0 Rank 1</p>
    <p>data</p>
    <p>RTS + cookie</p>
    <p>data</p>
    <p>DONE</p>
    <p>Rank 0 Rank 1</p>
    <p>Shared memory LMT RRVM LMT</p>
    <p>Recv posted Recv posted</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation (Intranode communication)  Nemesis intranode communication performance using lock-free shared</p>
    <p>memory queues (shm) Vs Direct remote memory access on Windows  Nemesis intranode communication performance on Windows Vs Linux  Direct intranode remote memory access implemented by a large message</p>
    <p>transfer plug-in Nemesis module (RRVM module) on Windows  Nemesis performs large message transfer for &gt;16KB with RRVM module</p>
    <p>on Windows and &gt;64KB on Linux  OSU micro-benchmarks used for measuring latency and bandwidth  Two cases considered</p>
    <p>Communicating processes share a 4MB CPU data cache  Communicating processes dont share a data cache</p>
  </div>
  <div class="page">
    <p>RRVM vs SHM Bandwidth on Windows</p>
    <p>Intranode communication bandwidth using RRVM module Vs lock-free shared memory queues on Windows, with and without shared CPU data caches</p>
  </div>
  <div class="page">
    <p>Intranode latency: Unix vs Windows</p>
    <p>Intranode communication latency on Windows and Unix with and without shared CPU data caches</p>
  </div>
  <div class="page">
    <p>Intranode bandwidth: Unix vs Windows</p>
    <p>Intranode communication bandwidth on Windows and Unix with and without shared CPU data caches</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation (Intranode communication)  RRVM module outperforms shared memory queues when processes dont</p>
    <p>share a data cache  Shared memory queues outperform RRVM for some message sizes when</p>
    <p>communicating processes share a data cache  RRVM provides good overall performance  Jagged graph</p>
    <p>At 16KB the double-buffering communication algorithm runs out of L1 cache  &gt;64KB Nemesis switches to LMT protocol  At 2MB the double-buffering algorithm runs out of L2 cache  More tuning required</p>
    <p>0 byte latency of 240ns on Unix and 275ns on Windows  Nemesis on Windows performs significantly better than Unix for large</p>
    <p>message sizes when processes dont share data caches</p>
  </div>
  <div class="page">
    <p>Internode communication</p>
    <p>MPI communication between processes on different nodes  Nemesis network modules (netmods) perform internode communication  Network modules are easily pluggable into the Nemesis framework.</p>
    <p>Separate network modules available to support TCP/IP sockets, IB etc  A Windows-specific TCP/IP netmod is available for Nemesis</p>
    <p>Uses an asynchronous progress engine for managing TCP/IP socket operations  Performance much better than older netmod using non-blocking progress  Tuned for good performance on Windows</p>
  </div>
  <div class="page">
    <p>Internode communication performance</p>
    <p>Nemesis Internode communication on Unix Vs Windows using TCP  Windows has an OS-specific network module for Nemesis  OSU micro-benchmarks used for measuring latency and bandwidth</p>
  </div>
  <div class="page">
    <p>Internode bandwidth</p>
    <p>Internode MPI communication bandwidth for Nemesis</p>
  </div>
  <div class="page">
    <p>Thread Support</p>
    <p>User requests a particular level of thread support, library notifies the level supported</p>
    <p>MPICH2 supports MPI_THREAD_MULTIPLE  Since multiple threads can interact with MPI library at the same time</p>
    <p>thread locking mechanisms are required  On Unix  a POSIX threads library  Windows  OS-specific implementation</p>
    <p>Choice of intraprocess locks Vs interprocess locks (costly) affects performance</p>
    <p>Shared memory communication typically involves interprocess locks that are costly</p>
    <p>MPICH2 (Nemesis) uses lock-free shared memory operations and hence only requires intraprocess locks</p>
  </div>
  <div class="page">
    <p>Cost of supporting threads</p>
    <p>Cost of supporting MPI_THREAD_MULTIPLE vs MPI_THREAD_SINGLE for MPICH2 on Windows Vs Unix</p>
    <p>OSU micro-benchmark for measuring latency  Modified to use MPI_Init_thread() instead of MPI_Init()  Both cases only one user thread  Latency overhead = Percentage increase in latency over MPI_THREAD_SINGLE</p>
    <p>when MPI_THREAD_MULTIPLE is enabled  Supporting thread safety</p>
    <p>On Unix, Pthread process private mutex locks  On Windows, Critical Sections</p>
    <p>Interprocess locks typically used for shared memory communication is costly  On Unix, pthread_mutexattr_setpshared(PTHREAD_PROCESS_SHARED/</p>
    <p>PTHREAD_PROCESS_PRIVATE)  On Windows, Mutexes Vs Critical Sections</p>
  </div>
  <div class="page">
    <p>Cost of supporting threads (contd)</p>
    <p>Latency Overhead ={ (Latencympi_thread_multiple- Latencympi_thread_single)/Latencympi_thread_single} * 100</p>
  </div>
  <div class="page">
    <p>Cost of supporting threads (contd)</p>
    <p>Overhead for supporting threads is lower on Windows than Unix  Older versions used interprocess locks on Windows (SHM channel)  Nemesis uses intraprocess locks on both Unix and Windows for</p>
    <p>guaranteeing thread safety  Nemesis uses lock-free shared memory queues</p>
    <p>i.e., No interprocess locks required for MPI communication  Intraprocess locks sufficient for guaranteeing thread safety</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Microsoft MPI (MSMPI)  http://msdn.microsoft.com/en-us/library/bb524831(VS.85).aspx</p>
    <p>Intel MPI  http://software.intel.com/en-us/intel-mpi-library/</p>
    <p>Open MPI  http://www.open-mpi.org/</p>
    <p>Deino MPI  http://mpi.deino.net/</p>
    <p>MPI.NET  Provides C# bindings for Microsoft .NET environment  http://osl.iu.edu/research/mpi.net/</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Good intranode communication performance (0-byte latency of 240275ns)</p>
    <p>Asynchronous progress improves performance on Windows  Internode communication performance gap; needs tuning  Cost of supporting user threads less on Windows than Pthread library on</p>
    <p>Linux  A modular architecture with clearly defined interfaces between modules</p>
    <p>makes life easy  Implementing a high-performance and portable MPI library requires</p>
    <p>careful design and leveraging OS-specific features</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Topology aware intranode communication  MPICH2 can use node topology information to select between using lock-free</p>
    <p>shared memory queues and Direct remote memory access  Fix internode communication performance gap</p>
    <p>More tuning is required  Allow multiple user threads to concurrently work with the asynchronous</p>
    <p>progress engine  MPI Collectives performance  MPI Application Scalability and performance  Support for NetworkDirect API</p>
    <p>A Nemesis network module for NetworkDirect  Network Direct service provider interface exposes advanced capabilities of</p>
    <p>modern high speed networks (e.g., Infiniband)</p>
  </div>
</Presentation>
