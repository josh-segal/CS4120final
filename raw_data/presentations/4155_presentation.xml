<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hardware Execution Throttling for Multicore Resource</p>
    <p>Management</p>
    <p>Xiao Zhang</p>
    <p>Sandhya Dwarkadas</p>
    <p>Kai Shen</p>
  </div>
  <div class="page">
    <p>The Multi-Core Challenge</p>
    <p>Multi-core chip  Dominant on market  Last level on-chip cache is</p>
    <p>commonly shared by sibling cores, however sharing is not well controlled</p>
    <p>Challenge: Performance Isolation  Poor &amp; unpredictable performance  Denial of service attacks</p>
    <p>source: http://www.intel.com</p>
  </div>
  <div class="page">
    <p>A Full Solution Includes</p>
    <p>Good mechanism  Should be both efficient and practical to</p>
    <p>deploy</p>
    <p>Main focus of this talk</p>
    <p>Good policy to govern mechanism  as important as mechanism, and not easy  Omitted in this talk</p>
  </div>
  <div class="page">
    <p>Existing Mechanism(I): Software based Page Coloring</p>
    <p>Thread A</p>
    <p>Thread B</p>
    <p>Shared Cache</p>
    <p>Way-1 Way-n</p>
    <p>Memory page A1</p>
    <p>A2</p>
    <p>A3</p>
    <p>A4</p>
    <p>A5</p>
    <p>Thread As footprint  Classic technique originally used to reduce cache miss, recently used by OS to manage cache partitioning</p>
    <p>Partition cache at coarse granularity</p>
    <p>No need for hardware support</p>
  </div>
  <div class="page">
    <p>Existing Mechanism(II): Scheduling Quantum Adjustment</p>
    <p>Shorten the time quantum of app that overuses cache</p>
    <p>May let core idle if there is no other active thread available</p>
    <p>Thread B</p>
    <p>Thread A idle</p>
    <p>Thread B</p>
    <p>Thread A idle</p>
    <p>Thread B</p>
    <p>Thread A idle Core 0</p>
    <p>Core 1</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>New Mechanism: Hardware Execution Throttling</p>
    <p>Throttle the execution speed of app that overuses cache  Duty cycle modulation</p>
    <p>CPU works only in duty cycles and stalls in non-duty cycles  Allow per-core control (vs. per-processor control for existing</p>
    <p>Dynamic Voltage Frequency Scaling)</p>
    <p>Enable/disable cache prefetchers  L1 prefetchers</p>
    <p>IP: keeps per-instruction load history to detect stride pattern  DCU: prefetches next line when it detects multiple loads from the same line</p>
    <p>within a time limit</p>
    <p>L2 prefetchers  Adjacent line: Prefetches the adjacent line of required data  Stream: looks at streams of data for regular patterns</p>
  </div>
  <div class="page">
    <p>Brief View of Hardware Execution Throttling</p>
    <p>Comparison to page coloring  Little complexity to kernel</p>
    <p>Code length: 40 lines in a single file (as a reference our page coloring implementation takes 700+ lines of code crossing 10+ files)</p>
    <p>Lightweight to configure  Read plus write register: duty-cycle 265 + 350 cycles, prefetcher 298 + 2065</p>
    <p>cycles, which is less than 1 microsecond on a 3Ghz CPU (as a reference recoloring a page takes 3 microseconds on the same CPU)</p>
    <p>Comparison to scheduling quantum adjustment  More fine-grained controlling</p>
    <p>Thread B</p>
    <p>Core 0</p>
    <p>Core 1</p>
    <p>Thread A idle</p>
    <p>Quantum adjustment Hardware execution throttling</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Evaluation  Candidate mechanisms</p>
    <p>Page coloring  Scheduling quantum adjustment  Hardware execution throttling</p>
    <p>Experiment setup  Conducted on a 3.0 Ghz Intel dual-core processor  3 SPECCPU-2000 apps (swim, mcf, &amp; equake) and 2</p>
    <p>server-style apps (SPECjbb2005 &amp; SPECweb99), running all possible pair-wise co-schedule</p>
    <p>Goal: evaluate their effectiveness in providing performance fairness  For each mechanism, tune its configuration offline to</p>
    <p>achieve best fairness</p>
  </div>
  <div class="page">
    <p>Fairness Comparison</p>
    <p>On average all three mechanisms are effective in improving fairness</p>
    <p>Case {swim, SPECweb} illustrates limitation of page coloring</p>
    <p>Unfairness factor: coefficient of variation (deviationto-mean ratio,  / ) of co-running apps normalized performances</p>
  </div>
  <div class="page">
    <p>Performance Comparison  System efficiency: geometric mean of co-running apps</p>
    <p>normalized performances  On average all three mechanisms achieve system</p>
    <p>efficiency comparable to default sharing</p>
    <p>Case where severe interthread cache conflicts exist favors segregation, e.g. {swim, mcf}</p>
    <p>Case where well-interleaved cache accesses exist favors sharing, e.g. {mcf, mcf}</p>
  </div>
  <div class="page">
    <p>Drawbacks of Page Coloring  Expensive re-coloring cost</p>
    <p>Prohibitive in a dynamic environment where frequent re-coloring may be necessary</p>
    <p>Complex memory management  Introduces artificial memory pressure</p>
    <p>Thread A</p>
    <p>Thread B</p>
    <p>Shared Cache</p>
    <p>Way-1 Way-n</p>
    <p>Memory page</p>
    <p>A1</p>
    <p>A2</p>
    <p>A3</p>
    <p>A4</p>
    <p>A5</p>
    <p>Thread As footprint</p>
    <p>For more details on tackling these problems, please read our Eurosys09 paper: Practical Page coloring based Multi-core Cache Management</p>
  </div>
  <div class="page">
    <p>Drawback of Scheduling Quantum Adjustment</p>
    <p>Coarse-grained control at scheduling quantum granularity may result in fluctuating service delays for individual transactions</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Hardware execution throttling mechanism for multi-core cache management  Fine-grained control  Lightweight solution that cleverly reuses existing hardware</p>
    <p>features  System efficiency is competitive to default sharing, largely</p>
    <p>comparable to scheduling quantum adjustment, but inferior to ideal page coloring</p>
    <p>Future work  Investigate policy for online configuration</p>
  </div>
</Presentation>
