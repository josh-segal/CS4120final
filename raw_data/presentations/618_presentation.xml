<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TransMR: Data Centric Programming Beyond Data</p>
    <p>Parallelism</p>
    <p>Naresh Rapolu</p>
    <p>Karthik Kambatla</p>
    <p>Prof. Suresh Jagannathan</p>
    <p>Prof. Ananth Grama</p>
  </div>
  <div class="page">
    <p>Limitations of Data-Centric Programming Models</p>
    <p>Data-centric programming models (MapReduce, Dryad etc.) are limited to data-parallelism in any phase.  Two map operators cannot communicate with each other.  This is mainly due to the deterministic-replay based fault</p>
    <p>tolerance model: Replay should not violate application semantics.</p>
    <p>Consider presence of side-effects: Writing to persistent storage or network based communication.</p>
  </div>
  <div class="page">
    <p>Need for side-effects</p>
    <p>Side-effects lead to communication/ datasharing across computations.</p>
    <p>Boruvkas algorithm to find MST</p>
    <p>Each iteration coalesces a node with its closes neighbor. Iterations which do not cause conflicts can be executed in parallel.</p>
  </div>
  <div class="page">
    <p>Beyond Data Parallelism</p>
    <p>Amorphous Data Parallelism  Most of the data can be operated on in parallel.</p>
    <p>Some of them conflict and can only be detected dynamically at runtime.  The Tao of Parallelism, Pingali et. al., PLDI 11</p>
    <p>The Galois system</p>
    <p>Online algorithms / Pipelined workflows  MapReduce Online [Condie10] is an approach</p>
    <p>needing heavy checkpointing.</p>
    <p>Software Transactional Memory (STM) Benchmark applications  STAMP, STMBench etc.</p>
  </div>
  <div class="page">
    <p>System Architecture</p>
    <p>N1 N2 Nn</p>
    <p>Distributed</p>
    <p>Execution Layer</p>
    <p>Distributed</p>
    <p>Key-Value Store</p>
    <p>GS</p>
    <p>CU</p>
    <p>LS</p>
    <p>CU</p>
    <p>LS</p>
    <p>GS</p>
    <p>CU</p>
    <p>LS</p>
    <p>CU</p>
    <p>LS</p>
    <p>GS</p>
    <p>CU</p>
    <p>LS</p>
    <p>CU</p>
    <p>LS</p>
    <p>Distributed key-value store provides a shared-memory abstraction to the distributed execution-layer</p>
  </div>
  <div class="page">
    <p>Semantics of TransMR (Transactional MapReduce)</p>
  </div>
  <div class="page">
    <p>Semantics Overview  Data-Centric function scope -- Map/Reduce/</p>
    <p>Merge etc. -- termed as a Computation Unit (CU)) is executed as a transaction.</p>
    <p>Optimistic reads and write-buffering. Local Store (LS) forms the write-buffer of a CU.  Put (K, V): Write to LS which is later atomically</p>
    <p>committed to GS.  Get (K, V): Return from LS, if already present;</p>
    <p>otherwise, fetch from GS and store in LS.  Other Op: Any thread local operation.</p>
    <p>The output of a CU is always committed to the GS before being visible to other CUs of the same or different type.  Eliminates the costly shuffle phase of MapReduce.</p>
  </div>
  <div class="page">
    <p>Design Principles  Optimistic concurrency control over pessimistic</p>
    <p>locking.  No locks are acquired. Write-buffer and read-set is</p>
    <p>validated against those of concurrent Trx assuring serializability.</p>
    <p>Client is potentially executing on the slowest node in the system; in this case, pessimistic locking hinders parallel transaction execution.</p>
    <p>Consistency (C) and Tolerance to Network Partitions (P) over Availability (A) in CAP Theorem for Distributed transactions.  Application correctness mandates strict consistency of</p>
    <p>execution. Relaxed consistency models are applicationspecific optimizations.</p>
    <p>Intermittent non-availability is not too costly for batchprocessing applications, where client is fault-prone in itself.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>We show performance gains on two applications, which are hitherto implemented sequentially without transactional support  Presence of Data dependencies.  Both exhibit Optimistic data-parallelism.</p>
    <p>Boruvkas MST  Each iteration is coded as a Map function with input</p>
    <p>as a node. Reduce is an identity function. Conflicting maps are serialized while others are executed in parallel.</p>
    <p>After n iterations of coalescing, we get the MST of an n node graph.</p>
    <p>A graph of 100 thousand nodes, with average degree of 50, generated based on the forest-fire model.</p>
  </div>
  <div class="page">
    <p>Boruvkas MST</p>
    <p>Speedup of 3.73 on 16 nodes, with less than 0.5 % re-executions due to aborts.</p>
  </div>
  <div class="page">
    <p>Maximum flow using Push-Relabel algorithm</p>
    <p>Each Map function executes a Push or a Relabel operation on the input node, depending on the constraints on its neighbors.</p>
    <p>Push operation increases the flow to a neighboring node and changes their Excess</p>
    <p>Relabel operation increases the height of the input node if it is the lowest among its neighbors.</p>
    <p>Conflicting Maps -- operating on neighboring nodes -- get serialized due to their transactional nature.</p>
    <p>Only sequential implementation possible without support for runtime conflict detection.</p>
  </div>
  <div class="page">
    <p>Speedup of 4.5 is observed on 16 nodes with 4% re-executions on a window of 40 iterations.</p>
  </div>
  <div class="page">
    <p>Conclusions  TransMR programming model enables data</p>
    <p>sharing in data-centric programming models for enhanced applicability.</p>
    <p>Similar to other data-centric programming models, the programmer only specifies operation on the individual data-element without concerning about its interaction with other operations.</p>
    <p>Prototype implementation shows that many important applications can be expressed in this model while extracting significant performance gains through increased parallelism.</p>
  </div>
  <div class="page">
    <p>Questions ?</p>
    <p>Thank You!</p>
  </div>
</Presentation>
