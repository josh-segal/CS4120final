<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Did the Model Understand the Question?</p>
    <p>Pramod Kaushik Mudrakarta</p>
    <p>joint work with Ankur Taly ( ), Mukund Sundararajan ( ), and Kedar Dhamdhere ( )</p>
    <p>PhD Student Intern</p>
  </div>
  <div class="page">
    <p>Read the question carefully!</p>
  </div>
  <div class="page">
    <p>Tabular QA</p>
    <p>Q: How many medals did India win? A: 197</p>
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Reading Comprehension</p>
    <p>Q: What is the name of the quarterback who was 38 in Super Bowl XXXIII? A: John Elway</p>
    <p>Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denvers Executive Vice President of Football Operations and General Manager</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA 1.0 dataset (state of the art = 66.7%)</p>
    <p>Neural Programmer (2016) 33.5% accuracy on WikiTableQuestions (state of the art)</p>
    <p>Yu et al (2018) model. 84.6 F-1 score on SQuAD (state of the art)</p>
  </div>
  <div class="page">
    <p>Tabular QA</p>
    <p>Q: How many medals did India win? A: 197</p>
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Reading Comprehension</p>
    <p>Q: What is the name of the quarterback who was 38 in Super Bowl XXXIII? A: John Elway</p>
    <p>Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denvers Executive Vice President of Football Operations and General Manager</p>
    <p>Have the models read the question carefully?</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA 1.0 dataset (state of the art = 66.7%)</p>
    <p>Neural Programmer (2016) 33.5% accuracy on WikiTableQuestions (state of the art)</p>
    <p>Yu et al (2018) model. 84.6 F-1 score on SQuAD (state of the art)</p>
  </div>
  <div class="page">
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA dataset (state of the art = 66.7%)</p>
  </div>
  <div class="page">
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How asymmetrical are the white bricks on either side of the building? A: very</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA dataset (state of the art = 66.7%)</p>
  </div>
  <div class="page">
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How asymmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How big are the white bricks on either side of the building? A: very</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA dataset (state of the art = 66.7%)</p>
  </div>
  <div class="page">
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How asymmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How big are the white bricks on either side of the building? A: very</p>
    <p>Q: How spherical are the white bricks on either side of the building? A: very</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA dataset (state of the art = 66.7%)</p>
  </div>
  <div class="page">
    <p>Visual QA</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How asymmetrical are the white bricks on either side of the building? A: very</p>
    <p>Q: How big are the white bricks on either side of the building? A: very</p>
    <p>Q: How spherical are the white bricks on either side of the building? A: very</p>
    <p>Q: How fast are the bricks speaking on either side of the building? A: very</p>
    <p>Kazemi and Elqursh (2017) model. 61.1% on VQA dataset (state of the art = 66.7%)</p>
  </div>
  <div class="page">
    <p>QA over tables</p>
    <p>Q: Which country won the most medals?</p>
    <p>Neural Programmer (2016) 33.5% validation accuracy on WikiTableQuestions dataset (state of the art)</p>
    <p>Neural Programmer: max(total), print(nation)</p>
  </div>
  <div class="page">
    <p>QA over tables</p>
    <p>Q: Which country won the most medals?</p>
    <p>Neural Programmer (2016) 33.5% validation accuracy on WikiTableQuestions dataset (state of the art)</p>
    <p>Neural Programmer: max(total), print(nation)</p>
    <p>A: Cuba</p>
  </div>
  <div class="page">
    <p>QA over tables</p>
    <p>Q: Which country won the most number of medals?</p>
    <p>Neural Programmer (2016) 33.5% validation accuracy on WikiTableQuestions dataset (state of the art)</p>
    <p>Neural Programmer: max(bronze), print(nation)</p>
    <p>A: Argentina</p>
  </div>
  <div class="page">
    <p>Test/dev accuracy does not show us the entire picture</p>
  </div>
  <div class="page">
    <p>Jia and Liang (2017): Adversarial Attacks on Reading Comprehension Models EMNLP 2017 Outstanding Paper Award</p>
    <p>Add an adversarial sentence to the paragraph to fool the model</p>
  </div>
  <div class="page">
    <p>Jia and Liang (2017): Adversarial Attacks on Reading Comprehension Models EMNLP 2017 Outstanding Paper Award</p>
    <p>Add an adversarial sentence to the paragraph to fool the model</p>
  </div>
  <div class="page">
    <p>Jia and Liang (2017): Adversarial Attacks on Reading Comprehension Models EMNLP 2017 Outstanding Paper Award</p>
    <p>Add an adversarial sentence to the paragraph to fool the model</p>
  </div>
  <div class="page">
    <p>Highly successful attacks: over 16 models, F1 score drops from 75% to 36%</p>
    <p>Their takeaway: reading comprehension models are overly stable; unable to distinguish a sentence that answers the question from one that merely has words common with the question</p>
    <p>Jia and Liang (2017): Adversarial Attacks on Reading Comprehension Models EMNLP 2017 Outstanding Paper Award</p>
  </div>
  <div class="page">
    <p>Highly successful attacks: over 16 models, F1 score drops from 75% to 36%</p>
    <p>Their takeaway: reading comprehension models are overly stable; unable to distinguish a sentence that answers the question from one that merely has words common with the question</p>
    <p>Question for us: How does overstability manifest? Why do their attacks work?</p>
    <p>Jia and Liang (2017): Adversarial Attacks on Reading Comprehension Models EMNLP 2017 Outstanding Paper Award</p>
  </div>
  <div class="page">
    <p>Our contributions</p>
    <p>A workflow based on attributions (word-importances) to understand</p>
    <p>input-output behavior of networks</p>
    <p>Identify weaknesses in the networks as suggested by attributions</p>
    <p>Craft adversarial examples by exploiting the weaknesses</p>
    <p>Explain and improve Jia and Liang (2017)s attacks</p>
  </div>
  <div class="page">
    <p>Attributions Problem statement: Attribute a complex deep networks prediction to input</p>
    <p>features, relative to a certain baseline (informationless) input</p>
    <p>E.g. : attribute an object recognition networks prediction to its pixels,</p>
    <p>a text sentiment networks prediction to individual words</p>
    <p>Explain F(input) - F(baseline) in terms of input features</p>
  </div>
  <div class="page">
    <p>Integrated Gradients (Sundararajan et al (2017), ICML)</p>
    <p>Why Integrated Gradients?  Axiomatic justification (see Sundararajan et al (2017) for details)  Ease of implementation; only gradient computations required  running time &lt; 0.5 seconds for a given input example 21</p>
  </div>
  <div class="page">
    <p>Visual QA attributions</p>
    <p>Q: How symmetrical are the white bricks on either side of the building? A: very</p>
    <p>How symmetrical are the white bricks on either side of the building?</p>
    <p>red: high attribution blue: negative attribution gray: near-zero attribution</p>
  </div>
  <div class="page">
    <p>Overstability Drop all words from the dataset except ones which are frequently top attributions E.g. How many players scored more than 10 goals?  How many</p>
  </div>
  <div class="page">
    <p>Overstability Drop all words from the dataset except ones which are frequently top attributions</p>
    <p>Visual QA</p>
    <p>color, many, what, how, doing, or, where, there,</p>
    <p>E.g. How many players scored more than 10 goals?  How many</p>
    <p>Neural Programmer</p>
    <p>many, tm_token, how, number, total, after,  24</p>
  </div>
  <div class="page">
    <p>Adversarial Examples</p>
  </div>
  <div class="page">
    <p>Stopword deletion attack Delete contentless words from the question</p>
    <p>show, tell, did, me, my, our, are, is, were, this, on, would, and, for, should, be, do, I, have, had, the, there, look, give, has, was, we, get, does, a, an, 's, that, by, based, in, of, bring, with, to, from, whole, being, been, want, wanted, as, can, see, doing, got, sorted, draw, listed, chart, only</p>
    <p>Neural Programmers accuracy falls from 33.5% to 28.5%</p>
    <p>VQA models accuracy falls from 61.1% to 52.0%</p>
  </div>
  <div class="page">
    <p>Subject ablation attack Replace the subject of a question with a low-attribution noun from the vocabulary</p>
    <p>Low-attribution nouns 'tweet',</p>
    <p>'childhood', 'copyrights', 'mornings', 'disorder',</p>
    <p>'importance', 'topless', 'critter', 'jumper',</p>
    <p>'fits'</p>
    <p>What is the man doing?  What is the tweet doing? How many children are there?  How many tweet are there?</p>
    <p>VQA models response remains same 75.6% of the time on questions that it originally answered correctly</p>
  </div>
  <div class="page">
    <p>Question concatenation attacks Prefix a content-free phrase to the question</p>
    <p>Neural Programmer Visual QA Original accuracy: 33.5% Original accuracy: 61.1%</p>
    <p>Low attribution words</p>
  </div>
  <div class="page">
    <p>Operator triggers in Neural Programmer</p>
  </div>
  <div class="page">
    <p>Question concatenation attacks Prefix a content-free phrase to the question</p>
    <p>Neural Programmer Visual QA Original accuracy: 33.5% Original accuracy: 61.1%</p>
    <p>Low-attribution words</p>
  </div>
  <div class="page">
    <p>Predicting the effectiveness of Jia and Liang (2017)s adversarial attacks</p>
    <p>Attacks are more likely to be effective when  High-attribution words are present in the adversarial sentence  Only low-attribution words are mutated</p>
    <p>red: high attribution, blue: negative attribution, gray: near-zero attribution 31</p>
  </div>
  <div class="page">
    <p>Summary  An attribution-based workflow to look inside and understand weaknesses of a model</p>
    <p>Explained how overstability manifests - QA networks do not focus on the right words!</p>
    <p>Crafted adversarial examples and improved Jia and Liang (2017)s attacks</p>
    <p>Outlook  Deep learning practitioners can easily use attributions to look inside models</p>
    <p>Adding soft network constraints</p>
    <p>E.g. add bias to attention vector so as to limit the influence of how, what, etc.</p>
    <p>Informed enrichment of datasets</p>
    <p>E.g. add more questions with word symmetrical such that answer is not very 32</p>
  </div>
  <div class="page">
    <p>If you would like to use our attribution-based workflow to understand your deep network/model</p>
    <p>https://github.com/pramodkaushik/acl18_results</p>
    <p>Contact me: pramodkm@uchicago.edu</p>
    <p>Ping me on Whova!</p>
    <p>Thank you!</p>
  </div>
</Presentation>
