<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Collaborative Ordinal Regression</p>
    <p>Shipeng Yu</p>
    <p>Joint work with Kai Yu, Volker Tresp and Hans-Peter Kriegel</p>
    <p>University of Munich, Germany Siemens Corporate Technology</p>
    <p>shipeng.yu@gmail.com</p>
  </div>
  <div class="page">
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>. . . . . . . . . . . . . . . . . . . .</p>
    <p>Superman The Pianist Star Wars</p>
    <p>The Matrix The Godfather</p>
    <p>American Beauty</p>
    <p>Motivations</p>
    <p>Very Dislike</p>
    <p>Very Like</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>?</p>
    <p>Genre</p>
    <p>Directors</p>
    <p>Actors</p>
    <p>Descriptions</p>
    <p>Ordinal Regression</p>
    <p>. . .</p>
    <p>Features Ratings</p>
  </div>
  <div class="page">
    <p>. . .</p>
    <p>. . . . . .</p>
    <p>. . . . . .</p>
    <p>. . . . . .</p>
    <p>. . . . . .</p>
    <p>. . . . . .</p>
    <p>. . . . . .</p>
    <p>Superman The Pianist Star Wars</p>
    <p>The Matrix The Godfather</p>
    <p>American Beauty</p>
    <p>Motivations (Cont.)</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>Features</p>
    <p>Collaborative Ordinal Regression</p>
    <p>. . .</p>
    <p>. . .</p>
    <p>Ratings</p>
    <p>? ? ?</p>
    <p>? ?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Goal: Assign ranks to objects</p>
    <p>Different from classification/regression problem  Binary classification: Has only 2 labels  Multi-class classification: Ignores ordering property  Regression: Only deals with real outputs</p>
    <p>Ranking Problem</p>
    <p>x1 2 x2 1 ...</p>
    <p>... xn r</p>
    <p>y</p>
    <p>Ordinal Regression</p>
    <p>Preference Learning</p>
    <p>xn  :::  x1  x2  :::</p>
    <p>x1  X   x2 X    ...</p>
    <p>... ...</p>
    <p>... ...</p>
    <p>xn    X</p>
    <p>lowest rank</p>
    <p>highest rank</p>
    <p>x1 xn</p>
    <p>x2 Rd</p>
  </div>
  <div class="page">
    <p>Goal: Assign ordered labels to objects</p>
    <p>Applications  User preference prediction  Web ranking for search engines</p>
    <p>Ordinal Regression</p>
    <p>Rd x1</p>
    <p>x2 xn</p>
    <p>f</p>
    <p>f (x)brbr 1b2b1b0 yny2 y1</p>
  </div>
  <div class="page">
    <p>Common in real world problems  Collaborative filtering: preference learning for multiple users  Web ranking: ranking of web pages for different queries</p>
    <p>Question: How to learn related ranking tasks jointly?</p>
    <p>One-task vs Multi-task</p>
    <p>Different ranking functions are</p>
    <p>correlated</p>
    <p>Each function only ranked part of data</p>
    <p>f1;f2;:::;fm Rd</p>
    <p>x1 x2</p>
    <p>xn f1</p>
    <p>f1(x)</p>
    <p>f2(x)</p>
    <p>fm(x)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Conditional model on ranking outputs  Ranking likelihood: Conditional on the latent function</p>
    <p>Prior: Gaussian Process prior for latent function</p>
    <p>Marginal ranking likelihood: Integrate out latent function values</p>
    <p>Ordinal regression likelihood</p>
    <p>Bayesian Ordinal Regression</p>
    <p>f  N(f;h;K )</p>
    <p>P(yjf;) = Q i P(yijf (xi);)</p>
    <p>P(yjX;f ;) = P(yjf (x1);:::;f (xn);) =P(yjf;)</p>
    <p>P(yjX;;h;K ) =</p>
    <p>Z P(yjf;)P(fjh;K )df</p>
  </div>
  <div class="page">
    <p>Need to define ranking likelihood  Example Model (1):</p>
    <p>GP Regression (GPR)  Assume a Gaussian form</p>
    <p>Regression on the ranking label directly</p>
    <p>Bayesian Ordinal Regression (1)</p>
    <p>P(yi jf (xi);)</p>
    <p>P(yi jf (xi);) / N(yi;f (xi); 2)</p>
  </div>
  <div class="page">
    <p>Need to define ranking likelihood  Example Model (2):</p>
    <p>GP Ordinal Regression (GPOR) (Chu &amp; Ghahramani, 2005)  A probit ranking likelihood</p>
    <p>Assign labels based on the surrounding area</p>
    <p>Bayesian Ordinal Regression (2)</p>
    <p>P(yi jf (xi);)</p>
    <p>b1 b2 b3 b4</p>
    <p>P(yijf (xi);) =  byi  f (xi )</p>
    <p>byi  1 f (xi )</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Multi-task Setting?</p>
    <p>Nave approach 1: Learn a GP model for each task  No share of information between tasks</p>
    <p>Nave approach 2: Fit one parametric kernel jointly  The parametric kernel is too restrictive to fit all tasks</p>
    <p>The collaborative effects  Common preferences:</p>
    <p>Functions share similar regression labels on some items  Similar variabilities:</p>
    <p>Functions tend to have same predictability on similar items</p>
  </div>
  <div class="page">
    <p>Collaborative Ordinal Regression</p>
    <p>Hierarchical GP model for multi-task ordinal regression  mean function:</p>
    <p>model common preferences  covariance matrix:</p>
    <p>model similar variabilities</p>
    <p>Both mean function and</p>
    <p>(non-stationary) covariance</p>
    <p>matrix are learned from data</p>
    <p>h;K</p>
    <p>f1 f2  fm</p>
    <p>y1 y2  ym</p>
    <p>x1 2 3  5 x2 1 1  2 ...</p>
    <p>... ... ...</p>
    <p>... xn 3 4  5</p>
    <p>GP Prior Ordinal</p>
    <p>Regression Likelihood</p>
  </div>
  <div class="page">
    <p>COR: The Model</p>
    <p>Hierarchical Bayes model on functions  All the latent functions are sampled from the same GP prior</p>
    <p>Allow different parameter settings for different tasks</p>
    <p>We may only observe part of rank labels for each function</p>
    <p>f j  N(f j ;h;K )</p>
    <p>P(DjX; ;h;K ) = mY</p>
    <p>j =1</p>
    <p>P(yj jX;j ;h;K ) = mY</p>
    <p>j =1</p>
    <p>Z P(yj jf j ;j )P(f j jh;K )df j</p>
  </div>
  <div class="page">
    <p>COR: The Key Points</p>
    <p>The GP prior connects all ordinal regression tasks  Model the first and second sufficient statistics</p>
    <p>The lower level features are incorporated naturally  More general than pure collaborative filtering</p>
    <p>We dont fix a parametric form for the kernel  Instead we assign the conjugate prior</p>
    <p>We can make predictions for new input data and new tasks</p>
    <p>P(h;K ) = N (h;h0; 1  K )I W(K ;;K 0)</p>
  </div>
  <div class="page">
    <p>Toy Problem (GPR Model)</p>
    <p>Mean function</p>
    <p>Mean rank</p>
    <p>labels</p>
    <p>New task prediction with base</p>
    <p>kernel (RBF)</p>
    <p>New task prediction</p>
    <p>with learned kernel</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Learning</p>
    <p>Variational lower bound</p>
    <p>EM Learning  E-step: Approximate each posterior as a Gaussian</p>
    <p>Estimate the mean vector and covariance matrix using EP  M-step: Fix and maximize w.r.t. and</p>
    <p>logP(DjX; ;h;K )  mX</p>
    <p>j =1</p>
    <p>Z Q(f j )log</p>
    <p>P(yj jf j ;j )P(f j jh;K ) Q(f j )</p>
    <p>df j</p>
    <p>Q(f j )</p>
    <p>Q(f j ) =N(f j ;f j ;K j )</p>
    <p>Q(f j ) (h;K )j</p>
  </div>
  <div class="page">
    <p>E-step</p>
    <p>The true posterior distribution factorizes:</p>
    <p>EP procedures  Deletion: Delete factor from the approximated Gaussian  Moments matching: Match moments by adding true likelihood  Update: Update the factor</p>
    <p>Can be done analytically for the example models  For GPR model the EP step is exact</p>
    <p>Q(f) / Q i P(yi jf (xi);)P(fjh;K )</p>
    <p>tk(X)</p>
    <p>tk(X)</p>
    <p>Approximate with Gaussian factor tk(X)</p>
  </div>
  <div class="page">
    <p>M-step</p>
    <p>Update GP prior:</p>
    <p>Does not depend on the form of ranking likelihood  The conjugate prior corresponds to a smooth term</p>
    <p>Update likelihood parameter  Do it separately for each task  Have the same update equation as the single-task case</p>
    <p>h = 1+m</p>
    <p>P m j =1</p>
    <p>f j +h0</p>
    <p>K = 1+m</p>
    <p>(h  h0)(h  h0)&gt; +K 0 +</p>
    <p>P m j =1</p>
    <p>h (f j  h)(f j  h)&gt; +K j</p>
    <p>i</p>
    <p>j</p>
  </div>
  <div class="page">
    <p>Inference</p>
    <p>Ordinal Regression</p>
    <p>Non-stationary kernel on test data is unknown!  Solution: work in the dual space (Yu et al. 2005)</p>
    <p>Posterior  By constraint , posterior  For test data we have</p>
    <p>f j  N(f j ;f j ;K j )</p>
    <p>f j =K j j  N(j ;K  1f j ;K</p>
    <p>1K j K  1)</p>
    <p>f j =k &gt;j  N(f j ;k</p>
    <p>&gt;K  1f j ;k &gt;K  1K j K</p>
    <p>1k)</p>
    <p>P(yj jx ;D;X;j ;h;K ) =</p>
    <p>Z P(yj jf</p>
    <p>j ;j )P(f</p>
    <p>j jx</p>
    <p>;D;X;h;K )df j</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Predict user ratings in movie data  MovieLens: 591 movies, 943 users</p>
    <p>19 features from the Genre part of each movie (binary)</p>
    <p>EachMovie: 1,075 movies, 72,916 users  23,753 features from online database (TF-IDF)</p>
    <p>Experimental Settings  Pick up 100 users with the most ratings as tasks  Randomly choose 10, 20, 50 ratings for each user for</p>
    <p>training  Base kernel: cosine similarity</p>
  </div>
  <div class="page">
    <p>Comparison Metrics</p>
    <p>Ordinal Regression Evaluation  Mean absolute error (MAE):</p>
    <p>Mean 0-1 error (MZOE):</p>
    <p>Use Macro &amp; Micro average over multiple tasks</p>
    <p>Ranking Evaluation  Normalized Discounted Cumulative Gain (NDCG):</p>
    <p>NDCG@10: Only count the top 10 ranked items</p>
    <p>MAE(R) = 1t P t i=1 jR(i)  R(i)j</p>
    <p>MZOE(R) = 1t P t i=1 1 R(i)6=R(i)</p>
    <p>NDCG(R) / P t k=1</p>
  </div>
  <div class="page">
    <p>Results - MovieLens</p>
    <p>N: Number of training items for each user  MMMF: Maximum Margin Matrix Factorization (Srebro et al 2005)</p>
    <p>State-of-the-art collaborative filtering model</p>
  </div>
  <div class="page">
    <p>Results - EachMovie</p>
    <p>N: Number of training items for each user  MMMF: Maximum Margin Matrix Factorization (Srebro et al 2005)</p>
    <p>State-of-the-art collaborative filtering model</p>
  </div>
  <div class="page">
    <p>New Ranking Functions</p>
    <p>Test on the rest users for MovieLens</p>
    <p>Use different kernels</p>
    <p>The more users we use for training, the better kernel we obtain!</p>
  </div>
  <div class="page">
    <p>Observations</p>
    <p>Collaborative models are always better than individual models  We can learn a good non-stationary kernel from users  GPR &amp; CGPR are fast in training and robust in testing</p>
    <p>Since there is no approximation  GPOR &amp; CGPOR are slow and sometimes overfit</p>
    <p>Due to the numerical M-step  We can use other ranking likelihood</p>
    <p>Then we may need to do numerical integration in EP step</p>
    <p>P(yijf (xi);)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivations  Ranking Problem  Bayesian Framework for Ordinal Regression  Collaborative Ordinal Regression  Learning and Inference  Experiments  Conclusion and Extensions</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>A Bayesian framework for multi-task ordinal regression</p>
    <p>An efficient EM-EP learning algorithm</p>
    <p>COR is better than individual OR algorithms</p>
    <p>COR is better than pure collaborative filtering</p>
    <p>Experiments show very encouraging results</p>
  </div>
  <div class="page">
    <p>Extensions</p>
    <p>The framework is applicable to preference learning  Collaborative version of GP preference learning (Chu &amp;</p>
    <p>Ghahramani, 2005)</p>
    <p>A probabilistic version of RankNet (Burges et al. 2005)</p>
    <p>GP mixture model for multi-task learning  Assign a Gaussian mixture model to each latent function  Prediction uses a linear combination of learned kernels  Connection to Dirichlet Processes</p>
    <p>P(yi  yj jf (xi)  f (xj )) / exp(f (xi ) f (xj )) 1+exp(f (xi ) f (xj ))</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Questions?</p>
  </div>
</Presentation>
