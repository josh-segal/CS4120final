<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Poseidon: An Efficient Communication Architecture for Distributed Deep</p>
    <p>Learning on GPU Clusters Hao Zhang</p>
    <p>Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting Hu, Jianliang Wei, Pengtao Xie, Eric P. Xing</p>
    <p>Petuum Inc. and Carnegie Mellon University</p>
  </div>
  <div class="page">
    <p>Deep Learning</p>
    <p>July 17 Hao Zhang 1</p>
    <p>Training data: images w/ labels</p>
    <p>Eagle</p>
    <p>Vulture</p>
    <p>Accipiter</p>
    <p>Osprey</p>
    <p>DL program</p>
  </div>
  <div class="page">
    <p>Forward through a Neural Network</p>
    <p>Essentially, A neural network is composed of a few layers  A layer in a NN is composed of a few (heavy) computational</p>
    <p>operations</p>
    <p>July 17 Hao Zhang 2</p>
    <p>Conv FC</p>
  </div>
  <div class="page">
    <p>Forward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Forward Pass: input , Loss</p>
    <p>July 17 Hao Zhang 3</p>
    <p>Forward pass</p>
  </div>
  <div class="page">
    <p>Forward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Forward Pass: input , Loss   Forward through the NN, one layer at a time  We use  ( = 1,,) to denote the th layer in a neural network</p>
    <p>July 17 Hao Zhang 4</p>
    <p>Forward pass</p>
  </div>
  <div class="page">
    <p>Forward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Forward pass: input   Forward through the NN, one layer at a time, until we get the loss   Denote the forward pass through layer  as an operation .</p>
    <p>July 17 Hao Zhang 5</p>
    <p>Forward pass</p>
    <p>/ 01</p>
  </div>
  <div class="page">
    <p>Backward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Backward pass: input is the loss</p>
    <p>July 17 Hao Zhang 6</p>
    <p>Backward pass</p>
  </div>
  <div class="page">
    <p>Backward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Backward pass derives the gradients of the parameters of a layer when backward through it</p>
    <p>July 17 Hao Zhang 7</p>
    <p>Backward pass</p>
  </div>
  <div class="page">
    <p>Backward through a Neural Network</p>
    <p>Backpropagation (BP) is a principled algorithm to train NNs  BP involves two passes through the network</p>
    <p>Backward derives the gradients of the parameters of a layer when backward through it</p>
    <p>Denote the backward pass through layer  as an operation .</p>
    <p>July 17 Hao Zhang 8</p>
    <p>Backward pass</p>
    <p>0/ 1</p>
  </div>
  <div class="page">
    <p>Stochastic Gradient Descent (SGD) via Backpropagation  Forward: sequentially executing /,1,,0  Backward: sequentially executing 0,03/,,/  Update: apply the gradients to update the model parameters  Repeat</p>
    <p>Formally, an iterative-convergence formulation</p>
    <p>01/</p>
    <p>01/</p>
    <p>Training a Neural Network</p>
    <p>Hao Zhang 9July 17</p>
    <p>Model parameters Forward</p>
    <p>Backward</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>Deep Learning on GPUs</p>
    <p>July 17 Hao Zhang 10</p>
    <p>Training data: images w/ labels</p>
    <p>Computational Worker w/ GPUs</p>
  </div>
  <div class="page">
    <p>Deep Learning on Distributed GPUs</p>
    <p>July 17 Hao Zhang 11</p>
    <p>Large scale Training data</p>
    <p>A Cluster of Workers with GPUs</p>
  </div>
  <div class="page">
    <p>Distributed Deep Learning</p>
    <p>Distributed DL: parallelize DL training using multiple machines.  i.e. we want to accelerate the heaviest workload (in the box) to</p>
    <p>multiple machines</p>
    <p>Forward</p>
    <p>Backward</p>
    <p>Data</p>
    <p>Forward and backward are the main computation (99%) workload of deep learning programs.</p>
    <p>Hao Zhang 12July 17</p>
  </div>
  <div class="page">
    <p>Data Parallelism with SGD</p>
    <p>We usually seek a parallelization strategy called data parallelism, based on SGD</p>
    <p>We partition data into different parts  Let different machines compute the gradient updates on different data</p>
    <p>partitions  Then aggregate/sync.</p>
    <p>Sync</p>
    <p>(one or more machines)</p>
    <p>Worker 1 Worker 2</p>
    <p>Worker 3 Worker 4</p>
    <p>Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Hao Zhang 13July 17</p>
  </div>
  <div class="page">
    <p>Data Parallel SGD</p>
    <p>Data-parallelism requires every worker to have read and write access to the shared model parameters , which causes communication among workers;</p>
    <p>Data partition p</p>
    <p>In total P workers</p>
    <p>Happening locally on each worker</p>
    <p>Collect and aggregate before application, where communication is required</p>
    <p>Hao Zhang 14July 17</p>
  </div>
  <div class="page">
    <p>Parameter Server</p>
    <p>Parameter server is a shared key-value storage that provides a shared access for the global model parameters  for ML</p>
    <p>The server is virtual  physically, it could be distributed (instead of centralized), i.e., a distributed key-value storage</p>
    <p>Worker 1 Worker 2</p>
    <p>Worker 3 Worker 4</p>
    <p>PS</p>
    <p>/ 1</p>
    <p>6 7</p>
    <p>Hao Zhang 15July 17</p>
    <p>Deal el al., 2012, Ho et al., 2013 Li et al., 2014 Cui et al., 2014, Cui et al., 2015 Wei et al., 2015 Zhang et al., 2015</p>
  </div>
  <div class="page">
    <p>Parameter Server for DL on GPUs</p>
    <p>Deep learning can be trivially data-parallelized over distributed workers using PS by 3 steps:</p>
    <p>Each worker computes the gradients (L) on their own data partition (:) and send them to remote servers;</p>
    <p>servers receive the updates and apply (+) them on globally shared parameters;</p>
    <p>Each worker pulls back the updated parameters (;)  However, directly applying PS for GPU-based distributed deep</p>
    <p>learning will underperform (as will show later).</p>
    <p>Hao Zhang 16July 17</p>
  </div>
  <div class="page">
    <p>Parameter Server on GPU Clusters</p>
    <p>What prevents the trivial realization of distributed DL on GPUs?  Communication challenges</p>
    <p>GPUs are at least one order of magnitude faster than CPUs  Ethernet has very limited bandwidth</p>
    <p>Hao Zhang 17July 17</p>
    <p>GPU are faster High CommLoad</p>
    <p>Memcpy overheads</p>
    <p>bottleneck</p>
    <p>Low GPU utilization</p>
    <p>Poor scalability with additional</p>
    <p>machines</p>
    <p>Bursty Communication</p>
    <p>Network latency</p>
    <p>GPU has dedicate memory</p>
    <p>Ethernet limitation</p>
    <p>Limited bandwidth</p>
  </div>
  <div class="page">
    <p>Train AlexNet, gradient generation rate 240M floats/(s*GPU)  61.5M float parameters, 0.25s/iter on Geforce Titan X (batchsize = 256)</p>
    <p>Parallelize it over 8 machines each w/ one GPU using PS.  To ensure the computation not blocked on GPU (i.e. linear</p>
    <p>speed-up with additional nodes)  Assume: every node holds 1/8 parameters as a PS shard  A node needs to transfer 240M * 7/8 * 4 = 840M floats/s = 26Gbps</p>
    <p>PS on GPU Clusters: an Example</p>
    <p>Hao Zhang 18July 17</p>
    <p>Figure from Krizhevsky et al. 2012</p>
  </div>
  <div class="page">
    <p>PS on GPU Clusters</p>
    <p>Lets see where we are</p>
    <p>Unfortunately, the problem will be more severe than described above  We only use 8 nodes (which is small). How about 32,128, or even 256?  We havent considered other issues, e.g.,</p>
    <p>Memory copy between DRAM and GPU will have a non-trivial cost [Cui et al. 2015]  The Ethernet might be shared with other tasks, i.e. available bandwidth is even less.  Burst communication happens very often on GPUs (which will explain later).</p>
    <p>Ethernet standards This is what the GPU workstation in most labs</p>
    <p>One of the most expensive instances AWS could provide you (18$/h?)</p>
    <p>Specialized hardware! Noncommodity anymore, unaffordable</p>
    <p>Hao Zhang 19July 17</p>
  </div>
  <div class="page">
    <p>Address the Communication Bottleneck</p>
    <p>A simple fact: communication time may be reduced, but cannot be eliminated (of course)</p>
    <p>Poseidons motivation: possible ideas to address the communication bottleneck</p>
    <p>Wait-free backpropagation (WFBP): hide the communication time by overlapping it with the computation time</p>
    <p>Hybrid communication (HybComm): (lossless) reduce the size of messages needed to be communications</p>
    <p>Hao Zhang 20July 17</p>
  </div>
  <div class="page">
    <p>Overlap Computation and Communication</p>
    <p>Recall on a single node the computational flow of BP  .: backpropagation computation through layer   ;: forward and backward computation at iteration t</p>
    <p>On multiple nodes, when communication is involved, we introduce two communication operations</p>
    <p>.: send out the gradients in layer  to the remote  .: pull back the globally shared parameters of layer  from the remote  ;: the set . .@/0 at iteration t  ;: the set . .@/0 at iteration t</p>
    <p>; ;B/</p>
    <p>01/</p>
    <p>;B1</p>
    <p>Hao Zhang 21July 17</p>
    <p>; ; ; ;B/ ;B/</p>
    <p>;B/</p>
    <p>01/ . .@/</p>
    <p>. .@/ 0</p>
    <p>Computation and communication happen sequentially!</p>
  </div>
  <div class="page">
    <p>WFBP: Wait-free backpropagation</p>
    <p>Idea: overlap computation and communication by utilizing concurrency</p>
    <p>Pipelining the updates and computation operations  Communication overhead is hidden under computation  Results: more computations in unit time</p>
    <p>01/</p>
    <p>061/</p>
    <p>061/</p>
    <p>01/</p>
    <p>. .@/ 0</p>
    <p>. .@/ 0</p>
    <p>reschedule</p>
    <p>Hao Zhang 22July 17</p>
    <p>; ; ; ;B/ ;B/</p>
    <p>; ; ;</p>
    <p>;B/</p>
    <p>;B/ ;B/ ;B/</p>
    <p>;B1 ;B1 ;B1</p>
    <p>;B6 ;B6 ;B6</p>
    <p>pipelining</p>
  </div>
  <div class="page">
    <p>How does WFBP perform?  Using Caffe as an engine:</p>
    <p>Using TensorFlow as engine</p>
    <p>WFBP: Wait-free Backpropagation</p>
    <p>Hao Zhang 23July 17</p>
    <p>Save your TensorFlow J</p>
  </div>
  <div class="page">
    <p>WFBP: Wait-free Backpropagation</p>
    <p>Does overlapping communication and computation solve all the problems?</p>
    <p>No, when communication time is longer than computation (see the figure below).</p>
    <p>Empirically, if communication and computation are perfectly overlapped, how many scalability we can achieve?</p>
    <p>; ; ;</p>
    <p>;</p>
    <p>Single node Distributed gap</p>
    <p>Hao Zhang 24July 17</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>A simple fact: communication time may be reduced, but cannot be eliminated (of course)</p>
    <p>Poseidons motivation: possible ideas to address the communication bottleneck</p>
    <p>Wait-free backpropagation (WFBP): hide the communication time by overlapping it with the computation time</p>
    <p>Hybrid communication (HybComm): Reduce the size of messages needed to be communicated</p>
    <p>Hao Zhang 25July 17</p>
  </div>
  <div class="page">
    <p>Sufficient Factor Broadcasting (SFB)</p>
    <p>Matrix-parametrized models (MPMs)</p>
    <p>Many MPMs have a good mathematical property  Full parameter matrix update W can be computed as outer product</p>
    <p>of two vectors G (called sufficient factors)</p>
    <p>#classes=325K</p>
    <p>Feature dim. = 20K</p>
    <p>Multiclass Logistic Regression Neural Network (AlexNet)</p>
    <p>#neurons in layer fc6=4096</p>
    <p>#neurons in layer fc7 =4096</p>
    <p>Hao Zhang 26July 17</p>
    <p>)();( 1</p>
    <p>min 1</p>
    <p>WhbWaf N</p>
    <p>N</p>
    <p>i iiiW +</p>
    <p>=</p>
    <p>T ( , ) ( )</p>
    <p>i i i</p>
    <p>i</p>
    <p>f Wa b W uv u v a</p>
    <p>Wa</p>
    <p>D = = =</p>
    <p>* * T</p>
    <p>N</p>
    <p>i iZ i f z h ZA</p>
    <p>N N= - +</p>
    <p>T i iW uv u z v aD = = D =</p>
  </div>
  <div class="page">
    <p>Sufficient Factor Broadcasting (SFB)</p>
    <p>Idea: Send lightweight SF updates (u,v), instead of expensive full-matrix W updates!</p>
    <p>Hao Zhang 27July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication: CNN</p>
    <p>Example: AlexNet CNN model  FC6 = 4096 * 30000 matrix (120M parameters)  Use SFB to communicate</p>
    <p>Decouple into two 4096 vectors: u, v  Transmit two vectors  Reconstruct the gradient matrix</p>
    <p>Hao Zhang 28</p>
    <p>Figure from Krizhevsky et al. 2012</p>
    <p>July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication: CNN</p>
    <p>Example: AlexNet CNN model  Convolutional layers = e.g. 11 * 11 matrix (121 parameters)  Use full-matrix updates to communicate</p>
    <p>SF decomposition does not save much</p>
    <p>Hao Zhang 29</p>
    <p>Figure from Krizhevsky et al. 2012</p>
    <p>July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication</p>
    <p>Idea  Sync FC layers using SFB  Sync Conv layer using PS</p>
    <p>Effectiveness  It directly reduces the size of</p>
    <p>messages in many situations  Is SFB always optimal?</p>
    <p>No, its communication load increases quadratically</p>
    <p>The right strategy: choose PS whenever it results in less communication</p>
    <p>Hao Zhang 30July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication</p>
    <p>How to choose? Where is the threshold?  Determine the best strategy depending on</p>
    <p>Layer type: CONV or FC?  Layer size  Batch size  # of Cluster nodes</p>
    <p>Hao Zhang 31July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication</p>
    <p>Hybrid communication algorithm</p>
    <p>Hao Zhang 32</p>
    <p>Determine the best strategy depending on  Layer type: CONV or FC?  Layer size: M, N  Batch size: K  # of Cluster nodes: /,1</p>
    <p>July 17</p>
  </div>
  <div class="page">
    <p>How does hybrid communication perform?  Using Caffe as an engine:</p>
    <p>Using TensorFlow as engine</p>
    <p>Hybrid Communication</p>
    <p>Hao Zhang 33July 17</p>
    <p>Improve over WFBP</p>
    <p>Improve over WFBP</p>
  </div>
  <div class="page">
    <p>Hybrid Communication</p>
    <p>More importantly, linear scalability on throughput, even with limited bandwidth!</p>
    <p>Make distributed deep learning affordable</p>
    <p>Hao Zhang 34July 17</p>
  </div>
  <div class="page">
    <p>Discussion: Utilizing SFs is not a new idea  Microsoft Adam uses the third strategy (c)</p>
    <p>Hybrid Communication</p>
    <p>PS SFB Push: SFs Pull: matrices</p>
    <p>Hao Zhang 35July 17</p>
  </div>
  <div class="page">
    <p>Hybrid Communication</p>
    <p>Problem: Adams strategy leads to communication bottleneck  Pushing SFs to server is fine  Pulling full matrices back will create a bottleneck on the server node.</p>
    <p>Hybrid communication yields communication load balancing  Which is important to address the problem of burst communication.</p>
    <p>Hao Zhang 36July 17</p>
  </div>
  <div class="page">
    <p>Poseidon System Architecture</p>
    <p>GPU CPU</p>
    <p>Stream Pool Thread Pool</p>
    <p>KV Store</p>
    <p>Synceri</p>
    <p>Coordinator</p>
    <p>SFB</p>
    <p>data flow allocation instruction</p>
    <p>KV Store</p>
    <p>Hao Zhang 37July 17</p>
  </div>
  <div class="page">
    <p>Poseidon as a Platform</p>
    <p>Poseidon: An efficient communication architecture  Efficient distributed platform for amplifying any DL toolkits  Preserve the programming interface for any high-level toolkits</p>
    <p>i.e. distribute the DL program without changing any line of code</p>
    <p>Poseidon</p>
    <p>toolkits</p>
    <p>platform</p>
    <p>Hao Zhang 38July 17</p>
  </div>
  <div class="page">
    <p>Summary: Take-home Messages</p>
    <p>Communication is a bottleneck in distributed DL on GPUs  GPUs are too fast  Ethernet has limited bandwidth and latency  Burst communication</p>
    <p>Poseidon is designed to alleviate this problem  WFBP: pipelining the synchronization and computation  Hybrid Communication: adaptive protocol to reduce the size of</p>
    <p>messages  Results:</p>
    <p>Linear throughput scalability across different dataset, model sizes, and hardware configuration (Ethernet bandwidth)</p>
    <p>A Platform to amplify existing DL toolboxes</p>
    <p>Hao Zhang 39July 17</p>
  </div>
  <div class="page">
    <p>Thank You! Q&amp;A</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
    <p>July 17 Hao Zhang 41</p>
  </div>
  <div class="page">
    <p>WFBP: Distributed Wait-free backpropagation</p>
    <p>Observation: Why DWBP is very effective in DL  More statistics of modern CNNs</p>
    <p>90% computation happens at bottom layers  90% communication happens at top layers  WFBP overlaps 90% communication with 90% computation</p>
    <p>Params/FLOP distribution of modern CNNs</p>
    <p>Hao Zhang 42July 17</p>
  </div>
  <div class="page">
    <p>Poseidon API</p>
    <p>KV Store, Syncer and Coordinator  Standard APIs similar to parameter server</p>
    <p>Push/Pull API for parameter synchronization  BestScheme method to return the best communication method</p>
    <p>July 17 Hao Zhang 43</p>
  </div>
  <div class="page">
    <p>Comparison: 1-bit Quantization</p>
    <p>Micrsoft CNTK has 1-bit quantization technique to lossy parameter compression</p>
    <p>While our experiments reveal that it might not work well in terms of statistical convergence in some domains</p>
    <p>Good news! Some recent works report exciting results on this line</p>
    <p>July 17 Hao Zhang 44</p>
  </div>
  <div class="page">
    <p>Results: Statistical Convergence</p>
    <p>Poseidon adopts fully synchronous consistency model  Distributed training = larger batch size  Turning parameters in distributed settings is an open problem</p>
    <p>Linear convergence speedup on ResNet-152 [He et al. 2015]</p>
    <p>July 17 Hao Zhang 45</p>
  </div>
  <div class="page">
    <p>Discussion: Synchronous vs. Asynchronous</p>
    <p>Empirical results: synchronous updates yield the faster periteration convergence in training modern deep nearal networks</p>
    <p>GeePS, Cui et al., Eurosys16  TensorFlow, Abadi et al., OSDI16  Poseidon, Zhang et al., ATC16, 17</p>
    <p>Stragglers  Yes, stragglers exist. However, wed still prefer synchronous training</p>
    <p>because the cost is less than having asynchrony  Does/will asynchronous training work?</p>
    <p>Yes, but it is domain-specific, e.g. in some speech/NLP application, there are some reported results that asynchrounous training yields same-quality results.</p>
    <p>There are more and more ongoing research towards this direction in both machine learning and systems.</p>
    <p>July 17 Hao Zhang 46</p>
  </div>
</Presentation>
