<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep Learning for NLP (without Magic)</p>
    <p>Richard Socher, Yoshua Bengio and Chris Manning</p>
    <p>ACL 2012</p>
  </div>
  <div class="page">
    <p>Deep Learning Most of current machine learning learns the predic:ve value (the weights) of human-given features</p>
    <p>Representa:on learning systems aBempt to automa:cally learn good representa:ons (or features)</p>
    <p>Deep learning algorithms aBempt to learn mul:ple levels of representa:on of increasing complexity/abstrac:on</p>
    <p>A system with such a sequence is a deep architecture</p>
    <p>The vast majority of such work has explored deep belief networks (DBNs) which are Markov Random Fields with mul:ple layers</p>
    <p>Several other variants of deep learning have seen a revival due to improved op:miza:on methods of mul:ple layer neural networks</p>
  </div>
  <div class="page">
    <p>A Deep Architecture</p>
    <p>Output layer</p>
    <p>Here predic:ng a supervised target</p>
    <p>Hidden layers</p>
    <p>These learn more abstract representa:ons as you head up</p>
    <p>Input layer</p>
    <p>This has raw sensory inputs (roughly) 3</p>
  </div>
  <div class="page">
    <p>Six Reasons to Explore Deep Learning</p>
    <p>Part 1.1: The Basics</p>
  </div>
  <div class="page">
    <p>#1 Learning features, not just handcrafting them</p>
    <p>Most NLP systems use very carefully hand-designed features and representa:ons</p>
    <p>Many of us are very experienced  and good  at such feature design</p>
    <p>In this world, machine learning reduces mostly to linear models (including CRFs) and nearest-neighbor-like features/ models (including n-grams)</p>
    <p>Hand-cra\ing features is :me-consuming and briBle; the features are o\en over-specified and incomplete</p>
  </div>
  <div class="page">
    <p>How can we automatically learn good features?</p>
    <p>We need to move the scope of machine learning beyond hand-cra\ed features and simple ML</p>
    <p>Humans develop representa:ons to enable learning and reasoning; our computers should do the same</p>
    <p>Deep learning provides a mechanism for learning good features and representa:ons</p>
    <p>Handcra@ed features can be combined with learned features, or new more abstract features learned on top of handcra@ed features 6</p>
  </div>
  <div class="page">
    <p>#2 The need for distributed representations</p>
    <p>Current NLP systems are incredibly fragile because of their atomic symbol representa:ons</p>
    <p>Subject Reduced that clause complement 7</p>
  </div>
  <div class="page">
    <p>#2 The need for distributed representations</p>
    <p>Learned word representa:ons, which model similari:es, help all NLP tasks enormously</p>
    <p>E.g., distribu:onal similarity based word clusters greatly help all applica:ons</p>
    <p>E.g., paraphrasing, word-sense disambigua:on, language modeling</p>
  </div>
  <div class="page">
    <p>Learning a set of features that are not mutually exclusive can be exponen:ally more efficient than nearest-neighbor-like or clustering-like models</p>
    <p>#2 The need for distributed representations</p>
    <p>Mul:- Clustering</p>
    <p>Clustering</p>
  </div>
  <div class="page">
    <p>#3 Unsupervised feature and weight learning</p>
    <p>Today, most prac:cal, good machine learning methods require labeled training data</p>
    <p>But almost all data is unlabeled</p>
    <p>The brain needs to learn about 1014 connec:on weights</p>
    <p>in about 109 seconds</p>
    <p>Labels cannot possibly provide enough informa:on</p>
    <p>(unless the weights were highly redundant)</p>
    <p>Most informa:on acquired in an unsupervised fashion 10</p>
  </div>
  <div class="page">
    <p>#4 Learning multiple levels of representation</p>
    <p>There is theore:cal and empirical evidence in favor of mul:ple levels of representa:on</p>
    <p>ExponenHal gain for some families of funcHons</p>
    <p>Biologically inspired learning</p>
    <p>Brain has a deep architecture</p>
    <p>Cortex seems to have a generic learning algorithm</p>
    <p>Humans first learn simpler concepts and then compose them to more complex ones</p>
  </div>
  <div class="page">
    <p>#4 Learning multiple levels of representation</p>
    <p>Successive model layers learn deeper intermediate representa:ons</p>
    <p>Layer 1</p>
    <p>Layer 2</p>
    <p>Layer 3 High-level</p>
    <p>linguis:c representa:ons</p>
    <p>[Lee, Largman, Pham &amp; Ng, NIPS 2009]</p>
  </div>
  <div class="page">
    <p>#4 Handling the recursivity of human language</p>
    <p>Human languages, ideas, and ar:facts are composed from simpler components</p>
    <p>Recursion: the same operator (same parameters) is applied repeatedly on different states/components of the computa:on</p>
    <p>xt-1 xt xt+1</p>
    <p>zt-1 zt zt+1</p>
  </div>
  <div class="page">
    <p>#4 Using a deep architecture  Deep architectures learn</p>
    <p>good intermediate representa:ons that can be shared across tasks</p>
    <p>Insufficient depth of representa:on can be exponen:ally inefficient</p>
    <p>Mul:ple levels of latent variables allow combinatorial sharing of sta:s:cal strength</p>
    <p>raw input x</p>
    <p>task 1 output y1</p>
    <p>task 3 output y3</p>
    <p>task 2 output y2 Task A Task B Task C</p>
    <p>Linguis:c input</p>
  </div>
  <div class="page">
    <p>#5 The curse of dimensionality To generalize locally (e.g.,</p>
    <p>nearest neighbors), we need representa:ve examples for all relevant varia:ons!</p>
    <p>We cannot hope to learn from raw signals or random bases</p>
    <p>Classical solu:ons:  Manual feature design  Hoping for a smooth enough target func:on (such as assuming a linear model) to allow simple predic:on</p>
  </div>
  <div class="page">
    <p>#5 Solving the curse of dimensionality</p>
    <p>Three approaches dominate recent work: (Log-)Linear models</p>
    <p>Flexibility achieved by adding more manually engineered features</p>
    <p>Kernel methods The basis func:ons are associated with data points, limi:ng complexity Selec:on of a subset of data points further limits complexity Output is linear in the output of near-neighbor detectors (kernel fn)</p>
    <p>Neural networks Parameterize and learn the kernel Can be nested to create a hierarchy of abstrac:on levels</p>
  </div>
  <div class="page">
    <p>#5 Solving the curse of dimensionality</p>
    <p>We need to build composi:onality into our ML models</p>
    <p>Just as human languages exploit composi:onality to give representa:ons and meanings to complex ideas</p>
    <p>Exploi:ng composi:onality gives an exponen:al gain in representa:onal power</p>
    <p>Distributed representa:ons / embeddings: feature learning</p>
    <p>Deep architecture: mul:ple levels of feature learning</p>
  </div>
  <div class="page">
    <p>#6 Why now? Despite prior inves:ga:on and understanding of many of the algorithmic techniques</p>
    <p>Before 2006 training deep architectures was unsuccessful (except for convolu:onal neural nets when used by people with French names)</p>
    <p>What has changed?</p>
    <p>New methods for unsupervised pre-training have been developed (Restricted Boltzmann Machines = RBMs, autoencoders, etc.)</p>
    <p>More efficient parameter es:ma:on methods  BeBer understanding of model regulariza:on</p>
  </div>
  <div class="page">
    <p>#6 Deep NLP Learning models already work well</p>
    <p>Neural Language Model [Mikolov et al. Interspeech 2011]</p>
    <p>State of the art performance for POS, NER, Chunking [Collobert et al. 2011]</p>
    <p>Model Eval WER (WSJ task)</p>
    <p>KN5 Baseline 17.2</p>
    <p>Discrimina:ve LM 16.9</p>
    <p>Recurrent NN combina:on 14.4</p>
  </div>
  <div class="page">
    <p>#6 Deep NLP Learning models already work well</p>
    <p>Sen:ment analysis of Sentences [Socher et al. 2011a]</p>
    <p>Paraphrase Detec:on [Socher et al. 2011b]</p>
    <p>Rela:on Classifica:on [Socher et al. 2012]</p>
    <p>Bench- mark</p>
    <p>Recursive NN</p>
    <p>MPQA sen:ment 86.1 86.4</p>
    <p>MSFT Paraphrase 82.3 83.6</p>
    <p>SemEval Rela:on 82.2 82.4</p>
  </div>
  <div class="page">
    <p>MSR MAVIS Speech System Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recogni:on (Dahl, Yu, Deng &amp; Acero TASLP 2012)</p>
    <p>Conversa:onal Speech Transcrip:on Using Context-Dependent Deep Neural Networks (Seide, Li &amp; Yu, Interspeech 2011)</p>
    <p>#6 Deep NLP Learning models already work well</p>
  </div>
  <div class="page">
    <p>#6 Why now? Deep learning models can now be very fast in some circumstances</p>
    <p>SENNA can do POS or NER extremely fast (16x to 122x) compared to other SOTA taggers, using 25x less memory</p>
    <p>Changes in compu:ng technology favor deep learning  In NLP, speed has tradi:onally come from exploi:ng sparsity  But with modern machines, branches, and widely spread memory accesses are costly</p>
    <p>Uniform parallel opera:ons on dense vectors are faster</p>
    <p>These trends hold even more strongly when using GPUs</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Outline of the Tutorial</p>
  </div>
  <div class="page">
    <p>Outline of the Tutorial</p>
  </div>
  <div class="page">
    <p>Outline of the Tutorial</p>
  </div>
  <div class="page">
    <p>From logistic regression to neural nets</p>
    <p>Part 1.2: The Basics</p>
  </div>
  <div class="page">
    <p>Demystifying neural networks</p>
    <p>Neural networks come with their own terminological baggage (of neurons, ac:va:on func:ons, and weight decay)</p>
    <p>just like SVMs</p>
    <p>But if you understand how maxent/logis:c regression models work</p>
    <p>Then you already understand the opera:on of a basic neural network neuron!</p>
    <p>A single neuron A computa:onal unit with n (3) inputs</p>
    <p>and 1 output and parameters W, b</p>
    <p>Ac:va:on func:on</p>
    <p>Inputs</p>
    <p>Bias unit corresponds to intercept term</p>
    <p>Output</p>
  </div>
  <div class="page">
    <p>From Maxent Classifiers to Neural Networks</p>
    <p>In NLP, a maxent classifier is normally wriBen as:</p>
    <p>Supervised learning gives us a distribu:on for datum d over classes in C</p>
    <p>Vector form:  Such a classifier is some:mes used as-is in a neural network</p>
    <p>O\en as the top layer (a so\max layer)  But for now well derive a two-class logis:c model for one neuron</p>
    <p>P(c |d,)= exp i fi(c,d)i exp i fi( &quot;c ,d)i&quot;c C</p>
    <p>P(c |d,)= e f (c,d)</p>
    <p>e f ( &quot;c ,d) &quot;c</p>
  </div>
  <div class="page">
    <p>From Maxent Classifiers to Neural Networks</p>
    <p>Vector form:</p>
    <p>Make two class:</p>
    <p>P(c |d,)= e f (c,d)</p>
    <p>e f ( &quot;c ,d) &quot;c</p>
    <p>P(c1 |d,)= e f (c1,d)</p>
    <p>e f (c1,d) +e f (c2,d) =</p>
    <p>e f (c1,d)</p>
    <p>e f (c1,d) +e f (c2,d)  e f (c1,d)</p>
    <p>e f (c1,d)</p>
    <p>P(c2 |d,)= e f (c2,d)</p>
    <p>e f (c1,d) +e f (c2,d) =</p>
    <p>e f (c2,d)</p>
    <p>e f (c1,d) +e f (c2,d)  e f (c1,d)</p>
    <p>e f (c1,d)</p>
    <p>= 1</p>
    <p>= e[ f (c2,d) f (c1,d)</p>
    <p>ex</p>
    <p>for x = f (c1, d) f (c2, d) 1</p>
  </div>
  <div class="page">
    <p>From Maxent Classifiers to Neural Networks</p>
    <p>Output of one class:</p>
    <p>We o\en have an always on feature for a class, which gives a class prior. We can separate it out as a bias term:</p>
    <p>Or we can have x1 be an always-on input  We separate things into the vector dot product and applying a</p>
    <p>non-linearity. Let f(z) = 1/(1 + exp(-z)), the logis:c func:on.</p>
    <p>Then:</p>
    <p>P(c1 | x,)= 1</p>
    <p>P(c1 | x,)= 1</p>
    <p>P(c1 | x,)= f (  x+b) 31</p>
  </div>
  <div class="page">
    <p>This is exactly what a neuron computes</p>
    <p>hw,b(x)= f (w  x+b)</p>
    <p>f (z)= 1</p>
    <p>w, b are the parameters of this neuron i.e., this logis:c regression model</p>
  </div>
  <div class="page">
    <p>A neural network = running several logistic regressions at the same time</p>
    <p>If we feed a vector of inputs through a bunch of logis:c regression func:ons, then we get a vector of outputs</p>
    <p>But we dont have to decide ahead of :me what variables these logis:c regressions are trying to predict!</p>
  </div>
  <div class="page">
    <p>A neural network = running several logistic regressions at the same time</p>
    <p>which we can feed into another logis:c regression func:on</p>
    <p>and it is the training criterion that will decide what those intermediate binary target variables should be, so as to make a good job of predic:ng the targets for the next layer, etc.</p>
  </div>
  <div class="page">
    <p>A neural network = running several logistic regressions at the same time</p>
    <p>Before we know it, we have a mul:layer neural network.</p>
  </div>
  <div class="page">
    <p>Matrix notation for a layer</p>
    <p>We have</p>
    <p>In matrix nota:on</p>
    <p>where f is applied element-wise:</p>
    <p>a1</p>
    <p>a2</p>
    <p>a3</p>
    <p>a1 = f (W11x1 +W12x2 +W13x3 +b1) a2 = f (W21x1 +W22x2 +W23x3 +b2) etc.</p>
    <p>z =Wx+b a = f (z)</p>
    <p>f ([z1,z2,z3])=[ f (z1), f (z2), f (z3)] 36</p>
  </div>
  <div class="page">
    <p>How do we train the weights w?</p>
    <p>For a single layer neural net, we can train the model just like a logis:c regression model</p>
    <p>We can do stochas:c gradient descent  We can use fancier methods, as we commonly do for maxent models like conjugate gradient or L-BFGS</p>
    <p>For a mul:layer net it is poten:ally more complex because the internal (hidden) logis:c units make the func:on non-convex  just as for hidden CRFs [QuaBoni et al. 04, Gunawardana et al. 05]</p>
    <p>But we use the same ideas  This leads into backpropaga:on, which we cover later</p>
  </div>
  <div class="page">
    <p>Non-linearities: Why theyre needed</p>
    <p>For logis:c regression, theyre mo:vated by mapping to probabili:es: [0,1]</p>
    <p>Here, theyre mo:vated by being able to do func:on approxima:on  Without non-lineari:es, neural networks cant do anything more than a linear transform: extra layers could just be compiled down into a single linear transform</p>
    <p>The probabilis:c interpreta:on for hidden units is usually unnecessary except in the Boltzmann machine models.</p>
  </div>
  <div class="page">
    <p>Non-linearities: Whats used</p>
    <p>logis:c (sigmoid) tanh</p>
    <p>tanh is just a rescaled and shi\ed sigmoid (2  as steep, [1,1]):</p>
    <p>tanh is what is most used and o\en performs best for deep nets [Glorot and Bengio AISTATS 2010]</p>
    <p>tanh(z)= 2logistic(2z)1</p>
  </div>
  <div class="page">
    <p>Non-linearities: There are various other choices</p>
    <p>hard tanh so\ sign rec:fier</p>
    <p>hard tanh is a mathema:cally awkward but computa:onally cheap tanh  [Glorot and Bengio AISTATS 2010] discuss uses of so\sign and rec:fier</p>
    <p>rect(z)= max(z,0)softsign(z)= a</p>
  </div>
  <div class="page">
    <p>Summary: Knowing the meaning of words!</p>
    <p>You should now understand the basics and how they relate to other models you use</p>
    <p>Neuron = logis:c regression or similar func:on  Input layer = input training/test vector  Bias unit = intercept term  Ac:va:on func:on is a sigmoid (or similar nonlinearity)  Ac:va:on = response  Backpropaga:on = running stochas:c gradient descent across a</p>
    <p>mul:layer network</p>
    <p>Weight decay = regulariza:on / Bayesian prior</p>
  </div>
  <div class="page">
    <p>Effective deep learning became possible through unsupervised pre-training</p>
    <p>[Erhan et al., JMLR 2010]</p>
    <p>Purely supervised neural net With unsupervised pre-training</p>
    <p>(with RBMs and Denoising Auto-Encoders)</p>
  </div>
  <div class="page">
    <p>Word Representations Part 1.3: The Basics</p>
  </div>
  <div class="page">
    <p>Word representations</p>
    <p>What is the main source of informa:on in NLP?</p>
    <p>Words.</p>
    <p>How do most systems handle words?</p>
    <p>Not very well.</p>
  </div>
  <div class="page">
    <p>The standard word representation</p>
    <p>The vast majority of NLP work regards words as atomic symbols: hotel, conference, walk</p>
    <p>Regardless of whether it is rule-based NLP or sta:s:cal NLP</p>
    <p>In vector space terms, this is a vector with one 1 and a lot of zeroes</p>
    <p>[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] Dimensionality: 20K (speech)  50K (PTB)  500K (big vocab)  3M (Google 1T)</p>
    <p>We call this a one-hot representa:on</p>
    <p>Can one do beBer? 45</p>
  </div>
  <div class="page">
    <p>The standard word representation</p>
    <p>A symbolic representa:on looks ridiculous as a vector</p>
    <p>But its what vector space model IR actually uses (conceptually)</p>
    <p>motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]</p>
    <p>AND hotel [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]</p>
    <p>= 0</p>
  </div>
  <div class="page">
    <p>How does conventional IR try to solve this problem?</p>
    <p>Query expansion (synonyms)</p>
    <p>EXPAND(motel) [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0] AND hotel [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] So\er methods like pseudo-relevance feedback</p>
    <p>PRF(motel) [0 0 0 0 0.1 0 0 0.2 0 0 1 0 0 0 0.4] AND hotel [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] Making representa:ons for whole documents, which are denser</p>
    <p>motel [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] AND doc vec [0 0 1 0 3 0 0 2 0 0 1 1 0 0 5]</p>
  </div>
  <div class="page">
    <p>Dealing with the Sparsity of language</p>
    <p>This problem of seeing connec:ons between words only becomes worse when we move to dealing with rela:ons</p>
    <p>between word pairs and triples</p>
    <p>Language modeling methods of backoff and interpola:on provide a poor way of dealing with a lack of informa:on</p>
    <p>about word similarity</p>
    <p>We need a method that generalizes based on other words that are seman:cally and syntac:cally similar</p>
  </div>
  <div class="page">
    <p>Distributional similarity based representations</p>
    <p>You can get a lot of value by represen:ng a word by means of its neighbors</p>
    <p>One of the most successful ideas of modern sta:s:cal NLP</p>
    <p>You shall know a word by the company it keeps (J. R. Firth 1957: 11)</p>
    <p>government debt problems turning into banking crises as has happened in</p>
    <p>saying that Europe needs unified banking regulation to replace the hodgepodge</p>
    <p>These words will represent banking</p>
  </div>
  <div class="page">
    <p>Distributional similarity based representations</p>
    <p>You shall know a word by the company it keeps (J. R. Firth 1957: 11)</p>
    <p>Methods vary in the kind of context they exploit</p>
    <p>Using a word or a few words to the le\ and/or right gives largely syntac:c word classes, with seman:c coloring</p>
    <p>players  musicians</p>
    <p>Using the whole document gives much more topical seman:c similarity</p>
    <p>inning  homered</p>
  </div>
  <div class="page">
    <p>Class-based (hard clustering) word representations</p>
    <p>These models learn word classes based on distribu:onal informa:on, roughly as a class HMM</p>
    <p>Brown clustering (Brown et al. 1992)  Exchange clustering (Mar:n et al. 1998, Clark 2003)</p>
    <p>Words are similar if they are put in the same class</p>
    <p>This is a simplis:c no:on of similarity, but it is sufficient to give a very useful desparsifica:on of word data</p>
    <p>Its not what were talking about today, but they are an example of unsupervised pre-training  And, in prac:ce, these are s:ll great features!</p>
  </div>
  <div class="page">
    <p>Soft clustering word representations</p>
    <p>These models learn for each cluster/dimension/factor a distribu:on over words as probabili:es or strengths</p>
    <p>Latent Seman:c Analysis (LSA/LSI)  Random projec:ons  Latent Dirichlet Analysis (LDA)  HMM clustering</p>
    <p>Broadly, neural word embeddings are in this space, combining vector space seman:cs with the predic:on of probabilis:c models</p>
    <p>(Bengio et al. 2003, Collobert &amp; Weston 2008, Turian et al. 2010)</p>
    <p>} Probabilis:c } Vector space</p>
  </div>
  <div class="page">
    <p>Distributed Representations</p>
    <p>In all of these approaches, including deep learning models, a word is represented as a dense vector:</p>
    <p>linguis;cs =</p>
    <p>[ ] Some approaches aim for some sparsity in the vector. In probabilis:c approaches, all the numbers are non-nega:ve.</p>
  </div>
  <div class="page">
    <p>Distributed Representations</p>
    <p>These are distributed representaHons: each word has a weigh:ng in each dimension or cluster</p>
    <p>In contrast to the the atomic or localist representa:ons employed in most of NLP, a distributed representa:on is one in which each en:ty is represented by a paBern of ac:vity distributed over many compu:ng elements, and each compu:ng element is involved in represen:ng many different en::es</p>
    <p>(Hinton Distributed representa:ons CMU-CS-84-157, 1984)</p>
  </div>
  <div class="page">
    <p>Neural word embeddings</p>
  </div>
  <div class="page">
    <p>Neural word embeddings</p>
  </div>
  <div class="page">
    <p>Neural word embeddings</p>
    <p>Most similar words to a few words</p>
    <p>France Jesus XBOX Reddish Scratched</p>
    <p>Spain Christ Playsta:on Yellowish Smashed</p>
    <p>Italy God Dreamcast Greenish Ripped</p>
    <p>Russia Resurrec:on PS### Brownish Brushed</p>
    <p>Poland Prayer SNES Bluish Hurled</p>
    <p>England Yahweh WH Creamy Grabbed</p>
    <p>Denmark Josephus NES Whi:sh Tossed</p>
    <p>Germany Moses Nintendo Blackish Squeezed</p>
    <p>Portugal Sin Gamecube Silvery Blasted</p>
    <p>Sweden Heaven PSP Greyish Tangled</p>
    <p>Austria Salva:on Amiga Paler Slashed</p>
    <p>(Collobert &amp; Weston, ICML 2008)</p>
    <p>France Spain</p>
    <p>Italy England</p>
    <p>Denmark Germany</p>
    <p>Jesus Christ God</p>
    <p>Prayer Sin</p>
  </div>
  <div class="page">
    <p>Advantages of the neural word embedding approach</p>
    <p>Compared to a method like LSA:  A neural embedding can learn higher-level features/abstrac:ons</p>
    <p>(beyond the word)</p>
    <p>Learning such an embedding forces a representa:on on the words themselves that is beBer and more meaningful</p>
    <p>Because everything is trained together  By adding supervision from either one task or mul:ple tasks</p>
    <p>simultaneously, we can improve the representa:on of the words for handling language analysis tasks</p>
  </div>
  <div class="page">
    <p>Unsupervised word vector learning</p>
    <p>Part 1.4: The Basics</p>
  </div>
  <div class="page">
    <p>A neural network for learning word vectors (Collobert &amp; Weston JMLR 2011)</p>
    <p>Idea: A word and its context is a posi:ve training sample; a random word in that same context gives a nega:ve training sample:</p>
    <p>cat chills on a mat cat chills Jeju a mat</p>
    <p>Similar: Implicit nega:ve evidence in Contras:ve Es:ma:on, Smith and Eisner (2005)</p>
  </div>
  <div class="page">
    <p>A neural network for learning word vectors</p>
    <p>Idea: A word and its context is a posi:ve training sample, a random word in that same context is a nega:ve training sample.</p>
    <p>score(cat chills on a mat) &gt; score(cat chills Jeju a mat)  How to compute the score?</p>
    <p>With a neural network  Each word is associated with an</p>
    <p>n-dimensional vector</p>
  </div>
  <div class="page">
    <p>Word embedding matrix</p>
    <p>Ini:alize all word vectors randomly to form a word embedding matrix |V|</p>
    <p>L =  n</p>
    <p>the cat mat</p>
    <p>These are the word features we want to learn  Also called a look-up table</p>
    <p>Conceptually you get a words vector by le\ mul:plying a one-hot vector o by L: x = Lo</p>
    <p>[ ]</p>
  </div>
  <div class="page">
    <p>score(cat chills on a mat)  To describe a phrase, retrieve (via index) the corresponding</p>
    <p>vectors from L</p>
    <p>cat chills on a mat</p>
    <p>Then concatenate them to (5n) vector:  x =[ ]  How do we then compute score(x)?</p>
    <p>Word vectors as input to a neural network</p>
  </div>
  <div class="page">
    <p>A Single Layer of a Neural Network</p>
    <p>A single layer is a combina:on of a linear layer and a nonlinearity:</p>
    <p>The neural ac:va:ons can then be used to compute some func:on.</p>
    <p>For instance, the score we care about:</p>
  </div>
  <div class="page">
    <p>Summary: Feed-forward Computation</p>
    <p>Compu:ng a windows score with a 3-layer Neural Net: s = score(cat chills on a mat)</p>
    <p>cat chills on a mat</p>
  </div>
  <div class="page">
    <p>Summary: Feed-forward Computation</p>
    <p>s = score(cat chills on a mat)  sc = score(cat chills Jeju a mat)</p>
    <p>Idea for training objec:ve: make score of true window larger and corrupt windows score lower (un:l theyre good enough): minimize</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>Assuming cost is &gt; 0, we compute the deriva:ves of s and sc wrt all the involved variables: Wscore, W, b, x</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>Lets consider the deriva:ve of a single weight Wij</p>
    <p>This only appears inside ai  For example: W23 is only used to compute a2</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>Deriva:ve of weight Wij:</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>Deriva:ve of single weight Wij:</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
    <p>Local error signal</p>
    <p>Local input signal</p>
  </div>
  <div class="page">
    <p>We want all combina:ons of i=1,2 and j=1,2,3</p>
    <p>Solu:on: Outer product:  where is the</p>
    <p>responsibility coming from each ac:va:on a</p>
    <p>Training with Backpropagation</p>
    <p>From single weight Wij to full W:</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>For biases b, we get:</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>We just almost learned the backpropaga:on rule by carefully taking deriva:ves and using the chain rule!</p>
    <p>The only remaining trick is to re-use deriva:ves that we computed once, for lower layers.</p>
    <p>Lets apply that idea for the last deriva:ves of this model, the word vectors in x.</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>Take deriva:ve of score with respect to single word vector (for simplicity a 1d vector, but same if it was longer)</p>
    <p>Now, we cannot just take into considera:on one ai because each xj is connected to all the neurons above and hence xj influences the overall score through all of these, hence:</p>
  </div>
  <div class="page">
    <p>Training with Backpropagation</p>
    <p>So in the last line, we re-used parts of the deriva:ve of a computa:on from a layer above.</p>
    <p>In the next sec:on, we will see how  with more deeper layers  even more of the computa:on of higher layers can be re-used in lower layers</p>
  </div>
  <div class="page">
    <p>Backpropagation Training Part 1.5: The Basics</p>
  </div>
  <div class="page">
    <p>Back-Prop</p>
    <p>Compute gradient of example-wise loss wrt parameters</p>
    <p>Simply applying the deriva:ves chain rule wisely</p>
    <p>If compu:ng the loss(example,parameters) is O(n) computa:on then so is compu:ng the gradient</p>
  </div>
  <div class="page">
    <p>Simple Chain Rule</p>
  </div>
  <div class="page">
    <p>Multiple Paths Chain Rule</p>
  </div>
  <div class="page">
    <p>Chain Rule in Flow Graph</p>
  </div>
  <div class="page">
    <p>Chain Rule in Flow Graph</p>
    <p>Flow graph: any directed acyclic graph node = computa:on result arc = computa:on dependency</p>
    <p>= successors of</p>
  </div>
  <div class="page">
    <p>Back-Prop in Multi-Layer Net</p>
  </div>
  <div class="page">
    <p>Back-Prop in General Flow Graph</p>
    <p>= successors of</p>
    <p>Compute gradient wrt each node using gradient wrt successors</p>
    <p>Single scalar output</p>
  </div>
  <div class="page">
    <p>Back-Prop in Recur{rent,sive} Net</p>
    <p>Replicate a parametrized func:on over different :me steps or nodes of a DAG</p>
    <p>Output state at one :me-step / node is used as input for another :me-step / node</p>
    <p>Very deep once unfolded!</p>
    <p>xt-1 xt xt+1</p>
    <p>zt-1 zt zt+1</p>
    <p>the cat runs fast</p>
  </div>
  <div class="page">
    <p>The image cannot be displayed. Your computer may not have enough memory to open the image, or the image may have been corrupted. Restart your computer, and then open the file again. If the red x still appears, you may have to delete the image and then insert it again.</p>
    <p>BP Through Structured Inference</p>
    <p>Inference  discrete choices  (e.g., shortest path in HMM, best output configura:on in CRF)</p>
    <p>E.g. Max over configura:ons or sum weighted by posterior  The loss to be op:mized depends on these choices  The inference opera:ons are flow graph nodes  If con:nuous, can perform stochas:c gradient descent</p>
    <p>Max(a,b) is con:nuous.</p>
    <p>Collobert &amp; Weston 2008: max-pooling layer</p>
  </div>
  <div class="page">
    <p>Automatic Differentiation  The gradient computa:on can be</p>
    <p>automa:cally inferred from the symbolic expression of the fprop.</p>
    <p>Makes it easier to quickly and safely try new models.</p>
    <p>Each node type needs to know how to compute its output and how to compute the gradient wrt its inputs given the gradient wrt its output.</p>
    <p>Theano Library (python) does it symbolically. Other neural network packages (Torch, Lush) do it numerically (at run-:me).</p>
  </div>
  <div class="page">
    <p>Learning word-level classifiers: POS and NER</p>
    <p>Part 1.6: The Basics</p>
  </div>
  <div class="page">
    <p>The Model</p>
    <p>Collobert &amp; Weston (2008)  Similar to the word vector</p>
    <p>learning but replaces the single scalar score with a</p>
    <p>so@max (maxent) classifier</p>
    <p>Training is again done via backpropaga:on which gives an error similar to the score in the unsupervised word vector learning model</p>
  </div>
  <div class="page">
    <p>The Model - Training</p>
    <p>We already know the so\max classifier and how to op:mize it  The interes:ng twist in deep learning is that the input features</p>
    <p>are also learned, similar to learning word vectors with a score:</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2 W23</p>
    <p>Wlabel</p>
    <p>c1 c2 c3</p>
  </div>
  <div class="page">
    <p>The Model - Training</p>
    <p>All deriva:ves of layers beneath the score were mul:plied by Wscore, for the so\max, the error vector becomes the difference between predicted and gold standard distribu:ons</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2</p>
    <p>s</p>
    <p>W23</p>
    <p>Wscore,2</p>
    <p>x1 x2 x3 +1</p>
    <p>a1 a2 W23</p>
    <p>Wlabel</p>
    <p>c1 c2 c3</p>
  </div>
  <div class="page">
    <p>Multi-Task Learning</p>
    <p>Generalizing beBer to new tasks is crucial to approach AI</p>
    <p>Deep architectures learn good intermediate representa:ons that can be shared across tasks</p>
    <p>Good representa:ons make sense for many tasks</p>
    <p>raw input x</p>
    <p>task 1 output y1</p>
    <p>task 3 output y3</p>
    <p>task 2 output y2</p>
    <p>shared intermediate representation h</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Collobert et al (2011) share word embeddings across:  Language modeling (predict next word)</p>
    <p>POS  Chunking  SRL  NER  Parsing</p>
    <p>POS (PWA)</p>
    <p>Chunk (F1)</p>
    <p>NER (F1)</p>
    <p>SRL (F1)</p>
    <p>SOTA 97.24 94.29 89.31 77.92</p>
    <p>Supervised 96.37 90.33 81.47 70.99</p>
    <p>Semi- supervised/ mul:-task</p>
    <p>+ hand- cra\ed features</p>
  </div>
  <div class="page">
    <p>Sharing statistical strength Part 1.7</p>
  </div>
  <div class="page">
    <p>Sharing Statistical Strength</p>
    <p>Besides very fast predictors, the main advantage of deep learning is sta:s:cal: poten:al to learn from less examples because of sharing of sta:s:cal strength:</p>
    <p>Unsupervised pre-training and semi-supervised training  Mul:-task learning  Mul:-data sharing, learning about symbolic objects and their rela:ons</p>
  </div>
  <div class="page">
    <p>Unsupervised Learning</p>
    <p>Major breakthrough in 2006:  Ability to train deep architectures</p>
    <p>by using layer-wise unsupervised learning, whereas previous supervised aBempts had failed</p>
    <p>Unsupervised feature learners:  RBMs  Auto-encoder variants  Sparse coding variants Montral</p>
    <p>Toronto</p>
    <p>Bengio</p>
    <p>Hinton Le Cun New York</p>
  </div>
  <div class="page">
    <p>Sharing Statistical Strength by SemiSupervised Learning</p>
    <p>Hypothesis: P(x) shares structure with P(y|x)</p>
    <p>purely supervised</p>
    <p>semi- supervised</p>
  </div>
  <div class="page">
    <p>Layer-wise Unsupervised Learning</p>
    <p>input</p>
  </div>
  <div class="page">
    <p>Layer-Wise Unsupervised Pre-training</p>
    <p>input</p>
    <p>features</p>
  </div>
  <div class="page">
    <p>Layer-Wise Unsupervised Pre-training</p>
    <p>input</p>
    <p>features</p>
    <p>reconstruction of input</p>
    <p>= ?</p>
    <p>input</p>
  </div>
  <div class="page">
    <p>Layer-Wise Unsupervised Pre-training</p>
    <p>input</p>
    <p>features</p>
  </div>
  <div class="page">
    <p>Layer-Wise Unsupervised Pre-training</p>
    <p>input</p>
    <p>features</p>
    <p>More abstract features</p>
  </div>
  <div class="page">
    <p>input</p>
    <p>features</p>
    <p>More abstract features</p>
    <p>reconstruction of features</p>
    <p>= ?</p>
    <p>Layer-Wise Unsupervised Pre-training Layer-wise Unsupervised Learning</p>
  </div>
  <div class="page">
    <p>input</p>
    <p>features</p>
    <p>More abstract features</p>
    <p>Layer-Wise Unsupervised Pre-training</p>
  </div>
  <div class="page">
    <p>input</p>
    <p>features</p>
    <p>More abstract features</p>
    <p>Even more abstract</p>
    <p>features</p>
    <p>Layer-wise Unsupervised Learning</p>
  </div>
  <div class="page">
    <p>input</p>
    <p>features</p>
    <p>More abstract features</p>
    <p>Even more abstract</p>
    <p>features</p>
    <p>Output f(X) six</p>
    <p>Target Y</p>
    <p>two! = ?</p>
    <p>Supervised Fine-Tuning</p>
    <p>Addi:onal hypothesis: features good for P(x) good for P(y|x) 105</p>
  </div>
  <div class="page">
    <p>Auto-Encoders  MLP whose target output = input  Reconstruc:on=decoder(encoder(input)), e.g.</p>
    <p>Probable inputs have small reconstruc:on error</p>
    <p>Can be stacked successfully (Bengio et al NIPS2006) to form highly non-linear representa:ons</p>
    <p>code= latent features</p>
    <p>encoder decoder</p>
    <p>input reconstruc:on</p>
  </div>
  <div class="page">
    <p>Auto-Encoder Variants  Discrete inputs: cross-entropy or log-likelihood reconstruc:on</p>
    <p>criterion (similar to used for discrete targets for MLPs)</p>
    <p>Preven:ng them to learn the iden:ty everywhere:  Undercomplete (eg PCA): boBleneck code smaller than input  Sparsity: penalize hidden unit ac:va:ons so at or near 0 [Goodfellow et al 2009]</p>
    <p>Denoising: predict true input from corrupted input [Vincent et al 2008]</p>
    <p>Contrac:ve: force encoder to have small deriva:ves [Rifai et al 2011]</p>
  </div>
  <div class="page">
    <p>Manifold Learning</p>
    <p>Addi:onal hypothesis: examples concentrate near a lower dimensional manifold (region of high density with only few opera:ons allowed which allow small changes while staying on the manifold)</p>
  </div>
  <div class="page">
    <p>PCA = Linear Manifold = Linear Auto-Encoder</p>
    <p>reconstruc:on error vector</p>
    <p>Linear manifold</p>
    <p>reconstruc:on(x)</p>
    <p>x</p>
    <p>input x, 0-mean features=code=h(x)=W x reconstruc:on(x)=WT h(x) = WT W x W = principal eigen-basis of Cov(X)</p>
    <p>LSA example: x = (normalized) distribu:on of co-occurrence frequencies</p>
  </div>
  <div class="page">
    <p>Auto-Encoders Learn Salient Variations, like a non-linear PCA</p>
    <p>Minimizing reconstruc:on error forces to keep varia:ons along manifold.</p>
    <p>Regularizer wants to throw away all varia:ons.</p>
    <p>Both: keep ONLY sensi:vity to varia:ons ON the manifold.</p>
  </div>
  <div class="page">
    <p>Stacking Auto-Encoders</p>
  </div>
  <div class="page">
    <p>Why is Unsupervised Pre-Training Working So Well?</p>
    <p>Regulariza:on hypothesis:  Unsupervised component forces model close to P(x)  Representa:ons good for P(x) are good for P(y|x)</p>
    <p>Op:miza:on hypothesis:  Unsupervised ini:aliza:on near beBer local minimum of supervised training error</p>
    <p>Can reach lower local minimum otherwise not achievable by random ini:aliza:on</p>
    <p>Erhan, Courville, Manzagol, Vincent, Bengio (JMLR, 2010)</p>
  </div>
  <div class="page">
    <p>Unsupervised Learning: Disentangling Factors of Variation</p>
    <p>[Goodfellow et al NIPS2009]: some hidden units more invariant (with more depth) to input geometry varia:ons</p>
    <p>[Glorot et al ICML2011]: some hidden units specialize on one aspect (domain) while others on another (sen:ment)</p>
    <p>We dont want invariant representa:ons because it is not clear to what aspects, but disentangling factors would help a lot</p>
    <p>Sparse/saturated units seem to help  Why?  How to train more towards that objecHve? 113</p>
  </div>
  <div class="page">
    <p>Invariance and Disentangling</p>
    <p>Invariant features</p>
    <p>Which invariances?</p>
    <p>Alterna:ve: learning to disentangle factors</p>
    <p>Good disentangling  avoid the curse of dimensionality</p>
  </div>
  <div class="page">
    <p>Advantages of Sparse Representations</p>
    <p>Just add a penalty on learned representa:on</p>
    <p>Informa:on disentangling (compare to dense compression)</p>
    <p>More likely to be linearly separable (high-dimensional space)</p>
    <p>Locally low-dimensional representa:on = local chart  Hi-dim. sparse = efficient variable size representa:on = data structure</p>
    <p>Few bits of informa:on Many bits of informa:on</p>
  </div>
  <div class="page">
    <p>Multi-Task Learning</p>
    <p>Generalizing beBer to new tasks is crucial to approach AI</p>
    <p>Deep architectures learn good intermediate representa:ons that can be shared across tasks</p>
    <p>Good representa:ons make sense for many tasks</p>
    <p>raw input x</p>
    <p>task 1 output y1</p>
    <p>task 3 output y3</p>
    <p>task 2 output y2</p>
    <p>shared intermediate representation h</p>
  </div>
  <div class="page">
    <p>Multi-Task Learning</p>
    <p>Collobert et al (2011) share word embeddings across:  Language modeling (predict next word)</p>
    <p>POS  Chunking  SRL  NER  Parsing</p>
    <p>POS (PWA)</p>
    <p>Chunk (F1)</p>
    <p>NER (F1)</p>
    <p>SRL (F1)</p>
    <p>SOTA 97.24 94.29 89.31 77.92</p>
    <p>Supervised 96.37 90.33 81.47 70.99</p>
    <p>Semi- supervised/ mul:-task</p>
    <p>+ hand- cra\ed features</p>
  </div>
  <div class="page">
    <p>Combining Multiple Sources of Evidence with Shared Embeddings</p>
    <p>Rela:onal learning  Mul:ple sources of informa:on / rela:ons  Some symbols (e.g. words, wikipedia entries) shared  Shared embeddings help propagate informa:on among data sources: e.g., WordNet, XWN, Wikipedia, FreeBase,</p>
  </div>
  <div class="page">
    <p>Recursive Neural Networks Part 2</p>
  </div>
  <div class="page">
    <p>Building on Word Vector Space Models x2</p>
    <p>x1 0 1 2 3 4 5 6 7 8 9 10</p>
    <p>Tuesday 9.5 1.5</p>
    <p>By mapping them into the same vector space!</p>
    <p>the country of my birth the place where I was born</p>
    <p>But how can we represent the meaning of longer phrases?</p>
    <p>France 2 2.5</p>
    <p>Germany 1 3</p>
  </div>
  <div class="page">
    <p>How should we map phrases into a vector space?</p>
    <p>the country of my birth</p>
    <p>Use principle of composi:onality</p>
    <p>The meaning (vector) of a sentence is determined by (1) the meanings of its words and (2) the rules that combine them.</p>
    <p>Recursive Neural Nets can jointly learn composi:onal vector representa:ons and parse trees</p>
    <p>x2</p>
    <p>x1 0 1 2 3 4 5 6 7 8 9 10</p>
    <p>the country of my birth</p>
    <p>the place where I was born</p>
    <p>Monday</p>
    <p>Tuesday</p>
    <p>France</p>
    <p>Germany</p>
  </div>
  <div class="page">
    <p>Recursive Neural Networks</p>
  </div>
  <div class="page">
    <p>Sentence Parsing: What we want</p>
    <p>NP NP</p>
    <p>PP</p>
    <p>S</p>
    <p>VP</p>
    <p>The cat sat on the mat. 123</p>
  </div>
  <div class="page">
    <p>Learn Structure and Representation</p>
    <p>NP NP</p>
    <p>PP</p>
    <p>S</p>
    <p>VP</p>
    <p>The cat sat on the mat.</p>
  </div>
  <div class="page">
    <p>Recursive Neural Networks for Structure Prediction</p>
    <p>on the mat.</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Inputs: two candidate childrens representa:ons Outputs: 1. The seman:c representa:on if the two nodes are merged. 2. Score of how plausible the new node would be.</p>
  </div>
  <div class="page">
    <p>Recursive Neural Network Definition</p>
    <p>score = WTscore p</p>
    <p>p = sigmoid(W + b),</p>
    <p>where sigmoid: 8 5</p>
    <p>Neural &quot; Network&quot;</p>
    <p>c1 c2</p>
    <p>c1 c2</p>
  </div>
  <div class="page">
    <p>Related Work to Socher et al. 2011a</p>
    <p>Pollack (1990): Recursive auto-associa:ve memories</p>
    <p>Previous Recursive Neural Networks work by [Goller &amp; Kchler 1996, Costa et al. 2003] assumed fixed tree structure and used one hot vectors.</p>
    <p>Hinton (1990) and BoBou (2011): Related ideas about recursive models and recursive operators as smooth versions of logic opera:ons</p>
  </div>
  <div class="page">
    <p>Parsing a sentence</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>The cat sat on the mat.</p>
  </div>
  <div class="page">
    <p>Parsing a sentence</p>
    <p>Neural &quot; Network&quot;</p>
    <p>The cat sat on the mat.</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
  </div>
  <div class="page">
    <p>Parsing a sentence</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>Neural &quot; Network&quot;</p>
    <p>The cat sat on the mat.</p>
  </div>
  <div class="page">
    <p>Parsing a sentence</p>
    <p>The cat sat on the mat.</p>
  </div>
  <div class="page">
    <p>Max-Margin Framework - Details  The score of a tree is computed by</p>
    <p>the sum of the parsing decision scores at each node.</p>
    <p>Similar to max-margin parsing (Taskar et al. 2004), we can formulate a supervised max-margin objec:ve</p>
    <p>The loss penalizes all incorrect decisions</p>
    <p>RNN&quot;</p>
  </div>
  <div class="page">
    <p>Backpropagation Through Structure (BTS)</p>
    <p>Introduced by [Goller et al. 1996]  Principally the same as general backpropaga:on (efficient</p>
    <p>matrix deriva:ve)</p>
    <p>Two differences resul:ng from the tree structure:</p>
    <p>Split deriva:ves at each node</p>
    <p>Sum deriva:ves of W from all nodes</p>
  </div>
  <div class="page">
    <p>BTS: Split derivatives at each node  During forward prop, the parent is computed using 2 children</p>
    <p>Hence, the errors need to be computed wrt each of them:</p>
    <p>where each childs error is n-dimensional</p>
    <p>c1 p = sigmoid(W + b) c1 c2</p>
    <p>c2</p>
    <p>c1 c2</p>
  </div>
  <div class="page">
    <p>BTS: Sum derivatives of all nodes  You can actually assume its a different W at each node  Intui:on via example:</p>
    <p>If take separate deriva:ves of each occurrence, we get same:</p>
  </div>
  <div class="page">
    <p>BTS: Optimization</p>
    <p>As before, we can plug the gradients into a standard off-the-shelf L-BFGS op:mizer</p>
    <p>For non-con:nuous objec:ve use subgradient method [Ratliff et al. 2007]</p>
  </div>
  <div class="page">
    <p>Details of Recursive Neural Networks  Structure search was maximally greedy</p>
    <p>Instead: Beam Search with Chart</p>
    <p>Include context: the word to the le\ and right</p>
  </div>
  <div class="page">
    <p>Labeling in Recursive Neural Networks</p>
    <p>Neural &quot; Network&quot;</p>
    <p>We can use each nodes representa:on as features for a so\max classifier:</p>
    <p>Softmax&quot; Layer&quot;</p>
    <p>NP</p>
  </div>
  <div class="page">
    <p>Experiments: Parsing Short Sentences  Standard WSJ train/test  Good results on short</p>
    <p>sentences</p>
    <p>More work is needed for longer sentences</p>
    <p>Model L15 Dev L15 Test</p>
    <p>Recursive Neural Network 92.1 90.3</p>
    <p>Sigmoid NN (Titov &amp; Henderson 2007) 89.5 89.3</p>
    <p>Berkeley Parser (Petrov &amp; Klein 2006) 92.1 91.6</p>
    <p>All the figures are adjusted for seasonal varia:ons 1. All the numbers are adjusted for seasonal fluctua:ons 2. All the figures are adjusted to remove usual seasonal paBerns</p>
    <p>Knight-Ridder wouldnt comment on the offer 1. Harsco declined to say what country placed the order 2. Coastal wouldnt disclose the terms</p>
    <p>Sales grew almost 7% to $UNK m. from $UNK m. 1. Sales rose more than 7% to $94.9 m. from $88.3 m. 2. Sales surged 40% to UNK b. yen from UNK b.&quot;</p>
  </div>
  <div class="page">
    <p>Paraphrase detection task  Goal is to say which of candidate phrases are a good</p>
    <p>paraphrase of a given phrase  Mo:vated by Machine Transla:on  Ini:al algorithms: Bannard &amp; Callison-Burch 2005 (BC 2005), Callison- Burch 2008 (CB 2008) exploit bilingual sentence-aligned corpora and hand-built linguis:c constraints</p>
    <p>We simply re-use our system learned on parsing the WSJ</p>
    <p>BC 2005 CB 2008 RNN</p>
    <p>F1 of Paraphrase DetecHon</p>
  </div>
  <div class="page">
    <p>Paraphrase detection task Target Candidates with human goodness label (15) ordered by our system</p>
    <p>the united states</p>
    <p>the usa (5) the us (5) united states (5) north america (4) united (1) the (1) of the united states (3) america (5) na:ons (2) we (3)</p>
    <p>around the world</p>
    <p>around the globe(5) throughout the world(5) across the world(5) over the world(2) in the world(5) of the budget(2) of the world(5)</p>
    <p>it would be it would represent (5) there will be (2) that would be (3) it would be ideal (2) it would be appropriate (2) it is (3) it would (2)</p>
    <p>of capital punishment</p>
    <p>of the death penalty (5) to death (2) the death penalty (2) of (1)</p>
    <p>in the long run</p>
    <p>in the long term (5) in the short term (2) for the longer term (5) in the future (5) in the end (3) in the long-term (5) in :me (5) of the (1)</p>
  </div>
  <div class="page">
    <p>Recursive Neural Networks</p>
  </div>
  <div class="page">
    <p>Recursive Autoencoders  Similar to Recursive Neural Net but instead of a supervised score we compute a reconstruc:on error at each node.</p>
    <p>x2 x3x1</p>
    <p>y1=f(W[x2;x3] + b)</p>
    <p>y2=f(W[x1;y1] + b)</p>
  </div>
  <div class="page">
    <p>Semi-supervised Recursive Autoencoder  In order for representa:ons to capture sen:ment, we add a so\max</p>
    <p>classifier</p>
    <p>Error is a weighted combina:on of reconstruc:on error and cross-entropy (distribu:on likelihood)</p>
    <p>Socher et al. 2011b</p>
    <p>Reconstruction error Cross-entropy error</p>
    <p>W(1)</p>
    <p>W(2) W(label)</p>
  </div>
  <div class="page">
    <p>Sentiment Detection and Bag-of-Words Models  Sen:ment detec:on is crucial to business intelligence, stock trading,</p>
    <p>Most methods start with a bag of words + linguis:c features/processing/lexica</p>
    <p>But such methods (including -idf) cant dis:nguish: + white blood cells destroying an infec:on</p>
    <p>- an infec:on destroying white blood cells</p>
  </div>
  <div class="page">
    <p>Single Scale Experiments: Movies</p>
    <p>Stealing Harvard doesnt care about cleverness, wit or any other kind of intelligent humor.</p>
    <p>a film of ideas and wry comic mayhem.</p>
  </div>
  <div class="page">
    <p>Accuracy of Positive/Negative Sentiment Classification</p>
    <p>Results on movie reviews (MR) and opinions (MPQA).  All other methods use hand-designed polarity shi\ing rules or sen:ment lexica.</p>
    <p>RAE: no hand-designed features, learns vector representa:ons for n-grams Method MR MPQA Phrase vo:ng with lexicons 63.1 81.7</p>
    <p>Bag of features with lexicons 76.4 84.1</p>
    <p>Tree-CRF (Nakagawa et al. 2010) 77.3 86.1</p>
    <p>RAE (this work) 77.7 86.4</p>
  </div>
  <div class="page">
    <p>Sorted Negative and Positive N-grams</p>
    <p>Most NegaHve N-grams Most PosiHve N-grams</p>
    <p>bad; boring; dull; flat; pointless touching; enjoyable; powerful</p>
    <p>that bad; abysmally pathe:c the beau:ful; with dazzling</p>
    <p>is more boring; manipula:ve and contrived</p>
    <p>funny and touching; a small gem</p>
    <p>boring than anything else.; a major waste ... generic</p>
    <p>cute, funny, heartwarming; with wry humor and genuine</p>
    <p>loud, silly, stupid and pointless. ; dull, dumb and deriva:ve horror film.</p>
    <p>, deeply absorbing piece that works as a; ... one of the most ingenious and entertaining;</p>
  </div>
  <div class="page">
    <p>Learning Compositionality from Movie Reviews  Probability of being posi:ve of several n-grams</p>
    <p>n-gram P(posiHve | n-gram) good 0.45 not good 0.20 very good 0.61 not very good 0.15</p>
    <p>not 0.03 very 0.23</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Sentiment Distribution Experiments  Learn distribu:ons over mul:ple complex sen:ments  New dataset and task</p>
    <p>Experience Project  hBp://www.experienceproject.com  I walked into a parked car  Sorry, Hugs; You rock; Tee-hee ; I understand; Wow just wow</p>
    <p>Over 31,000 entries with 113 words on average</p>
  </div>
  <div class="page">
    <p>Sentiment distributions  Sorry, Hugs; You rock; Tee-hee ; I understand; Wow just wow</p>
    <p>Predicted and Gold DistribuHon</p>
    <p>Anonymous Confession</p>
    <p>i am a very succesfull business man. i make good money but i have been addicted to crack for 13 years. i moved 1 hour away from my dealers 10 years ago to stop using now i dont use daily but  well i think hairy women are aBrac:ve</p>
    <p>Dear Love, I just want to say that I am looking for you. Tonight I felt the urge to write, and I am becoming more and more frustrated that I have not found you yet. Im also :red of spending so much heart on an old dream. ...</p>
  </div>
  <div class="page">
    <p>Experience Project most votes results Method Accuracy %</p>
    <p>Random 20</p>
    <p>Most frequent class 38</p>
    <p>Bag of words; MaxEnt classifier 46</p>
    <p>Spellchecker, sen:ment lexica, SVM 47</p>
    <p>SVM on neural net word features 46</p>
    <p>RAE (this work) 50</p>
    <p>Average KL between gold and predicted label distribu:ons:</p>
  </div>
  <div class="page">
    <p>Paraphrase Detection</p>
    <p>Pollack said the plain:ffs failed to show that Merrill and Blodget directly caused their losses</p>
    <p>Basically , the plain:ffs did not show that omissions in Merrills research caused the claimed losses</p>
    <p>The ini:al report was made to Modesto Police December 28</p>
    <p>It stems from a Modesto police report</p>
  </div>
  <div class="page">
    <p>How to compare the meaning of two sentences?</p>
  </div>
  <div class="page">
    <p>Recursive Autoencoders for Full Sentence Paraphrase Detection</p>
    <p>Unsupervised RAE and a pair-wise sentence comparison of nodes in parsed trees  Socher et al. 2011c</p>
  </div>
  <div class="page">
    <p>Unsupervised unfolding RAE</p>
  </div>
  <div class="page">
    <p>Nearest Neighbors of the Unfolding RAE</p>
    <p>Center Phrase RAE Unfolding RAE</p>
    <p>the U.S. the Swiss the former U.S.</p>
    <p>suffering low morale suffering due to no fault of my own</p>
    <p>suffering heavy casual:es</p>
    <p>advance to the next round</p>
    <p>advance to the final of the UNK 1.1 million Kremlin Cup</p>
    <p>advance to the semis</p>
    <p>a prominent poli:cal figure</p>
    <p>the second high-profile opposi:on figure</p>
    <p>a powerful business figure</p>
    <p>Seventeen people were killed</p>
    <p>Fourteen people were killed Fourteen people were killed</p>
    <p>condi:ons of his release condi:ons of peace, social stability and poli:cal harmony</p>
    <p>nego:a:ons for their release</p>
  </div>
  <div class="page">
    <p>Recursive Autoencoders for Full Sentence Paraphrase Detection  Experiments on Microso\ Research Paraphrase Corpus (Dolan et al. (2004))</p>
    <p>Method Acc. F1</p>
    <p>All Paraphrase Baseline 66.5 79.9</p>
    <p>Rus et al.(2008) 70.6 80.5</p>
    <p>Mihalcea et al.(2006) 70.3 81.3</p>
    <p>Islam et al.(2007) 72.6 81.3</p>
    <p>Qiu et al.(2006) 72.0 81.6</p>
    <p>Fernando et al.(2008) 74.1 82.4</p>
    <p>Wan et al.(2006) 75.6 83.0</p>
    <p>Das and Smith (2009) 73.9 82.3</p>
    <p>Das and Smith (2009) + 18 Surface Features 76.1 82.7</p>
    <p>Unfolding Recursive Autoencoder (our method) 76.4 83.4</p>
  </div>
  <div class="page">
    <p>Recursive Autoencoders for Full Sentence Paraphrase Detection</p>
  </div>
  <div class="page">
    <p>Recursive Neural Networks</p>
  </div>
  <div class="page">
    <p>Compositionality Through Recursive Matrix-Vector Spaces</p>
    <p>But what if words act mostly as an operator, e.g. very in</p>
    <p>very good</p>
    <p>p = sigmoid(W + b)</p>
    <p>c1 c2</p>
  </div>
  <div class="page">
    <p>Compositionality Through Recursive Matrix-Vector Recursive Neural Networks</p>
    <p>p = sigmoid(W + b)</p>
    <p>c1 c2</p>
    <p>p = sigmoid(W + b)</p>
    <p>C2c1 C1c2</p>
  </div>
  <div class="page">
    <p>Predicting Sentiment Distributions</p>
  </div>
  <div class="page">
    <p>MV-RNN for Relationship Classification</p>
    <p>RelaHonship Sentence with labeled nouns for which to predict relaHonships</p>
    <p>Cause-Effect(e2,e1)</p>
    <p>Avian [influenza]e1 is an infec:ous disease caused by type a strains of the influenza [virus]e2.</p>
    <p>En:ty-Origin(e1,e2) The [mother]e1 le\ her na:ve [land]e2 about the same :me and they were married in that city.</p>
    <p>Message-Topic(e2,e1)</p>
    <p>Roadside [aBrac:ons]e1 are frequently adver:sed with [billboards]e2 to aBract tourists.</p>
  </div>
  <div class="page">
    <p>MV-RNN for Relationship Classification</p>
  </div>
  <div class="page">
    <p>Summary: Recursive Deep Learning</p>
    <p>Recursive Deep Learning can predict hierarchical structure and classify the structured output using composi:onal vectors</p>
    <p>State-of-the-art performance on  Object detec:on on Stanford background and MSRC datasets  Sen:ment Analysis on mul:ple corpora  Paraphrase detec:on on the MSRP dataset  Rela:on Classifica:on on SemEval 2011, Task8</p>
    <p>x1 x3 x4x2</p>
    <p>y1=f(W (1)[x3;x4] + b)</p>
    <p>y2=f(W (1)[x2;y1] + b)</p>
    <p>y3=f(W (1)[x1;y2] + b)</p>
  </div>
  <div class="page">
    <p>Part 3</p>
  </div>
  <div class="page">
    <p>Existing NLP Applications</p>
    <p>Language Modeling  Speech Recogni:on  Machine Transla:on</p>
    <p>Part-Of-Speech Tagging  Chunking  Named En:ty Recogni:on  Seman:c Role Labeling  Sen:ment Analysis  Paraphrasing  Ques:on-Answering  Word-Sense Disambigua:on 169</p>
  </div>
  <div class="page">
    <p>Neural Language Models Part 3.1: Applica:ons</p>
  </div>
  <div class="page">
    <p>Neural Language Model</p>
    <p>Bengio et al NIPS2000 and JMLR 2003 A Neural Probabilis;c Language Model  Each word represented by a distributed con:nuous- valued code</p>
    <p>Generalizes to sequences of words that are seman:cally similar to training sequences</p>
  </div>
  <div class="page">
    <p>Bilinear Language Model</p>
    <p>Even a linear version of the Neural Language Model works</p>
    <p>beBer than n-grams</p>
    <p>r = X</p>
    <p>i</p>
    <p>Cirwir</p>
    <p>C</p>
    <p>rwi</p>
    <p>|V|-length Softmax layer</p>
    <p>n-length Embedding layer</p>
    <p>[Minh &amp; Hinton 2007]  APNews perplexity</p>
    <p>down from 117 (KN6) to 96.5</p>
  </div>
  <div class="page">
    <p>Language Modeling</p>
    <p>Predict next word given previous words</p>
    <p>Standard approach: output = P(next word | previous word)</p>
    <p>Applica:ons to Speech, Transla:on and Compression</p>
    <p>Computa:onal boBleneck: large vocabulary V means that compu:ng the output costs #hidden units x |V|.</p>
  </div>
  <div class="page">
    <p>Language Modeling Output Bottleneck</p>
    <p>[Schwenk et al 2002]: only predict most frequent words (short list) and use n-gram for the others</p>
    <p>[Morin &amp; Bengio 2005; Blitzer et al 2005; Mnih &amp; Hinton 2007,2009; Mikolov et al 2011]: hierarchical representa:ons, mul:ple output groups, condi:onally computed, predict  P(word category | context)  P(sub-category | context, category)  P(word | context, sub-category, category)</p>
    <p>With 2 levels, O(|V|) becomes O(sqrt(|V|))  With d levels, O(|V|) becomes O(log(|V|))</p>
    <p>Hard categories, can be arbitrary [Mikolov et al 2011]</p>
    <p>categories</p>
    <p>words within each category</p>
  </div>
  <div class="page">
    <p>Language Modeling Output Bottleneck: Hierarchical word categories</p>
    <p>Compute</p>
    <p>P(word|category,context) only for category=category(word)</p>
    <p>P(category|context)</p>
    <p>Instan:ated only for category(word)</p>
    <p>Context = previous words</p>
    <p>P(word|context,category)</p>
  </div>
  <div class="page">
    <p>Language Modeling Output Bottleneck: Sampling Methods</p>
    <p>Importance sampling to recover next-word probabili:es [Bengio &amp; Senecal 2003, 2008]</p>
    <p>Sampling a ranking loss [Collobert et al, 2008, 2011]  Increase score of observed words output  Decrease score of randomly selected word (nega:ve example)  (not anymore outpung probabili:es, ok if the goal is just to learn word embeddings)</p>
    <p>Importance sampling for reconstruc:ng bag-of-words [Dauphin et al 2011]</p>
  </div>
  <div class="page">
    <p>Neural Net Language Modeling for ASR</p>
    <p>[Schwenk 2009], real-:me ASR, perplexity AND word error rate improve (CTS evalua:on set 2003), perplexi:es go from 50.1 to 45.5</p>
  </div>
  <div class="page">
    <p>Application to Statistical Machine Translation</p>
    <p>Schwenk (NAACL 2012 workshop on the future of LM)  41M words, Arabic/English bitexts + 151M English from LDC</p>
    <p>Perplexity down from 71.1 (6 Gig back-off) to 56.9 (neural model, 500M memory)</p>
    <p>+1.8 BLEU score (50.75 to 52.28)</p>
    <p>Can take advantage of longer contexts</p>
    <p>Code: http://lium.univ-lemans.fr/cslm/!</p>
  </div>
  <div class="page">
    <p>Structured embedding of knowledge bases</p>
    <p>Part 3.1: Applica:ons</p>
  </div>
  <div class="page">
    <p>Learning Structured Embeddings of Knowledge Bases, Bordes, Weston, Collobert &amp; Bengio, AAAI 2011 Joint Learning of Words and Meaning Representa;ons for Open-Text Seman;c Parsing, Bordes, Glorot, Weston &amp; Bengio, AISTATS 2012</p>
    <p>Modeling Semantics</p>
  </div>
  <div class="page">
    <p>Model (lhs, rela:on, rhs) Each concept = 1 embedding vector Each rela:on = 2 matrices. Matrix acts like an operator. Ranking criterion Energy = low for training examples, high o/w</p>
    <p>lhs rela:on</p>
    <p>energy</p>
    <p>rhs</p>
    <p>choose vector choose matrices</p>
    <p>|| . ||1</p>
    <p>Modeling Relations with Matrices</p>
  </div>
  <div class="page">
    <p>Verb = rela:on. Too many to have a matrix each. Each concept = 1 embedding vector Each rela:on = 1 embedding vector Can handle rela:ons on rela:ons on rela:ons</p>
    <p>Allowing Relations on Relations</p>
    <p>lhs rela:on</p>
    <p>energy</p>
    <p>rhs</p>
    <p>choose vector</p>
    <p>|| . ||1</p>
    <p>mlp mlp</p>
  </div>
  <div class="page">
    <p>Use SENNA (Collobert 2010) = embedding-based NLP tagger for Seman:c Role Labeling, breaks sentence into (subject part, verb part, object part)</p>
    <p>Use max-pooling to aggregate embeddings of words inside each part</p>
    <p>Training on Full Sentences</p>
    <p>Subj. words Verb words</p>
    <p>energy</p>
    <p>Obj. words</p>
    <p>|| . ||1</p>
    <p>mlp mlp</p>
    <p>Element-wise max. Element-wise max. Element-wise max.</p>
    <p>black__2 eat__2 cat__1 white__1 mouse_2</p>
  </div>
  <div class="page">
    <p>Open-Text Semantic Parsing</p>
    <p>3 steps:</p>
    <p>last formula defines the Meaning Representa:on (MR).</p>
  </div>
  <div class="page">
    <p>Training Criterion</p>
    <p>Intui:on: if an en:ty of a triplet was missing, we would like our model to predict it correctly i.e. to give it the lowest energy. For example, this would allow us to answer ques:ons like what is part of a car?</p>
    <p>Hence, for any training triplet xi = (lhsi, reli, rhsi) we would like: (1) E(lhsi, reli, rhsi) &lt; E(lhsj, reli, rhsi),</p>
    <p>(2) E(lhsi, reli, rhsi) &lt; E(lhsi, relj, rhsi), (3) E(lhsi, reli, rhsi) &lt; E(lhsi, reli, rhsj),</p>
    <p>That is, the energy func:on E is trained to rank training samples below all other triplets.</p>
  </div>
  <div class="page">
    <p>Training Algorithm pseudo-likelihood + uniform sampling of negative variants</p>
    <p>Train by stochas:c gradient descent: 1. Randomly select a posi:ve training triplet xi = (lhsi, reli, rhsi).</p>
  </div>
  <div class="page">
    <p>Question Answering: implicitly adding new relations to WN or FB</p>
    <p>MRs inferred from text define triplets between WordNet synsets. Model captures knowledge about rela:ons between nouns and verbs.  Implicit addi:on of new rela:ons to WordNet!  Generalize Freebase!</p>
  </div>
  <div class="page">
    <p>Embedding Near Neighbors of Words &amp; Senses</p>
  </div>
  <div class="page">
    <p>Word Sense Disambiguation  Senseval-3 results (only sentences with</p>
    <p>Subject-Verb-Object</p>
    <p>structure)</p>
    <p>MFS=most frequent sense</p>
    <p>All=training from all sources</p>
    <p>Gamble=Decadt et al 2004 (Senseval-3 SOA)</p>
    <p>XWN results XWN = eXtended WN</p>
  </div>
  <div class="page">
    <p>Assorted other speech and NLP applications</p>
    <p>Part 3.1: Applica:ons</p>
  </div>
  <div class="page">
    <p>Learning Multiple Word Vectors</p>
    <p>Tackles problems with polysemous words</p>
    <p>Can be done with both standard -idf based methods [Reisinger and Mooney, NAACL 2010]</p>
    <p>Recent neural word vector model by [Huang et al. ACL 2012] learns mul:ple prototypes using both local and global context</p>
    <p>State of the art correla:ons with human similarity judgments</p>
  </div>
  <div class="page">
    <p>Learning Multiple Word Vectors  Visualiza:on of learned word vectors from</p>
    <p>Huang et al. (ACL 2012)</p>
  </div>
  <div class="page">
    <p>Recurrent Neural Net Language Modeling for ASR</p>
    <p>[Mikolov et al 2011] Bigger is beBer experiments on Broadcast News NIST-RT04</p>
    <p>perplexity goes from 140 to 102</p>
    <p>Paper shows how to train a recurrent neural net with a single core in a few days, with &gt; 1% absolute improvement in WER</p>
    <p>Code: http://www.fit.vutbr.cz/~imikolov/rnnlm/!</p>
    <p>Code: hBp://www.fit.vutbr.cz/~imikolov/rnnlm/</p>
  </div>
  <div class="page">
    <p>Phoneme-Level Acoustic Models</p>
    <p>[Mohamed et al, 2011, IEEE Tr.ASLP]</p>
    <p>Unsupervised pre-training as Deep Belief Nets (a stack of RBMs), supervised fine-tuning to predict phonemes</p>
    <p>Phoneme classifica:on on TIMIT:  CD-HMM: 27.3% error  CRFs: 26.6%  Triphone HMMs w. BMMI: 22.7%  Unsupervised DBNs: 24.5%  Fine-tuned DBNs: 20.7%</p>
    <p>Improved version by Dong Yu is RELEASED IN MICROSOFTS ASR system for Audio Video Indexing Service</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis on Bag-of-word representations</p>
    <p>[Glorot et al, ICML 2011] beats SOTA on Amazon benchmark</p>
    <p>Bag-of-words input  Embeddings pre-trained in</p>
    <p>denoising auto-encoder with rec:fier ac:va:on func:ons and sampled reconstruc:on</p>
    <p>Disentangling effect (features specialize to domain or sen:ment)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis: Transfer Learning</p>
    <p>25 Amazon.com domains: toys, so\ware, video, books, music, beauty,</p>
    <p>Unsupervised pre-training of input space on all domains</p>
    <p>Supervised SVM on 1 domain, generalize out- of-domain</p>
    <p>Baseline: bag-of-words + SVM</p>
  </div>
  <div class="page">
    <p>Resources: Code and References</p>
    <p>Part 3.2: Resources</p>
  </div>
  <div class="page">
    <p>Papers, Related Tutorials</p>
    <p>See Neural Net Language Models Scholarpedia entry  Deep Learning tutorials</p>
    <p>hBp://deeplearning.net/tutorials/  Stanford Deep learning tutorials with simple programming</p>
    <p>assignments and reading list</p>
    <p>hBp://deeplearning.stanford.edu/wiki/  Recursive Autoencoder class project:</p>
    <p>hBp://cseweb.ucsd.edu/~elkan/250B/learningmeaning.pdf</p>
  </div>
  <div class="page">
    <p>Software</p>
    <p>Theano (Python CPU/GPU mathema:cal and deep learning library</p>
    <p>hBp://deeplearning.net/so\ware/theano/  Senna by (Collobert et al 2011, JMLR)</p>
    <p>POS, Chunking, NER, SRL  State-of-the-art performance on many tasks  hBp://ronan.collobert.com/senna/  3500 lines of C, extremely fast and using very liBle memory</p>
    <p>Recurrent Neural Network Language Model  hBp://www.fit.vutbr.cz/~imikolov/rnnlm/</p>
    <p>Recursive Neural Net and RAE models for paraphrase detec:on, sen:ment analysis, rela:on classifica:on  www.socher.org</p>
  </div>
  <div class="page">
    <p>Deep Learning Tricks Part 3.3: Deep Learning Tricks</p>
  </div>
  <div class="page">
    <p>Deep Learning Tricks of the Trade  Y. Bengio (2012), Prac:cal Recommenda:ons for Gradient-</p>
    <p>Based Training of Deep Architectures</p>
    <p>Unsupervised pre-training  Stochas:c gradient descent and seng learning rates  Main hyper-parameters  Learning rate schedule  Early stopping  Minibatches  Parameter ini:aliza:on  Number of hidden units  L1 and L2 weight decay  Sparsity regulariza:on</p>
    <p>Debugging  How to efficiently search for hyper-parameter configura:ons</p>
  </div>
  <div class="page">
    <p>Stochastic Gradient Descent (SGD)  Gradient descent uses total gradient over all examples per</p>
    <p>update, SGD updates a\er only 1 or few examples:</p>
    <p>L = loss func:on, zt = current example,  = parameter vector, and t = learning rate.</p>
    <p>Ordinary gradient descent is a batch method, very slow, should never be used. Use 2nd order batch method such as LBFGS. On large datasets, SGD usually wins over all batch methods. On smaller datasets LBFGS or Conjugate Gradients win. Large-batch LBFGS extends the reach of LBFGS [Le et al ICML2011].</p>
  </div>
  <div class="page">
    <p>Learning Rates</p>
    <p>Simplest recipe: keep it fixed and use the same for all parameters.</p>
    <p>Collobert scales them by the inverse of square root of the fan-in of each neuron</p>
    <p>BeBer results can generally be obtained by allowing learning rates to decrease, typically in O(1/t) because of theore:cal convergence guarantees, e.g.,</p>
    <p>with hyper-parameters 0 and . 203</p>
  </div>
  <div class="page">
    <p>Long-Term Dependencies and Clipping Trick  In very deep networks such as recurrent networks (or possibly</p>
    <p>recursive ones), the gradient is a product of Jacobian matrices, each associated with a step in the forward computa:on. This can become very small or very large quickly [Bengio et al 1994], and the locality assump:on of gradient descent breaks down.</p>
    <p>The solu:on first introduced by Mikolov is to clip gradients to a maximum value. Makes a big difference in RNNs</p>
  </div>
  <div class="page">
    <p>Early Stopping</p>
    <p>Beau:ful FREE LUNCH (no need to launch many different training runs for each value of hyper-parameter for #itera:ons)</p>
    <p>Monitor valida:on error during training (a\er visi:ng # examples a mul:ple of valida:on set size)</p>
    <p>Keep track of parameters with best valida:on error and report them at the end</p>
    <p>If error does not improve enough (with some pa:ence), stop.</p>
  </div>
  <div class="page">
    <p>Parameter Initialization</p>
    <p>Ini:alize hidden layer biases to 0 and output (or reconstruc:on) biases to op:mal value if weights were 0 (e.g. mean target or inverse sigmoid of mean target).</p>
    <p>Ini:alize weights ~ Uniform(-r,r), r inversely propor:onal to fan- in (previous layer size) and fan-out (next layer size):</p>
    <p>for tanh units (and 4x bigger for sigmoid units).</p>
    <p>Note: for embedding weights, fan-in=1 and we dont care about fan-out, Collobert uses Uniform(-1,1).</p>
  </div>
  <div class="page">
    <p>Sampled Reconstruction Trick [Dauphin et al, ICML 2011]</p>
    <p>Auto-encoders and RBMs reconstruct the input, which is sparse and high-dimensional</p>
    <p>Applied to bag-of-words input for sen:ment analysis, with denoising auto-encoders</p>
    <p>Always reconstruct the non-zeros in the input, and reconstruct as many randomly chosen zeros</p>
    <p>code= latent features</p>
    <p>sparse input dense output probabilities</p>
    <p>cheap expensive</p>
  </div>
  <div class="page">
    <p>Discussion: Limitations, Advantages, Future Directions</p>
    <p>Part 3.4: Discussion</p>
  </div>
  <div class="page">
    <p>Outlook</p>
    <p>Current strengths and weaknesses  Represen:ng meaning at different levels  Inference challenges  Reaping the benefits of more abstract features: beBer transfer, e.g. to other languages</p>
  </div>
  <div class="page">
    <p>Criticisms</p>
    <p>Many algorithms and variants (burgeoning field)  Many hyper-parameters  use mul:-core machines, clusters and random sampling for cross-valida:on</p>
    <p>Not always obvious how to combine with exis:ng NLP  learn more abstract features, separate parsing and seman:c analysis, your research here</p>
    <p>Slower to train train linear models  only by a small constant factor, and much more compact that non- parametric (e.g. n-gram models)</p>
  </div>
  <div class="page">
    <p>Criticisms</p>
    <p>Youre learning everything. BeBer to encode prior knowledge about structure of language.</p>
    <p>Wasnt there a similar machine learning vs. linguists debate in NLP ~20 years ago.</p>
    <p>Research goal: Just like we now use off-the-shelf SVM packages, we can try to build off-the-shelf NLP classifica:on packages that are given as input only raw text and a label.</p>
  </div>
  <div class="page">
    <p>Problems with model interpretability</p>
    <p>Ways to interpret the output of models/ weights on features  More difficult compared to systems with small sets of hand-designed features, but possible through low-dimensional projec:ons of word vectors</p>
    <p>Learning about language through doing nlp  Can be done by careful visualiza:on, e.g. flawed in sen:ment analysis</p>
    <p>No discrete categories or words, everything is a con:nuous vector. Wed like have symbolic features like NP, VP, etc. and see why their combina:on makes sense.</p>
    <p>True, but most of language is fuzzy and many words have so\ rela:onships to each other. Also, many NLP features are already not human-understandable (e.g, concatena:ons/combina:ons of different features).</p>
  </div>
  <div class="page">
    <p>Inference Challenges</p>
    <p>Many latent variables involved in understanding language (sense ambiguity, parsing, seman:c role)</p>
    <p>Almost any inference mechanism can be combined with deep learning</p>
    <p>See [BoBou, Bengio, LeCun 97], [Graves 2012]</p>
    <p>Complex inference can be hard (exponen:ally) and needs to be approximate  learn to perform inference</p>
  </div>
  <div class="page">
    <p>Learning Multiple Levels of Abstraction</p>
    <p>The big payoff of deep learning is to allow learning higher levels of abstrac:on</p>
    <p>Going from symbols to embeddings already helps, and recent years have shown that phrase and sentence representa:ons work too, but the space of possibili:es is much wider there</p>
    <p>Higher-level abstrac:ons disentangle the factors of varia:on, which allows much easier generaliza:on and transfer</p>
  </div>
  <div class="page">
    <p>Advantages</p>
    <p>Despite a small community in the intersec:on of deep learning and NLP, already many state of the art results on a variety of language tasks</p>
    <p>O\en very simple matrix deriva:ves (backprop) for training and matrix mul:plica:ons for tes:ng</p>
    <p>Fast inference and well suited for mul:-core CPUs/GPUs</p>
  </div>
  <div class="page">
    <p>Transfer Learning</p>
    <p>Applica:on of deep learning could be in areas where there are not enough labeled data but a transfer is possible</p>
    <p>Domain adapta:on already showed that effect, thanks to unsupervised feature learning</p>
    <p>Transfer to resource-poor languages would be a great applica:on [Gouws, PhD proposal 2012]</p>
  </div>
  <div class="page">
    <p>The End</p>
  </div>
</Presentation>
