<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sahil Suneja, Elliott Baron, Eyal de Lara, Ryan Johnson</p>
    <p>Accelerating The Cloud with Heterogeneous Computing</p>
  </div>
  <div class="page">
    <p>GPGPU Computing</p>
    <p>Data Parallel Tasks  Apply a fixed operation in parallel to each</p>
    <p>element of a data array</p>
    <p>Examples  Bioinformatics  Data Mining  Computational Finance  NOT Systems Tasks</p>
    <p>High-latency memory copying</p>
  </div>
  <div class="page">
    <p>Game Changer  On-Chip GPUs</p>
    <p>Processors combining CPU/GPU on one die  AMD Fusion APU, Intel Sandy/Ivy Bridge  Share Main Memory  Very Low Latency  Energy Efficient</p>
  </div>
  <div class="page">
    <p>Accelerating The Cloud</p>
    <p>Use GPUs to accelerate Data Parallel Systems Tasks  Better Performance  Offload CPU for other tasks  No Cache Pollution  Better Energy Efficiency (Silberstein et al, SYSTOR 2011)</p>
    <p>Cloud Environment particularly attractive  Hybrid CPU/GPU will make it to the data center  GPU cores likely underutilized  Useful for Common Hypervisor Tasks</p>
  </div>
  <div class="page">
    <p>Data Parallel Cloud Operations</p>
    <p>Memory Scrubbing  Batch Page Table Updates  Memory Compression  Virus Scanning  Memory Hashing</p>
  </div>
  <div class="page">
    <p>Complications  Different Privilege Levels  Multiple Users</p>
    <p>Requirements  Performance Isolation  Memory Protection</p>
    <p>Hardware Management</p>
  </div>
  <div class="page">
    <p>Hardware Management</p>
    <p>Management Policies  VMM Only  Time Multiplexing  Space Multiplexing</p>
  </div>
  <div class="page">
    <p>Memory Access</p>
    <p>All Tasks mentioned assume GPU can Directly Access Main (CPU) Memory  Many require Write Access</p>
    <p>Currently, CPU &lt;-&gt; GPU copying required!  Even though both share Main Memory</p>
    <p>Makes some tasks infeasible on GPU, others less efficient</p>
  </div>
  <div class="page">
    <p>Case Study  Page Sharing</p>
    <p>De-duplicate Memory  Hashing identifies sharing candidates  Remove all, but one physical copy  Heavy on CPU  Scanning Frequency  Sharing Opportunities</p>
  </div>
  <div class="page">
    <p>Memory Hashing Evaluation</p>
    <p>CPU GPU CPU GPU</p>
    <p>Fusion Discrete</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>)</p>
    <p>Running Time (CPU vs. GPU)</p>
  </div>
  <div class="page">
    <p>Conclusion/Summary</p>
    <p>Hybrid CPU/GPU Processors Are Here  Get Full Benefit in Data Centres</p>
    <p>Accelerate and Offload Administrative Tasks</p>
    <p>Need to Consider Effective Management and Remedy Memory Access Issues</p>
    <p>Memory Hashing Example Shows Promise  Over Order of Magnitude Faster</p>
  </div>
  <div class="page">
    <p>Extra Slides</p>
  </div>
  <div class="page">
    <p>Memory Hashing Evaluation</p>
    <p>Memory Kernel Memory Kernel</p>
    <p>Fusion Discrete</p>
    <p>Ti m</p>
    <p>e (m</p>
    <p>s)</p>
    <p>Running Time (Memory vs. Kernel)</p>
  </div>
  <div class="page">
    <p>CPU Overhead</p>
    <p>Measure performance degradation of CPUHeavy program</p>
    <p>Hashing via CPU = 50% Overhead  Hashing via GPU = 25% Overhead</p>
    <p>Without Memory Transfers = 11% Overhead</p>
  </div>
</Presentation>
