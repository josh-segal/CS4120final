<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep Learning for Natural Language Inference NAACL-HLT 2019 Tutorial</p>
    <p>Sam Bowman NYU (New York)</p>
    <p>Xiaodan Zhu Queens University, Canada</p>
    <p>Follow the slides: nlitutorial.github.io</p>
  </div>
  <div class="page">
    <p>Introduction Motivations of the Tutorial Overview</p>
    <p>Starting Questions ...</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>NLI: What and Why (SB)</p>
    <p>Data for NLI (SB)</p>
    <p>Some Methods (SB)</p>
    <p>Deep Learning Models (XZ)</p>
    <p>Full Models</p>
    <p>---(Break, roughly at 10:30)--</p>
    <p>Sentence Vector Models</p>
    <p>Selected Topics</p>
    <p>Applications (SB)</p>
  </div>
  <div class="page">
    <p>Natural Language Inference: What and Why</p>
  </div>
  <div class="page">
    <p>Why NLI?</p>
  </div>
  <div class="page">
    <p>My take, as someone interested in natural language understanding...</p>
  </div>
  <div class="page">
    <p>The Motivating Questions</p>
    <p>Can current neural network methods learn to do anything that resembles compositional semantics?</p>
  </div>
  <div class="page">
    <p>The Motivating Questions Can current neural network</p>
    <p>methods learn to do anything that resembles compositional semantics?</p>
    <p>If we take this as a goal to work toward, whats our metric?</p>
  </div>
  <div class="page">
    <p>One possible answer: Natural Language</p>
    <p>Inference (NLI)</p>
    <p>also known as recognizing textual entailment (RTE)</p>
    <p>i'm not sure what the overnight low was</p>
    <p>{entails, contradicts, neither}</p>
    <p>I don't know how cold it got last night.</p>
    <p>Dagan et al. 05, MacCartney 09 Example from MNLI</p>
    <p>Premise or Text or Sentence A</p>
    <p>Hypothesis or Sentence B</p>
  </div>
  <div class="page">
    <p>A Definition</p>
    <p>We say that T entails H if, typically, a human reading T would infer that H is most likely true.</p>
    <p>- Ido Dagan 05</p>
    <p>(See Manning 06 for discussion.)</p>
  </div>
  <div class="page">
    <p>The Big Question What kind of a thing is the meaning of a sentence?</p>
  </div>
  <div class="page">
    <p>What kind of a thing is the meaning of a sentence?</p>
    <p>The Big Question</p>
  </div>
  <div class="page">
    <p>The Big Question What kind of a thing is the meaning of a sentence?</p>
    <p>Why not?</p>
  </div>
  <div class="page">
    <p>What kind of a thing is the meaning of a sentence?</p>
    <p>The Big Question</p>
  </div>
  <div class="page">
    <p>What kind of a thing is the meaning of a sentence?</p>
    <p>What concrete phenomena do you have to deal with to understand a sentence?</p>
    <p>The Big Question</p>
  </div>
  <div class="page">
    <p>Judging Understanding with NLI</p>
    <p>To reliably perform well at NLI, your method for sentence understanding must be able to interpret and use the full range of phenomena we talk about in compositional semantics:*</p>
    <p>Lexical entailment (cat vs. animal, cat vs. dog)  Quantification (all, most, fewer than eight)  Lexical ambiguity and scope ambiguity (bank, ...)  Modality (might, should, ...)  Common sense background knowledge</p>
    <p>* without grounding to the outside world.</p>
  </div>
  <div class="page">
    <p>Why not Other Tasks?</p>
    <p>Many tasks that have been used to evaluate sentence representation models dont require models to deal with the full complexity of compositional semantics:</p>
    <p>Sentiment analysis  Sentence similarity</p>
  </div>
  <div class="page">
    <p>Why not Other Tasks?</p>
    <p>NLI is one of many NLP tasks that require robust compositional sentence understanding:</p>
    <p>Machine translation  Question answering  Goal-driven dialog  Semantic parsing  Syntactic parsing  Imagecaption matching</p>
    <p>But its the simplest of these.</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>Most formal semantics research (and some semantic parsing research) deals with truth conditions.</p>
    <p>?</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>Most formal semantics research (and some semantic parsing research) deals with truth conditions.</p>
    <p>In this view understanding a sentence means (roughly) characterizing the set of situations in which that sentence is true.</p>
    <p>?</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>Most formal semantics research (and some semantic parsing research) deals with truth conditions.</p>
    <p>In this view understanding a sentence means (roughly) characterizing the set of situations in which that sentence is true.</p>
    <p>This requires some form of grounding:</p>
    <p>Truth-conditional semantics is strictly harder than NLI.</p>
    <p>?</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>If you know the truth conditions of two sentences, can you work out whether one entails the other?</p>
    <p>?</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>If you know the truth conditions of two sentences, can you work out whether one entails the other?</p>
    <p>S2 S1 ?</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>Can you work out whether one sentence entails another without knowing their truth conditions?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Detour: Entailments and Truth Conditions</p>
    <p>Can you work out whether one sentence entails another without knowing their truth conditions?</p>
    <p>Isobutylphenylpropionic acid is a medicine for headaches.</p>
    <p>{entails, contradicts, neither} ?</p>
    <p>Isobutylphenylpropionic acid is a medicine.</p>
    <p>See Katz 72</p>
  </div>
  <div class="page">
    <p>Another set of motivations...</p>
    <p>-Bill MacCartney, Stanford CS224U Slides</p>
    <p>Well re visit this</p>
    <p>later!</p>
  </div>
  <div class="page">
    <p>Natural Language Inference: Data</p>
    <p>...an incomplete survey</p>
  </div>
  <div class="page">
    <p>FraCaS Test Suite</p>
    <p>346 examples  Manually constructed by</p>
    <p>experts  Target strict logical entailment</p>
    <p>P: No delegate finished the report. H: Some delegate finished the report on time. Label: no entailment</p>
    <p>Cooper et al. 96, MacCartney 09</p>
  </div>
  <div class="page">
    <p>Recognizing Textual Entailment</p>
    <p>(RTE) 17</p>
    <p>Seven annual competitions (First PASCAL, then NIST)</p>
    <p>Some variation in format, but about 5000 NLI-format examples total</p>
    <p>Premises (texts) drawn from naturally occurring text, often long/complex</p>
    <p>Expert-constructed hypotheses</p>
    <p>P: Cavern Club sessions paid the Beatles 15 evenings and 5 lunchtime. H: The Beatles perform at Cavern Club at lunchtime. Label: entailment</p>
    <p>Dagan et al. 06 et seq.</p>
  </div>
  <div class="page">
    <p>Sentences Involving Compositional</p>
    <p>Knowledge (SICK)</p>
    <p>Corpus for a 2014 SemEval shared task competition</p>
    <p>Deliberately restricted task: No named entities, idioms, etc.</p>
    <p>Pairs created by semi-automatic manipulation rules on image and video captions</p>
    <p>About 10,000 examples, labeled for entailment and semantic similarity (15 scale)</p>
    <p>P: The brown horse is near a red barrel at the rodeo H: The brown horse is far from a red barrel at the rodeo Label: contradiction</p>
    <p>Marelli et al. 14</p>
  </div>
  <div class="page">
    <p>The Stanford NLI Corpus (SNLI)</p>
    <p>Premises derived from image captions (Flickr 30k), hypotheses created by crowdworkers</p>
    <p>About 550,000 examples; first NLI corpus to see encouraging results with neural networks</p>
    <p>P: A black race car starts up in front of a crowd of people. H: A man is driving down a lonely road. Label: contradiction</p>
    <p>Bowman et al. 15</p>
  </div>
  <div class="page">
    <p>Multi-Genre NLI (MNLI)</p>
    <p>Multi-genre follow-up to SNLI: Premises come from ten different sources of written and spoken language (mostly via OpenANC), hypotheses written by crowdworkers</p>
    <p>About 400,000 examples</p>
    <p>P: yes now you know if if everybody like in August when everybody's on vacation or something we can dress a little more casual H: August is a black out month for vacations in the company. Label: contradiction</p>
    <p>Williams et al. 18</p>
  </div>
  <div class="page">
    <p>Multi-Premise Entailment (MPE)</p>
    <p>Multi-premise entailment from a set of sentences describing a scene</p>
    <p>Derived from Flickr30k image captions</p>
    <p>About 10,000 examples</p>
  </div>
  <div class="page">
    <p>Crosslingual NLI (XNLI)</p>
    <p>P:   H:   Label: contradiction</p>
    <p>A new development and test set for MNLI, translated into 15 languages</p>
    <p>About 7,500 examples per language</p>
    <p>Meant to evaluate cross-lingual transfer: Train on English MNLI, evaluate on another target language(s)</p>
    <p>Sentences translated one-by-one, so some inconsistencies</p>
  </div>
  <div class="page">
    <p>P:   H:   Label: contradiction</p>
    <p>A new development and test set for MNLI, translated into 15 languages</p>
    <p>About 7,500 examples per language</p>
    <p>Meant to evaluate cross-lingual transfer: Train on English MNLI, evaluate on another target language(s)</p>
    <p>Sentences translated one-by-one, so some inconsistencies</p>
    <p>Crosslingual NLI (XNLI)</p>
  </div>
  <div class="page">
    <p>SciTail P: Cut plant stems and insert stem into tubing while stem is submerged in a pan of water. H: Stems transport water to other parts of the plant through a system of tubes. Label: neutral</p>
    <p>Created by pairing statements from science tests with information from the web</p>
    <p>First NLI set built entirely on existing text</p>
    <p>About 27,000 pairs</p>
  </div>
  <div class="page">
    <p>In Depth: SNLI and MNLI</p>
  </div>
  <div class="page">
    <p>First: Entity and Event Coreference in NLI</p>
  </div>
  <div class="page">
    <p>One event or two?</p>
    <p>Premise: A boat sank in the Pacific Ocean.</p>
    <p>Hypothesis: A boat sank in the Atlantic Ocean.</p>
  </div>
  <div class="page">
    <p>One event or two? One.</p>
    <p>Premise: A boat sank in the Pacific Ocean.</p>
    <p>Hypothesis: A boat sank in the Atlantic Ocean.</p>
    <p>Label: contradiction</p>
  </div>
  <div class="page">
    <p>Premise: Ruth Bader Ginsburg was appointed to the US Supreme Court.</p>
    <p>Hypothesis: I had a sandwich for lunch today</p>
    <p>One event or two?</p>
  </div>
  <div class="page">
    <p>Premise: Ruth Bader Ginsburg was appointed to the US Supreme Court.</p>
    <p>Hypothesis: I had a sandwich for lunch today</p>
    <p>Label: neutral</p>
    <p>One event or two? Two.</p>
  </div>
  <div class="page">
    <p>Premise: A boat sank in the Pacific Ocean.</p>
    <p>Hypothesis: A boat sank in the Atlantic Ocean.</p>
    <p>Label: neutral</p>
    <p>One event or two? Two.</p>
    <p>But if we allow for this, then can we ever get a contradiction between two natural sentences?</p>
  </div>
  <div class="page">
    <p>One event or two? One, always.</p>
    <p>Premise: A boat sank in the Pacific Ocean.</p>
    <p>Hypothesis: A boat sank in the Atlantic Ocean.</p>
    <p>Label: contradiction</p>
  </div>
  <div class="page">
    <p>Premise: Ruth Bader Ginsburg was appointed to the US Supreme Court.</p>
    <p>Hypothesis: I had a sandwich for lunch today</p>
    <p>Label: contradiction</p>
    <p>One event or two? One, always.</p>
    <p>How do we turn tricky constraint this into something annotators can learn quickly?</p>
  </div>
  <div class="page">
    <p>Premise: Ruth Bader Ginsburg being appointed to the US Supreme Court.</p>
    <p>Hypothesis: A man eating a sandwich for lunch.</p>
    <p>Label: cant be the same photo (so: contradiction)</p>
    <p>One photo or two? One, always.</p>
  </div>
  <div class="page">
    <p>Our Solution: The SNLI Data Collection Prompt</p>
  </div>
  <div class="page">
    <p>Source captions from Flickr30k: Young, et al. 14 48</p>
  </div>
  <div class="page">
    <p>Entailment</p>
    <p>Source captions from Flickr30k: Young, et al. 14 49</p>
  </div>
  <div class="page">
    <p>Entailment</p>
    <p>Neutral</p>
    <p>Source captions from Flickr30k: Young, et al. 14 50</p>
  </div>
  <div class="page">
    <p>Entailment</p>
    <p>Neutral</p>
    <p>Contradiction</p>
    <p>Source captions from Flickr30k: Young, et al. 14 51</p>
  </div>
  <div class="page">
    <p>What we got</p>
  </div>
  <div class="page">
    <p>Some sample results</p>
    <p>Premise: Two women are embracing while holding to go packages.</p>
    <p>Hypothesis: Two woman are holding packages.</p>
    <p>Label: Entailment</p>
  </div>
  <div class="page">
    <p>Some sample results</p>
    <p>Premise: A man in a blue shirt standing in front of a garage-like structure painted with geometric designs.</p>
    <p>Hypothesis: A man is repainting a garage</p>
    <p>Label: Neutral</p>
  </div>
  <div class="page">
    <p>MNLI</p>
  </div>
  <div class="page">
    <p>MNLI</p>
    <p>Same intended definitions for labels: Assume coreference.</p>
    <p>More genresnot just concrete visual scenes.  Needed more complex annotator guidelines and more</p>
    <p>careful quality control, but reached same level of annotator agreement.</p>
  </div>
  <div class="page">
    <p>What we got</p>
  </div>
  <div class="page">
    <p>Typical Dev Set Examples</p>
    <p>Premise: In contrast, suppliers that have continued to innovate and expand their use of the four practices, as well as other activities described in previous chapters, keep outperforming the industry as a whole.</p>
    <p>Hypothesis: The suppliers that continued to innovate in their use of the four practices consistently underperformed in the industry.</p>
    <p>Label: Contradiction</p>
    <p>Genre: Oxford University Press (Nonfiction books)</p>
  </div>
  <div class="page">
    <p>Typical Dev Set Examples</p>
    <p>Premise: someone else noticed it and i said well i guess thats true and it was somewhat melodious in other words it wasnt just you know it was really funny</p>
    <p>Hypothesis: No one noticed and it wasnt funny at all.</p>
    <p>Label: Contradiction</p>
    <p>Genre: Switchboard (Telephone Speech)</p>
  </div>
  <div class="page">
    <p>Key Figures</p>
  </div>
  <div class="page">
    <p>The Train-Test Split</p>
  </div>
  <div class="page">
    <p>Genre Train Dev Test</p>
    <p>Captions (SNLI Corpus) (550,152) (10,000) (10,000)</p>
    <p>Fiction 77,348 2,000 2,000</p>
    <p>Government 77,350 2,000 2,000</p>
    <p>Slate 77,306 2,000 2,000</p>
    <p>Switchboard (Telephone Speech) 83,348 2,000 2,000</p>
    <p>Travel Guides 77,350 2,000 2,000</p>
    <p>The MNLI Corpus</p>
  </div>
  <div class="page">
    <p>Genre Train Dev Test</p>
    <p>Captions (SNLI Corpus) (550,152) (10,000) (10,000)</p>
    <p>Fiction 77,348 2,000 2,000</p>
    <p>Government 77,350 2,000 2,000</p>
    <p>Slate 77,306 2,000 2,000</p>
    <p>Switchboard (Telephone Speech) 83,348 2,000 2,000</p>
    <p>Travel Guides 77,350 2,000 2,000</p>
    <p>Face-to-Face Speech 0 2,000 2,000</p>
    <p>Letters 0 2,000 2,000</p>
    <p>OUP (Nonfiction Books) 0 2,000 2,000</p>
    <p>Verbatim (Magazine) 0 2,000 2,000</p>
    <p>Total 392,702 20,000 20,000</p>
    <p>The MNLI Corpus</p>
  </div>
  <div class="page">
    <p>Genre Train Dev Test</p>
    <p>Captions (SNLI Corpus) (550,152) (10,000) (10,000)</p>
    <p>Fiction 77,348 2,000 2,000</p>
    <p>Government 77,350 2,000 2,000</p>
    <p>Slate 77,306 2,000 2,000</p>
    <p>Switchboard (Telephone Speech) 83,348 2,000 2,000</p>
    <p>Travel Guides 77,350 2,000 2,000</p>
    <p>Face-to-Face Speech 0 2,000 2,000</p>
    <p>Letters 0 2,000 2,000</p>
    <p>OUP (Nonfiction Books) 0 2,000 2,000</p>
    <p>Verbatim (Magazine) 0 2,000 2,000</p>
    <p>Total 392,702 20,000 20,000</p>
    <p>The MNLI Corpus</p>
    <p>genre-matched evaluation</p>
    <p>genre-mismatched evaluation</p>
    <p>Good news:</p>
    <p>Most models perform similarly on both sets!</p>
  </div>
  <div class="page">
    <p>Annotation Artifacts</p>
  </div>
  <div class="page">
    <p>Annotation Artifacts</p>
    <p>For SNLI:</p>
    <p>P: ???</p>
    <p>H: Someone is not crossing the road.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>Poliak et al. 18, Tsuchiya 18, Gururangan et al. 18</p>
  </div>
  <div class="page">
    <p>Annotation Artifacts</p>
    <p>For SNLI:</p>
    <p>P: ???</p>
    <p>H: Someone is not crossing the road.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>Poliak et al. 18, Tsuchiya 18, Gururangan et al. 18</p>
  </div>
  <div class="page">
    <p>Annotation Artifacts</p>
    <p>For SNLI:</p>
    <p>P: ???</p>
    <p>H: Someone is not crossing the road.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>P: ???</p>
    <p>H: Someone is outside.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>Poliak et al. 18, Tsuchiya 18, Gururangan et al. 18</p>
  </div>
  <div class="page">
    <p>Annotation Artifacts</p>
    <p>For SNLI:</p>
    <p>P: ???</p>
    <p>H: Someone is not crossing the road.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>P: ???</p>
    <p>H: Someone is outside.</p>
    <p>Label: entailment, contradiction, neutral?</p>
    <p>Poliak et al. 18, Tsuchiya 18, Gururangan et al. 18</p>
  </div>
  <div class="page">
    <p>Models can do moderately well on NLI datasets without looking at the hypothesis!</p>
    <p>Single-genre SNLI especially vulnerable. SciTail not immune.</p>
    <p>Annotation Artifacts</p>
  </div>
  <div class="page">
    <p>Models can do moderately well on NLI datasets without looking at the hypothesis!</p>
    <p>...but hypothesis-only models are still far below ceiling. These datasets are easier than they look, but not trivial.</p>
    <p>Annotation Artifacts</p>
  </div>
  <div class="page">
    <p>Natural Language Inference: Some Methods</p>
    <p>(This is not the deep learning part.) 72Sam Bowman</p>
  </div>
  <div class="page">
    <p>Feature-Based Models</p>
    <p>Some earlier NLI work involved learning with shallow features:</p>
    <p>Bag of words features on hypothesis</p>
    <p>Bag of word-pairs features to capture alignment</p>
    <p>Tree kernels  Overlap measures like BLEU</p>
    <p>These methods work surprisingly well, but not competitive on current benchmarks.</p>
  </div>
  <div class="page">
    <p>Natural Logic</p>
    <p>Much non-ML work on NLI involves natural logic:</p>
    <p>A formal logic for deriving entailments between sentences.</p>
    <p>Operates directly on parsed sentences (natural language), no explicit logical forms.</p>
    <p>Generally sound but far from completeonly supports inferences between sentences with clear structural parallels.</p>
    <p>Most NLI datasets arent strict logical entailment, and require some unstated premisesthis is hard.</p>
  </div>
  <div class="page">
    <p>Theorem Proving</p>
    <p>Another thread of work has attempted to translate sentences into logical forms (semantic parsing) and use theorem proving methods to find valid inferences.</p>
    <p>Open-domain semantic parsing is still hard!</p>
    <p>Unstated premises and common sense can still be a problem.</p>
  </div>
  <div class="page">
    <p>In Depth: Natural Logic</p>
  </div>
  <div class="page">
    <p>Monotonicity</p>
    <p>...</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Poll: Monotonicity</p>
    <p>Which of these contexts are upward monotone?</p>
    <p>Example: Some dogs are cute This is upward monotone, since you can replace dogs with a more general term like animals, and the sentence must still be true.</p>
  </div>
  <div class="page">
    <p>MacCartneys Natural Logic Label Set</p>
    <p>MacCartney and Manning 09 82</p>
  </div>
  <div class="page">
    <p>Beyond Up and Down: Projectivity</p>
    <p>MacCartney and Manning 09 83</p>
  </div>
  <div class="page">
    <p>Chains of Relations</p>
    <p>If we know A | B and B ^ C, what do we know?</p>
    <p>So A  C</p>
    <p>MacCartney and Manning 09 84</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>MacCartney and Manning 09</p>
    <p>Whats the relation between the things we substituted? Look this up.</p>
    <p>Whats the relation between this sentence and the previous sentence? Use projectivity/monotonicity.</p>
    <p>Whats the relation between this sentence and the original sentence? Use join.</p>
  </div>
  <div class="page">
    <p>Natural Logic: Limitations</p>
    <p>Efficient, sound inference procedure, but  ...not complete.</p>
    <p>De Morgans laws for quantifiers:  All dogs bark.  No dogs dont bark.</p>
    <p>(Plus common sense and unstated premises.)</p>
  </div>
  <div class="page">
    <p>Natural Language Inference: Deep Learning Methods</p>
  </div>
  <div class="page">
    <p>Deep-Learning Models for NLI</p>
    <p>Before we delve into Deep Learning (DL) models ...</p>
    <p>Right, there are many really good reasons we should be excited about DL-based models.</p>
  </div>
  <div class="page">
    <p>Deep-Learning Models for NLI</p>
    <p>Before we delve into Deep Learning (DL) models ...</p>
    <p>Right, there are many really good reasons we should be excited about DL-based models.</p>
    <p>But, there are also many good reasons we want to know nice non-DL research performed before.</p>
  </div>
  <div class="page">
    <p>Deep-Learning Models for NLI</p>
    <p>Before we delve into Deep Learning (DL) models ...</p>
    <p>Right, there are many really good reasons we should be excited about DL-based models.</p>
    <p>But, there are also many good reasons we want to know nice non-DL research performed before.</p>
    <p>Also, it is alway intriguing to think how the final NLI models (if any) would look like, or at least, whats the limitations of existing DL models.</p>
  </div>
  <div class="page">
    <p>We roughly organize our discussion on deep learning models for NLI by two typical categories:</p>
    <p>Category I: NLI models that explore both sentence representation and cross-sentence statistics (e.g., cross-sentence attention). (Full models)</p>
    <p>Category II: NLI models that do not use cross-sentence information. (Sentence-vector-based models)</p>
    <p>This category of models is of interest because NLI is a good test bed for learning representation for sentences, as discussed earlier in the tutorial.</p>
    <p>Two Categories of Deep Learning Models for NLI</p>
  </div>
  <div class="page">
    <p>Full deep-learning models for NLI  Baseline models and typical components  NLI models enhanced with syntactic structures  NLI models considering semantic roles  Incorporating external knowledge</p>
    <p>Incorporating human-curated structured knowledge</p>
    <p>Leveraging unstructured data with unsupervised pretraining</p>
    <p>Sentence-vector-based NLI models  A top-ranked model in RepEval-2017 Shared Task  Current top model based on dynamic self-attention</p>
    <p>Several additional topics</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Full deep-learning models for NLI  Baseline models and typical components  NLI models enhanced with syntactic structures  NLI models considering semantic roles  Incorporating external knowledge</p>
    <p>Incorporating human-curated structured knowledge</p>
    <p>Leveraging unstructured data with unsupervised pretraining</p>
    <p>Sentence-vector-based NLI models  A top-ranked model in RepEval-2017 Shared Task  Current top model based on dynamic self-attention</p>
    <p>Several additional topics</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Layer 3: Inference Composition/Aggregation</p>
    <p>Layer 2: Local Inference Modeling</p>
    <p>Layer 1: Input Encoding</p>
    <p>ESIM uses BiLSTM, but different architectures can be used here, e.g., transformer-based, ELMo, densely connected CNN, tree-based models, etc.</p>
    <p>Collect information to perform local inference between words or phrases. (Some heuristics works well in this layer.)</p>
    <p>Perform composition/aggregation over local inference output to make the global judgement.</p>
    <p>Enhanced Sequential Inference Models (ESIM)</p>
  </div>
  <div class="page">
    <p>Layer 3: Inference Composition/Aggregation</p>
    <p>Layer 2: Local Inference Modeling</p>
    <p>Layer 1: Input Encoding</p>
    <p>ESIM uses BiLSTM, but different architectures can be used here, e.g., transformer-based, ELMo, densely connected CNN, tree-based models, etc.</p>
    <p>Collect information to perform local inference between words or phrases. (Some heuristics works well in this layer.)</p>
    <p>Perform composition/aggregation over local inference output to make the global judgement.</p>
    <p>Enhanced Sequential Inference Models (ESIM)</p>
  </div>
  <div class="page">
    <p>Encoding Premise and Hypothesis</p>
    <p>For a premise sentence a and a hypothesis sentence b:</p>
    <p>we can apply different encoders (e.g., here BiLSTM):</p>
    <p>where i denotes the output vector of BiLSTM at the position i of premise, which encodes word ai and its context.</p>
  </div>
  <div class="page">
    <p>Enhanced Sequential Inference Models (ESIM)</p>
    <p>Layer 3: Inference Composition/Aggregation</p>
    <p>Layer 2: Local Inference Modeling</p>
    <p>Layer 1: Input Encoding</p>
    <p>ESIM uses BiLSTM, but different architectures can be used here, e.g., transformer-based, densely connected CNN, tree-based models, etc.</p>
    <p>Collect information to perform local inference between words or phrases. (Some heuristics works well in this layer.)</p>
    <p>Perform composition/aggregation over local inference output to make the global judgement.</p>
  </div>
  <div class="page">
    <p>There are animals outdoors</p>
    <p>Local Inference Modeling</p>
    <p>Two dogs are running through a fieldPremise</p>
    <p>Hypothesis</p>
  </div>
  <div class="page">
    <p>There are animals outdoors</p>
    <p>Local Inference Modeling</p>
    <p>Two dogs are running through a fieldPremise</p>
    <p>Hypothesis</p>
    <p>Attention Weights</p>
    <p>Attention content</p>
  </div>
  <div class="page">
    <p>There are animals outdoors</p>
    <p>Local Inference Modeling</p>
    <p>Two dogs are running through a fieldPremise</p>
    <p>Hypothesis</p>
    <p>Attention Weights</p>
    <p>Attention content</p>
  </div>
  <div class="page">
    <p>The (cross-sentence) attention content is computed along both the premise-to-hypothesis and hypothesis-to-premise direction.</p>
    <p>Local Inference Modeling</p>
    <p>where,</p>
    <p>(ESIM tried several more complicated functions of</p>
    <p>, which did not further help.) 101</p>
  </div>
  <div class="page">
    <p>With soft alignment ready, we can collect local inference information.</p>
    <p>Note that in various NLI models, the following heuristics have shown to work very well:</p>
    <p>For premise, at each time step i, concatenate i and i , together with their:</p>
    <p>element-wise product,</p>
    <p>element-wise difference.</p>
    <p>(The same is performed for the hypothesis.) 102</p>
    <p>Local Inference Modeling</p>
  </div>
  <div class="page">
    <p>Some questions:  Instead of using chain RNN, how about other NN</p>
    <p>architectures?  How if one has access to more knowledge than that in</p>
    <p>training data? - e.g., lexical entailment information like Minneapolis is</p>
    <p>part of Minnesota.</p>
    <p>We will come back to these questions later.</p>
    <p>Some questions so far ...</p>
  </div>
  <div class="page">
    <p>Enhanced Sequential Inference Models (ESIM)</p>
    <p>Layer 3: Inference Composition/Aggregation</p>
    <p>Layer 2: Local Inference Modeling</p>
    <p>Layer 1: Input Encoding</p>
    <p>ESIM uses BiLSTM, but different architectures can be used here, e.g., transformer-based, densely connected CNN, tree-based models, etc.</p>
    <p>Collect information to perform local inference between words or phrases. (Some heuristics works well in this layer.)</p>
    <p>Perform composition/aggregation over local inference output to make the global judgement.</p>
  </div>
  <div class="page">
    <p>The next component is to perform composition/aggregation over local inference knowledge collected above.</p>
    <p>BiLSTM can be used here to perform composition over local inference:</p>
    <p>where</p>
    <p>Then by concatenating the average and max-pooling of ma and mb, we obtain a vector v which is fed to a classifier.</p>
    <p>Inference Composition/Aggregation</p>
  </div>
  <div class="page">
    <p>Performance of ESIM on SNLI</p>
  </div>
  <div class="page">
    <p>Models Enhanced with Syntactic</p>
    <p>Structures</p>
  </div>
  <div class="page">
    <p>Syntax has been used in many non-neural NLI/RTE systems (MacCartney, 09; Dagan et al. 13).</p>
    <p>How to explore syntactic structures in NN-based NLI systems? Several typical models:</p>
    <p>Hierarchical Inference Models (HIM) (Chen et al., 17) (full model)</p>
    <p>Stack-augmented Parser-Interpreter Neural Network (SPINN) (Bowman et al., 16) and follow-up work (sentence-vector-based models)</p>
    <p>Tree-Based CNN (TBCNN) (Mou et al., 16) (sentence-vector-based models)</p>
    <p>Models Enhanced with Syntactic Structures</p>
  </div>
  <div class="page">
    <p>ESIM HIM</p>
    <p>Parse information can be considered in different phases of NLI.</p>
    <p>Chen et al. 17</p>
  </div>
  <div class="page">
    <p>Tree LSTM</p>
    <p>Chain LSTM</p>
    <p>Tree LSTM</p>
  </div>
  <div class="page">
    <p>ESIM HIM</p>
    <p>Parse information can be first used to encode input sentences.</p>
    <p>Chen et al. 17</p>
  </div>
  <div class="page">
    <p>Attention weights showed that the tree models aligned sitting down with standing and the classifier relied on that to make the correct judgement.</p>
    <p>The sequential model, however, soft-aligned sitting with both reading and standing and confused the classifier.</p>
  </div>
  <div class="page">
    <p>ESIM HIM</p>
    <p>where, ma,t and mb,t are first passed through a feed-forward layer F(.) to reduce the number of parameters and alleviate overfitting.</p>
    <p>Perform composition on local inference information over trees:</p>
    <p>Chen et al. 17</p>
  </div>
  <div class="page">
    <p>Accuracy on SNLI</p>
  </div>
  <div class="page">
    <p>Effects of Different Components: Ablation Analysis</p>
    <p>Ablation Analysis (The numbers are classification accuracy.)</p>
  </div>
  <div class="page">
    <p>Evans et al. (2018) constructed a dataset and explored deep learning models for detecting entailment in formal logic:</p>
    <p>The aim is to help understand two questions:</p>
    <p>Can neural networks understand logical formulae well enough to detect entailment?</p>
    <p>Which architectures are the best?</p>
    <p>When annotating the data, efforts have been made to avoid annotation artifacts.  E.g. positive (entailment) and negative (non-entailment)</p>
    <p>examples must have the same distribution w.r.t. length of the formulae.</p>
    <p>Tree Models for Entailment in Formal Logic</p>
  </div>
  <div class="page">
    <p>Tree Models for Entailment in Formal Logic</p>
    <p>The results suggested that, if the structure of input is given, unambiguous, and a central feature of the task, models that explicitly exploit structures (e.g., treeLSTM) outperform models which must implicitly model the structure of sequences.</p>
  </div>
  <div class="page">
    <p>SPINN: Doing Away with Test-Time Tree</p>
    <p>Shift-reduce parser:  Shift unattached leaves from a buffer onto a processing stack.  Reduce the top two child nodes on the stack to a single parent node.</p>
    <p>SPINN: Jointly train a treeRNN and a vector-based shift-reduce parser.</p>
    <p>During training time, trees offer supervision for shift-reduce parser. No need for test time trees!</p>
  </div>
  <div class="page">
    <p>SPINN: Doing Away with Test-Time Tree</p>
    <p>Word vectors start on buffer.  Shift: moves word vectors from buffer to stack.  Reduce: pops top two vectors off the stack, applies</p>
    <p>f R : R d  R d  R d, and pushes the result back to the stack (i.e., treeRNN composition).</p>
    <p>Tracker LSTM: tracks parser/composer state across operations, decides shift-reduce operations, and is supervised by both observed shift-reduce</p>
    <p>operations and end-task. 119</p>
  </div>
  <div class="page">
    <p>SPINN + RL: Doing Away with Training-Time Tree</p>
    <p>Identical to SPINN at test time, but uses the reinforce algorithm at training time to compute gradients for the transition classification function.</p>
    <p>Better than LSTM baselines: model captures and exploits structure.</p>
    <p>Model is not biased by what linguists think trees should be like.</p>
  </div>
  <div class="page">
    <p>Williams et al. (2018) conducted a comprehensive comparison on models that use explicit linguistic tree and latent trees.</p>
    <p>The models include those proposed by Yogatama et al. (2017) and Choi et al. (2018) as well as variants of SPINN.</p>
    <p>Their main findings include:</p>
    <p>The learned latent trees are helpful in the construction of semantic representations for sentences.</p>
    <p>The best available models for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics.</p>
    <p>Do Latent Tree Learning Identify Meaningful Structure?</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
  </div>
  <div class="page">
    <p>Intermission Slides: nlitutorial.github.io</p>
    <p>NLI Tutorial</p>
  </div>
  <div class="page">
    <p>Models Enhanced with Semantic Roles</p>
  </div>
  <div class="page">
    <p>Recent research (Zhang et al., 19) incorporated Semantic Role Labeling (SRL) into NLI and found it improved the performance.</p>
    <p>The proposed model simply concatenated SRL embedding into word embedding.</p>
    <p>Models Enhanced with Semantic Roles</p>
  </div>
  <div class="page">
    <p>The proposed method is reported to be very effective when used with pretrained models, e.g., ELMo (Peters et al., 17), GPT (Radford et al., 18), and BERT (Devlin et al., 18).</p>
    <p>ELMo: pretrained model is used to initialize an existing NLI models input-encoding layers. It does not change or replace the NLI model itself. (Feature-based pretrained models)</p>
    <p>GPT and BERT: pretrained architectures and parameters are both used to perform NLI, parameters are finetuned in NLI, and otherwise no NLI-specific models/components are further used. (Finetuning-based pretrained models)</p>
    <p>Models Enhanced with Semantic Roles</p>
  </div>
  <div class="page">
    <p>Models Enhanced with Semantic Roles</p>
    <p>Accuracy on SNLI</p>
    <p>Zhang et al. 19</p>
  </div>
  <div class="page">
    <p>Modeling External Knowledge</p>
    <p>There are at least two ways to add into NLI systems external knowledge that does not present in training data:</p>
    <p>leveraging structured (often human-curated) knowledge</p>
    <p>using unsupervisedly pretrained models</p>
  </div>
  <div class="page">
    <p>Leveraging Structured Knowledge</p>
    <p>Modeling External Knowledge</p>
  </div>
  <div class="page">
    <p>NLI Models Enhanced with External Knowledge: The KIM Model</p>
    <p>Overall architecture of Knowledge-based Inference Model (KIM) (Chen et al. 18)</p>
  </div>
  <div class="page">
    <p>Intuitively lexical semantics such as synonymy, antonymy, hypernymy, and co-hyponymy may help soft-align a premise to its hypothesis.</p>
    <p>Specifically, rij is a vector of semantic relations between i th</p>
    <p>word in a premise and jth word in its hypothesis. The relations can be extracted from resources such as WordNet/ConceptNet or embedding learned from a knowledge graph.</p>
    <p>NLI Models Enhanced with External Knowledge: The KIM Model</p>
    <p>Knowledge-enhanced co-attention:</p>
  </div>
  <div class="page">
    <p>Local inference with external knowledge:</p>
    <p>Enhancing inference composition/aggregation:</p>
    <p>In addition to helping soft-alignment, external knowledge can also bring richer entailment information that does not exist in training data.</p>
    <p>NLI Models Enhanced with External Knowledge: The KIM Model</p>
  </div>
  <div class="page">
    <p>Accuracy on SNLI</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Performance of KIM under different sizes of training-data.</p>
    <p>Performance of KIM under different amounts of external knowledge.</p>
    <p>Chen et al. 18</p>
  </div>
  <div class="page">
    <p>For a premise in SNLI, Glockner et al. (2018) generated a hypothesis by replacing a single word in the premise.</p>
    <p>The aim is to help test if a NLI systems can actually learn simple lexical and word knowledge.</p>
    <p>Premise: A South Korean woman gives a manicure.</p>
    <p>Hypothesis: A North North Korean woman gives a manicure.</p>
    <p>KIM performs much better than other models on this dataset.</p>
    <p>Accuracy on the Glockner Dataset</p>
  </div>
  <div class="page">
    <p>Modeling External Knowledge</p>
    <p>Leveraging Unsupervised Pretraining</p>
  </div>
  <div class="page">
    <p>Pretrained models can leverage large unannotated datasets, which have brought forward the state of the art of NLI and many other tasks.</p>
    <p>See (Peters et al., 17, Radford et al., 18, Devlin et al., 18) for more details.</p>
    <p>Whether/how the models using human-curated structured knowledge (e.g., KIM) and those using unsupervised pretraining (e.g., BERT) complement each other?</p>
    <p>Pretrained Models on Unannotated Data</p>
  </div>
  <div class="page">
    <p>External Knowledge: BERT vs. KIM</p>
  </div>
  <div class="page">
    <p>Oracle accuracy of pairs of systems (if one of the two systems under concern makes the correct prediction on a test case, we count it as correct) on a subset of the stress test proposed by Naik et al. (2018).</p>
    <p>BERT and KIM seem to complement each other more than other pairs, e.g., BERT and GPT.</p>
    <p>More Analysis on Pairs of Systems</p>
  </div>
  <div class="page">
    <p>Full deep-learning models for NLI  Baseline models and typical components  NLI models enhanced with syntactic structures  NLI models considering semantic roles and discourse</p>
    <p>information</p>
    <p>Incorporating external knowledge  Incorporating human-curated structured knowledge</p>
    <p>Leveraging unstructured data with self-supervision (aka. unsupervised pretraining)</p>
    <p>Sentence-vector-based NLI models  A top-ranked model in RepEval-2017  Current top models based on dynamic self-attention</p>
    <p>Several additional topics</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>As discussed above, NLI is an important test bed for representation learning for sentences.</p>
    <p>Indeed, a capacity for reliable, robust, open-domain natural language inference is arguably a necessary condition for full natural language understanding (NLU). (MacCartney, 09)</p>
    <p>Sentence-vector-based models encode sentences and test the modeling quality on NLI.</p>
    <p>No cross-sentence attention is allowed, since the goal is to test representation quality for individual sentence.</p>
    <p>Sentence-vector-based Models</p>
  </div>
  <div class="page">
    <p>The RepEval-2017 Shared Task (Nangia et al., 17) adopted the MNLI dataset to evaluate sentence representation.</p>
    <p>We will discuss one of the top-ranked models (Chen et al., 17b). Other top models can be found in (Nie and Bansal, 17; Balazs et al., 17).</p>
    <p>RepEval-2017 Shared Task</p>
  </div>
  <div class="page">
    <p>RNN-Based Inference Model with Gated Attention</p>
    <p>Chen et al. 17b</p>
  </div>
  <div class="page">
    <p>In addition to average and max-pooling, weighted average over output is used:</p>
    <p>Gated Attention on Output</p>
    <p>The weights are computed using the input, forget, and output gates of the top-layer BiLSTM.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Accuracy of models on the MNLI test sets. Sentence-vector-based models seem to be sensitive to operations performed at the top layer of the networks, e.g., pooling or element-wise diff/product. See (Chen et al, 18b) for more work on generalized pooling.</p>
    <p>Chen et al. 18b</p>
  </div>
  <div class="page">
    <p>CNN with Dynamic Self-Attention In</p>
    <p>p u</p>
    <p>t S</p>
    <p>e n</p>
    <p>te n</p>
    <p>ce S</p>
    <p>e n</p>
    <p>te n</p>
    <p>ce E</p>
    <p>m b</p>
    <p>e d</p>
    <p>d in</p>
    <p>g</p>
    <p>So far, the model proposed by Yoon et al. (2018) achieves the best performance on SNLI among sentence-vector-based models.</p>
    <p>Key idea: stacks a dynamic self-attention over CNN (with dense connection)  The proposed dynamic self-attention borrows ideas from the Capsule</p>
    <p>Network (Sabour et al. 17; Hinton et al., 18).</p>
    <p>Yoon et al. 18, Sabour et al. 17, Hinton et al. 18</p>
  </div>
  <div class="page">
    <p>One important motivation for the Capsule Network is to better model part-whole relationship in images.  To recognize the left figure is a face but not the right one, the parts</p>
    <p>(here, nose, eyes and mouth) need to agree on how a face should look like (e.g., the faces position and orientation).</p>
    <p>Each part and the whole (here, a face) is represented as a vector.  Agreement is computed through dynamic routing.</p>
    <p>Capsule Networks</p>
    <p>Sabour et al. 17, Hinton et al. 18</p>
  </div>
  <div class="page">
    <p>Key differences:  Input of a capsule cell is a number of vectors (u</p>
    <p>scalar (x 1</p>
    <p>is a scalar).</p>
    <p>Voting parameters c 1</p>
    <p>, c 2</p>
    <p>, c 3</p>
    <p>are not part of model parameters  they</p>
    <p>are learned through dynamic routing and are not kept after training.</p>
    <p>Capsule Networks</p>
    <p>Capsule cell Regular neuron</p>
    <p>Sabour et al. 17, Hinton et al. 18</p>
  </div>
  <div class="page">
    <p>Key ideas:  A capsule at a lower layer needs to decide how to send its message to</p>
    <p>higher level capsules.</p>
    <p>The essence of the above algorithm is to ensure a lower level capsule will send more message to the higher level capsule that agrees with it</p>
    <p>(indicated by a high similarity between them).</p>
    <p>Dynamic Routing</p>
    <p>Sabour et al. 17, Hinton et al. 18</p>
  </div>
  <div class="page">
    <p>CNN with Dynamic Self-Attention for NLI</p>
    <p>The proposed model borrows the idea of weight adaptation method in dynamic routing to adapt attention weight a</p>
    <p>ij . (Note that in dynamic self-attention, weights</p>
    <p>are normalized along lower-level vectors, indexed by k, while in dynamic routing in CapsuleNet normalization is performed along higher-level vectors/capsules.)</p>
    <p>In addition, instead of performing multihead attention, the work performs multiple dynamic self-attention (DSA).</p>
    <p>Yoon et al. 18</p>
  </div>
  <div class="page">
    <p>CNN with Dynamic Self-Attention for NLI</p>
    <p>Current leaderboard of sentence-vector-based</p>
    <p>models on SNLI (as of June 1st, 2019).</p>
    <p>Publications Model Description Accuracy</p>
  </div>
  <div class="page">
    <p>Full deep-learning models for NLI  Baseline models and typical components  NLI models enhanced with syntactic structures  NLI models considering semantic roles and discourse</p>
    <p>information</p>
    <p>Incorporating external knowledge  Incorporating human-curated structured knowledge</p>
    <p>Leveraging unstructured data with self-supervision (aka. unsupervised pretraining)</p>
    <p>Sentence-vector-based NLI models  A top-ranked model in RepEval-2017  Current top models based on dynamic self-attention</p>
    <p>Several additional topics</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Revisiting Artifacts of Data</p>
  </div>
  <div class="page">
    <p>Breaking NLI Systems with Sentences that Require Simple Lexical Inferences</p>
    <p>As discussed above, Glockner et al. (2018) create a new test set that shows the deficiency of NLI systems in modeling lexical and world knowledge.</p>
    <p>The set is developed upon the SNLIs test set: for a premise sentence, a hypothesis is constructed by replacing one word in premise.</p>
  </div>
  <div class="page">
    <p>Breaking NLI Systems with Sentences that Require Simple Lexical Inferences</p>
    <p>The performance of NLI systems on the new test set is substantially worse, suggesting some drawback of the existing NLI systems/datasets in actually modelling NLI.</p>
    <p>Accuracy of models on SNLI and the Glockner dataset.</p>
  </div>
  <div class="page">
    <p>Naik et al. (2018) proposed an evaluation methodology consisting of automatically constructed test examples.</p>
    <p>The stress tests constructed are organized into three classes:</p>
    <p>Competence test: numerical reasoning and antonymy understanding.</p>
    <p>Distraction test: robustness on lexical similarity, negation, and word overlap.</p>
    <p>Noise test: robustness on spelling errors.</p>
    <p>Stress Tests for NLI</p>
  </div>
  <div class="page">
    <p>Stress Tests for NLI</p>
    <p>Classification accuracy (%) of state-of-the-art models on the stress tests. Three of the models, NB (Nie and Bansal, 17), CH (Chen et al., 17b), and RC (Balazs et al., 17) are models submitted to RepEvel-2017. IS (Conneau et al., 17) is a model proposed to learn general sentence embedding trained on NLI.</p>
  </div>
  <div class="page">
    <p>Wang et al. (2018) proposed the following idea: swapping the premise and hypothesis in the test set to create the diagnostic test.</p>
    <p>For entailment, a better model is supposed to report a larger difference of performance on the original test set and swapped test set.</p>
    <p>Models should have comparable accuracy on the original test set and swapped test set for contradiction and neutral.</p>
    <p>Swapping Premise and Hypothesis</p>
  </div>
  <div class="page">
    <p>Performance (accuracy) of different models on the original and swapped SNLI test set. Bigger differences (Diff-Test) for entailment (label E) suggests better models for entailment. Models that consider external semantic knowledge, e.g., KIM, seem to perform better in this swapping test.</p>
    <p>Swapping Premise and Hypothesis</p>
    <p>More work on analyzing the properties of NLI datasets can be found in Poliak et. al, 18, Talman and Chatzikyriakidis, 19.</p>
  </div>
  <div class="page">
    <p>Bringing Explanation to NLI</p>
  </div>
  <div class="page">
    <p>e-SNLI: Bringing Explanation to NLI</p>
    <p>e-SNLI extends SNLI with an additional layer of human-annotated natural language explanation.</p>
    <p>More research problems can be further explored:  Not just predict a label but also generate explanation.  Obtain full sentence justifications of a models decision.  Help transfer to out-of-domain NLI datasets.</p>
  </div>
  <div class="page">
    <p>e-SNLI: Bringing Explanation to NLI</p>
    <p>PREMISEAGNOSTIC: Generate an explanation given only the hypothesis.</p>
    <p>PREDICTANDEXPLAIN: Jointly predict a label and generate an explanation for the predicted label.</p>
    <p>EXPLAINTHENPREDICT: Generate an explanation then predict a label.</p>
    <p>REPRESENT: Universal sentence representations.</p>
    <p>TRANSFER: Transfer without fine-tuning to out-of-domain NLI.</p>
  </div>
  <div class="page">
    <p>Natural Language Inference: Applications</p>
  </div>
  <div class="page">
    <p>Three major application types for NLI:</p>
    <p>Direct application of trained NLI models.</p>
    <p>NLI as a research and evaluation task for new methods.</p>
    <p>NLI as a pretraining task in transfer learning.</p>
    <p>Applications</p>
  </div>
  <div class="page">
    <p>Inspired by issues surrounding fake news and automatic fact checking:</p>
    <p>The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia</p>
    <p>Direct Applications</p>
  </div>
  <div class="page">
    <p>Inspired by issues surrounding fake news and automatic fact checking.</p>
    <p>SNLI/MNLI models used in many systems, including winner, to decide whether a piece of evidence supports a claim.</p>
    <p>Direct Applications</p>
    <p>Thorne et al. 18, Nie et al. 18</p>
  </div>
  <div class="page">
    <p>Multi-hop reading comprehension tasks like MultiRC or OpenBook require models to answer a question by combining multiple pieces of evidence from some long text.</p>
    <p>Integrating an SNLI/MNLI-trained ESIM model into a larger model in two places helps to select and combine relevant evidence for a question.</p>
    <p>Direct Applications</p>
    <p>Trivedi et al. 19 (NAACL)</p>
  </div>
  <div class="page">
    <p>Direct Applications</p>
    <p>When generating video captions, using an SNLI/MNLI-trained entailment model as part of the objective function can lead to more effective training.</p>
  </div>
  <div class="page">
    <p>Direct Applications</p>
    <p>When generating long-form text, using an SNLI/MNLI-trained entailment model as a cooperative discriminator can prevent a language model from contradicting itself.</p>
  </div>
  <div class="page">
    <p>Evaluation Several entailment corpora have become established benchmark datasets for studying new ML methods in NLP.</p>
    <p>Used as a major evaluation when developing self-attention networks, language model pretraining, and much more.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Several entailment corpora have become established benchmark datasets for studying new ML methods in NLP.</p>
    <p>Used as a major evaluation when developing self-attention networks, language model pretraining, and much more.</p>
    <p>Also included in the SentEval, GLUE, DecaNLP, and SuperGLUE benchmarks and associated software toolkits.</p>
  </div>
  <div class="page">
    <p>Evaluation (a Caveat) State of the art models are very close to human performance on major evaluation sets:</p>
  </div>
  <div class="page">
    <p>Transfer Learning Training neural network models on large NLI datasets (especially MNLI) and then fine-tuning them on target tasks often yields substantial improvements in target task performance.</p>
  </div>
  <div class="page">
    <p>Transfer Learning</p>
    <p>Training neural network models on large NLI datasets (especially MNLI) and then fine-tuning them on target tasks often yields substantial improvements in target task performance.</p>
    <p>This works well even in conjunction with strong baselines for pretraining like SkipThought, ELMo, or BERT.</p>
    <p>Responsible for the current state of the art on the GLUE benchmark.</p>
  </div>
  <div class="page">
    <p>Summary and Conclusions</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>The tutorial covers the recent advance on NLI (aka. RTE) research, which is powered by:</p>
    <p>Large annotated datasets</p>
    <p>Deep learning models over distributed representation</p>
    <p>We view and discuss NLI as an important test bed for representation learning for natural language.</p>
    <p>We discuss the existing and potential applications of NLI.</p>
  </div>
  <div class="page">
    <p>Better supervised models (of course)</p>
    <p>Harder naturalistic benchmark datasets</p>
    <p>Explainability</p>
    <p>Better Unsupervised DL approaches</p>
    <p>Application of NLI on more NLP tasks</p>
    <p>Multimodal NLI</p>
    <p>NLI in domains: adaptation</p>
    <p>...</p>
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Questions?</p>
    <p>Slides and contact information: nlitutorial.github.io</p>
  </div>
  <div class="page">
    <p>Extra Slides</p>
  </div>
  <div class="page">
    <p>XNLI: Evaluating Cross-lingual Sentence Representations</p>
    <p>As NLI is a good test bed for NLU, cross-lingual NLI can be a good test bed for cross-lingual NLU.</p>
    <p>XNL: cross-lingual NLI dataset for 15 languages, each having 7,500 NLI sentence pairs and in total 112,500 pairs.</p>
    <p>Following the the construction processing used to construct the MNLI corpora.</p>
    <p>Can be used to evaluate both cross-lingual NLI models and multilingual text embedding models.</p>
  </div>
  <div class="page">
    <p>XNLI: Evaluating Cross-lingual Sentence Representations</p>
    <p>Test accuracy of baseline models. See more recent advance in (Lample &amp; Conneau, 2019)</p>
  </div>
  <div class="page">
    <p>The Discourse Marker Augmented Network (DMAN, Pan et al., 2018) uses discourse marker information to guide NLI decision.</p>
    <p>Inductive bias is built in for discourse-related words like but, although, so, because, etc.</p>
    <p>The Discourse Marker Prediction (Nie et al., 2017) is incorporated into DMAN through a reinforcement learning component.</p>
    <p>Models Enhanced with Discourse Markers</p>
  </div>
</Presentation>
