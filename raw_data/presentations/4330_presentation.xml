<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Searching for the X-Factor: Exploring Corpus Subjectivity for</p>
    <p>Word Embeddings</p>
    <p>Maksim Tkachenko and Chong Cher Chia and Hady W. Lauw</p>
    <p>Singapore Management University</p>
  </div>
  <div class="page">
    <p>Word Embeddings</p>
    <p>Dense vectors of words</p>
    <p>Unsupervised training: GloVe, Word2Vec</p>
    <p>Words in similar context tend to have similar meaning</p>
    <p>Words with similar meanings tend to be close in embedding space</p>
    <p>good   0.0335,0.1018,0.2300,  300 apple</p>
    <p>banana</p>
    <p>Melbourne</p>
  </div>
  <div class="page">
    <p>good  8321,235,63444,  Vocabulary Size ( 300)</p>
    <p>Counting Contexts</p>
    <p>good   0.0335,0.1018,0.2300,   300</p>
    <p>Reducing Dimensionality</p>
    <p>This camera is good for high quality</p>
    <p>Target Word</p>
    <p>Context Words</p>
    <p>Training Word Embeddings</p>
  </div>
  <div class="page">
    <p>good  ?, ?, ? ,  300</p>
    <p>Counting Contexts</p>
    <p>Reducing Dimensionality</p>
    <p>?</p>
    <p>good  ?, ?, ? ,   Vocabulary Size ( 300)</p>
    <p>Different Input Corpora</p>
  </div>
  <div class="page">
    <p>An article must be written from a neutral point of view, which among other things means representing fairly, proportionately, and, as far as possible, without editorial bias, all of the significant views that have been published by reliable sources on a topic.</p>
  </div>
  <div class="page">
    <p>Amazon values diverse opinions and that content [customer reviews] you submit should be relevant and based on your own honest opinions and experience.</p>
  </div>
  <div class="page">
    <p>Subjectivity Scale</p>
    <p>More SubjectiveMore Objective</p>
    <p>Objective Embeddings (OE)</p>
    <p>Subjective Embeddings (SE)</p>
  </div>
  <div class="page">
    <p>Binary Classification Tasks</p>
    <p>Sentiment Classification (positive vs. negative):  Amazon Reviews (24 categories) + Rotten Tomatoes Reviews</p>
    <p>A very funny movie vs. One lousy movie</p>
    <p>Subjectivity Classification (subjective vs. objective)  Rotten Tomatoes Reviews</p>
    <p>The story needs more dramatic meat vs. She's an artist</p>
    <p>Topic Classification (in-topic vs. out-of-topic)  Newsgroups Dataset (6 categories)</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Cross-validation on balanced samples</p>
    <p>Binary logistic regression classifier</p>
    <p>Sentence embedding = average of word embeddings</p>
    <p>The same number of sentences and the same vocabulary when training embeddings</p>
  </div>
  <div class="page">
    <p>Empirical Findings</p>
    <p>Subjectivity Classification</p>
    <p>Topic Classification</p>
    <p>Amazon Sentiment</p>
    <p>Rotten Tomatoes Sentiment</p>
    <p>Accuracy</p>
    <p>Objective Embeddings (OE)</p>
    <p>Subjective Embeddings (SE)</p>
    <p>SE understand sentiment words better than OE?</p>
    <p>SE and OE are very similar on</p>
    <p>objective tasks</p>
  </div>
  <div class="page">
    <p>Top Words Similar to good</p>
    <p>Word Similarity</p>
    <p>bad 0.68</p>
    <p>decent 0.67</p>
    <p>nice 0.62</p>
    <p>poor 0.61</p>
    <p>Word Similarity</p>
    <p>decent 0.78</p>
    <p>great 0.76</p>
    <p>nice 0.69</p>
    <p>terrific 0.64</p>
    <p>Objective Embeddings Subjective Embeddings</p>
  </div>
  <div class="page">
    <p>Sentiment Words Still Cause Troubles!</p>
    <p>Subjective Embeddings</p>
    <p>Word A Word B Their Similarity</p>
    <p>waste Save 0.51</p>
    <p>love hate 0.60</p>
    <p>loves hates 0.68</p>
    <p>easy difficult 0.56</p>
  </div>
  <div class="page">
    <p>SentiVec Embeddings</p>
    <p>Similar to good Similarity</p>
    <p>bad 0.68</p>
    <p>decent 0.67</p>
    <p>nice 0.62</p>
    <p>poor 0.61</p>
    <p>Objective Word2Vec Embeddings</p>
    <p>Similar to good Similarity</p>
    <p>decent 0.79</p>
    <p>nice 0.76</p>
    <p>perfect 0.75</p>
    <p>excellent 0.73</p>
    <p>Objective SentiVec Embeddings</p>
  </div>
  <div class="page">
    <p>SentiVec Word2Vec</p>
    <p>Negative: waste, junk, horrible, defective,</p>
    <p>Positive: love, great, recommend, easy,</p>
    <p>= + Lexical</p>
    <p>Resource</p>
    <p>Predicts context words as in Word2Vec Skip-gram</p>
    <p>Predicts word category</p>
    <p>SentiVec: Infusing Sentiment</p>
  </div>
  <div class="page">
    <p>Logistic SentiVec</p>
    <p>This camera is good for high quality</p>
    <p>(good, camera) (good, is) (good, for) (good, high)</p>
    <p>vs.</p>
    <p>Random Noise (good, frog) (good, duck)</p>
    <p>Word2Vec Skip-gram objective</p>
    <p>Lexical objective of SentiVec (two classes)</p>
    <p>good   =  good</p>
    <p>good   = 1   good</p>
    <p>good</p>
  </div>
  <div class="page">
    <p>Spherical SentiVec</p>
    <p>Negative Words</p>
    <p>Neutral Words</p>
    <p>Positive Words</p>
  </div>
  <div class="page">
    <p>Empirical Findings</p>
    <p>Objective Embeddings</p>
    <p>Subjective Embeddings</p>
    <p>Objective Embeddings</p>
    <p>Subjective Embeddings</p>
    <p>Amazon Sentiment (average over 24 categories)</p>
    <p>Rotten Tomatoes Sentiment</p>
    <p>Accuracy</p>
    <p>SentiVec does not affect objective classification tasks</p>
  </div>
  <div class="page">
    <p>Changes in Similarity Positive Words</p>
    <p>Negative Words</p>
    <p>Neutral Words</p>
    <p>Target Word: Good Target Word: Bad</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Explored effects of corpus subjectivity for word embeddings</p>
    <p>SentiVec, a method for infusing lexical information into word embeddings</p>
    <p>Sentiment-infused SentiVec embeddings space facilitate better sentiment-related similarity</p>
    <p>Pre-trained Word Embeddings &amp; Code: https://sentivec.preferred.ai/</p>
  </div>
</Presentation>
