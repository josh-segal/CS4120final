<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Massively Multilingual Transfer for NER</p>
    <p>Afshin Rahimi, Yuan Li, and Trevor Cohn University of Melbourne</p>
  </div>
  <div class="page">
    <p>Wikipedia:Jroehl 2</p>
  </div>
  <div class="page">
    <p>Emergency Response Named Entity Recognition</p>
  </div>
  <div class="page">
    <p>Annotation Projection for Transfer</p>
    <p>kailangan namin ng mas maraming dugo sa Pagasanjan .</p>
    <p>we need more blood in Pgasanjan ..</p>
    <p>O O O O O B-LOC O</p>
    <p>Yarowsky et al. (2001)</p>
    <p>Tagalog</p>
    <p>English</p>
    <p>B-LOC</p>
  </div>
  <div class="page">
    <p>Representation Projection for Transfer</p>
    <p>Mis-matched Model</p>
    <p>kailangan namin ng mas maraming dugo sa Pagasanjan .</p>
    <p>language independent representation</p>
    <p>Cross-lingual word embeddings (Lample et al., 2018)</p>
    <p>Ideal: source-target similar in word order, script, syntax</p>
  </div>
  <div class="page">
    <p>Direct Transfer for NER</p>
    <p>English</p>
    <p>kailangan namin ng mas maraming dugo sa Pagasanjan.</p>
    <p>kailangan namin ng mas maraming dugo sa Pagasanjan.</p>
    <p>kailangan namin ng mas maraming dugo sa Pagasanjan.</p>
    <p>Input: Unlabelled sentences in the target language encoded with cross-lingual embeddings</p>
    <p>Arabic Afrikaans</p>
    <p>Output: Labelled sentences in the target language</p>
    <p>Pre-trained NER source</p>
    <p>models</p>
    <p>O O O O B-LOC OO O B-PER O O OO B-PER O O B-LOC O</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Direct Transfer Results (NER F1 score, WikiANN)</p>
    <p>unsuprising 7</p>
  </div>
  <div class="page">
    <p>Direct Transfer Results (NER F1 score, WikiANN)</p>
    <p>unrelated 8</p>
  </div>
  <div class="page">
    <p>Direct Transfer Results (NER F1 score, WikiANN)</p>
    <p>asymmetry 9</p>
  </div>
  <div class="page">
    <p>Voting &amp; English are often poor!</p>
  </div>
  <div class="page">
    <p>General findings  Transfer strongest within</p>
    <p>language family (Germanic, Roman, Slavic-Cyr, Slavic-Latin)</p>
    <p>Asymmetry between use as source vs target language (Slavic-Cyr, Greek/Turkish/...)</p>
    <p>But lots of odd results &amp; overall highly noisy</p>
  </div>
  <div class="page">
    <p>Problem Statement</p>
    <p>Input:</p>
    <p>N black-box source models  Unlabelled data in target language  Little or no labelled data (few shot and zero shot)</p>
    <p>Output:</p>
    <p>Good predictions in the target language</p>
  </div>
  <div class="page">
    <p>Model 1: Few Shot Ranking and Retraining (RaRe)</p>
    <p>Source Model AR</p>
    <p>Source Model VI F1VI</p>
    <p>Source model qualities</p>
    <p>F1EN</p>
    <p>F1AR</p>
  </div>
  <div class="page">
    <p>Model 1: Few Shot Ranking and Retraining (RaRe)</p>
    <p>Source Model AR</p>
    <p>Source Model VI</p>
    <p>N training sets in Tagalog</p>
    <p>Dataset AR</p>
    <p>Dataset EN</p>
    <p>Dataset VI</p>
  </div>
  <div class="page">
    <p>Model 1: Few Shot Ranking and Retraining (RaRe)</p>
    <p>Final training set, a mixture of distilled knowledge</p>
    <p>l  source langs.</p>
    <p>Dataset l g(F1l) Training Set</p>
  </div>
  <div class="page">
    <p>Model 1: Few Shot Ranking and Retraining (RaRe)</p>
    <p>Zero-shot variant: uniform sampling without fine-tuning (RaReuns)</p>
  </div>
  <div class="page">
    <p>Lample et al., (2016)</p>
    <p>Our method is independent of model choice.</p>
    <p>Hierarchical BiLSTM-CRF as model</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>What if no gold labels are available?</p>
    <p>Inspired by Kim and Ghahramani (2012)</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>Predicted label of instance i by model j (observed)</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>True label of instance i</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>Model js confusion matrix between True and predicted labels.</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>Categorical Distribution</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>Uninformative Dirichlet Priors</p>
  </div>
  <div class="page">
    <p>Model 2: Zero Shot Transfer (BEA)</p>
    <p>Find Z to maximises P(Z|Y,,), using variational meanfield approx.</p>
    <p>Warm-start with MV.</p>
  </div>
  <div class="page">
    <p>Extensions to BEA</p>
  </div>
  <div class="page">
    <p>Benchmark: BWET (Xie et al., 2018)</p>
    <p>Single source annotation projection with bilingual dictionaries from cross-lingual word embeddings</p>
    <p>Transfer english training data to German, Dutch, and Spanish.</p>
    <p>Train a transformer NER on the projected training data.</p>
    <p>State-of-the-art on zero-shot NER transfer (orthogonal to this) 26</p>
  </div>
  <div class="page">
    <p>CoNLL Results (avg F1 over de, nl, es)</p>
    <p>Use parallel data, dictionary or wikipedia</p>
    <p>Zero shot</p>
    <p>Few shot</p>
    <p>High-resource</p>
  </div>
  <div class="page">
    <p>CoNLL Results (avg F1 over de, nl, es)</p>
    <p>Zero shot</p>
    <p>Few shot</p>
    <p>High-resource</p>
  </div>
  <div class="page">
    <p>CoNLL Results (avg F1 over de, nl, es)</p>
    <p>Few shot</p>
    <p>High-resource</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>CoNLL Results (avg F1 over de, nl, es)</p>
    <p>Few shot</p>
    <p>High-resource</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>WIKIANN NER Datasets (Pan et al., 2017)</p>
    <p>Silver annotations from Wikipedia for 282 languages.  We picked 41 languages based on availability of bilingual</p>
    <p>dictionaries.  Created balanced training/dev/test partitions</p>
    <p>(varying size of training according to data availability)</p>
    <p>github.com/afshinrahimi/mmner</p>
  </div>
  <div class="page">
    <p>L.O.O. over 41 languages</p>
  </div>
  <div class="page">
    <p>Tagalog</p>
    <p>Transfer from 40 source languages</p>
    <p>L.O.O. over 41 languages</p>
  </div>
  <div class="page">
    <p>L.O.O. over 41 languages</p>
  </div>
  <div class="page">
    <p>Tamil</p>
    <p>Transfer from 40 source languages</p>
    <p>L.O.O. over 41 languages</p>
  </div>
  <div class="page">
    <p>Use fasttext monolingual wiki embeddings mapped to English space using Identical Character Strings.</p>
    <p>Word representation: FastText/MUSE</p>
    <p>Conneau et al. (2017)</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Supervised: no transfer</p>
    <p>Low-resource</p>
    <p>High-resource</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Many low quality source models</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Single source (en)</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Bayesian ensembling</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>+spammer removal</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>MV between top 3 sources</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
    <p>Few shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Estimate BEA confusion &amp; prior from annotations</p>
    <p>Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
    <p>Few shot</p>
  </div>
  <div class="page">
    <p>Results: WikiANN</p>
    <p>Ranking Retraining Method (using character info) Low-resource</p>
    <p>High-resource</p>
    <p>Zero shot</p>
    <p>Few shot</p>
  </div>
  <div class="page">
    <p>Effect of increasing #source languages</p>
    <p>Methods robust to many varying quality source languages.</p>
    <p>Even better with few-shot supervision.</p>
  </div>
  <div class="page">
    <p>Takeaways I</p>
    <p>Transfer from multiple source languages helps because for many languages we dont know the best source language.</p>
    <p>takeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary</p>
  </div>
  <div class="page">
    <p>Takeaways II</p>
    <p>With multiple source languages, you need to estimate their qualities because uniform voting doesnt perform well.</p>
    <p>takeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary</p>
  </div>
  <div class="page">
    <p>Takeaways III</p>
    <p>A small training set in target language helps, and can be done cheaply and quickly (Garrette and Baldridge, 2013).</p>
    <p>takeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Datasets &amp; code</p>
    <p>github.com/afshinrahimi/mmner</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Map all scripts to IPA or Roman alphabet (good for shared embeddings and character-level transfer)</p>
    <p>uroman: Hermjakob et al. (2018)  epitran: Mortensen et al. (2018)</p>
    <p>Can we estimate the quality of source models/languages for a specific target language based on language characteristics (Littell et al., 2017)?</p>
    <p>Technique should apply beyond NER to other tasks. 50</p>
  </div>
</Presentation>
