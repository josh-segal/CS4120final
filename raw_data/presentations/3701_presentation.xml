<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Adversarial Preprocessing Understanding and Preventing Image-Scaling Attacks in Machine Learning Erwin Quiring, David Klein, Daniel Arp, Martin Johns and Konrad Rieck</p>
    <p>USENIX Security Symposium 2020</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Preprocessing data is often necessary for machine learning Image scaling is omnipresent in machine learning</p>
    <p>Downscaling</p>
    <p>Image-Scaling Attack</p>
    <p>Do not enter</p>
    <p>One Way</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 2</p>
  </div>
  <div class="page">
    <p>Image-Scaling Attacks</p>
    <p>Manipulated image changes appearance after downscaling</p>
    <p>Solve</p>
    <p>Source Image S</p>
    <p>Target Image T</p>
    <p>Modified Source A Output Image D scale</p>
    <p>S  A</p>
    <p>T  D</p>
    <p>Optimization problem:</p>
    <p>min(22) s.t. scale(S + ) T 6 e</p>
    <p>Both goals must be achieved: T ' D and S ' A Xiao et al. 2019</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 3</p>
  </div>
  <div class="page">
    <p>Threat Scenario in Machine Learning Possible attacks</p>
    <p>False predictions at test time</p>
    <p>Data manipulations at training time</p>
    <p>Capabilities and knowledge Attack is agnostic to learning model or data Knowledge of scaling algorithm only needed</p>
    <p>Quiring and Rieck 2020, Xiao et al. 2019</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 4</p>
  </div>
  <div class="page">
    <p>Contribution</p>
    <p>Our work provides the first comprehensive analysis of scaling attacks</p>
    <p>Root-cause analysis Understand why and when the attack works</p>
    <p>Prevention defenses Derive secure scaling algorithms</p>
    <p>Adaptive adversaries Show that defenses are robust against an adaptive adversary</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 5</p>
  </div>
  <div class="page">
    <p>Root-Cause: Scaling in General (1D)</p>
    <p>Scaling: Convolution between a source signal s and kernel w Output is computed by moving w over s as a sliding window</p>
    <p>s and w</p>
    <p>Downscaled signal</p>
    <p>Not all pixels contribute equally If step size exceeds kernel width: pixels are even ignored</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 6</p>
  </div>
  <div class="page">
    <p>Root-Cause: Scaling-Attack</p>
    <p>a and w</p>
    <p>Downscaled signal</p>
    <p>Adversary only modifies pixels with high weights Success depends on two key parameters:</p>
    <p>The scaling ratio ( step size) The kernel width</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 7</p>
  </div>
  <div class="page">
    <p>Prevention Defenses</p>
    <p>We consider two defenses to prevent the attack Both do not change the API of machine learning workflows</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 8</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Common imaging libraries evaluated 1. OpenCV used by Caffe, DeepLearning4j 2. Pillow used by PyTorch 3. tf.image used by TensorFlow</p>
    <p>All implemented scaling algorithms tested</p>
    <p>ImageNet dataset with VGG19 model used for evaluating predictions</p>
    <p>Russakovsky et al. 2015, Simonyan and Zisserman 2014</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 9</p>
  </div>
  <div class="page">
    <p>Attack Performance</p>
    <p>Nearest Bilinear Bicubic</p>
    <p>Su cc</p>
    <p>es s</p>
    <p>Ra te</p>
    <p>[% ]</p>
    <p>Target T ' Downscaled output D</p>
    <p>OpenCV Pillow TensorFlow</p>
    <p>Attack succeeds: Downscaled output is close to target image However, visibility depends on scaling ratio and algorithm</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 10</p>
  </div>
  <div class="page">
    <p>Defense Performance</p>
    <p>Reconstruction prevents attacks against all vulnerable algorithms Attacker can thus not achieve T ' D Reconstruction increases visual quality Defense allows reconstructing the original prediction</p>
    <p>Attack image A Output scale(A) Restored image R Output scale(R)</p>
    <p>Original attack With defense</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 11</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Solve</p>
    <p>Source Image S</p>
    <p>Target Image T</p>
    <p>Modified Source A Output Image D</p>
    <p>scale</p>
    <p>S  A</p>
    <p>T  D</p>
    <p>Analysis of image-scaling attacks</p>
    <p>s and w</p>
    <p>Root-cause analysis</p>
    <p>Effective defenses Nearest Bilinear Bicubic</p>
    <p>Su cc</p>
    <p>es s</p>
    <p>Ra te</p>
    <p>[% ]</p>
    <p>Comprehensive evaluation</p>
    <p>Further information and implementation at: https://scaling-attacks.net</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 12</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>[1] Erwin Quiring and Konrad Rieck. Backdooring and Poisoning Neural Networks with Image-Scaling Attacks. In: Deep Learning and Security Workshop (DLS). 2020.</p>
    <p>[2] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. In: International Journal of Computer Vision (IJCV) 115.3 (2015).</p>
    <p>[3] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. Tech. rep. arXiv:1409.1556, 2014.</p>
    <p>[4] Qixue Xiao, Yufei Chen, Chao Shen, Yu Chen, and Kang Li. Seeing is Not Believing: Camouflage Attacks on Image Scaling Algorithms. In: Proc. of USENIX Security Symposium. 2019.</p>
    <p>Erwin Quiring Adversarial Preprocessing Page 13</p>
  </div>
</Presentation>
