<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Queues dont matter when you can JUMP them</p>
    <p>Matthew P. Grosvenor Malte Schwarzkopf Ionel Gog Andrew W. Moore Robert N. M. Watson Steven Hand Jon Crowcroft</p>
    <p>Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 2</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 3</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 4</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 5</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 6</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 7</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 8</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
  </div>
  <div class="page">
    <p>Context 9</p>
    <p>Datacenter Networks</p>
    <p>- Commodity hardware</p>
    <p>- Static network topology</p>
    <p>- Single administrative domain</p>
    <p>- Some level of cooperation</p>
    <p>- Statistically Multiplexed  Google google.com/datacenters</p>
    <p>NO GU</p>
    <p>AR AN</p>
    <p>TEE S</p>
    <p>late ncy ^</p>
  </div>
  <div class="page">
    <p>Application impact 10</p>
    <p>Illustrative experiment</p>
    <p>- 12 node 10G test cluster</p>
    <p>- 8 nodes Hadoop MR</p>
    <p>- 2 nodes PTPd</p>
    <p>- 2 nodes memcached</p>
    <p>2015 Malcolm Scott</p>
  </div>
  <div class="page">
    <p>Application impact Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>PTP sync offset: close to zero = good</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>PTPd offset</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>Application impact Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>PTP sync offset: close to zero = good</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>memcached latency: lower = good</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>PTPd offset</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute. memcached avg. latency</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>Application impact</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>memcached latency: higher = bad</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>PTPd offset</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute. memcached avg. latency</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>PTP sync offset: away from zero = bad</p>
  </div>
  <div class="page">
    <p>Whats the problem? 14</p>
  </div>
  <div class="page">
    <p>Network Interference 15</p>
    <p>Hadoop memcached PTPd</p>
    <p>Switch</p>
  </div>
  <div class="page">
    <p>Network Interference 16</p>
    <p>Hadoop memcached PTPd</p>
    <p>Switch</p>
    <p>Queuing caused by Hadoop</p>
  </div>
  <div class="page">
    <p>Network Interference 17</p>
    <p>Hadoop memcached PTPd</p>
    <p>Switch</p>
    <p>Queuing caused by Hadoop</p>
    <p>Delaying traffic from PTPd and memcached</p>
  </div>
  <div class="page">
    <p>Key Idea 18</p>
    <p>Congestion from one application causes queuing that delays traffic from another* application.</p>
    <p>*possibly related</p>
    <p>Network Interference:</p>
  </div>
  <div class="page">
    <p>Solving network interference?</p>
    <p>Packet by Packet Generalised Processor Sharing (PGPS)</p>
    <p>Borrow some old ideas</p>
    <p>Parekh-Gallager Theorem</p>
    <p>(Weighted) Fair Queuing (WFQ)</p>
    <p>Differentiated Service Classes (diff-serv)</p>
  </div>
  <div class="page">
    <p>Solving network interference?</p>
    <p>Packet by Packet Generalised Processor Sharing (PGPS)</p>
    <p>Borrow some old ideas</p>
    <p>Parekh-Gallager Theorem</p>
    <p>(Weighted) Fair Queuing (WFQ)</p>
    <p>Differentiated Service Classes (diff-serv)</p>
    <p>Apply in a new context : Datacenters</p>
  </div>
  <div class="page">
    <p>Opportunities &amp; Constraints 21</p>
    <p>Datacenter Opportunities</p>
  </div>
  <div class="page">
    <p>Opportunities &amp; Constraints 22</p>
    <p>- Static network - Single admin domain - Cooperation</p>
    <p>Datacenter Opportunities</p>
  </div>
  <div class="page">
    <p>Opportunities &amp; Constraints 23</p>
    <p>- Static network - Single admin domain - Cooperation</p>
    <p>Datacenter Opportunities</p>
    <p>Deployability Constraints</p>
  </div>
  <div class="page">
    <p>Opportunities &amp; Constraints 24</p>
    <p>- Static network - Single admin domain - Cooperation</p>
    <p>- Unmodified applications - Unmodified kernel code - Commodity hardware</p>
    <p>Datacenter Opportunities</p>
    <p>Deployability Constraints</p>
  </div>
  <div class="page">
    <p>Understanding delays 25</p>
    <p>Switch</p>
    <p>Delay type I - Queuing Delay (Dq)</p>
    <p>Dq</p>
  </div>
  <div class="page">
    <p>Understanding delays 26</p>
    <p>Switch</p>
    <p>Delay type II - Servicing Delay (Ds)</p>
    <p>Ds</p>
  </div>
  <div class="page">
    <p>Understanding delays 27</p>
    <p>Switch</p>
    <p>Delay type II - Servicing Delay (Ds)</p>
  </div>
  <div class="page">
    <p>Understanding delays 28</p>
    <p>Switch</p>
    <p>Delay type II - Servicing Delay (Ds)</p>
  </div>
  <div class="page">
    <p>Understanding delays 29</p>
    <p>Switch</p>
    <p>Delay type II - Servicing Delay (Ds)</p>
  </div>
  <div class="page">
    <p>Understanding delays 30</p>
    <p>Switch</p>
    <p>Delay type II - Servicing Delay (Ds)</p>
  </div>
  <div class="page">
    <p>Understanding delays 31</p>
    <p>Switch</p>
    <p>Dq</p>
    <p>Ds</p>
  </div>
  <div class="page">
    <p>Understanding delays 32</p>
    <p>Switch</p>
    <p>Servicing delay causes queuing delay</p>
    <p>Dq</p>
  </div>
  <div class="page">
    <p>Eliminating Queuing Delay 33</p>
    <p>Switch</p>
    <p>Ds 4 3 2 1</p>
  </div>
  <div class="page">
    <p>Eliminating Queuing Delay 34</p>
    <p>Switch</p>
    <p>Ds</p>
    <p>Ds</p>
  </div>
  <div class="page">
    <p>Key Idea 35</p>
    <p>If we can find a bound for servicing delay, we can ratelimit hosts so that they never experience queuing delay</p>
    <p>Rate-Limiting</p>
  </div>
  <div class="page">
    <p>Calculating Service Delay 36</p>
    <p>Switch</p>
    <p>Assume sending hosts n = 4</p>
  </div>
  <div class="page">
    <p>Calculating Service Delay 37</p>
    <p>Switch</p>
    <p>Assume edge speed</p>
    <p>R = 10Gb/s</p>
    <p>Assume sending hosts n = 4</p>
  </div>
  <div class="page">
    <p>Calculating Service Delay 38</p>
    <p>Switch</p>
    <p>Assume edge speed</p>
    <p>R = 10Gb/s</p>
    <p>Assume packet size P = 1500B</p>
    <p>Assume sending hosts n = 4</p>
  </div>
  <div class="page">
    <p>Calculating Service Delay 39</p>
    <p>Switch</p>
    <p>Assume edge speed</p>
    <p>R = 10Gb/s</p>
    <p>Assume packet size P = 1500B</p>
    <p>Assume sending hosts n = 4</p>
    <p>Delay per packet = P/R = 1500B / 10Gb/s = 1.5 s</p>
  </div>
  <div class="page">
    <p>Calculating Service Delay 40</p>
    <p>Switch</p>
    <p>Assume edge speed</p>
    <p>R = 10Gb/s</p>
    <p>Assume packet size P = 1500B</p>
    <p>Assume sending hosts n = 4</p>
    <p>Delay per packet = P/R = 1500B / 10Gb/s = 1.5 s</p>
  </div>
  <div class="page">
    <p>Calculating Servicing Delay 41</p>
    <p>P</p>
    <p>R n  servicing delay* =</p>
  </div>
  <div class="page">
    <p>Calculating Servicing Delay</p>
    <p>Where</p>
    <p>n - number of hosts P - bytes sent R - edge speed</p>
    <p>P</p>
    <p>R n  servicing delay* =</p>
    <p>*Assuming a fair scheduler</p>
  </div>
  <div class="page">
    <p>Calculating Servicing Delay</p>
    <p>Where</p>
    <p>n - number of hosts P - bytes sent R - edge speed</p>
    <p>P</p>
    <p>R n</p>
    <p>network** servicing delay* =</p>
    <p>*Assuming a fair scheduler **Apply hose constraint model</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>}Network Epoch Rate-Limiting</p>
  </div>
  <div class="page">
    <p>Eliminating Synchronization 45</p>
    <p>EpochEpoch</p>
  </div>
  <div class="page">
    <p>Eliminating Synchronization 46</p>
    <p>EpochEpoch</p>
  </div>
  <div class="page">
    <p>Eliminating Synchronization 47</p>
    <p>EpochEpoch</p>
  </div>
  <div class="page">
    <p>Eliminating Synchronization 48</p>
    <p>EpochEpoch</p>
  </div>
  <div class="page">
    <p>8 packets per epoch</p>
    <p>Eliminating Synchronization 49</p>
    <p>EpochEpoch</p>
  </div>
  <div class="page">
    <p>Eliminating Synchronization</p>
    <p>network epoch = 2n  P</p>
    <p>R</p>
    <p>Where</p>
    <p>n - number of hosts P - bytes sent R - edge speed 2 - mesochronous compensation</p>
  </div>
  <div class="page">
    <p>throughput =</p>
    <p>n is the number of hosts R is the edge speed</p>
    <p>Where</p>
  </div>
  <div class="page">
    <p>n = 1000 hosts R = 10 Gb/s</p>
    <p>Where</p>
    <p>The dark side of network epoch</p>
    <p>throughput*=</p>
  </div>
  <div class="page">
    <p>n = 1000 hosts R = 10 Gb/s</p>
    <p>Where</p>
    <p>The dark side of network epoch</p>
    <p>*at guaranteed latency!</p>
  </div>
  <div class="page">
    <p>solution: assume there is no problem?</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 55</p>
    <p>Pessimistic assumption of 4:1</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 56</p>
    <p>What if we assume 2:1?</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 57</p>
    <p>What if we assume 2:1? Hosts can send 2x the rate!</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 58</p>
    <p>What if we assume 1:1?</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 59</p>
    <p>What if we assume 1:1? Hosts can send 4x the rate!</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 60</p>
    <p>What if assumption is wrong?</p>
  </div>
  <div class="page">
    <p>Changing the assumptions 61</p>
    <p>What if assumption is wrong? Queuing will happen!</p>
  </div>
  <div class="page">
    <p>Which assumption? 62 4:1</p>
  </div>
  <div class="page">
    <p>Which assumption? 63 4:1</p>
    <p>Rate limit</p>
    <p>low throughput</p>
  </div>
  <div class="page">
    <p>Which assumption? 64</p>
    <p>Latency DistributionRate limit</p>
    <p>secs</p>
    <p>low throughput</p>
  </div>
  <div class="page">
    <p>Which assumption? 65 1:1</p>
  </div>
  <div class="page">
    <p>Which assumption? 66</p>
    <p>Rate limit</p>
    <p>line rate throughput</p>
  </div>
  <div class="page">
    <p>Which assumption? 67</p>
    <p>Latency DistributionRate limit</p>
    <p>secs</p>
    <p>line rate throughput</p>
  </div>
  <div class="page">
    <p>Which assumption? 68</p>
  </div>
  <div class="page">
    <p>Which assumption? 69</p>
  </div>
  <div class="page">
    <p>QJump with priorities 70</p>
    <p>Low latency</p>
    <p>High priority Low rate-limit</p>
  </div>
  <div class="page">
    <p>QJump with priorities 71</p>
    <p>Medium Latency Low latency</p>
    <p>Medium priority Medium rate-limit</p>
  </div>
  <div class="page">
    <p>QJump with priorities 72</p>
    <p>High Throughput Medium Latency Low latency</p>
    <p>Low priority No rate-limit</p>
  </div>
  <div class="page">
    <p>QJump with priorities 73</p>
    <p>High Throughput Medium Latency Low latency</p>
    <p>Queue Jumping!</p>
  </div>
  <div class="page">
    <p>QJump with priorities 74</p>
    <p>High Throughput Medium Latency Low latency</p>
    <p>Queues dont matter when you can Jump them!</p>
  </div>
  <div class="page">
    <p>Key Idea 75</p>
    <p>Use hardware priorities to run different QJump levels together, but isolated* from each other.</p>
    <p>Prioritization</p>
    <p>* from layers below</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
    <p>Linux TC</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
    <p>Linux TC</p>
    <p>~36 cycles / packet</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
    <p>Linux TC</p>
    <p>~36 cycles / packet</p>
    <p>Smart Buffer Sizing</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
    <p>Linux TC</p>
    <p>~36 cycles / packet</p>
    <p>Smart Buffer Sizing</p>
    <p>Unmodified Applications</p>
  </div>
  <div class="page">
    <p>ImplementationDraft of 01/10/2014, 17:49  please do not distribute. high throughput (low priority).</p>
    <p>We call the assignment of an f value to a priority a QJUMP level. The latency variance of a given QJUMP level is a function of the sum of the QJUMP levels above it. In Section 5, we discuss various ways of assigning f values to QJUMP levels.</p>
    <p>Rate limiting QJUMP differs from many other systems that use rate-limiters. Instead of requiring a rate-limiter for each flow, each host only needs one coarse-grained rate-limiter per QJUMP level. Hence, eight rate-limiters per host are sufficient when using IEEE 802.1Q priorities. As a result, QJUMP rate-limiters can be implemented efficiently in software.</p>
    <p>In our prototype, we use the queueing discipline (qdisc) mechanism offered by the Linux kernel traffic control (TC) subsystem to rate-limit packets. TC modules do not require kernel modifications and can be inserted and removed at runtime, making them flexible and easy to deploy. We also use Linuxs built-in 802.1Q VLAN support to send layer 2 priority-tagged packets.</p>
    <p>Listing 1 shows our a custom rate-limiter implementation. To keep the rate-limiter efficient, all operations quantify time in cycles. This requires us to initially convert the network epoch value from seconds into cycles (line 1). We then synthesize a clock from the CPU realtime counter (rdtsc, line 6). This gives microsecond granularity timing, but uses only one instruction on the critical path.</p>
    <p>When a new packet arrives at the rate-limiter, it is classified into a QJUMP level using the priority tag found in its sk buff (line 7). Users can set the priority directly in the application or use our application utility to assign priorities to sockets. Next, the rate-limiter checks if a new epoch has begun. If so, it issues a fresh allocation of bytes to itself (lines 810). It then checks to see if sufficient bytes are remaining to send the packet in this network epoch (line 12). If so, the packet is forwarded to the driver (line 1516), if not, the packet is dropped (line 13). In practice, packets are rarely dropped because our application utility also resizes socket buffers to apply early back-pressure.</p>
    <p>Forwarded packets are mapped onto individual driver</p>
    <p>Listing 1: QJUMP rate-limiter pseudocode.</p>
    <p>queues depending on the priority level. QJUMP therefore prioritizes low-latency traffic in the end-host itself, before packets are issued to the network card.</p>
    <p>Since Equation 2 assumes pessimal conditions, our rate-limiter also tolerates bursts up to the level-specific byte limit per epoch. This makes it compatible with hardware offload techniques such as TSO, LSO or GSO.</p>
    <p>On our test machines, we found no measurable effect of the rate-limiter on CPU utilization or throughput. On average it imposes a cost of 35.2 cycles per packet (s = 18.6; 99th% = 69 cycles) on the Linux kernel critical path of 8,000 cycles. This amounts to a less than 0.5% overhead.</p>
    <p>QJUMP Application Utility QJUMP requires that applications (or, specifically, sockets within applications) are assigned to QJUMP levels. This is easily done in application code directly with a setsockopt() using the SO PRIORITY option. However, we would also like to support unmodified applications without recompilation. To achieve this, we have implemented a utility that dynamically intercepts socket setup system calls and alters their options. We inject the utility into unmodified executables via the Linux dynamic linkers LD PRELOAD support (a similar technique to OpenOnload [31]).</p>
    <p>The utility performs two tasks: (i) it configures socket priority values, and (ii) it sets socket send buffer sizes. Modifying socket buffer sizes is an optimization to apply early back-pressure to applications. If an application sends more data than its QJUMP levels permits, an ENOBUFS error is returned rather than packets being dropped. While not strictly required, this optimization brings a significant performance benefit in practice as it helps avoid TCP retransmit timeouts (minRTOs).</p>
    <p>Linux TC</p>
    <p>~36 cycles / packet</p>
    <p>Smart Buffer Sizing</p>
    <p>Unmodified Applications</p>
  </div>
  <div class="page">
    <p>How well does it work?</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>How well does it work?</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>good = close to 1</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>good</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>good</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>ECN (WR</p>
    <p>ED)</p>
    <p>good</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>DCT CP</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>ECN (WR</p>
    <p>ED)</p>
    <p>good</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>DCT CP*</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>ECN (WR</p>
    <p>ED)</p>
    <p>good</p>
    <p>*currently requires kernel patch</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>DCT CP*</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>ECN (WR</p>
    <p>ED)</p>
    <p>good</p>
    <p>*currently requires kernel patch</p>
  </div>
  <div class="page">
    <p>How does it compare?Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Idea l</p>
    <p>Con tend</p>
    <p>ed</p>
    <p>Pau se fr</p>
    <p>ame s ECN DCT</p>
    <p>CP QJU</p>
    <p>MP 0.0</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R M</p>
    <p>S ap</p>
    <p>p. m</p>
    <p>et ric</p>
    <p>Hadoop runtime</p>
    <p>PTPd sync. offset</p>
    <p>memcached req. latency</p>
    <p>Figure 7: QJUMP comes closest to ideal performance for all of Hadoop, PTPd and memcached.</p>
    <p>down. Figure 7 shows that Ethernet Flow Control has a limited positive influence on memcached, but increases the RMS for PTPd. Hadoops performance remains unaffected.</p>
    <p>Early Congestion Notification (ECN) ECN is a network layer mechanism in which switches indicate queueing to end hosts by marking TCP packets. Our Arista 7050 switch implements ECN with Weighted Random Early Detection (WRED). The effectiveness of WRED depends on an administrator correctly configuring upper and lower marking thresholds. We investigated ten different marking thresholds pairs, ranging between [5, 10] and [2560, 5120] ([upper, lower], in packets). None of these settings achieve ideal performance for all three applications, but the best compromise was [40, 80]. With this configuration, ECN very effectively resolves the interference experienced by PTPd and memcached. However, this comes at the expense of increased Hadoop runtimes.</p>
    <p>Datacenter TCP (DCTCP) DCTCP uses the rate at which ECN markings are received to build an estimate of network congestion. It applies this to a new TCP congestion avoidance algorithm to achieve lower queueing delays [1]. We configured DCTCP with the recommended ECN marking thresholds of [65, 65]. Figure 7 shows that DCTCP reduces the variance in PTPd synchronization and memcached latency compared to the contended case. However, this comes at an increase in Hadoop job runtimes, as Hadoops bulk data transfers are affected by DCTCPs congestion avoidance.</p>
    <p>QJUMP Figure 7 shows that QJUMP achieves the best results. The variance in Hadoop, PTPd and memcached performance is close to (Hadoop, PTPd) or slightly better than (memcached) in the uncontended ideal case.</p>
    <p>Figure 8: 144 node leaf-spine topology used for simulation experiments.</p>
    <p>The pFabric architecture has been shown to schedule flows close to optimally [3]. Therefore, we compare QJUMP against pFabric to assess the quality of the network schedule it imposes. pFabric is a clean-slate design [that] requires modifications both at the switches and the end-hosts [3, 1] and is therefore only available in simulation. By contrast, QJUMP is far simpler and readily deployable, but applies rigid, global rate limits.</p>
    <p>We compare QJUMP against a TCP baseline, DCTCP and pFabric by extending an ns2 simulation provided by the authors of pFabric. This replicates the leaf-spine network topology used to evaluate pFabric (see Figure 8). We also run the same workloads derived from web search [1, 2.2] and data mining [16, 3.1] clusters in Microsoft datacenters, and show matching graphs in Figure 9.7 As in pFabric, we normalize flows to their ideal flow completion time on an idle network.</p>
    <p>Figure 9 reports the average and 99th percentile normalized FCTs for small flows (0kB, 100kB] and the average FCTs for large flows (10MB, ). For both workloads, QJUMP is configured with P = 9kB, n = 144, and { f0... f7} = {144,100,20,10,5,3,2,1}. We chose this configuration based on the distribution of flow sizes in the web search workload. However, in practice it worked well for both workloads.</p>
    <p>Despite its simplicity, QJUMP performs very well. As expected, it works best on short flows: on both workloads, QJUMP achieves average and 99th percentile FCTs close to or better than pFabrics. On the web-search workload, QJUMP beats pFabric by a margin of up to 32% at the 99th percentile (Fig. 9b). For larger flows, the results are mixed. On the web search workload, QJUMP</p>
    <p>DCT CP*</p>
    <p>Eth. Flow</p>
    <p>Cnt .</p>
    <p>ECN (WR</p>
    <p>ED)</p>
    <p>good</p>
    <p>best!</p>
    <p>QJU MP</p>
    <p>*currently requires kernel patch</p>
  </div>
  <div class="page">
    <p>Conclusions 92</p>
    <p>- \</p>
    <p>QJump applies datacenter simplifications to QoS rate calculations.</p>
  </div>
  <div class="page">
    <p>Conclusions 93</p>
    <p>- \</p>
    <p>QJump applies datacenter simplifications to QoS rate calculations.</p>
    <p>It provides service levels ranging from guaranteed latency through to line-rate throughput</p>
  </div>
  <div class="page">
    <p>Conclusions 94</p>
    <p>- \</p>
    <p>QJump applies datacenter opportunities to simplify QoS rate calculations.</p>
    <p>It can be deployed using without modifications to applications, kernel code or hardware.</p>
    <p>It provides service levels ranging from guaranteed latency through to line-rate throughput</p>
  </div>
  <div class="page">
    <p>Want to know more? 95</p>
  </div>
  <div class="page">
    <p>Want to know more? 96</p>
  </div>
  <div class="page">
    <p>Want to know more? 97</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
  </div>
  <div class="page">
    <p>Want to know more? 98</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[lo g 1 0 ]</p>
    <p>Max. latency 99%ile latency</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>kr eq</p>
    <p>/s ]</p>
    <p>best tradeoff</p>
    <p>Throughput</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
  </div>
  <div class="page">
    <p>Want to know more? 99</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[lo g 1 0 ]</p>
    <p>Max. latency 99%ile latency</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>kr eq</p>
    <p>/s ]</p>
    <p>best tradeoff</p>
    <p>Throughput</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>E nd</p>
    <p>-t o</p>
    <p>en d</p>
    <p>la te</p>
    <p>nc y</p>
    <p>[ s]</p>
    <p>A B C average 99th%ile 100th%ile</p>
    <p>latency bound</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
  </div>
  <div class="page">
    <p>Want to know more? 100</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[lo g 1 0 ]</p>
    <p>Max. latency 99%ile latency</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>kr eq</p>
    <p>/s ]</p>
    <p>best tradeoff</p>
    <p>Throughput</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>E nd</p>
    <p>-t o</p>
    <p>en d</p>
    <p>la te</p>
    <p>nc y</p>
    <p>[ s]</p>
    <p>A B C average 99th%ile 100th%ile</p>
    <p>latency bound</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>%</p>
    <p>Burst size / switch buffer size [log2]</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>re q/</p>
    <p>s]</p>
    <p>Broadcast UDP + QJump UDP + retries TCP</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>Want to know more? 101</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[lo g 1 0 ]</p>
    <p>Max. latency 99%ile latency</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>kr eq</p>
    <p>/s ]</p>
    <p>best tradeoff</p>
    <p>Throughput</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>E nd</p>
    <p>-t o</p>
    <p>en d</p>
    <p>la te</p>
    <p>nc y</p>
    <p>[ s]</p>
    <p>A B C average 99th%ile 100th%ile</p>
    <p>latency bound</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>%</p>
    <p>Burst size / switch buffer size [log2]</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>re q/</p>
    <p>s]</p>
    <p>Broadcast UDP + QJump UDP + retries TCP</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute. W</p>
    <p>eb -s</p>
    <p>ea rc</p>
    <p>h w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo g 1</p>
    <p>TCP DCTCP</p>
    <p>pFabric</p>
    <p>QJump</p>
    <p>(a) (0, 100kB]: average.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(b) (0, 100kB]: 99th percentile.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(c) (10MB, ): average.</p>
    <p>D at</p>
    <p>am</p>
    <p>in in</p>
    <p>g w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>TCP DCTCP</p>
    <p>pFabric QJump</p>
    <p>(d) (0, 100kB]: average.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(e) (0, 100kB]: 99th percentile.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(f) (10MB, ): average.</p>
    <p>Figure 9: Normalized flow completion times in a 144-host simulation (1 is ideal): QJUMP outperforms TCP, DCTCP and pFabric for small flows. N.B.: log-scale y-axis; QJUMP and pFabric overlap in (a), (d) and (e).</p>
    <p>outperforms pFabric by up to 20% at high load, but loses to pFabric by 15% at low load (Fig. 9c). On the data mining workload, QJUMPs average FCTs are between 30% and 63% worse than pFabrics (Fig. 9f).</p>
    <p>In the data-mining workload, 85% of all flows transfer fewer than 100kB, but over 80% of the bytes are transferred in flows of greater than 100MB (less than 15% of the total flows). QJUMPs short epoch intervals cannot sense the difference between large flows, so it does not apply any rate-limiting (scheduling) to them. This results in sub-optimal behavior. A combined approach where QJUMP regulates interactions between large flows and small flows, while DCTCP regulates the interactions between different large flows might improve this. We plan to investigate this in the future.</p>
    <p>a function of rate-limiting. Peak throughput is reached at a rate allocation of around 5Gb/s. At the same point, the request latency also stabilizes. Hence, a rate-limit of 5Gb/s gives the best tradeoff for memcached. This point has the strongest interference control possible without throughput restrictions. To convert this to a throughput factor, we get fi =</p>
    <p>nTi R by rearranging Equation 2 for fi.</p>
    <p>On our test-bed (n = 12 at R =10Gb/s), Ti =5Gb/s yields a throughput factor of f = 6. We can therefore choose a QJUMP level for memcached (e.g. f4) and set it to a throughput factor 6.</p>
    <p>QJUMP offers a bounded latency level at throughput factor f7. At this level, all packets admitted into the network must reach the destination by the end of the network epoch (3.1). We now show that our model and the derived configuration are correct. To do this, we perform a scale-up emulation using a 60-host virtualized topology running on ten physical machines (see Figure 12). In this topology, each machine runs a hypervisor (Linux kernel) with a 10Gb/s uplink to the network. Each hypervisor runs six guests (processes) each with a 1.6Gb/s network connection. We provision QJUMP for the number of guests and run two applications on each guest: (i) a coordination service that generates one 256 byte packet per network epoch at the highest QJUMP level, and (ii) a bulk sender that issues 1500 byte packets as fast as possible at the lowest QJUMP level. All coordination requests</p>
  </div>
  <div class="page">
    <p>Just one more thing 102</p>
  </div>
  <div class="page">
    <p>Just one more thing 103</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>All source data, patches and source code at http://camsas.org/qjump</p>
    <p>Conclusions 106</p>
    <p>This work was jointly supported by the EPSRC INTERNET Project EP/H040536/1 and the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL), under contract FA8750-11-C-0249. The views, opinions, and/or findings contained in this article/presentation are those of the author/presenter and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.</p>
    <p>QJump applies datacenter opportunities to simplify QoS rate calculations.</p>
    <p>It can be deployed using without modifications to applications, kernel code or hardware.</p>
    <p>It provides levels of service from guaranteed latency through to line-rate throughput</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>What is it good for? 108Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>%</p>
    <p>Burst size / switch buffer size [log2]</p>
    <p>hr ou</p>
    <p>gh pu</p>
    <p>t[ re</p>
    <p>q/ s]</p>
    <p>Broadcast UDP + QJump UDP + retries TCP</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>Accuracy of Switch Model 109</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[ s]</p>
    <p>modelled worst case</p>
  </div>
  <div class="page">
    <p>Accuracy of Switch Model 110</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s]</p>
    <p>modelled worst case</p>
  </div>
  <div class="page">
    <p>Sensitivity to f 111</p>
    <p>Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>nd -t</p>
    <p>oen</p>
    <p>d la</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s] A B C average 99th%ile 100th%ile</p>
    <p>latency bound</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
  </div>
  <div class="page">
    <p>ECN WRED Config. 112</p>
    <p>ECN minimum marking threshold [segments]</p>
    <p>or m</p>
    <p>al iz</p>
    <p>ed R</p>
    <p>M S</p>
    <p>la te</p>
    <p>nc y</p>
    <p>memcached PTPd Hadoop</p>
  </div>
  <div class="page">
    <p>Host based interference? 113</p>
  </div>
  <div class="page">
    <p>Switch Queue Interference 114Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>alone + iperf + iperf w/ QJ</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Ping (rpc) vs Iperf (bulk transfer)</p>
  </div>
  <div class="page">
    <p>How well does it work? 115Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>memcached key-value store vs Hadoop</p>
  </div>
  <div class="page">
    <p>How well does it work? 116Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>(a) CDF of packet latency across a switch. Note the change in x-axis scale at x = 5 s.</p>
    <p>(b) QJUMP reduces memcached request latency: CDF of 9 million samples.</p>
    <p>alone + Hadoop + Had. w/ QJ</p>
    <p>(c) QJ fixes Naiad barrier synchronization latency: CDF over 10k samples.</p>
    <p>Figure 3: Application-level latency experiments: QJUMP (green, dotted line) mitigates the latency tails from Figure 1.</p>
    <p>RPC vs. Bulk Transfer Remote Procedure Calls (RPCs) and bulk data transfers represent extreme ends of the latency-bandwidth spectrum. QJUMP resolves network interference at these extremes. As in 2.1, we emulate RPCs and bulk data transfers using ping and iperf respectively. We measure in-network latency for the ping traffic directly using a high resolution Endace DAG capture card and two optical taps on either side of a switch. This verifies that queueing latency at switches is reduced by QJUMP. By setting ping to the highest QJUMP level ( f7 = 1), we reduce its packets latency at the switch by over 300 (Figure 3a). The small difference between idle switch latency (1.6s) and QJUMP latency (24s) arises due a small on-chip FIFO through which the switch must process packets in-order. The switch processing delay, represented as e in Equation 2, is thus no more than 4s for each of our switches.</p>
    <p>Memcached QJUMP resolves network interference experienced by memcached sharing a network with Hadoop. We show this by repeating the memcached experiments in 2.2. In this experiment, memcached is configured at an intermediate QJUMP level, rate-limited to 5Gb/s (above memcacheds maximum throughput; see 6.5). Figure 3b shows the distribution (CDF) of memcached request latencies when running on an idle network, a shared network, and a shared network with</p>
    <p>QJUMP enabled. With QJUMP enabled, the request latencies are close to the ideal. The median latency improves from 824s in the shared case to 476s, a nearly 2 improvement.5</p>
    <p>Naiad Barrier Synchronization QJUMP also resolves network interference experienced by Naiad [24], a distributed system for executing data parallel dataflow programs. Figure 3c shows the latency distribution of a four-way barrier synchronization in Naiad. On an idle network network, 90% of synchronizations take no more than 600s. With interfering traffic from Hadoop, this value doubles to 1.2ms. When QJUMP is enabled, however, the distribution closely tracks the uncontended baseline distribution, despite sharing the network with Hadoop. QJUMP here offers a 25 improvement in application-level latency.</p>
    <p>Multi-application Environment In real-world datacenters, a range of applications with different latency and bandwidth requirements share same infrastructure. QJUMP effectively resolves network interference in these shared, multi-application environments. We consider a datacenter setup with three different applications: ptpd for time synchronization, memcached for serving small objects and Hadoop for batch data analysis. Since resolving on-host interference is outside the scope of our work, we avoid sharing hosts between applications in these experiments and share only the network infrastructure.</p>
    <p>Figure 5 (top) shows a timeline of average request latencies (over a 1ms window) for memcached and synchronization offsets for ptpd, each running alone on an otherwise idle network. Figure 5 (middle), shows the two applications sharing the network with Hadoop. In this case, average latencies increase for both applications and visible latency spikes (corresponding to Hadoops shuffle phases) emerge. With QJUMP deployed, we assign</p>
    <p>Naiad data processing framework vs Hadoop</p>
  </div>
  <div class="page">
    <p>How well does it work? 117Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>T im</p>
    <p>e [</p>
    <p>s]</p>
    <p>Figure 5: PTPd and memcached in isolation (top), with interfering traffic from Hadoop (middle) and with the interference mitigated by QJUMP (bottom).</p>
    <p>ptpd to f7 = 1, Hadoop to f0 = n = 12 and memcached to T5 = 5Gb/s =) f5 = 6 (see 6.5). The three applications now co-exist without interference (Figure 5 (bottom)). Hadoops performance is not noticeably affected by QJUMP, as we will further show in 6.3.</p>
    <p>Distributed Atomic Commit One of QJUMPs unique features is its guaranteed latency level (described in 3.1). Bounded latency enables interesting new designs for datacenter coordination software such as SDN control planes, fast failure detection and distributed consensus systems. To demonstrate the usefulness of QJUMPs bounded latency level, we built a simple distributed twophase atomic-commit (2PC) application.</p>
    <p>The application communicates over TCP or over UDP with explicit acknowledgements and retransmissions. Since QJUMP offers reliable delivery, the coordinator can send its messages by UDP broadcast when QJUMP is enabled. This optimization yields a 30% throughput improvement over both TCP and UDP.</p>
    <p>In Figure 6, we show the request rate for one coordinator and seven servers as a function of network interference. Interference is created with two traffic generators: on that generates a constant 10Gb/s of UDP traffic and another that sends fixed-size bursts followed by a 25ms pause. We report interference as the ratio of the burst size to the internal switch buffer size. Beyond a ratio of 200%, permanent queues build up in the switch. At this point the impact of retransmissions degrades throughput of the UDP and TCP implementations to 20% of the 10,000 requests/sec observed on an idle network. By</p>
    <p>Figure 6: QJUMP offers constant two-phase commit throughput even at high levels of network interference.</p>
    <p>contrast, the UDP-over-QJUMP implementation does not degrade as its messages jump the queue. At high interference ratios (&gt;200%), two-phase commit over QJUMP achieves 6.5 the throughput of standard TCP or UDP. Furthermore, QJUMPs reliable delivery and low latency enable very aggressive timeouts to be used for failure detection. Our 2PC system detects component failure within two network epochs (40s on our network), far faster than typical failure detection timeouts (e.g. 150 ms in RAMCloud [26, 4.6]).</p>
    <p>Since interference is transient in these experiments, we measure the degree to which it affects applications using the root mean square (RMS) of each application-specific metric.6 For Hadoop, the metric of interest is the job runtime, for PTPd it is the time synchronization offset and for memcached it is the request latency. Figure 7 shows six cases: an ideal case, a contended case and one for each of the four schemes used to mitigate network interference. All cases are normalized to the ideal case, which has each application running alone on an idle network. We discuss each result in turn.</p>
    <p>Ethernet Flow Control Like QJUMP, Ethernet Flow Control is a data link layer congestion control mechanism. Hosts and switches issue special pause messages when their queues are nearly full, alerting senders to slow</p>
  </div>
  <div class="page">
    <p>Flow Completion Times 118Draft of 01/10/2014, 17:49  please do not distribute. W</p>
    <p>eb -s</p>
    <p>ea rc</p>
    <p>h w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo g 1</p>
    <p>TCP DCTCP</p>
    <p>pFabric</p>
    <p>QJump</p>
    <p>(a) (0, 100kB]: average.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(b) (0, 100kB]: 99th percentile.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(c) (10MB, ): average.</p>
    <p>D at</p>
    <p>am</p>
    <p>in in</p>
    <p>g w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>TCP DCTCP</p>
    <p>pFabric QJump</p>
    <p>(d) (0, 100kB]: average.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(e) (0, 100kB]: 99th percentile.</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T [lo</p>
    <p>g 1 0 ]</p>
    <p>(f) (10MB, ): average.</p>
    <p>Figure 9: Normalized flow completion times in a 144-host simulation (1 is ideal): QJUMP outperforms TCP, DCTCP and pFabric for small flows. N.B.: log-scale y-axis; QJUMP and pFabric overlap in (a), (d) and (e).</p>
    <p>outperforms pFabric by up to 20% at high load, but loses to pFabric by 15% at low load (Fig. 9c). On the data mining workload, QJUMPs average FCTs are between 30% and 63% worse than pFabrics (Fig. 9f).</p>
    <p>In the data-mining workload, 85% of all flows transfer fewer than 100kB, but over 80% of the bytes are transferred in flows of greater than 100MB (less than 15% of the total flows). QJUMPs short epoch intervals cannot sense the difference between large flows, so it does not apply any rate-limiting (scheduling) to them. This results in sub-optimal behavior. A combined approach where QJUMP regulates interactions between large flows and small flows, while DCTCP regulates the interactions between different large flows might improve this. We plan to investigate this in the future.</p>
    <p>a function of rate-limiting. Peak throughput is reached at a rate allocation of around 5Gb/s. At the same point, the request latency also stabilizes. Hence, a rate-limit of 5Gb/s gives the best tradeoff for memcached. This point has the strongest interference control possible without throughput restrictions. To convert this to a throughput factor, we get fi =</p>
    <p>nTi R by rearranging Equation 2 for fi.</p>
    <p>On our test-bed (n = 12 at R =10Gb/s), Ti =5Gb/s yields a throughput factor of f = 6. We can therefore choose a QJUMP level for memcached (e.g. f4) and set it to a throughput factor 6.</p>
    <p>QJUMP offers a bounded latency level at throughput factor f7. At this level, all packets admitted into the network must reach the destination by the end of the network epoch (3.1). We now show that our model and the derived configuration are correct. To do this, we perform a scale-up emulation using a 60-host virtualized topology running on ten physical machines (see Figure 12). In this topology, each machine runs a hypervisor (Linux kernel) with a 10Gb/s uplink to the network. Each hypervisor runs six guests (processes) each with a 1.6Gb/s network connection. We provision QJUMP for the number of guests and run two applications on each guest: (i) a coordination service that generates one 256 byte packet per network epoch at the highest QJUMP level, and (ii) a bulk sender that issues 1500 byte packets as fast as possible at the lowest QJUMP level. All coordination requests</p>
  </div>
  <div class="page">
    <p>How to calculate f 119Draft of 01/10/2014, 17:49  please do not distribute.</p>
    <p>La te</p>
    <p>nc y</p>
    <p>[lo g 1 0 ]</p>
    <p>Max. latency 99%ile latency</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t[</p>
    <p>kr eq</p>
    <p>/s ]</p>
    <p>best tradeoff</p>
    <p>Throughput</p>
    <p>Figure 10: memcached throughput (top) and latency (bottom, log10) as a function of the QJUMP rate limit.</p>
    <p>Figure 11: Latency bound validation: 60 host fan-in of f7 and f0 traffic; 100 million samples per data point.</p>
    <p>are sent to a single destination. Figure 11 shows the latency distribution of coordina</p>
    <p>tion packets as a function of the throughput factor at the highest QJUMP level, f7. If the f7 is set to less than 1.0 (region A), the latency bound is met (as we would expect). In region B, where f7 is between 1.0 and 2.7, transient queueing affects some packetsas evident from the 100th percentile outliersbut all requests make it within the latency bound. Beyond f7 = 2.7 (region C), permanent queueing occurs.</p>
    <p>This experiment offers two further insights about QJUMPs rate-limiting: (i) at throughput factors near 1.0, the latency bound is usually still met, and (ii) via ratelimiting, QJUMP prevents latency-sensitive applications from interfering with their own traffic.</p>
    <p>Figure 12: Latency bound validation topology: 10 hypervisors (HV) and 60 guests (G1..60) and 120 apps.</p>
    <p>in 6.3 and 6.4. We categorize systems as deployable if they function on commodity hardware, unmodified transport protocols and unmodified application source code.</p>
    <p>Fastpass [29] employs a global arbiter that times the admission of packets into the network and routes them. While Fastpass eliminates in-network queueing, requests for allocation must queue at the centralized arbiter.</p>
    <p>EyeQ [22] primarily aims for bandwidth partitioning, although it also reduces latency tails. It, however, requires a full-bisection bandwidth network and a kernel patch in addition to a TC module.</p>
    <p>Deadline Aware TCP (D2TCP) [33] extends DCTCPs window adjustment algorithm with the notion of flow deadlines, scheduling flows with earlier deadlines first. Like DCTCP, D2TCP requires switches supporting ECN;8 it also requires inter-switch coordination, kernel and application modifications.</p>
    <p>HULL combines DCTCPs congestion avoidance applied on network links utilization (rather than queue length) with a special packet-pacing NIC [2]. Its ratelimiting is applied in reaction to ECN-marked packets.</p>
    <p>D3 [35] allocates bandwidth on a first-come-first-serve basis. It requires special switch and NIC hardware and modifies transport protocols.</p>
    <p>PDQ uses Earliest Deadline First (EDF) scheduling to prioritize straggler flows, but requires coordination across switches and application changes.</p>
    <p>DeTail [37] and pFabric [3] pre-emptively schedule flows using packet forwarding priorities in switches. DeTail also addresses load imbalance caused by poor flow hashing. Flow priorities are explicitly specified by modified applications (DeTail) or computed from the remaining flow duration (pFabric). However, both systems require special switch hardware: pFabric uses very short queues and 64-bit priority tags, and DeTail coordinates flows rates via special pause and unpause messages.</p>
    <p>SILO [21] employs a similar reasoning to QJUMP to estimate expected queue lengths. It places VMs according to traffic descriptions to limit queueing and paces hosts using null packets.</p>
    <p>TDMA Ethernet [34] trades bandwidth for reduced queueing by time diving network access, but requires in</p>
  </div>
</Presentation>
