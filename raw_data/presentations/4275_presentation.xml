<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Attention Strategies for Multi-Source Sequence-to-Sequence Learning</p>
    <p>Jindich Libovick, Jindich Helcl</p>
    <p>Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics</p>
    <p>Charles University</p>
    <p>August 2, 2017</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Attention over multiple source sequences relatively unexplored.  This work proposes two techniques:</p>
    <p>Flat attention combination  Hierarchical attention combination</p>
    <p>Applied to tasks of multimodal translation and automatic post-editing.</p>
    <p>Motivation No universal method that models explicitly the importance of each input.</p>
  </div>
  <div class="page">
    <p>Multi-Source Sequence-to-Sequence Learning</p>
    <p>Any number of input sequences with possibly different modalities.</p>
    <p>Figure 1: Multimodal translation example.</p>
    <p>Examples Multimodal translation, automatic post-editing, multi-source machine translation, ...</p>
  </div>
  <div class="page">
    <p>Attentive Sequence Learning</p>
    <p>In each decoder step i  compute distribution over encoder</p>
    <p>states given the decoder state  the decoder gets a context vector to</p>
    <p>decide about its output</p>
    <p>eij = va tanh(Wasi + Uahj) (1)</p>
    <p>ij = exp(eij)Tx</p>
    <p>k=1 exp(eik) (2)</p>
    <p>ci = Tx j=1</p>
    <p>ijhj (3)</p>
    <p>What about multiple inputs?</p>
  </div>
  <div class="page">
    <p>Attentive Sequence Learning</p>
    <p>In each decoder step i  compute distribution over encoder</p>
    <p>states given the decoder state  the decoder gets a context vector to</p>
    <p>decide about its output</p>
    <p>eij = va tanh(Wasi + Uahj) (1)</p>
    <p>ij = exp(eij)Tx</p>
    <p>k=1 exp(eik) (2)</p>
    <p>ci = Tx j=1</p>
    <p>ijhj (3)</p>
    <p>What about multiple inputs?</p>
  </div>
  <div class="page">
    <p>Context Vector Concatenation</p>
    <p>Widely used technique [Firat et al., 2016, Zoph and Knight, 2016].  Attention over input sequences computed independently.  Combination resolved later on in the network</p>
  </div>
  <div class="page">
    <p>Flat Attention Combination</p>
    <p>Attentio n</p>
    <p>Importance of different inputs reflected in the joint attention distribution.</p>
  </div>
  <div class="page">
    <p>Flat Attention Combination</p>
    <p>one source  N sources eij = va tanh(Wasi + Uahj)  e</p>
    <p>(k) ij = v</p>
    <p>a tanh(Wasi + Ua</p>
    <p>(k)hj)</p>
    <p>ij = exp(eij)Tx</p>
    <p>k=1 exp(eik)  (k)ij =</p>
    <p>exp(e(k)ij )N n=1</p>
    <p>T(n)x m=1 exp</p>
    <p>( e(n)im</p>
    <p>) ci =</p>
    <p>Tx j=1</p>
    <p>ijhj  ci = N</p>
    <p>k=1</p>
    <p>T(k)x j=1</p>
    <p>(k) ij Uc</p>
    <p>(k)h(k)j</p>
    <p>U(k)a , U (k) c project states to a common space</p>
    <p>Question: Should U(k)a = U (k) c ? (i.e. should the projection parameters be shared?)</p>
  </div>
  <div class="page">
    <p>Hierarchical Attention Combination</p>
    <p>Attentio n</p>
    <p>Attentio n</p>
    <p>Attention distribution is factored by input.</p>
  </div>
  <div class="page">
    <p>Hierarchical Attention Combination</p>
    <p>inputs</p>
    <p>Compute the context vector:</p>
    <p>c(k)i = T(k)x</p>
    <p>j=1  (k) ij h</p>
    <p>(k) j , where</p>
    <p>(k) ij =</p>
    <p>using the vanilla attention</p>
    <p>e(k)i = v  b tanh(Wbsi + U</p>
    <p>(k) b c</p>
    <p>(k) i )</p>
    <p>(k) i =</p>
    <p>exp(e(k)i )N n=1 exp(e</p>
    <p>(n) i )</p>
    <p>ci = N</p>
    <p>k=1  (k) i U</p>
    <p>(k) c c</p>
    <p>(k) i</p>
    <p>As in the flat scenario, the context vectors have to be projected to a shared space.  Same question arises  should U(k)b = U</p>
    <p>(k) c ?</p>
  </div>
  <div class="page">
    <p>Experiments and Results</p>
    <p>Experiments conducted on multimodal translation (MMT) and automatic post-editing (APE)</p>
    <p>In both flat and hierarchical scenarios, we tried both sharing and not sharing the projection matrices.</p>
    <p>Additionally, we tried using the sentinel gate [Lu et al., 2016], which enables the decoder to decide whether or not to attend to any encoder.</p>
    <p>Experiments conducted using Neural Monkey, code available here: https://github.com/ufal/neuralmonkey.</p>
  </div>
  <div class="page">
    <p>Experiments and Results</p>
    <p>sh ar</p>
    <p>e</p>
    <p>se nt</p>
    <p>. MMT APE BLEU METEOR BLEU HTER</p>
    <p>concat. 31.4  .8 48.0  .7 62.3  .5 24.4  .4</p>
    <p>fla t</p>
    <p>30.2  .8 46.5  .7 62.6  .5 24.2  .4   29.3  .8 45.4  .7 62.3  .5 24.3  .4   30.9  .8 47.1  .7 62.4  .6 24.4  .4   29.4  .8 46.9  .7 62.5  .6 24.2  .4</p>
    <p>hi er</p>
    <p>ar ch</p>
    <p>ic al   32.1  .8 49.1  .7 62.3  .5 24.1  .4   28.1  .8 45.5  .7 62.6  .6 24.1  .4   26.1  .7 42.4  .7 62.4  .5 24.3  .4   22.0  .7 38.5  .6 62.5  .5 24.1  .4</p>
    <p>Results on the Multi30k dataset and the APE dataset. The column share denotes whether the projection matrix is shared for energies and context vector computation,</p>
    <p>sent. indicates whether the sentinel vector has been used or not.</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Source:</p>
    <p>A man sleeping in a green room on a couch.</p>
    <p>Output with attention:</p>
    <p>e i n</p>
    <p>M a n n</p>
    <p>s c h l  f t</p>
    <p>a u f</p>
    <p>e i n e m</p>
    <p>g r  n e n</p>
    <p>S o f a</p>
    <p>i n e i n e m</p>
    <p>g r  n e n</p>
    <p>R a u m</p>
    <p>.</p>
    <p>(1) (2) (3)</p>
    <p>(1) source, (2) image, (3) sentinel</p>
    <p>Reference: ein Mann schlft in einem grnen Raum auf einem Sofa .</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The results show both methods achieve comparable results to the existing approach (concatenation of the context vectors).</p>
    <p>Hierarchical attention combination achieved best results on MMT, and is faster to train.</p>
    <p>Both methods provide a trivial way to inspect the attention distribution w.r.t. the individual inputs.</p>
    <p>Thank you for your attention!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>The results show both methods achieve comparable results to the existing approach (concatenation of the context vectors).</p>
    <p>Hierarchical attention combination achieved best results on MMT, and is faster to train.</p>
    <p>Both methods provide a trivial way to inspect the attention distribution w.r.t. the individual inputs.</p>
    <p>Thank you for your attention!</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, CA, USA, pages 866875. http://www.aclweb.org/anthology/N16-1101.</p>
    <p>Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2016. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. CoRR abs/1612.01887. http://arxiv.org/abs/1612.01887.</p>
    <p>Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, CA, USA, pages 3034. http://www.aclweb.org/anthology/N16-1004.</p>
  </div>
</Presentation>
