<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Large Pages May Be Harmful on NUMA Systems</p>
    <p>Fabien Gaud Simon Fraser University</p>
    <p>Bap?ste Lepers CNRS</p>
    <p>Jeremie Decouchant Grenoble University</p>
    <p>Jus?n Funston Simon Fraser University</p>
    <p>Alexandra Fedorova Simon Fraser University</p>
    <p>Vivien Quma Grenoble INP</p>
  </div>
  <div class="page">
    <p>Virtual-to-physical transla?on is done by the TLB and page table</p>
    <p>Virtual address TLB Physical address</p>
    <p>Page table</p>
    <p>TLB hit</p>
    <p>TLB miss</p>
    <p>Typical TLB size: 1024 entries (AMD Bulldozer), 512 entries (Intel i7).</p>
  </div>
  <div class="page">
    <p>Virtual-to-physical transla?on is done by the TLB and page table</p>
    <p>Virtual address TLB Physical address</p>
    <p>Page table</p>
    <p>TLB hit</p>
    <p>TLB miss</p>
    <p>Typical TLB size: 1024 entries (AMD Bulldozer), 512 entries (Intel i7).</p>
  </div>
  <div class="page">
    <p>Large pages known advantages &amp; downsides</p>
    <p>Known advantages:  Fewer TLB misses</p>
    <p>Fewer page allocations (reduces contention in the kernel memory manager)</p>
    <p>Known downsides:  Increased memory footprint  Memory fragmentation</p>
    <p>Page size 512 entries coverage 1024 entries coverage</p>
  </div>
  <div class="page">
    <p>New observa?on: large pages may hurt performance on NUMA machines</p>
    <p>-30 -20 -10</p>
    <p>B T.</p>
    <p>B</p>
    <p>C G</p>
    <p>.D</p>
    <p>D C</p>
    <p>.A</p>
    <p>E P.</p>
    <p>C</p>
    <p>FT .C</p>
    <p>IS</p>
    <p>.D</p>
    <p>LU .B</p>
    <p>M</p>
    <p>G .D</p>
    <p>S</p>
    <p>P. B</p>
    <p>U</p>
    <p>A .B</p>
    <p>U</p>
    <p>A .C</p>
    <p>W C</p>
    <p>W R</p>
    <p>K</p>
    <p>m ea</p>
    <p>ns</p>
    <p>M at</p>
    <p>rix M</p>
    <p>.</p>
    <p>pc a</p>
    <p>w rm</p>
    <p>em</p>
    <p>S S</p>
    <p>C A</p>
    <p>S P</p>
    <p>E C</p>
    <p>jb b</p>
    <p>P er</p>
    <p>f. Im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t re</p>
    <p>la tiv</p>
    <p>e to</p>
    <p>(% ) 24-core machine</p>
    <p>-30 -20 -10</p>
    <p>B T.</p>
    <p>B</p>
    <p>C G</p>
    <p>.D</p>
    <p>D C</p>
    <p>.A</p>
    <p>E P.</p>
    <p>C</p>
    <p>FT .C</p>
    <p>IS</p>
    <p>.D</p>
    <p>LU .B</p>
    <p>M</p>
    <p>G .D</p>
    <p>S</p>
    <p>P. B</p>
    <p>U</p>
    <p>A .B</p>
    <p>U</p>
    <p>A .C</p>
    <p>W C</p>
    <p>W R</p>
    <p>K</p>
    <p>m ea</p>
    <p>ns</p>
    <p>M at</p>
    <p>rix M</p>
    <p>.</p>
    <p>pc a</p>
    <p>w rm</p>
    <p>em</p>
    <p>S S</p>
    <p>C A</p>
    <p>S P</p>
    <p>E C</p>
    <p>jb b</p>
    <p>P er</p>
    <p>f. Im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t re</p>
    <p>la tiv</p>
    <p>e to</p>
    <p>(% ) 64-core machine</p>
    <p>-43</p>
    <p>Performance improvement of THP (2M pages) over 4K pages</p>
  </div>
  <div class="page">
    <p>Machines are NUMA</p>
    <p>Node 2 Node 3</p>
    <p>Memory Memory</p>
    <p>Memory Memory</p>
    <p>CPU0 CPU1</p>
    <p>CPU2 CPU3</p>
    <p>Remote memory accesses hurt performance</p>
  </div>
  <div class="page">
    <p>Machines are NUMA</p>
    <p>Node 2 Node 3</p>
    <p>Memory Memory</p>
    <p>Memory Memory</p>
    <p>CPU0 CPU1</p>
    <p>CPU2 CPU3</p>
    <p>Contention hurts performance even more.</p>
  </div>
  <div class="page">
    <p>Large pages on NUMA machines (1/2)</p>
    <p>Node 1</p>
    <p>Node 2 Node 3</p>
    <p>Node 0</p>
    <p>void *a = malloc(2MB);</p>
    <p>With 4K pages, load is balanced.</p>
  </div>
  <div class="page">
    <p>Large pages on NUMA machines (1/2)</p>
    <p>Node 1</p>
    <p>Node 2 Node 3</p>
    <p>Node 0</p>
    <p>void *a = malloc(2MB);</p>
    <p>With 2M pages, data are allocated on 1 node =&gt; contention.</p>
  </div>
  <div class="page">
    <p>Large pages on NUMA machines (1/2)</p>
    <p>Node 1</p>
    <p>Node 2 Node 3</p>
    <p>Node 0</p>
    <p>void *a = malloc(2MB);</p>
    <p>With 2M pages, data are allocated on 1 node =&gt; contention.</p>
    <p>HOT P AGE</p>
  </div>
  <div class="page">
    <p>Performance example (1/2)</p>
    <p>App. Perf. increase THP/4K</p>
    <p>(%)</p>
    <p>% of time spent in</p>
    <p>TLB miss 4K</p>
    <p>% of time spent in</p>
    <p>TLB miss 2M</p>
    <p>Imbalance 4K (%)</p>
    <p>Imbalance 2M (%)</p>
    <p>CG.D -43 0 0 1 59 SSCA.20 17 15 2 8 52 SpecJBB -6 7 0 16 39</p>
    <p>Using large pages, 1 node is overloaded in CG, SSCA and SpecJBB. Only SSCA benefits from the reduction of TLB misses.</p>
  </div>
  <div class="page">
    <p>Large pages on NUMA machines (2/2)</p>
    <p>Node 1</p>
    <p>Node 2 Node 3</p>
    <p>Node 0</p>
    <p>void *a = malloc(1.5MB); // node 0 void *b = malloc(1.5MB); // node 1 PAG</p>
    <p>E-LEV EL</p>
    <p>FALSE SHAR</p>
    <p>ING</p>
    <p>Page-level false sharing reduces the maximum achievable locality.</p>
  </div>
  <div class="page">
    <p>Performance example (2/2)</p>
    <p>App. Perf. increase THP/4K</p>
    <p>(%)</p>
    <p>Local Access Ratio 4K</p>
    <p>(%)</p>
    <p>Local Access</p>
    <p>Ratio 2M (%)</p>
    <p>UA.C -15 88 66</p>
    <p>The locality decreases when using large pages.</p>
  </div>
  <div class="page">
    <p>Can exis?ng memory management algorithms solve the problem?</p>
  </div>
  <div class="page">
    <p>-30 -20 -10</p>
    <p>C G</p>
    <p>.D</p>
    <p>LU .B</p>
    <p>U A</p>
    <p>.B</p>
    <p>U A</p>
    <p>.C</p>
    <p>M at</p>
    <p>rix M</p>
    <p>.</p>
    <p>w rm</p>
    <p>em</p>
    <p>S S</p>
    <p>C A</p>
    <p>S P</p>
    <p>E C</p>
    <p>jb b</p>
    <p>P er</p>
    <p>f. Im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t re</p>
    <p>la tiv</p>
    <p>e to</p>
    <p>(% )</p>
    <p>Exis?ng memory management algorithms do not solve the problem</p>
    <p>We run the application with Carrefour[1], the state-of-the-art memory management algorithm. Carrefour monitors memory accesses and places</p>
    <p>pages to minimize imbalance and maximize locality.</p>
    <p>[1] DASHTI M., FEDOROVA A., FUNSTON J., GAUD F.,LACHAIZE R., LEPERS B., QUEMA V., AND ROTH M. Traffic management: A holistic approach to memory placement on NUMA systems. ASPLOS 2013.</p>
    <p>Carrefour solves imbalance / locality issues on some applications</p>
    <p>But does not improve performance on some other applications (hot pages or page-level false sharing)</p>
  </div>
  <div class="page">
    <p>We need a beYer memory management algorithm</p>
  </div>
  <div class="page">
    <p>Our solu?on  Carrefour-LP  Built on top of Carrefour.  By default, 2M pages are activated.  Two components that run every second:</p>
    <p>Reactive component Conservative component Splits 2M pages</p>
    <p>Detects and removes hot p a g e s  a n d p a g e - l e v e l false sharing.</p>
    <p>Deactivate 2M page allocation</p>
    <p>Promotes 4K pages W h e n t h e t i m e s p e n t handling TLB misses is high. Forces 2M page allocation In case of contention in the page fault handler.</p>
    <p>We show in the paper that the two components are required.</p>
  </div>
  <div class="page">
    <p>Implementa?on Reactive component (splits 2M pages)</p>
    <p>Sample memory accesses using IBS</p>
    <p>A page represents more</p>
    <p>than 5% of all accesses and is accessed from multiple nodes?</p>
    <p>Split and interleave the hot page YES</p>
  </div>
  <div class="page">
    <p>Implementa?on Reactive component (splits 2M pages)</p>
    <p>Sample memory accesses using IBS</p>
    <p>Compute observed local access ratio (LAR1)  Compute the LAR that would have been obtained if each page was</p>
    <p>placed on the node that accessed it the most.</p>
    <p>LAR1 can be significantly improved?</p>
    <p>Run carrefour YES</p>
    <p>Compute the LAR that would have been obtained if each page was split and then placed on the node that accessed it the most.</p>
    <p>LAR1 can be significantly improved?</p>
    <p>Split all 2M pages and run carrefour YES</p>
    <p>NO</p>
  </div>
  <div class="page">
    <p>Implementa?on challenges Reactive component (splits 2M pages)</p>
    <p>Sample memory accesses using IBS</p>
    <p>Compute observed local access ratio (LAR1)  Compute the LAR that would have been obtained if each page was</p>
    <p>placed on the node that accessed it the most (without splitting).</p>
    <p>LAR1 can be significantly improved?</p>
    <p>Run carrefour YES</p>
    <p>Compute the LAR that would have been obtained if each page was split and then placed on the node that accessed it the most.</p>
    <p>LAR1 can be significantly improved?</p>
    <p>Split all 2M pages and run carrefour YES</p>
    <p>NO</p>
    <p>COSTLY</p>
    <p>COSTLY</p>
    <p>IMPRECISE</p>
  </div>
  <div class="page">
    <p>Implementa?on challenges Reactive component (splits 2M pages)</p>
    <p>We only have few IBS samples.</p>
    <p>The LAR with 2M pages split into 4K pages can be wrong.</p>
    <p>We try to be conservative by running Carrefour first and only splitting pages when necessary (splitting pages is expensive).</p>
    <p>Predicting that splitting a 2M page will increase TLB miss rate is too costly. This is why the conservative component is required.</p>
  </div>
  <div class="page">
    <p>Implementa?on Conservative component</p>
    <p>Monitor time spent in TLB misses (hardware counters)</p>
    <p>&gt; 5% Cluster 4K pages and force 2M pages allocation YES</p>
    <p>Monitor time spent in page fault handler (kernel statistics)</p>
    <p>&gt; 5% Force 2M page allocation YES</p>
  </div>
  <div class="page">
    <p>Evalua?on</p>
    <p>-30 -20 -10</p>
    <p>C G</p>
    <p>.D</p>
    <p>LU .B</p>
    <p>U A</p>
    <p>.B</p>
    <p>U A</p>
    <p>.C</p>
    <p>M at</p>
    <p>rix M</p>
    <p>.</p>
    <p>w rm</p>
    <p>em</p>
    <p>S S</p>
    <p>C A</p>
    <p>S P</p>
    <p>E C</p>
    <p>jb b</p>
    <p>P er</p>
    <p>f. Im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t re</p>
    <p>la tiv</p>
    <p>e to</p>
    <p>(% ) 24-core machine</p>
    <p>-30 -20 -10</p>
    <p>C G</p>
    <p>.D</p>
    <p>LU .B</p>
    <p>U A</p>
    <p>.B</p>
    <p>U A</p>
    <p>.C</p>
    <p>M at</p>
    <p>rix M</p>
    <p>.</p>
    <p>w rm</p>
    <p>em</p>
    <p>S S</p>
    <p>C A</p>
    <p>S P</p>
    <p>E C</p>
    <p>jb b</p>
    <p>P er</p>
    <p>f. Im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t re</p>
    <p>la tiv</p>
    <p>e to</p>
    <p>(% ) 64-core machine</p>
    <p>-43</p>
    <p>Carrefour-2M over Linux 4K</p>
    <p>Conservative over Linux 4K</p>
    <p>Reactive over Linux 4K Carrefour-LP over Linux 4K</p>
  </div>
  <div class="page">
    <p>Conclusion  Large pages can hurt performance on NUMA systems.</p>
    <p>We identified two new issues when using large pages on NUMA systems: hot pages and page-level false sharing.</p>
    <p>We designed a new algorithm, Carrefour-LP. On the set of applications:  46% better than Linux  50% better than THP.</p>
    <p>(The full set of applications is available in the paper.)</p>
    <p>Overhead:  Less than 3% CPU overhead.</p>
    <p>Carrefour-LP restores the performance when it was lost due to large pages and makes their benefits accessible to applications.</p>
  </div>
  <div class="page">
    <p>Ques?ons?</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Performance example App. Perf.</p>
    <p>increas e THP/</p>
    <p>Time spent</p>
    <p>in page fault</p>
    <p>handler 4K</p>
    <p>Time spent</p>
    <p>in page fault</p>
    <p>handler 2M</p>
    <p>Local acces</p>
    <p>s ratio 4K (%)</p>
    <p>Local Access ratio 2M</p>
    <p>(%)</p>
    <p>Imbalan ce 4K</p>
    <p>(%)</p>
    <p>Imbalan ce 2M</p>
    <p>(%)</p>
    <p>CG.D -43 2200ms (0.1%)</p>
    <p>UA.C -15 100ms (0.2%)</p>
    <p>WR 109 8700ms (38%)</p>
    <p>SSCA. 20</p>
    <p>SpecJB B</p>
    <p>-6 8400ms (2%)</p>
  </div>
</Presentation>
