<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>HetPipe: Enabling Large DNN Training on (Whimpy)</p>
    <p>Heterogeneous GPU Clusters through Integration of</p>
    <p>Pipelined Model Parallelism and Data Parallelism</p>
    <p>Jay H. Park,</p>
    <p>Gyeongchan Yun, Chang M. Yi, Nguyen T. Nguyen, Seungmin Lee,</p>
    <p>Jaesik Choi , Sam H. Noh, and Young-ri Choi</p>
  </div>
  <div class="page">
    <p>Motivation &amp; Background</p>
    <p>HetPipe in a Nutshell</p>
    <p>Our System: HetPipe</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
    <p>Contents</p>
  </div>
  <div class="page">
    <p>DNN (Deep Neural Network) models continue to grow</p>
    <p>Motivation</p>
    <p>Need more powerful GPUs for training!</p>
  </div>
  <div class="page">
    <p>Short release cycle of new GPU architectures</p>
    <p>Motivation</p>
    <p>Use of heterogeneous GPUs is inevitable!  What to do with whimpy GPUs?</p>
    <p>Whimpy GPUs</p>
  </div>
  <div class="page">
    <p>DNN Training</p>
    <p>Forward Pass</p>
    <p>Backward Pass</p>
    <p>+ =</p>
    <p>Minibatch  (Training Data)</p>
    <p>Loss</p>
    <p>Cat?</p>
    <p>Weight Parameter</p>
  </div>
  <div class="page">
    <p>Model parallelism (MP) Data parallelism (DP)</p>
    <p>Parallelizing DNN Training</p>
    <p>Low GPU utilization</p>
    <p>Weights synchronized through PS or AllReduce</p>
    <p>GPU memory limitation</p>
    <p>Worker 1</p>
    <p>Parameter Server (PS)</p>
    <p>Worker</p>
    <p>Forward pass Backward pass</p>
  </div>
  <div class="page">
    <p>Attempts to improve MP utilization</p>
    <p>Pipelined model parallelism (PMP)</p>
    <p>Parallelizing DNN Training</p>
    <p>Designed for homogeneous GPUs  Designed for a single PMP worker</p>
    <p>Forward pass Backward pass</p>
    <p>PMP Worker</p>
    <p>PipeDream [SOSP19]  GPipe [NIPS19]</p>
  </div>
  <div class="page">
    <p>HetPipe in a Nutshell</p>
    <p>Virtual Worker (VW) 1</p>
    <p>Parameter Server</p>
    <p>VW</p>
    <p>Integrates PMP + DP</p>
    <p>WSP (Wave Synchronous Parallel)</p>
    <p>Support Heterogeneous GPUs VW: A group of multiple GPUs</p>
    <p>GPU GPU GPU GPU</p>
    <p>GPU GPU GPU GPU R</p>
    <p>R G G</p>
    <p>R</p>
    <p>G</p>
    <p>GPU GPU GPU GPU</p>
    <p>GPU GPU GPU GPU V</p>
    <p>V Q Q</p>
    <p>V</p>
    <p>Q</p>
    <p>PMP DP</p>
    <p>PMP</p>
  </div>
  <div class="page">
    <p>Challenges in integration PMP+DP in Heterogeneous GPUs</p>
    <p>What weight version should be used by each VW to synchronize with other VWs?</p>
    <p>Parameter Server</p>
    <p>How do we reduce virtual worker stragglers when we consider DP?</p>
    <p>Many more in the paper</p>
  </div>
  <div class="page">
    <p>HetPipe Contributions</p>
    <p>Integrates PMP + DP Novel parameter synchronization model</p>
    <p>WSP (Wave Synchronous Parallel)</p>
    <p>Enable Large DNN Training on Heterogeneous GPUs Aggregate heterogeneous resources</p>
    <p>Reduce the straggler problem</p>
    <p>Proof of WSP Convergence</p>
  </div>
  <div class="page">
    <p>HetPipe Workflow</p>
    <p>Model Partitioner</p>
    <p>DNN Model</p>
    <p>Resource Allocator</p>
    <p>Cluster Configuration</p>
    <p>P2 P3 P4P1</p>
    <p>Assign k GPUs to each virtual worker</p>
    <p>VW 1</p>
    <p>Divide model into k partitions</p>
    <p>P1</p>
    <p>P2</p>
    <p>P3</p>
    <p>P4</p>
    <p>VW</p>
    <p>P1</p>
    <p>P2</p>
    <p>P3</p>
    <p>P4</p>
    <p>PS</p>
    <p>Time</p>
    <p>V</p>
    <p>V</p>
    <p>Q</p>
    <p>Q</p>
    <p>R</p>
    <p>R</p>
    <p>G</p>
    <p>G</p>
    <p>P2 P3 P4P1</p>
    <p>VW 1 VW</p>
  </div>
  <div class="page">
    <p>HetPipe Workflow</p>
    <p>Model Partitioner</p>
    <p>DNN Model</p>
    <p>Resource Allocator</p>
    <p>Cluster Configuration</p>
    <p>Assign k GPUs to each virtual worker</p>
    <p>VW 1</p>
    <p>Divide model into k partitions VW</p>
    <p>PS</p>
    <p>P2 P3 P4P1</p>
    <p>P1</p>
    <p>P2</p>
    <p>P3</p>
    <p>P4</p>
    <p>P1</p>
    <p>P2</p>
    <p>P3</p>
    <p>P4</p>
    <p>P2 P3 P4P1</p>
    <p>VW 1 VW</p>
    <p>Global</p>
    <p>Local</p>
    <p>Local</p>
    <p>Staleness</p>
    <p>V</p>
    <p>V</p>
    <p>Q</p>
    <p>Q</p>
    <p>R</p>
    <p>R</p>
    <p>G</p>
    <p>G</p>
  </div>
  <div class="page">
    <p>Motivation &amp; Background</p>
    <p>HetPipe in a Nutshell</p>
    <p>Our System: HetPipe</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Execution of a virtual worker</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>GPU3</p>
    <p>GPU4</p>
    <p>Time</p>
    <p>Forward pass Backward pass</p>
    <p>is a consistent version of weights within a VW</p>
    <p>minibatches processed concurrently in pipeline manner</p>
  </div>
  <div class="page">
    <p>Weight management procedure</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>GPU3</p>
    <p>GPU4</p>
    <p>Time</p>
    <p>Forward pass Backward pass</p>
    <p>Update     Initial weight version () =0=1=2=3=4</p>
    <p>1 2 3 4</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Local staleness (): maximum missing updates</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>GPU3</p>
    <p>GPU4</p>
    <p>Time</p>
    <p>Forward pass Backward pass</p>
    <p>missing updates of minibatches 2 to 4</p>
    <p>+</p>
    <p>= 3</p>
  </div>
  <div class="page">
    <p>Local staleness (): maximum missing updates</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>GPU1</p>
    <p>GPU2</p>
    <p>GPU3</p>
    <p>GPU4</p>
    <p>Time</p>
    <p>Forward pass Backward pass</p>
    <p>Update</p>
    <p>+</p>
    <p>missing updates of minibatches 3 to 5</p>
    <p>+</p>
    <p>= 3</p>
  </div>
  <div class="page">
    <p>Motivation &amp; Background</p>
    <p>HetPipe in a Nutshell</p>
    <p>Our System: HetPipe</p>
    <p>Pipelined Model Parallelism Within a VW</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Evaluation</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Data Parallelism with Multiple VWs</p>
    <p>Minibatch 1</p>
    <p>Minibatch 2</p>
    <p>Minibatch 3</p>
    <p>Minibatch 4VW 1</p>
    <p>Clock</p>
    <p>Wave 0 Wave 1</p>
    <p>Parameter Server:</p>
    <p>Progress of minibatch execution</p>
    <p>VW</p>
    <p>Push &amp; Pull</p>
    <p>Wave: Sequence of concurrently executing  minibatches</p>
  </div>
  <div class="page">
    <p>Push occurs every clock</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>VW</p>
    <p>+</p>
    <p>Push aggregated updates of wave0 ( )</p>
    <p>= 1+2+3+4</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
    <p>Wave 0</p>
  </div>
  <div class="page">
    <p>Pull occurs intermittently - Depending on user defined clock distance D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>- If D = 0 pull occurs every clock</p>
    <p>VW1 waits before pull until VW2 pushes</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
  </div>
  <div class="page">
    <p>Pull occurs intermittently - Depending on user defined clock distance D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>If D = 0</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
    <p>VW2 Push aggregated updates ()</p>
    <p>VW1 waits before pull until VW2 pushes</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Pull occurs intermittently - Depending on user defined clock distance D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>Pull occurs after all VWs have been pushed</p>
    <p>If D = 0</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
  </div>
  <div class="page">
    <p>Pull occurs intermittently - Depending on user defined clock distance D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>Minibatch 8 starts with 8</p>
    <p>If D = 0</p>
    <p>8 = 0+(1+ 2+ 3+ 4)vw1,vw2</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
  </div>
  <div class="page">
    <p>Local staleness () and global staleness () with WSP</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>11</p>
    <p>(8+ 9+ 10)vw1</p>
    <p>=0+ (1+ 2+ 3+ 4)vw1,vw2</p>
    <p>+ (1+ 2+ 3+ 4)vw1,vw2=0</p>
    <p>+ (5+ 6+ 7)vw1</p>
    <p>(5+ 6+ 7)vw2</p>
  </div>
  <div class="page">
    <p>Local staleness () and global staleness () with WSP</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Clock0 1</p>
    <p>Parameter Server:</p>
    <p>Minibatch 12 has to wait</p>
  </div>
  <div class="page">
    <p>Example of clock distance threshold D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>If D = 1</p>
    <p>Can start minibatch 8 without pull 1</p>
    <p>Clock0 1</p>
    <p>Parameter Server:</p>
    <p>Push &amp; Pull</p>
  </div>
  <div class="page">
    <p>Example of clock distance threshold D</p>
    <p>Data Parallelism with Multiple VWs</p>
    <p>Minibatch 12 has to wait</p>
    <p>Clock0 1</p>
    <p>11 =0+(1+ 2+ 3+ 4+ 5+ 6+ 7)vw1</p>
    <p>(8+ 9+10)vw1</p>
    <p>=0</p>
    <p>If D = 1</p>
    <p>VW 2</p>
    <p>(1+ 2+3+4+5+6+7)vw2 5</p>
  </div>
  <div class="page">
    <p>Motivation &amp; Background</p>
    <p>HetPipe in a Nutshell</p>
    <p>Our System: HetPipe</p>
    <p>Evaluation</p>
    <p>Setup</p>
    <p>Resource Allocation for Virtual Workers</p>
    <p>Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>InfiniBand (56 Gbps)</p>
    <p>Cluster setup - 4 heterogeneous GPU nodes</p>
    <p>Two DNN models</p>
    <p>Evaluation Setup</p>
    <p>ResNet-152 VGG-19</p>
    <p>Dataset, minibatch size ImageNet, 32</p>
    <p>Model parameter size 230 MB 548 MB</p>
    <p>Characteristic Large activation output Large parameter size</p>
    <p>Node  Node  Node  Node</p>
    <p>V0</p>
    <p>V1</p>
    <p>V2</p>
    <p>V3</p>
    <p>R0</p>
    <p>R1</p>
    <p>R2</p>
    <p>R3</p>
    <p>G0</p>
    <p>G1</p>
    <p>G2</p>
    <p>G3</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>Q2</p>
    <p>Q3</p>
    <p>TITAN V TITAN RTX GeForce RTX 2060Quadro P4000</p>
    <p>V R G Q</p>
    <p>Computation power</p>
    <p>&gt; &gt; &gt;</p>
    <p>VR GQ</p>
    <p>Memory size</p>
    <p>&gt; &gt; &gt;</p>
  </div>
  <div class="page">
    <p>NP (Node Partition)</p>
    <p>Resource Allocation for Virtual Workers: NP, ED, HD</p>
    <p>Node</p>
    <p>V0 V1 V2 V3</p>
    <p>VW</p>
    <p>Node</p>
    <p>Q0 Q1 Q2 Q3</p>
    <p>VW</p>
    <p>Node</p>
    <p>R0 R1 R2 R3</p>
    <p>VW</p>
    <p>Node</p>
    <p>G0 G1 G2 G3</p>
    <p>VW 4</p>
    <p>Minimum communication overhead within VW</p>
    <p>Performance of each virtual worker varies  Straggler may degrade performance with DP</p>
  </div>
  <div class="page">
    <p>ED (Equal Distribution)</p>
    <p>Resource Allocation for Virtual Workers: NP, ED, HD</p>
    <p>Node</p>
    <p>V0</p>
    <p>V1</p>
    <p>V2</p>
    <p>V3</p>
    <p>Node</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>Q2</p>
    <p>Q3</p>
    <p>Node</p>
    <p>R0</p>
    <p>R1</p>
    <p>R2</p>
    <p>R3</p>
    <p>Node</p>
    <p>G0</p>
    <p>G1</p>
    <p>G2</p>
    <p>G3</p>
    <p>VW</p>
    <p>VW</p>
    <p>VW</p>
    <p>VW</p>
    <p>Performance will be the same across the VWs  Mitigates the straggler problem</p>
    <p>High communication overhead within each VW</p>
  </div>
  <div class="page">
    <p>HD (Hybrid Distribution)</p>
    <p>Resource Allocation for Virtual Workers: NP, ED, HD</p>
    <p>Node</p>
    <p>V0</p>
    <p>V1</p>
    <p>V2</p>
    <p>V3</p>
    <p>Node</p>
    <p>Q0</p>
    <p>Q1</p>
    <p>Q2</p>
    <p>Q3</p>
    <p>Node</p>
    <p>R0</p>
    <p>R1</p>
    <p>R2</p>
    <p>R3</p>
    <p>Node</p>
    <p>G0</p>
    <p>G1</p>
    <p>G2</p>
    <p>G3</p>
    <p>VW</p>
    <p>VW</p>
    <p>VW</p>
    <p>VW 4</p>
    <p>Mitigates the straggler problem  Reduces communication overhead within each VW</p>
    <p>V R G Q</p>
    <p>Computation power</p>
    <p>&gt; &gt; &gt;</p>
    <p>VR GQ</p>
    <p>Memory size</p>
    <p>&gt; &gt; &gt;</p>
  </div>
  <div class="page">
    <p>Round-robin policy (default)</p>
    <p>Can be used in all three policies: NP, ED, and HD</p>
    <p>Parameter Placement</p>
    <p>Node</p>
    <p>V3</p>
    <p>Node</p>
    <p>Q3</p>
    <p>Node</p>
    <p>R3</p>
    <p>Node</p>
    <p>G3</p>
    <p>VW</p>
    <p>VW</p>
    <p>Example: ED</p>
    <p>Parameters of each layer:</p>
  </div>
  <div class="page">
    <p>Local placement policy</p>
    <p>ED-local</p>
    <p>Parameter Placement</p>
    <p>Node</p>
    <p>V3</p>
    <p>Node</p>
    <p>Q3</p>
    <p>Node</p>
    <p>R3</p>
    <p>Node</p>
    <p>G3</p>
    <p>VW</p>
    <p>VW</p>
    <p>Parameters of each layer:</p>
    <p>Significantly reduces communication overhead</p>
    <p>ED</p>
    <p>Parameter communication occurs</p>
  </div>
  <div class="page">
    <p>Baseline Horovod</p>
    <p>State-of-the-art DP using AllReduce</p>
    <p>Compare Throughput with Horovod</p>
    <p>ResNet-152 VGG-19</p>
    <p>problem  ED-local: significantly</p>
    <p>reduces communication overhead</p>
    <p>For ResNet-152, the whole model is too large to be loaded into a single G type GPU (batch size = 32)</p>
  </div>
  <div class="page">
    <p>Performance Improvement of Adding Whimpy GPUs</p>
    <p>Adding whimpy GPUs</p>
    <p>V RRRR QQQQ</p>
    <p>GGGG+ + +</p>
    <p>With additional GPUs, HetPipe achieves up to 2.3X speed up</p>
    <p>Additional whimpy systems allow for faster training</p>
    <p>VVVV</p>
  </div>
  <div class="page">
    <p>ResNet-152</p>
    <p>Convergence Results</p>
    <p>HetPipe reduces straggler problem in heterogeneous environment</p>
    <p>Target accuracy: 74%</p>
    <p>Up to 39% faster</p>
    <p>Adding four more whimpy G GPUs, performance improves even more</p>
  </div>
  <div class="page">
    <p>VGG-19</p>
    <p>Convergence Results</p>
    <p>Target accuracy: 67%</p>
    <p>HetPipe (D=0) is 29% faster than Horovod Up to 49% faster</p>
    <p>D=4 D=32 D=0</p>
    <p>Higher global staleness (i.e., 32) can degrade convergence performance</p>
  </div>
  <div class="page">
    <p>Provide convergence proof of WSP</p>
    <p>Partitioning algorithm</p>
    <p>Performance of a single virtual worker</p>
    <p>Comparison to PipeDream</p>
    <p>Not Presented But Discussed in Paper</p>
  </div>
  <div class="page">
    <p>HetPipe makes it possible to efficiently train large DNN models with heterogeneous GPUs</p>
    <p>Integrate pipelined model parallelism with data parallelism</p>
    <p>Propose a novel parameter synchronization model: WSP</p>
    <p>DNN models converge up to 49% faster with HetPipe</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
