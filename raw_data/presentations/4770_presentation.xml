<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Data Association for Topic Intensity Tracking</p>
    <p>Andreas Krause Jure Leskovec</p>
    <p>Carlos Guestrin</p>
    <p>School of Computer Science, Carnegie Mellon University</p>
  </div>
  <div class="page">
    <p>Document classification  Two topics: Conference and Hiking</p>
    <p>Will you go to ICML too?</p>
    <p>Lets go hiking on Friday!</p>
    <p>P(C | words) = .9 P(C | words) = .1</p>
    <p>Conference  Hiking</p>
  </div>
  <div class="page">
    <p>A more difficult example  Two topics: Conference and Hiking</p>
    <p>What if we had temporal information?  How about modeling emails as HMM?</p>
    <p>Lets have dinner after the talk.</p>
    <p>Should we go on Friday?</p>
    <p>P(C | words) = .7 P(C | words) = .5</p>
    <p>Could refer to both topics!</p>
    <p>Conference</p>
    <p>C1</p>
    <p>D1</p>
    <p>C2</p>
    <p>D2</p>
    <p>Ct</p>
    <p>Dt</p>
    <p>Ct+1</p>
    <p>Dt+1</p>
    <p>Assumes equal time steps, smooth topic changes. Valid assumptions?</p>
  </div>
  <div class="page">
    <p>Typical email traffic</p>
    <p>Email traffic very bursty  Cannot model with uniform time steps!   Bursts tell us, how intensely a topic is pursued</p>
    <p>Bursts are potentially very interesting!</p>
    <p>Time (days)</p>
    <p>N um</p>
    <p>be r</p>
    <p>of e</p>
    <p>m ai</p>
    <p>ls</p>
  </div>
  <div class="page">
    <p>Identifying both topics and bursts  Given:</p>
    <p>A stream of documents (emails):</p>
    <p>d1, d2, d3,</p>
    <p>and corresponding document inter-arrival times (time between consecutive documents):</p>
    <p>1, 2, 3, ...</p>
    <p>Simultaneously:  Classify (or cluster) documents into K topics  Predict the topic intensities  predict time between</p>
    <p>consecutive documents from the same topic</p>
  </div>
  <div class="page">
    <p>Data association problem  If we know the email topics, we can identify bursts</p>
    <p>If we dont know the topics, we cant identify bursts!  Nave solution: First classify documents, then identify bursts</p>
    <p>Can fail badly!   This paper:</p>
    <p>Simultaneously identify topics and bursts!</p>
    <p>High intensity for Conference, Low intensity for Hiking</p>
    <p>Low intensity for Conference, High intensity for Hiking</p>
    <p>time</p>
    <p>Intensity for Conference ??? Intensity for Hiking ???</p>
    <p>Conference</p>
    <p>Hiking</p>
  </div>
  <div class="page">
    <p>The Task  Have to solve a data association problem:  We observe:</p>
    <p>Message Deltas  time between the arrivals of consecutive documents</p>
    <p>We want to estimate:  Topic Deltas  time between messages of the same topic  We can then compute the topic intensity L = E[ 1/  Therefore, need to associate each document with a topic</p>
    <p>Need topics to identify intensity</p>
    <p>Need intensity to classify (better)</p>
    <p>Chicken and Egg problem:</p>
  </div>
  <div class="page">
    <p>How to reason about topic deltas?  Associate with each email timestamps vectors</p>
    <p>for topic arrivals</p>
    <p>C: 2:00 pm H: 2:30 pm</p>
    <p>Email 1, Conference At 2:00 pm</p>
    <p>C: 4:15pm H: 2:30 pm</p>
    <p>Email 2, Hiking At 2:30 pm</p>
    <p>C: 4:15 pm H: 7:30 pm</p>
    <p>Email 3, Conference At 4:15 pm</p>
    <p>Next arrival of email from Conference, Hiking</p>
    <p>Timestamp vector [t(C),t(H)]</p>
    <p>Message = 30min (betw. consecutive messages)</p>
    <p>Topic = 2h 15min (consecutive msg. of same topic)</p>
    <p>1 2 3</p>
  </div>
  <div class="page">
    <p>L(H)t-1 L (H)</p>
    <p>t L (H)</p>
    <p>t+1</p>
    <p>t-1 t t+1</p>
    <p>Ct Dtt</p>
    <p>Generative Model (conceptual) L(C)t-1 L</p>
    <p>(C) t L</p>
    <p>(C) t+1</p>
    <p>t = [t (C),t</p>
    <p>(H)] Time for next email</p>
    <p>from topic (exponential dist.)</p>
    <p>Time between subsequent</p>
    <p>emails</p>
    <p>Topic indicator</p>
    <p>(i.e., = Conference)</p>
    <p>Document (e.g., bag of</p>
    <p>words)</p>
    <p>Intensity for Conference (parameter for exponential d.)</p>
    <p>Intensity for Hiking</p>
    <p>(parameter for exponential d.)</p>
    <p>Problem:</p>
    <p>Need to reason about entire history of timesteps t! Makes inference intractable, even for few topics!</p>
  </div>
  <div class="page">
    <p>Key observation:  If topic  follow exponential distribution:</p>
    <p>P(t+1(C) &gt; 4pm | t (C) = 2pm, its now 3pm) = P(t+1(C) &gt; 4pm | t (C) = 3pm, its now 3pm)</p>
    <p>Exploit memorylessness to discard timestamps t</p>
    <p>Exponential distribution appropriate:  Previous work on document streams (E.g., Kleinberg 03)  Frequently used to model transition times  When adding hidden variables, can model arbitrary</p>
    <p>transition distributions (cf., Nodelman et al)</p>
    <p>Last arrival time irrelevant!</p>
  </div>
  <div class="page">
    <p>L(H)t-1 L (H)</p>
    <p>t L (H)</p>
    <p>t+1</p>
    <p>t-1 t t+1</p>
    <p>Ct Dtt</p>
    <p>Generative Model (conceptual) L(C)t-1 L</p>
    <p>(C) t L</p>
    <p>(C) t+1</p>
    <p>t = [t (C),t</p>
    <p>(H)] Time for next</p>
    <p>email from topic (exponential</p>
    <p>dist.)</p>
    <p>Time between Subsequent</p>
    <p>emails</p>
    <p>Topic indicator</p>
    <p>(i.e., = Conference)</p>
    <p>Document Representation</p>
    <p>(words)</p>
    <p>Intensity for Conference</p>
    <p>Intensity for Hiking</p>
    <p>Implicit Data Association (IDA) Model</p>
  </div>
  <div class="page">
    <p>Key modeling trick  Implicit data association (IDA) via</p>
    <p>exponential order statistics P(t | Lt) = min { Exp(Lt(C)), Exp(Lt(H)) } P(Ct | Lt) = argmin { Exp(Lt(C)), Exp(Lt(H)) }</p>
    <p>Simple closed form for these order statistics!  Quite general modeling idea  Turns model (essentially) into Factorial HMM  Many efficient inference techniques available!</p>
    <p>C: 2:00 pm H: 2:30 pm</p>
    <p>Email 1, Conference At 2:00 pm</p>
    <p>C: 4:15pm H: 2:30 pm</p>
    <p>Email 2, Hiking At 2:30 pm</p>
    <p>C: 4:15 pm H: 7:30 pm</p>
    <p>Email 3, Conference At 4:15 pm</p>
    <p>L(H)t</p>
    <p>Ct Dtt</p>
    <p>L(C)t</p>
  </div>
  <div class="page">
    <p>Inference Procedures  We consider:</p>
    <p>Full (conceptual) model:  Particle filter</p>
    <p>Simplified Model:  Particle filter  Fully factorized mean field  Extract inference</p>
    <p>Comparison to a Weighted Automaton Model for single topics, proposed by Kleinberg (first classify, then identify bursts)</p>
  </div>
  <div class="page">
    <p>Results (Synthetic data)  Periodic message arrivals (uninformative ) with</p>
    <p>noisy class assignments: ABBBABABABBB</p>
    <p>Misclassification Noise</p>
    <p>Message number</p>
    <p>T o</p>
    <p>p ic</p>
    <p>d e</p>
    <p>lta</p>
    <p>Topic</p>
  </div>
  <div class="page">
    <p>Results (Synthetic data)  Periodic message arrivals (uninformative ) with</p>
    <p>noisy class assigments: ABBBABABABBB</p>
    <p>Misclassification Noise</p>
    <p>Message number</p>
    <p>T o</p>
    <p>p ic</p>
    <p>d e</p>
    <p>lta</p>
    <p>Topic  Part. Filt. (Full model)</p>
  </div>
  <div class="page">
    <p>Results (Synthetic data)  Periodic message arrivals (uninformative ) with</p>
    <p>noisy class assigments: ABBBABABABBB</p>
    <p>Misclassification Noise</p>
    <p>Message number</p>
    <p>T o</p>
    <p>p ic</p>
    <p>d e</p>
    <p>lta</p>
    <p>Topic  Part. Filt. (Full model)</p>
    <p>Exact inference</p>
  </div>
  <div class="page">
    <p>Results (Synthetic data)  Periodic message arrivals (uninformative ) with</p>
    <p>noisy class assigments: ABBBABABABBB</p>
    <p>Misclassification Noise</p>
    <p>Message number</p>
    <p>T o</p>
    <p>p ic</p>
    <p>d e</p>
    <p>lta</p>
    <p>Topic  Part. Filt. (Full model)</p>
    <p>Exact inference</p>
    <p>Weighted automaton (first classify, then bursts)</p>
    <p>Implicit Data Association gets both topics and frequencies right, inspite severe (30%) label noise.</p>
    <p>Memorylessness trick doesnt hurt</p>
    <p>Separate topic and burst identification fails badly.</p>
  </div>
  <div class="page">
    <p>Inference comparison (Synthethic data)  Two topics, with different frequency pattern</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
  </div>
  <div class="page">
    <p>Inference comparison (Synthethic data)  Two topics, with different frequency pattern</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
    <p>Message</p>
  </div>
  <div class="page">
    <p>Inference comparison (Synthethic data)  Two topics, with different frequency pattern</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
    <p>Message</p>
    <p>Exact inference</p>
  </div>
  <div class="page">
    <p>Inference comparison (Synthethic data)  Two topics, with different frequency pattern</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
    <p>Message</p>
    <p>Exact inference</p>
    <p>Particle filter</p>
  </div>
  <div class="page">
    <p>Inference comparison (Synthethic data)  Two topics, with different frequency pattern</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
    <p>Message</p>
    <p>Exact inference</p>
    <p>Particle filter</p>
    <p>Meanfield</p>
    <p>Implicit Data Association identifies true frequency parameters (does not get distracted by observed )</p>
    <p>In addition to exact inference (for few topics), several approximate inference techniques perform well.</p>
  </div>
  <div class="page">
    <p>Experiments on real document streams  ENRON Email corpus</p>
    <p>517,431 emails from 151 employees  Selected 554 messages from tech-memos and</p>
    <p>universities folders of Kaminski  Stream between December 1999 and May 2001</p>
    <p>Reuters news archive  Contains 810,000 news articles  Selected 2,303 documents from four topics:</p>
    <p>wholesale prices, environment issues, fashion and obituaries</p>
  </div>
  <div class="page">
    <p>Intensity identification for Enron data</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic</p>
  </div>
  <div class="page">
    <p>Enron data</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic  WAM</p>
  </div>
  <div class="page">
    <p>Enron data</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Topic  WAM IDA-IT</p>
  </div>
  <div class="page">
    <p>Enron data</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>el ta</p>
    <p>Implicit Data Association identifies bursts which are missed by Weighted Automaton Model (separate approach)</p>
    <p>Topic  WAM IDA-IT</p>
  </div>
  <div class="page">
    <p>Reuters news archive</p>
    <p>Again, simultaneous topic and burst identification outperforms separate approach</p>
    <p>Message number T</p>
    <p>op ic</p>
    <p>d el</p>
    <p>ta</p>
    <p>True WAM IDA-IT</p>
    <p>Message number</p>
    <p>T op</p>
    <p>ic d</p>
    <p>e lta</p>
    <p>Topic  WAM IDA-IT</p>
  </div>
  <div class="page">
    <p>What about classification?  Temporal modeling effectively changes class</p>
    <p>prior over time.  Impact on classification accuracy?</p>
  </div>
  <div class="page">
    <p>Classification performance</p>
    <p>Modeling intensity leads to improved classification accuracy</p>
    <p>Nave Bayes</p>
    <p>IDA Model</p>
  </div>
  <div class="page">
    <p>Generalizations  Learning paradigms</p>
    <p>Not just supervised setting, but also:  Unsupervised- / semisupervised learning  Active learning (select most informative labels)  See paper for details.</p>
    <p>Other document representations  Other applications</p>
    <p>Fault detection  Activity recognition</p>
  </div>
  <div class="page">
    <p>L(H)t-1 L (H)</p>
    <p>t L (H)</p>
    <p>t+1</p>
    <p>Ct Dtt</p>
    <p>Topic tracking L(C)t-1 L</p>
    <p>(C) t L</p>
    <p>(C) t+1</p>
    <p>Time between Subsequent</p>
    <p>emails</p>
    <p>Topic indicator</p>
    <p>(i.e., = Conference)</p>
    <p>Document Representation</p>
    <p>(LSI)</p>
    <p>Intensity for Conference</p>
    <p>Intensity for Hiking</p>
    <p>tt-1 t+1</p>
    <p>Topic param. (Mean for LSI</p>
    <p>representation)</p>
    <p>t tracks topic means (Kalman Filter)</p>
  </div>
  <div class="page">
    <p>Conclusion  General model for data association in</p>
    <p>data streams</p>
    <p>A principled model for changing class priors over time</p>
    <p>Can be used in supervised, unsupervised and (semisupervised) active learning setting</p>
  </div>
  <div class="page">
    <p>Conclusion  Surprising performance of simplified IDA model</p>
    <p>Exponential order statistics enable implicit data association and tractable exact inference</p>
    <p>Synergetic effect between intensity estimation and classification on several real-world data sets</p>
  </div>
</Presentation>
