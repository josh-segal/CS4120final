<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sparse and Constrained Attention for</p>
    <p>Neural Machine Translation Chaitanya Malaviya1, Pedro Ferreira2, Andr F.T. Martins2,3</p>
  </div>
  <div class="page">
    <p>Repetitions</p>
    <p>Adequacy in Neural Machine Translation</p>
    <p>Dropped words</p>
    <p>Source: und wir benutzen dieses wort mit solcher verachtung .</p>
    <p>Translation: and we use this word with such contempt contempt .</p>
    <p>Reference: and we say that word with such contempt .</p>
    <p>Ein 28-jhriger Koch, der krzlich nach Pittsburgh gezogen war, wurde diese Woche im Treppenhaus eines rtlichen Einkaufszentrums tot aufgefunden .</p>
    <p>A 28-year-old chef who recently moved to Pittsburgh was found dead in the staircase this week .</p>
    <p>A 28-year-old chef who recently moved to Pittsburgh was found dead in the staircase of a local shopping mall this week .</p>
    <p>Source:</p>
    <p>Reference:</p>
    <p>Translation: 2</p>
  </div>
  <div class="page">
    <p>Previous Work</p>
    <p>Conditioning on coverage vectors to track attention history (Mi, 2016 ; Tu, 2016).</p>
    <p>Gating architectures and adaptive attention to control amount of source context (Tu, 2017; Li &amp; Zhu, 2017).</p>
    <p>Reconstruction Loss (Tu, 2017).</p>
    <p>Coverage penalty during decoding (Wu, 2016).</p>
  </div>
  <div class="page">
    <p>Main Contributions</p>
    <p>J'ai mang le sandwich</p>
  </div>
  <div class="page">
    <p>NMT + Attention Architecture</p>
  </div>
  <div class="page">
    <p>e1 e2 e3 e4</p>
    <p>f1 f2 f3 f4</p>
    <p>h1 h2 h3 h4</p>
    <p>attn_score</p>
    <p>attn_transform</p>
    <p>J'ai mang le sandwich</p>
    <p>I ate the sandwich</p>
    <p>attn_score:  dot-product (Luong, 2015)  bilinear function  MLP (Bahdanau, 2014)</p>
    <p>attn_transform:  traditional softmax  constrained softmax (Martins &amp; Kreutzer, 2017)  sparsemax (Martins &amp; Astudillo, 2016)  constrained sparsemax (this work)</p>
    <p>g1 c1</p>
    <p>g2 c2</p>
    <p>g3 c3</p>
    <p>g4 c4</p>
  </div>
  <div class="page">
    <p>Attention Transform Functions</p>
    <p>Sparsemax: Euclidean projection of z provides sparse probability distributions.</p>
    <p>Constrained Softmax: Returns the distribution closest to softmax whose attention probabilities are bounded by upper bounds u.</p>
  </div>
  <div class="page">
    <p>Attention Transform Functions</p>
    <p>Sparse and Constrained?</p>
  </div>
  <div class="page">
    <p>Constrained Sparsemax</p>
    <p>Provides sparse and bounded probability distributions.</p>
    <p>This transformation has two levels of sparsity: over time steps &amp; over attended words at each step.</p>
    <p>Efficient linear and sublinear time algorithms for forward and backward propagation.</p>
  </div>
  <div class="page">
    <p>Visualization: Attention transform functions</p>
    <p>csparsemax provides sparse and constrained probabilities.</p>
    <p>t=0</p>
    <p>t=1</p>
    <p>t=2</p>
  </div>
  <div class="page">
    <p>Fertility-based NMT Model</p>
  </div>
  <div class="page">
    <p>Fertility-based NMT</p>
    <p>Allocate fertilities for each source word as attention budgets that exhaust over decoding.</p>
    <p>Fertility Predictor : Train biLSTM model supervised by fertilities from fast_align (IBM Model 2).</p>
  </div>
  <div class="page">
    <p>Fertilities incorporated as:</p>
    <p>Fertility-based NMT</p>
    <p>Exhaustion strategy to encourage more attention for words with larger credit remaining:</p>
  </div>
  <div class="page">
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Experiments performed on 3 language pairs: De-En (IWSLT 2014), Ro-En (Europarl), Ja-En (KFTT).</p>
    <p>Joint BPE with 32K merge operations.</p>
    <p>Default hyperparameter settings in OpenNMT-Py.</p>
    <p>Baselines: Softmax, + CovPenalty (Wu, 2016) and + CovVector (Tu, 2016)</p>
  </div>
  <div class="page">
    <p>Evaluation Metrics: REP-Score &amp; DROP-Score</p>
    <p>REP Score:</p>
    <p>Penalizes n-gram repetitions in predicted translations.  Normalize by number of words in reference corpus.</p>
    <p>DROP Score:</p>
    <p>Find word alignments from source to reference &amp; source to predicted.</p>
    <p>% of source words aligned with some word in reference, but not with any word in predicted translation.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>De-En Ja-En Ro-En</p>
    <p>softmax softmax+CovPenalty softmax+CovVector csparsemax</p>
    <p>BLEU Scores</p>
  </div>
  <div class="page">
    <p>De-En Ja-En Ro-En 1.98</p>
    <p>softmax softmax+CovPenalty softmax+CovVector csparsemax</p>
    <p>REP Scores</p>
    <p>De-En Ja-En Ro-En</p>
    <p>DROP Scores</p>
    <p>Lower is better!</p>
  </div>
  <div class="page">
    <p>csparsemax yields sparse set of alignments and avoids repetitions.</p>
    <p>csparsemaxsoftmax 19</p>
  </div>
  <div class="page">
    <p>Examples of Translations</p>
  </div>
  <div class="page">
    <p>More in the paper</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Code: www.github.com/Unbabel/ sparse_constrained_attention</p>
    <p>Questions?</p>
  </div>
</Presentation>
