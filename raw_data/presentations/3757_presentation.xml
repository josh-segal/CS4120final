<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>What Kind of Language Is Hard to Language-Model? ACL 2019</p>
    <p>Sebastian J. Mielke and Ryan Cotterell, Kyle Gorman, Brian Roark, Jason Eisner</p>
    <p>Johns Hopkins University // City University of New York Graduate Center // Google sjmielke@jhu.edu</p>
    <p>Twitter: @sjmielke  paper and thread pinned!</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
    <p>No.</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
    <p>German.</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
    <p>It depends.</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
    <p>Actually, rather technical factors.</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
    <p>Its different, but not actually easier!</p>
  </div>
  <div class="page">
    <p>Questions and answers</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty</p>
    <p>Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en I love Florence! 0.03  5 bits</p>
    <p>de Ich gre meine Oma und die Familie dahein. 0.008  7 bits nl Alle mensen worden vrij en gelijk in waardigheid en rechten geboren. 0.0004  11 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en I love Florence! 0.03  5 bits de Ich gre meine Oma und die Familie dahein. 0.008  7 bits</p>
    <p>nl Alle mensen worden vrij en gelijk in waardigheid en rechten geboren. 0.0004  11 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en I love Florence! 0.03  5 bits de Ich gre meine Oma und die Familie dahein. 0.008  7 bits nl Alle mensen worden vrij en gelijk in waardigheid en rechten geboren. 0.0004  11 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en I love Florence! 0.03  5 bits de Ich gre meine Oma und die Familie dahein. 0.008  7 bits nl Alle mensen worden vrij en gelijk in waardigheid en rechten geboren. 0.0004  11 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en Resumption of the session. 0.013  6.5 bits de Wiederaufnahme der Sitzung. 0.011  6.3 bits nl Hervatting van de sessie. 0.012  6.4 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en Resumption of the session. 0.013  6.5 bits de Wiederaufnahme der Sitzung. 0.011  6.3 bits nl Hervatting van de sessie. 0.012  6.4 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en Resumption of the session. 0.013  6.5 bits de Wiederaufnahme der Sitzung. 0.011  6.3 bits nl Hervatting van de sessie. 0.012  6.4 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en Resumption of the session. 0.013  6.5 bits de Wiederaufnahme der Sitzung. 0.011  6.3 bits nl Hervatting van de sessie. 0.012  6.4 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to measure difficulty?</p>
    <p>Language models measure surprisal/information content (NLL; log p()):</p>
    <p>p()  NLL en Resumption of the session. 0.013  6.5 bits de Wiederaufnahme der Sitzung. 0.011  6.3 bits nl Hervatting van de sessie. 0.012  6.4 bits</p>
    <p>Issue 1: Different topics/styles/content</p>
    <p>Solution: train and test on translations!</p>
    <p>Europarl: 21 languages share ~40M chars Bibles: 62 languages share ~4M chars</p>
    <p>and this one takes a big ILP to solve, which is really fun</p>
    <p>Gurobi</p>
    <p>69 languages</p>
    <p>Issue 2: Comparing scores</p>
    <p>Use total bits of an open-vocabulary model.</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>Every UNK is cheating  morphologically rich languages have more UNKs, unfairly advantaging them.</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence [note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>Example: if puccz and Putschde are equally likely, they should be equally difficult.</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence [note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>Example: if puccz and Putschde are equally likely, they should be equally difficult.</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence [note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence</p>
    <p>[note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence</p>
    <p>[note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to compare your language models across languages</p>
    <p>just use overall bits (i.e., surprisal/NLL) of an aligned sentence [note: total easily obtainable from BPC or perplexity by multiplying with total chars/words]</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>For fully parallel corpora...</p>
    <p>we can just sum everything up and compare  that is fair.</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Although we were not al...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>Jetzt ist die Zeit ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>For fully parallel corpora...</p>
    <p>we can just sum everything up and compare  that is fair.</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Although we were not al...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>Jetzt ist die Zeit ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de y1,bg</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,en y3,de y3,bg</p>
    <p>y4,en y4,de y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>For fully parallel corpora... we can just sum everything up and compare  that is fair.</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Although we were not al...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>Jetzt ist die Zeit ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de y1,bg</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,en y3,de y3,bg</p>
    <p>y4,en y4,de y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de</p>
    <p>n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de = n2</p>
    <p>exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de = n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr</p>
  </div>
  <div class="page">
    <p>How to aggregate multiple intents surprisals into difficulties?</p>
    <p>But what if theres missing data? Or we want robustness?</p>
    <p>Resumption of the session</p>
    <p>Wiederaufnahme der ...</p>
    <p>The peace that ...</p>
    <p>Der gestern verein...</p>
    <p>,   ...</p>
    <p>Obwohl wir nicht ...</p>
    <p>...</p>
    <p>Now we can finally ...</p>
    <p>...</p>
    <p>en de bg</p>
    <p>aligned multi-text</p>
    <p>language</p>
    <p>model</p>
    <p>y1,en y1,de</p>
    <p>y2,en y2,de y2,bg</p>
    <p>y3,de y3,bg</p>
    <p>y4,en y4,bg</p>
    <p>LM surprisals/NLLs</p>
    <p>n1</p>
    <p>n2</p>
    <p>n3</p>
    <p>n4</p>
    <p>en</p>
    <p>de</p>
    <p>bg</p>
    <p>den dde dbg</p>
    <p>y2,de n2  exp dde</p>
    <p>log-normal noi se</p>
    <p>This is a probabilistic model we can perform inference in!</p>
    <p>not quite, our actual model isH E T E R O S C E D A S T I Cyi j = ni exp(dj) exp(i j)2i = ln</p>
    <p>i j  N</p>
    <p>22 i</p>
    <p>Image CC-BY Mike Grauer Jr / flickr 6</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>Good open-vocabulary language models</p>
    <p>(Mielke and Eisner, 2019)</p>
    <p>Formerly state-of-the-art-ish AWD-LSTM (Merity et al., 2018) language models:</p>
    <p>char-RNNLM: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>t h e c a t c h a s e d</p>
    <p>BPE-RNNLM, few merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the ca@@ t cha@@ sed</p>
    <p>BPE-RNNLM, many merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the cat cha@@ sed</p>
  </div>
  <div class="page">
    <p>Good open-vocabulary language models (Mielke and Eisner, 2019)</p>
    <p>Formerly state-of-the-art-ish AWD-LSTM (Merity et al., 2018) language models:</p>
    <p>char-RNNLM: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>t h e c a t c h a s e d</p>
    <p>BPE-RNNLM, few merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the ca@@ t cha@@ sed</p>
    <p>BPE-RNNLM, many merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the cat cha@@ sed</p>
  </div>
  <div class="page">
    <p>Good open-vocabulary language models (Mielke and Eisner, 2019)</p>
    <p>Formerly state-of-the-art-ish AWD-LSTM (Merity et al., 2018) language models:</p>
    <p>char-RNNLM: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>t h e c a t c h a s e d</p>
    <p>BPE-RNNLM, few merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the ca@@ t cha@@ sed</p>
    <p>BPE-RNNLM, many merges: RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>RNN cell</p>
    <p>the cat cha@@ sed</p>
  </div>
  <div class="page">
    <p>Choosing the number of BPE merges: how many is best?</p>
    <p>It depends on the language (total surprisal, given merges as a ratio of the vocabulary):</p>
    <p>ratio:</p>
    <p>lo</p>
    <p>w er</p>
    <p>is be</p>
    <p>tt er</p>
    <p>bg</p>
    <p>bg bg</p>
    <p>bg bg bg bg bg bg bg</p>
    <p>cs</p>
    <p>cs</p>
    <p>cs cs cs cs</p>
    <p>cs cs</p>
    <p>cs cs</p>
    <p>da</p>
    <p>da</p>
    <p>da da da da da da da da</p>
    <p>de</p>
    <p>de</p>
    <p>de de de de</p>
    <p>de de de</p>
    <p>el</p>
    <p>el</p>
    <p>el el el el el el</p>
    <p>el el</p>
    <p>en</p>
    <p>en</p>
    <p>en</p>
    <p>en en en en en</p>
    <p>en en</p>
    <p>es</p>
    <p>es es es es es es es es es</p>
    <p>et</p>
    <p>et et et</p>
    <p>et et et et et et</p>
    <p>fi</p>
    <p>fi fi</p>
    <p>fi fi</p>
    <p>fi fi fi fi fi</p>
    <p>fr</p>
    <p>fr fr</p>
    <p>fr fr fr</p>
    <p>fr fr fr fr</p>
    <p>hu</p>
    <p>hu hu hu</p>
    <p>hu hu</p>
    <p>hu hu hu hu</p>
    <p>it</p>
    <p>it it</p>
    <p>it</p>
    <p>it it</p>
    <p>it it it it lt</p>
    <p>lt lt</p>
    <p>lt lt lt lt lt lt lt</p>
    <p>lv</p>
    <p>lv lv lv lv lv</p>
    <p>lv lv lv lv</p>
    <p>nl</p>
    <p>nl</p>
    <p>nl nl nl nl</p>
    <p>nl nl nl nl</p>
    <p>pl</p>
    <p>pl pl pl pl</p>
    <p>pl pl pl pl</p>
    <p>pl</p>
    <p>pt</p>
    <p>pt pt</p>
    <p>pt pt pt pt pt pt</p>
    <p>pt</p>
    <p>ro</p>
    <p>ro ro</p>
    <p>ro ro</p>
    <p>ro ro ro</p>
    <p>ro ro</p>
    <p>sk</p>
    <p>sk</p>
    <p>sk sk sk</p>
    <p>sk sk sk sk sk</p>
    <p>sl</p>
    <p>sl sl</p>
    <p>sl sl sl sl</p>
    <p>sl sl sl sv</p>
    <p>sv</p>
    <p>sv sv sv</p>
    <p>sv sv sv sv sv</p>
    <p>average</p>
    <p>is this one going to be fine?</p>
    <p>Yeah:</p>
    <p>it doesnt matter</p>
    <p>that much.</p>
    <p>ch ars</p>
    <p>B P</p>
    <p>E (0</p>
    <p>B P</p>
    <p>E (best)</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv</p>
  </div>
  <div class="page">
    <p>Choosing the number of BPE merges: how many is best?</p>
    <p>It depends on the language (total surprisal, given merges as a ratio of the vocabulary):</p>
    <p>ratio:</p>
    <p>lo</p>
    <p>w er</p>
    <p>is be</p>
    <p>tt er</p>
    <p>bg</p>
    <p>bg bg</p>
    <p>bg bg bg bg bg bg bg</p>
    <p>cs</p>
    <p>cs</p>
    <p>cs cs cs cs</p>
    <p>cs cs</p>
    <p>cs cs</p>
    <p>da</p>
    <p>da</p>
    <p>da da da da da da da da</p>
    <p>de</p>
    <p>de</p>
    <p>de de de de</p>
    <p>de de de</p>
    <p>el</p>
    <p>el</p>
    <p>el el el el el el</p>
    <p>el el</p>
    <p>en</p>
    <p>en</p>
    <p>en</p>
    <p>en en en en en</p>
    <p>en en</p>
    <p>es</p>
    <p>es es es es es es es es es</p>
    <p>et</p>
    <p>et et et</p>
    <p>et et et et et et</p>
    <p>fi</p>
    <p>fi fi</p>
    <p>fi fi</p>
    <p>fi fi fi fi fi</p>
    <p>fr</p>
    <p>fr fr</p>
    <p>fr fr fr</p>
    <p>fr fr fr fr</p>
    <p>hu</p>
    <p>hu hu hu</p>
    <p>hu hu</p>
    <p>hu hu hu hu</p>
    <p>it</p>
    <p>it it</p>
    <p>it</p>
    <p>it it</p>
    <p>it it it it lt</p>
    <p>lt lt</p>
    <p>lt lt lt lt lt lt lt</p>
    <p>lv</p>
    <p>lv lv lv lv lv</p>
    <p>lv lv lv lv</p>
    <p>nl</p>
    <p>nl</p>
    <p>nl nl nl nl</p>
    <p>nl nl nl nl</p>
    <p>pl</p>
    <p>pl pl pl pl</p>
    <p>pl pl pl pl</p>
    <p>pl</p>
    <p>pt</p>
    <p>pt pt</p>
    <p>pt pt pt pt pt pt</p>
    <p>pt</p>
    <p>ro</p>
    <p>ro ro</p>
    <p>ro ro</p>
    <p>ro ro ro</p>
    <p>ro ro</p>
    <p>sk</p>
    <p>sk</p>
    <p>sk sk sk</p>
    <p>sk sk sk sk sk</p>
    <p>sl</p>
    <p>sl sl</p>
    <p>sl sl sl sl</p>
    <p>sl sl sl sv</p>
    <p>sv</p>
    <p>sv sv sv</p>
    <p>sv sv sv sv sv</p>
    <p>average</p>
    <p>is this one going to be fine?</p>
    <p>Yeah:</p>
    <p>it doesnt matter</p>
    <p>that much.</p>
    <p>ch ars</p>
    <p>B P</p>
    <p>E (0</p>
    <p>B P</p>
    <p>E (best)</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv</p>
  </div>
  <div class="page">
    <p>Choosing the number of BPE merges: how many is best?</p>
    <p>It depends on the language (total surprisal, given merges as a ratio of the vocabulary):</p>
    <p>ratio:</p>
    <p>lo</p>
    <p>w er</p>
    <p>is be</p>
    <p>tt er</p>
    <p>bg</p>
    <p>bg bg</p>
    <p>bg bg bg bg bg bg bg</p>
    <p>cs</p>
    <p>cs</p>
    <p>cs cs cs cs</p>
    <p>cs cs</p>
    <p>cs cs</p>
    <p>da</p>
    <p>da</p>
    <p>da da da da da da da da</p>
    <p>de</p>
    <p>de</p>
    <p>de de de de</p>
    <p>de de de</p>
    <p>el</p>
    <p>el</p>
    <p>el el el el el el</p>
    <p>el el</p>
    <p>en</p>
    <p>en</p>
    <p>en</p>
    <p>en en en en en</p>
    <p>en en</p>
    <p>es</p>
    <p>es es es es es es es es es</p>
    <p>et</p>
    <p>et et et</p>
    <p>et et et et et et</p>
    <p>fi</p>
    <p>fi fi</p>
    <p>fi fi</p>
    <p>fi fi fi fi fi</p>
    <p>fr</p>
    <p>fr fr</p>
    <p>fr fr fr</p>
    <p>fr fr fr fr</p>
    <p>hu</p>
    <p>hu hu hu</p>
    <p>hu hu</p>
    <p>hu hu hu hu</p>
    <p>it</p>
    <p>it it</p>
    <p>it</p>
    <p>it it</p>
    <p>it it it it lt</p>
    <p>lt lt</p>
    <p>lt lt lt lt lt lt lt</p>
    <p>lv</p>
    <p>lv lv lv lv lv</p>
    <p>lv lv lv lv</p>
    <p>nl</p>
    <p>nl</p>
    <p>nl nl nl nl</p>
    <p>nl nl nl nl</p>
    <p>pl</p>
    <p>pl pl pl pl</p>
    <p>pl pl pl pl</p>
    <p>pl</p>
    <p>pt</p>
    <p>pt pt</p>
    <p>pt pt pt pt pt pt</p>
    <p>pt</p>
    <p>ro</p>
    <p>ro ro</p>
    <p>ro ro</p>
    <p>ro ro ro</p>
    <p>ro ro</p>
    <p>sk</p>
    <p>sk</p>
    <p>sk sk sk</p>
    <p>sk sk sk sk sk</p>
    <p>sl</p>
    <p>sl sl</p>
    <p>sl sl sl sl</p>
    <p>sl sl sl sv</p>
    <p>sv</p>
    <p>sv sv sv</p>
    <p>sv sv sv sv sv</p>
    <p>average is this one going to be fine?</p>
    <p>Yeah:</p>
    <p>it doesnt matter</p>
    <p>that much.</p>
    <p>ch ars</p>
    <p>B P</p>
    <p>E (0</p>
    <p>B P</p>
    <p>E (best)</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv</p>
  </div>
  <div class="page">
    <p>Choosing the number of BPE merges: how many is best?</p>
    <p>It depends on the language (total surprisal, given merges as a ratio of the vocabulary):</p>
    <p>ratio:</p>
    <p>lo</p>
    <p>w er</p>
    <p>is be</p>
    <p>tt er</p>
    <p>bg</p>
    <p>bg bg</p>
    <p>bg bg bg bg bg bg bg</p>
    <p>cs</p>
    <p>cs</p>
    <p>cs cs cs cs</p>
    <p>cs cs</p>
    <p>cs cs</p>
    <p>da</p>
    <p>da</p>
    <p>da da da da da da da da</p>
    <p>de</p>
    <p>de</p>
    <p>de de de de</p>
    <p>de de de</p>
    <p>el</p>
    <p>el</p>
    <p>el el el el el el</p>
    <p>el el</p>
    <p>en</p>
    <p>en</p>
    <p>en</p>
    <p>en en en en en</p>
    <p>en en</p>
    <p>es</p>
    <p>es es es es es es es es es</p>
    <p>et</p>
    <p>et et et</p>
    <p>et et et et et et</p>
    <p>fi</p>
    <p>fi fi</p>
    <p>fi fi</p>
    <p>fi fi fi fi fi</p>
    <p>fr</p>
    <p>fr fr</p>
    <p>fr fr fr</p>
    <p>fr fr fr fr</p>
    <p>hu</p>
    <p>hu hu hu</p>
    <p>hu hu</p>
    <p>hu hu hu hu</p>
    <p>it</p>
    <p>it it</p>
    <p>it</p>
    <p>it it</p>
    <p>it it it it lt</p>
    <p>lt lt</p>
    <p>lt lt lt lt lt lt lt</p>
    <p>lv</p>
    <p>lv lv lv lv lv</p>
    <p>lv lv lv lv</p>
    <p>nl</p>
    <p>nl</p>
    <p>nl nl nl nl</p>
    <p>nl nl nl nl</p>
    <p>pl</p>
    <p>pl pl pl pl</p>
    <p>pl pl pl pl</p>
    <p>pl</p>
    <p>pt</p>
    <p>pt pt</p>
    <p>pt pt pt pt pt pt</p>
    <p>pt</p>
    <p>ro</p>
    <p>ro ro</p>
    <p>ro ro</p>
    <p>ro ro ro</p>
    <p>ro ro</p>
    <p>sk</p>
    <p>sk</p>
    <p>sk sk sk</p>
    <p>sk sk sk sk sk</p>
    <p>sl</p>
    <p>sl sl</p>
    <p>sl sl sl sl</p>
    <p>sl sl sl sv</p>
    <p>sv</p>
    <p>sv sv sv</p>
    <p>sv sv sv sv sv</p>
    <p>average is this one going to be fine?</p>
    <p>Yeah:</p>
    <p>it doesnt matter</p>
    <p>that much.</p>
    <p>ch ars</p>
    <p>B P</p>
    <p>E (0</p>
    <p>B P</p>
    <p>E (best)</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages</p>
    <p>and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl</p>
    <p>vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages</p>
    <p>and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl</p>
    <p>vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages</p>
    <p>and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl</p>
    <p>vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages</p>
    <p>and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl</p>
    <p>vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages and</p>
    <p>Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>de</p>
    <p>de de</p>
    <p>de de</p>
    <p>de</p>
    <p>de</p>
    <p>de</p>
    <p>de</p>
    <p>de</p>
    <p>de en</p>
    <p>en</p>
    <p>en</p>
    <p>en</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars cmn</p>
    <p>afr</p>
    <p>aln</p>
    <p>arb</p>
    <p>arz</p>
    <p>ayr</p>
    <p>ayr</p>
    <p>bba</p>
    <p>ben ben</p>
    <p>bqc</p>
    <p>bul</p>
    <p>bul</p>
    <p>cac</p>
    <p>cak</p>
    <p>ceb ceb ceb</p>
    <p>ces</p>
    <p>ces cnh</p>
    <p>cym dan</p>
    <p>deu</p>
    <p>deu deu</p>
    <p>deu deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>ell eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>epo</p>
    <p>fin</p>
    <p>fin</p>
    <p>finfra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>guj</p>
    <p>gur</p>
    <p>hat hat</p>
    <p>hrvhun</p>
    <p>hun</p>
    <p>ind ind</p>
    <p>ita</p>
    <p>itaita</p>
    <p>ita</p>
    <p>kek</p>
    <p>kek</p>
    <p>kjb</p>
    <p>lat</p>
    <p>lit</p>
    <p>mah mammri</p>
    <p>myanld</p>
    <p>nor</p>
    <p>nor</p>
    <p>plt</p>
    <p>poh</p>
    <p>por</p>
    <p>por</p>
    <p>por</p>
    <p>qub quh</p>
    <p>quy</p>
    <p>quz</p>
    <p>ron rus</p>
    <p>som</p>
    <p>tbz</p>
    <p>tcw</p>
    <p>tgl</p>
    <p>tlh</p>
    <p>tpi</p>
    <p>tpm</p>
    <p>ukrukr vie</p>
    <p>vie</p>
    <p>vie</p>
    <p>wal</p>
    <p>wbm</p>
    <p>xhozom</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars</p>
    <p>cmn</p>
    <p>afr</p>
    <p>aln</p>
    <p>arb</p>
    <p>arz</p>
    <p>ayr</p>
    <p>ayr</p>
    <p>bba</p>
    <p>ben ben</p>
    <p>bqc</p>
    <p>bul</p>
    <p>bul</p>
    <p>cac</p>
    <p>cak</p>
    <p>ceb ceb ceb</p>
    <p>ces</p>
    <p>ces cnh</p>
    <p>cym dan</p>
    <p>deu</p>
    <p>deu deu</p>
    <p>deu deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>ell eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>epo</p>
    <p>fin</p>
    <p>fin</p>
    <p>finfra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>guj</p>
    <p>gur</p>
    <p>hat hat</p>
    <p>hrvhun</p>
    <p>hun</p>
    <p>ind ind</p>
    <p>ita</p>
    <p>itaita</p>
    <p>ita</p>
    <p>kek</p>
    <p>kek</p>
    <p>kjb</p>
    <p>lat</p>
    <p>lit</p>
    <p>mah mammri</p>
    <p>myanld</p>
    <p>nor</p>
    <p>nor</p>
    <p>plt</p>
    <p>poh</p>
    <p>por</p>
    <p>por</p>
    <p>por</p>
    <p>qub quh</p>
    <p>quy</p>
    <p>quz</p>
    <p>ron rus</p>
    <p>som</p>
    <p>tbz</p>
    <p>tcw</p>
    <p>tgl</p>
    <p>tlh</p>
    <p>tpi</p>
    <p>tpm</p>
    <p>ukrukr vie</p>
    <p>vie</p>
    <p>vie</p>
    <p>wal</p>
    <p>wbm</p>
    <p>xhozom</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Difficulties for char-/BPE-RNNLM: 21 Europarl languages and 106 Bibles</p>
    <p>4 3 2 1 0 1 2 3 4 5 8</p>
    <p>6</p>
    <p>4</p>
    <p>2</p>
    <p>easier with chars</p>
    <p>bg cs</p>
    <p>da el</p>
    <p>es et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lt</p>
    <p>lv</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>sv en</p>
    <p>de</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Europarl vs.</p>
    <p>h ard</p>
    <p>er</p>
    <p>ea si</p>
    <p>er</p>
    <p>de</p>
    <p>en</p>
    <p>deu</p>
    <p>eng</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>el</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lt</p>
    <p>nl pt</p>
    <p>ro</p>
    <p>bul</p>
    <p>ces</p>
    <p>dan</p>
    <p>ell</p>
    <p>fin</p>
    <p>fra</p>
    <p>hun</p>
    <p>ita</p>
    <p>lit</p>
    <p>nld</p>
    <p>por</p>
    <p>ron</p>
    <p>15 10 5 0 5 10 15 20 25</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>easier with BPE</p>
    <p>easier with chars cmn</p>
    <p>afr</p>
    <p>aln</p>
    <p>arb</p>
    <p>arz</p>
    <p>ayr</p>
    <p>ayr</p>
    <p>bba</p>
    <p>ben ben</p>
    <p>bqc</p>
    <p>bul</p>
    <p>bul</p>
    <p>cac</p>
    <p>cak</p>
    <p>ceb ceb ceb</p>
    <p>ces</p>
    <p>ces cnh</p>
    <p>cym dan</p>
    <p>deu</p>
    <p>deu deu</p>
    <p>deu deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>deu</p>
    <p>ell eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>eng</p>
    <p>epo</p>
    <p>fin</p>
    <p>fin</p>
    <p>finfra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>fra</p>
    <p>guj</p>
    <p>gur</p>
    <p>hat hat</p>
    <p>hrvhun</p>
    <p>hun</p>
    <p>ind ind</p>
    <p>ita</p>
    <p>itaita</p>
    <p>ita</p>
    <p>kek</p>
    <p>kek</p>
    <p>kjb</p>
    <p>lat</p>
    <p>lit</p>
    <p>mah mammri</p>
    <p>myanld</p>
    <p>nor</p>
    <p>nor</p>
    <p>plt</p>
    <p>poh</p>
    <p>por</p>
    <p>por</p>
    <p>por</p>
    <p>qub quh</p>
    <p>quy</p>
    <p>quz</p>
    <p>ron rus</p>
    <p>som</p>
    <p>tbz</p>
    <p>tcw</p>
    <p>tgl</p>
    <p>tlh</p>
    <p>tpi</p>
    <p>tpm</p>
    <p>ukrukr vie</p>
    <p>vie</p>
    <p>vie</p>
    <p>wal</p>
    <p>wbm</p>
    <p>xhozom</p>
    <p>difficulty (100) using BPE-RNNLM with 0.4|V| merges</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1 0 0 )</p>
    <p>u si</p>
    <p>n g</p>
    <p>ch ar</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>Difficulties on Bibles</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>How about: morphological counting complexity (Sagot, 2013)</p>
    <p>4</p>
    <p>2</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>et</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lv</p>
    <p>lt</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt ro</p>
    <p>sk sl</p>
    <p>es</p>
    <p>sv</p>
    <p>MCC</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1</p>
    <p>, B</p>
    <p>P E</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>)</p>
    <p>5</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lv</p>
    <p>lt</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>es</p>
    <p>sv</p>
    <p>MCC</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1</p>
    <p>, ch</p>
    <p>ar -R</p>
    <p>N N</p>
    <p>LM )</p>
    <p>...not particularly striking. Perhaps Finnish was an outlier in Cotterell et al. (2018)?</p>
  </div>
  <div class="page">
    <p>How about: morphological counting complexity (Sagot, 2013)</p>
    <p>4</p>
    <p>2</p>
    <p>bg</p>
    <p>cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>et</p>
    <p>fi</p>
    <p>fr</p>
    <p>hu</p>
    <p>it</p>
    <p>lv</p>
    <p>lt</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt ro</p>
    <p>sk sl</p>
    <p>es</p>
    <p>sv</p>
    <p>MCC</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1</p>
    <p>, B</p>
    <p>P E</p>
    <p>-R N</p>
    <p>N LM</p>
    <p>)</p>
    <p>5</p>
    <p>bg cs</p>
    <p>da</p>
    <p>de</p>
    <p>el</p>
    <p>en</p>
    <p>et</p>
    <p>fi</p>
    <p>fr hu</p>
    <p>it</p>
    <p>lv</p>
    <p>lt</p>
    <p>nl</p>
    <p>pl</p>
    <p>pt</p>
    <p>ro</p>
    <p>sk sl</p>
    <p>es</p>
    <p>sv</p>
    <p>MCC</p>
    <p>d if</p>
    <p>fi cu</p>
    <p>lt y</p>
    <p>( 1</p>
    <p>, ch</p>
    <p>ar -R</p>
    <p>N N</p>
    <p>LM )</p>
    <p>...not particularly striking. Perhaps Finnish was an outlier in Cotterell et al. (2018)?</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)?</p>
    <p>...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)?</p>
    <p>...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)?</p>
    <p>...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))?</p>
    <p>...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Other linguistically motivated regressors</p>
    <p>WALS: Prefixing vs. Suffixing [...] Morphology (for languages where present)? ...no visible differences.</p>
    <p>WALS: Order of Subject, Object and Verb (for languages where present)? ...no visible differences.</p>
    <p>Head-POS Entropy (Dehouck and Denis, 2018)? ...neither mean and skew show correlation.</p>
    <p>Average dependency length (computed using UDPipe (Straka et al., 2016))? ...correlation! But not significant after correcting for multiple hypotheses.</p>
    <p>This is disappointing.</p>
  </div>
  <div class="page">
    <p>Very simple heuristics are very predictive</p>
    <p>Raw sequence length / # predictions  char-RNNLM difficulty</p>
    <p>Significant on:  Europarl at p &lt; .01</p>
    <p>Bibles at p &lt; .001</p>
    <p>i.e., for the char-RNNLM puccz is easier than Putschde!</p>
    <p>Raw vocabulary size  BPE-RNNLM difficulty</p>
    <p>Significant on:</p>
    <p>not Europarl</p>
    <p>but Bibles at p &lt; .00000000001</p>
    <p>i.e., the BPE-RNNLM still suffers if a language has high type-token-ratio!</p>
    <p>Wow! What is happening here? We have many conjectures...</p>
  </div>
  <div class="page">
    <p>Very simple heuristics are very predictive</p>
    <p>Raw sequence length / # predictions  char-RNNLM difficulty</p>
    <p>Significant on:  Europarl at p &lt; .01</p>
    <p>Bibles at p &lt; .001</p>
    <p>i.e., for the char-RNNLM puccz is easier than Putschde!</p>
    <p>Raw vocabulary size  BPE-RNNLM difficulty</p>
    <p>Significant on:</p>
    <p>not Europarl</p>
    <p>but Bibles at p &lt; .00000000001</p>
    <p>i.e., the BPE-RNNLM still suffers if a language has high type-token-ratio!</p>
    <p>Wow! What is happening here? We have many conjectures...</p>
  </div>
  <div class="page">
    <p>Very simple heuristics are very predictive</p>
    <p>Raw sequence length / # predictions  char-RNNLM difficulty</p>
    <p>Significant on:  Europarl at p &lt; .01</p>
    <p>Bibles at p &lt; .001</p>
    <p>i.e., for the char-RNNLM puccz is easier than Putschde!</p>
    <p>Raw vocabulary size  BPE-RNNLM difficulty</p>
    <p>Significant on:</p>
    <p>not Europarl</p>
    <p>but Bibles at p &lt; .00000000001</p>
    <p>i.e., the BPE-RNNLM still suffers if a language has high type-token-ratio!</p>
    <p>Wow! What is happening here? We have many conjectures...</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Difficulty Models and languages</p>
    <p>What correlates with difficulty?</p>
    <p>And... is Translationese really easier?</p>
  </div>
  <div class="page">
    <p>Translationese: translations as a separate language?</p>
    <p>Common assumption: Translationese is somehow simpler than native text.</p>
    <p>We have partial parallel data that we can use to evaluate our models:</p>
    <p>enoriginal entranslated deoriginal detranslated nloriginal nltranslated</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>...and indeed the original languages seem harder. But we missed something!</p>
  </div>
  <div class="page">
    <p>Translationese: translations as a separate language?</p>
    <p>Common assumption: Translationese is somehow simpler than native text.</p>
    <p>We have partial parallel data that we can use to evaluate our models:</p>
    <p>enoriginal entranslated deoriginal detranslated nloriginal nltranslated</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>...and indeed the original languages seem harder. But we missed something!</p>
  </div>
  <div class="page">
    <p>Translationese: translations as a separate language?</p>
    <p>Common assumption: Translationese is somehow simpler than native text.</p>
    <p>We have partial parallel data that we can use to evaluate our models:</p>
    <p>enoriginal entranslated deoriginal detranslated nloriginal nltranslated</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>...and indeed the original languages seem harder.</p>
    <p>But we missed something!</p>
  </div>
  <div class="page">
    <p>Translationese: translations as a separate language?</p>
    <p>Common assumption: Translationese is somehow simpler than native text.</p>
    <p>We have partial parallel data that we can use to evaluate our models:</p>
    <p>enoriginal entranslated deoriginal detranslated nloriginal nltranslated</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>...and indeed the original languages seem harder. But we missed something!</p>
  </div>
  <div class="page">
    <p>We trained on mostly translationese!</p>
    <p>en fr de es nl it pt sv el fi pl da ro hu sk cs sl lt bg et lv 0</p>
    <p>languages, sorted by absolute # native sentences</p>
    <p>% n</p>
    <p>at iv</p>
    <p>e of</p>
    <p>se n</p>
    <p>te n</p>
    <p>ce s</p>
    <p>Of course we will then find it easier... 17</p>
  </div>
  <div class="page">
    <p>Repeat the experiment with fairly balancing training data</p>
    <p>Change the training sets!</p>
    <p>We can rebalance a single language, leaving the others merged, i.e.:</p>
    <p>enoriginal entranslated de nl</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>And the result: the difficulties are now the same! (more precisely, native is 0.0040.02 easier)</p>
  </div>
  <div class="page">
    <p>Repeat the experiment with fairly balancing training data</p>
    <p>Change the training sets!</p>
    <p>We can rebalance a single language, leaving the others merged, i.e.:</p>
    <p>enoriginal entranslated de nl</p>
    <p>Resumption... Wiederauf-... Hervatten...</p>
    <p>The German... Der deutsche... De Duitse...</p>
    <p>Thank you... Vielen Dank... Hartelijk...</p>
    <p>And the result: the difficulties are now the same! (more precisely, native is 0.0040.02 easier)</p>
  </div>
  <div class="page">
    <p>Conclusion: cross-linguistic comparisons are tricky (hope we didnt mess up!)</p>
  </div>
  <div class="page">
    <p>Conclusion: cross-linguistic comparisons are tricky (hope we didnt mess up!)</p>
  </div>
  <div class="page">
    <p>Conclusion: cross-linguistic comparisons are tricky (hope we didnt mess up!)</p>
  </div>
  <div class="page">
    <p>Conclusion: cross-linguistic comparisons are tricky (hope we didnt mess up!)</p>
  </div>
  <div class="page">
    <p>Conclusion: cross-linguistic comparisons are tricky (hope we didnt mess up!)</p>
  </div>
  <div class="page">
    <p>What Kind of Language Is Hard to Language-Model? ACL 2019</p>
    <p>Sebastian J. Mielke and Ryan Cotterell, Kyle Gorman, Brian Roark, Jason Eisner</p>
    <p>Johns Hopkins University // City University of New York Graduate Center // Google sjmielke@jhu.edu</p>
    <p>Twitter: @sjmielke  paper and thread pinned!</p>
  </div>
</Presentation>
