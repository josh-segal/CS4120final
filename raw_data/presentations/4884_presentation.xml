<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko, Harri Valpola, Yann LeCun Aalto University, New York University</p>
    <p>AISTATS 2012</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Learning deep networks (many hidden layers) used to be difficult</p>
    <p>Layerwise pretraining by RBMs or denoising autoencoders helps</p>
    <p>Could similar performance be achieved with back-propagation?</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Proposed method  Standard MLP (only shallow shown)  Include shortcut connections C</p>
    <p>Add linear transformations to nonlinearities</p>
    <p>Alphas and betas are not learned, but set to make learning the weights A,B,C easier</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij =</p>
    <p>t</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Separate the nonlinear and linear problems by disabling linear dependencies from f</p>
    <p>by setting</p>
    <p>Compensate by changing C accordingly</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
  </div>
  <div class="page">
    <p>Theoretical Motivation  Fisher information matrix becomes more diagonal  Standard gradient becomes closer to natural gradient</p>
    <p>A</p>
    <p>B</p>
    <p>A</p>
    <p>B C</p>
    <p>C</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i =  1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i =  1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold  A(&quot;new  &quot;old)B  A(#new  #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where  is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i  1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>#</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i  1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p =</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i  1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p =</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0,</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables i and i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + ibixt + i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (4)</p>
    <p>by setting i and i to</p>
    <p>i =  1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh(bixt) (5)</p>
    <p>i =  1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold  A(new  old)B  A(new  old) [0 0 . . . 1] , (7)</p>
    <p>where  is a matrix with elements i on the diagonal and one empty row below for the bias term, and  is a column vector with components i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ 2 log p(yt | xt, A, B, C)</p>
    <p>ij</p>
    <p>% , (8)</p>
    <p>where  is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector  contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>aij</p>
    <p>ai!j! log p =</p>
    <p>&amp; 0 i $= i  1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>= i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>bjk</p>
    <p>bj!k! log p =</p>
    <p>#</p>
    <p>i</p>
    <p>2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f  j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>cik</p>
    <p>ci!k! log p =</p>
    <p>&amp; 0 i $= i  1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>= i. (11)</p>
    <p>The cross terms are</p>
    <p>aij</p>
    <p>bj!k log p =</p>
    <p>2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f  j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>cik</p>
    <p>ai!j log p =</p>
    <p>&amp; 0 i $= i  1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>= i</p>
    <p>(13)</p>
    <p>cik</p>
    <p>bjk! log p =</p>
    <p>2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>where f is a nonlinearity (such as tanh) applied to each component of the argument vector separately, A, B, and C are the weight matrices, and !t is the noise which is assumed to be zero mean and Gaussian, that is, p(!it) = N</p>
    <p>! !it; 0, &quot;</p>
    <p>&quot; . In order to avoid separate</p>
    <p>bias vectors that complicate formulas, the input vectors are assumed to have been supplemented with an additional component that is always one.</p>
    <p>Let us supplement the tanh nonlinearity with auxiliary scalar variables #i and $i for each nonlinearity fi. They are not learnt, but instead they will be set in a manner to help learn the other parameters. We define</p>
    <p>fi(bixt) = tanh(bixt) + #ibixt + $i, (2)</p>
    <p>where bi is the ith row vector of matrix B. An example fi can be seen in Figure 1. We will ensure that</p>
    <p>T#</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 (3)</p>
    <p>T#</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0 (4)</p>
    <p>by setting #i and $i to</p>
    <p>#i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>tanh!(bixt) (5)</p>
    <p>$i = ! 1</p>
    <p>T</p>
    <p>T#</p>
    <p>t=1</p>
    <p>[tanh(bixt) + #ibixt] (6)</p>
    <p>as shown in the appendix.</p>
    <p>The e!ect of the linear transformation can be compensated exactly by updating the shortcut mapping C by</p>
    <p>Cnew = Cold ! A(&quot;new ! &quot;old)B ! A(#new ! #old) [0 0 . . . 1] , (7)</p>
    <p>where &quot; is a matrix with elements #i on the diagonal and one empty row below for the bias term, and # is a column vector with components $i and one zero below for the bias term.</p>
    <p>We also emphasize making the inputs xk zero mean (and similar in scale) as a preprocessing step (see e.g. [10]).</p>
    <p>Schraudolph [14, 13] proposed centering the factors of the gradient to zero mean. It was argued that deviations from the gradient fall into the linear subspace that the shortcut connections operate in, so they do not harm the overall performance. Transforming the nonlinearities as proposed in this paper has a similar e!ect on the gradient. Equation (3) corresponds to Schraudolphs activity centering and Equation (4) corresponds to slope centering.</p>
    <p>Second-order optimization methods such as the natural gradient [1] or Newtons method decrease the number of required iterations compared to the basic gradient descent, but they cannot be easily used with large models due to heavy computations with large matrices. The natural gradient is the basic gradient multiplied from the left by the inverse of the Fisher information matrix. Using basic gradient descent can thus be seen as using the natural gradient while approximating the Fisher information with a unit matrix. We will see how the proposed transformations moves the non-diagonal elements of the Fisher information matrix closer to zero, thus making the basic gradient closer to the natural gradient.</p>
    <p>The Fisher information matrix contains elements</p>
    <p>Gij = #</p>
    <p>t</p>
    <p>$ %2 log p(yt | xt, A, B, C)</p>
    <p>%&amp;i%&amp;j</p>
    <p>% , (8)</p>
    <p>where &quot;# is the expectation over the Gaussian distribution of noise !t in Equation (1), and vector $ contains all the elements of matrices A, B, and C. Note that yt is a random variable and thus the Fisher information does not depend on the output data.</p>
    <p>These elements are:</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%ai!j! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)fj!(bj!xt) i</p>
    <p>! = i,</p>
    <p>(9) where aij is the ijth element of matrix A, fj is the jth nonlinearity, and bj is the jth row vector of matrix B. Similarly</p>
    <p>%</p>
    <p>%bjk</p>
    <p>%</p>
    <p>%bj!k! log p =</p>
    <p>! #</p>
    <p>i</p>
    <p>&quot;2i aijaij!</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)f ! j!(bj!xt)xktxk!t (10)</p>
    <p>and</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ci!k! log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t xktxk!t i</p>
    <p>! = i. (11)</p>
    <p>The cross terms are</p>
    <p>%</p>
    <p>%aij</p>
    <p>%</p>
    <p>%bj!k log p = !</p>
    <p>&quot;2i aij!</p>
    <p>#</p>
    <p>t</p>
    <p>fj(bjxt)f ! j!(bj!xt)xkt</p>
    <p>(12)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%ai!j log p =</p>
    <p>&amp; 0 i! $= i ! 1</p>
    <p>!2i</p>
    <p>' t fj(bjxt)xkt i</p>
    <p>! = i</p>
    <p>(13)</p>
    <p>%</p>
    <p>%cik</p>
    <p>%</p>
    <p>%bjk! log p = !</p>
    <p>&quot;2i aij</p>
    <p>#</p>
    <p>t</p>
    <p>f!j(bjxt)xktxk!t. (14)</p>
  </div>
  <div class="page">
    <p>Implementation Details  Learning algorithm: Stochastic gradient  Mini-batch size 1000, momentum 0.9  Transformations done initially and after every 1000 iterations  Soft-max for discrete outputs  Normalized random initialization, shortcut weights to zero  Learning rate decreased linearly in the second half of</p>
    <p>learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>MNIST Classification  CIFAR-10 Classification  MNIST Autoencoder</p>
    <p>Image data, but nothing image-specific</p>
  </div>
  <div class="page">
    <p>MNIST Classification</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>data</p>
    <p>PCA</p>
    <p>noise</p>
    <p>noise</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>MNIST Classification</p>
    <p>Error against learning rate Error against learning time</p>
    <p>Training (lower) and test errors (higher)</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>MNIST Classification</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
  </div>
  <div class="page">
    <p>Visualization of learned weights to randomly chosen hidden units on layers 1 and 2, and to the class outputs 0,1,...,9</p>
    <p>MNIST Classification</p>
  </div>
  <div class="page">
    <p>CIFAR-10 Classification</p>
    <p>500-500-500-10 network</p>
    <p>original data</p>
    <p>after PCA to 500</p>
    <p>with noise</p>
    <p>with noise</p>
  </div>
  <div class="page">
    <p>original shortcuts transformations</p>
    <p>CIFAR-10 Classification</p>
    <p>Classification error against learning time</p>
  </div>
  <div class="page">
    <p>Classification %</p>
    <p>linear original shortcuts transf. Krizhevsky</p>
    <p>(2009)</p>
    <p>Training error 58.07 23.21 22.46 4.56</p>
    <p>Test error 59.09 44.42 44.99 43.70 48.47</p>
    <p>CIFAR-10 Classification</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>MNIST Autoencoder</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>data reconstruction</p>
    <p>linear</p>
  </div>
  <div class="page">
    <p>MNIST Autoencoder</p>
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>Reconstruction error against learning time</p>
  </div>
  <div class="page">
    <p>Deep Learning Made Easier by Linear Transformations in Perceptrons</p>
    <p>Tapani Raiko1, Harri Valpola1, Yann LeCun2</p>
    <p>Proposed Transformations  Consider an MLP-network with a shortcut mapping C</p>
    <p>yt = Af (Bxt) + Cxt + !t,</p>
    <p>Supplement nonlinearity with auxiliary variables !i and &quot;i</p>
    <p>fi(bixt) = tanh(bixt) + !ibixt + &quot;i</p>
    <p>Ensure that output is zero-mean and zero-slope on average</p>
    <p>T !</p>
    <p>t=1</p>
    <p>fi(bixt) = 0 T !</p>
    <p>t=1</p>
    <p>f!i(bixt) = 0</p>
    <p>by setting !i and &quot;i to</p>
    <p>!i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>tanh!(bixt) &quot;i = &quot; 1</p>
    <p>T</p>
    <p>T !</p>
    <p>t=1</p>
    <p>[tanh(bixt) + !ibixt]</p>
    <p>and compensating the change by updating the shortcut mapping C</p>
    <p>Motivation</p>
    <p>Transformations do not change the model, but the optimization</p>
    <p>Fisher information matrix is closer to a diagonal because it contains terms with fi() and f</p>
    <p>! i()</p>
    <p>Gij = !</p>
    <p>t</p>
    <p>&quot;</p>
    <p>#2 log p(yt | A, B, C, xt)</p>
    <p>#$i#$j</p>
    <p>#</p>
    <p>Traditional gradient is thus closer to a natural gradient and parameters are more independent</p>
    <p>Side e!ect: nonlinearity does not saturate # avoid plateaus</p>
    <p>!5 !4 !3 !2 !1 0 1 2 3 4 5 !2</p>
    <p>!1.5</p>
    <p>!1</p>
    <p>!0.5</p>
    <p>tanh(x)</p>
    <p>tanh(x)!0.5x</p>
    <p>MNIST Classification</p>
    <p>layer hidden</p>
    <p>layer hidden</p>
    <p>input output</p>
    <p>class</p>
    <p>Network structures Data, after PCA, noise added$2</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning rate 0 200 400 600 800</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Test errors after 15 minutes as regularization methods are included:</p>
    <p>regularization none weight decay PCA noise (150 minutes) original 1.87 1.85 1.62 1.15 1.03 shortcuts 2.02 1.77 1.59 1.23 1.17 transform. 1.63 1.56 1.56 1.10 1.02</p>
    <p>!0.9 !0.8 !0.7 !0.6 !0.5 0</p>
    <p>!0.2 !0.1 0 0.1 0.2 0</p>
    <p>!2 !1 0 1 2 !0.5</p>
    <p>Histograms of !i and &quot;i in the first hidden layer. Examples of fi().</p>
    <p>MNIST Autoencoder</p>
    <p>layers layer</p>
    <p>input hidden hidden</p>
    <p>layersbottleneck</p>
    <p>(hidden)</p>
    <p>output</p>
    <p>Network structures Data, reconstruction, linear rec.</p>
    <p>x 10 4</p>
    <p>original</p>
    <p>shortcuts</p>
    <p>transformations</p>
    <p>Error against learning time</p>
    <p>Results after 1000 minutes of CPU time Compare to Hessian-free optimization (Martens, ICML 2010)</p>
    <p>linear original shortcuts transf. Martens (2010) training error 8.11 2.37 2.11 1.94 1.75 test error 7.85 2.76 2.61 2.44 2.55</p>
    <p># of iterations 92k 49k 38k 37k ?</p>
    <p>Implementation Details</p>
    <p>Learning algorithm: Stochastic gradient with momentum</p>
    <p>Transformations done initially and after every 1000 iterations</p>
    <p>Soft-max for discrete outputs</p>
    <p>Normalized random initialization, shortcut weigths to zero</p>
    <p>Learning rate decreased linearly in the second half of learning time</p>
    <p>Regularization: PCA in classification, weight decay, added noise to inputs</p>
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
    <p>MNIST Autoencoder</p>
    <p>x-h1 x-h2 x-h3</p>
    <p>h3-yh4-yh5-y</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Simple transformations make basic gradient competitive with state-of-the-art</p>
    <p>Making parameters more independent will also help variational Bayes and MCMC</p>
    <p>Could be initialized with unsupervised pretraining for further improvement</p>
    <p>How about doing classification and autoencoder as a multitask?</p>
  </div>
</Presentation>
