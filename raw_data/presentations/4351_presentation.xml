<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sequence-to-sequence Models for Cache Transition Systems</p>
    <p>Xiaochang Peng1, Linfeng Song1, Daniel Gildea1 and Giorgio Satta2</p>
  </div>
  <div class="page">
    <p>AMR</p>
    <p>John wants to go</p>
    <p>want-01</p>
    <p>boy</p>
    <p>go-01</p>
    <p>ARG1</p>
    <p>ARG0</p>
    <p>ARG0</p>
  </div>
  <div class="page">
    <p>AMR and</p>
    <p>believe-01</p>
    <p>op1</p>
    <p>formulate-01</p>
    <p>op2</p>
    <p>after</p>
    <p>time</p>
    <p>person</p>
    <p>ARG0</p>
    <p>capable-41</p>
    <p>ARG1</p>
    <p>have-org-role-91</p>
    <p>ARG0-of</p>
    <p>company</p>
    <p>ARG1</p>
    <p>CEO</p>
    <p>ARG2</p>
    <p>name</p>
    <p>name</p>
    <p>country</p>
    <p>mod</p>
    <p>IM</p>
    <p>op1</p>
    <p>name</p>
    <p>name</p>
    <p>United States</p>
    <p>op1 op2</p>
    <p>person</p>
    <p>ARG1 innovate-01</p>
    <p>ARG2</p>
    <p>employ-01</p>
    <p>ARG1-of</p>
    <p>each</p>
    <p>mod</p>
    <p>ARG0</p>
    <p>ARG0</p>
    <p>ARG0</p>
    <p>countermeasure</p>
    <p>ARG1</p>
    <p>strategy</p>
    <p>mod</p>
    <p>innovate-01</p>
    <p>purpose</p>
    <p>industry</p>
    <p>prep-in</p>
    <p>invent-01</p>
    <p>op1</p>
    <p>company</p>
    <p>ARG0</p>
    <p>machine</p>
    <p>ARG1</p>
    <p>compete-01</p>
    <p>ARG0-of</p>
    <p>ARG1</p>
    <p>wash-01</p>
    <p>ARG0-of</p>
    <p>load-01</p>
    <p>ARG1-of</p>
    <p>front</p>
    <p>mod</p>
    <p>After its competitor invented the front loading washing machine, the CEO of the American IM company believed that each of its employees had the ability for innovation, and formulated strategic countermeasures for innovation in the industry.</p>
  </div>
  <div class="page">
    <p>Transition-based AMR parsing</p>
    <p>There has been previous work (Sagae and Tsujii; Damonte et al.; Zhou et al.; Ribeyre et al.; Wang et al.) on transition-based graph parsing.</p>
    <p>Our work introduces a new data structure cache for generating graphs of certain treewidth.</p>
  </div>
  <div class="page">
    <p>Introduction to treewidth Gildea, Satta, and Peng</p>
    <p>I A B D F G J</p>
    <p>K L R M O E</p>
    <p>P</p>
    <p>C</p>
    <p>Q</p>
    <p>H</p>
    <p>S</p>
    <p>N</p>
    <p>(a)</p>
    <p>ALB LBR BRD RD M D MF MFO FOG</p>
    <p>K AL D MP OGE</p>
    <p>IK A MPH GE J</p>
    <p>PHC</p>
    <p>HCS CSQ SQN</p>
    <p>(b)</p>
    <p>Figure 2 (a) An optimal tree decomposition of graph G in Figure 1; this is a set of overlapping clusters of Gs vertices, arranged in a tree. (b) The high-level treelike structure of G becomes apparent when it is drawn ignoring Gs edges.</p>
    <p>An alternative representation of the same tree decomposition is shown in Figure 2(b), where we focus on the vertices and ignore the edges of the graph. It is easy to see that the vertex cover and the edge cover conditions in the definition of tree decomposition are both satisfied by T. As an example of the running intersection property, note that the vertex S appears in three adjacent nodes of the tree decomposition.</p>
    <p>While general tree decompositions are undirected trees, in this article we will work with rooted, directed tree decompositions, in which one node is designated as the root, and the children of each node are ordered. We say that a rooted, ordered tree decomposition of graph G having width k is smooth if each bag contains exactly k + 1 vertices, and each bag contains the same vertices as its parent bag, with exactly one vertex removed and one vertex added. The tree decomposition in Figure 2(b) is smooth.</p>
    <p>The concept of smooth tree decompositions, for standard unrooted tree decompositions, was introduced by Bodlaender (1996). Throughout this article, we also require that the root of a smooth tree decomposition contains k + 1 copies of the special symbol $, with vertices of G being added one at a time in the bags below the root. It is easy to see that the size of a smooth tree decomposition, i.e., the number of nodes of the tree, is the number of vertices in the graph plus one.</p>
    <p>Lemma 1 Any tree decomposition T of graph G can be transformed into a smooth tree decomposition T0 of G of equal width.</p>
    <p>Proof. Let k be the width of T. At each bag having fewer than k + 1 vertices, continue adding vertices from adjacent bags until all bags have the same size. If two adjacent bags B1 and B2 end up having the same vertices, collapse B1 and B2 into a single bag, and merge the children of the two bags in a way that preserves their order. If two adjacent bags B1 and B2 differ by more than one vertex in their contents, add intermediate bags</p>
    <p>Complete graph of N nodes: treewidth N-1</p>
    <p>Gildea, Satta, and Peng</p>
    <p>w</p>
    <p>j s</p>
    <p>m</p>
    <p>Figure 4 Graph for the semantic representation of the sentence John wants Mary to succeed. Vertex w represents word token wants, vertex j represents John, vertex s represents succeed, and vertex m represents Mary.</p>
    <p>r push(i, C) is parameterized by a position in the cache i 2 [m] and a set of positions in the cache C  [m] \ {i}. It takes a configuration:</p>
    <p>(s, [v1, . . . , vi1, vi, vi+1, . . . , vm], v|b, E)</p>
    <p>and moves to a configuration:</p>
    <p>(s|i|vi, [v1, . . . , vi1, vi+1, . . . , vm, v], b, E 0)</p>
    <p>E0 = E [ {(vk, v) | k 2 C}</p>
    <p>Here, we have shifted the next vertex v out of the buffer and moved it into the last position of the cache. We have also taken the vertex vi appearing in position i in the cache and pushed it onto the stack s, along with the integer i recording the position in the cache from which it came. Finally, we have added some edges to the graph being built, where the new edges connect the shifted vertex v with some subset of the other vertices in the cache. This subset is specified by the parameter C.r pop takes a configuration:</p>
    <p>(s|i|v, [v1, . . . , vm], b, E),</p>
    <p>and moves to a configuration:</p>
    <p>(s, [v1, . . . , vi1, v, vi, . . . , vm1], b, E)</p>
    <p>Here we have popped a vertex v from the stack, along with the integer i recording the position in the cache that it originally came from. We place v in position i in the cache shifting the remainder of the cache one position to the right, and discarding the last element in the cache.</p>
    <p>treewidth 2A tree: treewidth 1</p>
  </div>
  <div class="page">
    <p>Introduction to treewidth</p>
    <p>small tree width ~ 2.8 on average</p>
    <p>large tree width</p>
    <p>and</p>
    <p>believe-01</p>
    <p>op1</p>
    <p>formulate-01</p>
    <p>op2</p>
    <p>after</p>
    <p>time</p>
    <p>person</p>
    <p>ARG0</p>
    <p>capable-41</p>
    <p>ARG1</p>
    <p>have-org-role-91</p>
    <p>ARG0-of</p>
    <p>company</p>
    <p>ARG1</p>
    <p>CEO</p>
    <p>ARG2</p>
    <p>name</p>
    <p>name</p>
    <p>country</p>
    <p>mod</p>
    <p>IM</p>
    <p>op1</p>
    <p>name</p>
    <p>name</p>
    <p>United States</p>
    <p>op1 op2</p>
    <p>person</p>
    <p>ARG1 innovate-01</p>
    <p>ARG2</p>
    <p>employ-01</p>
    <p>ARG1-of</p>
    <p>each</p>
    <p>mod</p>
    <p>ARG0</p>
    <p>ARG0</p>
    <p>ARG0</p>
    <p>countermeasure</p>
    <p>ARG1</p>
    <p>strategy</p>
    <p>mod</p>
    <p>innovate-01</p>
    <p>purpose</p>
    <p>industry</p>
    <p>prep-in</p>
    <p>invent-01</p>
    <p>op1</p>
    <p>company</p>
    <p>ARG0</p>
    <p>machine</p>
    <p>ARG1</p>
    <p>compete-01</p>
    <p>ARG0-of</p>
    <p>ARG1</p>
    <p>wash-01</p>
    <p>ARG0-of</p>
    <p>load-01</p>
    <p>ARG1-of</p>
    <p>front</p>
    <p>mod</p>
  </div>
  <div class="page">
    <p>Tree decompositionGildea, Satta, and Peng</p>
    <p>I A B D F G J</p>
    <p>K L R M O E</p>
    <p>P</p>
    <p>C</p>
    <p>Q</p>
    <p>H</p>
    <p>S</p>
    <p>N</p>
    <p>(a)</p>
    <p>ALB LBR BRD RD M D MF MFO FOG</p>
    <p>K AL D MP OGE</p>
    <p>IK A MPH GE J</p>
    <p>PHC</p>
    <p>HCS CSQ SQN</p>
    <p>(b)</p>
    <p>Figure 2 (a) An optimal tree decomposition of graph G in Figure 1; this is a set of overlapping clusters of Gs vertices, arranged in a tree. (b) The high-level treelike structure of G becomes apparent when it is drawn ignoring Gs edges.</p>
    <p>An alternative representation of the same tree decomposition is shown in Figure 2(b), where we focus on the vertices and ignore the edges of the graph. It is easy to see that the vertex cover and the edge cover conditions in the definition of tree decomposition are both satisfied by T. As an example of the running intersection property, note that the vertex S appears in three adjacent nodes of the tree decomposition.</p>
    <p>While general tree decompositions are undirected trees, in this article we will work with rooted, directed tree decompositions, in which one node is designated as the root, and the children of each node are ordered. We say that a rooted, ordered tree decomposition of graph G having width k is smooth if each bag contains exactly k + 1 vertices, and each bag contains the same vertices as its parent bag, with exactly one vertex removed and one vertex added. The tree decomposition in Figure 2(b) is smooth.</p>
    <p>The concept of smooth tree decompositions, for standard unrooted tree decompositions, was introduced by Bodlaender (1996). Throughout this article, we also require that the root of a smooth tree decomposition contains k + 1 copies of the special symbol $, with vertices of G being added one at a time in the bags below the root. It is easy to see that the size of a smooth tree decomposition, i.e., the number of nodes of the tree, is the number of vertices in the graph plus one.</p>
    <p>Lemma 1 Any tree decomposition T of graph G can be transformed into a smooth tree decomposition T0 of G of equal width.</p>
    <p>Proof. Let k be the width of T. At each bag having fewer than k + 1 vertices, continue adding vertices from adjacent bags until all bags have the same size. If two adjacent bags B1 and B2 end up having the same vertices, collapse B1 and B2 into a single bag, and merge the children of the two bags in a way that preserves their order. If two adjacent bags B1 and B2 differ by more than one vertex in their contents, add intermediate bags</p>
    <p>Gildea, Satta, and Peng</p>
    <p>I A B D F G J</p>
    <p>K L R M O E</p>
    <p>P</p>
    <p>C</p>
    <p>Q</p>
    <p>H</p>
    <p>S</p>
    <p>N</p>
    <p>(a)</p>
    <p>ALB LBR BRD RD M D MF MFO FOG</p>
    <p>K AL D MP OGE</p>
    <p>IK A MPH GE J</p>
    <p>PHC</p>
    <p>HCS CSQ SQN</p>
    <p>(b)</p>
    <p>Figure 2 (a) An optimal tree decomposition of graph G in Figure 1; this is a set of overlapping clusters of Gs vertices, arranged in a tree. (b) The high-level treelike structure of G becomes apparent when it is drawn ignoring Gs edges.</p>
    <p>An alternative representation of the same tree decomposition is shown in Figure 2(b), where we focus on the vertices and ignore the edges of the graph. It is easy to see that the vertex cover and the edge cover conditions in the definition of tree decomposition are both satisfied by T. As an example of the running intersection property, note that the vertex S appears in three adjacent nodes of the tree decomposition.</p>
    <p>While general tree decompositions are undirected trees, in this article we will work with rooted, directed tree decompositions, in which one node is designated as the root, and the children of each node are ordered. We say that a rooted, ordered tree decomposition of graph G having width k is smooth if each bag contains exactly k + 1 vertices, and each bag contains the same vertices as its parent bag, with exactly one vertex removed and one vertex added. The tree decomposition in Figure 2(b) is smooth.</p>
    <p>The concept of smooth tree decompositions, for standard unrooted tree decompositions, was introduced by Bodlaender (1996). Throughout this article, we also require that the root of a smooth tree decomposition contains k + 1 copies of the special symbol $, with vertices of G being added one at a time in the bags below the root. It is easy to see that the size of a smooth tree decomposition, i.e., the number of nodes of the tree, is the number of vertices in the graph plus one.</p>
    <p>Lemma 1 Any tree decomposition T of graph G can be transformed into a smooth tree decomposition T0 of G of equal width.</p>
    <p>Proof. Let k be the width of T. At each bag having fewer than k + 1 vertices, continue adding vertices from adjacent bags until all bags have the same size. If two adjacent bags B1 and B2 end up having the same vertices, collapse B1 and B2 into a single bag, and merge the children of the two bags in a way that preserves their order. If two adjacent bags B1 and B2 differ by more than one vertex in their contents, add intermediate bags</p>
    <p>graph tree decomposition</p>
  </div>
  <div class="page">
    <p>Cache transition system</p>
    <p>Configuration c = ($,,',()  Stack $: place for temporarily storing concepts  Cache *: working zone for making edges,</p>
    <p>fixed size corresponding to the treewidth.  Buffer ': unprocessed concepts  E: set of already-built edges</p>
  </div>
  <div class="page">
    <p>Cache transition system  Actions</p>
    <p>SHIFT PUSH(i): shift one concept from buffer to rightmost position of cache, then select one concept (index i) from cache to stack.</p>
    <p>stack cache</p>
    <p>$ $ $</p>
    <p>buffer</p>
    <p>PER want-01 go-01</p>
    <p>stack</p>
    <p>($,1)</p>
    <p>cache</p>
    <p>$ $ PER</p>
    <p>buffer</p>
    <p>want-01 go-01</p>
    <p>SHIFT PUSH(1)</p>
  </div>
  <div class="page">
    <p>Cache transition system  Actions</p>
    <p>POP: pop the top from stack and put back to cache, then drop the right-most item from cache.</p>
    <p>stack</p>
    <p>($,1)</p>
    <p>cache</p>
    <p>$ $ PER</p>
    <p>buffer</p>
    <p>want-01 go-01</p>
    <p>stack cache</p>
    <p>$ $ $</p>
    <p>buffer</p>
    <p>want-01 go-01</p>
  </div>
  <div class="page">
    <p>Cache transition system  Actions</p>
    <p>Arc(i, l, d): make an arc (with direction d, label l) between the right-most node to node i. Arc(i,-,-) represents no edge between them.</p>
    <p>stack</p>
    <p>($,1), ($,1)</p>
    <p>cache</p>
    <p>$ PER want-01</p>
    <p>buffer</p>
    <p>go-01</p>
    <p>stack cache</p>
    <p>$ PER want-01</p>
    <p>buffer</p>
    <p>go-01</p>
    <p>Arc(1,-,-), Arc(2,L,ARG0)</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>$$</p>
    <p>stack cache buffer</p>
    <p>PER want-01 go-01$</p>
    <p>Action taken: Initialization</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>PER$</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01</p>
    <p>Action taken: SHIFT, PUSH(1)</p>
    <p>(1, $)</p>
    <p>PER</p>
    <p>$</p>
    <p>Hypothesis:</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>PER$</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01</p>
    <p>Action taken: SHIFT, PUSH(1)</p>
    <p>(1, $)</p>
    <p>PER</p>
    <p>$</p>
    <p>Hypothesis:</p>
    <p>Action taken: Arc(1, -, -), Arc(2, -, -)</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>PER</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01</p>
    <p>Action taken: SHIFT, PUSH(1)</p>
    <p>(1, $) (1, $)</p>
    <p>PER want-01</p>
    <p>$</p>
    <p>Hypothesis:</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>PER</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01</p>
    <p>Action taken: Arc(1, -, -), Arc(2, L, ARG0)</p>
    <p>(1, $) (1, $)</p>
    <p>PER want-01</p>
    <p>$</p>
    <p>ARG0</p>
    <p>Hypothesis:</p>
    <p>ARG0</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>PER</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01</p>
    <p>Action taken: SHIFT, PUSH(1)</p>
    <p>(1, $) (1, $) (1, $)</p>
    <p>PER want-01</p>
    <p>ARG0</p>
    <p>go-01Hypothesis:</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>Action taken: Arc(1, L, ARG0), Arc(2, R, ARG1)</p>
    <p>PER</p>
    <p>stack cache buffer</p>
    <p>want-01 go-01(1, $) (1, $) (1, $)</p>
    <p>PER want-01</p>
    <p>ARG0</p>
    <p>go-01</p>
    <p>ARG0</p>
    <p>ARG1 Hypothesis:</p>
    <p>ARG0 ARG1</p>
  </div>
  <div class="page">
    <p>Example of cache transition</p>
    <p>$$</p>
    <p>stack cache buffer</p>
    <p>$</p>
    <p>Action taken: POP POP POP</p>
    <p>PER want-01</p>
    <p>ARG0</p>
    <p>go-01</p>
    <p>ARG0</p>
    <p>ARG1 Hypothesis:</p>
  </div>
  <div class="page">
    <p>Sequence to sequence models for cache transition system</p>
    <p>Concepts are generated from input sentences by another classifier in the preprocessing step.</p>
    <p>Separate encoders are adopted for input sentences and sequences of concepts, respectively.</p>
    <p>One decoder for generating transition actions.</p>
  </div>
  <div class="page">
    <p>Seq2seq (soft-attention+features)</p>
    <p>John wants to go Per want-01 go-01</p>
    <p>... ...</p>
    <p>Input sequence Concept sequence</p>
    <p>SHIFT PushIndex(1)</p>
    <p>SHIFT</p>
  </div>
  <div class="page">
    <p>Seq2seq (hard-attention+features)</p>
    <p>John wants to go Per want-01 go-01</p>
    <p>...</p>
    <p>Input sequence Concept sequence</p>
    <p>ARC L-ARG0</p>
    <p>...</p>
    <p>NOARC SHIFT PushIndex(1)</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Dataset: LDC2015E86  16,833(train)/1,368(dev)/1,371(test)</p>
    <p>Evaluation: Smatch (Cai et al., 2013)</p>
  </div>
  <div class="page">
    <p>AMR Coverage with different cache sizes</p>
    <p>Computational Linguistics Volume xx, Number xx</p>
    <p>want-01</p>
    <p>person</p>
    <p>like-01 ARG0</p>
    <p>ARG1</p>
    <p>ARG1</p>
    <p>name</p>
    <p>John</p>
    <p>name</p>
    <p>op1</p>
    <p>person</p>
    <p>name</p>
    <p>Mary</p>
    <p>name</p>
    <p>op1</p>
    <p>ARG0</p>
    <p>Figure 8 An example AMR graph for the sentence: John wants Mary to like him.</p>
    <p>Figure 9 The distribution of AMR relative treewidth.</p>
    <p>on graph datasets for these representations, with the aim to assess the coverage that our cache parser provides with different cache sizes.</p>
    <p>We first evaluate our algorithm on Abstract Meaning Representation (AMR) (Banarescu et al. 2013). AMR is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 8 shows an example of an AMR graph in which the nodes represent the AMR concepts and the edges represent the relations between the concepts they connect. AMR concepts consist of predicate senses, named entity annotations, and in some cases, simply lemmas of English words. AMR relations consist of core semantic roles drawn from the Propbank (Palmer, Gildea, and Kingsbury 2005) as well as very fine-grained semantic relations defined specifically for AMR. We use the training set of LDC2015E86 for SemEval 2016 task 8 on meaning representation parsing (May 2016), which contains 16,833 sentences. This dataset covers various domains including newswire and web discussion forums.</p>
    <p>For each graph, we derive a vertex order corresponding to the English word order by using the automatically generated alignments provided with the dataset, which align</p>
  </div>
  <div class="page">
    <p>Development results</p>
    <p>Model P R F Soft 0.55 0.51 0.53 Soft+feats 0.69 0.63 0.66 Hard+feats 0.70 0.64 0.67</p>
    <p>cache size P R F 4 0.69 0.63 0.66 5 0.70 0.64 0.67 6 0.69 0.64 0.66</p>
    <p>Impact of various components Impact of cache size</p>
  </div>
  <div class="page">
    <p>Main results Model P R F Buys and Blunsom (2017) -- -- 0.60 Konstas et al. (2017) 0.60 0.65 0.62 Ballesteros and Al-Onaizan (2017) -- -- 0.64 Damonte et al. (2016) -- -- 0.64 Wang et al. (2015a) 0.70 0.63 0.66 Flanigan et al. (2016) 0.70 0.65 0.67 Wang and Xue (2017) 0.72 0.65 0.68 Lyu and Titov (2018) -- -- 0.74 Soft+feats 0.68 0.63 0.65 Hard+feats 0.69 0.64 0.66</p>
  </div>
  <div class="page">
    <p>Accuracy on reentrancies</p>
    <p>Model P R F Peng et al., (2018) 0.44 0.28 0.34 Damonte et al., (2017) -- -- 0.41 JAMR 0.47 0.38 0.42 Hard+feats (ours) 0.58 0.34 0.43</p>
  </div>
  <div class="page">
    <p>Reentrancy example</p>
    <p>i - desire-01 live-01 any city</p>
    <p>ARG0 ARG0</p>
    <p>polarity ARG1</p>
    <p>location</p>
    <p>$ $ i - desire-01</p>
    <p>polarity</p>
    <p>ARG0</p>
    <p>$ i - desire-01 live-01</p>
    <p>ARG1</p>
    <p>ARG0</p>
    <p>Our hard attention output:</p>
    <p>Sentence: I have no desire to live in any city .</p>
    <p>Cache arc decisions creating the reentrancy</p>
    <p>(cache size of 5):</p>
    <p>JAMR output:</p>
    <p>Peng et al. (2018) output:</p>
    <p>mod</p>
    <p>i - desire-01 live-01 any city</p>
    <p>polarity ARG1</p>
    <p>location mod</p>
    <p>i - desire-01 live-01 any city</p>
    <p>ARG0 polarity</p>
    <p>ARG1</p>
    <p>location mod</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Cache transition system based on a mathematical sound formalism for parsing to graphs.</p>
    <p>The cache transition process can be well-modeled by sequence-to-sequence models.  Features from transition states.  Monotonic hard attention.</p>
  </div>
  <div class="page">
    <p>Thank you for listening! Questions</p>
  </div>
</Presentation>
