<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Extraction of information in large graphs Automatic search for synonyms</p>
    <p>Pierre Senellart, under the direction of Prof. Vincent Blondel</p>
    <p>June 5th 2001 - August 3rd 2001</p>
  </div>
  <div class="page">
    <p>The dictionary graph</p>
    <p>Computation (n.) The act or process of computing; calculation; reckoning.</p>
    <p>Computation (n.) The result of computation; the amount computed.</p>
    <p>Computed (imp. &amp; p. p.) of Compute</p>
    <p>Computing (p. pr. &amp; vb. n.) of Compute</p>
    <p>Compute (v.t.) To determine calculation; to reckon; to count.</p>
    <p>Compute (n.) Computation.</p>
    <p>Computer (n.) One who computes.</p>
  </div>
  <div class="page">
    <p>ComputerCompute</p>
    <p>Computing</p>
    <p>Computation</p>
    <p>Computed</p>
    <p>Rest of the graph</p>
  </div>
  <div class="page">
    <p>Extraction of the graph</p>
    <p>Multiwords (e.g. All Saints, Surinam toad)</p>
    <p>Prefixes and suffixes (e.g un-, -ous)</p>
    <p>Different meanings of a word</p>
    <p>Derived forms (e.g. daisies, sought)</p>
    <p>Accentuated characters (e.g. proven/al, cr/che)</p>
    <p>Misspelled words</p>
  </div>
  <div class="page">
    <p>Lexical units</p>
    <p>Numbers (e.g. 14159265, 14th)</p>
    <p>Mathematical and chemical symbols (e.g. x3, fe3o4)</p>
    <p>Proper nouns (e.g. California, Aaron)</p>
    <p>Misspelled words (e.g. aligator, abudance)</p>
    <p>Undefined words (e.g. snakelike, unwound)</p>
    <p>Abbreviations (e.g. adj, etc)</p>
  </div>
  <div class="page">
    <p>Connectivity</p>
    <p>1 111, 982-vertex component</p>
    <p>3 2-vertex components</p>
    <p>anguinealanguineoussnakelike indissolvablenessindissolublenessindissolubility</p>
    <p>181 1-vertex components</p>
  </div>
  <div class="page">
    <p>Strong connectivity</p>
    <p>Number of vertices Number of components 30, 595 1</p>
    <p>Distribution of the size of strongly connected components</p>
  </div>
  <div class="page">
    <p>feminality</p>
    <p>Largest SCC (30,595 words)</p>
    <p>Graph resulting of the contraction of each SCC in one single vertex</p>
  </div>
  <div class="page">
    <p>A core of the language</p>
    <p>Definition. A core subgraph of a graph G is a subgraph of G with the two following properties:</p>
    <p>If you know the meaning of all the words in a core subgraph of the dictionary graph, you may learn the meaning of all words in the dictionary.</p>
    <p>The largest SCC (plus the other connected components and 12 words) is a core subgraph of the dictionary.</p>
  </div>
  <div class="page">
    <p>The independence degree</p>
    <p>Definition. The indepedence degree of a graph G is the minimum number of vertices of a core subgraph of G.</p>
    <p>Theorem. The computation of the independence degree of a graph is a NP  complete problem.</p>
    <p>Upper bound : 30, 905 + 187 + 12 = 30, 794.</p>
    <p>Good approximation algorithm?</p>
  </div>
  <div class="page">
    <p>A small world</p>
    <p>Definition. A graph is a small world graph if it has the following properties:</p>
  </div>
  <div class="page">
    <p>The Web, power distribution graphs, the Kevin Bacon graph are wellknown examples of small worlds.</p>
    <p>The underlying undirected graph of the largest connected component of the dictionary graph is a small world:</p>
    <p>Yet it does not fit the models of small worlds graphs proposed by Duncan J. Watts.</p>
    <p>Necessity of a model of directed small worlds?</p>
  </div>
  <div class="page">
    <p>Degree distributions</p>
    <p>A Zipfian dustribution: the probability that a node has indegree or outdegree i is proportional to 1/i for some .</p>
    <p>Indegree :   1.6. Outdegree :   3.1</p>
    <p>Concerning the outdegree distribution:</p>
    <p>Same kind of distributions as for the Web.</p>
  </div>
  <div class="page">
    <p>N u</p>
    <p>m b</p>
    <p>e r</p>
    <p>o f</p>
    <p>ve rt</p>
    <p>ic e</p>
    <p>s</p>
    <p>Indegree</p>
    <p>Indegree distribution</p>
    <p>Indegree distribution</p>
  </div>
  <div class="page">
    <p>N u</p>
    <p>m b</p>
    <p>e r</p>
    <p>o f</p>
    <p>ve rt</p>
    <p>ic e</p>
    <p>s</p>
    <p>Outdegree</p>
    <p>Outdegree distribution</p>
    <p>Outdegree distribution</p>
  </div>
  <div class="page">
    <p>Looking for near-synonyms</p>
    <p>Definition. The neighborhood graph of a node i in a directed graph G is the subgraph consisting of i, all parents of i and all children of i.</p>
    <p>i is some word we want a synonym of.</p>
    <p>A will be the adjacency matrix of the neighborhood graph of i in the dictionary graph.</p>
    <p>n is the order of A.</p>
  </div>
  <div class="page">
    <p>The vectors method</p>
    <p>For each 1  j  n,j 6= i, compute:</p>
    <p>(Ai,Aj,) + (A,i A,j)T</p>
    <p>(where   is some vector norm, Ai, and A,i are respectively the ith line and the ith column of A).</p>
    <p>For instance, if we choose the Euclidean norm, we compute:</p>
    <p>( n k=1</p>
    <p>(Ai,k Aj,k)2 )1</p>
    <p>+</p>
    <p>( n k=1</p>
    <p>(Ak,i Ak,j)2 )1</p>
    <p>The lower this value is, the best j is a synonym of i.</p>
  </div>
  <div class="page">
    <p>Kleinbergs algorithm</p>
    <p>Hub  Authority</p>
    <p>A mutually reinforcing relationship: good hubs are pages that point to good authorities and good authorities are pages pointed by good hubs.</p>
    <p>The principal eigenvectors of ATA and AAT give respectively the authority weights and hub weights of the vertices of the graph.</p>
  </div>
  <div class="page">
    <p>An extension of Kleinbergs algorithm</p>
    <p>Let M(m,m) and N(n,n) be the transition matrices of two oriented graphs.</p>
    <p>Let C = MN +MTNT where  is the Kronecker tensorial product.</p>
    <p>We assume that the greatest eigenvalue of C is strictly greater than the absolute value of all other eigenvalues.</p>
    <p>Then, the normalized principal eigenvector X of C gives the similarity between a vertex of M and a vertex of N: Xin+j characterizes the similarity between vertex i of M and vertex j of N.</p>
    <p>In particular, if M = (</p>
    <p>) , the result is that of Kleinbergs</p>
    <p>algorithm.</p>
  </div>
  <div class="page">
    <p>Application to the search for synonyms</p>
    <p>We are looking for vertices like 2 in the neighborhood graph of i.</p>
    <p>Let C = M A + MT AT where M =</p>
    <p>0 1 00 0 1</p>
    <p>.</p>
    <p>The principal eigenvector of C gives the similarity between a node in G and a node in the graph 1  2  3.</p>
    <p>We just select the subvector corresponding to the vertex 2 in order to have synonymy weights.</p>
  </div>
  <div class="page">
    <p>ArcRank</p>
    <p>PageRank (Google): stationary distribution of weights over vertices corresponding to the principal eigenvector of the adjacency matrix.</p>
    <p>ArcRank:</p>
    <p>rs,t = ps/|as| pt</p>
    <p>|as| is the outdegree of s.</p>
    <p>pt is the pagerank of t.</p>
    <p>The best synonyms of i are the other extremity of the best-ranked arcs arriving to or leaving from i.</p>
  </div>
  <div class="page">
    <p>Disappear</p>
    <p>Vectors Kleinberg ArcRank Wordnet Microsoft Word</p>
    <p>Table 1: Near-synonyms for disappear</p>
  </div>
  <div class="page">
    <p>Parallelogram</p>
    <p>Vectors Kleinberg ArcRank Wordnet Microsoft Word</p>
    <p>Table 2: Near-synonyms for parallelogram</p>
  </div>
  <div class="page">
    <p>Sugar</p>
    <p>Vectors Kleinberg ArcRank Wordnet Microsoft Word</p>
    <p>Table 3: Near-synonyms for sugar</p>
  </div>
  <div class="page">
    <p>Science</p>
    <p>Vectors Kleinberg ArcRank Wordnet Microsoft Word</p>
    <p>Table 4: Near-synonyms for science</p>
  </div>
  <div class="page">
    <p>Perspectives</p>
    <p>Extension of the subgraph</p>
    <p>Other dictionaries, other languages</p>
    <p>Other applications of the extension of Kleinbergs algorithm</p>
    <p>A model of small world directed graphs</p>
    <p>Invariants for languages</p>
  </div>
</Presentation>
