<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Detecting Feature Eligibility Illusions in Enterprise AI Autopilots</p>
    <p>opML 2020</p>
    <p>Fabio Casati, Veeru Metha, Gopal Sarda, Sagar Davasam, Kannan Govindarajan</p>
    <p>ServiceNow, Inc</p>
  </div>
  <div class="page">
    <p>Take-home message</p>
    <p>AI is not a buyers market Provocation: stop focusing on accuracy</p>
    <p>We need AI to support AI adoption  a greatly under researched topic</p>
    <p>Several reasonable problem formulations are possible  we can make measurable progress</p>
  </div>
  <div class="page">
    <p>McKinsey reports on low adoption  Why dont we have self driving enterprises  Why so few processes are supported by AI today</p>
    <p>From: AI adoption advances, but foundational barriers remain McKinsey</p>
  </div>
  <div class="page">
    <p>AI is not a buyers market</p>
    <p>Performance, accuracy, scalability are foundational issues</p>
    <p>But if you get to that stage, you have already walked a long road</p>
    <p>From: AI adoption advances, but foundational barriers remain McKinsey</p>
  </div>
  <div class="page">
    <p>Why AI Adoption is Hard  Try to imagine how it happens as a champion wants to sell AI to their Management</p>
    <p>Which problem should we solve? How?  Do we have the right data to get us where we want?  And, where actually do we want to go?  Knowing where and how to integrate it into a process requires lots of process knowledge, not (just) AI, and</p>
    <p>there are dozens of way in which this can be done  We can promise a (probabilistic) improvement over the status quo, but to do this we need to know our</p>
    <p>status quo for the aspect we plan to use AI for</p>
    <p>Models sometimes gets it right, sometimes gets it wrong  Can even give different answers to the same question if you ask it twice  Hard to test and replicate the same behavior in dev, test and production environments  Incidentally, this also means that calibration is as important as accuracy</p>
    <p>How to maintain AI: do we retrain, how do we test the model after retraining, do we do so in production,..</p>
    <p>And what about risks: are we introducing biases, (un)fairness,</p>
  </div>
  <div class="page">
    <p>Not your typical AutoML: if you worry about autoML, you are well ahead in your journey</p>
    <p>What can I learn</p>
    <p>What should I Learn</p>
    <p>Core AutoML</p>
    <p>Manual Training</p>
    <p>Start the dev/test/prod cycle</p>
    <p>Autopilots  or MetaAI: Learn what you can and should want to learn</p>
    <p>Understanding baselines, and what do I need in terms of accuracy, calibration</p>
    <p>Focus of MetaAI</p>
  </div>
  <div class="page">
    <p>Scoping the Problem</p>
    <p>Data (table) + Process, possibly Metrics</p>
    <p>What can we learn What should we learn Which are our baselines and targets</p>
  </div>
  <div class="page">
    <p>Scoping the Problem</p>
    <p>Data (table) + Process, possibly Metrics</p>
    <p>What can we learn What should we learn Which are our baselines and targets</p>
  </div>
  <div class="page">
    <p>Hardcoded exclusions Type</p>
    <p>Low Density</p>
    <p>Skew/ variability</p>
    <p>Inverse Causality Correlation</p>
    <p>Density+skew at inference time</p>
    <p>Rule-managed fields</p>
    <p>Snapshot-based analysis:</p>
    <p>History-based analysis:</p>
    <p>Correlation</p>
    <p>Eligibility as a relevant subproblem to reduce the problem space</p>
    <p>MetaAI: Learn what you can and should want to learn And how well should you learn it so that it is useful</p>
  </div>
  <div class="page">
    <p>Illusions</p>
    <p>Believing that we can train a model to predict Y based on X  and training it, and testing it successfully  when this is actually not the case</p>
    <p>Inverse causality  Observability  End state bias</p>
    <p>A lot of them have to do with a mismatch between the current state (value) of fields in records in the table and the values of the same fields when we make inferences</p>
  </div>
  <div class="page">
    <p>Inverse Causality</p>
  </div>
  <div class="page">
    <p>Observability Illusion</p>
  </div>
  <div class="page">
    <p>End state bias</p>
    <p>Distribution of values for a field may be different throughout the lifecycle</p>
  </div>
  <div class="page">
    <p>Uncovering Illusions (I): Logs</p>
    <p>This is the easy case: 1. observability can be estimated by looking at the field density at start, 2. causal inference can be heuristically identified by looking at which field in a pair (f1,f2) gets filled first, and 3. end state bias is determined based on percentage of cases in which a field changes from first time it is</p>
    <p>filled to end</p>
  </div>
  <div class="page">
    <p>Uncovering Illusions (II): Taking Snapshots</p>
    <p>Take snapshots of a movie Need to know where you are in the movie May want to know how long to wait  how fast the movie progresses</p>
    <p>both Poisson with rates  and  per time unit, then we can expect  new cases per time unit</p>
    <p>Many subtleties that make this hard in practice, with workarounds =&gt; see paper</p>
  </div>
  <div class="page">
    <p>Uncovering Illusions (II): Taking Snapshots</p>
  </div>
  <div class="page">
    <p>Take-home message</p>
    <p>AI is not a buyers market Provocation: stop focusing on accuracy</p>
    <p>We need AI to support AI adoption  a greatly under researched topic</p>
    <p>Several reasonable problem formulations are possible  we can make measurable progress</p>
  </div>
  <div class="page">
    <p>Thanks Send questions or feedback to: fabio.casati@servicenow.com</p>
  </div>
</Presentation>
