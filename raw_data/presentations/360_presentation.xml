<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ChunkStash: Speeding Up Storage Deduplication using Flash Memory</p>
    <p>Biplob Debnath+, Sudipta Sengupta*, Jin Li*</p>
    <p>*Microsoft Research, Redmond (USA) +Univ. of Minnesota, Twin Cities (USA)</p>
  </div>
  <div class="page">
    <p>Deduplication of Storage</p>
    <p>Detect and remove duplicate data in storage systems  e.g., Across multiple full backups</p>
    <p>Storage space savings</p>
    <p>Faster backup completion: Disk I/O and Network bandwidth savings</p>
    <p>Feature offering in many storage systems products  Data Domain, EMC, NetApp</p>
    <p>Backups need to complete over windows of few hours  Throughput (MB/sec) important performance metric</p>
    <p>High-level techniques  Content based chunking, detect/store unique chunks only</p>
    <p>Object/File level, Differential encoding</p>
  </div>
  <div class="page">
    <p>Impact of Dedup Savings Across Full Backups</p>
    <p>Source: Data Domain white paper</p>
  </div>
  <div class="page">
    <p>Deduplication of Storage</p>
    <p>Detect and remove duplicate data in storage systems  e.g., Across full backups</p>
    <p>Storage space savings</p>
    <p>Faster backup completion: Disk I/O and Network bandwidth savings</p>
    <p>Feature offering in many storage systems products  Data Domain, EMC, NetApp</p>
    <p>Backups need to complete over windows of few hours  Throughput (MB/sec) important performance metric</p>
    <p>High-level techniques  Content based chunking, detect/store unique chunks only</p>
    <p>Object/File level, Differential encoding</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
    <p>Declare a chunk boundary</p>
    <p>If Hash matches a particular pattern,</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
    <p>Declare a chunk boundary</p>
    <p>If Hash matches a particular pattern,</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
    <p>Declare a chunk boundary</p>
    <p>If Hash matches a particular pattern,</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
    <p>Declare a chunk boundary</p>
    <p>If Hash matches a particular pattern,</p>
  </div>
  <div class="page">
    <p>Content based Chunking</p>
    <p>Calculate Rabin fingerprint hash for each sliding window (16 byte)</p>
    <p>-4 -2 0 2 4</p>
    <p>Hash</p>
    <p>Declare a chunk boundary</p>
    <p>If Hash matches a particular pattern,</p>
  </div>
  <div class="page">
    <p>How to Obtain Chunk Boundaries?</p>
    <p>Content dependent chunking  When last n bits of Rabin hash = 0, declare chunk boundary</p>
    <p>Average chunk size = 2n bytes</p>
    <p>When data changes over time, new chunks correspond to new data regions only</p>
    <p>Compare with fixed size chunks (e.g., disk blocks)  Even unchanged data could be detected as new because of</p>
    <p>shifting</p>
    <p>How are chunks compared for equality?  20-byte SHA-1 hash (or, 32-byte SHA-256)</p>
    <p>Probability of collisions is less than that of hardware error by many orders of magnitude</p>
  </div>
  <div class="page">
    <p>Container Store and Chunk Parameters</p>
    <p>Chunks are written to disk in groups of containers</p>
    <p>Each container contains 1023 chunks</p>
    <p>New chunks added into currently open container, which is sealed when full</p>
    <p>Average chunk size = 8KB, Typical chunk compression ratio of 2:1</p>
    <p>Average container size  4MB</p>
    <p>Slide 19</p>
    <p>Chunk A</p>
    <p>. . .</p>
    <p>Chunk B Chunk A</p>
    <p>. . .</p>
    <p>Chunk B Chunk X</p>
    <p>. . . Chunk Y</p>
    <p>Data Container</p>
    <p>Container Store</p>
  </div>
  <div class="page">
    <p>Index for Detecting Duplicate Chunks</p>
    <p>Chunk hash index for identifying duplicate chunks  Key = 20-byte SHA-1 hash (or, 32-byte SHA-256)  Value = chunk metadata, e.g., length, location on disk  Key + Value  64 bytes</p>
    <p>Essential Operations  Lookup (Get)  Insert (Set)</p>
    <p>Need a high performance indexing scheme  Chunk metadata too big to fit in RAM  Disk IOPS is a bottleneck for disk-based index  Duplicate chunk detection bottlenecked by hard disk seek</p>
    <p>times (~10 msec)</p>
  </div>
  <div class="page">
    <p>Disk Bottleneck for Identifying Duplicate Chunks</p>
    <p>20 TB of unique data, average 8 KB chunk size</p>
    <p>160 GB of storage for full index (2.5  109 unique chunks @64 bytes per chunk metadata)</p>
    <p>Not cost effective to keep all of this huge index in RAM</p>
    <p>Backup throughput limited by disk seek times for index lookups</p>
    <p>10ms seek time =&gt; 100 chunk lookups per second =&gt; 800 KB/sec backup throughput</p>
    <p>No locality in the key space for chunk hash lookups</p>
    <p>Prefetching into RAM index mappings for entire container to exploit sequential predictability of lookups during 2nd</p>
    <p>and subsequent full backups (Zhu et al., FAST 2008)</p>
    <p>. . .</p>
    <p>Container</p>
  </div>
  <div class="page">
    <p>Storage Deduplication Process Schematic</p>
    <p>Chunk Index on HDDChunk Index on Flash HDD</p>
    <p>HDD</p>
    <p>(Chunks in currently open container)</p>
    <p>(RAM)</p>
    <p>(RAM)</p>
    <p>Chunk</p>
  </div>
  <div class="page">
    <p>Speedup Potential of a Flash based Index</p>
    <p>RAM hit ratio of 99% (using chunk metadata prefetching techniques)</p>
    <p>Average lookup time with on-disk index</p>
    <p>Average lookup time with on-flash index</p>
    <p>Potential of up to 50x speedup with index lookups served from flash</p>
  </div>
  <div class="page">
    <p>ChunkStash: Chunk Metadata Store on Flash</p>
    <p>Flash aware data structures and algorithms</p>
    <p>Random writes, in-place updates are expensive on flash memory</p>
    <p>Sequential writes, Random/Sequential reads great!</p>
    <p>Use flash in a log-structured manner</p>
    <p>Low RAM footprint</p>
    <p>Order of few bytes in RAM for each key-value pair stored on flash</p>
    <p>FusionIO 160GB ioDrive</p>
  </div>
  <div class="page">
    <p>ChunkStash Architecture</p>
    <p>Slide 25</p>
    <p>Chunk metadata organized on flash in logstructured manner in groups of 1023 chunks =&gt; 64 KB logical page (@64-byte metadata/ chunk)</p>
    <p>Chunk metadata indexed in RAM using a specialized space efficient hash table</p>
    <p>RAM write buffer for chunk mappings in currently open container</p>
    <p>Prefetch cache for chunk metadata in RAM for sequential predictability of chunk lookups</p>
  </div>
  <div class="page">
    <p>Low RAM Usage: Cuckoo Hashing</p>
    <p>High hash table load factors while keeping lookup times fast</p>
    <p>Collisions resolved using cuckoo hashing</p>
    <p>Key can be in one of K candidate positions</p>
    <p>Later inserted keys can relocate earlier keys to their other candidate positions</p>
    <p>K candidate positions for key x obtained using K hash functions h1(x), , hK(x)</p>
    <p>In practice, two hash functions can simulate K hash functions using hi(x) = g1(x) + i*g2(x)</p>
    <p>System uses value of K=16 and targets 90% hash table load factor</p>
    <p>Insert X</p>
  </div>
  <div class="page">
    <p>Low RAM Usage: Compact Key Signatures</p>
    <p>Compact key signatures stored in hash table</p>
    <p>2-byte key signature (vs. 20-byte SHA-1 hash)</p>
    <p>Key x stored at its candidate position i derives its signature from hi(x)</p>
    <p>False flash read probability &lt; 0.01%</p>
    <p>Total 6-10 bytes per entry (4-8 byte flash pointer)</p>
    <p>Related work on key-value stores on flash media</p>
    <p>MicroHash, FlashDB, FAWN, BufferHash Slide 27</p>
  </div>
  <div class="page">
    <p>RAM and Flash Capacity Considerations</p>
    <p>Whether RAM of flash size becomes bottleneck for store capacity depends on key-value size</p>
    <p>At 64 bytes per key-value pair, RAM is the bottleneck</p>
    <p>Example 4GB of RAM</p>
    <p>716 million key-value pairs (chunks) @6 bytes of RAM per entry</p>
    <p>At 8KB average chunk size, this corresponds to 6TB of deduplicated data</p>
    <p>At 64 bytes of metadata per chunk on flash, this uses 45GB of flash</p>
    <p>Larger chunk sizes =&gt; larger datasets for same amount of RAM and flash (but may tradeoff with dedup quality)</p>
    <p>Slide 28</p>
  </div>
  <div class="page">
    <p>Further Reducing RAM Usage in ChunkStash  Approach 1: Reduce the RAM requirements of the key</p>
    <p>value store (work in progress)</p>
    <p>Approach 2: Deduplication application specific</p>
    <p>Index in RAM only a small fraction of the chunks in each container (sample and index every i-th chunk)</p>
    <p>Flash still holds the metadata for all chunks in the system</p>
    <p>Prefetch chunk metadata into RAM as before</p>
    <p>Incur some loss in deduplication quality</p>
    <p>Fraction of chunks indexed is a powerful knob for tradeoff between RAM usage and dedup quality</p>
    <p>Index 10% chunks =&gt; 90% reduction in RAM usage =&gt; less than 1-byte of RAM usage per chunk metadata stored on flash</p>
    <p>And negligible loss in dedup quality!</p>
  </div>
  <div class="page">
    <p>Compare with Sparse Indexing Scheme</p>
    <p>Sparse indexing scheme (FAST 2009)</p>
    <p>Chop incoming stream into multi-MB segments, select chunk hooks in each segment using random sampling</p>
    <p>Use these hooks to find few segments seen in the recent past that share many chunks</p>
    <p>How does ChunkStash differ?</p>
    <p>Uniform interval sampling</p>
    <p>No concept of segment; all incoming chunks looked up in index</p>
    <p>Match incoming chunks with sampled chunks in all containers stored in the system, not just those see in recent past</p>
    <p>Slide 30</p>
  </div>
  <div class="page">
    <p>Performance Evaluation</p>
    <p>Comparison with disk index based system</p>
    <p>Disk based index (Zhu08-BDB-HDD)</p>
    <p>SSD replacement (Zhu08-BDB-SSD)</p>
    <p>SSD replacement + ChunkStash (ChunkStash-SSD)</p>
    <p>ChunkStash on hard disk (ChunkStash-HDD)</p>
    <p>Prefetching of chunk metadata in all systems</p>
    <p>Three datasets, 2 full backups for each</p>
    <p>BerkeleyDB used as the index on HDD/SSD</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Dataset 2</p>
    <p>Slide 32</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Dataset 3</p>
    <p>Slide 33</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Disk IOPS</p>
    <p>Slide 34</p>
  </div>
  <div class="page">
    <p>Indexing Chunk Samples in ChunkStash: Deduplication Quality</p>
    <p>Slide 35</p>
    <p>(1/64) (1/16) (1/8)</p>
  </div>
  <div class="page">
    <p>Indexing Chunk Samples in ChunkStash: Backup Throughput</p>
    <p>Slide 36</p>
  </div>
  <div class="page">
    <p>Flash Memory Cost Considerations</p>
    <p>Chunks occupy an average of 4KB on hard disk</p>
    <p>Store compressed chunks on hard disk</p>
    <p>Typical compression ratio of 2:1</p>
    <p>Flash storage is 1/64-th of hard disk storage</p>
    <p>64-byte metadata on flash per 4KB occupies space on hard disk</p>
    <p>Flash investment is about 16% of hard disk cost</p>
    <p>1/64-th additional storage @10x/GB cost = 16% additional cost</p>
    <p>Performance/dollar improvement of 22x</p>
    <p>25x performance at 1.16x cost</p>
    <p>Further cost reduction by amortizing flash across datasets</p>
    <p>Store chunk metadata on HDD and preload to flash Slide 37</p>
  </div>
  <div class="page">
    <p>Summary  Backup throughput in inline deduplication systems limited by</p>
    <p>chunk hash index lookups</p>
    <p>Flash-assisted storage deduplication system  Chunk metadata store on flash</p>
    <p>Flash aware data structures and algorithms</p>
    <p>Low RAM footprint</p>
    <p>Significant backup throughput improvements  7x-60x over over HDD index based system (BerkeleyDB)</p>
    <p>2x-4x over flash index based (but flash unaware) system (BerkeleyDB)</p>
    <p>Performance/dollar improvement of 22x (over HDD index)</p>
    <p>Reduce RAM usage further by 90-99%  Index small fraction of chunks in each container</p>
    <p>Negligible to marginal loss in deduplication quality</p>
  </div>
</Presentation>
