<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Less is More: Trading a li0le Bandwidth for Ultra-Low Latency in the Data Center</p>
    <p>Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat, and Masato Yasuda</p>
  </div>
  <div class="page">
    <p>Latency in Data Centers</p>
    <p>Latency is becoming a primary performance metric in DC</p>
    <p>Low latency applicaIons  High-frequency trading  High-performance compuIng  Large-scale web applicaIons  RAMClouds (want &lt; 10s RPCs)</p>
    <p>Desire predictable low-latency delivery of individual packets</p>
  </div>
  <div class="page">
    <p>Large-scale Web Application</p>
    <p>Why Does Latency Ma7er?</p>
    <p>Data Structures</p>
    <p>Traditional Application</p>
    <p>App. Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic</p>
    <p>App Logic Alice App</p>
    <p>Logic</p>
    <p>Who does she know? What has she done?</p>
    <p>Minnie Eric Pics Videos Apps</p>
    <p>Latency limits data access rate  Fundamentally limits applicaAons</p>
    <p>Possibly 1000s of RPCs per operaIon  Microseconds ma7er, even at the tail (e.g., 99.9th percenAle)</p>
  </div>
  <div class="page">
    <p>Reducing Latency</p>
    <p>SoXware and hardware are improving  Kernel bypass, RDMA; RAMCloud: soXware processing ~1s  Low latency switches forward packets in a few 100ns  Baseline fabric latency (propagaAon, switching) under 10s is achievable.</p>
    <p>Queuing delay: random and traffic dependent  Can easily reach 100s of microseconds or even milliseconds</p>
    <p>One 1500B packet = 12s @ 1Gbps</p>
    <p>Goal: Reduce queuing delays to zero. 4</p>
  </div>
  <div class="page">
    <p>Data Center Workloads:</p>
    <p>Short messages [100B-10KB]</p>
    <p>Large flows [1MB-100MB]</p>
    <p>Low Latency</p>
    <p>High Throughput</p>
    <p>Low Latency AND High Throughput</p>
    <p>We want baseline fabric latency AND high throughput.</p>
  </div>
  <div class="page">
    <p>Why do we need buffers?  Main reason: to create slack  Handle temporary oversubscripIon  Absorb TCPs rate fluctuaIons as it discovers path bandwidth</p>
    <p>Example: Bandwidth-delay product rule of thumb  A single TCP flow needs CRTT buffers for 100% Throughput.</p>
    <p>Th ro ug hp</p>
    <p>ut</p>
    <p>Bu ff er S iz e</p>
    <p>B B  CRTT</p>
    <p>B</p>
    <p>B &lt; CRTT</p>
  </div>
  <div class="page">
    <p>Overview of our Approach</p>
    <p>Use phantom queues  Signal congesIon before any queuing occurs</p>
    <p>Use DCTCP [SIGCOMM10]  MiIgate throughput loss that can occur without buffers</p>
    <p>Use hardware pacers  Combat bursIness due to offload mechanisms like LSO and Interrupt coalescing</p>
    <p>Main Idea</p>
  </div>
  <div class="page">
    <p>Source:  React in proporIon to the extent of congesIon  less fluctuaIons</p>
    <p>Reduce window size based on fracAon of marked packets.</p>
    <p>ECN Marks TCP DCTCP</p>
    <p>Review: DCTCP</p>
    <p>Switch:  Set ECN Mark when Queue Length &gt; K.</p>
    <p>B K Mark Dont Mark</p>
  </div>
  <div class="page">
    <p>Setup: Win 7, Broadcom 1Gbps Switch Scenario: 2 long-lived flows,</p>
    <p>(K by te s)</p>
    <p>ECN Marking Thresh = 30KB</p>
    <p>DCTCP vs TCP</p>
    <p>From Alizadeh et al [SIGCOMM10]</p>
  </div>
  <div class="page">
    <p>TCP: ~110ms</p>
    <p>DCTCP: ~100s</p>
    <p>~Zero Latency</p>
    <p>How do we get this?</p>
    <p>Achieving Zero Queuing Delay</p>
    <p>C Incoming Traffic</p>
    <p>TCP</p>
    <p>Incoming Traffic</p>
    <p>DCTCP K C</p>
  </div>
  <div class="page">
    <p>Phantom Queue</p>
    <p>Link Speed C</p>
    <p>Switch</p>
    <p>Bump on Wire (NetFPGA implementaAon)</p>
    <p>Key idea:  Associate congesIon with link uIlizaIon, not buffer occupancy  Virtual Queue (Gibbens &amp; Kelly 1999, Kunniyur &amp; Srikant 2001)</p>
    <p>Marking Thresh.</p>
    <p>C  &lt; 1: Creates bandwidth headroom</p>
  </div>
  <div class="page">
    <p>Throughput Switch latency (mean)</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t [</p>
    <p>M bp</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>M ea</p>
    <p>n S</p>
    <p>w itc</p>
    <p>h La</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>Throughput &amp; Latency vs. PQ Drain Rate</p>
  </div>
  <div class="page">
    <p>TCP traffic is very bursty  Made worse by CPU-offload opImizaIons like Large Send Offload and Interrupt Coalescing</p>
    <p>Causes spikes in queuing, increasing latency</p>
    <p>Example. 1Gbps flow on 10G NIC</p>
    <p>The Need for Pacing</p>
  </div>
  <div class="page">
    <p>Impact of Interrupt Coalescing</p>
    <p>Interrupt Coalescing</p>
    <p>Receiver CPU (%)</p>
    <p>Throughput (Gbps)</p>
    <p>Burst Size (KB)</p>
    <p>disabled 99 7.7 67.4</p>
    <p>rx-frames=2 98.7 9.3 11.4</p>
    <p>rx-frames=8 75 9.5 12.2</p>
    <p>rx-frames=32 53.2 9.5 16.5</p>
    <p>rx-frames=128 30.7 9.5 64.0</p>
    <p>More Interrupt Coalescing</p>
    <p>Lower CPU UAlizaAon &amp; Higher Throughput</p>
    <p>More BursAness 14</p>
  </div>
  <div class="page">
    <p>Algorithmic challenges:  At what rate to pace?</p>
    <p>Found dynamically:  Which flows to pace?</p>
    <p>Elephants: On each ACK with ECN bit set, begin pacing the flow with some probability.</p>
    <p>R  (1)R+Rmeasured +QTB</p>
    <p>Outgoing Packets From</p>
    <p>Server NIC</p>
    <p>Un-paced Traffic</p>
    <p>TX</p>
    <p>Token Bucket Rate Limiter</p>
    <p>Flow AssociaAon</p>
    <p>Table</p>
    <p>R QTB</p>
    <p>Hardware Pacer Module</p>
  </div>
  <div class="page">
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t [</p>
    <p>M bp</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>M ea</p>
    <p>n S</p>
    <p>w itc</p>
    <p>h La</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>Throughput Switch latency (mean)</p>
    <p>Throughput &amp; Latency vs. PQ Drain Rate (with Pacing)</p>
  </div>
  <div class="page">
    <p>M ea</p>
    <p>n S</p>
    <p>w itc</p>
    <p>h La</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>M ea</p>
    <p>n S</p>
    <p>w itc</p>
    <p>h La</p>
    <p>te nc</p>
    <p>y [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>No Pacing Pacing</p>
    <p>No Pacing vs Pacing (Mean Latency)</p>
  </div>
  <div class="page">
    <p>P er</p>
    <p>ce nt</p>
    <p>ile L</p>
    <p>at en</p>
    <p>cy [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>P er</p>
    <p>ce nt</p>
    <p>ile L</p>
    <p>at en</p>
    <p>cy [</p>
    <p>s]</p>
    <p>PQ Drain Rate [Mbps]</p>
    <p>ecn1k ecn3k ecn6k</p>
    <p>ecn15k ecn30k</p>
    <p>No Pacing Pacing</p>
    <p>No Pacing vs Pacing (99th PercenAle Latency)</p>
  </div>
  <div class="page">
    <p>The HULL Architecture</p>
    <p>Phantom Queue</p>
    <p>Hardware Pacer</p>
    <p>DCTCP CongesAon Control</p>
  </div>
  <div class="page">
    <p>SW1</p>
    <p>NF5</p>
    <p>S9 S10</p>
    <p>NF1 S1</p>
    <p>S2</p>
    <p>NF2 S3</p>
    <p>S4</p>
    <p>NF3 S6</p>
    <p>NF4 S7</p>
    <p>S8</p>
    <p>NF6 S5</p>
    <p>ImplementaIon  PQ, Pacer, and Latency Measurement modules implemented in NetFPGA</p>
    <p>DCTCP in Linux (patch available online)</p>
    <p>EvaluaIon  10 server testbed  Numerous micro-benchmarks</p>
    <p>StaIc &amp; dynamic workloads  Comparison with ideal 2-priority QoS scheme  Different marking thresholds, switch buffer sizes  Effect of parameters</p>
    <p>Large-scale ns-2 simulaIons</p>
    <p>ImplementaAon and EvaluaAon</p>
  </div>
  <div class="page">
    <p>Load: 20% Switch Latency (s) 10MB FCT (ms)</p>
    <p>Avg 99th Avg 99th</p>
    <p>TCP 111.5 1,224.8 110.2 349.6</p>
    <p>DCTCP-30K 38.4 295.2 106.8 301.7</p>
    <p>DCTCP-6K-Pacer 6.6 59.7 111.8 320.0</p>
    <p>DCTCP-PQ950-Pacer 2.8 18.6 125.4 359.9</p>
    <p>9 senders  1 receiver (80% 1KB flows, 20% 10MB flows).</p>
    <p>Dynamic Flow Experiment 20% load</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>The HULL architecture combines  Phantom queues  DCTCP  Hardware pacing</p>
    <p>We trade some bandwidth (that is relaIvely plenIful) for significant latency reducIons (oXen 10-40x compared to TCP and DCTCP).</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
