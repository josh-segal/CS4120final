<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Peregreen  modular database for efficient storage of historical time series</p>
    <p>in cloud environments</p>
    <p>Alexander A. Visheratin, Alexey Struckov, Semen Yufa, Alexey Muratov, Denis Nasonov, Nikolay Butakov</p>
    <p>ITMO University</p>
    <p>Yury Kuznetsov, Michael May Siemens</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Use cases:</p>
    <p>- Browsing large amounts of historical data  quick zoom-in and zoom-out in</p>
    <p>long time intervals.</p>
    <p>- Conditional search for the data (e.g. anomalies search)  complex conditions,</p>
    <p>visualization of results.</p>
  </div>
  <div class="page">
    <p>Requirements</p>
    <p>- Store terabytes of time series data for tens of years.</p>
    <p>- 1 second for conditional search in whole database.</p>
    <p>- Execution speed of 450,000 entries per second per node.</p>
    <p>- All data must be stored in Amazon S3.</p>
  </div>
  <div class="page">
    <p>Main goal: minimize network overheads for search and extraction.</p>
    <p>Challenges:</p>
    <p>- Precise navigation in files at the backend storage.</p>
    <p>- Minimize the number of requests for reading and writing.</p>
    <p>- Minimize the size of index and data files.</p>
    <p>Proposed approach</p>
  </div>
  <div class="page">
    <p>Three-tier data indexing</p>
    <p>- Split data into chunks by equal time steps (block interval).</p>
    <p>- Create index blocks with meta for extraction and metrics for search.</p>
    <p>- Combine blocks into segments by equal time steps (segment interval).</p>
    <p>- Combine segments into the sensor index.</p>
  </div>
  <div class="page">
    <p>Index metadata</p>
    <p>Block: - Number of elements (preallocate arrays during extraction). - Length of the encoded data (navigate and read data from backend storage). - Compression flag (whether to decompress binary data).</p>
    <p>Segment: - Unique identifier (file name in backend storage). - Start time (search and extraction). - Version (storage consistency).</p>
    <p>Index: - Unique identifier (sensor name). - Data type (int32, float32, int64, etc.) - Block and segment intervals (CRUD operations). - Version (storage consistency).</p>
  </div>
  <div class="page">
    <p>Read-optimized series encoding (I)</p>
    <p>Two-step process:</p>
    <p>Compressed byte arrays are stored in segment files.</p>
  </div>
  <div class="page">
    <p>Read-optimized series encoding (II)</p>
    <p>- Timestamp-value pairs are stored together (extraction with one read).</p>
    <p>- Timestamp deltas are stored as unsigned int32 (maximum 49 days in ms).</p>
    <p>- For values delta - IEEE 754 binary representations (longer zero-filled bit</p>
    <p>sequences).</p>
    <p>- Zstandard algorithm gives 2x compression and 10-15% overhead.</p>
  </div>
  <div class="page">
    <p>Peregreen overview</p>
    <p>- Core implements indexing and encoding logic.</p>
    <p>- Upload, delete, search and extract operations are supported via HTTP API.</p>
    <p>- Modules as programming interfaces for easy adding new functionality.</p>
    <p>- Raft cluster with replication to achieve consistency and fault-tolerance.</p>
  </div>
  <div class="page">
    <p>Peregreen core. Upload</p>
  </div>
  <div class="page">
    <p>Peregreen core. Search</p>
    <p>- Performed using only index.</p>
    <p>- Time granularity is the block interval.</p>
    <p>- Core checks time and value conditions in segments and blocks.</p>
    <p>- Time condition - interval set by user in the request.</p>
    <p>- Value condition - queries consisting of metric conditions combined by logic</p>
    <p>operations.</p>
    <p>Query example: min lte 200 &amp; max gte 50 | avg eq 75 (1) minimum metric is lower or equal to 200 and maximum metric is greater or equal to 50, or (2) average metric is equal to 75.</p>
  </div>
  <div class="page">
    <p>Four types of extraction:</p>
    <p>Peregreen core. Extraction (I)</p>
  </div>
  <div class="page">
    <p>segment start times and segment interval.</p>
    <p>storage. Uses encoded data lengths from blocks.</p>
    <p>reading.</p>
    <p>Peregreen core. Extraction (II)</p>
  </div>
  <div class="page">
    <p>Peregreen modules</p>
    <p>Integration with backend storage - indices reading and writing, writing segment files and reading parts of segment files.</p>
    <p>Reading data elements from the input stream one by one. Create data chunks on the fly.</p>
    <p>Converting results - search results, timestamp-value pairs, aggregations or transformations - into desirable output format.</p>
    <p>Aggregated statistical characteristics for the data. Can be combined (segment and block metrics).</p>
    <p>Change the data as it is extracted.</p>
  </div>
  <div class="page">
    <p>- High horizontal scalability.</p>
    <p>- Sensors are distributed equally across nodes.</p>
    <p>- Sensors replication for fault tolerance.</p>
    <p>- Write requests go through leader node that</p>
    <p>applies lock on the sensor.</p>
    <p>- Read requests are redirected to the responsible</p>
    <p>node.</p>
    <p>Peregreen cluster</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>nodes. Compared to ClickHouse and InfluxDB.</p>
    <p>attached EBS storages. 1, 4, 7 nodes. Compared EBS vs S3 as backend</p>
    <p>storages.</p>
  </div>
  <div class="page">
    <p>On-premise: Uploading</p>
    <p>Uploading time, minutes</p>
    <p>Peregreen 891 243 150</p>
    <p>ClickHouse 530 193 83</p>
    <p>InfluxDB 1847 583 N/A</p>
    <p>Stored data volume, GB</p>
    <p>Peregreen ClickHouse InfluxDB</p>
    <p>Initial data volume - 750 GB.</p>
  </div>
  <div class="page">
    <p>On-premise: Search and extraction</p>
    <p>Data search Data extraction</p>
    <p>Small requests (1 hour)</p>
    <p>Large requests (10 days)</p>
    <p>Y axis in log scale</p>
  </div>
  <div class="page">
    <p>Amazon EC2: Extraction</p>
    <p>S3 efficiency compared to EBS 1000 concurrent requests for 1, 6, 24 and 240 hours. 4 instances:</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Peregreen - fast time series database designed for efficient work with backend</p>
    <p>storage:</p>
    <p>- Three-tier data indexing for small and powerful indices.</p>
    <p>- Read-optimized series encoding for compact data representation.</p>
    <p>- 5 types of modules for great extensibility.</p>
    <p>- Fast uploading, search and extraction in both on-premise and cloud setups.</p>
  </div>
  <div class="page">
    <p>Thank you for your attention!</p>
    <p>Peregreen  modular database for efficient storage of historical time</p>
    <p>series in cloud environments</p>
    <p>Alexander A. Visheratin ITMO University</p>
    <p>alexvish91@gmail.com</p>
    <p>Acknowledgements This work is carried out in the Siemens-ITMO Industrial Cognitive Technologies Lab and is funded by Siemens.</p>
  </div>
</Presentation>
