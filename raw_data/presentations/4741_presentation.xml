<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Design Methodology for Domain-Optimized Power-Ecient Supercomputing</p>
    <p>Marghoob Mohiyuddin, Mark Murphy, Leonid Oliker, John Shalf, John Wawrzynek, Samuel Williams</p>
    <p>marghoob@eecs.berkeley.edu</p>
    <p>SC09, Nov 19, 2009</p>
  </div>
  <div class="page">
    <p>Takeaway</p>
    <p>HW/SW co-tuning as a new approach to HW design</p>
    <p>Applied the new approach to 3 scientic computing kernels and the Stanford Smart Memories multiprocessor</p>
    <p>Results show eciency improves signicantly when HW designed using co-tuning</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Will exascale happen?</p>
    <p>. . . with the current approach</p>
    <p>From Peter Kogge, DARPA exascale study</p>
  </div>
  <div class="page">
    <p>At what cost?</p>
    <p>Power-eciency not improving at historic rates</p>
    <p>Petaop systems already draw Megawatts of power</p>
    <p>DARPA exascale study predicts &gt; 100 Megawatts of power for exaop systems</p>
    <p>From Peter Kogge, DARPA exascale study</p>
  </div>
  <div class="page">
    <p>What is wrong with current HW design approaches?</p>
    <p>General-purpose commodity processors in many large machines are power-inecient HW customization improves energy eciency</p>
    <p>Simpler cores more power-ecient Intel Core2 sc: 15W@1000 MHz Tensilica XTensa DP: .09W@600 MHz</p>
  </div>
  <div class="page">
    <p>What is wrong with current HW design approaches?</p>
    <p>General-purpose commodity processors in many large machines are power-inecient HW customization improves energy eciency</p>
    <p>Simpler cores more power-ecient Intel Core2 sc: 15W@1000 MHz Tensilica XTensa DP: .09W@600 MHz</p>
    <p>Typical HW design space exploration</p>
    <p>HW cong parameters: # cores, cache/local store organization, interconnect, DRAM latency/bandwidth, etc</p>
    <p>Find the right balance of parameters: cores vs. cache, bandwidth vs. peak op rate</p>
    <p>Benchmarks not optimized for each HW cong considered</p>
  </div>
  <div class="page">
    <p>What is right with current SW tuning?</p>
    <p>Answer: auto-tuning</p>
    <p>Automate the process of optimizing SW for a variety of architectures</p>
    <p>Assumption: architectures evolve  optimizations still valid Key to portable high-performance libraries: ATLAS, OSKI, FFTW, SPIRAL</p>
  </div>
  <div class="page">
    <p>What is right with current SW tuning?</p>
    <p>Answer: auto-tuning</p>
    <p>Automate the process of optimizing SW for a variety of architectures</p>
    <p>Assumption: architectures evolve  optimizations still valid Key to portable high-performance libraries: ATLAS, OSKI, FFTW, SPIRAL</p>
  </div>
  <div class="page">
    <p>What is right with current SW tuning?</p>
    <p>Answer: auto-tuning</p>
    <p>Automate the process of optimizing SW for a variety of architectures</p>
    <p>Assumption: architectures evolve  optimizations still valid Key to portable high-performance libraries: ATLAS, OSKI, FFTW, SPIRAL</p>
  </div>
  <div class="page">
    <p>What is right with current SW tuning?</p>
    <p>Answer: auto-tuning</p>
    <p>Automate the process of optimizing SW for a variety of architectures</p>
    <p>Assumption: architectures evolve  optimizations still valid Key to portable high-performance libraries: ATLAS, OSKI, FFTW, SPIRAL</p>
    <p>Conventional SW autotuning</p>
    <p>SW cong parameters: register/cache block sizes, loop unroll factor, data structures, algorithms, etc</p>
    <p>Source code generators + parameterized routines + search heuristic</p>
    <p>Oine (install time), runtime tuning</p>
  </div>
  <div class="page">
    <p>HW/SW Co-tuning: The solution</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>HW/SW Co-tuning: The solution</p>
    <p>+</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>HW/SW Co-tuning</p>
    <p>Key idea: include SW autotuning in the loop for HW design</p>
    <p>A rigorous systematic approach to HW design</p>
    <p>This is also the approach taken in Green Flash where the target application is climate modeling</p>
    <p>Our results arm the eectiveness of co-tuning</p>
  </div>
  <div class="page">
    <p>HW/SW Co-tuning</p>
    <p>Key idea: include SW autotuning in the loop for HW design</p>
    <p>A rigorous systematic approach to HW design</p>
    <p>This is also the approach taken in Green Flash where the target application is climate modeling</p>
    <p>Our results arm the eectiveness of co-tuning</p>
  </div>
  <div class="page">
    <p>HW/SW Co-tuning</p>
    <p>Key idea: include SW autotuning in the loop for HW design</p>
    <p>A rigorous systematic approach to HW design</p>
    <p>This is also the approach taken in Green Flash where the target application is climate modeling</p>
    <p>Our results arm the eectiveness of co-tuning</p>
  </div>
  <div class="page">
    <p>A simple example</p>
    <p>Sparse matrix vector multiply performance (121K rows, nnz/row=27.3) on Stanford Smart Memories multiprocessor</p>
    <p>(DRAM bandwidth = 1.6 GB/s, cache/core = 64 KB)</p>
    <p>For tuned SpMV, best #cores = 2 (same performance as 4 cores with half the area)</p>
    <p>For untuned SpMV, best #cores = 4  overdesign</p>
  </div>
  <div class="page">
    <p>A simple example</p>
    <p>Sparse matrix vector multiply performance (121K rows, nnz/row=27.3) on Stanford Smart Memories multiprocessor</p>
    <p>(DRAM bandwidth = 1.6 GB/s, cache/core = 64 KB)</p>
    <p>Best config</p>
    <p>For tuned SpMV, best #cores = 2 (same performance as 4 cores with half the area)</p>
    <p>For untuned SpMV, best #cores = 4  overdesign</p>
  </div>
  <div class="page">
    <p>A simple example</p>
    <p>Sparse matrix vector multiply performance (121K rows, nnz/row=27.3) on Stanford Smart Memories multiprocessor</p>
    <p>(DRAM bandwidth = 1.6 GB/s, cache/core = 64 KB)</p>
    <p>For tuned SpMV, best #cores = 2 (same performance as 4 cores with half the area)</p>
    <p>For untuned SpMV, best #cores = 4  overdesign</p>
  </div>
  <div class="page">
    <p>A simple example</p>
    <p>Sparse matrix vector multiply performance (121K rows, nnz/row=27.3) on Stanford Smart Memories multiprocessor</p>
    <p>(DRAM bandwidth = 1.6 GB/s, cache/core = 64 KB)</p>
    <p>Best config</p>
    <p>For tuned SpMV, best #cores = 2 (same performance as 4 cores with half the area)</p>
    <p>For untuned SpMV, best #cores = 4  overdesign</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Application of HW/SW co-tuning</p>
    <p>Software: 3 kernels from scientic computing:</p>
    <p>Dense matrix matrix multiplication (dense linear algebra) 7pt stencil operator (heat equation PDE) Sparse matrix vector multiplication (sparse linear algebra) Varying computational characteristics  pull HW parameters in di. directions Success of co-tuning demonstrated by application on multiple kernels</p>
    <p>Hardware: Stanford Smart Memories multiprocessor</p>
    <p>Multiprocessor using Tensilica cores Analogous to the Green Flash design which uses the same cores</p>
  </div>
  <div class="page">
    <p>The Kernels Dense matrix matrix multiplication (GEMM)</p>
    <p>BC A</p>
    <p>Dense linear algebra</p>
    <p>High computational intensity</p>
    <p>Tuned code gets close to machine peak</p>
    <p>More cores  better performance</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>uNext(i,j,k)=</p>
    <p>alpha*u(i,j,k)+</p>
    <p>beta*(u(i+1,j,k)+u(i-1,j,k)+ u(i,j+1,k)+u(i,j-1,k)+</p>
    <p>u(i,j,k+1)+u(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>u(i,j,k)=</p>
    <p>alpha*( x(i+1,j,k)-x(i-1,j,k) )+</p>
    <p>beta*( y(i,j+1,k)-y(i,j-1,k) )+ gamma*( z(i,j,k+1)-z(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>x(i,j,k)=alpha*( u(i+1,j,k)-u(i-1,j,k) )</p>
    <p>y(i,j,k)= beta*( u(i,j+1,k)-u(i,j-1,k) )</p>
    <p>z(i,j,k)=gamma*( u(i,j,k+1)-u(i,j,k-1) )</p>
    <p>enddo enddo</p>
    <p>enddo</p>
    <p>xy product</p>
    <p>read_array[ ][ ] x dimension</p>
    <p>write_array[ ]</p>
    <p>xy product write_array[ ][ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>xy product</p>
    <p>write_array[ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>u</p>
    <p>u</p>
    <p>(a) (b) (c)</p>
    <p>Figure 1. (a) Laplacian, (b) Divergence, and (c) Gradient stencils. Top: 3D visualization of the nearest neighbor stencil operator. Middle: code as passed to the parser. Bottom: memory access pattern as the stencil sweeps from left to right. Note: the color represents cartesian component of the vector fields (scalar fields are gray).</p>
    <p>To show the broad utility of our framework, we select three conceptually easy-to-understand, yet deceptively difficult</p>
    <p>to optimize stencil kernels arising from the application of the finite difference method to the Laplacian (unext ! &quot;2u), Divergence (u ! &quot; F), and Gradient (F ! &quot;u) differential operators. Details of these kernels are shown in Figure 1 and Table 1. All three operators are implemented using the nearest-neighbor central-difference method on a 3D rectahedral</p>
    <p>block-structured grid using Jacobis method (out-of-place), and benchmarked on a 256#256#256 grid. Note that although the code generator has no restrictions on data structure, for brevity, we only explore the use of the structure of arrays form</p>
    <p>for vector fields. As described below, these kernels have such low arithmetic intensity that they are expected to be memory</p>
    <p>bandwidth bound, and thus deliver performance approximately equal to the product of their arithmetic intensity (AI) with</p>
    <p>the system stream bandwidth. Note that arithmetic intensity is defined as the ratio of arithmetic operations to memory</p>
    <p>traffic.</p>
    <p>Table 1 presents the performance-critical information for the three stencil operators, and sets our performance expec</p>
    <p>Explicit nite-dierence method for the heat equation</p>
    <p>Low computational intensity, regular memory accesses</p>
    <p>More bandwidth  better performance</p>
    <p>Sparse matrix vector multiplication (SpMV)</p>
    <p>col idx</p>
    <p>row start</p>
    <p>val 10 -1 3 4 1 6 2 2 73</p>
    <p>A in compressed sparse row format</p>
    <p>y xA</p>
    <p>-1 3</p>
    <p>Used in PDEs, sparse solvers</p>
    <p>Low computational intensity, irregular memory accesses</p>
    <p>More bandwidth  better performance</p>
  </div>
  <div class="page">
    <p>The Kernels Dense matrix matrix multiplication (GEMM)</p>
    <p>BC A</p>
    <p>Dense linear algebra</p>
    <p>High computational intensity</p>
    <p>Tuned code gets close to machine peak</p>
    <p>More cores  better performance</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>uNext(i,j,k)=</p>
    <p>alpha*u(i,j,k)+</p>
    <p>beta*(u(i+1,j,k)+u(i-1,j,k)+ u(i,j+1,k)+u(i,j-1,k)+</p>
    <p>u(i,j,k+1)+u(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>u(i,j,k)=</p>
    <p>alpha*( x(i+1,j,k)-x(i-1,j,k) )+</p>
    <p>beta*( y(i,j+1,k)-y(i,j-1,k) )+ gamma*( z(i,j,k+1)-z(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>x(i,j,k)=alpha*( u(i+1,j,k)-u(i-1,j,k) )</p>
    <p>y(i,j,k)= beta*( u(i,j+1,k)-u(i,j-1,k) )</p>
    <p>z(i,j,k)=gamma*( u(i,j,k+1)-u(i,j,k-1) )</p>
    <p>enddo enddo</p>
    <p>enddo</p>
    <p>xy product</p>
    <p>read_array[ ][ ] x dimension</p>
    <p>write_array[ ]</p>
    <p>xy product write_array[ ][ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>xy product</p>
    <p>write_array[ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>u</p>
    <p>u</p>
    <p>(a) (b) (c)</p>
    <p>Figure 1. (a) Laplacian, (b) Divergence, and (c) Gradient stencils. Top: 3D visualization of the nearest neighbor stencil operator. Middle: code as passed to the parser. Bottom: memory access pattern as the stencil sweeps from left to right. Note: the color represents cartesian component of the vector fields (scalar fields are gray).</p>
    <p>To show the broad utility of our framework, we select three conceptually easy-to-understand, yet deceptively difficult</p>
    <p>to optimize stencil kernels arising from the application of the finite difference method to the Laplacian (unext ! &quot;2u), Divergence (u ! &quot; F), and Gradient (F ! &quot;u) differential operators. Details of these kernels are shown in Figure 1 and Table 1. All three operators are implemented using the nearest-neighbor central-difference method on a 3D rectahedral</p>
    <p>block-structured grid using Jacobis method (out-of-place), and benchmarked on a 256#256#256 grid. Note that although the code generator has no restrictions on data structure, for brevity, we only explore the use of the structure of arrays form</p>
    <p>for vector fields. As described below, these kernels have such low arithmetic intensity that they are expected to be memory</p>
    <p>bandwidth bound, and thus deliver performance approximately equal to the product of their arithmetic intensity (AI) with</p>
    <p>the system stream bandwidth. Note that arithmetic intensity is defined as the ratio of arithmetic operations to memory</p>
    <p>traffic.</p>
    <p>Table 1 presents the performance-critical information for the three stencil operators, and sets our performance expec</p>
    <p>Explicit nite-dierence method for the heat equation</p>
    <p>Low computational intensity, regular memory accesses</p>
    <p>More bandwidth  better performance</p>
    <p>Sparse matrix vector multiplication (SpMV)</p>
    <p>col idx</p>
    <p>row start</p>
    <p>val 10 -1 3 4 1 6 2 2 73</p>
    <p>A in compressed sparse row format</p>
    <p>y xA</p>
    <p>-1 3</p>
    <p>Used in PDEs, sparse solvers</p>
    <p>Low computational intensity, irregular memory accesses</p>
    <p>More bandwidth  better performance</p>
  </div>
  <div class="page">
    <p>The Kernels Dense matrix matrix multiplication (GEMM)</p>
    <p>BC A</p>
    <p>Dense linear algebra</p>
    <p>High computational intensity</p>
    <p>Tuned code gets close to machine peak</p>
    <p>More cores  better performance</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>uNext(i,j,k)=</p>
    <p>alpha*u(i,j,k)+</p>
    <p>beta*(u(i+1,j,k)+u(i-1,j,k)+ u(i,j+1,k)+u(i,j-1,k)+</p>
    <p>u(i,j,k+1)+u(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>u(i,j,k)=</p>
    <p>alpha*( x(i+1,j,k)-x(i-1,j,k) )+</p>
    <p>beta*( y(i,j+1,k)-y(i,j-1,k) )+ gamma*( z(i,j,k+1)-z(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>x(i,j,k)=alpha*( u(i+1,j,k)-u(i-1,j,k) )</p>
    <p>y(i,j,k)= beta*( u(i,j+1,k)-u(i,j-1,k) )</p>
    <p>z(i,j,k)=gamma*( u(i,j,k+1)-u(i,j,k-1) )</p>
    <p>enddo enddo</p>
    <p>enddo</p>
    <p>xy product</p>
    <p>read_array[ ][ ] x dimension</p>
    <p>write_array[ ]</p>
    <p>xy product write_array[ ][ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>xy product</p>
    <p>write_array[ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>u</p>
    <p>u</p>
    <p>(a) (b) (c)</p>
    <p>Figure 1. (a) Laplacian, (b) Divergence, and (c) Gradient stencils. Top: 3D visualization of the nearest neighbor stencil operator. Middle: code as passed to the parser. Bottom: memory access pattern as the stencil sweeps from left to right. Note: the color represents cartesian component of the vector fields (scalar fields are gray).</p>
    <p>To show the broad utility of our framework, we select three conceptually easy-to-understand, yet deceptively difficult</p>
    <p>to optimize stencil kernels arising from the application of the finite difference method to the Laplacian (unext ! &quot;2u), Divergence (u ! &quot; F), and Gradient (F ! &quot;u) differential operators. Details of these kernels are shown in Figure 1 and Table 1. All three operators are implemented using the nearest-neighbor central-difference method on a 3D rectahedral</p>
    <p>block-structured grid using Jacobis method (out-of-place), and benchmarked on a 256#256#256 grid. Note that although the code generator has no restrictions on data structure, for brevity, we only explore the use of the structure of arrays form</p>
    <p>for vector fields. As described below, these kernels have such low arithmetic intensity that they are expected to be memory</p>
    <p>bandwidth bound, and thus deliver performance approximately equal to the product of their arithmetic intensity (AI) with</p>
    <p>the system stream bandwidth. Note that arithmetic intensity is defined as the ratio of arithmetic operations to memory</p>
    <p>traffic.</p>
    <p>Table 1 presents the performance-critical information for the three stencil operators, and sets our performance expec</p>
    <p>Explicit nite-dierence method for the heat equation</p>
    <p>Low computational intensity, regular memory accesses</p>
    <p>More bandwidth  better performance</p>
    <p>Sparse matrix vector multiplication (SpMV)</p>
    <p>col idx</p>
    <p>row start</p>
    <p>val 10 -1 3 4 1 6 2 2 73</p>
    <p>A in compressed sparse row format</p>
    <p>y xA</p>
    <p>-1 3</p>
    <p>Used in PDEs, sparse solvers</p>
    <p>Low computational intensity, irregular memory accesses</p>
    <p>More bandwidth  better performance</p>
  </div>
  <div class="page">
    <p>The Kernels Dense matrix matrix multiplication (GEMM)</p>
    <p>BC A</p>
    <p>Dense linear algebra</p>
    <p>High computational intensity</p>
    <p>Tuned code gets close to machine peak</p>
    <p>More cores  better performance</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>uNext(i,j,k)=</p>
    <p>alpha*u(i,j,k)+</p>
    <p>beta*(u(i+1,j,k)+u(i-1,j,k)+ u(i,j+1,k)+u(i,j-1,k)+</p>
    <p>u(i,j,k+1)+u(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>u(i,j,k)=</p>
    <p>alpha*( x(i+1,j,k)-x(i-1,j,k) )+</p>
    <p>beta*( y(i,j+1,k)-y(i,j-1,k) )+ gamma*( z(i,j,k+1)-z(i,j,k-1) )</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>enddo</p>
    <p>do k=2,nz-1,1</p>
    <p>do j=2,ny-1,1 do i=2,nx-1,1</p>
    <p>x(i,j,k)=alpha*( u(i+1,j,k)-u(i-1,j,k) )</p>
    <p>y(i,j,k)= beta*( u(i,j+1,k)-u(i,j-1,k) )</p>
    <p>z(i,j,k)=gamma*( u(i,j,k+1)-u(i,j,k-1) )</p>
    <p>enddo enddo</p>
    <p>enddo</p>
    <p>xy product</p>
    <p>read_array[ ][ ] x dimension</p>
    <p>write_array[ ]</p>
    <p>xy product write_array[ ][ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>xy product</p>
    <p>write_array[ ]</p>
    <p>x dimension read_array[ ]</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>x</p>
    <p>y</p>
    <p>z</p>
    <p>u</p>
    <p>u</p>
    <p>u</p>
    <p>(a) (b) (c)</p>
    <p>Figure 1. (a) Laplacian, (b) Divergence, and (c) Gradient stencils. Top: 3D visualization of the nearest neighbor stencil operator. Middle: code as passed to the parser. Bottom: memory access pattern as the stencil sweeps from left to right. Note: the color represents cartesian component of the vector fields (scalar fields are gray).</p>
    <p>To show the broad utility of our framework, we select three conceptually easy-to-understand, yet deceptively difficult</p>
    <p>to optimize stencil kernels arising from the application of the finite difference method to the Laplacian (unext ! &quot;2u), Divergence (u ! &quot; F), and Gradient (F ! &quot;u) differential operators. Details of these kernels are shown in Figure 1 and Table 1. All three operators are implemented using the nearest-neighbor central-difference method on a 3D rectahedral</p>
    <p>block-structured grid using Jacobis method (out-of-place), and benchmarked on a 256#256#256 grid. Note that although the code generator has no restrictions on data structure, for brevity, we only explore the use of the structure of arrays form</p>
    <p>for vector fields. As described below, these kernels have such low arithmetic intensity that they are expected to be memory</p>
    <p>bandwidth bound, and thus deliver performance approximately equal to the product of their arithmetic intensity (AI) with</p>
    <p>the system stream bandwidth. Note that arithmetic intensity is defined as the ratio of arithmetic operations to memory</p>
    <p>traffic.</p>
    <p>Table 1 presents the performance-critical information for the three stencil operators, and sets our performance expec</p>
    <p>Explicit nite-dierence method for the heat equation</p>
    <p>Low computational intensity, regular memory accesses</p>
    <p>More bandwidth  better performance</p>
    <p>Sparse matrix vector multiplication (SpMV)</p>
    <p>col idx</p>
    <p>row start</p>
    <p>val 10 -1 3 4 1 6 2 2 73</p>
    <p>A in compressed sparse row format</p>
    <p>y xA</p>
    <p>-1 3</p>
    <p>Used in PDEs, sparse solvers</p>
    <p>Low computational intensity, irregular memory accesses</p>
    <p>More bandwidth  better performance</p>
  </div>
  <div class="page">
    <p>The Hardware: Stanford Smart Memories Multiprocessor</p>
    <p>Low Power, External DDR DRAM</p>
    <p>crossbar / coherency</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>memory controller</p>
    <p>Configurable Multicore Processor</p>
    <p>Each core has a single-precision FPU</p>
    <p>Constant `area' of 35mm2 added to include the impact of DRAM cost</p>
  </div>
  <div class="page">
    <p>The Hardware: Stanford Smart Memories Multiprocessor</p>
    <p>Low Power, External DDR DRAM</p>
    <p>crossbar / coherency</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>memory controller</p>
    <p>Configurable Multicore Processor</p>
    <p>Core power/ perf. model from Tensilica tools</p>
    <p>Core power/perf/area model from Tensilica tools</p>
    <p>Each core has a single-precision FPU</p>
    <p>Constant `area' of 35mm2 added to include the impact of DRAM cost</p>
  </div>
  <div class="page">
    <p>The Hardware: Stanford Smart Memories Multiprocessor</p>
    <p>Low Power, External DDR DRAM</p>
    <p>crossbar / coherency</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>memory controller</p>
    <p>Configurable Multicore Processor</p>
    <p>On-chip mem. power/perf. model from the CACTI tool</p>
    <p>On-chip power/perf/area model using CACTI tool</p>
    <p>Each core has a single-precision FPU</p>
    <p>Constant `area' of 35mm2 added to include the impact of DRAM cost</p>
  </div>
  <div class="page">
    <p>The Hardware: Stanford Smart Memories Multiprocessor</p>
    <p>Low Power, External DDR DRAM</p>
    <p>crossbar / coherency</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>memory controller</p>
    <p>Configurable Multicore Processor</p>
    <p>Network energy model from Bill Dally's paper</p>
    <p>Each core has a single-precision FPU</p>
    <p>Constant `area' of 35mm2 added to include the impact of DRAM cost</p>
  </div>
  <div class="page">
    <p>The Hardware: Stanford Smart Memories Multiprocessor</p>
    <p>Low Power, External DDR DRAM</p>
    <p>crossbar / coherency</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>D$ or LS</p>
    <p>XTensa Core</p>
    <p>memory controller</p>
    <p>Configurable Multicore Processor</p>
    <p>Power model from Micron datasheets</p>
    <p>Each core has a single-precision FPU</p>
    <p>Constant `area' of 35mm2 added to include the impact of DRAM cost</p>
  </div>
  <div class="page">
    <p>Hardware Parameters</p>
    <p>Fixed:</p>
    <p>Core: single-issue, 500 MHz Cache/local store: 16 KB I-cache, cache associativity = 4, linesize = 64 bytes DRAM: latency = 100 core cycles</p>
    <p>Variable:</p>
    <p># cores: 1/4/16 On-chip data memory type: cache/local store Cache/local store per core: 16, 32, 64, 128 KB DRAM bandwidth: 0.8, 1.6, 3.2 GB/s 72 HW congs</p>
    <p>Baseline cong: Fastest HW</p>
    <p>On-chip memory type: cache Cache per core: 128 KB DRAM bandwidth: 3.2 GB/s</p>
  </div>
  <div class="page">
    <p>Hardware Parameters</p>
    <p>Fixed:</p>
    <p>Core: single-issue, 500 MHz Cache/local store: 16 KB I-cache, cache associativity = 4, linesize = 64 bytes DRAM: latency = 100 core cycles</p>
    <p>Variable:</p>
    <p># cores: 1/4/16 On-chip data memory type: cache/local store Cache/local store per core: 16, 32, 64, 128 KB DRAM bandwidth: 0.8, 1.6, 3.2 GB/s 72 HW congs</p>
    <p>Baseline cong: Fastest HW</p>
    <p>On-chip memory type: cache Cache per core: 128 KB DRAM bandwidth: 3.2 GB/s</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Optimized Metrics</p>
    <p>Focus on scientic computing apps running on large-scale systems</p>
    <p>Emphasize node eciency instead of node performance</p>
    <p>Power eciency (MFlops/Watt)</p>
    <p>Running costs Maximize performance given a power budget</p>
    <p>Area eciency (MFlops/mm2)</p>
    <p>System cost, reliability dependent on area Maximize performance given an area budget</p>
    <p>Power eciency, area eciency can result in dierent optimal HW cong</p>
    <p>In general, would want to optimize a combination of power-, area-eciencies</p>
  </div>
  <div class="page">
    <p>Eect of SW tuning on performance</p>
    <p>DRAM bw = 1.6 GB/s, D-cache/local store = 64 KB</p>
    <p>(CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM gains a lot from tuning</p>
    <p>Software-managed caches get better performance</p>
    <p>Bandwidth-saturation for stencil and SpMV</p>
  </div>
  <div class="page">
    <p>Eect of SW tuning on performance</p>
    <p>DRAM bw = 1.6 GB/s, D-cache/local store = 64 KB</p>
    <p>(CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM gains a lot from tuning</p>
    <p>Software-managed caches get better performance</p>
    <p>Bandwidth-saturation for stencil and SpMV</p>
  </div>
  <div class="page">
    <p>Eect of SW tuning on performance</p>
    <p>DRAM bw = 1.6 GB/s, D-cache/local store = 64 KB</p>
    <p>(CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM gains a lot from tuning</p>
    <p>Software-managed caches get better performance</p>
    <p>Bandwidth-saturation for stencil and SpMV</p>
  </div>
  <div class="page">
    <p>Eect of memory bandwidth on tuned performance</p>
    <p>D-cache/local store = 64 KB (CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM least sensitive to memory bandwidth</p>
    <p>SpMV performance scales with memory bandwidth for enough cores</p>
  </div>
  <div class="page">
    <p>Eect of memory bandwidth on tuned performance</p>
    <p>D-cache/local store = 64 KB (CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM least sensitive to memory bandwidth</p>
    <p>SpMV performance scales with memory bandwidth for enough cores</p>
  </div>
  <div class="page">
    <p>Eect of memory bandwidth on tuned performance</p>
    <p>D-cache/local store = 64 KB (CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM least sensitive to memory bandwidth</p>
    <p>SpMV performance scales with memory bandwidth for enough cores</p>
  </div>
  <div class="page">
    <p>Eect of memory bandwidth on tuned performance</p>
    <p>D-cache/local store = 64 KB (CC=cache, LS=local store)</p>
    <p>GEMM Stencil</p>
    <p>SpMV</p>
    <p>GEMM least sensitive to memory bandwidth</p>
    <p>SpMV performance scales with memory bandwidth for enough cores</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: GEMM</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) Untuned cache</p>
    <p>Each point represent a HW cong (AE = most area ecient, PE = most power ecient)</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
    <p>Eciency improvements from SW tuning dramatic</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: GEMM</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong (AE = most area ecient, PE = most power ecient)</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
    <p>Eciency improvements from SW tuning dramatic</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: GEMM</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong (AE = most area ecient, PE = most power ecient)</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
    <p>Eciency improvements from SW tuning dramatic</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: GEMM</p>
    <p>Fastest HW</p>
    <p>Each point represent a HW cong (AE = most area ecient, PE = most power ecient)</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
    <p>Eciency improvements from SW tuning dramatic</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: Stencil</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) Untuned cache</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: Stencil</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: Stencil</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: Stencil</p>
    <p>Fastest HW</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: SpMV</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 ) Untuned cache</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: SpMV</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 )</p>
    <p>AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: SpMV</p>
    <p>A re</p>
    <p>a ef</p>
    <p>fic ie</p>
    <p>nc y</p>
    <p>(M F</p>
    <p>lo p/</p>
    <p>s/ m</p>
    <p>m 2 )</p>
    <p>AE</p>
    <p>PE</p>
    <p>Untuned cache Tuned cache Local store</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Eciency Improvements: SpMV</p>
    <p>Fastest HW</p>
    <p>Each point represent a HW cong</p>
    <p>Best SW performance chosen by autotuner used for computing eciencies</p>
  </div>
  <div class="page">
    <p>Co-tuning for multiple kernels</p>
    <p>Results so far nd best HW cong given kernel</p>
    <p>How about an application composed of multiple kernels?</p>
    <p>Simple case: kernels dont interact, all ops contributed by the given kernels  sucient to tune kernels instead of full application</p>
    <p>Performance/power for application on a HW cong = weighted performance/power of kernels on the cong Weights = relative contribution of dierent kernels</p>
  </div>
  <div class="page">
    <p>Co-tuning for multiple kernels</p>
    <p>Results so far nd best HW cong given kernel</p>
    <p>How about an application composed of multiple kernels?</p>
    <p>Simple case: kernels dont interact, all ops contributed by the given kernels  sucient to tune kernels instead of full application</p>
    <p>Performance/power for application on a HW cong = weighted performance/power of kernels on the cong Weights = relative contribution of dierent kernels</p>
  </div>
  <div class="page">
    <p>Co-tuning for multiple kernels</p>
    <p>Results so far nd best HW cong given kernel</p>
    <p>How about an application composed of multiple kernels?</p>
    <p>Simple case: kernels dont interact, all ops contributed by the given kernels  sucient to tune kernels instead of full application</p>
    <p>Performance/power for application on a HW cong = weighted performance/power of kernels on the cong Weights = relative contribution of dierent kernels</p>
  </div>
  <div class="page">
    <p>Tuning Multi-Kernel Application</p>
    <p>ra ct</p>
    <p>io n</p>
    <p>of flo</p>
    <p>ps fr</p>
    <p>om S</p>
    <p>te nc</p>
    <p>il</p>
    <p>CC, 16 128, 0.8</p>
    <p>CC, 4 64, 0.8</p>
    <p>LS, 4 64, 0.8</p>
    <p>CC/LS, #Cores</p>
    <p>CC/LS, BW Size (KB) (GB/s)</p>
    <p>Legend</p>
    <p>M F</p>
    <p>lo p/</p>
    <p>s/ W</p>
    <p>at t</p>
    <p>Each box represents the most power-ecient HW cong for the given relative weights of kernels</p>
  </div>
  <div class="page">
    <p>Summary of results</p>
    <p>Baseline: SW tuning done on the fastest HW cong</p>
    <p>GEMM: 1.2 and 1.5 improvements in power and area eciencies</p>
    <p>Stencil: 2.4 and 3 improvements in power and area eciencies</p>
    <p>SpMV: 1.7 and 1.6 improvements in power and area eciencies</p>
    <p>Weighted combination of GEMM, stencil, SpMV: improvements vary from 1.2 to 2.4 depending on relative contribution</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Novel approach to designing power-ecient supercomputers</p>
    <p>Leverage software auto-tuning to improve eciency Power eciency improved 1.22.4, area eciency improved 1.53 Improvements also in multi-kernel applications Co-tuning can cut down procurement and running costs</p>
    <p>Future work</p>
    <p>Explore a larger HW design space  need intelligent exploration Use FPGA-based emulation of hardware for speeding up exploration Eciently co-tuning for applications with interacting kernels Green Flash design</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Novel approach to designing power-ecient supercomputers</p>
    <p>Leverage software auto-tuning to improve eciency Power eciency improved 1.22.4, area eciency improved 1.53 Improvements also in multi-kernel applications Co-tuning can cut down procurement and running costs</p>
    <p>Future work</p>
    <p>Explore a larger HW design space  need intelligent exploration Use FPGA-based emulation of hardware for speeding up exploration Eciently co-tuning for applications with interacting kernels Green Flash design</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>Kernel 3: Matrices</p>
    <p>Dense 2K x 2K</p>
    <p>Name Dimensions Descriptionspyplot</p>
    <p>FEM / Spheres</p>
    <p>FEM / Cantilever</p>
    <p>Wind Tunnel</p>
    <p>QCD</p>
    <p>FEM/Ship</p>
    <p>Epidemiology</p>
    <p>Circuit</p>
    <p>Nonzeros (nnz/row)</p>
    <p>(6)</p>
    <p>Dense matrix in sparse format</p>
    <p>FEM concentric spheres</p>
    <p>FEM cantilever</p>
    <p>Pressurized wind tunnel</p>
    <p>Quark propagators (QCD/LGT)</p>
    <p>FEM Ship section/detail</p>
    <p>Motorola circuit simulation</p>
    <p>SpMV performance dependent on matrix nonzero pattern</p>
    <p>Matrices chosen to represent dierent applications</p>
    <p>Dense matrix in sparse format used for tuning</p>
    <p>For each HW cong, SpMV performance = performance of median matrix</p>
  </div>
</Presentation>
