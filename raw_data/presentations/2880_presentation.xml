<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Probabilistic Inference for Machine Translation</p>
    <p>Phil Blunsom, Miles Osborne</p>
    <p>School of Informatics The University of Edinburgh pblunsom@inf.ed.ac.uk</p>
    <p>EMNLP 2008</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 1 / 22</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 2 / 22</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Where is the currency exchange office ?</p>
    <p>?</p>
    <p>Wed like to model Machine Translation as an instance of structured prediction:</p>
    <p>We observe the input and output, but not the process in between.</p>
    <p>Probabilistic inference provides a natural method for modelling this latent structure.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 3 / 22</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>In this work we:</p>
    <p>Extend our previous probabilistic model (ACL 08) to include a language model.</p>
    <p>Demonstrate the flexibility of this modelling framework by extracting source conditioned syntactic features.</p>
    <p>Achieve result comparable to current state-of-the-art models.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 4 / 22</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 5 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Synchronous Context Free Grammar</p>
    <p>SCFG</p>
    <p>S  S 1 X 2 , S 1 X 2  S  X 1 , X 1</p>
    <p>X  il, he X  ne X 1 pas, does not X 1</p>
    <p>X  va, go</p>
    <p>Example Derivation</p>
    <p>S  S 1 X 2 , S 1 X 2</p>
    <p>X 1 X 2 , X 1 X 2</p>
    <p>il X 2 , he X 2</p>
    <p>il ne X 1 pas, he does not X 1</p>
    <p>il ne va pas, he does not go</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 6 / 22</p>
  </div>
  <div class="page">
    <p>Discriminative Model: Parametric form</p>
    <p>Conditional probability of a derivation</p>
    <p>p(d, e|f) = exp</p>
    <p>k k Hk (d, e, f) Z(f)</p>
    <p>.</p>
    <p>Conditional probability of a translation</p>
    <p>p(e|f) =</p>
    <p>d(e,f)</p>
    <p>p(d, e|f).</p>
    <p>We model the distribution over all derivations with a log-linear model.</p>
    <p>We marginalise out the derivations: Important for estimating sparse derivation dependent features.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 7 / 22</p>
  </div>
  <div class="page">
    <p>Discriminative Model: Parametric form</p>
    <p>Conditional probability of a derivation</p>
    <p>p(d, e|f) = exp</p>
    <p>k k Hk (d, e, f) Z(f)</p>
    <p>.</p>
    <p>Conditional probability of a translation</p>
    <p>p(e|f) =</p>
    <p>d(e,f)</p>
    <p>p(d, e|f).</p>
    <p>We model the distribution over all derivations with a log-linear model.</p>
    <p>We marginalise out the derivations: Important for estimating sparse derivation dependent features.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 7 / 22</p>
  </div>
  <div class="page">
    <p>Features</p>
    <p>The features must decompose with the rules:</p>
    <p>Hk (d, e, f) =  rd</p>
    <p>hk (f, r, q(r, d))</p>
    <p>Any part of the source may be used for features: Source syntax, morphology, lexical context etc.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 8 / 22</p>
  </div>
  <div class="page">
    <p>Features</p>
    <p>Example Derivation</p>
    <p>X[1,2] q=(Everything)</p>
    <p>X[3,4] q=(anything)</p>
    <p>X[1,3] q=(Everything and)</p>
    <p>s</p>
    <p>X[2,3] q=(and)</p>
    <p>X[1,4] q=(Everything * anything)</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / anything</p>
    <p>q(r, d) returns the order m target lexical context of the rule in the derivation.</p>
    <p>The target side of the rule and its lexical context can then be used in features.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 9 / 22</p>
  </div>
  <div class="page">
    <p>Approximating Z(f)</p>
    <p>Z(f)</p>
    <p>Z(f)  Z(f) =</p>
    <p>e</p>
    <p>d{(e,f)}</p>
    <p>exp</p>
    <p>k</p>
    <p>k Hk (d, e, f)</p>
    <p>We must approximate the space of all possible derivations.</p>
    <p>Obvious approach: Use a beam search to produce a pruned chart of possible derivations.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 10 / 22</p>
  </div>
  <div class="page">
    <p>Approximating Z(f)</p>
    <p>Z(f)</p>
    <p>Z(f)  Z(f) =</p>
    <p>e</p>
    <p>d{(e,f)}</p>
    <p>exp</p>
    <p>k</p>
    <p>k Hk (d, e, f)</p>
    <p>We must approximate the space of all possible derivations.</p>
    <p>Alternative: Build a chart from sampled derivations: Draw samples from an exact model trained without the LM (pLM ),</p>
    <p>create a chart from the union of these derivations with the LM context included,</p>
    <p>this creates a less biased sample of derivations.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 10 / 22</p>
  </div>
  <div class="page">
    <p>Approximating Z(f): Using sampling</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / anything</p>
    <p>ist / is</p>
    <p>vorstellbar / possible</p>
    <p>X X</p>
    <p>X S</p>
    <p>X[1,2] Everything</p>
    <p>X[3,4] anything</p>
    <p>X[1,3] Everything</p>
    <p>and</p>
    <p>s</p>
    <p>X[4,5] is</p>
    <p>X[5,6] possible</p>
    <p>X[1,5] Everything *</p>
    <p>is</p>
    <p>S[1,6] Everything *</p>
    <p>possible</p>
    <p>X[2,3] and</p>
    <p>X[1,4] Everything *</p>
    <p>anything 21 3 4 5 6</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 11 / 22</p>
  </div>
  <div class="page">
    <p>Approximating Z(f): Using sampling</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / anything</p>
    <p>ist / is</p>
    <p>vorstellbar / possible</p>
    <p>X X</p>
    <p>X S</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / everyone</p>
    <p>ist / is</p>
    <p>vorstellbar / conceivable</p>
    <p>X X</p>
    <p>X S</p>
    <p>X[1,2] Everything</p>
    <p>X[3,4] anything</p>
    <p>X[1,3] Everything</p>
    <p>and</p>
    <p>s</p>
    <p>X[4,5] is</p>
    <p>X[5,6] possible</p>
    <p>X[1,5] Everything *</p>
    <p>is</p>
    <p>S[1,6] Everything *</p>
    <p>possible</p>
    <p>X[5,6] conceivable</p>
    <p>X[2,3] and</p>
    <p>X[1,4] Everything *</p>
    <p>anything X[1,4]</p>
    <p>Everything * everyone</p>
    <p>X[3,4] everyone</p>
    <p>S[1,6] Everything * conceivable</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 11 / 22</p>
  </div>
  <div class="page">
    <p>Approximating Z(f): Using sampling</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / anything</p>
    <p>ist / is</p>
    <p>vorstellbar / possible</p>
    <p>X X</p>
    <p>X S</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / everyone</p>
    <p>ist / is</p>
    <p>vorstellbar / conceivable</p>
    <p>X X</p>
    <p>X S</p>
    <p>X[1,2] Everything</p>
    <p>X[3,4] anything</p>
    <p>X[1,3] Everything</p>
    <p>and</p>
    <p>s</p>
    <p>X[4,5] is</p>
    <p>X[5,6] possible</p>
    <p>X[1,5] Everything *</p>
    <p>is</p>
    <p>S[1,6] Everything *</p>
    <p>possible</p>
    <p>X[5,6] conceivable</p>
    <p>X[2,3] and</p>
    <p>X[1,4] Everything *</p>
    <p>anything X[1,4]</p>
    <p>Everything * everyone</p>
    <p>X[3,4] everyone</p>
    <p>S[1,6] Everything * conceivable</p>
    <p>Alles / Everything</p>
    <p>und / and</p>
    <p>jedes / anything</p>
    <p>ist / is</p>
    <p>vorstellbar / conceivable</p>
    <p>X X</p>
    <p>X S</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 11 / 22</p>
  </div>
  <div class="page">
    <p>Regularisation</p>
    <p>Prior</p>
    <p>p+LM0 (k )  e  |k</p>
    <p>LM k</p>
    <p>| 2</p>
    <p>The mean parameters of the Gaussian prior are set to LM .</p>
    <p>Any features that fall outside the approximated model will simply retain the weight assigned by LM .</p>
    <p>The prior will penalise substantial deviations away from LM .</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 12 / 22</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Objective function</p>
    <p>L =</p>
    <p>(ei ,fi )D</p>
    <p>log p+LM (ei|fi ) +</p>
    <p>k</p>
    <p>log p+LM0 (k )</p>
    <p>First train an exact model pLM ,</p>
    <p>pLM is then used for sampling and in the prior.</p>
    <p>Dynamic programming is used to efficiently calculate these feature expectations.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 13 / 22</p>
  </div>
  <div class="page">
    <p>Experimental Details</p>
    <p>All experiments are on data from the IWSLT Chinese-English evaluation.</p>
    <p>Training: 40k training sentences pairs drawn from a travel domain. 38k have a reachable reference.</p>
    <p>Development and Testing 2005 test set used for evaluation. 2004 test set used for development. Each has 16 references: one translation and 15 paraphrases.</p>
    <p>Features on each rule, word penalty and LM (2.9M features).</p>
    <p>Evaluation with various definitions of the BLEU brevity penalty.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 14 / 22</p>
  </div>
  <div class="page">
    <p>Evaluation: Beam approximation</p>
    <p>Model Max-translation (Sampling)</p>
    <p>pLM (e|f) 32.6 p+LM (e|f) (Z</p>
    <p>cb  (f)) 39.8</p>
    <p>p+LM (e|f) (Z sam  (f)) 40.6</p>
    <p>Both techniques for incorporating the LM are effective.</p>
    <p>Sampling has a slight advantage over the use of a beam.</p>
    <p>Sampling technique results in 50% smaller charts.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 15 / 22</p>
  </div>
  <div class="page">
    <p>Evaluation: Beam approximation</p>
    <p>Model Max-translation (Sampling)</p>
    <p>pLM (e|f) 32.6 p+LM (e|f) (Z</p>
    <p>cb  (f)) 39.8</p>
    <p>p+LM (e|f) (Z sam  (f)) 40.6</p>
    <p>Both techniques for incorporating the LM are effective.</p>
    <p>Sampling has a slight advantage over the use of a beam.</p>
    <p>Sampling technique results in 50% smaller charts.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 15 / 22</p>
  </div>
  <div class="page">
    <p>Evaluation: Beam approximation</p>
    <p>Model Max-translation (Sampling)</p>
    <p>pLM (e|f) 32.6 p+LM (e|f) (Z</p>
    <p>cb  (f)) 39.8</p>
    <p>p+LM (e|f) (Z sam  (f)) 40.6</p>
    <p>Both techniques for incorporating the LM are effective.</p>
    <p>Sampling has a slight advantage over the use of a beam.</p>
    <p>Sampling technique results in 50% smaller charts.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 15 / 22</p>
  </div>
  <div class="page">
    <p>Comparison with MERT</p>
    <p>Model BLEU NIST BLEU IBM BLEU FirstRef</p>
    <p>pLM (e|f) 33.5 35.2 25.2 p+LM (e|f) 44.6 44.6 31.2 MERT (BLEU NIST ) 46.2 44.5 30.2</p>
    <p>Our model optimises likelihood, not BLEU.</p>
    <p>The brevity penalty makes direct comparison difficult.</p>
    <p>Our training regime achieves comparable performance to MERT,</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 16 / 22</p>
  </div>
  <div class="page">
    <p>Source side syntactic features</p>
    <p>We exploit our models ability to incorporate sparse feature over the source sentence:</p>
    <p>We extract source syntax fragments for frequent (&gt;1) rule applications.</p>
    <p>Resulting in 4.2M syntactic features extracted (for a total of 7.1M).</p>
    <p>This avoids the issues associated with directly incorporating syntax into the grammar.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 17 / 22</p>
  </div>
  <div class="page">
    <p>Source side syntactic features</p>
    <p>Example</p>
    <p>V WH ?NN</p>
    <p>NP</p>
    <p>VP</p>
    <p>SQ</p>
    <p>Where is the currency exchange office ?</p>
    <p>(Step 2) X2 -&gt; &lt; [X1]   , Where is the [X1] ?&gt;</p>
    <p>(Step 1) X1 -&gt; &lt; , currency exchange office&gt;</p>
    <p>Example Derivation:</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 18 / 22</p>
  </div>
  <div class="page">
    <p>Source side syntactic features</p>
    <p>Example</p>
    <p>V WH ?NN</p>
    <p>NP</p>
    <p>VP</p>
    <p>SQ</p>
    <p>Where is the currency exchange office ?</p>
    <p>(Step 2) X2 -&gt; &lt; [X1]   , Where is the [X1] ?&gt;</p>
    <p>(Step 1) X1 -&gt; &lt; , currency exchange office&gt;</p>
    <p>NPExample Syntax feature = for Step 1</p>
    <p>Example Derivation:</p>
    <p>currency exchange</p>
    <p>office</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 18 / 22</p>
  </div>
  <div class="page">
    <p>Source side syntactic features</p>
    <p>Example</p>
    <p>V WH ?NN</p>
    <p>NP</p>
    <p>VP</p>
    <p>SQ</p>
    <p>Where is the currency exchange office ?</p>
    <p>(Step 2) X2 -&gt; &lt; [X1]   , Where is the [X1] ?&gt;</p>
    <p>(Step 1) X1 -&gt; &lt; , currency exchange office&gt;</p>
    <p>NP</p>
    <p>SQ</p>
    <p>?</p>
    <p>Where is the [X1] ? Example Syntax feature =</p>
    <p>for Step 2</p>
    <p>Example Derivation:</p>
    <p>X1</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 18 / 22</p>
  </div>
  <div class="page">
    <p>Source side syntactic features</p>
    <p>Model BLEU NIST BLEU IBM BLEU FirstRef</p>
    <p>p+LM (e|f) 44.6 44.6 31.2 p+LM (e|f) + syntax 45.3 45.2 31.8</p>
    <p>The model is able to gain advantage from sparse syntactic features,</p>
    <p>without being overwhelmed by the noise.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 19 / 22</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>There are many questions for future research:</p>
    <p>Should we iterate the training algorithm like MERT?</p>
    <p>Incorporate loss functions through expected loss.</p>
    <p>Can we do without word alignments, inducing the grammar through the models latent structure?</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 20 / 22</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>In this work we have:</p>
    <p>Presented approximate training algorithms for including a language model in a probabilistic discriminative model of translation.</p>
    <p>Demonstrated the flexibility of this modelling framework by estimating millions of noisy source conditioned syntactic features.</p>
    <p>Achieved results comparable to current state-of-the-art models.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 21 / 22</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Thank you.</p>
    <p>Blunsom, Osborne (UoE) Probabilistic Inference for MT EMNLP 2008 22 / 22</p>
  </div>
</Presentation>
