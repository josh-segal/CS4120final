<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Avoiding Communication in</p>
    <p>Linear Algebra</p>
    <p>Jim Demmel UC Berkeley</p>
    <p>bebop.cs.berkeley.edu</p>
  </div>
  <div class="page">
    <p>Motivation  Running time of an algorithm is sum of 3 terms:</p>
    <p># flops * time_per_flop  # words moved / bandwidth  # messages * latency</p>
    <p>communication</p>
  </div>
  <div class="page">
    <p>Motivation  Running time of an algorithm is sum of 3 terms:</p>
    <p># flops * time_per_flop  # words moved / bandwidth  # messages * latency</p>
    <p>Exponentially growing gaps between  Time_per_flop &lt;&lt; 1/Network BW &lt;&lt; Network Latency</p>
    <p>Improving 59%/year vs 26%/year vs 15%/year  Time_per_flop &lt;&lt; 1/Memory BW &lt;&lt; Memory Latency</p>
    <p>Improving 59%/year vs 23%/year vs 5.5%/year</p>
    <p>communication</p>
  </div>
  <div class="page">
    <p>Motivation  Running time of an algorithm is sum of 3 terms:</p>
    <p># flops * time_per_flop  # words moved / bandwidth  # messages * latency</p>
    <p>Exponentially growing gaps between  Time_per_flop &lt;&lt; 1/Network BW &lt;&lt; Network Latency</p>
    <p>Improving 59%/year vs 26%/year vs 15%/year  Time_per_flop &lt;&lt; 1/Memory BW &lt;&lt; Memory Latency</p>
    <p>Improving 59%/year vs 23%/year vs 5.5%/year</p>
    <p>Goal : reorganize linear algebra to avoid communication  Not just hiding communication (speedup  2x )  Arbitrary speedups possible</p>
    <p>communication</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Avoiding Communication in Dense Linear</p>
    <p>Algebra  Avoiding Communication in Sparse Linear</p>
    <p>Algebra</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Avoiding Communication in Dense Linear</p>
    <p>Algebra  Avoiding Communication in Sparse Linear</p>
    <p>Algebra  A poem in memory of Gene Golub (separate file)</p>
  </div>
  <div class="page">
    <p>Collaborators (so far)  UC Berkeley</p>
    <p>Kathy Yelick, Ming Gu  Mark Hoemmen, Marghoob Mohiyuddin, Kaushik Datta,</p>
    <p>George Petropoulos, Sam Williams, BeBOp group  Lenny Oliker, John Shalf</p>
    <p>CU Denver  Julien Langou</p>
    <p>INRIA  Laura Grigori, Hua Xiang</p>
    <p>Much related work  Complete references in technical reports</p>
  </div>
  <div class="page">
    <p>Why all our problems are solved for dense linear algebra in theory</p>
    <p>(Talk by Ioana Dumitriu on Monday)  Thm (D., Dumitriu, Holtz, Kleinberg) (Numer.Math. 2007)</p>
    <p>Given any matmul running in O(n) ops for some &gt;2, it can be made stable and still run in O(n+) ops, for any &gt;0.  Current record:   2.38</p>
    <p>Thm (D., Dumitriu, Holtz) (Numer. Math. 2008)  Given any stable matmul running in O(n+) ops, it is possible to</p>
    <p>do backward stable dense linear algebra in O(n+) ops:  GEPP, QR  rank revealing QR (randomized)  (Generalized) Schur decomposition, SVD (randomized)</p>
    <p>Also reduces communication to O(n+)  But constants?</p>
  </div>
  <div class="page">
    <p>Summary (1)  Avoiding Communication in Dense Linear Algebra</p>
    <p>QR or LU decomposition of m x n matrix, m &gt;&gt; n  Parallel implementation</p>
    <p>Conventional: O( n log p ) messages  New: O( log p ) messages - optimal</p>
    <p>Serial implementation with fast memory of size F  Conventional: O( mn/F ) moves of data from slow to fast memory</p>
    <p>mn/F = how many times larger matrix is than fast memory  New: O(1) moves of data - optimal</p>
    <p>Lots of speed up possible (measured and modeled)  Price: some redundant computation, stability?</p>
    <p>Extends to square case, with optimality results  Extends to other architectures (eg multicore)  (Talk by Julien Langou Monday, on QR)</p>
  </div>
  <div class="page">
    <p>Minimizing Comm. in Parallel QR</p>
    <p>W = W0 W1 W2 W3</p>
    <p>R00 R10 R20 R30</p>
    <p>R01</p>
    <p>R11</p>
    <p>R02</p>
    <p>QR decomposition of m x n matrix W, m &gt;&gt; n  TSQR = Tall Skinny QR  P processors, block row layout</p>
    <p>Usual Parallel Algorithm  Compute Householder vector for each column  Number of messages  n log P</p>
    <p>Communication Avoiding Algorithm  Reduction operation, with QR as operator  Number of messages  log P</p>
  </div>
  <div class="page">
    <p>TSQR in more detail</p>
    <p>.</p>
    <p>R</p>
    <p>R</p>
    <p>R</p>
    <p>R</p>
    <p>Q</p>
    <p>Q</p>
    <p>Q</p>
    <p>Q</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>. R</p>
    <p>R</p>
    <p>Q</p>
    <p>Q</p>
    <p>R</p>
    <p>R</p>
    <p>R</p>
    <p>R</p>
    <p>R</p>
    <p>Q is represented implicitly as a product (tree of factors)</p>
  </div>
  <div class="page">
    <p>Minimizing Communication in TSQR</p>
    <p>W = W0 W1 W2 W3</p>
    <p>R00 R10 R20 R30</p>
    <p>R01</p>
    <p>R11</p>
    <p>R02Parallel:</p>
    <p>W = W0 W1 W2 W3</p>
    <p>R01 R02</p>
    <p>R00</p>
    <p>R03 Sequential:</p>
    <p>W = W0 W1 W2 W3</p>
    <p>R00 R01</p>
    <p>R01 R11</p>
    <p>R02</p>
    <p>R11 R03</p>
    <p>Dual Core:</p>
    <p>Choose reduction tree dynamically Multicore / Multisocket / Multirack / Multisite / Out-of-core: ?</p>
  </div>
  <div class="page">
    <p>Performance of TSQR vs Sca/LAPACK</p>
    <p>Parallel  Pentium III cluster, Dolphin Interconnect, MPICH</p>
    <p>Up to 6.7x speedup (16 procs, 100K x 200)  BlueGene/L</p>
    <p>Up to 4x speedup (32 procs, 1M x 50)  Both use Elmroth-Gustavson locally  enabled by TSQR</p>
    <p>Sequential  OOC on PowerPC laptop</p>
    <p>As little as 2x slowdown vs (predicted) infinite DRAM  See UC Berkeley EECS Tech Report 2008-74</p>
  </div>
  <div class="page">
    <p>QR for General Matrices  CAQR  Communication Avoiding QR for general A</p>
    <p>Use TSQR for panel factorizations  Apply to rest of matrix</p>
    <p>Cost of CAQR vs ScaLAPACKs PDGEQRF  n x n matrix on P1/2 x P1/2 processor grid, block size b  Flops: (4/3)n3/P + (3/4)n2b log P/P1/2 vs (4/3)n3/P  Bandwidth: (3/4)n2 log P/P1/2 vs same  Latency: 2.5 n log P / b vs 1.5 n log P</p>
    <p>Close to optimal (modulo log P factors)  Assume: O(n2/P) memory/processor, O(n3) algorithm,  Choose b near n / P1/2 (its upper bound)  Bandwidth lower bound: (n2 /P1/2)  just log(P) smaller  Latency lower bound: (P1/2)  just polylog(P) smaller  Extension of Irony/Toledo/Tishkin (2004)</p>
    <p>Implementation  Juliens summer project</p>
  </div>
  <div class="page">
    <p>Modeled Speedups of CAQR vs ScaLAPACK</p>
    <p>Petascale up to 22.9x</p>
    <p>IBM Power 5 up to 9.7x</p>
    <p>Grid up to 11x</p>
    <p>Petascale machine with 8192 procs, each at 500 GFlops/s, a bandwidth of 4 GB/s. ./102,10,102 9512 wordsss</p>
  </div>
  <div class="page">
    <p>TSLU: LU factorization of a tall skinny matrix</p>
    <p>.</p>
    <p>U</p>
    <p>U</p>
    <p>U</p>
    <p>U</p>
    <p>L</p>
    <p>L</p>
    <p>L</p>
    <p>L</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>W</p>
    <p>..</p>
    <p>U</p>
    <p>U</p>
    <p>L</p>
    <p>L</p>
    <p>U</p>
    <p>U</p>
    <p>U</p>
    <p>U</p>
    <p>020202</p>
    <p>UL U</p>
    <p>U</p>
    <p>First try the obvious generalization of TSQR:</p>
  </div>
  <div class="page">
    <p>Growth factor for TSLU based factorization</p>
    <p>Unstable for large P and large matrices. When P = # rows, TSLU is equivalent to parallel pivoting.</p>
    <p>Courtesy of H. Xiang</p>
  </div>
  <div class="page">
    <p>Making TSLU Stable</p>
    <p>At each node in tree, TSLU selects b pivot rows from 2b candidates from its 2 child nodes</p>
    <p>At each node, do LU on 2b original rows selected by child nodes, not U factors from child nodes</p>
    <p>When TSLU done, permute b selected rows to top of original matrix, redo b steps of LU without pivoting</p>
    <p>CALU  Communication Avoiding LU for general A  Use TSLU for panel factorizations  Apply to rest of matrix  Cost: redundant panel factorizations</p>
    <p>Benefit:  Stable in practice, but not same pivot choice as GEPP  b times fewer messages overall - faster</p>
  </div>
  <div class="page">
    <p>Growth factor for better CALU approach</p>
    <p>Like threshold pivoting with worst case threshold = .33 , so |L| &lt;= 3 Testing shows about same residual as GEPP</p>
  </div>
  <div class="page">
    <p>Performance vs ScaLAPACK  TSLU</p>
    <p>IBM Power 5  Up to 4.37x faster (16 procs, 1M x 150)</p>
    <p>Cray XT4  Up to 5.52x faster (8 procs, 1M x 150)</p>
    <p>CALU  IBM Power 5</p>
    <p>Up to 2.29x faster (64 procs, 1000 x 1000)  Cray XT4</p>
    <p>Up to 1.81x faster (64 procs, 1000 x 1000)  Optimality analysis analogous to QR  See INRIA Tech Report 6523 (2008)</p>
  </div>
  <div class="page">
    <p>Petascale machine with 8192 procs, each at 500 GFlops/s, a bandwidth of 4 GB/s.</p>
    <p>Speedup prediction for a Petascale machine - up to 81x faster</p>
    <p>./102,10,102 9512 wordsss</p>
    <p>P = 8192</p>
  </div>
  <div class="page">
    <p>Summary (2)  Avoiding Communication in Sparse Linear Algebra</p>
    <p>Take k steps of Krylov subspace method  GMRES, CG, Lanczos, Arnoldi  Assume matrix well-partitioned, with modest surface</p>
    <p>to-volume ratio  Parallel implementation</p>
    <p>Conventional: O(k log p) messages  New: O(log p) messages - optimal</p>
    <p>Serial implementation  Conventional: O(k) moves of data from slow to fast memory  New: O(1) moves of data  optimal</p>
    <p>Can incorporate some preconditioners  Hierarchical, semiseparable matrices</p>
    <p>Lots of speed up possible (modeled and measured)  Price: some redundant computation</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Locally Dependent Entries for [x,Ax], A tridiagonal, 2 processors</p>
    <p>Can be computed without communication</p>
    <p>Proc 1 Proc 2</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Can be computed without communication</p>
    <p>Proc 1 Proc 2</p>
    <p>Locally Dependent Entries for [x,Ax,A2x], A tridiagonal, 2 processors</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Can be computed without communication</p>
    <p>Proc 1 Proc 2</p>
    <p>Locally Dependent Entries for [x,Ax,,A3x], A tridiagonal, 2 processors</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Can be computed without communication</p>
    <p>Proc 1 Proc 2</p>
    <p>Locally Dependent Entries for [x,Ax,,A4x], A tridiagonal, 2 processors</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Locally Dependent Entries for [x,Ax,,A8x], A tridiagonal, 2 processors</p>
    <p>Can be computed without communication k=8 fold reuse of A</p>
    <p>Proc 1 Proc 2</p>
  </div>
  <div class="page">
    <p>Remotely Dependent Entries for [x,Ax,,A8x], A tridiagonal, 2 processors</p>
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>One message to get data needed to compute remotely dependent entries, not k=8 Minimizes number of messages = latency cost</p>
    <p>Price: redundant work  surface/volume ratio</p>
    <p>Proc 1 Proc 2</p>
  </div>
  <div class="page">
    <p>Fewer Remotely Dependent Entries for [x,Ax,,A8x], A tridiagonal, 2 processors</p>
    <p>x</p>
    <p>Ax</p>
    <p>A2x</p>
    <p>A3x</p>
    <p>A4x</p>
    <p>A5x</p>
    <p>A6x</p>
    <p>A7x</p>
    <p>A8x</p>
    <p>Reduce redundant work by half</p>
    <p>Proc 1 Proc 2</p>
  </div>
  <div class="page">
    <p>Remotely Dependent Entries for [x,Ax,A2x,A3x], A irregular, multiple processors</p>
  </div>
  <div class="page">
    <p>Sequential [x,Ax,,A4x], with memory hierarchy</p>
    <p>v</p>
    <p>One read of matrix from slow memory, not k=4 Minimizes words moved = bandwidth cost</p>
    <p>No redundant work</p>
  </div>
  <div class="page">
    <p>Performance Results</p>
    <p>Measured  Sequential/OOC speedup up to 3x</p>
    <p>Modeled  Sequential/multicore speedup up to 2.5x  Parallel/Petascale speedup up to 6.9x  Parallel/Grid speedup up to 22x</p>
    <p>See bebop.cs.berkeley.edu/#pubs</p>
  </div>
  <div class="page">
    <p>Optimizing Communication Complexity of Sparse Solvers</p>
    <p>Example: GMRES for Ax=b on 2D Mesh  x lives on n-by-n mesh  Partitioned on p -by- p grid  A has 5 point stencil (Laplacian)</p>
    <p>(Ax)(i,j) = linear_combination(x(i,j), x(i,j1), x(i1,j))  Ex: 18-by-18 mesh on 3-by-3 grid</p>
  </div>
  <div class="page">
    <p>Minimizing Communication of GMRES  What is the cost = (#flops, #words, #mess) of</p>
    <p>k steps of standard GMRES?</p>
    <p>GMRES, ver.1: for i=1 to k w = A * v(i-1) MGS(w, v(0),,v(i-1)) update v(i), H endfor solve LSQ problem with H n/p</p>
    <p>n/p</p>
    <p>Cost(A * v) = k * (9n2 /p, 4n / p , 4 )  Cost(MGS) = k2/2 * ( 4n2 /p , log p , log p )  Total cost ~ Cost( A * v ) + Cost (MGS)  Can we reduce the latency?</p>
  </div>
  <div class="page">
    <p>Minimizing Communication of GMRES  Cost(GMRES, ver.1) = Cost(A*v) + Cost(MGS)</p>
    <p>Cost(W) = ( ~ same, ~ same , 8 )  Latency cost independent of k  optimal</p>
    <p>Cost (MGS) unchanged  Can we reduce the latency more?</p>
    <p>= ( 9kn2 /p, 4kn / p , 4k ) + ( 2k2n2 /p , k2 log p / 2 , k2 log p / 2 )</p>
    <p>How much latency cost from A*v can you avoid? Almost all</p>
    <p>GMRES, ver. 2: W = [ v, Av, A2v,  , Akv ] [Q,R] = MGS(W) Build H from R, solve LSQ problem</p>
    <p>k = 3</p>
  </div>
  <div class="page">
    <p>Minimizing Communication of GMRES  Cost(GMRES, ver. 2) = Cost(W) + Cost(MGS)</p>
    <p>= ( 9kn2 /p, 4kn / p , 8 ) + ( 2k2n2 /p , k2 log p / 2 , k2 log p / 2 )</p>
    <p>How much latency cost from MGS can you avoid? Almost all</p>
    <p>Cost(TSQR) = ( ~ same, ~ same , log p )  Latency cost independent of s - optimal</p>
    <p>GMRES, ver. 3: W = [ v, Av, A2v,  , Akv ] [Q,R] = TSQR(W)  Tall Skinny QR Build H from R, solve LSQ problem</p>
    <p>W = W1 W2 W3 W4</p>
    <p>R1 R2 R3 R4</p>
    <p>R12</p>
    <p>R34</p>
    <p>R1234</p>
  </div>
  <div class="page">
    <p>Minimizing Communication of GMRES  Cost(GMRES, ver. 2) = Cost(W) + Cost(MGS)</p>
    <p>= ( 9kn2 /p, 4kn / p , 8 ) + ( 2k2n2 /p , k2 log p / 2 , k2 log p / 2 )</p>
    <p>How much latency cost from MGS can you avoid? Almost all</p>
    <p>Cost(TSQR) = ( ~ same, ~ same , log p )  Oops</p>
    <p>GMRES, ver. 3: W = [ v, Av, A2v,  , Akv ] [Q,R] = TSQR(W)  Tall Skinny QR Build H from R, solve LSQ problem</p>
    <p>W = W1 W2 W3 W4</p>
    <p>R1 R2 R3 R4</p>
    <p>R12</p>
    <p>R34</p>
    <p>R1234</p>
  </div>
  <div class="page">
    <p>Minimizing Communication of GMRES  Cost(GMRES, ver. 2) = Cost(W) + Cost(MGS)</p>
    <p>= ( 9kn2 /p, 4kn / p , 8 ) + ( 2k2n2 /p , k2 log p / 2 , k2 log p / 2 )</p>
    <p>How much latency cost from MGS can you avoid? Almost all</p>
    <p>Cost(TSQR) = ( ~ same, ~ same , log p )  Oops  W from power method, precision lost!</p>
    <p>GMRES, ver. 3: W = [ v, Av, A2v,  , Akv ] [Q,R] = TSQR(W)  Tall Skinny QR Build H from R, solve LSQ problem</p>
    <p>W = W1 W2 W3 W4</p>
    <p>R1 R2 R3 R4</p>
    <p>R12</p>
    <p>R34</p>
    <p>R1234</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Minimizing Communication of GMRES  Cost(GMRES, ver. 3) = Cost(W) + Cost(TSQR)</p>
    <p>= ( 9kn2 /p, 4kn / p , 8 ) + ( 2k2n2 /p , k2 log p / 2 , log p )</p>
    <p>Latency cost independent of k, just log p  optimal  Oops  W from power method, so precision lost  What to do?</p>
    <p>Use a different polynomial basis  Not Monomial basis W = [v, Av, A2v, ], instead   Newton Basis WN = [v, (A  1 I)v , (A  2 I)(A  1 I)v, ] or  Chebyshev Basis WC = [v, T1(v), T2(v), ]</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Summary and Conclusions (1/2)</p>
    <p>Possible to minimize communication complexity of much dense and sparse linear algebra  Practical speedups  Approaching theoretical lower bounds</p>
    <p>Optimal asymptotic complexity algorithms for dense linear algebra  also lower communication</p>
    <p>Hardware trends mean the time has come to do this</p>
    <p>Lots of prior work (see pubs)  and some new</p>
  </div>
  <div class="page">
    <p>Summary and Conclusions (2/2)</p>
    <p>Many open problems  Automatic tuning - build and optimize complicated</p>
    <p>data structures, communication patterns, code automatically: bebop.cs.berkeley.edu</p>
    <p>Extend optimality proofs to general architectures  Dense eigenvalue problems  SBR or spectral D&amp;C?  Sparse direct solvers  CALU or SuperLU?  Which preconditioners work?  Why stop at linear algebra?</p>
  </div>
</Presentation>
