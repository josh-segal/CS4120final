<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Your computer is already a distributed system.</p>
    <p>Why isnt your OS?</p>
    <p>Andrew Baumann</p>
    <p>Simon Peter, Adrian Schpbach, Akhilesh Singhania, Timothy Roscoe</p>
    <p>Paul Barham, Rebecca Isaacs</p>
    <p>Systems Group, ETH Zurich Microsoft Research, Cambridge</p>
    <p>cSystems Group | Department of Computer Science | ETH Zurich HotOS, 19th May 2009</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>I Observation: Modern multicore hardware is a network, and exhibits classic networking effects</p>
    <p>I The OS should be designed as a distributed system</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Observations Latency Heterogeneity Dynamic changes</p>
    <p>Implications Message passing vs. shared memory Replication and consistency Heterogeneity</p>
    <p>The multikernel architecture</p>
  </div>
  <div class="page">
    <p>Observations Does this look like a network to you?</p>
  </div>
  <div class="page">
    <p>Communication latency Cycles to access cache from core 0</p>
    <p>I Can shared data structures take advantage of this?</p>
  </div>
  <div class="page">
    <p>Communication latency Cycles to access cache from core 0</p>
    <p>I Can shared data structures take advantage of this?</p>
  </div>
  <div class="page">
    <p>Communication latency Cycles to access cache from core 0</p>
    <p>I Can shared data structures take advantage of this? 19.05.2009 Your computer is already a distributed system. Why isnt your OS? 5</p>
  </div>
  <div class="page">
    <p>Node heterogeneity</p>
    <p>I Within a system: I Programmable NICs I GPUs I FPGAs (in CPU sockets)</p>
    <p>I Architectural differences on a single die: I Streaming instructions (SIMD, SSE, etc.) I Virtualisation support, power management</p>
    <p>I Existing OS architectures have trouble accommodating this</p>
  </div>
  <div class="page">
    <p>Node heterogeneity</p>
    <p>I Within a system: I Programmable NICs I GPUs I FPGAs (in CPU sockets)</p>
    <p>I Architectural differences on a single die: I Streaming instructions (SIMD, SSE, etc.) I Virtualisation support, power management</p>
    <p>I Existing OS architectures have trouble accommodating this</p>
  </div>
  <div class="page">
    <p>Dynamic changes</p>
    <p>I Hot-plug of devices, memory, (cores?) I Power-management</p>
    <p>I Partial failure</p>
  </div>
  <div class="page">
    <p>Dynamic changes</p>
    <p>I Hot-plug of devices, memory, (cores?) I Power-management I Partial failure</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>I Latency, heterogeneity, dynamic changes I All classic characteristics of a distributed, networked system I Why dont we program the machine this way?</p>
  </div>
  <div class="page">
    <p>The OS as a distributed system</p>
    <p>What are the implications of building an OS as a distributed system?</p>
    <p>I Extreme position: clean slate design I Fully explore ramifications I No regard for compatibility</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Observations Latency Heterogeneity Dynamic changes</p>
    <p>Implications Message passing vs. shared memory Replication and consistency Heterogeneity</p>
    <p>The multikernel architecture</p>
  </div>
  <div class="page">
    <p>Message passing vs. shared memory</p>
    <p>I Access to remote shared data can be seen as a blocking RPC I Processor stalled while line is fetched or invalidated I Limited by latency of interconnect round-trips</p>
    <p>I Performance scales with size of data (number of cache lines)</p>
    <p>I By sending an explicit RPC (message), we: I Send a compact high-level description of the operation I Reduce the time spent blocked, waiting for the interconnect</p>
    <p>I Potential for more efficient use of interconnect bandwidth I Cf. RPC vs. DSVM in distributed systems</p>
  </div>
  <div class="page">
    <p>Message passing vs. shared memory</p>
    <p>I Access to remote shared data can be seen as a blocking RPC I Processor stalled while line is fetched or invalidated I Limited by latency of interconnect round-trips</p>
    <p>I Performance scales with size of data (number of cache lines) I By sending an explicit RPC (message), we:</p>
    <p>I Send a compact high-level description of the operation I Reduce the time spent blocked, waiting for the interconnect</p>
    <p>I Potential for more efficient use of interconnect bandwidth I Cf. RPC vs. DSVM in distributed systems</p>
  </div>
  <div class="page">
    <p>Why message passing?</p>
    <p>I We can reason about it I Decouples system structure from</p>
    <p>inter-core communication mechanism I Communication patterns explicitly expressed I Naturally supports heterogeneous cores I Naturally supports non-coherent interconnects (PCIe)</p>
    <p>I Better match for future hardware I . . . with cheap explicit message passing (e.g. Tile64) I . . . without cache-coherence (e.g. Intel 80-core)</p>
  </div>
  <div class="page">
    <p>Message passing vs. shared memory: tradeoff 24-core Intel (shared bus)</p>
    <p>La te</p>
    <p>n cy</p>
    <p>( cy</p>
    <p>cl e</p>
    <p>s</p>
    <p>Cache lines</p>
    <p>Shared: clients modify shared array (no locking) Message: URPC to server core 19.05.2009 Your computer is already a distributed system. Why isnt your OS? 13</p>
  </div>
  <div class="page">
    <p>Replication Given no sharing, what do we do with the state?</p>
    <p>I Some state naturally partitions I Other state must be replicated I Used as an optimisation in previous systems:</p>
    <p>Tornado, K42 clustered objects Linux read-only data, kernel text</p>
    <p>I We argue that replication should be the default</p>
  </div>
  <div class="page">
    <p>Consistency</p>
    <p>I How do we maintain consistency of replicated data? I Depends on consistency and ordering requirements, e.g.:</p>
    <p>TLBs (unmap) single-phase commit Memory reallocation (capabilities) two-phase commit Cores come and go (power management, hotplug) agreement</p>
  </div>
  <div class="page">
    <p>Change of programming model: why wait?</p>
    <p>I In a traditional OS, achieving consistency implies blocking I e.g. unmap, global TLB shootdown</p>
    <p>Idea: change programming model: I Dont wait: do something else in the meantime I Make such operations split-phase from user space</p>
    <p>I Propose a change, receive success/failure notification = tradeoff latency vs. overhead</p>
  </div>
  <div class="page">
    <p>Heterogeneity</p>
    <p>I Message-based communication handles core heterogeneity I Can specialise implementation and data structures</p>
    <p>I Doesnt deal with other aspects I What should run where? I How should complex resources be allocated?</p>
    <p>I Our solution based on constraint logic programming [Schpbach et al., MMCS08]</p>
    <p>I System knowledge base stores rich, detailed representation of hardware, performs online reasoning</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Observations Latency Heterogeneity Dynamic changes</p>
    <p>Implications Message passing vs. shared memory Replication and consistency Heterogeneity</p>
    <p>The multikernel architecture</p>
  </div>
  <div class="page">
    <p>The multikernel architecture</p>
  </div>
  <div class="page">
    <p>Optimisation Sharing as an optimisation in multikernels</p>
    <p>I Weve replaced shared memory with explicit messaging I But sharing/locking might be faster between some cores</p>
    <p>I Hyperthreads, or cores with shared L2/3 cache = Re-introduce shared memory as optimisation</p>
    <p>I Hidden, local I Only when faster, as decided at runtime I Basic model remains split-phase</p>
  </div>
  <div class="page">
    <p>Conclusion I Modern computers are inherently distributed systems</p>
    <p>I Communication latency, network effects I Heterogeneity I Dynamic behaviour</p>
    <p>I We should be programming them as such I Message passing vs. sharing I Replication, consistency I Explicit management of heterogeneity</p>
    <p>I Multikernel: a new OS architecture based on these ideas</p>
    <p>I Barrelfish: our implementation</p>
    <p>www.barrelfish.org</p>
  </div>
  <div class="page">
    <p>Conclusion I Modern computers are inherently distributed systems</p>
    <p>I Communication latency, network effects I Heterogeneity I Dynamic behaviour</p>
    <p>I We should be programming them as such I Message passing vs. sharing I Replication, consistency I Explicit management of heterogeneity</p>
    <p>I Multikernel: a new OS architecture based on these ideas</p>
    <p>I Barrelfish: our implementation</p>
    <p>www.barrelfish.org</p>
  </div>
</Presentation>
