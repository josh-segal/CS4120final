<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Case for NUMA-aware Contention Management on Multicore Systems</p>
    <p>Sergey Blagodurov sergey_blagodurov@sfu.ca Sergey Zhuravlev sergey_zhuravlev@sfu.ca Mohammad Dashti mohammad_dashti@sfu.ca Alexandra Fedorova alexandra_fedorova@sfu.ca</p>
    <p>USENIX ATC11 / Scheduling session 15th of June</p>
  </div>
  <div class="page">
    <p>Memory</p>
    <p>Controller HyperTransport</p>
    <p>Shared L3 Cache</p>
    <p>System Request Interface Crossbar switch</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>NUMA Domain 0</p>
    <p>to other domains</p>
    <p>USENIX ATC11 / Scheduling session</p>
    <p>An AMD Opteron 8356 Barcelona domain</p>
    <p>-2</p>
  </div>
  <div class="page">
    <p>An AMD Opteron system with 4 domains</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>USENIX ATC11 / Scheduling session -3</p>
  </div>
  <div class="page">
    <p>Contention for the shared last-level cache (CA)</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>USENIX ATC11 / Scheduling session -4</p>
  </div>
  <div class="page">
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>Contention for the memory controller (MC)</p>
    <p>USENIX ATC11 / Scheduling session -5</p>
  </div>
  <div class="page">
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>Contention for the inter-domain interconnect (IC)</p>
    <p>USENIX ATC11 / Scheduling session -6</p>
  </div>
  <div class="page">
    <p>USENIX ATC11 / Scheduling session</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>Memory node 0</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>Remote access latency (RL)</p>
    <p>A</p>
    <p>-7</p>
  </div>
  <div class="page">
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>USENIX ATC11 / Scheduling session</p>
    <p>A B</p>
    <p>Memory node 0</p>
    <p>Isolating Memory controller contention (MC)</p>
    <p>-8</p>
  </div>
  <div class="page">
    <p>Memory Controller (MC) and InterConnect (IC)</p>
    <p>contention are key factors hurting performance</p>
    <p>Dominant degradation factors</p>
    <p>USENIX ATC11 / Scheduling session -9</p>
  </div>
  <div class="page">
    <p>Characterization method  Given two threads, decide if they will hurt each</p>
    <p>others performance if co-scheduled</p>
    <p>Scheduling algorithm</p>
    <p>Separate threads that are expected to interfere</p>
    <p>A B</p>
    <p>A B</p>
    <p>Contention-Aware Scheduling</p>
    <p>USENIX ATC11 / Scheduling session -10</p>
  </div>
  <div class="page">
    <p>Limited observability  We do not know for sure if threads compete and how severely  Hardware does not tell us</p>
    <p>Trial and error infeasible on large systems</p>
    <p>Cant try all possible combinations  Even sampling becomes difficult</p>
    <p>A good trade-off: measure LLC Miss rate!</p>
    <p>Assumes that threads interfere if they have high miss rates  No account for cache contention impact  Works well because cache contention is not dominant</p>
    <p>Characterization Method</p>
    <p>USENIX ATC11 / Scheduling session -11</p>
  </div>
  <div class="page">
    <p>Sort threads by LLC missrate:</p>
    <p>Goal: isolate threads that compete for shared resources</p>
    <p>High contention: Low contention?</p>
    <p>A B</p>
    <p>A B</p>
    <p>X Y</p>
    <p>C D</p>
    <p>Domain 1 Domain 2 Domain 1 Domain 2</p>
    <p>Migrate competing threads to different domains</p>
    <p>Memory node 1</p>
    <p>MC HT</p>
    <p>Our previous work: an algorithm for UMA systems Distributed Intensity (DI-Plain) USENIX ATC11 / Scheduling session</p>
    <p>A</p>
    <p>B</p>
    <p>Memory node 2</p>
    <p>MC HT MC HT</p>
    <p>Memory node 2</p>
    <p>Memory node 1</p>
    <p>MC HT X</p>
    <p>Y A</p>
    <p>B X</p>
    <p>Y</p>
    <p>-12</p>
  </div>
  <div class="page">
    <p>USENIX ATC11 / Scheduling session</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>Failing to migrate memory leaves MC and introduces RL</p>
    <p>A B</p>
    <p>-13</p>
    <p>Memory node 0</p>
    <p>Shared L3 Cache</p>
  </div>
  <div class="page">
    <p>DI-Plain hurts performance on NUMA systems because it does not migrate memory! USENIX ATC11 / Scheduling session</p>
    <p>SPEC CPU 2006 SPEC MPI 2007</p>
    <p>% im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r D</p>
    <p>E FA</p>
    <p>U LT</p>
    <p>-14</p>
  </div>
  <div class="page">
    <p>Goal: isolate threads that compete for shared resources and pull the memory to the local node upon migration</p>
    <p>A B C D</p>
    <p>Domain 1 Domain 2 Domain 1 Domain 2</p>
    <p>Migrate competing threads along with memory to different domains</p>
    <p>Memory node 1</p>
    <p>MC HT</p>
    <p>Solution #1: Distributed Intensity with memory migration (DI-Migrate) USENIX ATC11 / Scheduling session</p>
    <p>A</p>
    <p>B</p>
    <p>Memory node 2</p>
    <p>MC HT MC HT</p>
    <p>Memory node 2</p>
    <p>Memory node 1</p>
    <p>MC HT X</p>
    <p>Y A</p>
    <p>B X</p>
    <p>Y</p>
    <p>Sort threads by LLC missrate: A B X Y</p>
    <p>-15</p>
  </div>
  <div class="page">
    <p>DI-Migrate performs too many migrations for MPI. Migrations are expensive on NUMA systems. USENIX ATC11 / Scheduling session</p>
    <p>SPEC CPU 2006 (low migration rate)</p>
    <p>SPEC MPI 2007 (high migration rate)</p>
    <p>% im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r D</p>
    <p>E FA</p>
    <p>U LT</p>
    <p>-16</p>
  </div>
  <div class="page">
    <p>USENIX ATC11 / Scheduling session</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 0 L1, L2 cache</p>
    <p>Core 4 L1, L2 cache</p>
    <p>Core 8 L1, L2 cache</p>
    <p>Core 12 L1, L2 cache</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 2 L1, L2 cache</p>
    <p>Memory node 2</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Shared L3 Cache</p>
    <p>Core 3 L1, L2 cache</p>
    <p>Core 7 L1, L2 cache</p>
    <p>Core 11 L1, L2 cache</p>
    <p>Core 15 L1, L2 cache</p>
    <p>Memory node 1</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>MC HT</p>
    <p>Memory node 3</p>
    <p>N U</p>
    <p>M A</p>
    <p>D om</p>
    <p>ai n</p>
    <p>Core 6 L1, L2 cache</p>
    <p>Core 10 L1, L2 cache</p>
    <p>Core 14 L1, L2 cache</p>
    <p>Shared L3 Cache</p>
    <p>Core 1 L1, L2 cache</p>
    <p>Core 5 L1, L2 cache</p>
    <p>Core 9 L1, L2 cache</p>
    <p>Core 13 L1, L2 cache</p>
    <p>Migrating too frequently causes IC</p>
    <p>A B</p>
    <p>-13</p>
    <p>Memory node 0</p>
    <p>MC</p>
    <p>Memory node 1</p>
    <p>Shared L3 Cache</p>
  </div>
  <div class="page">
    <p>DI-Migrate: threads sorted by miss rate if array positions change, we migrate thread and memory DINO: threads sorted by class only migrate if we jump from one class to another</p>
    <p>USENIX ATC11 / Scheduling session</p>
    <p>Solution #2: Distributed Intensity NUMA Online (DINO)</p>
    <p>C1 &lt;= 10 10 &lt; C2 &lt;= 100 100 &lt; C3</p>
    <p>C1 &lt;= 10 10 &lt; C2 &lt;= 100 100 &lt; C3</p>
    <p>-17</p>
  </div>
  <div class="page">
    <p>Loose correlation between miss rate and degradation, so most migrations will not payoff USENIX ATC11 / Scheduling session -18</p>
  </div>
  <div class="page">
    <p>Average number of memory migrations per hour of execution (DI-Migrate and DINO)</p>
    <p>USENIX ATC11 / Scheduling session</p>
    <p>DINO significantly reduces the number of migrations</p>
    <p>-19</p>
  </div>
  <div class="page">
    <p>DINO results</p>
    <p>USENIX ATC11 / Scheduling session</p>
    <p>SPEC CPU 2006 SPEC MPI 2007 LAMP</p>
    <p>% im</p>
    <p>pr ov</p>
    <p>em en</p>
    <p>t o ve</p>
    <p>r D</p>
    <p>E FA</p>
    <p>U LT</p>
    <p>-20</p>
  </div>
  <div class="page">
    <p>On NUMA systems we need to schedule threads and memory  Memory Controller contention when memory</p>
    <p>is not migrated  Interconnect Contention when memory</p>
    <p>is migrated too frequently</p>
    <p>DINO is the contention-aware scheduling algorithm for NUMA systems that</p>
    <p>migrates the memory along with the application  eliminates excessive migrations by trying to keep the workload</p>
    <p>on their old nodes, if possible  utilizes Instruction Based Sampling to perform partial memory</p>
    <p>migration of hot pages</p>
    <p>Summary</p>
    <p>USENIX ATC11 / Scheduling session -21</p>
  </div>
  <div class="page">
    <p>Read our Linux Symposium 2011 paper:</p>
    <p>User-level scheduling on NUMA multicore systems under Linux</p>
    <p>Source code is available at: http://clavis.sourceforge.net</p>
    <p>For further information</p>
    <p>USENIX ATC11 / Scheduling session -22</p>
  </div>
  <div class="page">
    <p>Any [time for] questions?</p>
    <p>A Case for NUMA-aware Contention Management on Multicore Systems USENIX ATC11 / Scheduling session</p>
  </div>
</Presentation>
