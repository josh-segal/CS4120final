<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Brent Stephens Aditya Akella, Mike Swift</p>
    <p>NSDI 2019</p>
    <p>Loom: Flexible and Efficient NIC Packet Scheduling</p>
  </div>
  <div class="page">
    <p>Loom is a new Network Interface Card (NIC) design that offloads all per-flow scheduling decisions out of the OS and into the NIC</p>
    <p>Why is packet scheduling important?  What is wrong with current NICs?  Why should all packet scheduling be</p>
    <p>offloaded to the NIC? 42</p>
  </div>
  <div class="page">
    <p>Why is packet scheduling important?</p>
  </div>
  <div class="page">
    <p>Collocation (Application and Tenant) is Important for Infrastructure Efficiency</p>
    <p>Tenant 1 Tenant 2 CPU Isolation Policy: Tenant 1:</p>
    <p>Memcached: 3 cores Spark: 1 core</p>
    <p>Tenant 2: Spark: 4 cores</p>
  </div>
  <div class="page">
    <p>Network Performance Goals Different applications have differing network performance goals</p>
    <p>Low Latency High Throughput</p>
  </div>
  <div class="page">
    <p>Network Policies</p>
    <p>Network operators must specify and enforce a network isolation policy  Enforcing a network isolation policy requires scheduling</p>
    <p>Pri_1</p>
    <p>VM1VM1 Pseudocode</p>
    <p>Tenant_1.Memcached -&gt; Pri_1:high Tenant_1.Spark -&gt; Pri_1:low Pri_1 -&gt; RL_WAN(Dst == WAN: 15Gbps) Pri_1 -&gt; RL_None(Dst != WAN: No Limit) RL_WAN -&gt; FIFO_1; RL_None -&gt; FIFO_1 FIFO_1-&gt; Fair_1:w1 Tenants_2.Spark -&gt; Fair_1:w1 Fair_1 -&gt; Wire</p>
  </div>
  <div class="page">
    <p>Network Policies</p>
    <p>Network operators must specify and enforce a network isolation policy  Enforcing a network isolation policy requires scheduling</p>
    <p>Pri_1</p>
    <p>VM1VM1 Pseudocode</p>
    <p>Tenant_1.Memcached -&gt; Pri_1:high Tenant_1.Spark -&gt; Pri_1:low Pri_1 -&gt; RL_WAN(Dst == WAN: 15Gbps) Pri_1 -&gt; RL_None(Dst != WAN: No Limit) RL_WAN -&gt; FIFO_1; RL_None -&gt; FIFO_1 FIFO_1-&gt; Fair_1:w1 Tenants_2.Spark -&gt; Fair_1:w1 Fair_1 -&gt; Wire</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>Wire</p>
    <p>Network Policies</p>
    <p>Network operators must specify and enforce a network isolation policy  Enforcing a network isolation policy requires scheduling</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1 Pseudocode</p>
    <p>Tenant_1.Memcached -&gt; Pri_1:high Tenant_1.Spark -&gt; Pri_1:low Pri_1 -&gt; RL_WAN(Dst == WAN: 15Gbps) Pri_1 -&gt; RL_None(Dst != WAN: No Limit) RL_WAN -&gt; FIFO_1; RL_None -&gt; FIFO_1 FIFO_1-&gt; Fair_1:w1 Tenants_2.Spark -&gt; Fair_1:w1 Fair_1 -&gt; Wire</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>What is wrong with current NICs?</p>
  </div>
  <div class="page">
    <p>Single Queue Packet Scheduling Limitations</p>
    <p>Single core throughput is limited (although high with Eiffel)  Especially with very small packets  Energy-efficient architectures may</p>
    <p>prioritize scalability over single-core performance</p>
    <p>Software scheduling consumes CPU</p>
    <p>Core-to-core communication increases latency</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>App 1 App 2</p>
    <p>NIC</p>
    <p>SQ struggles to drive line-rate</p>
  </div>
  <div class="page">
    <p>Multi Queue NIC Background and Limitations  Multi-queue NICs enable parallelism</p>
    <p>Throughput can be scaled across many tens of cores</p>
    <p>Multi-queue NICs have packet scheduler that chose which queue to send packets from</p>
    <p>The one-queue-per-core multi-queue model (MQ) attempts to enforces the policy at every core independently  This is the best possible without inter</p>
    <p>core coordination, but it is not effective</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>App 1 App 2</p>
    <p>NIC</p>
    <p>MQ struggles to enforce policies!</p>
  </div>
  <div class="page">
    <p>MQ Scheduler Problems</p>
    <p>CPU</p>
    <p>NIC (Network</p>
    <p>Interface Card)</p>
    <p>Time (t)</p>
    <p>Nave NIC packet scheduling prevents</p>
    <p>colocation!</p>
    <p>It leads to:  High latency  Unfair and variable</p>
    <p>throughput</p>
    <p>Packet Scheduler</p>
  </div>
  <div class="page">
    <p>MQ Scheduler Problems</p>
    <p>CPU</p>
    <p>NIC (Network</p>
    <p>Interface Card)</p>
    <p>Time (t)</p>
    <p>Nave NIC packet scheduling prevents</p>
    <p>colocation!</p>
    <p>It leads to:  High latency  Unfair and variable</p>
    <p>throughput</p>
    <p>Packet Scheduler</p>
  </div>
  <div class="page">
    <p>Why should all packet scheduling be offloaded to the NIC?</p>
  </div>
  <div class="page">
    <p>Where to divide labor between the OS and NIC?</p>
    <p>CPU</p>
    <p>NIC Wire</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>Where to divide labor between the OS and NIC?</p>
    <p>CPU</p>
    <p>NIC Option 1: Single Queue (SQ)</p>
    <p>Enforce entire policy in software  Low Tput/High CPU Utilization Wire</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>Where to divide labor between the OS and NIC?</p>
    <p>CPU</p>
    <p>NIC Option 1: Single Queue (SQ)</p>
    <p>Enforce entire policy in software  Low Tput/High CPU Utilization</p>
    <p>Option 2: Multi Queue (MQ)  Every core independently enforces</p>
    <p>policy on local traffic  Cannot ensure polices are</p>
    <p>enforced</p>
    <p>Wire</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>Where to divide labor between the OS and NIC?</p>
    <p>CPU</p>
    <p>NIC Option 1: Single Queue (SQ)</p>
    <p>Enforce entire policy in software  Low Tput/High CPU Utilization</p>
    <p>Option 2: Multi Queue (MQ)  Every core independently enforces</p>
    <p>policy on local traffic  Cannot ensure polices are</p>
    <p>enforced</p>
    <p>Option 3: Loom  Every flow uses its own queue  All policy enforcement is offloaded to</p>
    <p>the NIC  Precise policy + low CPU</p>
    <p>Wire</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
  </div>
  <div class="page">
    <p>Loom is a new NIC design that moves all per-flow scheduling decisions out</p>
    <p>of the OS and into the NIC</p>
    <p>Loom uses a queue per flow and offloads all packet scheduling to the NIC</p>
  </div>
  <div class="page">
    <p>Core Problem:</p>
    <p>It is not currently possible to offload all packet scheduling because NIC packet schedulers are inflexible and configuring them is inefficient</p>
  </div>
  <div class="page">
    <p>Core Problem:</p>
    <p>It is not currently possible to offload all packet scheduling because NIC packet schedulers are inflexible and configuring them is inefficient</p>
    <p>NIC packet schedulers are currently standing in the way of performance isolation!</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Intro: Loom is a new NIC design that moves all per-flow scheduling decisions out of the OS and into the NIC</p>
    <p>Contributions:</p>
    <p>Specification: A new network policy abstraction: restricted directed acyclic graphs (DAGs)</p>
    <p>Enforcement: A new programmable packet scheduling hierarchy designed for NICs</p>
    <p>Updating: A new expressive and efficient OS/NIC interface</p>
    <p>Implementation and Evaluation: BESS prototype and CloudLab</p>
  </div>
  <div class="page">
    <p>Outline Contributions:</p>
  </div>
  <div class="page">
    <p>What scheduling polices are needed for performance isolation?</p>
    <p>How should policies be specified?</p>
  </div>
  <div class="page">
    <p>Solution: Loom Policy DAG Two types of nodes:</p>
    <p>Fair_1</p>
    <p>Pri_1</p>
    <p>VM1 VM2VM1</p>
    <p>FIFO_1</p>
    <p>RL_WAN RL_None</p>
    <p>Legend:</p>
    <p>Shaping Node</p>
    <p>Scheduling Node</p>
    <p>Child 1</p>
    <p>(a)</p>
    <p>Child 2</p>
    <p>Parent P1 P2</p>
    <p>Child</p>
    <p>(b)</p>
    <p>Child</p>
    <p>(c)</p>
    <p>FIFO</p>
    <p>R1 R2 R3</p>
    <p>Child</p>
    <p>(d)</p>
    <p>P1</p>
    <p>R1 R2 R3</p>
    <p>P2 P3</p>
    <p>Legend:</p>
    <p>Shaping Node</p>
    <p>Scheduling Node</p>
    <p>Child 1</p>
    <p>(a)</p>
    <p>Child 2</p>
    <p>Parent P1 P2</p>
    <p>Child</p>
    <p>(b)</p>
    <p>Child</p>
    <p>(c)</p>
    <p>FIFO</p>
    <p>R1 R2 R3</p>
    <p>Child</p>
    <p>(d)</p>
    <p>P1</p>
    <p>R1 R2 R3</p>
    <p>P2 P3</p>
    <p>Scheduling nodes: Work-conserving policies for sharing the local link bandwidth</p>
    <p>Shaping nodes: Rate-limiting policies for sharing the network core (WAN and DCN)</p>
    <p>Programmability: Every node is programmable with a custom enqueue and dequeue function</p>
    <p>Loom can express policies that cannot be expressed with either Linux Traffic Control (Qdisc) or with Domino (PIFO)!</p>
    <p>Important systems like BwE (sharing the WAN) and EyeQ (sharing the DCN) require Looms policy DAG!</p>
  </div>
  <div class="page">
    <p>Types of Loom Scheduling Policies:</p>
    <p>Scheduling:  All of the flows from competing</p>
    <p>Spark jobs J1 and J2 in VM1 fairly share network bandwidth</p>
    <p>Shaping:  All of the flows from VM1 to VM2 are</p>
    <p>rate limited to 50Gbps</p>
  </div>
  <div class="page">
    <p>Types of Loom Scheduling Policies:</p>
    <p>Scheduling:  All of the flows from competing</p>
    <p>Spark jobs J1 and J2 in VM1 fairly share network bandwidth</p>
    <p>Shaping:  All of the flows from VM1 to VM2 are</p>
    <p>rate limited to 50Gbps</p>
    <p>Group by source</p>
    <p>Group by destination</p>
  </div>
  <div class="page">
    <p>Types of Loom Scheduling Policies:</p>
    <p>Scheduling:  All of the flows from competing</p>
    <p>Spark jobs J1 and J2 in VM1 fairly share network bandwidth</p>
    <p>Shaping:  All of the flows from VM1 to VM2 are</p>
    <p>rate limited to 50Gbps Because Scheduling and Shaping polices may aggregate flows</p>
    <p>differently, they cannot be expressed as a tree!</p>
    <p>Group by source</p>
    <p>Group by destination</p>
  </div>
  <div class="page">
    <p>Loom: Policy Abstraction Policies are expressed as restricted acyclic graphs (DAGs)</p>
    <p>Legend:</p>
    <p>Shaping Node</p>
    <p>Scheduling Node</p>
    <p>Child 1</p>
    <p>(a)</p>
    <p>Child 2</p>
    <p>Parent P1 P2</p>
    <p>Child</p>
    <p>(b)</p>
    <p>Child</p>
    <p>(c)</p>
    <p>FIFO</p>
    <p>R1 R2 R3</p>
    <p>Child</p>
    <p>(d)</p>
    <p>P1</p>
    <p>R1 R2 R3</p>
    <p>P2 P3</p>
    <p>DAG restriction: Scheduling nodes form a tree when the shaping nodes are removed</p>
    <p>(b) And (d) are prevented because they allow parents to reorder packets that were already ordered by a child node.</p>
  </div>
  <div class="page">
    <p>Loom: Policy Abstraction Policies are expressed as restricted acyclic graphs (DAGs)</p>
    <p>Legend:</p>
    <p>Shaping Node</p>
    <p>Scheduling Node</p>
    <p>Child 1</p>
    <p>(a)</p>
    <p>Child 2</p>
    <p>Parent P1 P2</p>
    <p>Child</p>
    <p>(b)</p>
    <p>Child</p>
    <p>(c)</p>
    <p>FIFO</p>
    <p>R1 R2 R3</p>
    <p>Child</p>
    <p>(d)</p>
    <p>P1</p>
    <p>R1 R2 R3</p>
    <p>P2 P3</p>
    <p>DAG restriction: Scheduling nodes form a tree when the shaping nodes are removed</p>
    <p>(b) And (d) are prevented because they allow parents to reorder packets that were already ordered by a child node.</p>
  </div>
  <div class="page">
    <p>Outline Contributions:</p>
  </div>
  <div class="page">
    <p>How do we build a NIC that can enforce Looms new DAG abstraction?</p>
  </div>
  <div class="page">
    <p>Loom Enforcement Challenge No existing hardware scheduler can efficiently enforce Loom Policy DAGs</p>
    <p>Scheduling</p>
    <p>Domino PIFO Block</p>
    <p>Shaping</p>
    <p>Scheduling</p>
    <p>New PIFO Block?</p>
    <p>Shaping</p>
    <p>N x</p>
    <p>Requiring separate shaping queues for every shaping traffic class would be prohibitive!</p>
  </div>
  <div class="page">
    <p>Insight: All shaping can be done with a single queue because all shaping can use</p>
    <p>wall clock time as a rank</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F2 F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F2 F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F3</p>
  </div>
  <div class="page">
    <p>Loom Enforcement</p>
    <p>In Loom, scheduling and shaping queues are separate</p>
    <p>F1 F2 F3</p>
    <p>Pri</p>
    <p>Mem  25Gbps RL</p>
    <p>Mem  No RL</p>
    <p>Spark  No RL</p>
    <p>Scheduling Shaping</p>
    <p>F1 F1 F1</p>
    <p>F3</p>
  </div>
  <div class="page">
    <p>Outline Contributions:</p>
  </div>
  <div class="page">
    <p>PCIe Limitations:</p>
    <p>NIC doorbell and update limitations:1</p>
    <p>NIC</p>
    <p>PCIe Engine</p>
    <p>App</p>
    <p>Core</p>
    <p>DB1 DB2 DB3</p>
    <p>DB4  DB_F</p>
    <p>PCIe 1</p>
    <p>PSPAT: software packet scheduling at hardware speed</p>
    <p>Luigi Rizzo1, Paolo Valente2, Giuseppe Lettieri1, Vincenzo Maffione2 1Univ. di Pisa, 2Univ.di Modena e Reggio Emilia</p>
    <p>rizzo.unipi@gmail.com. Work supported by H2020 project SSICLOPS. Authors copy 20160921, please do not redistribute.</p>
    <p>Abstract</p>
    <p>Tenants in a cloud environment run services, such as Virtual Network Function instantiations, that may legitimately generate millions of packets per second. The hosting platform, hence, needs robust scheduling mechanisms that support these rates and, at the same time, provide isolation and dependable service guarantees.</p>
    <p>Current hardware or software packet scheduling solutions fail to meet all these requirements, most commonly lacking on either performance or guarantees.</p>
    <p>In this paper we propose an architecture, called PSPAT, to build efficient and robust software packet schedulers suitable to high speed, highly concurrent environments. PSPAT decouples clients, scheduler and device driver through lock-free mailboxes, thus removing lock contention, increasing performance and providing opportunities to parallelise operation.</p>
    <p>We describe the operation of our system, discussion implementation and system issues, provide analytical bounds on the service guarantees of PSPAT, and validate the behaviour of its Linux implementation even at high link utilization comparing it with current hardware and software solutions. Our prototype can make over 15 million scheduling decisions per second, and keep latency low, even with tens of concurrent clients running on a multi-core, multi-socket system.</p>
    <p>Allocating and accounting for available capacity is the foundation of cloud environments, where multiple tenants share resources managed by the cloud provider. Dynamic resource scheduling in the Operating System (OS) ensures that CPU, disk, and network capacity are assigned to tenants as specified by contracts and configuration. It is fundamental that the platform guarantees isolation and predictable performance even when overloaded.</p>
    <p>PROBLEM AND USE CASE: In this work we focus on packet scheduling for very high packet rates and large number of concurrent clients. This is an increasingly common scenario in servers that host cloud clients: Virtual Machines (VMs), OS containers, or any other mechanism to manage and account for resources.</p>
    <p>Current hosts feature multiple CPU sockets with tens of CPU cores, and Network Interfaces (NICs) with an aggregate rate of 10..100 Gbit/s. Even at such data rates, handling bulk TCP traffic is doable: large frames (from 1500 up to 64 Kbyte segments with hardware segmentation offloading) imply relatively modest packet rates. On the contrary, Virtual Network Function (VNF) instances are challenging, as they often operate with very small packets and rates of 10+ Millions of packets per second (pps).</p>
    <p>THE CHALLENGE: Scheduling the links capacity in a fair and robust way almost always requires to look at the global state of the system. This translates in some centralised data structure/decision point that is very expensive to implement in a high rate, highly concurrent environment. A Packet Scheduler that cannot sustain the links rate not only reduces communication speed, but may easily fail to achieve the desired bandwidth allocation or delay bounds, sometimes by a large factor. We give several such examples in Sections 2.3, 5.5 and 5.6.</p>
    <p>PCIe bus</p>
    <p>NIC</p>
    <p>Packet scheduler</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>multiqueue NIC + HW scheduler</p>
    <p>PCIe bus</p>
    <p>Figure 1: Common architectures for software and hardware packet schedulers in OSes.</p>
  </div>
  <div class="page">
    <p>PCIe Limitations:</p>
    <p>NIC doorbell and update limitations:1</p>
    <p>NIC</p>
    <p>PCIe Engine</p>
    <p>App</p>
    <p>Core</p>
    <p>DB1 DB2 DB3</p>
    <p>DB4  DB_F</p>
    <p>PCIe 1</p>
    <p>PSPAT: software packet scheduling at hardware speed</p>
    <p>Luigi Rizzo1, Paolo Valente2, Giuseppe Lettieri1, Vincenzo Maffione2 1Univ. di Pisa, 2Univ.di Modena e Reggio Emilia</p>
    <p>rizzo.unipi@gmail.com. Work supported by H2020 project SSICLOPS. Authors copy 20160921, please do not redistribute.</p>
    <p>Abstract</p>
    <p>Tenants in a cloud environment run services, such as Virtual Network Function instantiations, that may legitimately generate millions of packets per second. The hosting platform, hence, needs robust scheduling mechanisms that support these rates and, at the same time, provide isolation and dependable service guarantees.</p>
    <p>Current hardware or software packet scheduling solutions fail to meet all these requirements, most commonly lacking on either performance or guarantees.</p>
    <p>In this paper we propose an architecture, called PSPAT, to build efficient and robust software packet schedulers suitable to high speed, highly concurrent environments. PSPAT decouples clients, scheduler and device driver through lock-free mailboxes, thus removing lock contention, increasing performance and providing opportunities to parallelise operation.</p>
    <p>We describe the operation of our system, discussion implementation and system issues, provide analytical bounds on the service guarantees of PSPAT, and validate the behaviour of its Linux implementation even at high link utilization comparing it with current hardware and software solutions. Our prototype can make over 15 million scheduling decisions per second, and keep latency low, even with tens of concurrent clients running on a multi-core, multi-socket system.</p>
    <p>Allocating and accounting for available capacity is the foundation of cloud environments, where multiple tenants share resources managed by the cloud provider. Dynamic resource scheduling in the Operating System (OS) ensures that CPU, disk, and network capacity are assigned to tenants as specified by contracts and configuration. It is fundamental that the platform guarantees isolation and predictable performance even when overloaded.</p>
    <p>PROBLEM AND USE CASE: In this work we focus on packet scheduling for very high packet rates and large number of concurrent clients. This is an increasingly common scenario in servers that host cloud clients: Virtual Machines (VMs), OS containers, or any other mechanism to manage and account for resources.</p>
    <p>Current hosts feature multiple CPU sockets with tens of CPU cores, and Network Interfaces (NICs) with an aggregate rate of 10..100 Gbit/s. Even at such data rates, handling bulk TCP traffic is doable: large frames (from 1500 up to 64 Kbyte segments with hardware segmentation offloading) imply relatively modest packet rates. On the contrary, Virtual Network Function (VNF) instances are challenging, as they often operate with very small packets and rates of 10+ Millions of packets per second (pps).</p>
    <p>THE CHALLENGE: Scheduling the links capacity in a fair and robust way almost always requires to look at the global state of the system. This translates in some centralised data structure/decision point that is very expensive to implement in a high rate, highly concurrent environment. A Packet Scheduler that cannot sustain the links rate not only reduces communication speed, but may easily fail to achieve the desired bandwidth allocation or delay bounds, sometimes by a large factor. We give several such examples in Sections 2.3, 5.5 and 5.6.</p>
    <p>PCIe bus</p>
    <p>NIC</p>
    <p>Packet scheduler</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>multiqueue NIC + HW scheduler</p>
    <p>PCIe bus</p>
    <p>Figure 1: Common architectures for software and hardware packet schedulers in OSes.</p>
  </div>
  <div class="page">
    <p>PCIe Limitations:</p>
    <p>NIC doorbell and update limitations:1</p>
    <p>NIC</p>
    <p>PCIe Engine</p>
    <p>App</p>
    <p>Core</p>
    <p>DB1 DB2 DB3</p>
    <p>DB4  DB_F</p>
    <p>PCIe 1</p>
    <p>PSPAT: software packet scheduling at hardware speed</p>
    <p>Luigi Rizzo1, Paolo Valente2, Giuseppe Lettieri1, Vincenzo Maffione2 1Univ. di Pisa, 2Univ.di Modena e Reggio Emilia</p>
    <p>rizzo.unipi@gmail.com. Work supported by H2020 project SSICLOPS. Authors copy 20160921, please do not redistribute.</p>
    <p>Abstract</p>
    <p>Tenants in a cloud environment run services, such as Virtual Network Function instantiations, that may legitimately generate millions of packets per second. The hosting platform, hence, needs robust scheduling mechanisms that support these rates and, at the same time, provide isolation and dependable service guarantees.</p>
    <p>Current hardware or software packet scheduling solutions fail to meet all these requirements, most commonly lacking on either performance or guarantees.</p>
    <p>In this paper we propose an architecture, called PSPAT, to build efficient and robust software packet schedulers suitable to high speed, highly concurrent environments. PSPAT decouples clients, scheduler and device driver through lock-free mailboxes, thus removing lock contention, increasing performance and providing opportunities to parallelise operation.</p>
    <p>We describe the operation of our system, discussion implementation and system issues, provide analytical bounds on the service guarantees of PSPAT, and validate the behaviour of its Linux implementation even at high link utilization comparing it with current hardware and software solutions. Our prototype can make over 15 million scheduling decisions per second, and keep latency low, even with tens of concurrent clients running on a multi-core, multi-socket system.</p>
    <p>Allocating and accounting for available capacity is the foundation of cloud environments, where multiple tenants share resources managed by the cloud provider. Dynamic resource scheduling in the Operating System (OS) ensures that CPU, disk, and network capacity are assigned to tenants as specified by contracts and configuration. It is fundamental that the platform guarantees isolation and predictable performance even when overloaded.</p>
    <p>PROBLEM AND USE CASE: In this work we focus on packet scheduling for very high packet rates and large number of concurrent clients. This is an increasingly common scenario in servers that host cloud clients: Virtual Machines (VMs), OS containers, or any other mechanism to manage and account for resources.</p>
    <p>Current hosts feature multiple CPU sockets with tens of CPU cores, and Network Interfaces (NICs) with an aggregate rate of 10..100 Gbit/s. Even at such data rates, handling bulk TCP traffic is doable: large frames (from 1500 up to 64 Kbyte segments with hardware segmentation offloading) imply relatively modest packet rates. On the contrary, Virtual Network Function (VNF) instances are challenging, as they often operate with very small packets and rates of 10+ Millions of packets per second (pps).</p>
    <p>THE CHALLENGE: Scheduling the links capacity in a fair and robust way almost always requires to look at the global state of the system. This translates in some centralised data structure/decision point that is very expensive to implement in a high rate, highly concurrent environment. A Packet Scheduler that cannot sustain the links rate not only reduces communication speed, but may easily fail to achieve the desired bandwidth allocation or delay bounds, sometimes by a large factor. We give several such examples in Sections 2.3, 5.5 and 5.6.</p>
    <p>PCIe bus</p>
    <p>NIC</p>
    <p>Packet scheduler</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>multiqueue NIC + HW scheduler</p>
    <p>PCIe bus</p>
    <p>Figure 1: Common architectures for software and hardware packet schedulers in OSes.</p>
  </div>
  <div class="page">
    <p>PCIe Limitations:</p>
    <p>NIC doorbell and update limitations:1</p>
    <p>NIC</p>
    <p>PCIe Engine</p>
    <p>App</p>
    <p>Core</p>
    <p>DB1 DB2 DB3</p>
    <p>DB4  DB_F</p>
    <p>PCIe 1</p>
    <p>PSPAT: software packet scheduling at hardware speed</p>
    <p>Luigi Rizzo1, Paolo Valente2, Giuseppe Lettieri1, Vincenzo Maffione2 1Univ. di Pisa, 2Univ.di Modena e Reggio Emilia</p>
    <p>rizzo.unipi@gmail.com. Work supported by H2020 project SSICLOPS. Authors copy 20160921, please do not redistribute.</p>
    <p>Abstract</p>
    <p>Tenants in a cloud environment run services, such as Virtual Network Function instantiations, that may legitimately generate millions of packets per second. The hosting platform, hence, needs robust scheduling mechanisms that support these rates and, at the same time, provide isolation and dependable service guarantees.</p>
    <p>Current hardware or software packet scheduling solutions fail to meet all these requirements, most commonly lacking on either performance or guarantees.</p>
    <p>In this paper we propose an architecture, called PSPAT, to build efficient and robust software packet schedulers suitable to high speed, highly concurrent environments. PSPAT decouples clients, scheduler and device driver through lock-free mailboxes, thus removing lock contention, increasing performance and providing opportunities to parallelise operation.</p>
    <p>We describe the operation of our system, discussion implementation and system issues, provide analytical bounds on the service guarantees of PSPAT, and validate the behaviour of its Linux implementation even at high link utilization comparing it with current hardware and software solutions. Our prototype can make over 15 million scheduling decisions per second, and keep latency low, even with tens of concurrent clients running on a multi-core, multi-socket system.</p>
    <p>Allocating and accounting for available capacity is the foundation of cloud environments, where multiple tenants share resources managed by the cloud provider. Dynamic resource scheduling in the Operating System (OS) ensures that CPU, disk, and network capacity are assigned to tenants as specified by contracts and configuration. It is fundamental that the platform guarantees isolation and predictable performance even when overloaded.</p>
    <p>PROBLEM AND USE CASE: In this work we focus on packet scheduling for very high packet rates and large number of concurrent clients. This is an increasingly common scenario in servers that host cloud clients: Virtual Machines (VMs), OS containers, or any other mechanism to manage and account for resources.</p>
    <p>Current hosts feature multiple CPU sockets with tens of CPU cores, and Network Interfaces (NICs) with an aggregate rate of 10..100 Gbit/s. Even at such data rates, handling bulk TCP traffic is doable: large frames (from 1500 up to 64 Kbyte segments with hardware segmentation offloading) imply relatively modest packet rates. On the contrary, Virtual Network Function (VNF) instances are challenging, as they often operate with very small packets and rates of 10+ Millions of packets per second (pps).</p>
    <p>THE CHALLENGE: Scheduling the links capacity in a fair and robust way almost always requires to look at the global state of the system. This translates in some centralised data structure/decision point that is very expensive to implement in a high rate, highly concurrent environment. A Packet Scheduler that cannot sustain the links rate not only reduces communication speed, but may easily fail to achieve the desired bandwidth allocation or delay bounds, sometimes by a large factor. We give several such examples in Sections 2.3, 5.5 and 5.6.</p>
    <p>PCIe bus</p>
    <p>NIC</p>
    <p>Packet scheduler</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>multiqueue NIC + HW scheduler</p>
    <p>PCIe bus</p>
    <p>Figure 1: Common architectures for software and hardware packet schedulers in OSes.</p>
  </div>
  <div class="page">
    <p>PCIe Limitations:</p>
    <p>NIC doorbell and update limitations:1</p>
    <p>Loom Goal: Less than 1Mops @ 100Gbps</p>
    <p>NIC</p>
    <p>PCIe Engine</p>
    <p>App</p>
    <p>Core</p>
    <p>DB1 DB2 DB3</p>
    <p>DB4  DB_F</p>
    <p>PCIe 1</p>
    <p>PSPAT: software packet scheduling at hardware speed</p>
    <p>Luigi Rizzo1, Paolo Valente2, Giuseppe Lettieri1, Vincenzo Maffione2 1Univ. di Pisa, 2Univ.di Modena e Reggio Emilia</p>
    <p>rizzo.unipi@gmail.com. Work supported by H2020 project SSICLOPS. Authors copy 20160921, please do not redistribute.</p>
    <p>Abstract</p>
    <p>Tenants in a cloud environment run services, such as Virtual Network Function instantiations, that may legitimately generate millions of packets per second. The hosting platform, hence, needs robust scheduling mechanisms that support these rates and, at the same time, provide isolation and dependable service guarantees.</p>
    <p>Current hardware or software packet scheduling solutions fail to meet all these requirements, most commonly lacking on either performance or guarantees.</p>
    <p>In this paper we propose an architecture, called PSPAT, to build efficient and robust software packet schedulers suitable to high speed, highly concurrent environments. PSPAT decouples clients, scheduler and device driver through lock-free mailboxes, thus removing lock contention, increasing performance and providing opportunities to parallelise operation.</p>
    <p>We describe the operation of our system, discussion implementation and system issues, provide analytical bounds on the service guarantees of PSPAT, and validate the behaviour of its Linux implementation even at high link utilization comparing it with current hardware and software solutions. Our prototype can make over 15 million scheduling decisions per second, and keep latency low, even with tens of concurrent clients running on a multi-core, multi-socket system.</p>
    <p>Allocating and accounting for available capacity is the foundation of cloud environments, where multiple tenants share resources managed by the cloud provider. Dynamic resource scheduling in the Operating System (OS) ensures that CPU, disk, and network capacity are assigned to tenants as specified by contracts and configuration. It is fundamental that the platform guarantees isolation and predictable performance even when overloaded.</p>
    <p>PROBLEM AND USE CASE: In this work we focus on packet scheduling for very high packet rates and large number of concurrent clients. This is an increasingly common scenario in servers that host cloud clients: Virtual Machines (VMs), OS containers, or any other mechanism to manage and account for resources.</p>
    <p>Current hosts feature multiple CPU sockets with tens of CPU cores, and Network Interfaces (NICs) with an aggregate rate of 10..100 Gbit/s. Even at such data rates, handling bulk TCP traffic is doable: large frames (from 1500 up to 64 Kbyte segments with hardware segmentation offloading) imply relatively modest packet rates. On the contrary, Virtual Network Function (VNF) instances are challenging, as they often operate with very small packets and rates of 10+ Millions of packets per second (pps).</p>
    <p>THE CHALLENGE: Scheduling the links capacity in a fair and robust way almost always requires to look at the global state of the system. This translates in some centralised data structure/decision point that is very expensive to implement in a high rate, highly concurrent environment. A Packet Scheduler that cannot sustain the links rate not only reduces communication speed, but may easily fail to achieve the desired bandwidth allocation or delay bounds, sometimes by a large factor. We give several such examples in Sections 2.3, 5.5 and 5.6.</p>
    <p>PCIe bus</p>
    <p>NIC</p>
    <p>Packet scheduler</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>clients kernel</p>
    <p>protocol processing</p>
    <p>device driver</p>
    <p>multiqueue NIC + HW scheduler</p>
    <p>PCIe bus</p>
    <p>Figure 1: Common architectures for software and hardware packet schedulers in OSes.</p>
  </div>
  <div class="page">
    <p>Loom Efficient Interface Challenges</p>
    <p>Insufficient data: Before reading any packet data</p>
    <p>(headers), the NIC must schedule DMA reads for a</p>
    <p>queue</p>
    <p>Too many PCIe writes: In the worst case (every packet</p>
    <p>is from a new flow), the OS must generate 2 PCIe writes</p>
    <p>per-packet</p>
  </div>
  <div class="page">
    <p>Loom Design</p>
    <p>Loom introduces a new efficient OS/NIC interface that reduces the</p>
    <p>number of PCIe writes through batched updates and inline metadata</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>Batched Doorbells</p>
    <p>Using on-NIC Doorbell FIFOs allows for updates to different queues (flows) to be batched</p>
    <p>NIC</p>
    <p>Per-core Doorbell</p>
    <p>FIFO</p>
    <p>App</p>
    <p>Core</p>
    <p>PCIe</p>
    <p>Per-core FIFOs still enable parallelism</p>
  </div>
  <div class="page">
    <p>NIC</p>
    <p>Inline Metadata</p>
    <p>Scheduling metadata (traffic</p>
    <p>class and scheduling updates)</p>
    <p>is inlined to reduce PCIe writes Descriptor inlining allows for</p>
    <p>scheduling before reading</p>
    <p>packet data</p>
    <p>Wire</p>
    <p>DMA Engine</p>
    <p>PIFOs</p>
    <p>Mem</p>
    <p>Q5 Q_F</p>
    <p>Q1 Q2 Q3</p>
  </div>
  <div class="page">
    <p>NIC</p>
    <p>Inline Metadata</p>
    <p>Scheduling metadata (traffic</p>
    <p>class and scheduling updates)</p>
    <p>is inlined to reduce PCIe writes Descriptor inlining allows for</p>
    <p>scheduling before reading</p>
    <p>packet data</p>
    <p>Wire</p>
    <p>DMA Engine</p>
    <p>PIFOs</p>
    <p>Mem</p>
    <p>Q5 Q_F</p>
    <p>Q1</p>
    <p>Q2 Q3</p>
  </div>
  <div class="page">
    <p>NIC</p>
    <p>Inline Metadata</p>
    <p>Scheduling metadata (traffic</p>
    <p>class and scheduling updates)</p>
    <p>is inlined to reduce PCIe writes Descriptor inlining allows for</p>
    <p>scheduling before reading</p>
    <p>packet data</p>
    <p>Wire</p>
    <p>DMA Engine</p>
    <p>PIFOs</p>
    <p>Mem</p>
    <p>Q5 Q_F</p>
    <p>Q1</p>
    <p>Q2 Q3</p>
  </div>
  <div class="page">
    <p>Outline Contributions:</p>
    <p>Evaluation:</p>
  </div>
  <div class="page">
    <p>Loom Implementation</p>
    <p>Software prototype of Loom in Linux on the Berkeley Extensible Software Switch (BESS)1</p>
    <p>Programmable Packet Scheduling at Line Rate</p>
    <p>Anirudh Sivaraman*, Suvinay Subramanian*, Mohammad Alizadeh*, Sharad Chole, Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan*, Tom Edsall, Sachin Katti+, Nick McKeown+</p>
    <p>*MIT CSAIL, Barefoot Networks, Cisco Systems, +Stanford University</p>
    <p>ABSTRACT Switches today provide a small menu of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a programmable packet scheduler, which allows scheduling algorithmspotentially algorithms that are unknown todayto be programmed into a switch without requiring hardware redesign.</p>
    <p>Our design uses the property that scheduling algorithms make two decisions: in what order to schedule packets and when to schedule them. Further, we observe that in many scheduling algorithms, definitive decisions on these two questions can be made when packets are enqueued. We use these observations to build a programmable scheduler using a single abstraction: the push-in first-out queue (PIFO), a priority queue that maintains the scheduling order or time.</p>
    <p>We show that a PIFO-based scheduler lets us program a wide variety of scheduling algorithms. We present a hardware design for this scheduler for a 64-port 10 Gbit/s sharedmemory (output-queued) switch. Our design costs an additional 4% in chip area. In return, it lets us program many sophisticated algorithms, such as a 5-level hierarchical scheduler with programmable decisions at each level.</p>
    <p>CCS Concepts Networks ! Programmable networks;</p>
    <p>Keywords Programmable scheduling; switch hardware</p>
    <p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGCOMM 16, August 22 - 26, 2016, Florianopolis , Brazil</p>
    <p>2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00 DOI: http://dx.doi.org/10.1145/2934872.2934899</p>
    <p>provide a small menu of scheduling algorithms: typically, a combination of Deficit Round Robin [36], strict priority scheduling, and traffic shaping. A network operator can change parameters in these algorithms, but cannot change the core logic in an existing algorithm, or program a new one, without building new switch hardware.</p>
    <p>By contrast, with a programmable packet scheduler, network operators could customize scheduling algorithms to application requirements, e.g., minimizing flow completion times [12] using Shortest Remaining Processing Time [35], allocating bandwidth flexibly across flows or tenants [26, 33] using Weighted Fair Queueing [20], minimizing tail packet delays using Least Slack Time First [29], etc. Moreover, with a programmable packet scheduler, switch vendors could implement scheduling algorithms as programs running on a programmable switching chip, making it easier to verify and modify these algorithms compared to baking in the same algorithms into a chip as rigid hardware.</p>
    <p>This paper presents a design for programmable packet scheduling in line-rate switches. All scheduling algorithms make two basic decisions: in what order packets should be scheduled and when they should be scheduled, corresponding to work-conserving and non-work-conserving algorithms respectively. Furthermore, for many scheduling algorithms, these two decisions can be made when a packet is enqueued. This observation suggests a natural hardware primitive for packet scheduling: a push-in first-out queue (PIFO) [19, 38]. A PIFO is a priority queue that allows elements to be pushed into an arbitrary position based on an elements rank (the scheduling order or time),1 but always dequeues elements from the head.</p>
    <p>We develop a programming model for scheduling (2) based on PIFOs with two key ideas. First, we allow users to set a packets rank in a PIFO by supplying a small program for computing packet ranks (2.1). Coupling this program with a single PIFO allows the user to program any scheduling algorithm where the relative scheduling order of buffered packets does not change with future packet arrivals. Second, users can compose PIFOs together in a tree to program</p>
    <p>http://github.com/bestephe/loom</p>
    <p>C++ PIFO2 implementation is used for scheduling</p>
  </div>
  <div class="page">
    <p>Loom Evaluation</p>
    <p>Can Loom drive line rate? Can Loom enforce network policies?</p>
    <p>Experiment: Microbenchmarks with iPerf</p>
    <p>Can Loom isolate real applications?</p>
    <p>Experiment: CloudLab experiments with memcached and Spark</p>
    <p>How effective is Looms efficient OS/NIC interface?</p>
    <p>Experiment: Analysis of PCIe writes in Linux (QPF) versus Loom</p>
  </div>
  <div class="page">
    <p>Loom 40Gbps Evaluation</p>
    <p>Setup:  Every 2s a new tenant</p>
    <p>starts or stops  Each tenant i starts 4i</p>
    <p>flows (4-256 total flows)</p>
    <p>Fair</p>
    <p>T2T1 T3 T4</p>
    <p>Policy: All tenants should receive an equal share. 0 5 10 15 20</p>
    <p>Time (seconds)</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>SQ</p>
    <p>MQ</p>
    <p>Loom</p>
  </div>
  <div class="page">
    <p>Loom 40Gbps Evaluation</p>
    <p>Setup:  Every 2s a new tenant</p>
    <p>starts or stops  Each tenant i starts 4i</p>
    <p>flows (4-256 total flows)</p>
    <p>Fair</p>
    <p>T2T1 T3 T4</p>
    <p>Policy: All tenants should receive an equal share. 0 5 10 15 20</p>
    <p>Time (seconds)</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>T1 T2 T3 T4</p>
    <p>Loom can drive line-rate and isolate competing tenants and flows</p>
    <p>SQ</p>
    <p>MQ</p>
    <p>Loom</p>
  </div>
  <div class="page">
    <p>Application Performance: Fairness</p>
    <p>vs</p>
    <p>Bandwidth Hungry</p>
    <p>Bandwidth Hungry</p>
    <p>Policy: Bandwidth is fairly shared between Spark jobs</p>
    <p>Fair</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>Job1 Job2</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>Job1 Job2</p>
    <p>Linux Loom</p>
  </div>
  <div class="page">
    <p>Application Performance: Fairness</p>
    <p>vs</p>
    <p>Bandwidth Hungry</p>
    <p>Bandwidth Hungry</p>
    <p>Policy: Bandwidth is fairly shared between Spark jobs</p>
    <p>Fair</p>
    <p>Loom can ensure competing jobs share bandwidth even if they have different numbers of flows</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>Job1 Job2</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (G</p>
    <p>bp s)</p>
    <p>Job1 Job2</p>
    <p>Linux Loom</p>
  </div>
  <div class="page">
    <p>Application Performance: Latency</p>
    <p>vs</p>
    <p>Latency Sensitive Bandwidth Hungry</p>
    <p>Setup: Linux software packet scheduling (Qdisc) is configured to prioritize memcached traffic over Spark traffic</p>
    <p>Pri 0</p>
    <p>Loom Linux (MQ)</p>
    <p>Pe rc</p>
    <p>en til</p>
    <p>e La</p>
    <p>te nc</p>
    <p>y (u</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>Application Performance: Latency</p>
    <p>vs</p>
    <p>Latency Sensitive Bandwidth Hungry</p>
    <p>Setup: Linux software packet scheduling (Qdisc) is configured to prioritize memcached traffic over Spark traffic</p>
    <p>Pri</p>
    <p>MQ cannot isolate latency-sensitive applications!</p>
    <p>Loom Linux (MQ)</p>
    <p>Pe rc</p>
    <p>en til</p>
    <p>e La</p>
    <p>te nc</p>
    <p>y (u</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>Loom Interface Evaluation</p>
    <p>Line-rate</p>
    <p>Existing approaches: PCIe Writes per second</p>
    <p>Loom: PCIe Writes per second</p>
    <p>Worse case scenario: Packets are sent in 64KB batches and each</p>
    <p>packet is from a different flow</p>
  </div>
  <div class="page">
    <p>Loom Interface Evaluation</p>
    <p>Line-rate</p>
    <p>Existing approaches: PCIe Writes per second</p>
    <p>Loom: PCIe Writes per second</p>
    <p>Worse case scenario: Packets are sent in 64KB batches and each</p>
    <p>packet is from a different flow</p>
    <p>Loom Goal: Less than 1Mops @ 100Gbps</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Loom is a new NIC design that completely offloads all packet scheduling to the NIC with</p>
    <p>low CPU overhead</p>
    <p>Looms benefits translate into reductions in latency, increases in throughput, and</p>
    <p>improvements in fairness</p>
    <p>Current NICs cannot ensure that competing applications are isolated</p>
  </div>
  <div class="page">
    <p>Related Work (Eiffel)</p>
    <p>Eiffel</p>
    <p>NIC Scheduling does not eliminate the need for software scheduling</p>
    <p>Loom and Eiffel can be used together</p>
    <p>Bucketed priority queues could be used to build efficient PIFOs</p>
  </div>
</Presentation>
