<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>2010 VMware Inc. All rights reserved</p>
    <p>Demand Based Hierarchical QoS Using Storage Resource Pools</p>
    <p>Ajay Gulati, Ganesha Shanmuganathan, Xuechen Zhang*, Peter Varman+</p>
    <p>Distributed Resource Management Team VMware Inc., *Wayne State University, +Rice University</p>
    <p>Usenix ATC</p>
    <p>June 13th, 2012</p>
  </div>
  <div class="page">
    <p>What you see</p>
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>The Problem</p>
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>What you want to see</p>
    <p>Storage Array LUN Storage Array LUN</p>
  </div>
  <div class="page">
    <p>The Problem</p>
    <p>Existing solutions: specify per VM QoS (weights, IOPS or )  Sum of per-VM requirement may be &lt; total application requirement</p>
    <p>A single virtual App</p>
    <p>Challenges:  Hard to set per virtual disk value  Need support for statistical-multiplexing  De-centralized solution needed</p>
  </div>
  <div class="page">
    <p>Resource Allocation Models</p>
    <p>Various models proposed in literature  Assuming three clients: 1, 2 and 3</p>
    <p>Weights: SFQ(D), Stonehedge, Argon, Zygaria, PARDA,   Latency: Faade, SARC+Avatar  Weights, Latency, Burst: pClock  Weights, R, L: mClock (single host)</p>
    <p>Weights Latency Weights, Latency, Burst Weights, Reservation, Limit</p>
    <p>W1</p>
    <p>Lat1 W1, Lat1, B1 W1, R1, L1</p>
    <p>W2 Lat2 W2, Lat2, B2 W2, R2, L2</p>
    <p>W3 Lat3 W3, Lat3, B3 W3, R3, L3</p>
  </div>
  <div class="page">
    <p>What is a Storage Resource Pool</p>
    <p>A powerful abstraction for aggregation and isolation of IO resources  Customer creates a resource pool and sets business requirements  Sharing within a pool, isolation across pools</p>
    <p>Storage Resource Pool</p>
    <p>Organization</p>
    <p>Departments</p>
    <p>VMs</p>
    <p>VMDKs</p>
  </div>
  <div class="page">
    <p>Controls: Reservation (IOPS)</p>
    <p>Minimum IOPS guaranteed  We need to distribute the R value hierarchically using current demand</p>
    <p>Storage Resource Pool</p>
    <p>Sales, R=500</p>
    <p>r = 50 for all vmdks</p>
    <p>R = 1000 IOPS</p>
    <p>Marketing, R=100</p>
    <p>r = 0 for all vmdks</p>
  </div>
  <div class="page">
    <p>Controls: Limit (IOPS)  Maximum IOPS allowed  We need to set children limits based on parents L value, using</p>
    <p>current demand</p>
    <p>Storage Resource Pool</p>
    <p>Sales, L=Max</p>
    <p>l=Max for all vmdks</p>
    <p>L = 5000 IOPS</p>
    <p>Marketing, L=1000</p>
    <p>Max Max Max</p>
    <p>l=MAX for all vmdks</p>
    <p>Max Max</p>
  </div>
  <div class="page">
    <p>Controls: Shares (aka Weights) (No Units)  Relative priority between siblings in the tree  Need to divide parent shares among children  Straight-forward and not based on demand</p>
    <p>Storage Resource Pool</p>
    <p>Sales, S=40</p>
    <p>s=80 for all vmdks</p>
    <p>S = 1000</p>
    <p>Marketing, S=10</p>
    <p>s=200 for all vmdks</p>
  </div>
  <div class="page">
    <p>Virtual View of the World  Storage resource pool as specified by user  In practice, life isnt so simple!</p>
    <p>Storage Resource Pool</p>
    <p>Sales, &lt;R,L,S&gt;</p>
    <p>&lt;r,l,s&gt; for each vmdk</p>
    <p>&lt;R,L,S&gt;</p>
    <p>Marketing, &lt;R,L,S&gt;</p>
    <p>&lt;r,l,s&gt;=sum of vmdks values</p>
  </div>
  <div class="page">
    <p>Physical View of the World  VMs would get placed on different hosts  VMDKs get placed on different datastores</p>
    <p>Datastore A Datastore B</p>
  </div>
  <div class="page">
    <p>What Do we need to Support Storage Resource Pools</p>
    <p>If (Parent R &gt; Sum of children R) We need to distribute spare R among children</p>
    <p>If (Parent L &lt; Sum of children L) We need to restrict children to parent limit</p>
    <p>Distribute parents shares among children  Use current demand of children for dynamic allocation</p>
    <p>These tasks are done periodically</p>
  </div>
  <div class="page">
    <p>Per Datastore RP Tree</p>
    <p>Focus on per datastore tree  A tree spanning multiple devices can be broken up across devices  Need to handle distributed setting with multiple hosts</p>
    <p>Datastore A Datastore A</p>
  </div>
  <div class="page">
    <p>System Architecture</p>
    <p>mClock Host-Level Issue Queues</p>
    <p>Datastore A</p>
    <p>Array Queue</p>
    <p>Queue lengths varied dynamically</p>
    <p>mClock</p>
    <p>Two-level scheduling: 1. Per host LUN queue depth 2. Per VM local IO scheduling using mClock that support R, L, S</p>
    <p>(user space) SRP module</p>
    <p>(user space) SRP module</p>
    <p>(hypervisor)</p>
    <p>(hypervisor)</p>
  </div>
  <div class="page">
    <p>Two main steps: 1. Compute dynamic R,L,S settings per VM based on demand 2. Adjust per host LUN queue depth for host-level differentiation</p>
    <p>These adjustments are done every few seconds (4 sec in our prototype)</p>
    <p>Storage Resource Pools</p>
  </div>
  <div class="page">
    <p>Step 1: Per VM Settings  Compute per VM R, L, S using divvy algorithm on the tree  Set the controls for local VMs on each host</p>
    <p>Datastore A</p>
    <p>r=30,s=1 r=0,s=1 r=0,s=4 r=20,s=2</p>
    <p>R=100</p>
    <p>R=30, S=20 R=25, S=10</p>
    <p>VM-level values are set in the hypervisor for mClock</p>
  </div>
  <div class="page">
    <p>Step 2: Host-level QueueDepth Setting</p>
    <p>Compute current array capacity (in terms of total queue depth)  Allocate that to hosts based on VMs running there  Use queue depth per host as the key control</p>
    <p>Per VM allocation on a host is handled by local scheduler (mClock)</p>
  </div>
  <div class="page">
    <p>Step 2: How to compute total QueueDepth?</p>
    <p>Compute total queue depth</p>
    <p>Compute queue size Q(t) using cluster-wide average latency L(t)</p>
    <p>L : congestion threshold, operating point for IO latency  : smoothing parameter between 0 and 1 Note: Q increases if latency is lower than congestion threshold, decreases</p>
    <p>otherwise</p>
    <p>Q(t +1)= (1!!)Q(t)+! L L(t)</p>
    <p>Q(t) &quot;</p>
    <p># $</p>
    <p>%</p>
    <p>&amp; '</p>
  </div>
  <div class="page">
    <p>Step 2: Host-level QueueDepth Setting  Compute current array capacity  Allocate that to hosts based on VMs running there  Use queue depth per host as the key control</p>
    <p>Per VM allocation on a host is handled by local scheduler (mClock)</p>
    <p>Datastore A</p>
    <p>Example: VM Shares = 20 20 10 10 R= 0 for all L= Max for all No idle VMs Host Queues: 32 16</p>
    <p>Lets say total QueueDepth = 48</p>
  </div>
  <div class="page">
    <p>Storage-specific Practical Issues</p>
    <p>How to set R value at root?  For disk-based devices, we use the random IO performance as upper bound  For SSD-based devices, we can use a fraction of total IOPS performance</p>
    <p>using some read-write ratio</p>
    <p>Handling of IO sizes  IOPS can be specified for specific IO size range ( &lt;= 32KB)  Higher IO sizes will lead to lower values</p>
    <p>Sequential vs. Random workloads  mClock optimizes for sequential workload by using batching  Typically virtualization workloads are random due to IO-blending, thin</p>
    <p>provisioning, de-duplication etc.</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Built a prototype on ESX hypervisor  User-space module + kernel changes</p>
    <p>We use multiple ESX hosts  Experiment 1:  Six Windows Server 2003 VMs running Iometer  Each VM has 4 GB OS disk and 8 GB experimental disk  Each VM is running a different workload profile</p>
    <p>Experiment 2:  Eight VMs running different enterprise workloads: webserver, mail, oltp,</p>
    <p>DvdStore</p>
  </div>
  <div class="page">
    <p>Experiment 1 Setup</p>
    <p>RP1 RP2 RP3 R=1500, S=1 R=0, S=1 R=0, S=2</p>
    <p>VMs have R=0 S=1 L=Max</p>
  </div>
  <div class="page">
    <p>Case 1: High Reservation At Pool Level</p>
    <p>At t=1000 sec, RP1 settings are changed to R=0 and L = 500</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Time(Seconds)</p>
    <p>RP1 (VM1+VM4) limited to 500 IOPS VM1 (RP1) VM2 (RP2) VM3 (RP3) VM4 (RP1) VM5 (RP2) VM6 (RP3)</p>
    <p>Figure 5: RP1s reservation is changed to 0 and its limit</p>
    <p>is set to 500 at t = 1000 sec. SRP always satisfies reser</p>
    <p>vations and limits while doing allocation in proportion to</p>
    <p>shares.</p>
    <p>Figure 5 shows the experimentally measured throughputs</p>
    <p>of all the VMs. The throughputs of VMs 1 and 4 are</p>
    <p>close to 750 IOPS as expected. The total throughput of</p>
    <p>RP2 (VMs 2 and 5) is a little less than twice that of RP3</p>
    <p>(VMs 3 and 6). The reason is because VMs 3 and 6 have</p>
    <p>highly sequential workloads (80% and 100%), and get</p>
    <p>some preferential treatment from the array, resulting in</p>
    <p>a little higher throughput than their entitled allocations.</p>
    <p>After about 1000 seconds, the reservation of RP1 is set to</p>
    <p>limit of 500 IOPS. The rest of the capacity is divided</p>
    <p>between RP2 and RP3 as before in a rough 2 : 1 ratio.</p>
    <p>We also experimented with setting the controls directly</p>
    <p>on the VMs instead of the RP nodes. We set reservations</p>
    <p>of 750 each for the VMs in RP1, and shares of 2000</p>
    <p>(1000) to each of the VMs in RP2 (respectively RP3).</p>
    <p>The observed VM througphputs were similar to the ini</p>
    <p>tial portion of Figure 5. The ability to set controls at the</p>
    <p>RP nodes instead of individual VMs provides a very con</p>
    <p>venient way to share resources using very few explicit</p>
    <p>settings.</p>
    <p>In this experiment we show how RPs allow for stronger</p>
    <p>sharing and better multiplexing among VMs in a pool so</p>
    <p>that resources stay within it. This has advantages when</p>
    <p>VMs are not continuously backlogged; the capacity freed</p>
    <p>up during idle periods is preferentially allocated to other</p>
    <p>(sibling) VMs within the pool rather than spread across</p>
    <p>all VMs.</p>
    <p>The setup is identical to the previous experiment. At</p>
    <p>the start, the throughputs of the VMs shown in Figure 6</p>
    <p>match the initial portion of Figure 5 as expected. Starting</p>
    <p>at time 250 seconds, VM 1 goes idle. We see that the en</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Time(Seconds)</p>
    <p>VM1 Idle VM2 Idle</p>
    <p>VM3 Idle</p>
    <p>VM1 (RP1) VM2 (RP2) VM3 (RP3) VM4 (RP1) VM5 (RP2) VM6 (RP3)</p>
    <p>Figure 6: VMs 1, 2 and 3 get idle at t = 250, 500 and 750</p>
    <p>sec respectively. Spare IOPS are allocated to the sibling</p>
    <p>VMs first</p>
    <p>tire slack is picked up by VM4, its sibling in RP1, whose</p>
    <p>throughput rises from 750 to 1500. The other VMs do</p>
    <p>not get to use this freed-up reservation since VM4 has</p>
    <p>first priority for it and it has enough demand to use it</p>
    <p>completely.</p>
    <p>At time 550 seconds, VM2 goes idle, and its sibling</p>
    <p>VM5 on the other host sees the benefit within just a few</p>
    <p>seconds. VM6 which runs on the same host as VM5 also</p>
    <p>gets a slight boost from the increase in queue depth allo</p>
    <p>cated to this host. The array also becomes more efficient</p>
    <p>and this benefit is given to all the active VMs in propor</p>
    <p>tion to their shares. After VM2 becomes idle, RP2 gets</p>
    <p>higher IOPS than RP3 due to its higher shares.</p>
    <p>Finally VM3 goes idle and VM6 gets the benefit. There</p>
    <p>is not much benefit to the other workloads when the se</p>
    <p>quential workload becomes idle. But still the reservations</p>
    <p>are always met and the workloads under RP2 and RP3 are</p>
    <p>roughly in the ratio of 2 : 1. This experiment shows the</p>
    <p>flow of resources within a resource pool and the isolation</p>
    <p>between pools.</p>
    <p>We compared Storage Resource Pool with a state-of-the</p>
    <p>art system that supports reservation, limit and shares. We</p>
    <p>ran both PARDA [6] and mClock [8] together on ESX</p>
    <p>hosts. PARDA does proportional allocation for VMs</p>
    <p>based on the share settings. PARDA works across a clus</p>
    <p>ter of hosts by leveraging a control algorithm that is very</p>
    <p>similar to FAST TCP [11] for flow control in networks.</p>
    <p>mClock is used as the local scheduler which supports</p>
    <p>reservations, limits and shares for VMs on the same host.</p>
    <p>Since PARDA and mClock do not support resource</p>
    <p>pools, we created a single pool with VMs as its children</p>
    <p>and set the controls only at the VM-level. We found that</p>
    <p>Reservations &amp; Limit controls are enforced at pool level Remaining capacity allocated in proportion of shares (2:1)</p>
  </div>
  <div class="page">
    <p>Case 2: Allocation Within a Pool</p>
    <p>VM1, 2 and 3 get idle at t=250, 500 and 750 seconds respectively</p>
    <p>Capacity is allocated to VMs within the same pool VM4 (in the same RP) get R=1500</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Time(Seconds)</p>
    <p>RP1 (VM1+VM4) limited to 500 IOPS VM1 (RP1) VM2 (RP2) VM3 (RP3) VM4 (RP1) VM5 (RP2) VM6 (RP3)</p>
    <p>Figure 5: RP1s reservation is changed to 0 and its limit</p>
    <p>is set to 500 at t = 1000 sec. SRP always satisfies reser</p>
    <p>vations and limits while doing allocation in proportion to</p>
    <p>shares.</p>
    <p>Figure 5 shows the experimentally measured throughputs</p>
    <p>of all the VMs. The throughputs of VMs 1 and 4 are</p>
    <p>close to 750 IOPS as expected. The total throughput of</p>
    <p>RP2 (VMs 2 and 5) is a little less than twice that of RP3</p>
    <p>(VMs 3 and 6). The reason is because VMs 3 and 6 have</p>
    <p>highly sequential workloads (80% and 100%), and get</p>
    <p>some preferential treatment from the array, resulting in</p>
    <p>a little higher throughput than their entitled allocations.</p>
    <p>After about 1000 seconds, the reservation of RP1 is set to</p>
    <p>limit of 500 IOPS. The rest of the capacity is divided</p>
    <p>between RP2 and RP3 as before in a rough 2 : 1 ratio.</p>
    <p>We also experimented with setting the controls directly</p>
    <p>on the VMs instead of the RP nodes. We set reservations</p>
    <p>of 750 each for the VMs in RP1, and shares of 2000</p>
    <p>(1000) to each of the VMs in RP2 (respectively RP3).</p>
    <p>The observed VM througphputs were similar to the ini</p>
    <p>tial portion of Figure 5. The ability to set controls at the</p>
    <p>RP nodes instead of individual VMs provides a very con</p>
    <p>venient way to share resources using very few explicit</p>
    <p>settings.</p>
    <p>In this experiment we show how RPs allow for stronger</p>
    <p>sharing and better multiplexing among VMs in a pool so</p>
    <p>that resources stay within it. This has advantages when</p>
    <p>VMs are not continuously backlogged; the capacity freed</p>
    <p>up during idle periods is preferentially allocated to other</p>
    <p>(sibling) VMs within the pool rather than spread across</p>
    <p>all VMs.</p>
    <p>The setup is identical to the previous experiment. At</p>
    <p>the start, the throughputs of the VMs shown in Figure 6</p>
    <p>match the initial portion of Figure 5 as expected. Starting</p>
    <p>at time 250 seconds, VM 1 goes idle. We see that the en</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Time(Seconds)</p>
    <p>VM1 Idle VM2 Idle</p>
    <p>VM3 Idle</p>
    <p>VM1 (RP1) VM2 (RP2) VM3 (RP3) VM4 (RP1) VM5 (RP2) VM6 (RP3)</p>
    <p>Figure 6: VMs 1, 2 and 3 get idle at t = 250, 500 and 750</p>
    <p>sec respectively. Spare IOPS are allocated to the sibling</p>
    <p>VMs first</p>
    <p>tire slack is picked up by VM4, its sibling in RP1, whose</p>
    <p>throughput rises from 750 to 1500. The other VMs do</p>
    <p>not get to use this freed-up reservation since VM4 has</p>
    <p>first priority for it and it has enough demand to use it</p>
    <p>completely.</p>
    <p>At time 550 seconds, VM2 goes idle, and its sibling</p>
    <p>VM5 on the other host sees the benefit within just a few</p>
    <p>seconds. VM6 which runs on the same host as VM5 also</p>
    <p>gets a slight boost from the increase in queue depth allo</p>
    <p>cated to this host. The array also becomes more efficient</p>
    <p>and this benefit is given to all the active VMs in propor</p>
    <p>tion to their shares. After VM2 becomes idle, RP2 gets</p>
    <p>higher IOPS than RP3 due to its higher shares.</p>
    <p>Finally VM3 goes idle and VM6 gets the benefit. There</p>
    <p>is not much benefit to the other workloads when the se</p>
    <p>quential workload becomes idle. But still the reservations</p>
    <p>are always met and the workloads under RP2 and RP3 are</p>
    <p>roughly in the ratio of 2 : 1. This experiment shows the</p>
    <p>flow of resources within a resource pool and the isolation</p>
    <p>between pools.</p>
    <p>We compared Storage Resource Pool with a state-of-the</p>
    <p>art system that supports reservation, limit and shares. We</p>
    <p>ran both PARDA [6] and mClock [8] together on ESX</p>
    <p>hosts. PARDA does proportional allocation for VMs</p>
    <p>based on the share settings. PARDA works across a clus</p>
    <p>ter of hosts by leveraging a control algorithm that is very</p>
    <p>similar to FAST TCP [11] for flow control in networks.</p>
    <p>mClock is used as the local scheduler which supports</p>
    <p>reservations, limits and shares for VMs on the same host.</p>
    <p>Since PARDA and mClock do not support resource</p>
    <p>pools, we created a single pool with VMs as its children</p>
    <p>and set the controls only at the VM-level. We found that</p>
  </div>
  <div class="page">
    <p>Experiment 2 Setup</p>
    <p>Webservers &lt;600,Max,3&gt;</p>
    <p>Mail &lt;100, 1500,2&gt;</p>
    <p>Oltp &lt;100,1500,2&gt;</p>
    <p>VM have R=0 S=1 L=Max</p>
    <p>Dvdstore &lt;200,Max,1&gt;</p>
  </div>
  <div class="page">
    <p>Experiment 2: Results</p>
    <p>We ran this with &amp; without SRPs (R=1,L=Max and S=1, for all)</p>
    <p>Setting pool level controls helps VMs receive higher/lower performance</p>
    <p>Next we tested storage resource pools for more realis</p>
    <p>tic enterprise workloads with bursty arrivals, variable</p>
    <p>locality of reference, variable IO sizes and variable de</p>
    <p>mand. We used four different workloads for this experi</p>
    <p>ment: Filebench-Mail, Filebench-Webserver, Filebench</p>
    <p>Oltp and DVDStore.</p>
    <p>The first three workloads are based on different per</p>
    <p>sonalities of the well-known Filebench workload emula</p>
    <p>tor [18], and the fourth workload is based on DVDStore</p>
    <p>[2] database test suite, which represents a complete on</p>
    <p>line e-commerce application running on SQL database.</p>
    <p>For each VM, we used one OS disk and one or more data</p>
    <p>disks. These eight VMs, with a total of nineteen virtual</p>
    <p>disks, were spread across three ESX hosts accessing the</p>
    <p>same underlying device using VMFS.</p>
    <p>Root</p>
    <p>RP-ws</p>
    <p>RP-mail</p>
    <p>ws1 ws2</p>
    <p>RP-oltp</p>
    <p>Host1 Host2</p>
    <p>RP-dvdstore</p>
    <p>Host3</p>
    <p>ws3 mail1 mail2 oltp1 oltp2 dvdstore</p>
    <p>Figure 9: SRP configuration for Enterprise workloads</p>
    <p>ws1 ws2 ws3 mail1 mail2 oltp1 oltp2 DVDstoreT hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>O or</p>
    <p>de rs</p>
    <p>/M in</p>
    <p>fo r D</p>
    <p>V D</p>
    <p>st or</p>
    <p>e, O</p>
    <p>ps /s</p>
    <p>fo r r</p>
    <p>es t)</p>
    <p>Higher Throughput due to Reservations</p>
    <p>Lower Throughput due to Limit</p>
    <p>SRP-with-controls SRP-without-controls</p>
    <p>Figure 10: Comparison of application level throughput</p>
    <p>with and without SRP</p>
    <p>For this experiment, we partitioned these four work</p>
    <p>loads into four different resource pools. These pools are</p>
    <p>called RP-mail, RP-oltp, RP-ws and RP-dvdstore respec</p>
    <p>tively. RP-mail contains two VMs running the mail server</p>
    <p>workload, RP-ws contains three VMs running the web</p>
    <p>server workload, RP-oltp contains two VMs running the</p>
    <p>oltp workload and RP-dvdstore contains one VM running</p>
    <p>the DVDStore workload.</p>
    <p>Figure 9 shows the settings for these resource pools</p>
    <p>and the individual VMs. First we ran these workloads</p>
    <p>ws1 ws2 ws3 mail1 mail2 oltp1 oltp2 DVDstore</p>
    <p>A ve</p>
    <p>ra ge</p>
    <p>L at</p>
    <p>en cy</p>
    <p>(M s)</p>
    <p>Lower Latency due to Reservations</p>
    <p>Higher Latency due to Limit</p>
    <p>SRP-with-controls SRP-without-controls</p>
    <p>Figure 11: Comparison of application level average la</p>
    <p>tency with and without SRP</p>
    <p>with no reservation, infinite limit and equal shares. Then</p>
    <p>we set reservations and limit at the pool level to favor</p>
    <p>some workloads over others. The VM-level settings were</p>
    <p>unchanged. We set a reservation of 600 IOPS for web</p>
    <p>server pool (RP-ws) and a reservation of 200 IOPS for</p>
    <p>RP-dvdstore. This reflects the users concern that these</p>
    <p>workloads have a smaller latency. We also set a limit of</p>
    <p>done to contain the impact of these very bursty VMs on</p>
    <p>others. We ran all these workloads together on the same</p>
    <p>underlying Equallogic datastore for 30 minutes, once for</p>
    <p>each setting.</p>
    <p>Figures 10 and 11 show the overall application-level</p>
    <p>throughput in terms of Ops/sec (orders/min in case of</p>
    <p>DVDStore) and application-level average latency (in ms)</p>
    <p>for all VMs. Since we had set a reservation on the web</p>
    <p>server and dvdstore pools, those VMs got lower latency</p>
    <p>and higher IOPS compared to other VMs. On the other</p>
    <p>hand the mail server VMs got higher latency because their</p>
    <p>aggregate demand is higher than the limit of 1500 IOPS.</p>
    <p>Interestingly, the effect on oltp VMs was much smaller</p>
    <p>because their overall demand is close to 1500 IOPS, so</p>
    <p>the limit didnt have as much of an impact.</p>
    <p>This shows that by setting controls at the resource pool</p>
    <p>level, one can effectively isolate the workloads from one</p>
    <p>another. An advantage of setting controls at the resource</p>
    <p>pool level is that one doesnt have to worry about per VM</p>
    <p>controls, and VMs within a pool can take advantage of</p>
    <p>each others idleness.</p>
    <p>In this paper we studied the problem of doing hierarchi</p>
    <p>cal IO resource allocation in a distributed environment,</p>
    <p>where VMs running across multiple hosts are accessing</p>
    <p>the same underlying storage. We propose a novel and</p>
    <p>powerful abstraction called storage resource pools (SRP),</p>
    <p>Next we tested storage resource pools for more realis</p>
    <p>tic enterprise workloads with bursty arrivals, variable</p>
    <p>locality of reference, variable IO sizes and variable de</p>
    <p>mand. We used four different workloads for this experi</p>
    <p>ment: Filebench-Mail, Filebench-Webserver, Filebench</p>
    <p>Oltp and DVDStore.</p>
    <p>The first three workloads are based on different per</p>
    <p>sonalities of the well-known Filebench workload emula</p>
    <p>tor [18], and the fourth workload is based on DVDStore</p>
    <p>[2] database test suite, which represents a complete on</p>
    <p>line e-commerce application running on SQL database.</p>
    <p>For each VM, we used one OS disk and one or more data</p>
    <p>disks. These eight VMs, with a total of nineteen virtual</p>
    <p>disks, were spread across three ESX hosts accessing the</p>
    <p>same underlying device using VMFS.</p>
    <p>Root</p>
    <p>RP-ws</p>
    <p>RP-mail</p>
    <p>ws1 ws2</p>
    <p>RP-oltp</p>
    <p>Host1 Host2</p>
    <p>RP-dvdstore</p>
    <p>Host3</p>
    <p>ws3 mail1 mail2 oltp1 oltp2 dvdstore</p>
    <p>Figure 9: SRP configuration for Enterprise workloads</p>
    <p>ws1 ws2 ws3 mail1 mail2 oltp1 oltp2 DVDstoreT hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>O or</p>
    <p>de rs</p>
    <p>/M in</p>
    <p>fo r D</p>
    <p>V D</p>
    <p>st or</p>
    <p>e, O</p>
    <p>ps /s</p>
    <p>fo r r</p>
    <p>es t)</p>
    <p>Higher Throughput due to Reservations</p>
    <p>Lower Throughput due to Limit</p>
    <p>SRP-with-controls SRP-without-controls</p>
    <p>Figure 10: Comparison of application level throughput</p>
    <p>with and without SRP</p>
    <p>For this experiment, we partitioned these four work</p>
    <p>loads into four different resource pools. These pools are</p>
    <p>called RP-mail, RP-oltp, RP-ws and RP-dvdstore respec</p>
    <p>tively. RP-mail contains two VMs running the mail server</p>
    <p>workload, RP-ws contains three VMs running the web</p>
    <p>server workload, RP-oltp contains two VMs running the</p>
    <p>oltp workload and RP-dvdstore contains one VM running</p>
    <p>the DVDStore workload.</p>
    <p>Figure 9 shows the settings for these resource pools</p>
    <p>and the individual VMs. First we ran these workloads</p>
    <p>ws1 ws2 ws3 mail1 mail2 oltp1 oltp2 DVDstore</p>
    <p>A ve</p>
    <p>ra ge</p>
    <p>L at</p>
    <p>en cy</p>
    <p>(M s)</p>
    <p>Lower Latency due to Reservations</p>
    <p>Higher Latency due to Limit</p>
    <p>SRP-with-controls SRP-without-controls</p>
    <p>Figure 11: Comparison of application level average la</p>
    <p>tency with and without SRP</p>
    <p>with no reservation, infinite limit and equal shares. Then</p>
    <p>we set reservations and limit at the pool level to favor</p>
    <p>some workloads over others. The VM-level settings were</p>
    <p>unchanged. We set a reservation of 600 IOPS for web</p>
    <p>server pool (RP-ws) and a reservation of 200 IOPS for</p>
    <p>RP-dvdstore. This reflects the users concern that these</p>
    <p>workloads have a smaller latency. We also set a limit of</p>
    <p>done to contain the impact of these very bursty VMs on</p>
    <p>others. We ran all these workloads together on the same</p>
    <p>underlying Equallogic datastore for 30 minutes, once for</p>
    <p>each setting.</p>
    <p>Figures 10 and 11 show the overall application-level</p>
    <p>throughput in terms of Ops/sec (orders/min in case of</p>
    <p>DVDStore) and application-level average latency (in ms)</p>
    <p>for all VMs. Since we had set a reservation on the web</p>
    <p>server and dvdstore pools, those VMs got lower latency</p>
    <p>and higher IOPS compared to other VMs. On the other</p>
    <p>hand the mail server VMs got higher latency because their</p>
    <p>aggregate demand is higher than the limit of 1500 IOPS.</p>
    <p>Interestingly, the effect on oltp VMs was much smaller</p>
    <p>because their overall demand is close to 1500 IOPS, so</p>
    <p>the limit didnt have as much of an impact.</p>
    <p>This shows that by setting controls at the resource pool</p>
    <p>level, one can effectively isolate the workloads from one</p>
    <p>another. An advantage of setting controls at the resource</p>
    <p>pool level is that one doesnt have to worry about per VM</p>
    <p>controls, and VMs within a pool can take advantage of</p>
    <p>each others idleness.</p>
    <p>In this paper we studied the problem of doing hierarchi</p>
    <p>cal IO resource allocation in a distributed environment,</p>
    <p>where VMs running across multiple hosts are accessing</p>
    <p>the same underlying storage. We propose a novel and</p>
    <p>powerful abstraction called storage resource pools (SRP),</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Storage resource pools provide a powerful abstraction to the user  SRPs are able to provide isolation between pools &amp; sharing within</p>
    <p>pools  SRPs enforces controls across hosts in a distributed manner</p>
    <p>Future work:  Automatically set R,L,S values based on latency requirements or other input  Test on SDD-based and multi-tiered storage devices</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Q &amp; A</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Backup slides</p>
  </div>
  <div class="page">
    <p>Implementation Details</p>
    <p>SRP is implemented as user-world on ESX  Steps followed by SRP module: 1. Publish per-VM demands on a shared file 2. Read this shared file and aggregate VM demand up the tree</p>
  </div>
  <div class="page">
    <p>How to do QueueDepth divvy?</p>
    <p>Convert Q(t+1) to IOPS using Littles law:</p>
    <p>Divvy Qiops using R,L,S and demand across the tree  Let VM entitlement Ei = divvied value</p>
    <p>Host queue depth is set in proportion to its shares of VM entitlements</p>
    <p>Qhost(t +1)= Ei</p>
    <p>VMsonhost ! Qiops</p>
    <p>*Q(t +1)</p>
    <p>Qiops(t +1)= Q(t +1) L</p>
  </div>
  <div class="page">
    <p>Break Cluster-level Tree into per Datastore Tree</p>
    <p>Customers specify resource pools at datastore cluster level  SDRS can periodically splits them into per datastore resource pools</p>
    <p>Cluster-level SRP</p>
    <p>Datastore 1</p>
    <p>Datastore 2</p>
    <p>Datastore-level RPs</p>
  </div>
</Presentation>
