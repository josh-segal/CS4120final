<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Linear Time Constituency Parsing with</p>
    <p>RNNs and Dynamic Programming</p>
    <p>Juneki Hong 1 Liang Huang 1,2</p>
  </div>
  <div class="page">
    <p>Span Parsing is SOTA in Constituency Parsing</p>
    <p>Cross+Huang 2016 introduced Span Parsing  But with greedy decoding.</p>
    <p>Stern et al. 2017 had Span Parsing with Exact Search and Global Training  But was too slow: O(n3)</p>
    <p>Can we get the best of both worlds?  Something that is both fast and accurate?</p>
    <p>S pe</p>
    <p>ed</p>
    <p>Cross + Huang 2016</p>
    <p>Stern et al. 2017</p>
    <p>Our Work</p>
    <p>Kitaev + Klein 2018 Joshi et al. 2018</p>
    <p>New at ACL 2018! Also Span Parsing!</p>
  </div>
  <div class="page">
    <p>Both Fast and Accurate!</p>
    <p>Baseline Chart Parser (Stern et al. 2017a) 91.79</p>
    <p>Our Linear Time Parser 91.97</p>
    <p>ch ar</p>
    <p>t p ar</p>
    <p>si ng</p>
    <p>our work</p>
  </div>
  <div class="page">
    <p>In this talk, we will discuss:</p>
    <p>Linear Time Constituency Parsing using dynamic programming  Going slower in order to go faster: O(n3)  O(n4)  O(n)  Cube Pruning to speed up Incremental Parsing with Dynamic Programming  From O(n b2) to O(n b log b)  An improved loss function for Loss-Augmented Decoding  2nd highest accuracy among single systems trained on PTB only</p>
    <p>O(2n) ! O(n3) ! O(n4) O(nb2) O(nb log b) &lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;</p>
  </div>
  <div class="page">
    <p>Span Parsing</p>
    <p>Span differences are taken from an encoder (in our case: a bi-LSTM)</p>
    <p>A span is scored and labeled by a feed-forward network.</p>
    <p>The score of a tree is the sum of all the labeled span scores</p>
    <p>s</p>
    <p>stree(t) = P</p>
    <p>(i,j,X)2t s(i, j, X)</p>
    <p>(fj  fi, bi  bj)</p>
    <p>s(i, j, X)</p>
    <p>/ss</p>
    <p>f0 f1 f2 f3 f4 f5</p>
    <p>b1 b2 b3 b4 b5b0</p>
    <p>Cross + Huang 2016 Stern et al. 2017 Wang + Chang 2016</p>
  </div>
  <div class="page">
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>S</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>VP</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>NP</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>temp</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>?</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>NP</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>temp</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>?</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>NP</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>NP</p>
    <p>NP</p>
    <p>S</p>
    <p>temp</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>PP</p>
    <p>IN</p>
    <p>after</p>
    <p>?</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>NP</p>
    <p>PP</p>
    <p>NP</p>
    <p>S</p>
    <p>temp</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>?</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Action Label Stack</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>Incremental Span Parsing Example</p>
    <p>Eat VB</p>
    <p>cream NN</p>
    <p>after IN</p>
    <p>lunch NN</p>
    <p>S</p>
    <p>VP</p>
    <p>PP</p>
    <p>NP</p>
    <p>NN</p>
    <p>lunch</p>
    <p>IN</p>
    <p>after</p>
    <p>NP</p>
    <p>NN</p>
    <p>cream</p>
    <p>NN</p>
    <p>ice</p>
    <p>VB</p>
    <p>Eat</p>
    <p>Action Label Stack</p>
    <p>NP</p>
    <p>PP</p>
    <p>NP</p>
    <p>S-VP</p>
    <p>Cross + Huang 2016</p>
  </div>
  <div class="page">
    <p>How Many Possible Parsing Paths?</p>
    <p>O(2n)</p>
    <p>2 actions per state.  O(2n)</p>
  </div>
  <div class="page">
    <p>Equivalent Stacks?</p>
    <p>Observe that all stacks that end with (i, j) will be treated the same!  Until (i, j) is popped off.</p>
    <p>So we can treat these as temporarily equivalent, and merge.</p>
    <p>[(0, 2), (2, 7), (7, 9)]</p>
    <p>[(0, 3), (3, 7), (7, 9)] [, (7, 9)]becomes</p>
    <p>Graph-Structured Stack (Tomita 1988; Huang + Sagae 2010)</p>
  </div>
  <div class="page">
    <p>Equivalent Stacks?</p>
    <p>Observe that all stacks that end with (i, j) will be treated the same!  Until (i, j) is popped off.</p>
    <p>This is our new stack representation.</p>
    <p>[, (2, 7)]</p>
    <p>[, (3, 7)]</p>
    <p>[, (0, 2)]</p>
    <p>Left Pointers</p>
    <p>[, (7, 9)]</p>
    <p>[, (0, 3)]</p>
    <p>Graph-Structured Stack (Tomita 1988; Huang + Sagae 2010)</p>
  </div>
  <div class="page">
    <p>Equivalent Stacks?</p>
    <p>Observe that all stacks that end with (i, j) will be treated the same!  Until (i, j) is popped off.</p>
    <p>[, (k, i)] [, (i, j)] [, (k, j)]</p>
    <p>Reduce Actions: O(n3)</p>
    <p>[, (2, 9)]</p>
    <p>[, (3, 9)]</p>
    <p>Left Pointers</p>
    <p>[, (2, 7)][, (0, 2)] [, (7, 9)]</p>
    <p>[, (3, 7)] [, (0, 3)]</p>
    <p>red uce</p>
    <p>reduce</p>
    <p>Graph-Structured Stack (Tomita 1988; Huang + Sagae 2010)</p>
  </div>
  <div class="page">
    <p>Dynamic Programming: Merging Stacks</p>
    <p>O(2n)</p>
    <p>Temporarily merging stacks will make our state space polynomial.</p>
    <p>And our parsing state is represented by top span (i, j).</p>
    <p>O(n3)</p>
  </div>
  <div class="page">
    <p>Shift-Reduce Parsers are traditionally action synchronous.  This makes beam-search straight forward.  We will also do the same</p>
    <p>But will show that this will slow down our DP (before applying beam-search)</p>
    <p>Becoming Action Synchronous</p>
    <p>O(2n)</p>
    <p>O(n4)</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh</p>
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1)</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh</p>
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Left Pointers</p>
    <p>Gold Parse</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Shift (2, 3)</p>
    <p>Left Pointers</p>
    <p>Gold Parse</p>
  </div>
  <div class="page">
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Shift (2, 3)</p>
    <p>Reduce (1, 3)</p>
    <p>Left Pointers 1 2 3 4</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>Gold Parse</p>
  </div>
  <div class="page">
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Shift (2, 3)</p>
    <p>Reduce (1, 3)</p>
    <p>Reduce (0, 3)</p>
    <p>Left Pointers 1 2 3 4 5</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>Gold Parse</p>
  </div>
  <div class="page">
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Shift (2, 3)</p>
    <p>Reduce (1, 3)</p>
    <p>Reduce (0, 3)</p>
    <p>Shift (3, 4)</p>
    <p>Left Pointers</p>
    <p>Gold Parse</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Action Synchronous Parsing Example</p>
    <p>Gold: Shift (0,1) Shift (1,2)</p>
    <p>Shift (2, 3)</p>
    <p>Reduce (1, 3)</p>
    <p>Reduce (0, 3)</p>
    <p>Shift (3, 4)</p>
    <p>Shift (4, 5)</p>
    <p>Reduce (3, 5)</p>
    <p>Reduce (0, 5)</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Runtime Analysis: .</p>
    <p>O(n4)</p>
    <p>Huang+Sagae 2010</p>
  </div>
  <div class="page">
    <p>Runtime Analysis: .</p>
    <p>O(n4)</p>
    <p>#steps: 2n  1 = O(n)</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Huang+Sagae 2010</p>
  </div>
  <div class="page">
    <p>Runtime Analysis: .</p>
    <p>O(n4) O(n2)#states per step:</p>
    <p>(i, j)</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Huang+Sagae 2010</p>
  </div>
  <div class="page">
    <p>Runtime Analysis: .</p>
    <p>O(n4) (i, j)</p>
    <p>O(n2)#states per step:</p>
    <p>statesO(n3)</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Huang+Sagae 2010</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>statesO(n3)</p>
    <p>Runtime Analysis: .</p>
    <p>O(n4) (i, j)</p>
    <p>#left pointers per state: O(n)</p>
    <p>O(n2)#states per step:</p>
    <p>Check out the paper for our new theorem:</p>
    <p>Huang+Sagae 2010</p>
    <p>l = l  2( j  i) + 1 Thanks to Dezhong Deng!</p>
    <p>l: [, (k, i)] l: [, (i, j)] l+1: [, (k, j)]</p>
    <p>O(n4)</p>
  </div>
  <div class="page">
    <p>Going slower to go faster</p>
    <p>Our Action-Synchronous algorithm has a slower runtime than CKY!  However, it also becomes straightforward to prune using beam search.  So we can achieve a linear runtime in the end.</p>
    <p>DP+GSS</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5) O(n4)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>DP+GSS+Beam</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5) O(n)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5) (approx. DP)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>+ b ea m</p>
    <p>DP+GSS</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5) O(n4)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (3,5) (2,5)</p>
    <p>(2,3) (3,4) (1,4) (4,5) (3,5)</p>
    <p>(0,3) (2,4) (0,4) (4,5)</p>
    <p>(3,4)</p>
    <p>DP+GSS+Beam</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5) O(n)</p>
    <p>(0,2) (2,3) (0,3) (3,4) (0,4) (4,5) (approx. DP)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh r</p>
    <p>r</p>
    <p>sh</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>rr</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>+ b ea m</p>
  </div>
  <div class="page">
    <p>Now our runtime is O(n).</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
  </div>
  <div class="page">
    <p>But this O(n) is hiding a constant.</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5)</p>
    <p>(2,3) (0,3) (3,4)</p>
  </div>
  <div class="page">
    <p>But this O(n) is hiding a constant.</p>
    <p>O(b)</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5)</p>
    <p>(2,3) (0,3) (3,4)</p>
    <p>left pointers per state</p>
    <p>b states per action step</p>
    <p>O(nb2) runtime</p>
  </div>
  <div class="page">
    <p>Cube Pruning</p>
    <p>We can apply cube pruning to make</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>O(nb log b)</p>
    <p>Chiang 2007</p>
    <p>Huang+Chiang 2007</p>
  </div>
  <div class="page">
    <p>Cube Pruning</p>
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>We can apply cube pruning to make O(nb log b)</p>
    <p>By pushing all states and their left pointers into a heap</p>
    <p>Chiang 2007</p>
    <p>Huang+Chiang 2007</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Cube Pruning</p>
    <p>By pushing all states and their left pointers into a heap  And popping the top b unique subsequent states</p>
    <p>We can apply cube pruning to make O(nb log b)</p>
    <p>Chiang 2007</p>
    <p>Huang+Chiang 2007</p>
  </div>
  <div class="page">
    <p>(0,1) (1,2) (2,3) (3,4) (4,5) (3,5) (2,5) (1,5) (0,5)</p>
    <p>(0,2) (1,3) (2,4) (4,5) (4,5) (3,5)</p>
    <p>(2,3) (0,3) (3,4) (0,4) (4,5)</p>
    <p>sh sh sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>sh</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>sh</p>
    <p>r</p>
    <p>r</p>
    <p>r</p>
    <p>Cube Pruning</p>
    <p>By pushing all states and their left pointers into a heap  And popping the top b unique subsequent states  First time Cube-Pruning has been applied to Incremental Parsing</p>
    <p>We can apply cube pruning to make O(nb log b)</p>
    <p>Chiang 2007</p>
    <p>Huang+Chiang 2007</p>
  </div>
  <div class="page">
    <p>Runtime on PTB and Discourse Treebank</p>
    <p>ch ar</p>
    <p>t p ar</p>
    <p>si ng</p>
    <p>our work</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Structured SVM approach (Taskar et al. 2003; Stern et al. 2017):  Goal: Score the gold tree higher than all others by a margin:</p>
    <p>Loss Augmented Decoding:  During Training: Return the most violated tree (i.e., highest augmented score):</p>
    <p>Minimize: 43</p>
    <p>t = arg max t</p>
    <p>s(t) + (t, t)</p>
    <p>s(t) + (t, t)</p>
    <p>s(t)</p>
  </div>
  <div class="page">
    <p>Loss Function</p>
    <p>Counts the incorrectly labeled spans in the tree (Stern et al. 2017)  Happens to be decomposable, so can even be used to compare partial trees.</p>
    <p>(t, t) = P</p>
    <p>(i,j,X)2t 1  X 6= t(i,j)</p>
  </div>
  <div class="page">
    <p>Novel Cross-Span Loss</p>
    <p>We observe that the null label  is used in two different ways:  To facilitate ternary and n-ary branching trees.  As a default label for incorrect spans that violate other gold spans.</p>
    <p>i j</p>
    <p>t*(i, j) =  i j</p>
    <p>t*(i, j) =</p>
  </div>
  <div class="page">
    <p>We modify the loss to account for incorrect spans in the tree.</p>
    <p>Novel Cross-Span Loss</p>
    <p>(t, t) = P</p>
    <p>(i,j,X)2t 1  X 6= t(i,j)</p>
  </div>
  <div class="page">
    <p>We modify the loss to account for incorrect spans in the tree.</p>
    <p>Indicates whether (i, j) is crossing a span in the gold tree</p>
    <p>Still decomposable over spans, so can be used to compare partial trees.</p>
    <p>cross(i, j, t) = 9 (k, l) 2 t :</p>
    <p>Novel Cross-Span Loss</p>
    <p>(t, t) = P</p>
    <p>(i,j,X)2t 1  X 6= t(i,j) _ cross(i, j, t</p>
    <p>)  i j</p>
  </div>
  <div class="page">
    <p>Take the largest augmented loss value across all time steps.  This is the Max-Violation, that we use to train.</p>
    <p>Max-Violation Updates</p>
    <p>Huang et. al. 2012 ea</p>
    <p>rly</p>
    <p>m ax</p>
    <p>vi</p>
    <p>ol at</p>
    <p>io n</p>
    <p>la te</p>
    <p>st</p>
    <p>fu ll</p>
    <p>(s ta nd ar d)</p>
    <p>best in the beam</p>
    <p>worst in the beam falls off</p>
    <p>the beam biggest violation</p>
    <p>last valid update</p>
    <p>correct sequence</p>
    <p>invalid update!</p>
  </div>
  <div class="page">
    <p>Comparison with Baseline Chart Parser</p>
    <p>Model Note F1 (PTB test)</p>
    <p>Stern et al. (2017a) Baseline Chart Parser 91.79</p>
    <p>+our cross-span loss 91.81</p>
    <p>Our Work Beam 15 91.84</p>
    <p>Beam 20 91.97</p>
  </div>
  <div class="page">
    <p>Comparison to Other Parsers</p>
    <p>Model Note F1</p>
    <p>Durett + Klein 2015 91.1</p>
    <p>Cross + Huang 2016 Original Span Parser 91.3</p>
    <p>Liu + Zhang 2016 91.7</p>
    <p>Dyer et al. 2016 Discriminative 91.7</p>
    <p>Stern et al. 2017a Baseline Chart Parser 91.79</p>
    <p>Stern et al. 2017c Separate Decoding 92.56</p>
    <p>Our Work Beam 20 91.97</p>
    <p>Model Note F1</p>
    <p>Vinyals et al. 2015 Ensemble 90.5</p>
    <p>Dyer et al. 2016 Generative Reranking 93.3</p>
    <p>Choe + Charniak 2016 Reranking 93.8</p>
    <p>Fried et al. 2017 Ensemble Reranking 94.25</p>
    <p>Reranking, Ensemble, Extra DataPTB only, Single Model, End-to-End</p>
  </div>
  <div class="page">
    <p>Conclusions  Linear-Time, Span-Based Constituency Parsing with Dynamic Programming  Cube-Pruning to speedup Incremental Parsing with Dynamic Programming  Cross-Span Loss extension for improving Loss-Augmented Decoding  Result: Faster and more accurate than cubic-time Chart Parsing  2nd highest accuracy for single-model end-to-end systems trained on PTB only  Stern et al. 2017c is more accurate, but with separate decoding, and is much slower</p>
    <p>After this ACL, definitely no longer true. (e.g. Joshi et al. 2018, Kitaev+Klein 2018)  But both are Span-Based Parsers and can be linearized in the same way!</p>
    <p>O(2n) ! O(n3) ! O(n4) O(nb2) O(nb log b) &lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;&lt;latexit sha1_base64=&quot;J5yt+Ykz1sg9to8zcqinVqW85aE=&quot;&gt;AAACPnicbZBNTwIxEIa7+IX4hXr00khM4EJ2kUSPRC/ewEQ+ElhIt9tdGrrtpu1qCOGXefE3ePPoxYPGePVogT0oOEmTd56ZyXReL2ZUadt+sTJr6xubW9nt3M7u3v5B/vCopUQiMWliwYTseEgRRjlpaqoZ6cSSoMhjpO2Nrmf19j2Rigp+p8cxcSMUchpQjLRBg3yzXqz0eQn2JA2HGkkpHmC9yPvnq6hqECPIV1rMcuj1K8vEpCKEXmmQL9hlex5wVTipKIA0GoP8c88XOIkI15ghpbqOHWt3gqSmmJFprpcoEiM8QiHpGslRRJQ7mZ8/hWeG+DAQ0jyu4Zz+npigSKlx5JnOCOmhWq7N4H+1bqKDS3dCeZxowvFiUZAwaM6deQl9KgnWbGwEwpKav0I8RBJhbRzPGROc5ZNXRatSduyyc1st1K5SO7LgBJyCInDABaiBG9AATYDBI3gF7+DDerLerE/ra9GasdKZY/AnrO8fb6arpA==&lt;/latexit&gt;</p>
  </div>
  <div class="page">
    <p>Thank you! Questions?</p>
    <p>ch ar</p>
    <p>t p ar</p>
    <p>si ng</p>
    <p>our work</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>Dezhong Deng for his theorem for predecessor states.  And his mathematical proofreading of the training sections.</p>
    <p>Mitchell Stern for releasing his code and his suggestions.</p>
  </div>
</Presentation>
