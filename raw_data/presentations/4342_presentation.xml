<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Automatic Metric Validation for Grammatical Error Correction</p>
    <p>Leshem Choshen and Omri Abend</p>
    <p>Hebrew University Jerusalem Israel</p>
  </div>
  <div class="page">
    <p>Meta view</p>
    <p>The task - Level 1</p>
    <p>Evaluation - Level 2</p>
    <p>Evaluation of evaluation - Level 3</p>
    <p>Peers - Level 4</p>
  </div>
  <div class="page">
    <p>the task</p>
    <p>Input: a text which is perhaps ungramatical  Output: a grammatical text saying the same</p>
    <p>meaning/content.</p>
    <p>Example: However , there are both sides of stories</p>
  </div>
  <div class="page">
    <p>The task</p>
    <p>Input: a text which is perhaps ungramatical ungrammatical  Output: a grammatical text saying conveying the same</p>
    <p>meaning/content.</p>
    <p>Example: However , there are both sides of stories  However , there are two sides to the story.</p>
  </div>
  <div class="page">
    <p>The task - Level 1</p>
    <p>Evaluation - Level 2</p>
    <p>Evaluation of evaluation - Level 3</p>
    <p>Peers - Level 4</p>
  </div>
  <div class="page">
    <p>Test Set</p>
    <p>Learner sentences (perhaps ungrammatical)  References - word edits and the error type corrected by them</p>
    <p>Since ancient times , human interact with others face by face .  Since ancient times , human humans (Noun number) interact with others face by to (Wrong Preposition) face .</p>
  </div>
  <div class="page">
    <p>Metrics</p>
    <p>There are many suggestions for evaluation metrics: M2, GLEU, I-measure, LT, etc. More on that in the paper.</p>
  </div>
  <div class="page">
    <p>The task - Level 1</p>
    <p>Evaluation - Level 2</p>
    <p>Evaluation of evaluation - Level 3</p>
    <p>Peers - Level 4</p>
  </div>
  <div class="page">
    <p>Human Rankings</p>
    <p>Sentence 1 You have become powerful, I sense the dark side in you. 2 Powerful you have become, I sense the dark side in you. 2 You have become powerful, the dark side I sense in you. 3 Powerful you have become, the dark side I sense in you.</p>
  </div>
  <div class="page">
    <p>Existing Metric Validation Human Rankings</p>
    <p>Annotation  Humans rank system corrections  Two benchmarks  GJG15 (Grundkiewicz et al. 2015), and</p>
    <p>NSPT15 (Napoles et al. 2015).</p>
    <p>Score  correlation between metric and human rankings  Rank each system by the metric scores of its outputs  Rank each system by the human ranks of its outputs</p>
    <p>Methodologically troublesome  Correlate the two</p>
  </div>
  <div class="page">
    <p>Human Rankings - not a perfect solution What Machine Translation has already found</p>
    <p>Costly  Low agreement</p>
    <p>Ranking is hard (correcting is easy)  Some sentences are uncomparable</p>
    <p>Not detailed  ...</p>
    <p>Combined GJG15 NSPT15  P-val  Rank  Rank</p>
    <p>GLEU 0.771 0.001 0.512 1 0.758 1 LT 0.692 0.006 0.358 4 0.615 3 M2 0.626 0.017 0.398 3 0.703 2 BLEU 0.143 0.626 0.455 2 -0.126 6</p>
  </div>
  <div class="page">
    <p>Human Rankings (CHR) - inherent biases The vicious loop</p>
    <p>Problematic:  Systems have similar biases  under-correct &amp; favor correcting</p>
    <p>specific error types (Choshen &amp; Abend 2018)  Metrics are evaluated based on distribution of errors in</p>
    <p>outputs, rather than true distribution</p>
  </div>
  <div class="page">
    <p>MAEGE Methodology for Automatic Evaluation of GEC Evaluation</p>
    <p>Annotation  Humans correct errors in sentences  Widely available  regular GEC corpora</p>
    <p>Lattice  graded quality  Original sentences Oi  Partial corrections, apply some edits  Reference sentences R(j)i</p>
    <p>R (1) 1 R</p>
    <p>(1) k</p>
    <p>O1</p>
    <p>R (n) 1 R</p>
    <p>(n) k</p>
    <p>On</p>
  </div>
  <div class="page">
    <p>Human Rankings</p>
    <p>Since ancient times , human humans (Noun number) interact with others face by to (Wrong Preposition) face .</p>
    <p>Corrections Sentence 2 Since ancient times , humans interact with others face to face . 1 Since ancient times , human interact with others face to face . 0 Since ancient times , human interact with others face by face .</p>
  </div>
  <div class="page">
    <p>Corpus Level</p>
    <p>Models  Set of randomly chosen corrections  Models score</p>
    <p>maege score  the expected number of applied edits  We sample models from the lattices with different distributions</p>
    <p>Score  correlation between the two rankings</p>
    <p>Interesting results  Positive low correlation with CHR  The best metric is LT (number of detected errors)  With precision-oriented models maege is similar to CHR</p>
    <p>Indication that CHR is biased due to precision-oriented models</p>
  </div>
  <div class="page">
    <p>Types</p>
  </div>
  <div class="page">
    <p>Types - sensitivity analysis Surprising results</p>
  </div>
  <div class="page">
    <p>The task - Level 1</p>
    <p>Evaluation - Level 2</p>
    <p>Evaluation of evaluation - Level 3</p>
    <p>Peers - Level 4</p>
  </div>
  <div class="page">
    <p>Take-home message</p>
    <p>Metrics emphasize some aspects of the task over others.  Metric validation should tell you which  If validation is opaque, metrics and systems may tune towards</p>
    <p>one another (vicious loop)</p>
    <p>maege breaks the loop by not relying on system outputs  Instead compile naturally ranked corpus</p>
  </div>
  <div class="page">
    <p>Take-home message</p>
    <p>Metrics emphasize some aspects of the task over others.  maege breaks the loop by not relying on system outputs  Instead compile naturally ranked corpus  Use maege</p>
  </div>
  <div class="page">
    <p>Take-home message</p>
    <p>Metrics emphasize some aspects of the task over others.  maege breaks the loop by not relying on system outputs  Instead compile naturally ranked corpus  Use maege</p>
    <p>UCCA Semantic Parsing shared task SemEval 2019</p>
  </div>
</Presentation>
