<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Distributed I/O with ParaMEDIC: Experiences with a Worldwide</p>
    <p>Supercomputer</p>
    <p>P. Balaji, W. Feng, H. Lin, J. Archuleta, S. Matsuoka, A. Warren, J. Setubal, E. Lusk, R.</p>
    <p>Thakur, I. Foster, D. S. Katz, S. Jha, K. Shinpaugh, S. Coghlan, D. Reed</p>
    <p>Math. and Computer Science, Argonne National Laboratory</p>
    <p>Computer Science and Engg., Virginia Tech Dept. of Computer Sci., North Carolina State University</p>
    <p>Dept. of Math. And Computing Sci, Tokyo Inst. of Technology</p>
    <p>Virginia Bioinformatics Institute, Virginia Tech Center for Computation and Tech., Louisiana State</p>
    <p>University Scalable Computing and Multicore Division, Microsoft</p>
    <p>Research</p>
  </div>
  <div class="page">
    <p>Distributed Computation and I/O  Growth of combined compute and I/O requirements</p>
    <p>E.g., Genomic sequence search, Large-scale data mining, data visual analytics and communication profiling</p>
    <p>Commonality: Require a lot of compute power and use and generate a lot of data  Data has to be managed for later processing or archival</p>
    <p>Managing large data volumes: Distributed I/O  Non-local access to large compute systems</p>
    <p>Data generated remotely and transferred to local systems</p>
    <p>Resource locality: Applications need compute and storage  Data generated at one site and moved to another</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Distributed I/O: The Necessary Evil  Lot of prior research tries to improve distributed I/O</p>
    <p>Continues to be the elusive holy grail  Not everyone has a lambda grid</p>
    <p>Scientists run jobs on large centers from their local system</p>
    <p>There is just too much data!  Very difficult to achieve high performance for real data</p>
    <p>[1]</p>
    <p>Bandwidth is not everything  Real software requires synchronization (milliseconds)</p>
    <p>High-speed TCP eats up memory  slows down applications</p>
    <p>Data encryption or endianness conversion required in some cases</p>
    <p>Solution: FEDEX ! ISC '08</p>
    <p>[1] Wide Area Filesystem Performance Using Lustre on the Teragrid, S. Simms, G. Pike, D. Balog. Teragrid Conference, 2007</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>Genomic Sequence Search on the Grid</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>ParaMEDIC on a Worldwide Supercomputer</p>
    <p>Experimental Results</p>
    <p>Concluding Remarks</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Why is Sequence Search So Important?</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Challenges in Sequence Search</p>
    <p>Genome database size doubles every 12 months  Compute power doubles 18-24</p>
    <p>months</p>
    <p>Consequence:  Compute time to search this</p>
    <p>database increases  Amount of data generated</p>
    <p>increases</p>
    <p>Parallel Sequence search helps with computational requirements  E.g., mpiBLAST, ScalaBLAST</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Large-scale Sequence Search: Reason 1  The Case of the Missing Genes</p>
    <p>Problem: Most current genes have been detected by</p>
    <p>a gene-finder program, which can miss real genes</p>
    <p>Approach: Every possible location along a genome</p>
    <p>should be checked for the presence of genes</p>
    <p>Solution:</p>
    <p>All-to-all sequence search of all 567 microbial</p>
    <p>genomes that have been completed to date</p>
    <p>but requires more resources than can be</p>
    <p>traditionally found at a single supercomputer center</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Large-scale Sequence Search: Reason 2</p>
    <p>The Search for a Genome Similarity Tree</p>
    <p>Problem: Genome databases are stored as an</p>
    <p>unstructured collection of sequences in a flat ASCII file</p>
    <p>Approach: Correlate sequences by matching each</p>
    <p>sequence with every other</p>
    <p>Solution</p>
    <p>Use results from all-to-all sequence search to create</p>
    <p>genome similarity tree</p>
    <p>but requires more resources than can be traditionally</p>
    <p>found at a single supercomputer center</p>
    <p>Level 1: 250 matches; Level 2: 2502 = 62,500 matches;</p>
    <p>Level 3: 2503 = 15,625,000 matches</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Genomic Sequence Search on the Grid  All-to-all sequence search for microbial genomes</p>
    <p>Potential to solve many unsolved problems</p>
    <p>Resource requirements shoots out of the roof top  Compute: 263 trillion sequence searches</p>
    <p>Storage: Can generate more than a petabyte of data</p>
    <p>Plan:  Use a distributed supercomputer taking compute</p>
    <p>resources from multiple supercomputing centers</p>
    <p>Store output data in a storage center for later processing</p>
    <p>Using distributed compute resources is easy (relatively)</p>
    <p>Storing a petabyte of data remotely?ISC '08</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>Genomic Sequence Search on the Grid</p>
    <p>ParaMEDIC: Framework to Decouple Compute</p>
    <p>and I/O</p>
    <p>ParaMEDIC on a Worldwide Supercomputer</p>
    <p>Experimental Results</p>
    <p>Concluding Remarks</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>ParaMEDIC Overview  ParaMEDIC: Parallel Meta-data Environment for</p>
    <p>Distributed I/O and Computing [2]  Transforms output to application-specific meta</p>
    <p>data  Application generates output data  ParaMEDIC takes over:</p>
    <p>Transforms output to (orders-of-magnitude smaller) application-specific meta-data at the compute site</p>
    <p>Transports meta-data over the WAN to the storage site  Transforms meta-data back to the original data at the</p>
    <p>storage site (host site for the global file-system)</p>
    <p>Similar to compression, yet different  Deals with data as abstract objects, not as a byte</p>
    <p>stream ISC '08</p>
    <p>[2] Semantics-based Distributed I/O with the ParaMEDIC Framework, P. Balaji, W. Feng and H. Lin. IEEE International Conference on High Performance Distributed Computing (HPDC), 2008</p>
  </div>
  <div class="page">
    <p>The ParaMEDIC Framework</p>
    <p>ISC '08</p>
    <p>Applications</p>
    <p>mpiBLAST Communication</p>
    <p>Profiling Remote</p>
    <p>Visualization</p>
    <p>ParaMEDIC Data Tools</p>
    <p>Data Encryption</p>
    <p>Data Integrity</p>
    <p>Communication Services</p>
    <p>Direct Network</p>
    <p>Global Filesystem</p>
    <p>Application Plugins</p>
    <p>mpiBLAST Plugin</p>
    <p>Communication Profiling Plugin</p>
    <p>Basic Compression</p>
    <p>ParaMEDIC API (PMAPI)</p>
    <p>Other Utilities</p>
    <p>Column Parsing</p>
    <p>Data Sorting</p>
  </div>
  <div class="page">
    <p>Tradeoffs in the ParaMEDIC Framework  Trading Computation and I/O</p>
    <p>More computation: Converting output to meta-data and back requires extra work</p>
    <p>Lesser I/O: Only meta-data is transferred over the WAN, so lesser bandwidth usage on the WAN</p>
    <p>But, well, computation is free; I/O is not !</p>
    <p>Trading Portability and Performance  Utility functions help develop application plugins, but</p>
    <p>will always need non-zero effort  Data is dealt has high-level objects: Better chance of</p>
    <p>improved performance</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Sequence Search with mpiBLAST</p>
    <p>ISC '08</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
    <p>Sequential Search of Queries Parallel Search of Queries</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
  </div>
  <div class="page">
    <p>mpiBLAST Meta-Data</p>
    <p>ISC '08</p>
    <p>Query Sequences</p>
    <p>Database Sequences</p>
    <p>Output</p>
    <p>Alignment information for</p>
    <p>a bunch of sequences</p>
    <p>Alignment of two sequences is</p>
    <p>independent of the remaining sequences</p>
    <p>Meta-data (IDs of matched sequences)</p>
    <p>Communicate over the WAN</p>
    <p>Query Sequences</p>
    <p>Temporary Database</p>
    <p>Sequences</p>
  </div>
  <div class="page">
    <p>ParaMEDIC-powered mpiBLAST</p>
    <p>Compute Master I/O Master</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>Query Raw Metadata Query</p>
    <p>Write Results</p>
    <p>Generate Temp Database</p>
    <p>Read Temp Database</p>
    <p>I/O Work e rs</p>
    <p>Com pute Work e rs</p>
    <p>I/O Se rve rs hos ting file</p>
    <p>s ys te m</p>
    <p>The ParaMEDIC Framework</p>
    <p>Compute Sites Storage SiteWAN</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>Genomic Sequence Search on the Grid</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>ParaMEDIC on a Worldwide Supercomputer</p>
    <p>Experimental Results</p>
    <p>Concluding Remarks</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Our Worldwide Supercomputer</p>
    <p>Name Locatio</p>
    <p>n Core</p>
    <p>s Arch.</p>
    <p>Memor y (GB)</p>
    <p>Network Storage</p>
    <p>(TB)</p>
    <p>Distanc e from Storag</p>
    <p>e</p>
    <p>SystemX VT 2200 PPC 970FX 4 IB NFS (30) 11,000</p>
    <p>Breadboard Argonne 128 Opteron 4 10GE NFS (5) 10,000</p>
    <p>Blue Gene/L Argonne 2048 PPC 440 1 Proprietary PVFS (14) 10,000</p>
    <p>SiCortex Argonne 5832 MIPS 3 Proprietary NFS (4) 10,000</p>
    <p>Jazz Argonne 700 Xeon 1-2 GE G/PVFS (20) 10,000</p>
    <p>TeraGrid (UC)</p>
    <p>U. Chicago</p>
    <p>NFS (4) 10,000</p>
    <p>TeraGrid (SDSC)</p>
    <p>San Diego</p>
    <p>GPFS (50) 9,000</p>
    <p>Oliver LSU 512 Xeon 4 IB Lustre (12) 11,000</p>
    <p>Open Science Grid</p>
    <p>U.S. 200 Opteron +</p>
    <p>Xeon 1-2 GE - 11,000</p>
    <p>TSUBAME TiTech 72 Opteron 16 GE Lustre (350) 0</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Dynamic Availability of Compute Clients  Two possible extremes:</p>
    <p>Complete parallelism across all nodes --- single failure will lose all existing output</p>
    <p>Sequential computation of tasks (using different processors to do each task) --- out-of-core computation !</p>
    <p>Hierarchical computation with small-scale parallelism</p>
    <p>Clients maintain very little state  Each client set (a few processors) runs a separate</p>
    <p>instance of mpiBLAST  Each client set gets a task, computes on it and sends</p>
    <p>the output to the storage system</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Performance Optimizations  Architectural Heterogeneity</p>
    <p>Data to be converted to architecture independent format</p>
    <p>Trouble for vanilla mpiBLAST; not so much for ParaMEDIC</p>
    <p>Utilizing Parallelism on Processing Nodes  ParaMEDIC I/O has three parts</p>
    <p>Compute clients, Post-processing servers and I/O servers  Post-processing: Each server handles a different stream</p>
    <p>Simple, but only effective when there are enough streams</p>
    <p>Disconnected or Cached I/O  Clients cache output from multiple tasks locally  Allows data aggregation for better bandwidth and</p>
    <p>merging ISC '08</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>Genomic Sequence Search on the Grid</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>ParaMEDIC on a Worldwide Supercomputer</p>
    <p>Experimental Results</p>
    <p>Concluding Remarks</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>I/O Time Measurements</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
    <p>Absolute Time</p>
    <p>Number of Query Sequence Sets</p>
    <p>I/ O</p>
    <p>T im</p>
    <p>e (</p>
    <p>s e</p>
    <p>c o</p>
    <p>n d</p>
    <p>s )</p>
    <p>Number of Query Sequence Sets</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Storage Bandwidth Utilization (Lustre)</p>
    <p>ISC '08</p>
    <p>mpiBLAST ParaMEDIC MPI-IOTest</p>
    <p>Number of Query Sequence Sets</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>b p</p>
    <p>s )</p>
    <p>I/O Percent</p>
    <p>Compute Percent</p>
    <p>Number of Query Sequence Sets</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
  </div>
  <div class="page">
    <p>Storage Bandwidth Utilization (ext3fs)</p>
    <p>ISC '08</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
    <p>MPI-IO-Test</p>
    <p>Number of Query Sequence Sets</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>b p</p>
    <p>s )</p>
    <p>I/O Percent</p>
    <p>Compute Percent</p>
    <p>Number of Query Sequence Sets</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
  </div>
  <div class="page">
    <p>Microbial Genome Database Search</p>
    <p>Semantic-aware metadata gives scientists 2.5*1014 searches at their finger-tips  All metadata results from all searches can fit on iPod</p>
    <p>Nano  Semantically compressed 1 Petabyte into 4</p>
    <p>Gigabytes (106X)  Usual compression results in 1 PB into 300 TB (3X)</p>
    <p>Semantic Compression</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Preliminary Analysis of the Output</p>
    <p>Analysis of the Similarity Tree  Expect that replicons (i.e., chromosomes) will match</p>
    <p>other replicons reasonably well  But many replicons do not match many other</p>
    <p>replicons  25% of all replicon-replicon searches do not match at</p>
    <p>all! Percentage Not Matched</p>
    <p>Replicon ID</p>
    <p>P e rc</p>
    <p>e n t</p>
    <p>Percentage Matched</p>
    <p>Replicon ID</p>
    <p>P e rc</p>
    <p>e n t</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Distributed I/O on the WAN</p>
    <p>Genomic Sequence Search on the Grid</p>
    <p>ParaMEDIC: Framework to Decouple Compute and</p>
    <p>I/O</p>
    <p>ParaMEDIC on a Worldwide Supercomputer</p>
    <p>Experimental Results</p>
    <p>Concluding Remarks</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Concluding Remarks  Distributed I/O is a necessary evil</p>
    <p>Difficult to get high performance for real data</p>
    <p>Traditional approaches deal with data as a stream of bytes (allows for portability across any type of data)</p>
    <p>We proposed ParaMEDIC  Semantics-based meta-data transformation of data</p>
    <p>Trade Portability for Performance</p>
    <p>Evaluated on a World-wide Supercomputer  Self Sequence searched all completed microbial</p>
    <p>genomes</p>
    <p>Generated a petabyte of data that was stored half-way around the world</p>
    <p>ISC '08</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Email: balaji@mcs.anl.gov Web: http://www.mcs.anl.gov/~balaji</p>
    <p>Acknowledgments:</p>
    <p>U. Chicago: R. Kettimuthu, M. Papka and J. Insley</p>
    <p>Argonne National Lab: N. Desai and R. Bradshaw</p>
    <p>Virginia Tech: G. Zelenka, J. Lockhart, N. Ramakrishnan, L. Zhang, L. Heath, and C. Ribbens</p>
    <p>Renaissance Computing Institute: M. Rynge and J. McGee</p>
    <p>Tokyo Institute of Technology: R. Fukushima, T. Nishikawa, T. Kujiraoka, and S. Ihara</p>
    <p>Sun Microsystems: S. Vail, S. Cochrane, C. Kingwood, B. Cauthen, S. See, J. Fragalla, J. Bates, R. Cagle, R. Gaines, and</p>
    <p>C. Bohm</p>
    <p>Louisiana State University: H. Liu</p>
  </div>
</Presentation>
