<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Automatic Application-Aware Forwarding Resource Allocation</p>
    <p>Xu Ji, Bin Yang, Tianyu Zhang, Xiaosong Ma, Xiupeng Zhu, Xiyang Wang, Nosayba El-Sayed, Jidong Zhai, Weiguo Liu, Wei Xue</p>
  </div>
  <div class="page">
    <p>Sunway TaihuLight</p>
    <p>Summit Sierra</p>
    <p>Storage Crucial for Supercomputing</p>
    <p>Climate simulation</p>
    <p>Fluid dynamics</p>
    <p>Molecular dynamics</p>
    <p>Graph computing Neural network</p>
    <p>Bioscience</p>
  </div>
  <div class="page">
    <p>I/O Forwarding Layer</p>
    <p>Storage system crucial for speed + scalability  Performance bottleneck  Contention point =&gt; inter-job performance interference</p>
    <p>I/O Forwarding architecture  Enables lean OS on compute nodes  Reduces #connection  Connects different network domains  New layer of perfecting/caching</p>
  </div>
  <div class="page">
    <p>I/O Forwarding Widely Adopted</p>
    <p>- 9 out of top 20 supercomputers use I/O forwarding (Nov 2018)</p>
  </div>
  <div class="page">
    <p>Challenges with I/O Forwarding</p>
    <p>Forwarding resource provisioning  How many forwarding nodes and how much forwarding capacity?</p>
    <p>Need to consider application I/O demands, machine utilization, budget constraint</p>
    <p>Design and procurement often finish years before application test runs</p>
    <p>Forwarding resource management  How many forwarding nodes to allocate to each job?</p>
    <p>Which forwarding nodes to assign to each job?</p>
    <p>Current common practice: Fixed Forwarding Mapping (FFM)  Static compute-to-forwarding node mapping</p>
  </div>
  <div class="page">
    <p>Sample Forwarding Configuration: TaihuLight</p>
    <p>IB FDR</p>
    <p>Supernode</p>
    <p>(256 nodes)</p>
    <p>TaihuLight compute</p>
    <p>nodes (40960)</p>
    <p>I/O forwarding</p>
    <p>nodes (240)</p>
    <p>IB FDR</p>
    <p>Storage nodes</p>
    <p>(2*144)</p>
    <p>Metadata nodes (2)</p>
    <p>Interconnect network</p>
    <p>Sugon</p>
    <p>DS800</p>
    <p>Disk arrays</p>
    <p>(144)</p>
    <p>Supernode</p>
    <p>(256 nodes)</p>
    <p>Supernode</p>
    <p>(256 nodes)</p>
    <p>Fixed Forwarding (FFM) and over-provisioning when TaihuLight was just built</p>
  </div>
  <div class="page">
    <p>IOPS-heavy</p>
    <p>CAM (2017 GB finalist) atmospheric model</p>
    <p>AWP (2017 GB prize) earthquake simulation</p>
    <p>Shentu (2018 GB finalist) graph engineXCFD fluid dynamics</p>
    <p>DNDC agro-ecosystems</p>
    <p>WRF weather forecasting</p>
    <p>APT particle dynamics</p>
    <p>swDNN neuronal network</p>
    <p>LAMMPS molecular dynamics</p>
    <p>CESM climate simulator</p>
    <p>Apps have different I/O behaviors!</p>
    <p>N-1 I/O mode</p>
    <p>Metadata-heavy</p>
    <p>BW-heavy 1-1 I/O mode</p>
    <p>FFM Problem 1: Resource Misallocation</p>
    <p>BW-heavy</p>
    <p>BW-heavy BW-heavy</p>
  </div>
  <div class="page">
    <p>I/ O</p>
    <p>s p</p>
    <p>e e</p>
    <p>d u</p>
    <p>p</p>
    <p># Forwarding nodes</p>
    <p>WRF-256(1-1) XCFD-128(N-N)</p>
    <p>WRF weather forecasting</p>
    <p>FFM Problem 1: Resource Misallocation</p>
    <p>XCFD fluid dynamics</p>
    <p>P1</p>
    <p>File</p>
    <p>N-N I/O mode</p>
    <p>P1</p>
    <p>File1</p>
    <p>P2</p>
    <p>File2</p>
    <p>PN</p>
    <p>FileN</p>
    <p>N-1 I/O mode</p>
    <p>P1</p>
    <p>File</p>
    <p>P2 PN</p>
    <p>Does not benefit from additional</p>
    <p>forwarding nodes</p>
    <p>Benefits from additional forwarding</p>
    <p>nodes (up to ~4)</p>
  </div>
  <div class="page">
    <p>FFM Problem 2: I/O Interference</p>
    <p>dedicated fwd nodes shared fwd nodes</p>
    <p>T im</p>
    <p>e i</p>
    <p>n s</p>
    <p>e co</p>
    <p>n d</p>
    <p>s AWP-256 Shentu-1024 LAMMPS-256</p>
    <p>Applications may suffer from I/O interference at I/O forwarding layer!</p>
    <p>dedicated fwd nodes shared fwd nodes</p>
    <p>fwd nodes fwd nodescompute nodescompute nodes</p>
    <p>AWP earthquake simulation</p>
    <p>Shentu graph engine</p>
    <p>LAMMPS molecular dynamics</p>
    <p>I/O interference brings I/O performance inconsistency and degradation</p>
    <p>Slowdown factor</p>
  </div>
  <div class="page">
    <p>FFM Problem 3: Forwarding Node Anomaly</p>
    <p>App performance severely hurt when assigned fail-slow forwarding nodes</p>
    <p>B a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (G</p>
    <p>B /s</p>
    <p>)</p>
    <p>Forwarding node ID</p>
    <p>Write Read</p>
    <p>Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems, Gunawi, FAST18</p>
    <p>Fail-slow is also a significant problem to FFM</p>
  </div>
  <div class="page">
    <p>Our Solution: DFRA (Dynamic Forwarding Resource Allocation)</p>
    <p>Main idea: to upgrade forwarding node allocation on demand</p>
    <p>Disruptive  private allocation Normal - default policy</p>
    <p>Demanding - more allocation</p>
  </div>
  <div class="page">
    <p>Automatic, transparent multi-level I/O monitoring  Allows DFRA to look up node status and per-application I/O profiles  Intuition: HPC applications have consistent I/O behavior across runs (jobs)</p>
    <p>Deployed at TaihuLight since April, 2017</p>
    <p>Paper at NSDI 19  End-to-end I/O Monitoring on a Leading Supercomputer, by Yang et al.</p>
    <p>Code and I/O monitoring data released  https://github.com/Beaconsys/Beacon</p>
    <p>Per-App I/O Profiling : Beacon End-to-end I/O Monitor</p>
  </div>
  <div class="page">
    <p>Automatic Forwarding Node Scaling</p>
    <p>Identifying jobs needing more than default FFM allocation</p>
    <p>Target job eligible for consideration if application  has enough I/O volume (&gt; Vmin), and</p>
    <p>has enough nodes performing I/O (&gt; Nmin), and</p>
    <p>is not metadata-bound (avarage metadata queue length &lt; Wmetadata)</p>
    <p>Estimating adjusted forwarding node allocation S  S = #I/O Nodes * R</p>
    <p>R = I/O bandwidth per compute node / I/O bandwidth per forwarding node</p>
    <p>Upgrade if S &gt; #Forwarding Nodes</p>
    <p>Do nothing otherwise</p>
  </div>
  <div class="page">
    <p>I/O Interference Avoidance</p>
    <p>I/O slowdown factor pairs</p>
    <p>Apps MPI-ION APT DNDC WRF1 WRFN Shentu CAM AWP</p>
    <p>MPI-ION (2.1,2.1) (1.1,9.3) (4.8,1.1) (1.0,1.0) (2.1,2.0) (1.3,4.5) (1.0,1.0) (3.3,1.1)</p>
    <p>APT - (2.0,2.1) (33.3,1.0) (1.0,1.0) (4.3,1.4) (6.3,1.3) (1.0,1.0) (50.0,1.1)</p>
    <p>DNDC - - (2.0,2.0) (1.0,25.0) (1.0,11.1) (1.1,16.7) (1.0,33.3) (2.2,2.4)</p>
    <p>WRF1 - - - (1.0,1.0) (1.0,1.0) (1.0,1.0) (1.0,1.0) (50.0,1.0)</p>
    <p>WRFN - - - - (2.1,2.1) (2.0,2.3) (1.0,1.0) (12.5,1.3)</p>
    <p>Shentu - - - - - (2.0,2.0) (1.0,1.0) (12.5,1.1)</p>
    <p>CAM - - - - - - (1.0,1.0) (100.0,1.0)</p>
    <p>AWP - - - - - - - (2.0,2.0)</p>
    <p>Identifying jobs requiring dedicated forwarding nodes  Based on comprehensive inter-application I/O interference study</p>
    <p>Pair-wise interference measurement on 8 apps with representative behaviors  256-node runs, two apps sharing one forwarding node</p>
  </div>
  <div class="page">
    <p>I/O Interference Source 1: N-1 I/O Mode</p>
    <p>Apps MPI-ION APT DNDC WRF1 WRFN Shentu CAM AWP</p>
    <p>MPI-ION (2.1,2.1) (1.1,9.3) (4.8,1.1) (1.0,1.0) (2.1,2.0) (1.3,4.5) (1.0,1.0) (3.3,1.1)</p>
    <p>APT - (2.0,2.1) (33.3,1.0) (1.0,1.0) (4.3,1.4) (6.3,1.3) (1.0,1.0) (50.0,1.1)</p>
    <p>DNDC - - (2.0,2.0) (1.0,25.0) (1.0,11.1) (1.1,16.7) (1.0,33.3) (2.2,2.4)</p>
    <p>WRF1 - - - (1.0,1.0) (1.0,1.0) (1.0,1.0) (1.0,1.0) (50.0,1.0)</p>
    <p>WRFN - - - - (2.1,2.1) (2.0,2.3) (1.0,1.0) (12.5,1.3)</p>
    <p>Shentu - - - - - (2.0,2.0) (1.0,1.0) (12.5,1.1)</p>
    <p>CAM - - - - - - (1.0,1.0) (100.0,1.0)</p>
    <p>AWP - - - - - - - (2.0,2.0)</p>
    <p>N-1 I/O mode not only slow, but highly disruptive</p>
    <p>AWP</p>
    <p>CAM</p>
    <p>Queue</p>
    <p>Long-time processing</p>
  </div>
  <div class="page">
    <p>I/O Interference Source 2: Metadata-heavy Applications</p>
    <p>Apps MPI-ION APT DNDC WRF1 WRFN Shentu CAM AWP</p>
    <p>MPI-ION (2.1,2.1) (1.1,9.3) (4.8,1.1) (1.0,1.0) (2.1,2.0) (1.3,4.5) (1.0,1.0) (3.3,1.1)</p>
    <p>APT - (2.0,2.1) (33.3,1.0) (1.0,1.0) (4.3,1.4) (6.3,1.3) (1.0,1.0) (50.0,1.1)</p>
    <p>DNDC - - (2.0,2.0) (1.0,25.0) (1.0,11.1) (1.1,16.7) (1.0,33.3) (2.2,2.4)</p>
    <p>WRF1 - - - (1.0,1.0) (1.0,1.0) (1.0,1.0) (1.0,1.0) (50.0,1.0)</p>
    <p>WRFN - - - - (2.1,2.1) (2.0,2.3) (1.0,1.0) (12.5,1.3)</p>
    <p>Shentu - - - - - (2.0,2.0) (1.0,1.0) (12.5,1.1)</p>
    <p>CAM - - - - - - (1.0,1.0) (100.0,1.0)</p>
    <p>AWP - - - - - - - (2.0,2.0)</p>
    <p>High-metadata applications (e.g., DNDC) generate heavy interference to concurrent workloads</p>
  </div>
  <div class="page">
    <p>I/O Interference Study: Summary</p>
    <p>Identified I/O interference root causes:  N-1 I/O mode workloads  metadata-heavy workloads  high-bandwidth workloads</p>
    <p>[see paper for more details]</p>
    <p>High-IOPS workloads more vulnerable</p>
    <p>Detect I/O interference by checking both target job and existing neighbors on shared forwarding nodes to be allocated via FFM  Assign target job dedicated forwarding nodes if either party belongs to above</p>
    <p>categories</p>
  </div>
  <div class="page">
    <p>DFRA Workflow</p>
    <p>Job Scheduler</p>
    <p>I/O monitoring</p>
    <p>system</p>
    <p>I/O perf DB</p>
    <p>Runtime I/O</p>
    <p>monitor</p>
    <p>A Forwarding node</p>
    <p>scaling for A</p>
    <p>More forwarding</p>
    <p>nodes?</p>
    <p>interfere detected?</p>
    <p>Estimated I/O behavior of A</p>
    <p>Default # of alloc fwd node</p>
    <p>Estimated I/O behavior of Neighbors</p>
    <p>Target alloc S</p>
    <p>No</p>
    <p>No</p>
    <p>Use default mapping</p>
    <p>Yes</p>
    <p>Yes</p>
    <p>OccupiedFFM Idle Faulty/slow</p>
    <p>Allocate forwarding nodes</p>
    <p>Faulty/slow</p>
    <p>FFM Pool DFRA Pool</p>
    <p>DFRA</p>
    <p># of idle forwarding nodes</p>
  </div>
  <div class="page">
    <p>Implementation and Deployment Status</p>
    <p>Implemented to be used with SLURM scheduler, in C and Python</p>
    <p>Remapping to more/dedicated forwarding nodes when job approved for upgrade</p>
    <p>By relinking RDMA connection  Per-job basis, new mapping removed at end of job run  Currently no downgrading</p>
    <p>Partially deployed on TaihuLight production system since Feb 2018  Users opt in with job submission command  Intend to switch to opt out in the future</p>
    <p>DFRA to be included in parallel I/O design for TaihuLights successor (Exa-scale machine)</p>
  </div>
  <div class="page">
    <p>Evaluation 1: Upgrade Eligibility of Historical Jobs</p>
    <p>Benefit from DFRA, 14%</p>
    <p>Low I/O volume, 78%</p>
    <p>Inefficent I/O processes , 8%</p>
    <p>By job count</p>
    <p>18 months I/O history analysis (Apr 2017  Sep 2018)</p>
    <p>Benefit from DFRA, 80%</p>
    <p>Low I/O volume, 18%</p>
    <p>Inefficent I/O processes , 2%</p>
    <p>By core-hours consumed</p>
    <p>Few jobs but consuming considerable core-hours are benefiting from DFRA</p>
  </div>
  <div class="page">
    <p>Evaluation 2: I/O Interference Reduction</p>
    <p>FFM DFRA</p>
    <p>DFRA reduce variation and improve I/O performance at I/O forwarding layer</p>
  </div>
  <div class="page">
    <p>Evaluation 3: Remapping Overhead vs. Saving</p>
    <p>T im</p>
    <p>e i</p>
    <p>n s</p>
    <p>e co</p>
    <p>n d</p>
    <p>#Compute nodes</p>
    <p>remapping overhead dispatch time w.o remapping</p>
    <p>Overhead acceptable considering benefit  Average I/O time saving of 6 minutes for I/O-intensive jobs  Estimated saving of 200 million core-hours in past 8 months</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Take away points:</p>
    <p>Dont guess future user I/O demands  Over-provision, give low basic plan, then upgrade when needed</p>
    <p>DFRA applicable to shared burst buffer management too</p>
  </div>
  <div class="page">
    <p>Q&amp;A</p>
    <p>Thank you!</p>
    <p>Partial I/O monitoring data released at https://github.com/Beaconsys/Beacon</p>
  </div>
  <div class="page">
    <p>Evaluation 2: DFRA Effectiveness</p>
    <p>DFRA reduces I/O variation and improves I/O performance at I/O forwarding layer</p>
    <p>Overall performance from 10 runs (each box) in production environment</p>
  </div>
</Presentation>
