<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improving I/O Resource Sharing of Linux Cgroup for NVMe SSDs on Multi-core Systems</p>
    <p>USENIX HotStorage 2016</p>
    <p>Sungyong Ahn*, Kwanghyun La*, Jihong Kim**</p>
    <p>*Memory Business, Samsung Electronics Co., Ltd.</p>
    <p>**Seoul National University</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Multiple isolated instances (containers) running on a single host.</p>
    <p>OS-level Virtualization</p>
  </div>
  <div class="page">
    <p>Multiple isolated instances (containers) running on a single host.</p>
    <p>Hardware resources should be isolated and allocated to containers</p>
    <p>OS-level Virtualization</p>
  </div>
  <div class="page">
    <p>Multiple isolated instances (containers) running on a single host.</p>
    <p>Hardware resources should be isolated and allocated to containers</p>
    <p>Different resource requirements should be satisfied</p>
    <p>OS-level Virtualization</p>
    <p>I have CPU-bound jobs</p>
    <p>I have I/O-bound jobs</p>
  </div>
  <div class="page">
    <p>Kernel-level resource manager of Linux</p>
    <p>Linux Control Groups (Cgroups)</p>
    <p>I have I/O-bound jobs</p>
    <p>I have CPU-bound jobs</p>
  </div>
  <div class="page">
    <p>I/O bandwidth is shared according to I/O weights</p>
    <p>Proportional I/O scheme in Linux Cgroups</p>
    <p>N o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>I /O</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th C(10) C(5)</p>
    <p>C(2.5) C(1)</p>
    <p>Ideal proportional I/O sharing</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Questions</p>
    <p>When Linux Cgroups work with NVMe SSD,</p>
  </div>
  <div class="page">
    <p>Questions</p>
    <p>When Linux Cgroups work with NVMe SSD,</p>
  </div>
  <div class="page">
    <p>BASELINE N</p>
    <p>o rm</p>
    <p>a li</p>
    <p>ze d</p>
    <p>I/ O</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th C(10) C(5)</p>
    <p>C(2.5) C(1)</p>
    <p>Existing Cgroups cannot support the proportional I/O to NVMe SSDs</p>
    <p>Proportional I/O with NVMe SSDs</p>
  </div>
  <div class="page">
    <p>NVMe SSDs have different I/O stack from SATA storage</p>
    <p>Because...</p>
    <p>SATA I/O stack</p>
    <p>NVMe I/O stack</p>
    <p>Existing proportional I/O scheme is implemented in single queue block layer</p>
  </div>
  <div class="page">
    <p>First Attempt: Using the Existing Static Throttling</p>
    <p>Upper limit of I/O bandwidth</p>
    <p>Limit the maximum number of bytes or I/O requests for particular time interval (throttling window)</p>
    <p>Throttling Window</p>
    <p>throttled</p>
    <p>Container A upper limit=10</p>
    <p>Time</p>
  </div>
  <div class="page">
    <p>Static throttling is not enough to support the proportional I/O</p>
    <p>First Attempt: Using the Existing Static Throttling</p>
    <p>Static Throttling</p>
    <p>N o</p>
    <p>rm a</p>
    <p>li ze</p>
    <p>d I/</p>
    <p>O b</p>
    <p>a n</p>
    <p>d w</p>
    <p>id th C(10) C(5)</p>
    <p>C(2.5) C(1)</p>
  </div>
  <div class="page">
    <p>Because...</p>
    <p>I/O workloads fluctuate with time</p>
  </div>
  <div class="page">
    <p>I/O workloads fluctuate with time</p>
    <p>Because...</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based Dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>We achieved the proportional I/O for NVMe SSDs.</p>
    <p>We achieved the scalable performance of Linux Cgroups.</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based Dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Overview of WDT Scheme</p>
    <p>Container</p>
    <p>Block Layer</p>
    <p>Container Container</p>
    <p>weight w1 weight w2 weight w3 Credit allocation</p>
    <p>CPM Monitoring</p>
    <p>Data flow</p>
    <p>Future I/O Demand</p>
    <p>Predictor</p>
    <p>Budget Distributor</p>
    <p>TotalCredit Updater</p>
    <p>Residual Credits</p>
    <p>Carryover</p>
    <p>TotalCredit 1  + 1</p>
    <p>&gt; 1</p>
    <p>Monitoring</p>
    <p>1</p>
    <p>2  + 2</p>
    <p>&gt; 2</p>
    <p>3  + 3</p>
    <p>&gt; 3</p>
    <p>Monitoring</p>
    <p>2 Monitoring</p>
    <p>3</p>
    <p>3 2 1</p>
    <p>WDT</p>
    <p>To update TotalCredit, future I/O demand is predicted</p>
    <p>Distributing the credits to containers according to I/O weights</p>
  </div>
  <div class="page">
    <p>All containers are allocated credits in proportion to their I/O weight.</p>
    <p>Budget Distributor</p>
    <p>Throttling Window</p>
    <p>Container A I/O weight=10</p>
    <p>Time</p>
    <p>Container B I/O weight=5</p>
    <p>Throttling Window</p>
    <p>TotalCredit = 15</p>
  </div>
  <div class="page">
    <p>All containers are allocated credits in proportion to their I/O weight.</p>
    <p>Credits are replenished periodically.</p>
    <p>Budget Distributor</p>
    <p>Throttling Window</p>
    <p>Container A I/O weight=10</p>
    <p>Time</p>
    <p>Container B I/O weight=5</p>
    <p>Throttling Window</p>
    <p>TotalCredit = 15 Replenishment of Credit Replenishment of Credit</p>
    <p>TotalCredit = 15</p>
  </div>
  <div class="page">
    <p>All containers are allocated credits in proportion to their I/O weight.</p>
    <p>Credits are replenished periodically.</p>
    <p>If a container has no available credit, it is throttled.</p>
    <p>Budget Distributor</p>
    <p>Throttling Window</p>
    <p>Container A I/O weight=10</p>
    <p>Time</p>
    <p>Container B I/O weight=5</p>
    <p>Throttling Window</p>
    <p>TotalCredit = 15 Replenishment of Credit Replenishment of Credit</p>
    <p>TotalCredit = 15</p>
    <p>throttled</p>
    <p>throttled</p>
    <p>throttled</p>
    <p>throttled</p>
  </div>
  <div class="page">
    <p>All containers are allocated credits in proportion to their I/O weight.</p>
    <p>Credits are replenished periodically.</p>
    <p>If a container has no available credit, it is throttled.</p>
    <p>Budget Distributor</p>
  </div>
  <div class="page">
    <p>In order to remove storage idle time, TotalCredit is adjusted.</p>
    <p>TotalCredit Updater</p>
  </div>
  <div class="page">
    <p>Technical Challenge</p>
    <p>How to predict TotalCredit required for the next interval?</p>
  </div>
  <div class="page">
    <p>Technical Challenge</p>
    <p>How to predict future I/O demand for the next interval?</p>
  </div>
  <div class="page">
    <p>Monitoring I/O demand of each container for every interval</p>
    <p>Prediction of the future I/O demand from cumulative distribution function</p>
    <p>80th percentile of a cumulative distribution of I/O demand (assuming normal distribution)</p>
    <p>Future I/O Demand Predictor</p>
    <p>Container</p>
    <p>Block Layer</p>
    <p>Container Container</p>
    <p>weight w1 weight w2 weight w3 Credit allocation</p>
    <p>CPM Monitoring</p>
    <p>Data flow</p>
    <p>Future I/O Demand</p>
    <p>Predictor</p>
    <p>Budget Distributor</p>
    <p>TotalCredit Updater</p>
    <p>Residual Credits</p>
    <p>Carryover</p>
    <p>TotalCredit 1  + 1</p>
    <p>&gt; 1</p>
    <p>Monitoring</p>
    <p>1</p>
    <p>2  + 2</p>
    <p>&gt; 2</p>
    <p>3  + 3</p>
    <p>&gt; 3</p>
    <p>Monitoring</p>
    <p>2 Monitoring</p>
    <p>3</p>
    <p>3 2 1</p>
    <p>WDT</p>
  </div>
  <div class="page">
    <p>Questions</p>
    <p>When Linux Cgroups work with NVMe SSD,</p>
  </div>
  <div class="page">
    <p>Scalability problem of the existing Cgroups throttling layer</p>
    <p>Scalability of the Existing Cgroups on NUMA</p>
    <p>K IO</p>
    <p>P S</p>
    <p>The number of NUMA nodes</p>
    <p>C1 C2</p>
    <p>C3 C4</p>
  </div>
  <div class="page">
    <p>All containers share a single request_queue lock across NUMA nodes</p>
    <p>Because...</p>
    <p>U c</p>
    <p>a c h</p>
    <p>e m</p>
    <p>is s r</p>
    <p>a ti</p>
    <p>o (</p>
    <p>% )</p>
    <p>The number of NUMA nodes</p>
    <p>Hardware</p>
    <p>Linux</p>
    <p>Container A Container B</p>
    <p>Single-queue Block Layer</p>
    <p>Container C Container D</p>
    <p>Multi-queue Block Layer</p>
    <p>Proportional I/O (CFQ)</p>
    <p>Cgroup I/O throttling</p>
    <p>LOCK</p>
    <p>Lock contention  Remote memory accesses to the lock</p>
    <p>state  Cacheline invalidations caused by</p>
    <p>cache coherence protocol</p>
  </div>
  <div class="page">
    <p>We adopt fine-grained per-container locks</p>
    <p>The cache miss ratio decreases to 12.8% from 38.2%</p>
    <p>Per-container Locks</p>
    <p>Hardware</p>
    <p>Linux</p>
    <p>Container A Container B</p>
    <p>Single-queue Block Layer</p>
    <p>Container C Container D</p>
    <p>Multi-queue Block Layer</p>
    <p>Proportional I/O (CFQ)</p>
    <p>Cgroup I/O throttling</p>
    <p>LOCK</p>
    <p>Hardware</p>
    <p>Linux</p>
    <p>Container A Container B</p>
    <p>Single-queue Block Layer</p>
    <p>Container C Container D</p>
    <p>Multi-queue Block Layer</p>
    <p>Proportional I/O (CFQ)</p>
    <p>Cgroup I/O throttling</p>
    <p>LOCK LOCK LOCK LOCK</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Linux kernel 4.0.4 (modified)</p>
    <p>Dell 4-nodes NUMA machine</p>
    <p>Samsung NVMe SSDs</p>
    <p>Block I/O traces replayer</p>
    <p>UMass trace repository</p>
    <p>SNIA IOTTA repository</p>
    <p>Experiment Setup</p>
    <p>Dell R920 Server (4-nodes NUMA machine)</p>
    <p>Samsung XS1715 NVMe SSDs</p>
  </div>
  <div class="page">
    <p>WDT scheme satisfies the proportional sharing requirements</p>
    <p>Result 1: Proportional I/O Support</p>
    <p>BASELINE Static Throttling WDT</p>
    <p>N o</p>
    <p>r m</p>
    <p>a li</p>
    <p>z e d</p>
    <p>I /O</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th</p>
  </div>
  <div class="page">
    <p>WDT- : Using single spin lock</p>
    <p>WDT : Using per-container locks</p>
    <p>Result 2: Performance Scalability</p>
    <p>WDT- WDT</p>
    <p>I/ O</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (M</p>
    <p>B /s</p>
    <p>) C1 C2</p>
    <p>C3 C4</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Contributions</p>
    <p>Weight-based dynamic throttling (WDT) scheme</p>
    <p>Experimental Results</p>
    <p>Conclusion</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Proposed the weight-based dynamic throttling scheme to support proportional I/O sharing for NVMe SSDs.</p>
    <p>Proposed the per-container locks for scalable performance.</p>
    <p>Conclusion</p>
  </div>
  <div class="page"/>
</Presentation>
