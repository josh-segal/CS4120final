<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations</p>
  </div>
  <div class="page">
    <p>Semantic Parsing with Execution</p>
    <p>Text</p>
    <p>Meaning Representation</p>
    <p>Denotation (Answer)</p>
    <p>Environment Semantic Parsing</p>
    <p>Execution</p>
  </div>
  <div class="page">
    <p>Semantic Parsing with Execution</p>
    <p>What nation scored the most points?</p>
    <p>Select Nation Where Points is Max</p>
    <p>England</p>
    <p>Environment Semantic Parsing</p>
    <p>Execution</p>
    <p>Index Name Nation Points Games Pts/game</p>
  </div>
  <div class="page">
    <p>No gold programs during training</p>
    <p>Indirect Supervision</p>
    <p>What nation scored the most points?</p>
    <p>Select Nation Where Points is Max</p>
    <p>England</p>
    <p>Environment Semantic Parsing</p>
    <p>Execution</p>
    <p>Index Name Nation Points Games Pts/game</p>
  </div>
  <div class="page">
    <p>Neural Model  x: What nation scored the most points?  y: Select Nation Where Index is Minimum  neural models  score(x, y): encode x, encode y, and produce scores</p>
    <p>Argmax procedure  Beamseach: argmax score(x, y)</p>
    <p>Indirect supervision  Find approximated gold meaning representations  Reinforcement learning algorithms</p>
    <p>Learning</p>
  </div>
  <div class="page">
    <p>Question: What nation scored the most points?  Answer: England</p>
    <p>Semantic Parsing with Indirect Supervision</p>
    <p>Index Name Nation Points Games Pts/game</p>
    <p>For Training</p>
  </div>
  <div class="page">
    <p>Search for Training</p>
    <p>A correct program should execute to the gold answer.  In general, there are several spurious programs that execute to</p>
    <p>the gold answer but are semantically incorrect.</p>
  </div>
  <div class="page">
    <p>Select Nation Where Points = 44  England Select Nation Where Index is Minimum  England Select Nation Where Pts/game is Maximum  England Select Nation Where Point is Maximum  England</p>
    <p>Search for Training: Spurious Programs</p>
    <p>Search for training. Goal: find semantically correct parse!  Question: What nation scored the most points?</p>
    <p>All programs above generate right answers but only one is correct.</p>
  </div>
  <div class="page">
    <p>Update Step</p>
    <p>Generally there are several methods to update the model.  Examples: maximum marginal likelihood, reinforcement</p>
    <p>learning, margin methods.</p>
  </div>
  <div class="page">
    <p>(1) Policy Shaping for handling spurious programs (2) Generalized Update Equation for generalizing common update strategies and allowing novel updates.</p>
    <p>(1) and (2) seem independent, but they interact with each other!!</p>
    <p>5% absolute improvement over SOTA on SQA dataset</p>
    <p>Contributions</p>
  </div>
  <div class="page">
    <p>Learning from Indirect Supervision</p>
    <p>[Search for Training] With x, t, z, beam search suitable ={y}</p>
    <p>[Update] Update , according K = {y}</p>
    <p>Question x, Table t, Answer z, Parameters</p>
  </div>
  <div class="page">
    <p>Spurious Programs</p>
    <p>If the model selects a spurious program for update then it increases the chance of selecting spurious programs in future.</p>
    <p>Question x, Table t, Answer z, Parameters</p>
    <p>[Search for Training] With x, t, z, beam search suitable {y}</p>
  </div>
  <div class="page">
    <p>Policy Shaping [Griffith et al., NIPS-2013]</p>
  </div>
  <div class="page">
    <p>Search with Shaped Policy</p>
    <p>Question x, Table t, Answer z, Parameters</p>
    <p>[Search for Training] With x, t, z, beam search suitable {y}</p>
  </div>
  <div class="page">
    <p>Critique Policy</p>
  </div>
  <div class="page">
    <p>Critique Policy Features</p>
    <p>Select Nation Where Points = 44 Select Nation Where Index is Minimum Select Nation Where Pts/game is Maximum Select Nation Where Points is Maximum Select Nation Where Name = Karen Andrew</p>
    <p>Question: What nation scored the most points?</p>
    <p>lexical pair match</p>
    <p>surface-form match 16</p>
  </div>
  <div class="page">
    <p>Learning Pipeline Revisited</p>
    <p>[Search for Training] With x, t, z, beam search suitable ={y}</p>
    <p>[Update] Update , according K = {y}</p>
    <p>Using policy shaping to find better K</p>
    <p>What is the better objective function J?</p>
    <p>Shaping affects here</p>
  </div>
  <div class="page">
    <p>Objective Functions Look Different!  Maximum Marginal Likelihood (MML)</p>
    <p>Reinforcement learning (RL)</p>
    <p>Maximum Margin Reward (MMR)</p>
    <p>Most violated program generated according to reward augment inference</p>
    <p>Maximum Reward Program</p>
  </div>
  <div class="page">
    <p>Update Rules are Similar  Maximum Marginal Likelihood (MML)</p>
    <p>Reinforcement learning (RL)</p>
    <p>Maximum Margin Reward (MMR)</p>
  </div>
  <div class="page">
    <p>Generalized Update Equation</p>
  </div>
  <div class="page">
    <p>MMR</p>
    <p>MAVER</p>
    <p>Improvement over Margin Approaches</p>
  </div>
  <div class="page">
    <p>Results on SQA: Answer Accuracy (%)</p>
    <p>Policy shaping helps improve performance.  With policy shaping, different updates matters even more  Achieves new state-of-the-art (previously 44.7%) on SQA</p>
  </div>
  <div class="page">
    <p>Comparing Updates</p>
    <p>MMR and MAVER are more aggressive than MML  MMR and MAVER update towards to one program  MML updates toward to all programs that can generate the</p>
    <p>correct answer</p>
    <p>MML:</p>
    <p>MMR:</p>
  </div>
  <div class="page">
    <p>Discussed problem with search and update steps in semantic parsing from denotation.</p>
    <p>Introduced policy shaping for biasing the search away from spurious programs.</p>
    <p>Introduced generalized update equation that generalizes common update strategies and allows novel updates.</p>
    <p>Policy shaping allows more aggressive update!</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>BACKUP</p>
  </div>
  <div class="page">
    <p>Generalized Update as an Analysis Tool</p>
    <p>MMR and MAVER are more aggressive than MML  MMR and MAVER only pick one  MML gives credits to all {y} that satisfies {z}  MMR and MAVER benefit more from shaping</p>
  </div>
  <div class="page">
    <p>Learning from Indirect Supervision</p>
    <p>Question x, Table t, Answer z, Parameters</p>
    <p>[Search for Training] With x, t, z, beam search suitable {y}</p>
    <p>[Update] Update , according {y}</p>
    <p>Search in training. Goal: finding semantically correct y</p>
    <p>Many different ways of update  27</p>
  </div>
  <div class="page">
    <p>Shaping and update</p>
    <p>Better search  more aggressive update</p>
    <p>[Search for Training] With x, t, z, beam search suitable ={y}</p>
    <p>[Update] Update , according K = {y}</p>
    <p>Using policy shaping to find better K</p>
    <p>What is the better objective function J?</p>
    <p>Shaping affects here directly</p>
    <p>Shaping affects here indirectly</p>
  </div>
  <div class="page">
    <p>Mixing the MMRs intensity and MMLs competing distribution gives an update that outperforms MMR.</p>
    <p>Novel Learning Algorithm</p>
    <p>Intensity Competing Distribution Dev Performance</p>
    <p>w/o shaping</p>
    <p>Maximum Marginal Likelihood (MML)</p>
    <p>Maximum Marginal Likelihood (MML)</p>
    <p>Maximum Margin Reward (MMR) Maximum Margin Reward (MMR) 40.7</p>
    <p>Maximum Margin Reward (MMR) Maximum Marginal Likelihood</p>
    <p>(MML) 41.9</p>
  </div>
  <div class="page">
    <p>Novel Learning Algorithms</p>
  </div>
  <div class="page">
    <p>Learning Method #1  Maximum Marginal Likelihood (MML)</p>
  </div>
  <div class="page">
    <p>Learning Method #2  Reinforcement Learning (RL)</p>
  </div>
  <div class="page">
    <p>Learning Method #3  Maximum Margin Reward (MMR)</p>
  </div>
  <div class="page">
    <p>Learning Method #4  Maximum Margin Average Violation Reward (MAVER)</p>
  </div>
</Presentation>
