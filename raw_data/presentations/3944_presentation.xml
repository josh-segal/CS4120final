<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>On Sockets and System Calls</p>
    <p>Minimizing Context Switches for Sockets</p>
    <p>Tom Hrub Teodor Crivat Herbert Bos Andrew S. Tanenbaum</p>
    <p>Vrije Universiteit Amsterdam</p>
  </div>
  <div class="page">
    <p>POSIX Socket API</p>
    <p>POSIX sockets are de facto standard for network applications</p>
    <p>Well understood API</p>
    <p>Used by legacy software</p>
    <p>Portable across operating systems</p>
  </div>
  <div class="page">
    <p>POSIX Socket API</p>
    <p>The Sockets are also problematic!</p>
    <p>Each socket API call is a system call</p>
    <p>System calls are disruptive!</p>
    <p>OS execution interleaves with applications</p>
    <p>Nonblocking socket calls are expensive</p>
    <p>Application</p>
    <p>Net Stack</p>
    <p>NIC</p>
    <p>Kernel</p>
  </div>
  <div class="page">
    <p>POSIX Socket API</p>
    <p>The widely accepted solution is inventing new APIs</p>
    <p>Non-POSIX interfaces</p>
    <p>Breaks portability!</p>
    <p>Legacy code does not benefit</p>
  </div>
  <div class="page">
    <p>Socket API - current solutions</p>
    <p>Relaxing of file-based POSIX Socket API</p>
    <p>Batching of system calls</p>
    <p>Per-core channels to the OS (kernel)</p>
    <p>Polling - timers &amp; thresholds</p>
  </div>
  <div class="page">
    <p>POSIX Socket API</p>
    <p>Our hypothesis :</p>
    <p>The problem is just the implementation not the API itself!</p>
  </div>
  <div class="page">
    <p>System Calls Overhead</p>
    <p>Problem is the system call!</p>
    <p>Synchronous execution transfer from applications to the OS</p>
    <p>CPU switches stacks and instruction pointers</p>
    <p>OS saves and restores the application context</p>
    <p>CPU structures are thrashed by switching execution streams</p>
    <p>Recovering optimal performance takes time</p>
  </div>
  <div class="page">
    <p>Socket API - event-driven servers</p>
    <p>High-performance apps use event-driven model</p>
    <p>Single thread per core</p>
    <p>Apps require efficient way of testing readiness of sockets</p>
  </div>
  <div class="page">
    <p>Socket API - event-driven servers</p>
    <p>Applications often try if a call would succeed</p>
    <p>Cycles wasted when many speculative calls fail</p>
    <p>Latency increases</p>
  </div>
  <div class="page">
    <p>POSIX Socket API</p>
    <p>The challenge :</p>
    <p>Eliminate system calls from sockets!</p>
  </div>
  <div class="page">
    <p>Exposed socket buffers</p>
    <p>What if applications could see what is in the buffers w/o asking?</p>
    <p>Most calls are resolved in user space</p>
    <p>Failing nonblocking calls is almost free!</p>
    <p>Reads and writes straight from/to the socket buffers</p>
    <p>System calls used only to block when there is no work!</p>
  </div>
  <div class="page">
    <p>Exposed socket buffers</p>
    <p>head tail</p>
    <p>data_head data_tail</p>
    <p>Each socket is : memory shared between OS and the application</p>
  </div>
  <div class="page">
    <p>Exposed socket buffers</p>
    <p>head tail</p>
    <p>data_head data_tail</p>
    <p>The application can simply test :</p>
    <p>if the queue is empty when reading / accepting</p>
    <p>if the queue is full when writing</p>
    <p>How much space or data is in the buffer</p>
  </div>
  <div class="page">
    <p>Exposed socket buffers</p>
    <p>Testing the socket again is cheap!</p>
  </div>
  <div class="page">
    <p>Signaling - how does it work?</p>
    <p>Apps poll as long as there is a ready socket</p>
    <p>Apps block only if scanning sockets in user space fails</p>
    <p>Apps block on select-like system calls (or epoll, kqueue, . . . )</p>
    <p>Blocking calls internally use the select</p>
    <p>System wakes up the apps when sockets ready again</p>
    <p>The network stack cannot poll easily without disturbing the apps!</p>
  </div>
  <div class="page">
    <p>Network stack on a different core</p>
    <p>Lets place the stack on a dedicated core! The network stack can poll</p>
    <p>No interleaving with applications</p>
    <p>Only if an application is active, check the sockets</p>
    <p>When the network stack blocks, it uses MWAIT</p>
    <p>Applications can notify the network stack by a memory write</p>
    <p>Application</p>
    <p>Net Stack</p>
    <p>NIC</p>
  </div>
  <div class="page">
    <p>Mapping exposed socket buffers</p>
    <p>Mapped by the OS before apps get a handle</p>
    <p>Creating new mappings is slow</p>
    <p>We recycle existing mappings after closing sockets</p>
    <p>Reusing mappings is fast</p>
    <p>When many unused mappings, we unmap many at once</p>
    <p>Application</p>
    <p>Net Stack</p>
    <p>NIC</p>
  </div>
  <div class="page">
    <p>New Sockets - summary</p>
    <p>We do not need system calls for :</p>
    <p>Reading and writing</p>
    <p>Polling</p>
    <p>Signaling to the OS</p>
    <p>In addition :</p>
    <p>Copying within the address space of the applications</p>
    <p>OS used only for blocking</p>
  </div>
  <div class="page">
    <p>Does this actually work?</p>
  </div>
  <div class="page">
    <p>NewtOS - a multiserver system</p>
    <p>Multiserver system based on MINIX 3</p>
    <p>Implemented by isolated processes on top of a microkernel</p>
    <p>Multiserver systems primarily focus on reliability</p>
    <p>Low performance - is it inherent?</p>
  </div>
  <div class="page">
    <p>NewtOS - a multiserver system</p>
    <p>The network stack is in user space!</p>
    <p>System calls implemented by message passing</p>
    <p>Possibly a cascade of messages for each system call</p>
    <p>Execution of OS processes interleaves with apps.</p>
    <p>Application Net Stack NIC</p>
    <p>Microkernel</p>
  </div>
  <div class="page">
    <p>NewtOS - a multiserver system</p>
    <p>Multiserver systems easily embrace multicores</p>
    <p>Simple to run OS parts on dedicated cores</p>
    <p>Application Net Stack NIC</p>
    <p>Microkernel</p>
  </div>
  <div class="page">
    <p>NewtOS network stack</p>
    <p>microkernel</p>
    <p>NetDrvTCP SYSCALL IPApp</p>
    <p>Network stack partitioned for improved reliability</p>
    <p>Individual components on dedicated cores (+ UDP and PF)</p>
    <p>Each system call includes SYSCALL and at least 1 server</p>
  </div>
  <div class="page">
    <p>Multiserver systems - the cost of system calls</p>
    <p>Cycles to complete a recvfrom() call that returns EAGAIN</p>
    <p>NewtOS - kernel msgs 79800</p>
    <p>NewtOS - user space msgs 19950</p>
    <p>NewtOS - system call involves 3 processes on 3 different cores!</p>
  </div>
  <div class="page">
    <p>Multiserver systems - the cost of system calls</p>
    <p>Cycles to complete a recvfrom() call that returns EAGAIN</p>
    <p>NewtOS - kernel msgs 79800</p>
    <p>NewtOS - user space msgs 19950</p>
    <p>Linux 478</p>
    <p>NewtOS - system call involves 3 processes on 3 different cores!</p>
  </div>
  <div class="page">
    <p>Multiserver systems - the cost of system calls</p>
    <p>Cycles to complete a recvfrom() call that returns EAGAIN</p>
    <p>NewtOS - kernel msgs 79800</p>
    <p>NewtOS - user space msgs 19950</p>
    <p>Linux 478</p>
    <p>NewtOS - no system calls 137</p>
    <p>NewtOS - system call involves 3 processes on 3 different cores!</p>
  </div>
  <div class="page">
    <p>Evaluation - testbed</p>
    <p>lighttpd web server - single process</p>
    <p>Serving static files cached in memory</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ki lo</p>
    <p>-r eq</p>
    <p>ue st</p>
    <p>s / s</p>
    <p># of parallel connections</p>
    <p>Syscalls</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ki lo</p>
    <p>-r eq</p>
    <p>ue st</p>
    <p>s / s</p>
    <p># of parallel connections</p>
    <p>Syscalls Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>k il o -r</p>
    <p>e q u e s ts</p>
    <p>/ s</p>
    <p># of parallel connections</p>
    <p>No syscalls Syscalls</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ki lo</p>
    <p>-r eq</p>
    <p>ue st</p>
    <p>s / s</p>
    <p># of parallel connections</p>
    <p>Syscalls</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ki lo</p>
    <p>-r eq</p>
    <p>ue st</p>
    <p>s / s</p>
    <p># of parallel connections</p>
    <p>Syscalls Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>k il o -r</p>
    <p>e q u e s ts</p>
    <p>/ s</p>
    <p># of parallel connections</p>
    <p>No syscalls Syscalls</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ki lo</p>
    <p>-r eq</p>
    <p>ue st</p>
    <p>s / s</p>
    <p># of parallel connections</p>
    <p>No scalls Syscalls</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>re q u e s ts</p>
    <p>/ s</p>
    <p># of parallel connections</p>
    <p>No syscalls Syscalls</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Benefits of running system and applications on different cores Applications and the system can use entire cores</p>
    <p>Smaller instruction cache miss rate</p>
    <p>Linux 8.5%</p>
    <p>NewtOS - system calls 4.5%</p>
    <p>NewtOS - w/o system calls 1.5%</p>
    <p>lighttpd instruction cache miss rate</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>We cannot handle fork</p>
    <p>Shared memory would be used by independent processes</p>
    <p>We revert to legacy sockets - OS is responsible for synchronization</p>
    <p>fork is expensive, outweighs benefits for short-lived connections</p>
    <p>We can use ioctl as a workaround - requires code changes in legacy applications</p>
  </div>
  <div class="page">
    <p>Related work</p>
    <p>FlexSC - OSDI10 General mechanism decouples invocation from execution</p>
    <p>Batching as many syscalls as possible in shared memory</p>
    <p>MegaPipe - OSDI12</p>
    <p>Relaxes file-based POSIX Socket API - different API !!!</p>
    <p>IsoStack - ATC10 Network stack polls on a dedicated core</p>
    <p>No contention on data structures accessed by many cores</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Network stack on a different core and exposed socket buffers make the following possible :</p>
    <p>We can avoid 99% of system calls when load is high</p>
    <p>Applications can read information directly from the sockets</p>
    <p>Speculative nonblocking API calls are cheap</p>
    <p>Execution of applications and OS does not interleave</p>
    <p>The new socket design makes networking in reliable multiserver systems competitive with commodity operating systems!</p>
  </div>
</Presentation>
