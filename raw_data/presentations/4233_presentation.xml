<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Transfer Learning in Natural Language Processing</p>
    <p>June 2, 2019 NAACL-HLT 2019</p>
    <p>Sebastian Ruder</p>
    <p>Matthew Peters</p>
    <p>Swabha Swayamdipta</p>
    <p>Thomas Wolf</p>
  </div>
  <div class="page">
    <p>Transfer Learning in Natural Language ProcessingTransfer Learning in NLP</p>
    <p>Follow along with the tutorial:</p>
    <p>Slides: http://tiny.cc/NAACLTransfer  Colab: http://tiny.cc/NAACLTransferColab  Code: http://tiny.cc/NAACLTransferCode</p>
    <p>Questions:</p>
    <p>Twitter: #NAACLTransfer during the tutorial  Whova: Questions for the tutorial on Transfer Learning in NLP topic  Ask us during the break or after the tutorial</p>
  </div>
  <div class="page">
    <p>Pan and Yang (2010)</p>
    <p>What is transfer learning?</p>
  </div>
  <div class="page">
    <p>Why transfer learning in NLP?  Many NLP tasks share common knowledge about language (e.g. linguistic</p>
    <p>representations, structural similarities)  Tasks can inform each othere.g. syntax and semantics  Annotated data is rare, make use of as much supervision as available.</p>
    <p>Empirically, transfer learning has resulted in SOTA for many supervised NLP tasks (e.g. classification, information extraction, Q&amp;A, etc).</p>
  </div>
  <div class="page">
    <p>Why transfer learning in NLP? (Empirically) Performance on Named Entity Recognition (NER) on CoNLL-2003 (English) over time</p>
  </div>
  <div class="page">
    <p>Ruder (2019)</p>
    <p>We will focus on this</p>
    <p>Types of transfer learning in NLP</p>
  </div>
  <div class="page">
    <p>What this tutorial is about and what its not about</p>
    <p>Goal: provide broad overview of transfer methods in NLP, focusing on the most empirically successful methods as of today (mid 2019)</p>
    <p>Provide practical, hands on advice  by end of tutorial, everyone has ability to apply recent advances to text classification task</p>
    <p>What this is not: Comprehensive (its impossible to cover all related papers in one tutorial!)</p>
    <p>(Bender Rule: This tutorial is mostly for work done in English, extensibility to other languages depends on availability of data and resources.)</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Sequential transfer learning Learn on one task / dataset, then transfer to another task / dataset</p>
    <p>word2vec GloVe skip-thought InferSent ELMo ULMFiT GPT BERT</p>
    <p>classification sequence labeling Q&amp;A ....</p>
    <p>Pretraining Adaptation</p>
  </div>
  <div class="page">
    <p>Pretraining tasks and datasets  Unlabeled data and self-supervision</p>
    <p>Supervised pretraining  Very common in vision, less in NLP due to lack of large supervised datasets  Machine translation  NLI for sentence representations  Task-specifictransfer from one Q&amp;A dataset to another</p>
    <p>Easy to gather very large corpora: Wikipedia, news, web crawl, social media, etc.  Training takes advantage of distributional hypothesis: You shall know a word by the company</p>
    <p>it keeps (Firth, 1957), often formalized as training some variant of language model  Focus on efficient algorithms to make use of plentiful data</p>
  </div>
  <div class="page">
    <p>Target tasks and datasets Target tasks are typically supervised and span a range of common NLP tasks:</p>
    <p>Sentence or document classification (e.g. sentiment)  Sentence pair classification (e.g. NLI, paraphrase)  Word level (e.g. sequence labeling, extractive Q&amp;A)  Structured prediction (e.g. parsing)  Generation (e.g. dialogue, summarization)</p>
  </div>
  <div class="page">
    <p>Concrete exampleword vectors Word embedding methods (e.g. word2vec) learn one vector per word:</p>
    <p>cat = [0.1, -0.2, 0.4, ]</p>
    <p>dog = [0.2, -0.1, 0.7, ]</p>
  </div>
  <div class="page">
    <p>Concrete exampleword vectors Word embedding methods (e.g. word2vec) learn one vector per word:</p>
    <p>cat = [0.1, -0.2, 0.4, ]</p>
    <p>dog = [0.2, -0.1, 0.7, ]</p>
    <p>PRP VBP PRP NN CC NN .</p>
    <p>I love my cat and dog .</p>
  </div>
  <div class="page">
    <p>Concrete exampleword vectors Word embedding methods (e.g. word2vec) learn one vector per word:</p>
    <p>cat = [0.1, -0.2, 0.4, ]</p>
    <p>dog = [0.2, -0.1, 0.7, ]</p>
    <p>PRP VBP PRP NN CC NN .</p>
    <p>I love my cat and dog .</p>
    <p>I love my cat and dog . }-&gt; positive&quot;</p>
  </div>
  <div class="page">
    <p>Major Themes</p>
  </div>
  <div class="page">
    <p>Major themes: From words to words-in-context Word vectors</p>
    <p>cats = [0.2, -0.3, ]</p>
    <p>dogs = [0.4, -0.5, ]</p>
    <p>Sentence / doc vectors</p>
    <p>Its raining cats and dogs.</p>
    <p>We have two cats.</p>
    <p>[0.8, 0.9, ]</p>
    <p>[-1.2, 0.0, ]}</p>
    <p>}</p>
    <p>Word-in-context vectors</p>
    <p>We have two cats.</p>
    <p>}</p>
    <p>[1.2, -0.3, ]</p>
    <p>Its raining cats and dogs.</p>
    <p>}</p>
    <p>[-0.4, 0.9, ]</p>
  </div>
  <div class="page">
    <p>Major themes: LM pretraining  Many successful pretraining approaches are based on language modeling  Informally, a LM learns P(text) or P(text | some other text)</p>
    <p>Doesnt require human annotation  Many languages have enough text to learn high capacity model  Versatilecan learn both sentence and word representations with a variety of</p>
    <p>objective functions</p>
  </div>
  <div class="page">
    <p>Bengio et al 2003: A Neural Probabilistic Language Model</p>
    <p>Devlin et al 2019: BERT: Pre-training of Deep Bidirectional Transformers for Language</p>
    <p>Understanding</p>
    <p>Major themes: From shallow to deep</p>
  </div>
  <div class="page">
    <p>Major themes: pretraining vs target task</p>
    <p>Sentence / document representations not useful for word level predictions  Word vectors can be pooled across contexts, but often outperformed by other</p>
    <p>methods  In contextual word vectors, bidirectional context important</p>
    <p>Choice of pretraining and target tasks are coupled</p>
    <p>In general:</p>
    <p>Similar pretraining and target tasks  best results</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page">
    <p>Image credit: Creative Stall 22</p>
  </div>
  <div class="page">
    <p>Overview  Language model pretraining</p>
    <p>Word vectors</p>
    <p>Sentence and document vectors</p>
    <p>Contextual word vectors</p>
    <p>Interesting properties of pretraining</p>
    <p>Cross-lingual pretraining</p>
  </div>
  <div class="page">
    <p>Word Type RepresentationLM pretraining word2vec, Mikolov et al (2013)</p>
    <p>We [have a ??? and three] dogs We have a ???</p>
    <p>We have a MASK and three dogs</p>
    <p>ELMo, Peters et al. 2018, ULMFiT (Howard &amp; Ruder 2018), GPT (Radford et al. 2018)</p>
    <p>We have a ???</p>
    <p>We like pets. } Skip-Thought (Kiros et al., 2015)</p>
    <p>BERT, Devlin et al 2019 ???</p>
  </div>
  <div class="page">
    <p>Word vectors</p>
  </div>
  <div class="page">
    <p>Why embed words?</p>
    <p>Embeddings are themselves parameterscan be learned</p>
    <p>Sharing representations across tasks</p>
    <p>Lower dimensional space</p>
    <p>Better for computationdifficult to handle sparse vectors.</p>
  </div>
  <div class="page">
    <p>Word Type RepresentationUnsupervised pretraining : Pre-Neural Latent Semantic Analysis (LSA)SVD of term-document matrix, (Deerwester et al., 1990)</p>
    <p>Latent Dirichlet Allocation (LDA)Documents are mixtures of topics and topics are mixtures of words (Blei et al., 2003)</p>
    <p>Brown clusters, hard hierarchical clustering based on n-gram LMs, (Brown et al. 1992)</p>
  </div>
  <div class="page">
    <p>Word Type RepresentationWord vector pretraining n-gram neural language model (Bengio et al. 2003)</p>
    <p>Supervised multitask word embeddings (Collobert and Weston, 2008)</p>
  </div>
  <div class="page">
    <p>word2vec Efficient algorithm + large scale training  high quality word vectors</p>
    <p>(Mikolov et al., 2013)</p>
    <p>See also:  Pennington et al. (2014): GloVe  Bojanowski et al. (2017): fastText</p>
  </div>
  <div class="page">
    <p>Sentence and document vectors</p>
  </div>
  <div class="page">
    <p>Doc2vecParagraph vector Unsupervised paragraph embeddings (Le &amp; Mikolov, 2014)</p>
    <p>SOTA classification (IMDB, SST)</p>
  </div>
  <div class="page">
    <p>Doc2vecSkip-Thought Vectors Predict previous / next sentence with seq2seq model (Kiros et al., 2015)</p>
    <p>Hidden state of encoder transfers to sentence tasks (classification, semantic similarity)</p>
  </div>
  <div class="page">
    <p>Dai &amp; Le (2015): Pretrain a sequence autoencoder (SA) and generative LM</p>
    <p>Autoencoder pretraining</p>
    <p>SOTA classification (IMDB)</p>
    <p>See also:  Socher et. al (2011): Semi-supervised recursive auto encoder  Bowman et al. (2016): Variational autoencoder (VAE)  Hill et al. (2016): Denoising autoencoder</p>
  </div>
  <div class="page">
    <p>Autoencoder pretrainingSupervised sentence embeddings</p>
    <p>Also possible to train sentence embeddings with supervised objective</p>
    <p>Paragram-phrase: uses paraphrase database for supervision, best for paraphrase and semantic similarity (Wieting et al. 2016)</p>
    <p>InferSent: bi-LSTM trained on SNLI + MNLI (Conneau et al. 2017)  GenSen: multitask training (skip-thought, machine translation, NLI, parsing)</p>
    <p>(Subramanian et al. 2018)</p>
  </div>
  <div class="page">
    <p>Contextual word vectors</p>
  </div>
  <div class="page">
    <p>Contextual word vectors - Motivation Word vectors compress all contexts into a single vector</p>
    <p>Nearest neighbor GloVe vectors to play</p>
    <p>VERB playing played</p>
    <p>NOUN game games players football</p>
    <p>?? plays Play</p>
    <p>ADJ multiplayer</p>
  </div>
  <div class="page">
    <p>Contextual word vectors - Key Idea Instead of learning one vector per word, learn a vector that depends on context</p>
    <p>f(play | The kids play a game in the park.)</p>
    <p>f(play | The Broadway play premiered yesterday.)</p>
    <p>!=</p>
    <p>Many approaches based on language models</p>
  </div>
  <div class="page">
    <p>Sentence completion Lexical substitution</p>
    <p>WSD</p>
    <p>Use bidirectional LSTM and cloze prediction objective (a 1 layer masked LM)</p>
    <p>Learn representations for both words and contexts (minus word)</p>
    <p>context2vec</p>
    <p>(Melamud et al., CoNLL 2016)38</p>
  </div>
  <div class="page">
    <p>Pretrain two LMs (forward and backward) and add to sequence tagger. SOTA NER and chunking results</p>
    <p>TagLM</p>
    <p>(Peters et al. ACL 2017) 39</p>
  </div>
  <div class="page">
    <p>Pretrain encoder and decoder with LMs (everything shaded</p>
    <p>is pretrained).</p>
    <p>Large boost for MT.</p>
    <p>Unsupervised Pretraining for Seq2Seq</p>
    <p>(Ramachandran et al, EMNLP 2017) 40</p>
  </div>
  <div class="page">
    <p>Pretrain bidirectional encoder with MT supervision, extract LSTM states</p>
    <p>Adding CoVe with GloVe gives improvements for classification, NLI, Q&amp;A</p>
    <p>CoVe</p>
    <p>(McCann et al, NeurIPS 2017) 41</p>
  </div>
  <div class="page">
    <p>Pretrain deep bidirectional LM, extract contextual word vectors as learned linear combination of hidden states</p>
    <p>SOTA for 6 diverse tasks</p>
    <p>ELMo</p>
    <p>(Peters et al, NAACL 2018) 42</p>
  </div>
  <div class="page">
    <p>ULMFiT</p>
    <p>Pretrain AWD-LSTM LM, fine-tune LM in two stages with different adaptation techniques</p>
    <p>SOTA for six classification datasets</p>
    <p>(Howard and Ruder, ACL 2018) 43</p>
  </div>
  <div class="page">
    <p>GPT</p>
    <p>(Radford et al., 2018)</p>
    <p>Pretrain large 12-layer left-to-right Transformer, fine tune for sentence, sentence pair and multiple choice questions.</p>
    <p>SOTA results for 9 tasks.</p>
  </div>
  <div class="page">
    <p>BERT</p>
    <p>(Devlin et al. 2019)</p>
    <p>BERT pretrains both sentence and contextual word representations, using masked LM and next sentence prediction. BERT-large has 340M parameters, 24 layers!</p>
  </div>
  <div class="page">
    <p>BERT</p>
    <p>(Devlin et al. 2019)</p>
    <p>SOTA GLUE benchmark results (sentence pair classification).</p>
  </div>
  <div class="page">
    <p>BERT</p>
    <p>(Devlin et al. 2019)</p>
    <p>SOTA SQuAD v1.1 (and v2.0) Q&amp;A</p>
  </div>
  <div class="page">
    <p>Other pretraining objectives</p>
    <p>Contextual string representations (Akbik et al., COLING 2018)SOTA NER results</p>
    <p>Cross-view training (Clark et al. EMNLP 2018)improve supervised tasks with unlabeled data</p>
    <p>Cloze-driven pretraining (Baevski et al. (2019)SOTA NER and constituency parsing</p>
  </div>
  <div class="page">
    <p>Why does language modeling work so well?  Language modeling is a very difficult task, even for humans.  Language models are expected to compress any possible context into a</p>
    <p>vector that generalizes over possible completions.  They walked down the street to ???</p>
    <p>To have any chance at solving this task, a model is forced to learn syntax, semantics, encode facts about the world, etc.</p>
    <p>Given enough data, a huge model, and enough compute, can do a reasonable job!</p>
    <p>Empirically works better than translation, autoencoding: Language Modeling Teaches You More Syntax than Translation Does (Zhang et al. 2018)</p>
  </div>
  <div class="page">
    <p>Sample efficiency</p>
  </div>
  <div class="page">
    <p>Pretraining reduces need for annotated data</p>
    <p>(Peters et al, NAACL 2018) 51</p>
  </div>
  <div class="page">
    <p>Pretraining reduces need for annotated data</p>
    <p>(Howard and Ruder, ACL 2018) 52</p>
  </div>
  <div class="page">
    <p>Pretraining reduces need for annotated data</p>
    <p>(Clark et al. EMNLP 2018) 53</p>
  </div>
  <div class="page">
    <p>Scaling up pretraining</p>
  </div>
  <div class="page">
    <p>Scaling up pretraining</p>
    <p>More data  better word</p>
    <p>vectors</p>
    <p>(Pennington et al 2014)</p>
  </div>
  <div class="page">
    <p>Pretrained Language Models: More Data</p>
    <p>Baevski et al. (2019)</p>
    <p>Scaling up pretraining</p>
  </div>
  <div class="page">
    <p>Scaling up pretraining</p>
    <p>Bigger model  better results</p>
    <p>(Devlin et al 2019)</p>
  </div>
  <div class="page">
    <p>Cross-lingual pretraining</p>
  </div>
  <div class="page">
    <p>Cross-lingual pretraining</p>
    <p>Much work on training cross-lingual word embeddings (Overview: Ruder et al. (2017))</p>
    <p>Idea: train each language separately, then align.</p>
    <p>Recent work aligning ELMo: Schuster et al., (NAACL 2019)</p>
    <p>ACL 2019 Tutorial on Unsupervised Cross-lingual Representation Learning</p>
  </div>
  <div class="page">
    <p>Cross-lingual Polyglot Pretraining Key idea: Share vocabulary and representations across languages by training one model on many languages.</p>
    <p>Advantages: Easy to implement, enables cross-lingual pretraining by itself</p>
    <p>Disadvantages: Leads to under-representation of low-resource languages  LASER: Use parallel data for sentence representations (Artetxe &amp; Schwenk,</p>
  </div>
  <div class="page">
    <p>Hands-on #1: Pretraining a Transformer Language Model</p>
    <p>Image credit: Chanaky 61</p>
  </div>
  <div class="page">
    <p>Hands-on: Overview</p>
    <p>Goals:  Lets make these recent works uncool again i.e. as accessible as possible  Expose all the details in a simple, concise and self-contained code-base  Show that transfer learning can be simple (less hand-engineering) &amp; fast (pretrained model)</p>
    <p>Plan  Build a GPT-2 / BERT model  Pretrain it on a rather large corpus with ~100M words  Adapt it for a target task to get SOTA performances</p>
    <p>Material:  Colab: http://tiny.cc/NAACLTransferColab  code of the following slides  Code: http://tiny.cc/NAACLTransferCode  same code organized in a repo</p>
    <p>Current developments in Transfer Learning combine new approaches for training schemes (sequential training) as well as models (transformers)  can look intimidating and complex</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training Colab: http://tiny.cc/NAACLTransferColab Repo: http://tiny.cc/NAACLTransferCode</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training</p>
    <p>summing words and position embeddings  applying a succession of transformer blocks with:</p>
    <p>layer normalisation  a self-attention module  dropout and a residual connection</p>
    <p>another layer normalisation  a feed-forward module with one hidden layer and</p>
    <p>a non linearity: Linear  ReLU/gelu  Linear  dropout and a residual connection</p>
    <p>The</p>
    <p>(Child et al, 2019)</p>
    <p>Our core model will be a Transformer. Large-scale transformer architectures (GPT-2, BERT, XLM) are very similar to each other and consist of:</p>
    <p>Main differences between GPT/GPT-2/BERT are the objective functions:  causal language modeling for GPT  masked language modeling for BERT (+ next sentence prediction)</p>
    <p>Well play with both</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training Lets code the backbone of our model!</p>
    <p>PyTorch 1.1 now has a nn.MultiHeadAttention module: lets us encapsulate the self-attention logic while still controlling the internals of the Transformer.</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training Two attention masks?</p>
    <p>padding_mask masks the padding tokens. It is specific to each sample in the batch:</p>
    <p>attn_mask is the same for all samples in the batch. It masks the previous tokens for causal transformers:</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training</p>
    <p>To pretrain our model, we need to add a few elements: a head, a loss and initialize weights.</p>
    <p>We add these elements with a pretraining model encapsulating our model.</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training</p>
    <p>Hyper-parameters taken from Dai et al., 2018 (Transformer-XL)  ~50M parameters causal model.</p>
    <p>Now lets take care of our data and configurationWe'll use a pre-defined open vocabulary tokenizer: BERTs model cased tokenizer.</p>
    <p>Use a large dataset for pre-trainining: WikiText-103 with 103M tokens (Merity et al., 2017).</p>
    <p>Instantiate our model and optimizer (Adam)</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training</p>
    <p>A simple update loop. We use gradient accumulation to have a large batch size even on 1 GPU (&gt;64).</p>
    <p>Learning rate schedule: - linear warmup to start - then cosine or inverse square root decrease</p>
    <p>Go!</p>
    <p>And were done: lets train!</p>
    <p>no warm-up</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training  Concluding remarks  On pretraining</p>
    <p>Intensive: in our case 5h20h on 8 V100 GPUs (few days w. 1 V100) to reach a good perplexity  share your pretrained models</p>
    <p>Robust to the choice of hyper-parameters (apart from needing a warm-up for transformers)  Language modeling is a hard task, your model should not have enough capacity to overfit if your</p>
    <p>dataset is large enough  you can just start the training and let it run.  Masked-language modeling: typically 2-4 times slower to train than LM</p>
    <p>We only mask 15% of the tokens  smaller signal</p>
    <p>For the rest of this tutorial We dont have enough time to do a full pretraining  we pretrained two models for you before the tutorial</p>
  </div>
  <div class="page">
    <p>Hands-on pre-training  Concluding remarks  First model:</p>
    <p>exactly the one we built together  a 50M parameters causal Transformer  Trained 15h on 8 V100  Reached a word-level perplexity of 29 on wikitext-103 validation set (quite competitive)</p>
    <p>Second model:  Same model but trained with a masked-language modeling objective (see the repo)  Trained 30h on 8 V100  Reached a masked-word perplexity of 8.3 on wikitext-103 validation set</p>
    <p>Dai et al., 2018</p>
    <p>Wikitext-103 Validation/Test PPL</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page">
    <p>Image credit: Caique Lima 73</p>
  </div>
  <div class="page">
    <p>Why care about what is in a representation?  Extrinsic evaluation with downstream tasks</p>
    <p>Complex, diverse with task-specific quirks</p>
    <p>Interpretability!  Are we getting our results because of the right reasons?  Uncovering biases...</p>
    <p>Language-aware representations  To generalize to other tasks, new inputs  As intermediates for possible improvements to pretraining</p>
  </div>
  <div class="page">
    <p>What to analyze?</p>
    <p>Embeddings  Word  Contextualized</p>
    <p>Network Activations</p>
    <p>Variations  Architecture (RNN / Transformer)  Layers  Pretraining Objectives</p>
  </div>
  <div class="page">
    <p>Analysis Method 1: Visualization Hold the embeddings / network activations static or frozen</p>
  </div>
  <div class="page">
    <p>Plotting embeddings in a lower dimensional (2D/3D) space  t-SNE van der Maaten &amp; Hinton, 2008  PCA projections</p>
    <p>Visualizing Embedding Geometries</p>
    <p>Image: Tensorflow</p>
    <p>Visualizing word analogies Mikolov et al. 2013  Spatial relations  wking - wman + wwoman ~ wqueen</p>
    <p>High-level view of lexical semantics  Only a limited number of examples  Connection to other tasks is unclear</p>
    <p>Goldberg, 2017</p>
  </div>
  <div class="page">
    <p>Karpathy et al., 2016</p>
    <p>Neuron activation values correlate with features / labels</p>
    <p>Visualizing Neuron Activations</p>
    <p>Indicates learning of recognizable features  How to select which neuron? Hard to scale!  Interpretable != Important (Morcos et al., 2018)</p>
    <p>Radford et al., 2017</p>
  </div>
  <div class="page">
    <p>Layer-wise analysis (static)</p>
    <p>How important is each layer for a given performance on a downstream task?  Weighted average of layers</p>
    <p>Visualizing Layer-Importance Weights</p>
    <p>Peters et al.. EMNLP 2018</p>
    <p>Task and architecture specific!</p>
    <p>Also see Tenney et al., ACL 2019</p>
  </div>
  <div class="page">
    <p>Visualization: Attention WeightsVisualizing Attention Weights  Popular in machine translation, or</p>
    <p>other seq2seq architectures:  Alignment between words of source and</p>
    <p>target.  Long-distance word-word dependencies</p>
    <p>(intra-sentence attention)</p>
    <p>Vaswani et al., 2017</p>
    <p>Sheds light on architectures  Having sophisticated attention mechanisms</p>
    <p>can be a good thing!  Layer-specific</p>
    <p>Interpretation can be tricky  Few examples only - cherry picking?  Robust corpus-wide trends? Next!</p>
  </div>
  <div class="page">
    <p>Analysis Method 2: Behavioral Probes</p>
    <p>Linzen et al., 2016; Gulordava et al. 2018; Marvin et al., 2018</p>
    <p>RNN-based language models  number agreement in subject-verb dependencies  natural and nonce or ungrammatical sentences  evaluate on output perplexity</p>
    <p>RNNs outperform other non-neural baselines.</p>
    <p>Performance improves when trained explicitly with syntax (Kuncoro et al. 2018)</p>
    <p>Kuncoro et al. 2018</p>
  </div>
  <div class="page">
    <p>Analysis Method 2: Behavioral Probes</p>
    <p>Linzen et al., 2016; Gulordava et al. 2018; Marvin et al., 2018</p>
    <p>RNN-based language models (RNN-based)  number agreement in subject-verb dependencies  For natural and nonce/ungrammatical sentences  LM perplexity differences</p>
    <p>RNNs outperform other non-neural baselines.</p>
    <p>Performance improves when trained explicitly with syntax (Kuncoro et al. 2018)</p>
    <p>Probe: Might be vulnerable to co-occurrence biases  dogs in the neighborhood bark(s)  Nonce sentences might be too different from original...</p>
    <p>Kuncoro et al. 2018</p>
  </div>
  <div class="page">
    <p>Analysis Method 3: Classifier Probes Hold the embeddings / network activations static and</p>
    <p>train a simple supervised model on top</p>
    <p>Probe classification task (Linear / MLP)</p>
  </div>
  <div class="page">
    <p>Given a sentence, predict properties such as  Length  Is a word in the sentence?</p>
    <p>Probing Surface-level Features</p>
    <p>Zhang et al. 2018; Liu et al., 2018; Conneau et al., 2018</p>
    <p>Given a word in a sentence predict properties such as:  Previously seen words, contrast with language model  Position of word in the sentence</p>
    <p>Checks ability to memorize  Well-trained, richer architectures tend to fare better  Training on linguistic data memorizes better</p>
  </div>
  <div class="page">
    <p>Sentence-level Syntax</p>
    <p>Tree Depth</p>
    <p>Tense of main clause verb</p>
    <p>Top Constituents</p>
    <p>Long-distance number agreement</p>
    <p># Objects</p>
    <p>Adi et al., 2017; Conneau et al., 2018; Belinkov et al., 2017; Zhang et al., 2018; Blevins et al., 2018; Tenney et al. 2019; Liu et al., 2019</p>
    <p>Probing Morphology, Syntax, Semantics Subject-Verb Agreement</p>
    <p>Morphology</p>
    <p>Word-level syntax  POS tags, CCG supertags  Constituent parent,</p>
    <p>grandparent</p>
    <p>Partial syntax  Dependency relations</p>
    <p>Partial semantics  Entity Relations  Coreference  Roles</p>
  </div>
  <div class="page">
    <p>Probing classifier findings</p>
    <p>Liu et al. NAACL 2019</p>
    <p>Tenney et al., ACL 2019</p>
    <p>Hewitt et al., 2019</p>
  </div>
  <div class="page">
    <p>Probing classifier findings</p>
    <p>Liu et al. (NAACL 2019)</p>
    <p>Tenney et al., ACL 2019</p>
    <p>Contextualized &gt; non-contextualized  Especially on syntactic tasks  Closer performance on semantic tasks  Bidirectional context is important</p>
    <p>BERT (large) almost always gets the highest performance  Grain of salt: Different contextualized</p>
    <p>representations were trained on different data, using different architectures...</p>
    <p>Hewitt et. al., 2019</p>
  </div>
  <div class="page">
    <p>Fig. from Liu et al. (NAACL 2019)</p>
    <p>Layer-wise analysis (dynamic)</p>
    <p>RNN layers: General linguistic properties  Lowest layers: morphology  Middle layers: syntax  Highest layers: Task-specific semantics</p>
    <p>Transformer layers:  Different trends for different tasks; middle-heavy  Also see Tenney et. al., 2019</p>
    <p>Probing: Layers of the network</p>
  </div>
  <div class="page">
    <p>Language modeling outperforms other unsupervised and supervised objectives.  Machine Translation  Dependency Parsing  Skip-thought</p>
    <p>Low-resource settings (size of training data) might result in opposite trends.</p>
    <p>Zhang et al., 2018; Blevins et al., 2018; Liu et al., 2019;</p>
    <p>Probing: Pretraining Objectives</p>
  </div>
  <div class="page">
    <p>Representations are predictive of certain linguistic phenomena:  Alignments in translation, Syntactic hierarchies</p>
    <p>What have we learnt so far?</p>
    <p>Pretraining with and without syntax:  Better performance with syntax  But without, some notion of syntax at least (Williams et al. 2018)</p>
    <p>Network architectures determine what is in a representation  Syntax and BERT Transformer (Tenney et al., 2019; Goldberg, 2019)  Different layer-wise trends across architectures</p>
  </div>
  <div class="page">
    <p>What information should a good probe look for?  Probing a probe!</p>
    <p>Open questions about probes</p>
    <p>What does probing performance tell us?  Hard to synthesize results across a variety of baselines...</p>
    <p>Can introduce some complexity in itself  linear or non-linear classification.  behavioral: design of input sentences</p>
    <p>Should we be using probes as evaluation metrics?  might defeat the purpose...</p>
  </div>
  <div class="page">
    <p>Progressively erase or mask network components  Word embedding dimensions  Hidden units  Input - words / phrases</p>
    <p>Analysis Method 4: Model Alterations</p>
    <p>Li et al., 2016 92</p>
  </div>
  <div class="page">
    <p>So, what is in a representation?  Depends on how you look at it!</p>
    <p>Visualization:  birds eye view  few samples -- might call to mind cherry-picking</p>
    <p>Probes:  discover corpus-wide specific properties  may introduce own biases...</p>
    <p>Network ablations:  great for improving modeling,  could be task specific</p>
    <p>Analysis methods as tools to aid model development!</p>
  </div>
  <div class="page">
    <p>Very current and ongoing!</p>
    <p>First column for citations in and before 2015</p>
  </div>
  <div class="page">
    <p>Whats next?</p>
    <p>Conneau et al., 2018</p>
    <p>Correlation of probes to downstream tasks</p>
    <p>Linguistic Awareness</p>
    <p>Interpretability</p>
    <p>Interpretability + transferability to downstream tasks is key</p>
    <p>Up next!</p>
  </div>
  <div class="page">
    <p>Suite of word-based and word-pair-based tasks: Liu et al. 2019 (3B Semantics)</p>
    <p>https://github.com/nelson-liu/contextual-repr-analysis</p>
    <p>Structural Probes: Hewitt &amp; Manning 2019 (9E Machine Learning)</p>
    <p>Overview of probes : Belinkov &amp; Glass, 2019 (7F Poster #18)</p>
    <p>Some Pointers</p>
  </div>
  <div class="page">
    <p>Break</p>
    <p>Image credit: Andrejs Kirma 97</p>
  </div>
  <div class="page">
    <p>Transfer Learning in Natural Language ProcessingTransfer Learning in NLP</p>
    <p>Follow along with the tutorial:</p>
    <p>Slides: https://tinyurl.com/NAACLTransfer  Colab: https://tinyurl.com/NAACLTransferColab  Code: https://tinyurl.com/NAACLTransferCode</p>
    <p>Questions:</p>
    <p>Twitter: #NAACLTransfer during the tutorial  Whova: Questions for the tutorial on Transfer Learning in NLP topic  Ask us during the break or after the tutorial</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page">
    <p>Image credit: Ben Didier 100</p>
  </div>
  <div class="page">
    <p>Several orthogonal directions we can make decisions on:</p>
  </div>
  <div class="page">
    <p>Two general options:</p>
    <p>Image credit: Darmawansyah</p>
    <p>A. Keep pretrained model internals unchanged: Add classifiers on top, embeddings at the bottom, use outputs as features</p>
    <p>B. Modify pretrained model internal architecture: Initialize encoder-decoders, task-specific modifications, adapters</p>
  </div>
  <div class="page">
    <p>LM b. Not always needed: some adaptation schemes</p>
    <p>re-use the pretraining objective/task, e.g. for multi-task learning</p>
  </div>
  <div class="page">
    <p>Task-specific, randomly initialized</p>
    <p>General, pretrained</p>
    <p>the pretrained model</p>
  </div>
  <div class="page">
    <p>the pretrained model b. More complex: model output as input for</p>
    <p>a separate model c. Often beneficial when target task requires</p>
    <p>interactions that are not available in pretrained embedding</p>
  </div>
  <div class="page">
    <p>a. Ex: Pretraining with a single input sequence (ex: language modeling) but adapting to a task with several input sequences (ex: translation, conditional generation...)</p>
    <p>b. Use the pretrained model weights to initialize as much as possible of a structurally different target task model</p>
    <p>c. Ex: Use monolingual LMs to initialize encoder and decoder parameters for MT (Ramachandran et al., EMNLP 2017; Lample &amp; Conneau, 2019)</p>
  </div>
  <div class="page">
    <p>are useful for the target task b. Ex: Adding skip/residual connections, attention</p>
    <p>(Ramachandran et al., EMNLP 2017)</p>
  </div>
  <div class="page">
    <p>size of model parameters c. Ex: add bottleneck modules (adapters)</p>
    <p>between the layers of the pretrained model (Rebuffi et al., NIPS 2017; CVPR 2018)</p>
    <p>Various reasons:</p>
  </div>
  <div class="page">
    <p>Image credit: Caique Lima 109</p>
    <p>Commonly connected with a residual connection in parallel to an existing layer</p>
    <p>Most effective when placed at every layer (smaller effect at bottom layers)</p>
    <p>Different operations (convolutions, self-attention) possible</p>
    <p>Particularly suitable for modular architectures like Transformers (Houlsby et al., ICML 2019; Stickland and Murray, ICML 2019)</p>
  </div>
  <div class="page">
    <p>Multi-head attention (MH; shared across layers) is used in parallel with self-attention (SA) layer of BERT</p>
    <p>Both are added together and fed into a layer-norm (LN)</p>
    <p>Adapters (Stickland &amp; Murray, ICML 2019)</p>
  </div>
  <div class="page">
    <p>Hands-on #2: Adapting our pretrained model</p>
    <p>Image credit: Chanaky 111</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation</p>
    <p>Plan  Start from our Transformer language model  Adapt the model to a target task:</p>
    <p>keep the model core unchanged, load the pretrained weights  add a linear layer on top, newly initialized  use additional embeddings at the bottom, newly initialized</p>
    <p>Reminder  material is here:  Colab http://tiny.cc/NAACLTransferColab  code of the following slides  Code http://tiny.cc/NAACLTransferCode  same code in a repo</p>
    <p>Lets see how a simple fine-tuning scheme can be implemented with our pretrained model:</p>
  </div>
  <div class="page">
    <p>Adaptation task  We select a text classification task as the downstream task</p>
    <p>TREC-6: The Text REtrieval Conference (TREC) Question Classification (Li et al., COLING 2002)</p>
    <p>TREC consists of open-domain, fact-based questions divided into broad semantic categories contains 5500 labeled training questions &amp; 500 testing questions with 6 labels:</p>
    <p>NUM, LOC, HUM, DESC, ENTY, ABBR</p>
    <p>Hands-on: Model adaptation</p>
    <p>Ex:  How did serfdom develop in and then leave Russia ? &gt; DESC  What films featured the character Popeye Doyle ? &gt; ENTY</p>
    <p>(Howard and Ruder, ACL 2018)</p>
    <p>Transfer learning models shine on this type of low-resource task</p>
  </div>
  <div class="page">
    <p>Modifications:  Keep model internals unchanged  Add a linear layer on top  Add an additional embedding (classification token) at the bottom</p>
    <p>Computation flow:  Model input: the tokenized question with a classification token at the end  Extract the last hidden-state associated to the classification token  Pass the hidden-state in a linear layer and softmax to obtain class</p>
    <p>probabilities</p>
    <p>Hands-on: Model adaptation</p>
    <p>(Radford et al., 2018)</p>
    <p>First adaptation scheme</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation</p>
    <p>Lets load and prepare our dataset:</p>
    <p>Fine-tuning hyper-parameters:  6 classes in TREC-6  Use fine tuning hyper parameters from Radford et al., 2018:</p>
    <p>learning rate from 6.5e-5 to 0.0  fine-tune for 3 epochs</p>
    <p>- trim to the transformer input size &amp; add a classification token at the end of each sample, - pad to the left, - convert to tensors, - extract a validation set.</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation Adapt our model architecture</p>
    <p>Replace the pre-training head (language modeling) with the classification head: A linear layer, which takes as input the hidden-state of the [CLF] token (using a mask)</p>
    <p>Keep our pretrained model unchanged as the backbone.</p>
    <p>* Initialize all the weights of the model. * Reload common weights from the pretrained model.</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation Our fine-tuning code:</p>
    <p>We will evaluate on our validation and test sets: * validation: after each epoch * test: at the end</p>
    <p>A simple training update function: * prepare inputs: transpose and build padding &amp; classification token masks * we have options to clip and accumulate gradients</p>
    <p>Schedule: * linearly increasing to lr * linearly decreasing to 0.0</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation  Results We can now fine-tune our model on TREC:</p>
    <p>We are at the state-of-the-art (ULMFiT)</p>
    <p>Remarks:  The error rate goes down quickly! After one epoch we already have &gt;90% accuracy.</p>
    <p>Fine-tuning is highly data efficient in Transfer Learning  We took our pre-training &amp; fine-tuning hyper-parameters straight from the literature on related models.</p>
    <p>Fine-tuning is often robust to the exact choice of hyper-parameters</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation  Results Lets conclude this hands-on with a few additional words on robustness &amp; variance.  Large pretrained models (e.g. BERT large) are</p>
    <p>prone to degenerate performance when fine-tuned on tasks with small training sets.</p>
    <p>Observed behavior is often on-off: it either works very well or doesnt work at all.</p>
    <p>Understanding the conditions and causes of this behavior (models, adaptation schemes) is an open research question.</p>
    <p>Phang et al., 2018 119</p>
  </div>
  <div class="page">
    <p>Several directions when it comes to the optimization itself:</p>
    <p>Image credit: ProSymbols, purplestudio, Markus, Alfredo 120</p>
    <p>A. Choose which weights we should update Feature extraction, fine-tuning, adapters</p>
    <p>B. Choose how and when to update the weights From top to bottom, gradual unfreezing, discriminative fine-tuning</p>
    <p>C. Consider practical trade-offs Space and time complexity, performance</p>
  </div>
  <div class="page">
    <p>The main question: To tune or not to tune (the pretrained weights)?</p>
    <p>Image credit: purplestudio 121</p>
    <p>A. Do not change pretrained weights Feature extraction, adapters</p>
    <p>B. Change pretrained weights Fine-tuning</p>
  </div>
  <div class="page">
    <p>Dont touch the pretrained weights!</p>
    <p>Feature extraction:  Weights are frozen</p>
  </div>
  <div class="page">
    <p>Feature extraction:  Weights are frozen  A linear classifier is trained on top of</p>
    <p>the pretrained representations</p>
  </div>
  <div class="page">
    <p>Feature extraction:  Weights are frozen  A linear classifier is trained on top of the</p>
    <p>pretrained representations  Dont just use features of the top layer!  Learn a linear combination of layers</p>
    <p>(Peters et al., NAACL 2018, Ruder et al., AAAI 2019)</p>
  </div>
  <div class="page">
    <p>Dont touch the pretrained weights!</p>
    <p>Feature extraction:  Alternatively, pretrained</p>
    <p>representations are used as features in downstream model</p>
  </div>
  <div class="page">
    <p>Adapters  Task-specific modules that are</p>
    <p>added in between existing layers</p>
  </div>
  <div class="page">
    <p>Dont touch the pretrained weights!</p>
    <p>Adapters  Task-specific modules that are</p>
    <p>added in between existing layers  Only adapters are trained</p>
  </div>
  <div class="page">
    <p>Fine-tuning:</p>
    <p>Pretrained weights are used as initialization for parameters of the downstream model</p>
    <p>The whole pretrained architecture is trained during the adaptation phase</p>
  </div>
  <div class="page">
    <p>Hands-on #3: Using Adapters and freezing</p>
    <p>Image credit: Chanaky 129</p>
  </div>
  <div class="page">
    <p>Modifications:  add Adapters inside the backbone</p>
    <p>model: Linear  ReLU  Linear with a skip-connection</p>
    <p>As previously:  add a linear layer on top  use an additional embedding</p>
    <p>(classification token) at the bottom</p>
    <p>Hands-on: Model adaptation Second adaptation scheme: Using Adapters</p>
    <p>Houlsby et al., ICML 2019</p>
    <p>We will only train the adapters, the added linear layer and the embeddings. The other parameters of the model will be frozen.</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation Lets adapt our model architecture</p>
    <p>Add the adapter modules: Bottleneck layers with 2 linear layers and a non-linear activation function (ReLU)</p>
    <p>Hidden dimension is small: e.g. 32, 64, 256</p>
    <p>Inherit from our pretrained model to have all the modules.</p>
    <p>The Adapters are inserted inside skip-connections after:  the attention module  the feed-forward module</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation Now we need to freeze the portions of our model we dont want to train.</p>
    <p>We just indicate that no gradient is needed for the frozen parameters by setting param.requires_grad to False for the frozen parameters:</p>
    <p>In our case we will train 25% of the parameters. The model is small &amp; deep (many adapters) and we need to train the embeddings so the ratio stay quite high. For a larger model this ratio would be a lot lower.</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation</p>
    <p>Results similar to full-fine-tuning case with advantage of training only 25% of the full model parameters. For a small 50M parameters model this method is overkill  for 300M1.5B parameters models.</p>
    <p>We use a hidden dimension of 32 for the adapters and a learning rate ten times higher for the fine-tuning (we have added quite a lot of newly initialized parameters to train from scratch).</p>
  </div>
  <div class="page">
    <p>We have decided which weights to update, but in which order and how should be update them?</p>
    <p>Motivation: We want to avoid overwriting useful pretrained information and maximize positive transfer.</p>
    <p>Related concept: Catastrophic forgetting (McCloskey &amp; Cohen, 1989; French, 1999) When a model forgets the task it was originally trained on.</p>
    <p>Image credit: Markus 134</p>
  </div>
  <div class="page">
    <p>Progressively in time: freezing  Progressively in intensity: Varying the</p>
    <p>learning rates  Progressively vs. the pretrained model:</p>
    <p>Regularization</p>
  </div>
  <div class="page">
    <p>Solution: Train layers individually to give them time to adapt to new task and data.</p>
    <p>Goes back to layer-wise training of early deep neural networks (Hinton et al., 2006; Bengio et al., 2007).</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time 1. Train new layer</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time 1. Train new layer 2. Train one layer at a time</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time 1. Train new layer 2. Train one layer at a time</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time 1. Train new layer 2. Train one layer at a time</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time 1. Train new layer 2. Train one layer at a time 3. Train all layers</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
    <p>Sequential unfreezing (Chronopoulou et al., NAACL 2019): hyper-parameters that determine length of fine-tuning 1. Fine-tune additional parameters for epochs</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
    <p>Sequential unfreezing (Chronopoulou et al., NAACL 2019): hyper-parameters that determine length of fine-tuning 1. Fine-tune additional parameters for epochs 2. Fine-tune pretrained parameters without embedding</p>
    <p>layer for epochs 148</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
    <p>Sequential unfreezing (Chronopoulou et al., NAACL 2019): hyper-parameters that determine length of fine-tuning 1. Fine-tune additional parameters for epochs 2. Fine-tune pretrained parameters without embedding</p>
    <p>layer for epochs 3. Train all layers until convergence 149</p>
  </div>
  <div class="page">
    <p>Freezing all but the top layer (Long et al., ICML 2015)</p>
    <p>Chain-thaw (Felbo et al., EMNLP 2017): training one layer at a time</p>
    <p>Gradually unfreezing (Howard &amp; Ruder, ACL 2018): unfreeze one layer after another</p>
    <p>Sequential unfreezing (Chronopoulou et al., NAACL 2019): hyper-parameters that determine length of fine-tuning</p>
    <p>Commonality: Train all parameters jointly in the end</p>
  </div>
  <div class="page">
    <p>Hands-on #4: Using gradual unfreezing</p>
    <p>Image credit: Chanaky 151</p>
  </div>
  <div class="page">
    <p>Hands-on: Adaptation Gradual unfreezing is similar to our previous freezing process. We start by freezing all the model except the newly added parameters:</p>
    <p>We then gradually unfreeze an additional block along the training so that we train the full model at the end:</p>
    <p>Find index of layer to unfreeze</p>
    <p>Name pattern matching</p>
    <p>Unfreezing interval</p>
  </div>
  <div class="page">
    <p>Hands-on: Adaptation Gradual unfreezing has not been investigated in details for Transformer models</p>
    <p>no specific hyper-parameters advocated in the literature Residual connections may have an impact on the method</p>
    <p>should probably adapt LSTM hyper-parameters</p>
    <p>We show simple experiments in the Colab. Better hyper-parameters settings can probably be found.</p>
  </div>
  <div class="page">
    <p>Where and when?</p>
    <p>Lower layers (capture general information)  Early in training (model still needs to adapt</p>
    <p>to target distribution)  Late in training (model is close to</p>
    <p>convergence)</p>
  </div>
  <div class="page">
    <p>ACL 2018)  Lower layers capture general information</p>
    <p>Use lower learning rates for lower layers</p>
  </div>
  <div class="page">
    <p>ACL 2018)  Quickly move to a suitable region, then slowly</p>
    <p>converge over time</p>
  </div>
  <div class="page">
    <p>ACL 2018)  Quickly move to a suitable region, then slowly</p>
    <p>converge over time  Also known as learning rate warm-up  Used e.g. in Transformer (Vaswani et al., NIPS</p>
    <p>Facilitates optimization; easier to escape suboptimal local minima</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Regularize new parameters not to deviate too much from pretrained ones (Wiese et al., CoNLL 2017):</p>
  </div>
  <div class="page">
    <p>weight consolidation; EWC): Focus on parameters that are important for the pretrained task based on the Fisher information matrix (Kirkpatrick et al., PNAS 2017):</p>
  </div>
  <div class="page">
    <p>EWC has downsides in continual learning:</p>
    <p>May over-constrain parameters</p>
    <p>Computational cost is linear in the number of tasks (Schwarz et al., ICML 2018)</p>
  </div>
  <div class="page">
    <p>If tasks are similar, we may also encourage source and target predictions to be close based on cross-entropy, similar to distillation:</p>
  </div>
  <div class="page">
    <p>Hands-on #5: Using discriminative learning</p>
    <p>Image credit: Chanaky 163</p>
  </div>
  <div class="page">
    <p>Hands-on: Model adaptation Discriminative learning rate can be implemented using two steps in our example:</p>
    <p>We can then compute the learning rate of each group depending on its label (at each training iteration):</p>
    <p>First we organize the parameters of the various layers in labelled parameters groups in the optimizer:</p>
    <p>Hyper-parameter</p>
  </div>
  <div class="page">
    <p>Several trade-offs when choosing which weights to update:</p>
    <p>Image credit: Alfredo</p>
    <p>A. Space complexity Task-specific modifications, additional parameters, parameter reuse</p>
    <p>B. Time complexity Training time</p>
    <p>C. Performance</p>
  </div>
  <div class="page">
    <p>Many Few</p>
    <p>Feature extraction Fine-tuningAdapters Task-specific modifications</p>
    <p>Many Few</p>
    <p>Feature extraction Fine-tuningAdaptersAdditional parameters</p>
    <p>All None</p>
    <p>Feature extraction Fine-tuningAdaptersParameter reuse</p>
  </div>
  <div class="page">
    <p>Feature extraction Fine-tuningAdapters Training time</p>
    <p>Slow Fast</p>
  </div>
  <div class="page">
    <p>Rule of thumb: If task source and target tasks are dissimilar*, use feature extraction (Peters et al., 2019)</p>
    <p>Otherwise, feature extraction and fine-tuning often perform similar  Fine-tuning BERT on textual similarity tasks works significantly better  Adapters achieve performance competitive with fine-tuning  Anecdotally, Transformers are easier to fine-tune (less sensitive to</p>
    <p>hyper-parameters) than LSTMs</p>
    <p>*dissimilar: certain capabilities (e.g. modelling inter-sentence relations) are beneficial for target task, but pretrained model lacks them (see more later)</p>
  </div>
  <div class="page">
    <p>Image credit: Naveen 169</p>
    <p>A. From fine-tuning a single model on a single adaptation task. The Basic: fine-tuning the model with a simple classification objective</p>
    <p>B.  to gathering signal from other datasets and related tasks  Fine-tuning with Weak Supervision, Multi-tasking and Sequential Adaptation</p>
    <p>C.  to ensembling models Combining the predictions of several fine-tuned models</p>
  </div>
  <div class="page">
    <p>A. Extract a single fixed-length vector from the model: hidden state of first/last token or mean/max of hidden-states</p>
    <p>B. Project to the classification space with an additional classifier</p>
    <p>C. Train with a classification objective</p>
  </div>
  <div class="page">
    <p>Intermediate fine-tuning on related datasets and tasks</p>
    <p>B. Multi-task fine-tuning with related tasks Such as NLI tasks in GLUE</p>
    <p>C. Dataset Slicing When the model consistently underperforms on particular slices of the data</p>
    <p>D. Semi-supervised learning Use unlabelled data to improve model consistency</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Helps particularly for tasks with limited data and similar tasks (Phang et al., 2018)</p>
    <p>Improves sample complexity on target task (Yogatama et al., 2019)</p>
  </div>
  <div class="page">
    <p>For each optimization step, sample a task and a batch for training.</p>
    <p>Train via multi-task learning for a couple of epochs.</p>
  </div>
  <div class="page">
    <p>For each optimization step, sample a task and a batch for training.</p>
    <p>Train via multi-task learning for a couple of epochs.</p>
    <p>Fine-tune on the target task only for a few epochs at the end.</p>
  </div>
  <div class="page">
    <p>Language modelling is a related task!  Fine-tuning the LM helps adapting the</p>
    <p>pretrained parameters to the target dataset.</p>
    <p>Helps even without pretraining (Rei et al., ACL 2017)</p>
    <p>Can optionally anneal ratio (Chronopoulou et al., NAACL 2019)</p>
    <p>Used as a separate step in ULMFiT</p>
  </div>
  <div class="page">
    <p>Analyze errors of the model  Use heuristics to automatically identify</p>
    <p>challenging subsets of the training data</p>
    <p>Train auxiliary heads jointly with main head</p>
    <p>See also Massive Multi-task Learning with Snorkel MeTaL</p>
  </div>
  <div class="page">
    <p>Main idea: Minimize distance between predictions on original input and perturbed input</p>
  </div>
  <div class="page">
    <p>Perturbation can be noise, masking (Clark et al., EMNLP 2018), data augmentation, e.g. back-translation (Xie et al., 2019)</p>
  </div>
  <div class="page">
    <p>Ensembling models Combining the predictions of models fine-tuned with various hyper-parameters</p>
    <p>Knowledge distillation Distill an ensemble of fine-tuned models in a single smaller model</p>
  </div>
  <div class="page">
    <p>Model fine-tuned...</p>
    <p>on different tasks  on different dataset-splits  with different parameters</p>
    <p>(dropout, initializations)  from variant of pre-trained</p>
    <p>models (e.g. cased/uncased)</p>
    <p>Combining the predictions of models fine-tuned with various hyper-parameters.</p>
  </div>
  <div class="page">
    <p>knowledge distillation: train a student model on soft targets produced by the teacher (the ensemble)</p>
    <p>Relative probabilities of the teacher labels contain information about how the teacher generalizes</p>
    <p>Distilling ensembles of large models back in a single model</p>
  </div>
  <div class="page">
    <p>Hands-on #6: Using multi-task learning</p>
    <p>Image credit: Chanaky 183</p>
  </div>
  <div class="page">
    <p>Hands-on: Multi-task learning Multitasking with a classification loss + language modeling loss.</p>
    <p>Create two heads:  language modeling head  classification head</p>
    <p>Total loss is a weighted sum of  language modeling loss and  classification loss</p>
  </div>
  <div class="page">
    <p>Hands-on: Multi-task learning</p>
    <p>Multi-tasking helped us improve over single-task full-model fine-tuning!</p>
    <p>We use a coefficient of 1.0 for the classification loss and 0.5 for the language modeling loss and fine-tune a little longer (6 epochs instead of 3 epochs, the validation loss was still decreasing).</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page">
    <p>Image credit: Fahmi 187</p>
  </div>
  <div class="page">
    <p>A. What are the various applications of transfer learning in NLP Document/sequence classification, Token-level classification, Structured prediction and Language generation</p>
    <p>B. How to leverage several frameworks &amp; libraries for practical applications Tensorflow, PyTorch, Keras and third-party libraries like fast.ai, HuggingFace...</p>
  </div>
  <div class="page">
    <p>Practical considerationsFrameworks &amp; libraries: practical considerations  Pretraining large-scale models is costly</p>
    <p>Use open-source models Share your pretrained models</p>
    <p>Energy and Policy Considerations for Deep Learning in NLP - Strubell, Ganesh, McCallum - ACL 2019</p>
    <p>Sharing/accessing pretrained models  Hubs: Tensorflow Hub, PyTorch Hub  Author released checkpoints: ex BERT, GPT...  Third-party libraries: AllenNLP, fast.ai, HuggingFace</p>
    <p>Design considerations  Hubs/libraries:</p>
    <p>Simple to use but can be difficult to modify model internal architecture  Author released checkpoints:</p>
    <p>More difficult to use but you have full control over the model internals</p>
  </div>
  <div class="page">
    <p>A. Sequence and document level classification Hands-on: Document level classification (fast.ai)</p>
    <p>B. Token level classification Hands-on: Question answering (Google BERT &amp; Tensorflow/TF Hub)</p>
    <p>C. Language generation Hands-on: Dialog Generation (OpenAI GPT &amp; HuggingFace/PyTorch Hub)</p>
    <p>Icons credits: David, Susannanova, Flatart, ProSymbols 190</p>
  </div>
  <div class="page">
    <p>Target task: IMDB: a binary sentiment classification dataset containing 25k highly polar movie reviews for training, 25k for testing and additional unlabeled data. http://ai.stanford.edu/~amaas/data/sentiment/</p>
    <p>Fast.ai has in particular:  a pre-trained English model available for download  a standardized data block API  easy access to standard datasets like IMDB</p>
    <p>Fast.ai is based on PyTorch</p>
  </div>
  <div class="page">
    <p>DataBunch for the language model and the classifier</p>
    <p>Load IMDB dataset &amp; inspect it.</p>
    <p>Load an AWD-LSTM (Merity et al., 2017) pretrained on WikiText-103 &amp; fine-tune it on IMDB using the language modeling loss.</p>
    <p>Fast.ai then comprises all the high level modules needed to quickly setup a transfer learning experiment.</p>
    <p>The library is designed for speed of experimentation, e.g. by importing all necessary modules at once in interactive computing environments, like:</p>
  </div>
  <div class="page">
    <p>Now we fine-tune in two steps:</p>
    <p>Once we have a fine-tune language model (AWD-LSTM), we can create a text classifier by adding a classification head with:  A layer to concatenate the final outputs of the RNN with the maximum and average of all the intermediate outputs (along the sequence length)  Two blocks of nn.BatchNorm1d  nn.Dropout  nn.Linear  nn.ReLU with a hidden dimension of 50.</p>
    <p>Colab: http://tiny.cc/NAACLTransferFastAiColab</p>
  </div>
  <div class="page">
    <p>Target task: SQuAD: a question answering dataset. https://rajpurkar.github.io/SQuAD-explorer/</p>
    <p>In this example we will directly use a Tensorflow checkpoint  Example: https://github.com/google-research/bert  We use the usual Tensorflow workflow: create model graph comprising</p>
    <p>the core model and the added/modified elements  Take care of variable assignments when loading the checkpoint</p>
  </div>
  <div class="page">
    <p>Replace the pre-training head (language modeling) with a classification head: a linear projection layer to estimate 2 probabilities for each token:  being the start of an answer  being the end of an answer.</p>
    <p>Keep our core model unchanged.</p>
  </div>
  <div class="page">
    <p>Load our pretrained checkpoint</p>
    <p>To load our checkpoint, we just need to setup an assignement_map from the variables of the checkpoint to the model variable, keeping only the variables in the model.</p>
    <p>And we can use tf.train.init_from_ckeckpoint</p>
  </div>
  <div class="page">
    <p>TensorFlow Hub is a library for sharing machine learning models as self-contained pieces of TensorFlow graph with their weights and assets.</p>
    <p>Working directly with TensorFlow requires to have access toand include in your code the full code of the pretrained model.</p>
    <p>Modules are automatically downloaded and cached when instantiated.</p>
    <p>Each time a module m is called e.g. y = m(x), it adds operations to the current TensorFlow graph to compute y from x.</p>
  </div>
  <div class="page">
    <p>Tensorflow Hub can also used with Keras exactly how we saw in the BERT example</p>
    <p>The main limitations of Hubs are:  No access to the source code of the model (black-box)  Not possible to modify the internals of the model (e.g. to add Adapters)</p>
  </div>
  <div class="page">
    <p>Target task: ConvAI2  The 2nd Conversational Intelligence Challenge for training and evaluating models for non-goal-oriented dialogue systems, i.e. chit-chat http://convai.io</p>
    <p>HuggingFace library of pretrained models  a repository of large scale pre-trained models with BERT, GPT, GPT-2, Transformer-XL  provide an easy way to download, instantiate and train pre-trained models in PyTorch</p>
    <p>HuggingFaces models are now also accessible using PyTorch Hub</p>
  </div>
  <div class="page">
    <p>A dialog generation task:</p>
    <p>Language generation tasks are close to the language modeling pre-training objective, but:  Language modeling pre-training involves a single input: a sequence of words.  In a dialog setting: several type of contexts are provided to generate an output sequence:</p>
    <p>knowledge base: persona sentences,  history of the dialog: at least the last utterance from the user,  tokens of the output sequence that have already been generated.</p>
    <p>How should we adapt the model?</p>
  </div>
  <div class="page">
    <p>Golovanov, Kurbanov, Nikolenko, Truskovskyi, Tselousov and Wolf, ACL 2019</p>
    <p>Several options:  Duplicate the model to initialize an encoder-decoder structure</p>
    <p>e.g. Lample &amp; Conneau, 2019  Use a single model with concatenated inputs</p>
    <p>see e.g. Wolf et al., 2019, Khandelwal et al. 2019</p>
    <p>Concatenate the various context separated by delimiters and add position and segment embeddings</p>
  </div>
  <div class="page">
    <p>Now most of the work is about preparing the inputs for the model.</p>
    <p>Then train our model using the pretraining language modeling objective.</p>
    <p>And add a few new tokens to the vocabulary</p>
    <p>We organize the contexts in segments</p>
    <p>Add delimiter at the extremities of the segments</p>
    <p>And build our word, position and segment inputs for the model.</p>
  </div>
  <div class="page">
    <p>In our case, to use torch.hub instead of pytorch-pretrained-bert, we can simply call torch.hub.load with the path to pytorch-pretrained-bert GitHub repository:</p>
    <p>PyTorch Hub will fetch the model from the master branch on GitHub. This means that you dont need to package your model (pip) &amp; users will always access the most recent version (master).</p>
  </div>
  <div class="page">
    <p>Agenda</p>
    <p>[2] Pretraining [4] Adaptation</p>
    <p>[6] Open Problems</p>
    <p>[5] Downstream</p>
    <p>[3] Whats in a representation?</p>
    <p>[1] Introduction</p>
  </div>
  <div class="page">
    <p>Image credit: Yazmin Alanis 205</p>
  </div>
  <div class="page">
    <p>A. Shortcomings of pretrained language models</p>
    <p>B. Pretraining tasks</p>
    <p>C. Tasks and task similarity</p>
    <p>D. Continual learning and meta-learning</p>
    <p>E. Bias</p>
    <p>Image credit: Yazmin Alanis 206</p>
  </div>
  <div class="page">
    <p>Shortcomings of pretrained language models  Recap: LM can be seen as a general pretraining task; with enough data,</p>
    <p>compute, and capacity a LM can learn a lot.  In practice, many things that are less represented in text are harder to learn  Pretrained language models are bad at</p>
    <p>fine-grained linguistic tasks (Liu et al., NAACL 2019)  common sense (when you actually make it difficult; Zellers et al., ACL 2019); natural language</p>
    <p>generation (maintaining long-term dependencies, relations, coherence, etc.)  tend to overfit to surface form information when fine-tuned; rapid surface learners  ...</p>
  </div>
  <div class="page">
    <p>Shortcomings of pretrained language models</p>
    <p>Large, pretrained language models can be difficult to optimize.</p>
    <p>Fine-tuning is often unstable and has a high variance, particularly if the target datasets are very small</p>
    <p>Devlin et al. (NAACL 2019) note that large (24-layer) version of BERT is particularly prone to degenerate performance; multiple random restarts are sometimes necessary as also investigated in detail in (Phang et al., 2018)</p>
  </div>
  <div class="page">
    <p>Shortcomings of pretrained language models Current pretrained language models are very large.</p>
    <p>Do we really need all these parameters?  Recent work shows that only a few of the attention heads in BERT are</p>
    <p>required (Voita et al., ACL 2019).  More work needed to understand model parameters.  Pruning and distillation are two ways to deal with this.  See also: the lottery ticket hypothesis (Frankle et al., ICLR 2019).</p>
  </div>
  <div class="page">
    <p>Pretraining tasks Shortcomings of the language modeling objective:</p>
    <p>Not appropriate for all models  If we condition on more inputs, need to pretrain those parts  E.g. the decoder in sequence-to-sequence learning (Song et al., ICML 2019)</p>
    <p>Left-to-right bias not always be best  Objectives that take into account more context (such as masking) seem useful (less</p>
    <p>sample-efficient)  Possible to combine different LM variants (Dong et al., 2019)</p>
    <p>Weak signal for semantics and long-term context vs. strong signal for syntax and short-term word co-occurrences  Need incentives that promote encoding what we care about, e.g. semantics</p>
  </div>
  <div class="page">
    <p>Pretraining tasks More diverse self-supervised objectives</p>
    <p>Taking inspiration from computer vision</p>
    <p>Sampling a patch and a neighbour and predicting their spatial configuration (Doersch et al., ICCV 2015)</p>
    <p>Image colorization (Zhang et al., ECCV 2016)</p>
    <p>Self-supervision in language mostly based on word co-occurrence (Ando and Zhang, 2005)</p>
    <p>Supervision on different levels of meaning  Discourse, document, sentence, etc.  Using other signals, e.g. meta-data</p>
    <p>Emphasizing different qualities of language</p>
  </div>
  <div class="page">
    <p>Pretraining tasks Specialized pretraining tasks that teach what our model is missing</p>
    <p>Develop specialized pretraining tasks that explicitly learn such relationships  Word-pair relations that capture background knowledge (Joshi et al., NAACL 2019)  Span-level representations (Swayamdipta et al., EMNLP 2018)  Different pretrained word embeddings are helpful (Kiela et al., EMNLP 2018)</p>
    <p>Other pretraining tasks could explicitly learn reasoning or understanding  Arithmetic, temporal, causal, etc.; discourse, narrative, conversation, etc.</p>
    <p>Pretrained representations could be connected in a sparse and modular way  Based on linguistic substructures (Andreas et al., NAACL 2016) or experts (Shazeer et al., ICLR</p>
  </div>
  <div class="page">
    <p>Pretraining tasks Need for grounded representations</p>
    <p>Limits of distributional hypothesisdifficult to learn certain types of information from raw text  Human reporting bias: not stating the obvious (Gordon and Van Durme, AKBC 2013)  Common sense isnt written down  Facts about named entities  No grounding to other modalities</p>
    <p>Possible solutions:  Incorporate other structured knowledge (e.g. knowledge bases like ERNIE, Zhang et al 2019)  Multimodal learning (e.g. with visual representations like VideoBERT, Sun et al. 2019)  Interactive/human-in-the-loop approaches (e.g. dialog, Hancock et al. 2018)</p>
  </div>
  <div class="page">
    <p>Tasks and task similarity Many tasks can be expressed as variants of language modeling</p>
    <p>Language itself can directly be used to specify tasks, inputs, and outputs, e.g. by framing as QA (McCann et al., 2018)</p>
    <p>Dialog-based learning without supervision by forward prediction (Weston, NIPS 2016)</p>
    <p>NLP tasks formulated as cloze prediction objective (Children Book Test, LAMBADA, Winograd, ...)</p>
    <p>Triggering task behaviors via prompts e.g. TL; DR:, translation prompt (Radford, Wu et al. 2019); enables zero-shot adaptation</p>
    <p>Questioning the notion of a task in NLP</p>
  </div>
  <div class="page">
    <p>Tasks and task similarity  Intuitive similarity of pretraining and target tasks (NLI, classification)</p>
    <p>correlates with better downstream performance  Do not have a clear understanding of when and how two tasks are similar and</p>
    <p>relate to each other  One way to gain more understanding: Large-scale empirical studies of</p>
    <p>transfer such as Taskonomy (Zamir et al., CVPR 2018)  Should be helpful for designing better and specialized pretraining tasks</p>
  </div>
  <div class="page">
    <p>Continual and meta-learning  Current transfer learning performs adaptation once.  Ultimately, wed like to have models that continue to retain and accumulate</p>
    <p>knowledge across many tasks (Yogatama et al., 2019).  No distinction between pretraining and adaptation; just one stream of tasks.  Main challenge towards this: Catastrophic forgetting.  Different approaches from the literature:</p>
    <p>Memory, regularization, task-specific weights, etc.</p>
  </div>
  <div class="page">
    <p>Continual and meta-learning  Objective of transfer learning: Learn a representation that is general and</p>
    <p>useful for many tasks.  Objective does not incentivize ease of adaptation (often unstable); does not</p>
    <p>learn how to adapt it.  Meta-learning combined with transfer learning could make this more</p>
    <p>feasible.  However, most existing approaches are restricted to the few-shot setting and</p>
    <p>only learn a few steps of adaptation.</p>
  </div>
  <div class="page">
    <p>Bias  Bias has been shown to be pervasive in word embeddings and neural models</p>
    <p>in general  Large pretrained models necessarily have their own sets of biases  There is a blurry boundary between common-sense and bias  We need ways to remove such biases during adaptation  A small fine-tuned model should be harder to misuse</p>
  </div>
  <div class="page">
    <p>Conclusion  Themes: words-in-context, LM pretraining, deep models</p>
    <p>Pretraining gives better sample-efficiency, can be scaled up</p>
    <p>Predictive of certain featuresdepends how you look at it</p>
    <p>Performance trade-offs, from top-to-bottom</p>
    <p>Transfer learning is simple to implement, practically useful</p>
    <p>Still many shortcomings and open problems</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Slides: http://tiny.cc/NAACLTransfer  Colab: http://tiny.cc/NAACLTransferColab  Code: http://tiny.cc/NAACLTransferCode</p>
    <p>Twitter: #NAACLTransfer</p>
    <p>Whova: Questions for the tutorial on Transfer Learning in NLP topic</p>
  </div>
  <div class="page">
    <p>Extra slides</p>
  </div>
  <div class="page">
    <p>https://paperswithcode.com/sota/question-answering-on-squad20</p>
    <p>BERT + X</p>
    <p>Why transfer learning in NLP? (Empirically)</p>
  </div>
  <div class="page">
    <p>*General Language Understanding Evaluation (GLUE; Wang et al., 2019): includes 11 diverse NLP tasks</p>
    <p>GLUE* performance over time</p>
  </div>
  <div class="page">
    <p>Pretrained Language Models: More Parameters</p>
  </div>
  <div class="page">
    <p>More word vectors</p>
    <p>GLoVe: very large scale (840B tokens), co-occurrence based. Learns linear relationships (SOTA word analogy) (Pennington et al., 2014)</p>
    <p>fastText: incorporates subword information (Bojanowski et al., 2017)</p>
    <p>fastText</p>
    <p>skipgram 225</p>
  </div>
  <div class="page">
    <p>Semi-supervised Sequence Modeling with Cross-View Training</p>
    <p>(Clark et al. EMNLP 2018) SOTA sequence modeling results</p>
  </div>
  <div class="page">
    <p>Pretrain bidirectional character level model, extract embeddings from first/last character</p>
    <p>SOTA CoNLL 2003 NER results</p>
    <p>Contextual String Embeddings</p>
    <p>(Akbik et al., COLING 2018) (see also Akbik et al., NAACL 2019) 227</p>
  </div>
  <div class="page">
    <p>Cloze-driven Pretraining of Self-attention Networks</p>
    <p>Pretraining Fine-tuning</p>
    <p>SOTA NER and PTB constituency parsing, ~3.3% less than BERT-large for GLUE</p>
    <p>Baevski et al. (2019)</p>
  </div>
  <div class="page">
    <p>UniLM - Dong et al., 2019</p>
    <p>Model is jointly pretrained on three variants of LM</p>
    <p>(bidirectional, left-to-right, seq-to-seq)</p>
    <p>SOTA on three natural language generation tasks</p>
  </div>
  <div class="page">
    <p>Masked Sequence to Sequence Pretraining (MASS)</p>
    <p>(Song et al., ICML 2019)</p>
    <p>Pretrain encoder-decoder</p>
  </div>
  <div class="page">
    <p>Probing tasks for sentential features:</p>
    <p>Bag-of-Vectors is surprisingly good at capturing sentence-level properties, thanks to redundancies in natural linguistic input.</p>
    <p>BiLSTM-based models are better than CNN-based models at capturing interesting linguistic knowledge, with same objective</p>
    <p>Objective matters - training on NLI is bad. Most tasks are structured so a seq 2 tree objective works best.</p>
    <p>Supervised objectives for sentence embeddings do better than unsupervised, like SkipThought (Kiros et al.)</p>
    <p>What matters: Pretraining Objective, Encoder</p>
  </div>
  <div class="page">
    <p>From lower to higher layers, information goes from general to task-specific.</p>
    <p>Image credit: Distill</p>
    <p>An inspiration from Computer Vision</p>
  </div>
  <div class="page">
    <p>Other analyses</p>
    <p>Textual omission and multi-modal: Kadar et al. , 16</p>
    <p>Adversarial Approaches  Adversary: input which differs from original just enough to</p>
    <p>change the desired prediction  SQuAD: Jia &amp; Liang, 2017  NLI: Glockner et al., 2018; Minervini &amp; Riedel, 2018  Machine Translation: Belinkov &amp; Bisk, 2018</p>
    <p>Requires identification (manual or automatic) of inputs to modify.</p>
    <p>Other methods for analysis</p>
    <p>Adversarial methods</p>
  </div>
  <div class="page">
    <p>Analysis: Inputs and Outputs</p>
    <p>What to analyze?</p>
    <p>Embeddings  Word types and tokens  Sentence  Document</p>
    <p>Network Activations  RNNs  CNNs  Feed-forward nets</p>
    <p>Layers  Pretraining Objectives</p>
    <p>What to look for?</p>
    <p>Surface-level features  Lexical features</p>
    <p>E.g. POS tags</p>
    <p>Morphology  Syntactic Structure</p>
    <p>Word-level  Sentence-level</p>
    <p>Semantic Structure  E.g. Roles, Coreference</p>
    <p>Belinkov et al. (2019)More details in Table 1. 234</p>
  </div>
  <div class="page">
    <p>Analysis: Methods</p>
    <p>Visualization:  2-D plots  Attention mechanisms  Network activations</p>
    <p>Model Alterations</p>
    <p>Visualization</p>
    <p>Model Alterations:  Network Erasure  Perturbations</p>
    <p>Model Probes:  Surface-level features  Syntactic features  Semantic features</p>
    <p>Model Probes</p>
    <p>* Not hard and fast categories</p>
  </div>
  <div class="page">
    <p>Adversarial Approaches</p>
    <p>How does this say whats in a representation?  Roundabout: whats wrong with a representation...</p>
    <p>Credits: Jia &amp; Liang (2017) and Percy Liang. AI Frontiers. 2018</p>
    <p>Analysis / Evaluation : Adversarial Methods</p>
  </div>
  <div class="page">
    <p>Liu et al., NAACL 2019</p>
    <p>Probes are simple linear / neural layers</p>
  </div>
  <div class="page">
    <p>Interpretability is difficult Lipton et al., 2016  Many variables make synthesis challenging  Choice of model architecture, pretraining</p>
    <p>objective determines informativeness of representations</p>
    <p>What is still left unanswered?</p>
    <p>Conneau et al., 2018</p>
    <p>Transferability to downstream tasks</p>
    <p>Interpretability is important, but not enough on its own.</p>
    <p>Interpretability + transferability to downstream tasks is key - thats next!</p>
  </div>
</Presentation>
