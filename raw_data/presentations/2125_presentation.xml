<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Non-uniformly Communicating Non-contiguous Data:</p>
    <p>A Case Study with PETSc and MPI</p>
    <p>P. Balaji, D. Buntinas, S. Balay, B. Smith, R. Thakur and W. Gropp</p>
    <p>Mathematics and Computer Science Argonne National Laboratory</p>
  </div>
  <div class="page">
    <p>Numerical Libraries in HEC  Developing parallel applications is a complex task</p>
    <p>Discretizing physical equations to numerical forms</p>
    <p>Representing the domain of interest as data points</p>
    <p>Libraries allow developers to abstract low-level</p>
    <p>details</p>
    <p>E.g., Numerical Analysis, Communication, I/O</p>
    <p>Numerical libraries (e.g., PETSc, ScaLAPACK,</p>
    <p>PESSL)</p>
    <p>Parallel data layout and processing</p>
    <p>Tools for distributed data layout (matrix, vector)</p>
    <p>Tools for data processing (SLES, SNES)</p>
  </div>
  <div class="page">
    <p>Overview of PETSc  Portable, Extensible Toolkit for</p>
    <p>Scientific Computing</p>
    <p>Software tools for solving PDEs  Suite of routines to create</p>
    <p>vectors, matrices and distributed arrays</p>
    <p>Sequential/parallel data layout</p>
    <p>Linear and nonlinear numerical solvers</p>
    <p>Widely used in Nanosimulations, Molecular dynamics, etc.</p>
    <p>Uses MPI for communication</p>
    <p>BLAS LAPACK MPI</p>
    <p>Matrices Vectors Index Sets</p>
    <p>KSP (Krylov subspace Methods)</p>
    <p>PC (Preconditioners)</p>
    <p>Draw</p>
    <p>SNES (Nonlinear Equation Solvers) SLES</p>
    <p>(Linear Equation Solvers)</p>
    <p>TS (Time Stepping)</p>
    <p>PDE Solvers</p>
    <p>Application Codes Level of</p>
    <p>Abstraction</p>
  </div>
  <div class="page">
    <p>Handling Parallel Data Layouts in PETSc  Grid layout exposed to the application</p>
    <p>Structured or Unstructured (1D, 2D, 3D)</p>
    <p>Internally managed as a single vector of data</p>
    <p>elements</p>
    <p>Representation often suited to optimize its operations</p>
    <p>Impact on communication:</p>
    <p>Data representation and communication pattern</p>
    <p>might not be ideal for MPI communication operations</p>
    <p>Non-uniformity and Non-contiguity in communication</p>
    <p>are the primary culprits</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Impact of PETSc Data Layout and Processing on</p>
    <p>MPI</p>
    <p>MPI Enhancements and Optimizations</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Local Data Point</p>
    <p>Data Layout and Processing in PETSc</p>
    <p>Grid layouts: data is divided among processes  Ghost data points shared</p>
    <p>Non-contiguous Data Communication  2nd dimension of the grid</p>
    <p>Non-uniform communication  Structure of the grid  Stencil type used  Sides larger than corners</p>
    <p>Process Boundary</p>
    <p>Ghost Data Point</p>
    <p>Proc 1Proc 0</p>
    <p>Box-type stencil</p>
    <p>Proc 1Proc 0</p>
    <p>Star-type stencil</p>
  </div>
  <div class="page">
    <p>MPI Derived Datatypes  Application describes noncontiguous data layout to</p>
    <p>MPI  Data is either packed to contiguous buffers and</p>
    <p>pipelined (sparse layouts) or sent individually (dense layouts)</p>
    <p>Good for simple algorithms, but very restrictive  Lookup upcoming content to predecide algorithm to</p>
    <p>use  Multiple parses on the datatype loses context!</p>
    <p>Non-contiguous Communication in MPI</p>
    <p>Non-contiguous Data layout</p>
    <p>Save Context Send DataSave Context</p>
    <p>Packing Buffer</p>
  </div>
  <div class="page">
    <p>Issues with Lost Datatype Context  Rollback of context not possible</p>
    <p>Datatypes could be recursive</p>
    <p>Duplication of context not possible  Context information might be large  When datatype elements are small, context could be</p>
    <p>larger than the datatype itself</p>
    <p>Search of context possible, but very expensive  Quadratically increasing search time with increasing</p>
    <p>datatype size  Currently used mechanism!</p>
  </div>
  <div class="page">
    <p>Non-uniform Collective Communication</p>
    <p>Non-uniform communication</p>
    <p>algorithms are optimized for</p>
    <p>uniform communication</p>
    <p>Case Studies</p>
    <p>Allgatherv uses a ring</p>
    <p>algorithm</p>
    <p>Causes idleness if data</p>
    <p>volumes are very different</p>
    <p>Alltoallw sends data to nodes</p>
    <p>in round-robin manner</p>
    <p>MPI processing is sequential</p>
    <p>Large Message</p>
    <p>Small Message</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Impact of PETSc Data Layout and Processing on MPI</p>
    <p>MPI Enhancements and Optimizations</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Dual-context Approach for Non-contiguous Communication  Previous approaches are in-efficient in complex</p>
    <p>designs  E.g., if a look-ahead is performed to understand the</p>
    <p>structure of the upcoming data, the saved context is lost</p>
    <p>Dual-context approach retains the data context  Look-aheads are performed using a separate context  Completely eliminates the search time</p>
    <p>Non-contiguous Data layout</p>
    <p>Save Context</p>
    <p>Send Data</p>
    <p>Save Context Look-ahead</p>
    <p>Packing Buffer</p>
  </div>
  <div class="page">
    <p>Non-Uniform Communication: AllGatherv</p>
    <p>Single point of distribution is the primary bottleneck</p>
    <p>Identify if a small fraction of messages are very large  Floyd and Rivest Algorithm  Linear time detection of</p>
    <p>outliers</p>
    <p>Binomial Algorithms  Recursive doubling or</p>
    <p>Dissemination  Logarithmic time</p>
    <p>Large Message</p>
    <p>Small Message</p>
  </div>
  <div class="page">
    <p>Non-uniform Communication: Alltoallw  Distributing messages to be sent out as bins</p>
    <p>(based on message size) allows differential treatment to nodes</p>
    <p>Send out small messages first  Nodes waiting for small messages have to wait lesser  Ratio of increase in time for nodes waiting for larger</p>
    <p>messages is much smaller  No skew for zero-byte data with lesser</p>
    <p>synchronization</p>
    <p>Most helpful for non-contiguous messages  MPI processing (e.g., packing) is sequential for non</p>
    <p>contiguous messages</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Impact of PETSc Data Layout and Processing on MPI</p>
    <p>MPI Enhancements and Optimizations</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Testbed  64-node Cluster</p>
    <p>32 nodes with dual Intel EM64T 3.6GHz processors  2MB L2 Cache, 2GB DDR2 400MHz SDRAM  Intel E7520 (Lindenhurst) Chipset</p>
    <p>32 nodes with dual Opteron 2.8GHz processors  1MB L2 Cache, 4GB DDR 400MHz SDRAM  NVidia 2200/2050 Chipset</p>
    <p>RedHat AS4 with kernel.org kernel 2.6.16</p>
    <p>InfiniBand DDR (16Gbps) Network:  MT25208 adapters connected through a 144-port</p>
    <p>switch</p>
    <p>MVAPICH2-0.9.6 MPI implementation</p>
  </div>
  <div class="page">
    <p>Non-uniform Communication Evaluation</p>
    <p>Search time can dominate performance if the working context is lost!</p>
  </div>
  <div class="page">
    <p>AllGatherv Evaluation</p>
  </div>
  <div class="page">
    <p>Alltoallw Evaluation</p>
    <p>Our algorithm reduces the skew introduced due to the Alltoallw operations by sending out smaller messages first and allowing the corresponding applications to progress</p>
  </div>
  <div class="page">
    <p>PETSc Vector Scatter</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Impact of PETSc Data Layout and Processing on MPI</p>
    <p>MPI Enhancements and Optimizations</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks and Future Work</p>
  </div>
  <div class="page">
    <p>Concluding Remarks and Future Work  Non-uniform and Non-contiguous communication is</p>
    <p>inherent in several libraries and applications</p>
    <p>Current algorithms deal with non-uniform communication in a same way as uniform communication</p>
    <p>Demonstrated that more sophisticated algorithms can give close to 10x improvements in performance</p>
    <p>Designs are a part of MPICH2-1.0.5 and 1.0.6  To be picked up by MPICH2 derivatives in later</p>
    <p>releases</p>
    <p>Future Work:  Skew tolerance in non-uniform communication</p>
    <p>Other libraries and applications</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Group Web-page: http://www.mcs.anl.gov/radix</p>
    <p>Home-page: http://www.mcs.anl.gov/~balaji</p>
    <p>Email: balaji@mcs.anl.gov</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Noncontiguous Communication in PETSc</p>
    <p>Copy Buffer</p>
    <p>vector (count = 8, stride = 8)</p>
    <p>contiguous (count = 3)</p>
    <p>double | double | double double | double | double double | double | double</p>
    <p>contiguous (count = 3) contiguous (count = 3)</p>
    <p>Data might not always be</p>
    <p>contiguously laid out in</p>
    <p>memory</p>
    <p>E.g., Second dimension of a</p>
    <p>structured grid</p>
    <p>Communication is performed</p>
    <p>by packing data</p>
    <p>Pipelining copy and</p>
    <p>communication is important</p>
    <p>for performance</p>
  </div>
  <div class="page">
    <p>Hand-tuning vs. Automated optimization  Nonuniformity and noncontiguity in data</p>
    <p>communication is inherent in several applications  Communicating unequal amounts of data to the</p>
    <p>different peer processes  Communication data from noncontiguous memory</p>
    <p>locations</p>
    <p>Previous research has primarily focused on uniform and contiguous data communication</p>
    <p>Accordingly applications and libraries tried handtuning attempts to convert communication formats  Manually packing noncontiguous data  Re-implementing collective operations in the</p>
    <p>application</p>
  </div>
  <div class="page">
    <p>Non-contiguous Communication in MPI  MPI Derived Datatypes</p>
    <p>Common approach for non-contiguous communication</p>
    <p>Application describes noncontiguous data layout to MPI</p>
    <p>Data is either packed into contiguous memory (sparse layouts) or sent as independent segments (dense layouts)</p>
    <p>Pipelining of packing and communication improves performance, but requires context information!</p>
    <p>Non-contiguous Data layout</p>
    <p>Save Context</p>
    <p>Send Data</p>
    <p>Save Context</p>
    <p>Packing Buffer</p>
  </div>
  <div class="page">
    <p>Issues with Non-contiguous Communication  Current approach is simple and works as long as</p>
    <p>there is a single parse on the noncontiguous data</p>
    <p>More intelligent algorithms might suffer:  E.g., lookup upcoming datatype content to predecide</p>
    <p>algorithm to use  Multiple parses on the datatype lose the context !  Searching for the lost context every time requires</p>
    <p>quadratically increasing time with datatype size</p>
    <p>PETSc non-contiguous communication suffers with such high search times</p>
  </div>
  <div class="page">
    <p>MPI-level Evaluation</p>
  </div>
  <div class="page">
    <p>Experimental Results  MPI-level Micro-benchmarks</p>
    <p>Non-contiguous data communication time</p>
    <p>Non-uniform collective communication</p>
    <p>Allgatherv Operation</p>
    <p>Alltoallw Operation</p>
    <p>PETSc Vector Scatter Benchmark</p>
    <p>Performs communication only</p>
    <p>3-D Laplacian Multigrid Solver Application</p>
    <p>Partial differential equation solver</p>
    <p>Utilizes PETSc numerical solver operations</p>
  </div>
</Presentation>
