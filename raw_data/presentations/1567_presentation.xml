<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SMaRT: An Approach to Shingled Magnetic</p>
    <p>Recording Translation</p>
    <p>Weiping He and David H.C. Du</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>SMR Backgrounds</p>
    <p>Characteristics</p>
    <p>Types of SMR Drives</p>
    <p>Challenges</p>
    <p>Write Amplification</p>
    <p>GC Overhead</p>
    <p>Motivations</p>
    <p>Current Design of SMR drives</p>
    <p>Advantages of Track-Based Mapping</p>
    <p>Proposed Approach</p>
    <p>SMaRT</p>
    <p>Evaluations</p>
  </div>
  <div class="page">
    <p>SMR Background</p>
    <p>Traditional HDDs (perpendicular magnetic recording) are</p>
    <p>reaching areal density limit</p>
    <p>Shingled magnetic recording (SMR) is a new promising</p>
    <p>technology by overlapping tracks</p>
    <p>Non shingled Shingled</p>
  </div>
  <div class="page">
    <p>Write head width is larger than read head width</p>
    <p>Write/update a block in place may destroy the valid data on the subsequent</p>
    <p>tracks if any</p>
    <p>Sequential write is preferred</p>
    <p>a b c</p>
    <p>Shingling</p>
    <p>direction</p>
    <p>a</p>
    <p>Read head width</p>
    <p>Write head width</p>
    <p>b c</p>
    <p>Simplified</p>
    <p>diagram</p>
    <p>SMR Characteristics</p>
  </div>
  <div class="page">
    <p>Current Types of SMR Drives</p>
    <p>Device-Managed SMR (DM-SMR)</p>
    <p>The device handles address mapping</p>
    <p>Block I/O interface</p>
    <p>Drop-in replacement for HDDs.</p>
    <p>E.g., Seagate 8TB Archive [1]</p>
    <p>Host-Aware SMR (HA-SMR)  T10 and T13</p>
    <p>The host is preferred to follow I/O rules (e.g., writing data sequentially to the location of</p>
    <p>write-pointer in each zone).</p>
    <p>I/Os violating the rules will be processed in a DM-SMR way. i.e., go to persistent cache.</p>
    <p>Host-Managed SMR (HM-SMR)  T10 and T13</p>
    <p>The host has to strictly follow rules</p>
    <p>I/Os violating the rules will be rejected.</p>
    <p>E.g., WD/HGST 10TB UltraStar Ha10 [2]</p>
  </div>
  <div class="page">
    <p>DM-SMR HA-SMR HM-SMR</p>
    <p>Conventional</p>
    <p>zone Mandatory Optional Optional</p>
    <p>Persistent Cache Optional Optional Optional</p>
    <p>Seq. write pref.</p>
    <p>zone Not supported Mandatory Not supported</p>
    <p>Seq. write req.</p>
    <p>zone Not supported Not supported Mandatory</p>
    <p>Zone Configurations</p>
    <p>More Information on HA-SMR and HM-SMR can be referred to a presentation by Tim Feldmann</p>
    <p>- Host-Aware SMR (Tim Feldmann OpenZFS 14) [3]</p>
    <p>Current Types of SMR Drives</p>
  </div>
  <div class="page">
    <p>Basic Layout of SMR Drives</p>
    <p>Conventional Zones</p>
    <p>Miscellaneous usages: metadata, journal, etc.</p>
    <p>Shingled Zones</p>
    <p>DM-SMR: Present a consecutive logical space to host</p>
    <p>HM-SMR: sequential write required zones (fail violating I/Os)</p>
    <p>HA-SMR: Sequential write preferred zones (direct violating I/Os</p>
    <p>to cache, GC later)</p>
    <p>O D</p>
    <p>P e</p>
    <p>rs is</p>
    <p>te n t C</p>
    <p>a c h</p>
    <p>e</p>
    <p>ID</p>
    <p>.</p>
    <p>Shingled zones</p>
    <p>Write pointers</p>
    <p>C o</p>
    <p>n v e</p>
    <p>n tio</p>
    <p>n a</p>
    <p>l z o</p>
    <p>n e</p>
    <p>Violating I/O Non-violating I/O</p>
  </div>
  <div class="page">
    <p>Current Types of SMR Drives</p>
    <p>Device-Managed SMR (DM-SMR)</p>
    <p>The device handles address mapping</p>
    <p>Block I/O interface</p>
    <p>Drop-in replacement for HDDs.</p>
    <p>E.g., Seagate 8TB Archive [1]</p>
    <p>Host-Aware SMR (HA-SMR)  T10 and T13</p>
    <p>The host is preferred to follow I/O rules (writing data sequentially to the location of</p>
    <p>write-pointer in each zone).</p>
    <p>I/Os violating the rules will be processed in a DM-SMR way. i.e., go to persistent cache.</p>
    <p>Host-Managed SMR (HM-SMR)  T10 and T13</p>
    <p>The host has to strictly follow rules</p>
    <p>I/Os violating the rules will be rejected.</p>
    <p>E.g., WD/HGST 10TB UltraStar Ha10 [2]</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>Challenges of DM-SMRs:</p>
    <p>Write amplifications (one write becomes multiple writes)</p>
    <p>Garbage collections (persistent cache cleaning and zone cleaning)</p>
    <p>One of Seagates Solutions [4]:</p>
    <p>Persistent cache</p>
    <p>Static mapping for zones.</p>
    <p>Aggressive GCs</p>
    <p>Pros:</p>
    <p>Simple and clean</p>
    <p>Cons:</p>
    <p>Workload picky: Suitable for workloads with idle times.</p>
    <p>Data staging in persistent cache</p>
    <p>Persistent cache</p>
    <p>zones</p>
  </div>
  <div class="page">
    <p>Motivations</p>
    <p>Two inherent properties of SMRs</p>
    <p>Advantage of Track-based mapping</p>
    <p>An invalid track can be reused immediately without erase like operations in SSDs</p>
    <p>Block-based mapping will create huge mapping table and will introduce invalidated blocks</p>
    <p>problem (cannot be used right away)</p>
    <p>A track supports in-place update if its following track is free.</p>
    <p>Can we exploit these properties to  ?</p>
    <p>Reduce write amplification</p>
    <p>Reduce read fragmentations</p>
    <p>Improve overall I/O performance</p>
    <p>Remove or mitigate the use of persistent cache</p>
  </div>
  <div class="page">
    <p>Our Proposed Solution: SMaRT</p>
    <p>SMR Drive Layout Assumption  Conventional zone</p>
    <p>Many Shingled zones</p>
    <p>Two Function Modules Are Designed:</p>
    <p>A dynamic track-based mapping table</p>
    <p>It supports block-level address mapping</p>
    <p>Hybrid update strategy</p>
    <p>A space management module which handles</p>
    <p>free track allocation, AND</p>
    <p>Garbage collection</p>
  </div>
  <div class="page">
    <p>SMaRT Overall Architecture</p>
    <p>Host Software</p>
    <p>Block Interface</p>
    <p>Track-based</p>
    <p>Mapping Table</p>
    <p>Space Management</p>
    <p>Module</p>
    <p>Raw Drive</p>
    <p>CHS</p>
    <p>SMaRT</p>
    <p>(a) General Architecture (b) Drive Physical Layout</p>
    <p>(c) Track Usage In A Zone</p>
  </div>
  <div class="page">
    <p>SMaRT Space Management  1</p>
    <p>Space, Space Element and Hybrid Update</p>
    <p>Free space: [[4, 5], [7,8,9], [13], [18, 19], [22, 23]]</p>
    <p>Free space element: a group of consecutive free tracks.</p>
    <p>Tracks 4, 7, 8, 18, 22 are usable</p>
    <p>Bigger free space groups have more usable tracks.</p>
    <p>Used space: [[0, 1, 2, 3], [6], [10, 11, 12], [14, 15, 16, 17], [20, 21]]</p>
    <p>Used space element: a group of consecutive used tracks.</p>
    <p>Tracks 3, 6, 12, 17 and 21 support in-place update.</p>
    <p>Used Track Free Track</p>
  </div>
  <div class="page">
    <p>SMaRT Space Management  2</p>
    <p>Track Allocation</p>
    <p>Allocation pool is the largest free space element.</p>
    <p>The whole zone is an allocation pool for an empty zone.</p>
    <p>Write cursor is used to indicate the next available free track</p>
    <p>for data allocation.</p>
    <p>Shingling</p>
    <p>Direction</p>
    <p>Write Cursor</p>
    <p>Z o n</p>
    <p>e</p>
    <p>OD ID</p>
    <p>An</p>
    <p>empty</p>
    <p>zone</p>
  </div>
  <div class="page">
    <p>SMaRT Space Management  2</p>
    <p>Track Allocation</p>
    <p>Allocation pool is the largest free space element.</p>
    <p>The whole zone is an allocation pool for an empty zone.</p>
    <p>Write cursor is used to indicate the next available free track for data allocation.</p>
    <p>Shingling</p>
    <p>Direction</p>
    <p>Write Cursor</p>
    <p>Z o n</p>
    <p>e</p>
    <p>OD ID</p>
    <p>An aged</p>
    <p>zone</p>
  </div>
  <div class="page">
    <p>Track Allocation Example</p>
    <p>All writes (new data and updated data) go to the write cursor sequentially</p>
    <p>Newly updated tracks are deemed as hot</p>
    <p>Hot tracks are predicted to be accessed again in the near future</p>
    <p>SMaRT allocates an extra track as safety gap for each hot track if space utilization is less than 50%.</p>
    <p>When the current allocation pool is fully consumed, choose the currently largest free space element as</p>
    <p>the new allocation pool.</p>
    <p>Z o n e</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>SMaRT Space Management  3</p>
    <p>Garbage Collection</p>
    <p>Fragmentation Ratio R (Evaluated for incoming writes.)  F: total number of free tracks</p>
    <p>N: number of free space elements</p>
    <p>Pick victim  A small used space element of size W</p>
    <p>U is the space utilization.</p>
    <p>Pick destination  Allocate to the first free space element to the left that fits it.</p>
    <p>Or simply shift left and append to its left neighbour if failing to meet the above condition.</p>
  </div>
  <div class="page">
    <p>SMaRT Space Management  4</p>
    <p>Automatic Cold Data Progression</p>
    <p>Cold data Migrations</p>
    <p>Updated and New Track Allocations</p>
    <p>Shingling</p>
    <p>DirectionWrite Cursor</p>
    <p>Z o n e</p>
    <p>OD ID</p>
    <p>GC is essentially free space consolidation</p>
    <p>GC algorithm</p>
    <p>Pick victim</p>
    <p>Pick appending destination</p>
    <p>Cold data migration</p>
    <p>hot as recently updated data</p>
  </div>
  <div class="page">
    <p>Scheme Reliability</p>
    <p>Power failure can happen before the updates to the mapping table is</p>
    <p>flushed to disk.</p>
    <p>We designed an economic solution based on Backpointer-Assisted</p>
    <p>Lazy Indexing [5]</p>
    <p>Store a backpointer to the logical track when writing a physical track</p>
    <p>Flush mapping table whenever an allocation pool is fully consumed.</p>
    <p>To recover from power failure:</p>
    <p>Scan the latest allocation pool</p>
    <p>Append these LTN-to-PTN mapping entries to the disk copy</p>
  </div>
  <div class="page">
    <p>Z o n e</p>
    <p>.</p>
    <p>Timestamp PTN LTN</p>
    <p>T1 11 X1</p>
    <p>T2 12 X2</p>
    <p>T3 13 X3</p>
    <p>T  y1 14 X4</p>
    <p>T  y2 15 X5</p>
    <p>Mapping</p>
    <p>table on</p>
    <p>disk + =</p>
    <p>Recovered</p>
    <p>Mapping</p>
    <p>table</p>
    <p>Scheme Reliability</p>
  </div>
  <div class="page">
    <p>Evaluations</p>
    <p>Competitor schemes:</p>
    <p>HDD</p>
    <p>Seagate SMR drive exploited in Skylight (denoted as Skylight)</p>
    <p>Trace-based simulations:</p>
    <p>Seagate Cheetah disk drive</p>
    <p>146GB based on 512B block or 1.1TB based on 4KB block</p>
    <p>Traces:  mds_0, proj_0, stg_0 and rsrch_0</p>
    <p>Write intensive</p>
    <p>Evaluation points for drive utilizations:</p>
    <p>30%, 60% and 90%</p>
    <p>Measure Metrics:</p>
    <p>Response time, read fragmentation, write amplifications and GC overhead</p>
  </div>
  <div class="page">
    <p>Response Time</p>
    <p>rsrch_0 @ 30% rsrch_0@60% rsrch_0@90%</p>
    <p>Response time: the difference between the time a request is queued and the time it is completed.</p>
    <p>Skylight briefly crosses HDD and SMaRT in the lower range, due to persistent cache.</p>
    <p>Skylight lags behind for the majority of the requests and response times.</p>
  </div>
  <div class="page">
    <p>Read Fragmentation - 1</p>
    <p>F ra</p>
    <p>g m</p>
    <p>e n</p>
    <p>te d</p>
    <p>r e</p>
    <p>a d</p>
    <p>%</p>
    <p>The percentage of fragmented reads.</p>
    <p>SMaRT is more consistent because its mostly decided</p>
    <p>by the request sizes.</p>
    <p>Skylight is a bit more random, depending on how data</p>
    <p>scatters between persistent cache and zones.</p>
  </div>
  <div class="page">
    <p>Read Fragmentation - 2</p>
    <p>Read fragmentation ratio: the number of sub-reads created by a single read request.</p>
    <p>SMaRT is more consistent and has narrower spectrum.</p>
    <p>Skylight has wider specturm</p>
    <p>rsrch_0 @ 30% rsrch_0@60% rsrch_0@90%</p>
  </div>
  <div class="page">
    <p>Write Amplification - 1</p>
    <p>A m</p>
    <p>p li fi e</p>
    <p>d W</p>
    <p>ri te</p>
    <p>%</p>
    <p>The percentage of amplified writes.</p>
    <p>SMaRT has no amplification for 30% but higher</p>
    <p>amplifications for 60% and 90%.</p>
    <p>These numbers are generally low, because both</p>
    <p>schemes use background GCs.</p>
  </div>
  <div class="page">
    <p>Write Amplification - 2</p>
    <p>Write amplification ratio: the number of sub-I/Os created by a single write request.</p>
    <p>SMaRT has very narrow spectrum.</p>
    <p>Skylight has much wider specturm</p>
    <p>rsrch_0 @ 30% rsrch_0@60% rsrch_0@90%</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>A DM-SMR solution that exploits inherent SMR properties.</p>
    <p>Relatively simple and clean</p>
    <p>No requirement of persistent cache</p>
    <p>Suitable for primary workloads</p>
    <p>Friendly to cold write workloads</p>
    <p>Low metadata overhead</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>I/O scheduler optimized for SMR drives</p>
    <p>Construct storage system with SMR drives, e.g., RAID and erasure codes</p>
    <p>Hybrid SWDs</p>
    <p>HA-SMR and HM-SMR solutions</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[1] http://www.seagate.com/products/enterprise-servers-storage/nearline-storage/archive-hdd/</p>
    <p>[2] http://www.hgst.com/products/hard-drives/ultrastar-archive-ha10</p>
    <p>[3] http:// http://www.open-zfs.org/w/images/2/2a/Host-Aware_SMR-Tim_Feldman.pdf</p>
    <p>[4] A. Aghayev and P. Desnoyers. Skylighta window on shingled disk operation. In13th USENIX</p>
    <p>Con-ference on File and Storage Technologies (FAST15)</p>
    <p>[5] Y. Lu, J. Shu, W. Zheng, et al. Extending the life-time of flash-based storage through reducing</p>
    <p>write amplification from file systems. In FAST, pages 257270, 2013</p>
  </div>
  <div class="page">
    <p>www.cris.umn.edu</p>
  </div>
</Presentation>
