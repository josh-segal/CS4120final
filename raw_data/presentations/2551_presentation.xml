<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Robustness in the Salus scalable block store</p>
    <p>Yang Wang, Manos Kapritsos, Zuocheng Ren, Prince Mahajan, Jeevitha Kirubanandam,</p>
    <p>Lorenzo Alvisi, and Mike Dahlin University of Texas at AusIn</p>
  </div>
  <div class="page">
    <p>Storage: Do not lose data</p>
    <p>Fsck, IRON, ZFS, Past</p>
    <p>Now &amp; Future</p>
  </div>
  <div class="page">
    <p>Adding robustness to scalable systems</p>
    <p>Scalable systems: Crash failure + certain disk corrupIon (GFS/Bigtable, HDFS/HBase, WAS, )</p>
    <p>Strong protecIons: Arbitrary failure (BFT, End-to-end verificaIon, ) Salus</p>
  </div>
  <div class="page">
    <p>Salus: A robust and scalable block store</p>
    <p>Low overhead Better performance in certain cases Comparable performance in other cases</p>
    <p>Maintain scalability</p>
    <p>Strong robustness Clients never read corrupted data. Durable and available despite f failures.</p>
  </div>
  <div class="page">
    <p>Start from scalable key-value store</p>
    <p>File system Key-value DatabaseBlock storeBlock store</p>
  </div>
  <div class="page">
    <p>Scalable architecture</p>
    <p>Metadata server</p>
    <p>Data servers</p>
    <p>Clients</p>
    <p>Parallel data transfer</p>
    <p>Data is replicated for durability and availability</p>
    <p>CompuIng node:  No persistent storage</p>
    <p>Storage nodes</p>
  </div>
  <div class="page">
    <p>Problems</p>
    <p>?</p>
    <p>No ordering guarantees for writes  Single points of failures:</p>
    <p>CompuIng nodes can corrupt data.  Client can not verify data integrity.</p>
  </div>
  <div class="page">
    <p>Metadata server</p>
    <p>Data servers</p>
    <p>Clients</p>
    <p>Block store requires barrier semanIcs</p>
  </div>
  <div class="page">
    <p>Single point of failure</p>
    <p>Clients</p>
    <p>Metadata server</p>
    <p>Data servers</p>
  </div>
  <div class="page">
    <p>Single point of failure</p>
    <p>Does a disk checksum work? Can not prevent errors in memory, etc.</p>
    <p>Clients</p>
    <p>Metadata server</p>
    <p>Data servers</p>
  </div>
  <div class="page">
    <p>Salus: Overview</p>
    <p>Metadata server</p>
    <p>Block drivers (single driver per virtual disk)</p>
    <p>Pipelined commit</p>
    <p>AcIve storage</p>
    <p>End-to-end verificaIon</p>
    <p>Failure model: Almost arbitrary but no impersonaIon</p>
  </div>
  <div class="page">
    <p>Pipelined commit</p>
    <p>Goal: barrier semanIcs</p>
    <p>Nave soluIon:  Waits at a barrier: Lose parallelism</p>
    <p>Look similar to distributed transacIon  Well-known soluIon: Two phase commit (2PC)</p>
  </div>
  <div class="page">
    <p>Problem of 2PC</p>
    <p>Prepared</p>
    <p>Driver</p>
    <p>Servers</p>
    <p>Leader</p>
    <p>Prepared</p>
    <p>Leader</p>
    <p>Batch i</p>
    <p>Batch i+1</p>
    <p>Barriers</p>
  </div>
  <div class="page">
    <p>Problem of 2PC</p>
    <p>Driver</p>
    <p>Servers</p>
    <p>Leader</p>
    <p>Leader</p>
    <p>Batch i</p>
    <p>Batch i+1</p>
    <p>Commit</p>
    <p>Commit</p>
    <p>Problem can sIll happen. Need ordering guarantee between different batches.</p>
  </div>
  <div class="page">
    <p>Pipelined commit</p>
    <p>Lead of batch i-1</p>
    <p>Driver</p>
    <p>Servers</p>
    <p>Leader</p>
    <p>Commit</p>
    <p>Commit Leader</p>
    <p>Batch i</p>
    <p>Batch i+1</p>
    <p>Parallel data transfer</p>
    <p>Batch i-1 commieed</p>
    <p>Batch i commieed</p>
    <p>Sequential commit</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Challenges  Salus</p>
    <p>Pipelined commit  AcIve storage  Scalable end-to-end checks</p>
    <p>EvaluaIon</p>
  </div>
  <div class="page">
    <p>AcIve storage</p>
    <p>Goal: A single node cannot corrupt data</p>
    <p>Well-known soluIon: BFT replicaIon  Problem: Requires at least 2f+1 replicas</p>
    <p>Salus approach: Decouple safety and liveness  For safety, unanimous consent of f+1 replicas  How about availability/liveness?</p>
  </div>
  <div class="page">
    <p>AcIve storage: Restore availability Computing nodes</p>
    <p>Storage nodes</p>
    <p>What if something goes wrong?  Problem: We may not know which one is faulty.</p>
    <p>Replace all compuIng nodes  CompuIng nodes have only sog states.</p>
  </div>
  <div class="page">
    <p>AcIve storage: Restore availability Computing nodes</p>
    <p>Storage nodes</p>
    <p>What if something goes wrong?  Problem: We may not know which one is faulty.</p>
    <p>Replace all compuIng nodes  CompuIng nodes have only sog states.</p>
  </div>
  <div class="page">
    <p>AcIve storage: Beeer performance Computing nodes</p>
    <p>Storage nodes</p>
    <p>AddiIonal benefit:  Colocate compuIng and storage: Save network bandwidth</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Challenges  Salus</p>
    <p>Pipelined commit  AcIve storage  Scalable end-to-end checks</p>
    <p>EvaluaIon</p>
  </div>
  <div class="page">
    <p>EvaluaIon</p>
    <p>Is Salus robust against failures?</p>
    <p>What is the overhead of Salus?</p>
    <p>Does the overhead grow with the scale of the system?</p>
  </div>
  <div class="page">
    <p>Is Salus robust against failures?</p>
    <p>Failures HBaseHBase SalusSalusFailures Safe Live Safe Live</p>
    <p>Yes Yes Yes Yes</p>
    <p>Yes No Yes No</p>
    <p>No - Yes Yes</p>
    <p>- - Yes No</p>
  </div>
  <div class="page">
    <p>What is the overhead of Salus?</p>
    <p>Sequential read Random read Sequential write Random write</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>)</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>Fig. 10: Aggregate throughput on small nodes. HBase-N and Salus-N disable compactions.</p>
    <p>HBase Salus Throughput (MB/s) 27 47</p>
    <p>Network consumption (network bytes per byte written by the client)</p>
    <p>Fig. 11: Aggregate sequential write throughput and network bandwidth usage with fewer server machines but more disks per machine.</p>
    <p>tive replication of region servers, introduced to improve robustness, can reduce network bandwidth and significantly improve performance when the total disk bandwidth exceeds the aggregate network bandwidth.</p>
    <p>Figure 10 reports experiments on our small-server testbed with nine nodes acting as combined region server and datanode servers and we increase the number of clients until the throughput does not increase.</p>
    <p>For sequential read, both systems can achieve about 110MB/s. Pipelining reads in Salus does not improve aggregate throughput since also HBase has multiple clients to parallelize network and disk operations. For random reads, disk seek and rotation are the bottleneck, and both systems achieve only about 3MB/s.</p>
    <p>The relative slowdown of Salus versus HBase for sequential and random writes is respectively of 11.1% to 19.4% and significantly lower when compaction is enabled since compaction adds more disk operations to both HBase and Salus. Salus reduces network bandwidth at the expense of higher disk and CPU usage, but this trade-off does not help in our system because disk and network bandwidth are comparable. Even so, we find this to be an acceptable price for the stronger guarantees provided by Salus.</p>
    <p>Figure 11 shows what happens when we run the sequential write experiment using the three 10-disk storage nodes as servers. Here, the tables are turned and Salus outperforms HBase (47MB/s versus 27MB/s). Our profiling shows that in both experiments, the bottleneck is the network topology that constrains the aggregate bandwidth to 1.2Gbit/s.</p>
    <p>Figure 11 also compares the network bandwidth usage of HBase and Salus under the sequential write workload. HBase sends more than five bytes for each byte written by the client (two network transfers each for logging and flushing, but fewer than two for compaction, since some blocks are overwritten.) Salus only uses two bytes per-byte-written to forward the request to replicas;</p>
    <p>Sequential write Random write</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>p e r</p>
    <p>re g io</p>
    <p>n s</p>
    <p>e rv</p>
    <p>e r</p>
    <p>(M B</p>
    <p>/s )</p>
    <p>H B a se</p>
    <p>-9</p>
    <p>H B a se</p>
    <p>-9</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>H B a se</p>
    <p>-1 0 8</p>
    <p>H B a se</p>
    <p>-1 0 8</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>Fig. 12: Write throughput per server with 9 servers and 108 servers (compaction disabled).</p>
    <p>logging, flushing, and compaction are performed locally. The actual number is slightly higher than 2, because of Saluss additional metadata. Salus halves network bandwidth usage compared to HBase, which explains why its throughput is 74% higher than that HBase when network bandwidth is limited.</p>
    <p>Note that we do not measure the aggregate throughput of EBS because we do not know its internal architecture and thus we do not know how to saturate it.</p>
    <p>For our testbed we use EC2s extra large instances, with datanodes and region servers configured to use 3GB of memory each. Some preliminary tests run to measure the characteristics of our testbed show that each EC2 instance can reach a maximum network and disk bandwidth of about 100MB/s, meaning that network bandwidth is not a bottleneck; thus, we do not expect Salus to outperform HBase in this setting.</p>
    <p>Given our limited resources, we focus our attention on measuring the throughput of sequential and random writes: we believe this is reasonable since the only additional overhead for reads are the end-to-end checks performed by the clients, which are easy to make scalable. We run each experiment with an equal number of clients and servers and for each 11-minute-long experiment we report the throughput of the last 10 minutes.</p>
    <p>Because we do not have full control over EC2s internal architecture, and because one users virtual machines in EC2 may share resources such as disks and networks with other users, these experiments have limitations: the performance of EC2s instances fluctuates noticeably and it becomes hard to even determine what the stable throughput for a given experimental configuration is. Further, while in most cases, as expected, we find that HBase performs better than Salus, some experi</p>
    <p>Sequential read Random read Sequential write Random write</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>)</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>H B a se</p>
    <p>-N</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>S a lu s</p>
    <p>N</p>
    <p>Fig. 10: Aggregate throughput on small nodes. HBase-N and Salus-N disable compactions.</p>
    <p>HBase Salus Throughput (MB/s) 27 47</p>
    <p>Network consumption (network bytes per byte written by the client)</p>
    <p>Fig. 11: Aggregate sequential write throughput and network bandwidth usage with fewer server machines but more disks per machine.</p>
    <p>tive replication of region servers, introduced to improve robustness, can reduce network bandwidth and significantly improve performance when the total disk bandwidth exceeds the aggregate network bandwidth.</p>
    <p>Figure 10 reports experiments on our small-server testbed with nine nodes acting as combined region server and datanode servers and we increase the number of clients until the throughput does not increase.</p>
    <p>For sequential read, both systems can achieve about 110MB/s. Pipelining reads in Salus does not improve aggregate throughput since also HBase has multiple clients to parallelize network and disk operations. For random reads, disk seek and rotation are the bottleneck, and both systems achieve only about 3MB/s.</p>
    <p>The relative slowdown of Salus versus HBase for sequential and random writes is respectively of 11.1% to 19.4% and significantly lower when compaction is enabled since compaction adds more disk operations to both HBase and Salus. Salus reduces network bandwidth at the expense of higher disk and CPU usage, but this trade-off does not help in our system because disk and network bandwidth are comparable. Even so, we find this to be an acceptable price for the stronger guarantees provided by Salus.</p>
    <p>Figure 11 shows what happens when we run the sequential write experiment using the three 10-disk storage nodes as servers. Here, the tables are turned and Salus outperforms HBase (47MB/s versus 27MB/s). Our profiling shows that in both experiments, the bottleneck is the network topology that constrains the aggregate bandwidth to 1.2Gbit/s.</p>
    <p>Figure 11 also compares the network bandwidth usage of HBase and Salus under the sequential write workload. HBase sends more than five bytes for each byte written by the client (two network transfers each for logging and flushing, but fewer than two for compaction, since some blocks are overwritten.) Salus only uses two bytes per-byte-written to forward the request to replicas;</p>
    <p>Sequential write Random write</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>p e r</p>
    <p>re g io</p>
    <p>n s</p>
    <p>e rv</p>
    <p>e r</p>
    <p>(M B</p>
    <p>/s )</p>
    <p>H B a se</p>
    <p>-9</p>
    <p>H B a se</p>
    <p>-9</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>H B a se</p>
    <p>-1 0 8</p>
    <p>H B a se</p>
    <p>-1 0 8</p>
    <p>S a lu s</p>
    <p>S a lu s</p>
    <p>Fig. 12: Write throughput per server with 9 servers and 108 servers (compaction disabled).</p>
    <p>logging, flushing, and compaction are performed locally. The actual number is slightly higher than 2, because of Saluss additional metadata. Salus halves network bandwidth usage compared to HBase, which explains why its throughput is 74% higher than that HBase when network bandwidth is limited.</p>
    <p>Note that we do not measure the aggregate throughput of EBS because we do not know its internal architecture and thus we do not know how to saturate it.</p>
    <p>For our testbed we use EC2s extra large instances, with datanodes and region servers configured to use 3GB of memory each. Some preliminary tests run to measure the characteristics of our testbed show that each EC2 instance can reach a maximum network and disk bandwidth of about 100MB/s, meaning that network bandwidth is not a bottleneck; thus, we do not expect Salus to outperform HBase in this setting.</p>
    <p>Given our limited resources, we focus our attention on measuring the throughput of sequential and random writes: we believe this is reasonable since the only additional overhead for reads are the end-to-end checks performed by the clients, which are easy to make scalable. We run each experiment with an equal number of clients and servers and for each 11-minute-long experiment we report the throughput of the last 10 minutes.</p>
    <p>Because we do not have full control over EC2s internal architecture, and because one users virtual machines in EC2 may share resources such as disks and networks with other users, these experiments have limitations: the performance of EC2s instances fluctuates noticeably and it becomes hard to even determine what the stable throughput for a given experimental configuration is. Further, while in most cases, as expected, we find that HBase performs better than Salus, some experi</p>
    <p>With sufficient network bandwidth</p>
    <p>With limited network bandwidth</p>
  </div>
  <div class="page">
    <p>Does the overhead grow with the scale of the system?</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Pipelined commit  Write in parallel  Provide ordering guarantees</p>
    <p>AcIve Storage  Eliminate single point of failures  Consume less network bandwidth</p>
    <p>High robustness  Low performance</p>
  </div>
</Presentation>
