<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Inference Complexity As Learning Bias</p>
    <p>Daniel Lowd Dept. of Computer</p>
    <p>and Information Science University of Oregon</p>
    <p>Joint work with Pedro Domingos</p>
  </div>
  <div class="page">
    <p>Dont use model complexity as your learning bias</p>
    <p>Use inference complexity.</p>
  </div>
  <div class="page">
    <p>The Goal</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data Model</p>
    <p>Learning algorithms Inference algorithms</p>
    <p>The actual task</p>
    <p>This talk:  How to learn accurate and efficient models by</p>
    <p>tightly integrating learning and inference  Experiments: exact inference in &lt; 100ms</p>
    <p>in models with treewidth &gt; 100</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Standard solutions (and why they fail)  Background</p>
    <p>Learning with Bayesian networks  Inference with arithmetic circuits</p>
    <p>Learning arithmetic circuits  Scoring  Search  Efficiency</p>
    <p>Experiments  Conclusion</p>
  </div>
  <div class="page">
    <p>Solution 1: Exact Inference</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data Model Jointree</p>
    <p>SLOW SLOW</p>
  </div>
  <div class="page">
    <p>Solution 2: Approximate Inference</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data Model Approximation</p>
    <p>Approximations are often too inaccurate. More accurate algorithms tend to be slower.</p>
    <p>SLOW</p>
  </div>
  <div class="page">
    <p>Solution 3: Learn a tractable model Related work: Thin junction trees</p>
    <p>Answers!Data Jointree</p>
    <p>Polynomial in data, but still exponential in treewidth</p>
    <p>[E.g.: Chechetka &amp; Guestrin, 2007]</p>
  </div>
  <div class="page">
    <p>Thin junction trees are thin</p>
    <p>Maximum effective treewidth is 2-5</p>
    <p>We have learned models with treewidth &gt;100</p>
    <p>Their junction trees Our junction trees</p>
  </div>
  <div class="page">
    <p>Solution 3: Learn a tractable model Our work: Arithmetic circuits with penalty on circuit size</p>
    <p>A</p>
    <p>B C</p>
    <p>Answers!Data</p>
    <p>Model</p>
    <p>Circuit</p>
    <p>+ +</p>
    <p>( )</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Standard solutions (and why they fail)  Background</p>
    <p>Learning with Bayesian networks  Inference with arithmetic circuits</p>
    <p>Learning arithmetic circuits  Overview  Optimizations</p>
    <p>Experiments  Conclusion</p>
  </div>
  <div class="page">
    <p>Bayesian networks</p>
    <p>Problem: Compactly represent probability distribution over many variables</p>
    <p>Solution: Conditional independence</p>
    <p>P(A,B,C,D) = P(A) P(B|A) P(C|A) P(D|B,C)</p>
    <p>A</p>
    <p>B C</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>With decision-tree CPDs</p>
    <p>Problem: Number of parameters is exponential in the maximum number of parents</p>
    <p>Solution: Context-specific independence</p>
    <p>P(D|B,C) = B=?</p>
    <p>false</p>
    <p>false</p>
    <p>C=? tru</p>
    <p>e</p>
    <p>tru e</p>
  </div>
  <div class="page">
    <p>Compiled to circuits</p>
    <p>Problem: Inference is exponential in treewidth</p>
    <p>Solution: Compile to arithmetic circuits</p>
    <p>A A</p>
    <p>+ +</p>
    <p>A A</p>
    <p>B B</p>
    <p>B B</p>
  </div>
  <div class="page">
    <p>Arithmetic circuits</p>
    <p>Directed, acyclic graph with single root  Leaf nodes are inputs  Interior nodes are addition or multiplication  Can represent any distribution</p>
    <p>Inference is linear in model size!  Never larger than junction tree  Can exploit local structure to save time/space</p>
    <p>A A</p>
    <p>+ +</p>
    <p>A A</p>
    <p>B B</p>
    <p>B B</p>
  </div>
  <div class="page">
    <p>ACs for Inference</p>
    <p>Bayesian network: P(A,B,C) = P(A) P(B) P(C|A,B)</p>
    <p>Network polynomial: ABC ABC|AB + ABC ABC|AB +</p>
    <p>Can compute arbitrary marginal queries by evaluating network polynomial.</p>
    <p>Arithmetic circuits (ACs) offer efficient, factored representations of this polynomial.</p>
    <p>Can take advantage of local structure such as context-specific independence.</p>
  </div>
  <div class="page">
    <p>BN Structure Learning</p>
    <p>Start with an empty network</p>
    <p>Greedily add splits to decision trees one at a time, enforcing acyclicity</p>
    <p>[Chickering et al., 1996]</p>
    <p>score(C,T) = log P(T|C)  kp np(C) (accuracy  # parameters)</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>For an arithmetic circuit C on an i.i.d. training sample T: Typical cost function:</p>
    <p>Our cost function:</p>
    <p>score(C,T) = log P(T|C)  kp np(C)  ke ne(C) (accuracy  # parameters  circuit size)</p>
    <p>score(C,T) = log P(T|C)  kp np(C) (accuracy  # parameters)</p>
  </div>
  <div class="page">
    <p>Basic algorithm</p>
    <p>Following Chickering et al. (1996), we induce our statistical models by greedily selecting splits for the decision-tree CPDs. Our approach has two key differences:</p>
  </div>
  <div class="page">
    <p>Efficiency</p>
    <p>Compiling each candidate AC from scratch at each step is too expensive.</p>
    <p>Instead: Incrementally modify AC as we add splits.</p>
  </div>
  <div class="page">
    <p>A A A A B B</p>
    <p>Before split</p>
  </div>
  <div class="page">
    <p>+</p>
    <p>A A|B A A|B A A|B A A|B</p>
    <p>B B</p>
    <p>After split</p>
  </div>
  <div class="page">
    <p>Algorithm Create initial product of marginals circuit Create initial split list Until convergence:</p>
    <p>For each split in list Apply split to circuit Score result Undo split Apply highest-scoring split to circuit Add new child splits to list Remove inconsistent splits from list</p>
  </div>
  <div class="page">
    <p>How to split a circuit D: Parameter nodes to be split V: Indicators for the splitting variable M: First mutual ancestors of D and V</p>
    <p>For each indicator  in V, Copy all nodes between M and D or V, conditioned on .</p>
    <p>For each m in M, Replace children of m that are ancestors of D or V with a sum over copies of the ancestors times the  each copy was conditioned on.</p>
  </div>
  <div class="page">
    <p>Optimizations</p>
    <p>We avoid rescoring splits every iteration by: 1. Noting that likelihood gain never changes, only</p>
    <p>number of edges added 2. Evaluating splits with higher likelihood gain first,</p>
    <p>since likelihood gain is an upper bound on score. 3. Re-evaluate number of edges added only when</p>
    <p>another split may have affected it (AC-Greedy). 4. Assume the number of edges added by a split only</p>
    <p>increases as the algorithm progresses (AC-Quick).</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Datasets:  KDD-Cup 2000: 65 variables</p>
    <p>Anonymous MSWeb: 294 variables  EachMovie (subset): 500 variables</p>
    <p>Algorithms:  WinMine toolkit + Gibbs sampling</p>
    <p>AC + Exact inference</p>
    <p>Queries: Generated queries from the test data with varying numbers of evidence and query variables</p>
  </div>
  <div class="page">
    <p>Learning time</p>
  </div>
  <div class="page">
    <p>Inference time</p>
  </div>
  <div class="page">
    <p>Accuracy: EachMovie</p>
  </div>
  <div class="page">
    <p>Accuracy: MSWeb</p>
  </div>
  <div class="page">
    <p>Accuracy: KDD Cup</p>
  </div>
  <div class="page">
    <p>Our method learns complex, accurate models</p>
    <p>AC-Greedy AC-Quick WinMine</p>
    <p>EachMovie 55.7 54.9 53.7</p>
    <p>KDD Cup 2.16 2.16 2.16</p>
    <p>MSWeb 9.85 9.85 9.69</p>
    <p>AC-Greedy AC-Quick WinMine</p>
    <p>EachMovie 35 54 281</p>
    <p>KDD Cup 38 38 53</p>
    <p>MSWeb 114 127 118</p>
    <p>Log-likelihood of test data:</p>
    <p>Treewidth of learned models:</p>
    <p>(235 = 34 billion)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Problem: Learning accurate intractable models = Learning inaccurate models</p>
    <p>Solution: Use inference complexity as learning bias</p>
    <p>Algorithm: Learn arithmetic circuits with penalty on circuit size</p>
    <p>Result: Much faster and more accurate inference than standard Bayes net learning</p>
  </div>
</Presentation>
