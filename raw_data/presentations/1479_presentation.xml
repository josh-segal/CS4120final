<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>File Recipe Compression in Data Deduplication Systems</p>
    <p>Dirk Meister Andr Brinkmann Tim S</p>
  </div>
  <div class="page">
    <p>File Recipes  Zero-Chunk Suppression  Chunk Index Page-Based  Statistical Approaches  Evaluation</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Most file-based deduplication systems use file recipes  List of chunk fingerprints of a file  Similar to data block pointer in normal file systems  Differences</p>
    <p>Variable-sized chunks  Fingerprints at least 20 bytes</p>
    <p>File Recipes</p>
    <p>LQRVL]HW\SH</p>
    <p>EFFEEIDFIDGGFIVL]H</p>
    <p>LQRVL]HW\SH</p>
    <p>EEFHIEDEGFHVL]H</p>
    <p>HDHEEFEIFFFIGGHVL]H</p>
    <p>FIDFDIEHGIIIHHHEGVL]H</p>
  </div>
  <div class="page">
    <p>File recipes can occupy significant disk space  Chunk data grows with post-deduplication capacity  File recipes grows with pre-deduplication capacity</p>
    <p>Examples</p>
    <p>Year of weekly full backups, 95% weekly redundancy 6.6% of (estimated) total disk space</p>
    <p>Year of daily full backups, 99% daily redundancy 28.4% of (estimated) total disk space</p>
    <p>The compression of file recipes promises significant savings of the overall storage usage</p>
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>Use (shorter) code word instead of fingerprint in file recipe</p>
    <p>Requirement  Low overhead: memory, IO, storage  Limited impact on write and restore speed</p>
    <p>Assumptions  Fingerprinting-based data deduplication systems  Full chunk index available, e.g. on SSD storage  Backup workloads  Reverse lookup necessary</p>
    <p>File Recipe Compression</p>
  </div>
  <div class="page">
    <p>Chunk reference skew  Few chunks responsible for high number of references</p>
    <p>Zero-chunk  A chunk completely filled with zeros  Fingerprint of zero-chunk can be pre-calculated</p>
    <p>Special handling in some deduplication systems  Never stored, never looked up on disk</p>
    <p>Replace zero-chunk with 1 byte code word  Likely already established, e.g., Wei et al., dedupv1</p>
  </div>
  <div class="page">
    <p>Assign a code word to each fingerprint  Determine code word based on chunk index</p>
    <p>Chunk index page, e.g. of persistent hash table  Code word = page id | unique identifier in page</p>
    <p>Code word for 0x123123123123123 is 17000|9</p>
    <p>&amp;KXQN,QGH[ 3DJHV</p>
    <p>K[</p>
    <p>[VXIIL[FRQWDLQHU [$)($(%VXIIL[FRQWDLQHU</p>
  </div>
  <div class="page">
    <p>Generalization of zero-chunk suppression  Determine entropy of each chunk</p>
    <p>entropy = -log2(reference count / total references)  Easy with reference-counting garbage collection  Only possible after some backup runs</p>
    <p>Assign code words to chunks below a entropy threshold</p>
    <p>E.g., around 0.3% of the chunks  Code words equally sized, e.g., 24 bit  Maintain (in-memory) reverse index</p>
    <p>Similar to Huffman coding  But: relaxed, non-optimal, scales to billions of chunks</p>
  </div>
  <div class="page">
    <p>Per Chunk  Determine most-likely following fingerprint</p>
    <p>(This is the order-1 statistic)  Select as prediction fingerprint  Replace correct prediction by 1 byte code</p>
    <p>Using locality property of backup workloads  Data structure for order-1 too large at scale  Using estimation of most-frequent following fingerprint  Prediction not exact, but sufficient</p>
  </div>
  <div class="page">
    <p>Trace-based simulation of weekly full backup</p>
    <p>Three data sets in paper  Here: HOME1 data set</p>
    <p>Home directory file server of the University of Paderborn  Content-defined chunking, expected chunk size 8 KB.  15 weekly full backup traces</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Zero-Chunk Suppression (ZC)</p>
    <p>Page-Based (PB) Statistical Dictionary (SD)</p>
    <p>Statistical Prediction (SP)</p>
    <p>S av</p>
    <p>in gs</p>
    <p>in %</p>
    <p>Compression Ratio</p>
  </div>
  <div class="page">
    <p>Compression Ratio (Combined)</p>
    <p>S av</p>
    <p>in gs</p>
    <p>in %</p>
  </div>
  <div class="page">
    <p>File recipes can occupy a significant fraction of disk space  New compression approaches for file recipes  Combination shrinks file recipes by more than 90%.  File recipe compression enables additional data reduction</p>
    <p>File recipes are largely unexplored territory in data</p>
    <p>deduplication research.</p>
    <p>Simulation tool FS-C and dedupv1 are open source.</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>THANK YOU Questions?</p>
  </div>
</Presentation>
