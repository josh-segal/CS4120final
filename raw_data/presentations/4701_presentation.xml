<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Disk Scrubbing in Large Archival Storage Systems</p>
    <p>Thomas Schwarz, S.J.1,2 Qin Xin1,3, Ethan Miller1, Darrell Long1, Andy Hospodor1,2,</p>
    <p>Spencer Ng3</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Large archival storage systems:  Protect data more proactively  Keep disks powered off for long</p>
    <p>periods of time  Have low rate of data access</p>
    <p>Protect data by storing it redundantly.</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Failures can happen  At the block level.  At the device level.</p>
    <p>Failures may remain undetected for long periods of time.</p>
    <p>A failure may unmask one or more additional failures.  Reconstruction procedure accesses data on</p>
    <p>other devices.  Those devices can have suffered previous</p>
    <p>failures.</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>We investigate the efficacy of disk scrubbing.</p>
    <p>Disk Scrubbing accesses a disk to see whether the data can still be read.  Reading a single block shows that the</p>
    <p>device still works.  Reading all blocks shows that we can</p>
    <p>read all the data on the block.</p>
  </div>
  <div class="page">
    <p>Contents</p>
  </div>
  <div class="page">
    <p>Disk Failure Taxonomy</p>
    <p>Disk Blocks  512B sector uses error control coding  Read to a block successfully either</p>
    <p>corrects all errors, or retries and then:  flags block as unreadable, or  misreads block.</p>
    <p>Disk Failure Rates  Depend highly on</p>
    <p>Environment:  Temperature, Vibrations, Air quality</p>
    <p>Age.  Vintage.</p>
  </div>
  <div class="page">
    <p>Disk Failure Taxonomy  Block Failure Rate estimate:</p>
    <p>Since:  1/3 of all field returns for server drives are due to hard</p>
    <p>errors.  RAID users (90%) do not return drives with hard errors.  10% of all disks sold account for 1/3 of all errors.</p>
    <p>Hence:  Mean Time between Block Failures is 3/10 MTBF of all disk</p>
    <p>failures.  Mean time to disk failure is 3/2 of MTBF.  1 million hour rated drive has</p>
    <p>3*105 mean time between block failure.  1.5*106 mean time between disk failure.</p>
    <p>This is one back of the envelope calculation based on numbers by one anonymous disk manufacturer. The results seem to be accepted by many.</p>
  </div>
  <div class="page">
    <p>System Overview</p>
    <p>Disks are powered down when not in use.</p>
    <p>Use m+k redundancy scheme:  Store data in large blocks.  m blocks grouped into an r-group.  Add k parity data blocks to r-group.</p>
    <p>Small blocks lead to fast reconstruction and good reconstruction load distribution.</p>
    <p>Large blocks have slightly better reliability.</p>
  </div>
  <div class="page">
    <p>System Overview</p>
    <p>Disk Scrubbing  Scrub an S - block</p>
    <p>Can read one block  device not failed.  Can read all blocks  can access all data.  Can read and verify all blocks  data can</p>
    <p>be read correctly.  Use algebraic signatures for that.  Can even verify that parity data accurately</p>
    <p>reflects client data.</p>
  </div>
  <div class="page">
    <p>System Overview</p>
    <p>If a bad block is detected, we usually can reconstruct its contents with parity / mirrored data.</p>
    <p>Scrubbing finds the error before it can hurt you.</p>
  </div>
  <div class="page">
    <p>Modeling Scrubbing</p>
    <p>Random Scrubbing: Scrub an S-block at random. (Exponential distribution).</p>
    <p>Deterministic Scrubbing: Scrub an S-block at regular intervals.</p>
  </div>
  <div class="page">
    <p>Modeling Scrubbing Opportunistic Scrubbing:</p>
    <p>Try to scrub when you access the disk anyway.</p>
    <p>Piggyback scrubs on disk accesses</p>
    <p>Efficiency depends on the frequency of accesses.</p>
    <p>MTBA: Mean Time Between Accesses (103</p>
    <p>hours).</p>
    <p>Average scrub interval 104 hours.</p>
    <p>Block MTBF 105 hours.</p>
  </div>
  <div class="page">
    <p>Power Cycling and Reliability</p>
    <p>Turning a disk on or off has a significant impact.  Even if disks move actuators away from</p>
    <p>surface (laptop disks).  No direct data to measure impact of</p>
    <p>Power On Hours (POH).  Extrapolate from Seagate data:</p>
    <p>One on / off cycle is roughly equivalent to running a disk for eight hours.</p>
  </div>
  <div class="page">
    <p>Determining Scrubbing Intervals</p>
    <p>Interval too short:  Too much traffic  Disks busy</p>
    <p>Increased error rate  Lower system MTBF.</p>
    <p>Interval too long:  A failure more likely to unmask other</p>
    <p>failures.  More failures catastrophic.  Lower system MTBF.</p>
  </div>
  <div class="page">
    <p>Determining Scrubbing Intervals</p>
    <p>Mirrored reliability block</p>
    <p>N = 250 disks.</p>
    <p>Device failure rate: 5105 hours</p>
    <p>Block failure rate: 10-5</p>
    <p>Time to read disk: 4 hours.</p>
    <p>Deterministic: without considering power-up effects.</p>
    <p>Deterministic with cycling: considering power-up effects.</p>
    <p>Opportunistic does not pay power-on penalty, but runs disk longer.</p>
    <p>Random does not pay power-on penalty. Random with cycling would be below the deterministic with cycling graph.</p>
  </div>
  <div class="page">
    <p>Determining Scrubbing Intervals</p>
    <p>Scrub frequently: You never know what you might find.</p>
    <p>Mirrored disks using opportunistic scrubbing (no power-on penalty).</p>
    <p>Assumes a high disk access rate.</p>
  </div>
  <div class="page">
    <p>Simulation Results</p>
    <p>1PB archival data store.  Disks have MTBF of 105 hours.  10,000 disk drives  10GB reliability blocks.  ~1TB/day traffic</p>
  </div>
  <div class="page">
    <p>Simulation Results</p>
    <p>Two-way Mirroring</p>
  </div>
  <div class="page">
    <p>Simulation Results</p>
    <p>RAID 5 redundancy scheme</p>
  </div>
  <div class="page">
    <p>Simulation Results Mirroring.</p>
    <p>Opportunistic scrubbing with ~ three disk accesses per year.</p>
    <p>Observe that additional scrubbing leads to more power-on cycles that slightly increase occurrence of data losses.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>We have shown that disk scrubbing is a necessity for very large scale storage systems.</p>
    <p>Our simulations show the impact of power-on / power-off on reliability.</p>
    <p>We also note that lack of numbers on disk drive reliability prevents public research.</p>
  </div>
</Presentation>
