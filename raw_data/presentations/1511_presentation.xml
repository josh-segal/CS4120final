<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards SLO Complying SSDs Through OPS Isola*on</p>
    <p>Jaeho Kim (University of Seoul, Korea) Donghee Lee (University of Seoul) Sam H. Noh (Hongik University)</p>
  </div>
  <div class="page">
    <p>Applica&lt;ons of Flash Memory</p>
    <p>Area of flash storage  From embedded to server storage</p>
    <p>ApplicaOon field</p>
    <p>Target environment</p>
  </div>
  <div class="page">
    <p>Introduc&lt;on &amp; Mo&lt;va&lt;on  VirtualizaOon system</p>
    <p>Need to saOsfy Service Level ObjecOve (SLO) for each VM  SLO is provided through hardware resource isolaOon</p>
    <p>ExisOng soluOons for isolaOng CPU and memory  Distributed resource scheduler [VMware inc.]  Memory resource management in VMware ESX server [SIGOPS OSR 2002]</p>
    <p>VM VM VM VM</p>
    <p>Hypervisor</p>
    <p>CPU Memory SSD</p>
    <p>SSD isolation? Any studies?</p>
  </div>
  <div class="page">
    <p>Do SSDs provide decent performance isola&lt;on?</p>
    <p>Does each VM proporOonally consume I/O bandwidth of shared SSD among VMs?</p>
    <p>How does proporOonality vary as state of SSD is varied?</p>
    <p>Bronze</p>
    <p>Silver</p>
    <p>Gold</p>
    <p>PlaOnum</p>
    <p>VM1 VM2 VM3 VM4</p>
    <p>Everybody happy with given weight?</p>
    <p>State of SSD</p>
    <p>Clean SSD Aged SSD</p>
  </div>
  <div class="page">
    <p>Ini&lt;al Experiments on Commercial SSD  Linux kernel-based virtual machine (KVM) on 4 VMs  ProporOonal I/O weight (by Cgroups feature in Linux kernel 3.13.x)</p>
    <p>VM-x: x is I/O weight value (Higher value  Allocate higher throughput)  SSD as shared storage</p>
    <p>128GB capacity, SATA3 interface, MLC Flash  clean SSD: empty SSD  aged SSD: full SSD (busy performing garbage collecOon)</p>
    <p>Aging is conducted by issuing 4KB ~ 32KB sized random writes for a total write that exceeds the SSD capacity</p>
    <p>Each VM runs the same workload concurrently  Financial, MSN, and Exchange</p>
    <p>Bronze</p>
    <p>Silver</p>
    <p>Gold</p>
    <p>PlaOnum</p>
    <p>Hypervisor (KVM)</p>
    <p>CPU Memory SSD</p>
    <p>Two state: Clean &amp; Aged</p>
    <p>VM-1 VM-2 VM-5 VM-10 I/O weight</p>
  </div>
  <div class="page">
    <p>Ideal HDD SSD clean</p>
    <p>SSD aged</p>
    <p>Pr op</p>
    <p>or Oo</p>
    <p>na lit y of I/ O</p>
    <p>ba nd</p>
    <p>w id th</p>
    <p>VM-1</p>
    <p>VM-2</p>
    <p>VM-5</p>
    <p>VM-10</p>
    <p>Results: Propor&lt;onality of I/O Bandwidth  For all workloads, on HDD, proporOonality is close to I/O weight except</p>
    <p>for VM-10  ProporOonality deviaOon is worse for aged SSD than clean SSD</p>
    <p>Ideal proportionality I/O weight</p>
    <p>I/O bandwidth relative to VM-1</p>
    <p>Why?</p>
  </div>
  <div class="page">
    <p>Monitor Internal Workings of SSD  Commercial SSD: Proprietary, black box SSDs  Monitor using Simulator</p>
    <p>SSD simulator: DiskSim SSD Extension  Workloads: Financial, MSN, and Exchange</p>
    <p>Traces are captured as VMs run concurrently on real system</p>
    <p>VM-F (Financial)</p>
    <p>VM-M (MSN)</p>
    <p>VM-E (Exchange)</p>
    <p>Hypervisor (KVM)</p>
    <p>SSD</p>
    <p>blktrace . . .</p>
    <p>VM-F VM-M VM-E</p>
    <p>SSD simulator</p>
    <p>Trace input</p>
    <p>SSD</p>
  </div>
  <div class="page">
    <p>Analysis #1 : Mixture of Data  Within block (GC unit): mixture of data from all VMs</p>
    <p>SSD</p>
    <p>: VM-F data : Invalid data : Free page</p>
    <p>OPS</p>
    <p>VM-M (MSN)</p>
    <p>VM-E (Exchange)</p>
    <p>: VM-E data : VM-M data</p>
    <p>VM-F (Financial)</p>
    <p>block</p>
    <p>page</p>
    <p>Data layout of conventional SSD</p>
    <p>Over-Provisioned Space (OPS) - reserved space for write reqs. - used for garbage collection (GC)</p>
    <p>Data of all VMs are mixed into a block</p>
  </div>
  <div class="page">
    <p>Analysis #2 : Interference among VMs during GC  Movement of data: live pages of workloads other than the one invoking GC</p>
    <p>SSD</p>
    <p>: VM-F data : Invalid data : Free page</p>
    <p>OPS</p>
    <p>VM-M (MSN)</p>
    <p>VM-E (Exchange)</p>
    <p>: VM-E data : VM-M data</p>
    <p>VM-F (Financial)</p>
    <p>Data layout of conventional SSD</p>
    <p>Over-Provisioned Space (OPS) - reserved space for write reqs. - used for garbage collection (GC)</p>
  </div>
  <div class="page">
    <p>Analysis #3: Work induced by other VMs  From one VMs viewpoint: doing unnecessary work induc</p>
    <p>ed by other workloads</p>
    <p>N um</p>
    <p>be r of p ag es m</p>
    <p>ov ed</p>
    <p>Owned by Exchange</p>
    <p>Owned by MSN</p>
    <p>Owned by Financial</p>
    <p>While executing the VM-F (Financial) workload,</p>
    <p>only 30% of them are its own pages</p>
    <p>Number of pages moved for each workloads during GC</p>
  </div>
  <div class="page">
    <p>More Closely  GC leads to interference problem among VMs  GC operaOon employed by one VM is burdened with other VMs pages</p>
    <p>VM-F VM-M VM-E</p>
    <p>garbage garbage</p>
    <p>garbage</p>
    <p>garbage garbage</p>
    <p>Data Area</p>
    <p>OPS Area</p>
    <p>SSD</p>
    <p>(GC)</p>
    <p>GC operaOon in convenOonal SSD</p>
    <p>Employed by VM-F Why do I have to clean others?</p>
  </div>
  <div class="page">
    <p>Avoiding Interference  Cost of GC is major factor in SSD I/O performance  Each VM should pay only for its own GC operaOon</p>
    <p>VM-F VM-M VM-E</p>
    <p>Data Area SSD</p>
    <p>VM-F OPS VM-M OPS VM-E OPS (GC) (GC) (GC)</p>
    <p>garbage garbage</p>
    <p>garbage</p>
    <p>garbage garbage</p>
  </div>
  <div class="page">
    <p>Proposed scheme: OPS isola*on  Dedicate flash memory blocks, including OPS, to each VM separately when</p>
    <p>allocaOng pages to VMs  Prevent interference during GC</p>
    <p>SSD with OPS isola*on</p>
    <p>SSD VM-F OPS</p>
    <p>VM-E OPS</p>
    <p>VM-M (MSN)</p>
    <p>VM-E (Exchange)</p>
    <p>VM-M OPS</p>
    <p>VM-F (Financial)</p>
    <p>: VM-F data : Invalid data : Free page</p>
    <p>: VM-E data : VM-M data</p>
    <p>SSD</p>
    <p>OPS</p>
    <p>VM-M (MSN)</p>
    <p>VM-E (Exchange)</p>
    <p>VM-F (Financial)</p>
    <p>ConvenOonal SSD</p>
  </div>
  <div class="page">
    <p>VM OPS Alloca&lt;on  How much OPS for each VMs to saOsfy SLO?</p>
    <p>OPS size per VM?</p>
    <p>SSD</p>
    <p>Data Space VM-F OPS VM-E OPS</p>
    <p>VM-M OPS</p>
    <p>Flexible size Fixed size</p>
  </div>
  <div class="page">
    <p>IOPS of SSD</p>
    <p>Parameter Meaning tGC Time to GC (depends on utilization (u) of victim block at GC) tPROG Time for programming a page (constant value) tXfer Time for transferring a page (constant value)</p>
    <p>IOPS = 1 / (tGC + tPROG + tXfer)</p>
    <p>SSD</p>
    <p>Determined by OPS size</p>
    <p>Constant value Constant value</p>
    <p>Variable value (Crucial factor for IOPS)</p>
  </div>
  <div class="page">
    <p>How to Meet SLO (IOPS) of each VM? : Dynamically adjus&lt;ng OPS</p>
    <p>SSD  state #1</p>
    <p>Data Space</p>
    <p>IOPS of VM1 = Prev. IOPS +  IOPS of VM2 = Prev. IOPS IOPS of VM3 = Prev. IOPS</p>
    <p>Data Space OPS VM1 OPS VM3</p>
    <p>Enlarged Shrunk</p>
    <p>OPS VM2</p>
    <p>SSD  state #2</p>
    <p>IOPS of state #2</p>
    <p>OPS VM1</p>
    <p>OPS VM3</p>
    <p>OPS VM2</p>
  </div>
  <div class="page">
    <p>Evalua&lt;on of OPS isola*on</p>
    <p>EvaluaOon environment  SSD simulator: DiskSim SSD Extension</p>
    <p>FTL: Page-mapped FTL  GC: Greedy policy  Aged state SSD</p>
    <p>Workloads:  Financial, MSN, and Exchange</p>
    <p>Traces are captured as VMs run concurrently on real system</p>
    <p>Host interface  Tags of VM ID are informed to SSD</p>
    <p>Parameter Description Page size 4KB Block size 512KB Page read 60us Page write 800us Block erase 1.5ms Xfer latency (Page unit)</p>
    <p>OPS 5%</p>
  </div>
  <div class="page">
    <p>Results  x-axis: groups of VMs that are executed concurrently  y-axis: proporOonality of I/O bandwidth relaOve to smallest weight</p>
    <p>Pr op</p>
    <p>or Oo</p>
    <p>na lit y of I/ O</p>
    <p>ba nd</p>
    <p>w id th</p>
    <p>VM-F (Financial) VM-M (MSN) VM-E (Exchange)</p>
    <p>Weights allotted to VMs</p>
    <p>SLO satisfied by OPS isolation</p>
    <p>(Ideal)</p>
    <p>Group of VMs</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Performance SLOs can not be saOsfied with current commer cial SSDs  Garbage collecOon interference among VMs</p>
    <p>Propose OPS isola*on, allocates flash memory blocks so that VM is isolated from other VMs  Do not allow mix of pages in same block  Size of OPS is dynamically adjusted per VM</p>
    <p>EvaluaOon showed that OPS isolaOon is an effecOve way for SSDs to provide performance SLOs to compeOng VMs</p>
  </div>
  <div class="page">
    <p>Towards SLO Complying SSDs Through OPS Isola*on</p>
    <p>Please visit our poster at tonight.</p>
    <p>Jaeho Kim (kjhnet@gmail.com, University of Seoul, Korea) Donghee Lee (University of Seoul) Sam H. Noh (Hongik University)</p>
    <p>Thank you! &amp; QuesOons?</p>
  </div>
</Presentation>
