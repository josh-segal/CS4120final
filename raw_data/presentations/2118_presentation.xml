<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Evaluation of Different Page g Allocation Strategies on High</p>
    <p>Speed SSDsSpeed SSDs Myoungsoo JungMyoungsoo Jung Mahmut Kandemir</p>
  </div>
  <div class="page">
    <p>Main insight E l iti i t l ll li i b i k Exploiting internal parallelism is becoming a key design issue in SSDs</p>
    <p>Different level parallelism Different level parallelism  SSD systems employ multiple NAND flash</p>
    <p>packages (system level)packages (system-level)  In parallel, NAND flash technologies are being</p>
    <p>developed to extract maximizing parallelism (flash-developed to extract maximizing parallelism (flash level)</p>
    <p>Internal parallelism behaviors are still unclear!!Internal parallelism behaviors are still unclear!!  simulated 24 flash page allocation strategies</p>
    <p>geared toward exploiting both system and flash g p g y level parallelism</p>
    <p>present several optimization points based on our p p p findings</p>
  </div>
  <div class="page">
    <p>Performance DisparityPerformance Disparity</p>
    <p>Hi h f i f High-performance interfaces  16GT/sec in PCI-e Gen3  6Gb/sec in SATA3</p>
    <p>NAND flash memoryNAND flash memory  Maximum bandwidth is limited by 40MB/sec</p>
    <p>sec /sec Performance Wall</p>
    <p>[Image Source: gabrielcatalano.com][Image Source: hardwareanalysis.com]</p>
  </div>
  <div class="page">
    <p>How can we reduce the performance disparity?</p>
    <p>I t l ll li d ll li iHigh-speed Interface I/O Request Group</p>
    <p>Internal parallelism and parallelizing data accesses are key</p>
    <p>Multiple Flash Packagesp g</p>
  </div>
  <div class="page">
    <p>Internals of SSD and FlashInternals of SSD and Flash Channel striping Plane sharing</p>
    <p>NAND Flash</p>
    <p>NAND Flash</p>
    <p>SSD Internals</p>
    <p>CTRL</p>
    <p>NAND Flash</p>
    <p>NAND Flash k*</p>
    <p>j oc</p>
    <p>ks</p>
    <p>CTRL</p>
    <p>NAND Flash</p>
    <p>NAND Flash</p>
    <p>k Bl</p>
    <p>o</p>
    <p>HOST</p>
    <p>Flash Flash</p>
    <p>CTRL</p>
    <p>NAND NAND Flash Flash</p>
    <p>CTRL</p>
    <p>Way pipelining Die interleaving</p>
    <p>System-Level Resources Flash-Level Resources</p>
    <p>Die interleaving</p>
  </div>
  <div class="page">
    <p>Software Stack &amp; Page AllocationSoftware Stack &amp; Page Allocation Host Interface LayerHost Interface Layer</p>
    <p>- Responsible for communication</p>
    <p>Flash Translation Layer Address translation between the host</p>
    <p>Page allocation strategies are directly related with physical data layout and access sequences, which have impact on Address translation between the host address space and physical addresses</p>
    <p>Hardware Abstraction Layer</p>
    <p>the performance and internal parallelism</p>
    <p>HIL</p>
    <p>Committing flash transaction to underlying flash memory chips</p>
    <p>FTL</p>
    <p>HAL 24</p>
    <p>Page Allocation Strategies</p>
    <p>[Image:micron.com]</p>
  </div>
  <div class="page">
    <p>Questions &amp; FindingsQuestions</p>
    <p>Whi h ll i h ld b</p>
    <p>&amp; Findings</p>
    <p>Which page allocation scheme would be globally optimal?</p>
    <p>Flash-level resource first page allocation strategies have a better position in a performance angle</p>
    <p>What are the relationship between different level concurrency methods?</p>
    <p>Channel first page allocation schemes render high flashlevel parallelism difficult</p>
    <p>What are the resource utilization of different page allocation strategies?</p>
    <p>With most of the current parallel data access methods, internal resources are significantly underutilized</p>
  </div>
  <div class="page">
    <p>Page Allocation Strategies (Palloc)Page Allocation Strategies (Palloc)</p>
    <p>Ch l fi ll Channel-first pallocs  Allocate internal resources in favor of</p>
    <p>channel striping method</p>
    <p>Way-first pallocsy p  Are oriented forward taking advantage of</p>
    <p>the way pipeliningy p p g</p>
    <p>Die-first and plane-first pallocs Allocate die and plane in an attempt to reap Allocate die and plane in an attempt to reap the benefit of flash-level parallelism</p>
  </div>
  <div class="page">
    <p>Channel first Page AllocationChannel-first Page Allocation</p>
    <p>Ch l t i i CWDP -- Channel-Way-Die-Plane</p>
    <p>Channel stripingCDPW CDWP</p>
    <p>W Pi li i</p>
    <p>CPDW CPWD CWPD Way Pipelining</p>
    <p>Plane Sharing</p>
    <p>CWPD</p>
    <p>Die Interleaving</p>
    <p>ne 0</p>
    <p>ne 0</p>
    <p>ne 1</p>
    <p>ne 1</p>
    <p>Channel first page allocation strategies introduce low</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl aChannel first page allocation strategies introduce low flash-level locality</p>
  </div>
  <div class="page">
    <p>Way first Page AllocationWay-first Page Allocation</p>
    <p>Ch l t i i Pl Sh i WDCP -- Way-Die-Channel-Plane</p>
    <p>Channel striping Plane SharingWCPD WDCP WDPC WPCD WPDC W Pi li iWPDC Way Pipelining</p>
    <p>Die Interleaving</p>
    <p>ne 0</p>
    <p>ne 0</p>
    <p>ne 1</p>
    <p>ne 1</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
  </div>
  <div class="page">
    <p>Die first Page AllocationDie-first Page Allocation</p>
    <p>Di I t l i ith M lti l DPWC -- Die-Plane-Way-Channel</p>
    <p>Die Interleaving with MultiplaneDCPW DCWP</p>
    <p>W Pi li i</p>
    <p>DPCW DWCP DWPC Way PipeliningDWPC</p>
    <p>Channel striping Die Interleaving</p>
    <p>ne 0</p>
    <p>ne 0</p>
    <p>ne 1</p>
    <p>ne 1</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
  </div>
  <div class="page">
    <p>Plane first Page AllocationPlane-first Page Allocation</p>
    <p>Di I t l i ith M lti l Plane SharingPWCD -- Plane-Way-Channel-Die</p>
    <p>Die Interleaving with MultiplanePCWD PCWD PDCW PDWC PWDC</p>
    <p>Way Pipelining</p>
    <p>PWDC</p>
    <p>Channel striping</p>
    <p>ne 0</p>
    <p>ne 0</p>
    <p>ne 1</p>
    <p>ne 1</p>
    <p>Die- and Plane-first page allocations serve transactions</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl a</p>
    <p>Pl aDie and Plane first page allocations serve transactions with high flash-level parallelism</p>
  </div>
  <div class="page">
    <p>SSD SetupSSD Setup</p>
    <p>NAND Fl h Chi NAND Flash Chip  Fine-grained NAND command  Advanced commands  Strong address constraints  Intrinsic latency variation</p>
    <p>SSD Framework  8 channels, 8 flash per channel (64 total)  Dual-die package format, 32 entry queuep g , y q  A page-level mapping and greedy garbage</p>
    <p>collection algorithm</p>
  </div>
  <div class="page">
    <p>Performance Comparison 2</p>
    <p>Performance ComparisonWay and flash-level resource first pallocs have better IOPS performance position than channel-first palloc</p>
    <p>CWDP</p>
    <p>WCPD</p>
    <p>WCPD</p>
    <p>DPCW</p>
    <p>PDCW</p>
    <p>msnfs usr fin1 web fin2 sql0 sql1 sql2 sql3</p>
    <p>[Normalized IOPS]</p>
    <p>CWDP</p>
    <p>Channel-first palloc provide shorter latencies than flashlevel resource first pallocs</p>
    <p>DPCW</p>
    <p>PDCW</p>
    <p>msnfs usr fin1 web fin2 sql0 sql1 sql2 sql3</p>
    <p>[Normalized Latency]</p>
  </div>
  <div class="page">
    <p>Parallelism Breakdown</p>
    <p>L fl h l l ll li i b d d ll</p>
    <p>Parallelism Breakdown</p>
    <p>Low flash-level parallelism is observed under palloc schemes in favor of channel</p>
    <p>They render advanced flash commandThey render advanced flash command compositions difficult at runtime (due to low flashlevel localities) High parallelism</p>
    <p>od ty</p>
    <p>pe (</p>
    <p>% )</p>
    <p>Die interleaving with multiplane write Die interleaving with multiplane read Plane sharing write Plane sharing read Die interleaving write Die interleaving read Striped lagacy write Striped lagacy read</p>
    <p>da ta</p>
    <p>a cc</p>
    <p>es m</p>
    <p>e</p>
    <p>W P W D P D W P W C P C W D W C D C P D P C D C</p>
    <p>ct io</p>
    <p>n of</p>
    <p>p ra</p>
    <p>lle l d</p>
    <p>C D</p>
    <p>P W</p>
    <p>C D</p>
    <p>W P</p>
    <p>C P</p>
    <p>D W</p>
    <p>C P</p>
    <p>W D</p>
    <p>C W</p>
    <p>D P</p>
    <p>C W</p>
    <p>P D</p>
    <p>D C</p>
    <p>P W</p>
    <p>D C</p>
    <p>W P</p>
    <p>D P</p>
    <p>C W</p>
    <p>D P</p>
    <p>W C</p>
    <p>D W</p>
    <p>C P</p>
    <p>D W</p>
    <p>P C</p>
    <p>P C</p>
    <p>D W</p>
    <p>P C</p>
    <p>W D</p>
    <p>P D</p>
    <p>C W</p>
    <p>P D</p>
    <p>W C</p>
    <p>P W</p>
    <p>C D</p>
    <p>P W</p>
    <p>D C</p>
    <p>W C</p>
    <p>D P</p>
    <p>W C</p>
    <p>P D</p>
    <p>W D</p>
    <p>C P</p>
    <p>W D</p>
    <p>P C</p>
    <p>W P</p>
    <p>C D</p>
    <p>W P</p>
    <p>D C</p>
    <p>T he</p>
    <p>fr ac</p>
    <p>Channel-first Die-first Plane-first Way-first [Parallelism breakdown]</p>
  </div>
  <div class="page">
    <p>Resource UtilizationResource Utilization e msnfs</p>
    <p>ce nt</p>
    <p>ag e</p>
    <p>of c</p>
    <p>ha nn</p>
    <p>at io</p>
    <p>n (%</p>
    <p>)</p>
    <p>msnfs usr fin1 web fin2 sql0 sql1</p>
    <p>Channel-first palloc Die-first palloc Plane-first palloc Way-first palloc</p>
    <p>channel resources CD</p>
    <p>PW CD</p>
    <p>WP CPD</p>
    <p>W CPW</p>
    <p>D CW</p>
    <p>DP CW</p>
    <p>PD DC</p>
    <p>PW DC</p>
    <p>WP DPC</p>
    <p>W DPW</p>
    <p>C DW</p>
    <p>CP DW</p>
    <p>PC PCD</p>
    <p>W PCW</p>
    <p>D PDC</p>
    <p>W PDW</p>
    <p>C PW</p>
    <p>CD PW</p>
    <p>DC WC</p>
    <p>DP WC</p>
    <p>PD WD</p>
    <p>CP WD</p>
    <p>PC WP</p>
    <p>CD WP</p>
    <p>DC 0</p>
    <p>P er</p>
    <p>c ut</p>
    <p>ili z sql2</p>
    <p>sql3 [Average Channel Utilization]</p>
    <p>are utilized about 43.1% on average with most parallel data access methods</p>
    <p>Idl ti Idle time  About 80% of the total execution time are spent idle</p>
    <p>e fr</p>
    <p>ac tio</p>
    <p>n (%</p>
    <p>) Idle Flash-level conflict Bus contention Bus activate Flash cell activate</p>
    <p>fr ac</p>
    <p>tio n</p>
    <p>(% )</p>
    <p>C D</p>
    <p>P W</p>
    <p>C D</p>
    <p>W P</p>
    <p>C P</p>
    <p>D W</p>
    <p>C P</p>
    <p>W D</p>
    <p>C W</p>
    <p>D P</p>
    <p>C W</p>
    <p>P D</p>
    <p>D C</p>
    <p>P W</p>
    <p>D C</p>
    <p>W P</p>
    <p>D P</p>
    <p>C W</p>
    <p>D P</p>
    <p>W C</p>
    <p>D W</p>
    <p>C P</p>
    <p>D W</p>
    <p>P C</p>
    <p>P C</p>
    <p>D W</p>
    <p>P C</p>
    <p>W D</p>
    <p>P D</p>
    <p>C W</p>
    <p>P D</p>
    <p>W C</p>
    <p>P W</p>
    <p>C D</p>
    <p>P W</p>
    <p>D C</p>
    <p>W C</p>
    <p>D P</p>
    <p>W C</p>
    <p>P D</p>
    <p>W D</p>
    <p>C P</p>
    <p>W D</p>
    <p>P C</p>
    <p>W P</p>
    <p>C D</p>
    <p>W P</p>
    <p>D C</p>
    <p>E xe</p>
    <p>. t im</p>
    <p>C D</p>
    <p>P W</p>
    <p>C D</p>
    <p>W P</p>
    <p>C P</p>
    <p>D W</p>
    <p>C P</p>
    <p>W D</p>
    <p>C W</p>
    <p>D P</p>
    <p>C W</p>
    <p>P D</p>
    <p>D C</p>
    <p>P W</p>
    <p>D C</p>
    <p>W P</p>
    <p>D P</p>
    <p>C W</p>
    <p>D P</p>
    <p>W C</p>
    <p>D W</p>
    <p>C P</p>
    <p>D W</p>
    <p>P C</p>
    <p>P C</p>
    <p>D W</p>
    <p>P C</p>
    <p>W D</p>
    <p>P D</p>
    <p>C W</p>
    <p>P D</p>
    <p>W C</p>
    <p>P W</p>
    <p>C D</p>
    <p>P W</p>
    <p>D C</p>
    <p>W C</p>
    <p>D P</p>
    <p>W C</p>
    <p>P D</p>
    <p>W D</p>
    <p>C P</p>
    <p>W D</p>
    <p>P C</p>
    <p>W P</p>
    <p>C D</p>
    <p>W P</p>
    <p>D C</p>
    <p>E xe</p>
    <p>. t im</p>
    <p>e</p>
    <p>[Write Intensive -- Execution breakdown] [Read Intensive -- Execution breakdown]</p>
  </div>
  <div class="page">
    <p>Optimization PointOptimization Point</p>
    <p>CWDP IDEALAvoiding resource conflicts</p>
    <p>PWCD DPWC ou</p>
    <p>rc e</p>
    <p>DPWC</p>
    <p>Flash-level parallelism</p>
    <p>zi n g r</p>
    <p>es o</p>
    <p>liz at</p>
    <p>io n</p>
    <p>M ax</p>
    <p>im iz</p>
    <p>i u ti l</p>
    <p>WDCP</p>
    <p>M</p>
  </div>
  <div class="page">
    <p>Future workFuture work</p>
    <p>Future work  Incorporating a high-speed flash interface co po at g a g speed as te ace</p>
    <p>(e.g., 400MHz)  Further evaluating with varying parametersFurther evaluating with varying parameters</p>
    <p>(e.g., different queue/buffer management flash firmware varying internalmanagement, flash firmware, varying internal resource parameters) More diverse workloads including micro More diverse workloads including microbenchmark</p>
  </div>
  <div class="page">
    <p>ConclusionConclusion</p>
    <p>schemes have better performance</p>
  </div>
  <div class="page">
    <p>Q&amp;A</p>
    <p>Acknowledgements  Umesh Maheshwari (our shepherd)Umesh Maheshwari (our shepherd)  Ellis H. Wilson (Pennsylvania State Univ.)  John Shalf (Lawrence Berkley National Lab.)( y )</p>
  </div>
</Presentation>
