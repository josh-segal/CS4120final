<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MODI: Mobile Deep Inference</p>
    <p>Samuel S. Ogden Tian Guo</p>
    <p>Made Efficient by Edge Computing</p>
  </div>
  <div class="page">
    <p>Sneak Peek</p>
    <p>Aim is to be a dynamic solution to a dynamic problem that has previously been solved statically</p>
    <p>ssogden@WPI.EDU-MODI2</p>
  </div>
  <div class="page">
    <p>Background  Mobile Inference</p>
    <p>Using deep learning models in mobile application  Increasingly common to use deep learning models within mobile</p>
    <p>applications  Image recognition  Speech recognition</p>
    <p>ssogden@wpi.edu - MODI3</p>
    <p>Two major metrics  Model Accuracy  End-to-end latency</p>
    <p>On-device vs. Remote inference</p>
  </div>
  <div class="page">
    <p>Mobile Deep Inference Limitations</p>
    <p>Highly constrained resource models  Battery constraints  Lack of limited hardware in general case</p>
    <p>Highly dynamic environment  Variable network conditions</p>
    <p>Common approaches are statically applied  Choosing a one-size-fits-all model for on-device inference  Using the same remote API for all inference requests</p>
    <p>ssogden@wpi.edu - MODI4</p>
  </div>
  <div class="page">
    <p>Our Vision: MODI</p>
    <p>How do we balance accuracy and latency based on dynamic constraints?</p>
    <p>Provide a wide array of models  Model-usage families and derived models</p>
    <p>Dynamically choose inference location and model  Make decision based on inference environment</p>
    <p>e.g., network, power, model availability</p>
    <p>Make the choice transparent</p>
    <p>ssogden@wpi.edu - MODI5</p>
  </div>
  <div class="page">
    <p>MODI: System design</p>
    <p>MODI Models &amp; Stats</p>
    <p>MODI On-device Inference</p>
    <p>MODI Remote inference</p>
    <p>Mobile Device Edge Servers</p>
    <p>Central Manager</p>
    <p>Requests Response</p>
    <p>Metadata</p>
    <p>M et</p>
    <p>ad at</p>
    <p>a</p>
    <p>M od</p>
    <p>el s</p>
  </div>
  <div class="page">
    <p>MODI: System design</p>
    <p>MODI Models &amp; Stats</p>
    <p>MODI On-device Inference</p>
    <p>MODI Remote inference</p>
    <p>Mobile Device Edge Servers</p>
    <p>Central Manager</p>
    <p>Requests Response</p>
    <p>Metadata</p>
    <p>M et</p>
    <p>ad at</p>
    <p>a</p>
    <p>M od</p>
    <p>el s</p>
  </div>
  <div class="page">
    <p>MODI: System design</p>
    <p>MODI Models &amp; Stats</p>
    <p>MODI On-device Inference</p>
    <p>MODI Remote inference</p>
    <p>Mobile Device Edge Servers</p>
    <p>Central Manager</p>
    <p>Requests Response</p>
    <p>Metadata</p>
    <p>M et</p>
    <p>ad at</p>
    <p>a</p>
    <p>M od</p>
    <p>el s</p>
  </div>
  <div class="page">
    <p>Design Principles</p>
    <p>Maximize usage of on-device resources</p>
    <p>Storage and analysis of metadata</p>
    <p>Dynamic model selection</p>
    <p>ssogden@wpi.edu - MODI9</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Which compression techniques are useful?</p>
    <p>Which model versions to store where?</p>
    <p>When to offload to edge servers?</p>
    <p>ssogden@wpi.edu - MODI10</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Which compression techniques are useful?</p>
    <p>Which model versions to store where?</p>
    <p>When to offload to edge servers?</p>
    <p>ssogden@wpi.edu - MODI11</p>
  </div>
  <div class="page">
    <p>Results  Model Compression</p>
    <p>Storage requirements reduced by 75% for quantized models</p>
    <p>InceptionV3 image classification model1 running on a Google Pixel2 device</p>
  </div>
  <div class="page">
    <p>Results  Model Compression</p>
    <p>Load time reduced by up to 66%  Leads to ~6% reduction in accuracy</p>
    <p>InceptionV3 image classification model1 running on a Google Pixel2 device</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Which compression techniques are useful?  Quantization and gzip significantly reduce model size</p>
    <p>Which model versions to store where?</p>
    <p>When to offload to edge servers?</p>
    <p>ssogden@wpi.edu - MODI14</p>
  </div>
  <div class="page">
    <p>Results  Model Comparison across devices</p>
    <p>Pixel2 over 2.5x faster than older devices  Specialized deep-learning hardware</p>
    <p>InceptionV3 image classification model optimized for inference</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Which compression techniques are useful?  Quantization and gzip significantly reduce model size</p>
    <p>Which model versions to store where?  Mobile devices can reduce runtime up to 2.4x</p>
    <p>When to offload to edge servers?</p>
    <p>ssogden@wpi.edu - MODI16</p>
  </div>
  <div class="page">
    <p>Results  Inference Offloading Feasibility</p>
    <p>Network transfer is up to 66.7% of end-toend time</p>
    <p>Used AWS t2.medium instance running InceptionV3</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Which compression techniques are useful?  Quantization and gzip significantly reduce model size</p>
    <p>Which model versions to store where?  Mobile devices can reduce runtime up to 2.4x</p>
    <p>When to offload to edge servers?  Slower networks would hinder remote inference</p>
    <p>ssogden@wpi.edu - MODI18</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Questions</p>
    <p>Key points:  MODI allows for dynamic mobile inference model selection through post</p>
    <p>training model management  Enables greater flexibility for mobile deep inference</p>
    <p>Controversial:  Whether using a low-tier AWS instance is similar to edge</p>
    <p>Looking forward:  Integrating MODI with existing deep learning frameworks  Explore explicit trade-off points between on-device and remote inference  Exploring how far in the edge is ideal for remote inference  What other devices could this be used for?</p>
    <p>ssogden@wpi.edu - MODI19</p>
  </div>
  <div class="page"/>
</Presentation>
