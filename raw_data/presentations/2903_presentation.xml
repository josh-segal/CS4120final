<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Domain Adaptation  by Constraining Inter-Domain Variability</p>
    <p>of Latent Feature Representation Ivan Titov</p>
    <p>Saarland University</p>
  </div>
  <div class="page">
    <p>Domain Adaptation</p>
    <p>} Standard supervised learning techniques assume that the test data originates from the same distribution as the learning data</p>
    <p>}  but this is often not the case</p>
    <p>} e.g., sentiment classification:</p>
    <p>} The classification accuracy is normally very significantly affected</p>
    <p>} Domain-adaptation methods address this problem by exploiting small amounts of labeled data or/and unlabeled data from the target domain</p>
    <p>Train on hotel reviews (Source domain)</p>
    <p>Test on movie reviews (Target domain)</p>
  </div>
  <div class="page">
    <p>Exploiting unlabeled data from the target domain</p>
    <p>} Inducing shared feature representation</p>
    <p>} Map examples to a new representation such that examples from both domains look similar</p>
    <p>} Learn a classifier on the source domain in the new representation</p>
    <p>} The most well-known approach of this type: Structural Correspondence Learning (SCL, Blitzer et al., 2006):</p>
    <p>} Engineer auxiliary sub-tasks on the basis of unlabeled data from both domains</p>
    <p>} Apply a dimensionality reduction technique to induce the shared representation</p>
    <p>} Our approach:</p>
    <p>Learn a latent variable model on the data from both domains  Ensure that latent variables have a similar posterior distribution on both</p>
    <p>domains</p>
    <p>No need for auxiliary tasks, potentially easy to apply to any task</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Latent Variable Model</p>
    <p>} Constraints on Inter-Domain Variability</p>
    <p>} Learning and Inference</p>
    <p>} Evaluation</p>
  </div>
  <div class="page">
    <p>Latent Variable Model</p>
    <p>} A combination of an undirected Harmonium model (Smolensky, 1986) and a directed Sigmoid Belief Network (SBN, Saul et al., 1996):</p>
    <p>} The generative story</p>
    <p>} Jointly draw the latent and observable features</p>
    <p>} Draw the label conditioned on latent features</p>
    <p>Observable binary features</p>
    <p>Latent feature representation</p>
    <p>Predicted label</p>
    <p>(x, z)  P(x, z|v)</p>
    <p>y  (w0 + m</p>
    <p>i=1 wizi)</p>
    <p>The logistic regression (t)= 1</p>
    <p>(1+et)</p>
    <p>Harmonium model: P(x, z|v)  exp(</p>
    <p>i,j vxjizi)</p>
    <p>Undirected part, parameterized by v</p>
    <p>Essentially a linear classifier, parameterized by w</p>
    <p>both encodes dependencies between the observable feature components and dependencies between and the label</p>
    <p>z xj</p>
    <p>y</p>
    <p>...</p>
    <p>...</p>
    <p>y</p>
    <p>z</p>
    <p>x</p>
    <p>} }</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Learning the model</p>
    <p>} Data:</p>
    <p>} labeled data from the source domain</p>
    <p>} unlabeled data from the source domain</p>
    <p>} unlabeled data from the target domain</p>
    <p>} A hybrid (multi-conditional) objective:</p>
    <p>Balancing the discriminative objective (mapping should be predictive of the label) and the generative objective (explaining the unlabeled data well)</p>
    <p>Conditional likelihood</p>
    <p>{x(l), y(l)}lSL {x(l)}lSU {x(l)}lTU</p>
    <p>L(, ) =</p>
    <p>lSLlog P(y (l)|x(l), ) +</p>
    <p>lSU TU SL log P(x</p>
    <p>(l)|)</p>
    <p>Likelihood of the unlabeled data, (forces clustering of the features)</p>
    <p>Encodes importance of the smaller labeled dataset when</p>
    <p>All the model parameters  = (vT , wT )T</p>
    <p>&gt; 1</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>} The danger: inducing domain-specific features (i.e. clustering within domains instead of across domains)</p>
    <p>} When adapting from reviews of Electronics to reviews of Hotels it can induce features: reliability vs. cleanliness</p>
    <p>} The cleanliness feature will fire only on the unlabeled target data</p>
    <p>} This important feature will never be used in learning the classifier (discriminative part) and the corresponding weight will be zero</p>
    <p>} The resulting accuracy on the target domain will be mediocre</p>
    <p>w</p>
    <p>We do not want to induce a model which has features specific to individual domains: we enforce that the frequency of each feature is similar across the domains</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Latent Variable Model</p>
    <p>} Constraints on Inter-Domain Variability</p>
    <p>} Learning and Inference</p>
    <p>} Evaluations</p>
  </div>
  <div class="page">
    <p>Connecting to the theoretical analysis</p>
    <p>} We can write the probability mass assigned to examples where classifiers and disagree under the distribution as</p>
    <p>} The drop in accuracy due to the transfer from the distribution to is upper-bounded by the discrepancy distance (Blitzer et al., 2007):</p>
    <p>i.e. the maximal change in the size of the disagreement set</p>
    <p>} For more restricted classifiers which consider only a single feature such distance will be equal to</p>
    <p>} As the family of linear classifiers includes all the restricted classifiers:</p>
    <p>dz(S,T) = maxf,f|EPS [f(z)=f(z)]  EPT [f(z)=f(z)]|</p>
    <p>PS PT</p>
    <p>EP [f(z) = f(z)] f</p>
    <p>f P</p>
    <p>dz(S,T)  maxi=1,...,m |EPS [zi = 1]  EPT [zi = 1]|</p>
    <p>Minimizing the difference in the marginal distribution of latent variables can be regarded as a coarse approximation to minimization of the discrepancy distance</p>
    <p>i |EPS (zi = 1)EPT (zi = 1)|</p>
  </div>
  <div class="page">
    <p>The expectation criterion</p>
    <p>} The sample estimates of marginal distributions of latent variables under the source and target distributions: and , e.g. :</p>
    <p>} Our goal is to penalize differences in the marginal distributions</p>
    <p>} The penalty should not be very sensitive to small changes but penalize severely extreme differences in the marginal distributions</p>
    <p>} Instead of the uniform ( ) norm motivated above we use the (symmetrized) Kullback-Leibler divergence as the regularizer:</p>
    <p>PS(zi|) PT (zi|) PT (zi = 1|) = 1|TU |</p>
    <p>lTU P(zi = 1|x</p>
    <p>(l), )</p>
    <p>L</p>
    <p>G() = m</p>
    <p>i=1 D(PS(zi|)||PT (zi|)) + D(PT (zi|)||PS(zi|)) The KL-divergence penalizes extreme difference stronger than the smoother Jensen-Shannon divergence</p>
    <p>The resulting composite objective is LR(, , ) = L(, )  G(),  &gt; 0</p>
    <p>Can be regarded as an Expectation Criterion (Mann and McCallum, 2010)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Latent Variable Model</p>
    <p>} Constraints on Inter-Domain Variability</p>
    <p>} Learning and Inference</p>
    <p>} Evaluation</p>
  </div>
  <div class="page">
    <p>Estimating model parameters</p>
    <p>} The learning objective:</p>
    <p>} A basic mean-field approximation to estimate all the terms</p>
    <p>} The stochastic gradient descent algorithm</p>
    <p>} The contribution of an example to the gradient of each of the three terms needs to be estimated</p>
    <p>LR(, , ) =</p>
    <p>lSL</p>
    <p>log P(y(l)|x(l), ) +</p>
    <p>lSU TU SL</p>
    <p>log P(x(l)|  G()</p>
    <p>Conditional Likelihood Unlabeled Likelihood Regularizer</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>...</p>
    <p>...</p>
    <p>Conditional Likelihood Term</p>
    <p>} The term:</p>
    <p>} A basic mean-field approximation:</p>
    <p>} Means of latent variables:</p>
    <p>} The probability of the label:</p>
    <p>lSL</p>
    <p>log P(y(l)|x(l), )</p>
    <p>i = P(zi = 1|x, v) =(v0i + |x|</p>
    <p>j=1</p>
    <p>vxji).</p>
    <p>the logistic sigmoid function P(y = 1|x, )=</p>
    <p>z</p>
    <p>P(y|z, w)P(z|x, v)  (w0 + m</p>
    <p>i=1</p>
    <p>wii).</p>
    <p>Equivalent to the computation of the two-layer perceptron and therefore the standard back-propagation algorithm can be used to compute the gradient  log P(y(l)|x(l), )</p>
    <p>z</p>
    <p>y</p>
    <p>x</p>
    <p>v</p>
    <p>w</p>
    <p>Assume binary classification</p>
    <p>} }</p>
  </div>
  <div class="page">
    <p>...</p>
    <p>...</p>
    <p>Unlabeled Likelihood Term</p>
    <p>} The term:</p>
    <p>} We use a deterministic version of the Contrastive Divergence (CD) estimation (Bengio and Delalleau, 2007) = the mean-field approximation to reconstruction error</p>
    <p>} Learn such a mapping that after predicting from some input you can reconstruct from this by applying the reverse mapping</p>
    <p>} Intuitively, similar to the likelihood estimation (in fact, a biased estimator)</p>
    <p>} A basic mean-field approximation:</p>
    <p>} As before, means of latent variables:</p>
    <p>} The reconstruction error approximation</p>
    <p>The gradient can be again computed efficiently with back-propagation</p>
    <p>i = P(zi = 1|x, v) =(v0i + |x|</p>
    <p>j=1</p>
    <p>vxji).</p>
    <p>v</p>
    <p>Irrelevant part</p>
    <p>x z xz</p>
    <p>log P(x(l)|)  log P(x(l)|, v)</p>
    <p>lSU TU SL</p>
    <p>log P(x(l)|)</p>
    <p>vlog P(x(l)|, v)</p>
    <p>vT</p>
    <p>z</p>
    <p>y</p>
    <p>x</p>
    <p>v}</p>
  </div>
  <div class="page">
    <p>} The term:</p>
    <p>} where and are sample-based domain-specific marginal distributions:</p>
    <p>} The gradient of the contribution of a given example :</p>
    <p>where and if is from the source domain,</p>
    <p>otherwise, and</p>
    <p>} Formally, and should be recomputed after every update, instead we use an amortization technique</p>
    <p>Regularization term</p>
    <p>The constraint is easy to integrate in the learning algorithm</p>
    <p>G() = m</p>
    <p>i=1 D(PS(zi|)||PT (zi|)) + D(PT (zi|)||PS(zi|))</p>
    <p>PS PT</p>
    <p>PT (zi = 1|) = 1</p>
    <p>|TU|</p>
    <p>lTU</p>
    <p>(l) i</p>
    <p>l</p>
    <p>p = PS(zi = 1|) p = PT (zi = 1|) l</p>
    <p>p = PT (zi = 1|) p = PS(zi = 1|)</p>
    <p>PS(zi = 1|) = 1</p>
    <p>|SU  SL|</p>
    <p>lSU SL</p>
    <p>(l) i</p>
    <p>PS(zi = 1|) PT (zi = 1|)</p>
    <p>For the Jensen-Shannon divergence these terms would be missing</p>
    <p>dG(l)(v)</p>
    <p>dvki =(log</p>
    <p>p</p>
    <p>p log</p>
    <p>p</p>
    <p>p +</p>
    <p>(l) i</p>
    <p>dvki ,</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>} Latent Variable Model</p>
    <p>} Constraints on Inter-Domain Variability</p>
    <p>} Learning and Inference</p>
    <p>} Evaluation</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>} The multi-domain sentiment classification dataset (Blitzer et al., 2007)</p>
    <p>} Domains: reviews of books, DVDs, electronics and kitchen appliances</p>
    <p>} Each of the 4 domains:</p>
    <p>} 1000 labeled positive examples + 1000 labeled negative examples</p>
    <p>} unlabeled data: from 3,685 reviews for DVDs to 5,945 reviews for kitchen appliances</p>
    <p>} Split: } random 1600 examples for training and 400 for testing</p>
    <p>} the same proportions as used for the evaluation of SCL (Blitzer et al., 2007) but the actual split may be different</p>
    <p>} Features:</p>
    <p>} unigram and bigram, frequency cut-off of 30</p>
    <p>Does not seem to adversely affect the accuracy and reduced the learning time (to around 20 minutes)</p>
    <p>For every experiment we use all the source data and unlabeled target data for training</p>
  </div>
  <div class="page">
    <p>Parameters and models</p>
    <p>} Parameters of the regularized (Reg) and not regularized versions (NoReg)</p>
    <p>} The latent representation size is 10</p>
    <p>} a Gaussian prior on weights and</p>
    <p>} For details about the parameters and how they are set see the paper</p>
    <p>} Baseline</p>
    <p>} The model trained on the labeled data from the source domain not using any data from the target domain (Base)</p>
    <p>} Combined (product of experts) both Reg and Base (Reg+), and NoReg and Base (NoReg+)</p>
    <p>} Analogous to SCL (Blitzer et al, 2007) where the initial and shared representations are combined</p>
    <p>} Upper-bound</p>
    <p>} The supervised model trained on the target-domain data (InDomain)</p>
    <p>wv</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>} Average results accuracies when transferring to books, DVDs, electronics and kitchen appliances domains + average over all 12 domain pairs</p>
    <p>} Reg &gt; NoReg in all cases } Reg+ and NoReg+ &gt;&gt; Base } Reg+ &gt; NoReg+</p>
    <p>Base NoReg</p>
    <p>Reg</p>
    <p>Reg+ In-domain</p>
    <p>NoReg+</p>
    <p>Books DVDs Electronics Kitchens Average</p>
  </div>
  <div class="page">
    <p>Drops in accuracy due to transfer</p>
    <p>Absolute drop due to the domain transfer: } computed as InDomain  Model</p>
    <p>} Relative reduction of the drop is 35% (absolute reduction 8.9  5.7 = 3.2%)</p>
    <p>} Absolute results for our method (Reg+) are better than for SCL, reported in (Blitzer et al., 2007): 75.6% vs. 74.5% } the drop for SCL-MI is computed with respect to InDomain reported in Blitzer et al. (07)</p>
    <p>to Domain Base NoReg Reg NoReg+ Reg+ SCL-MI</p>
    <p>Books 10.6 12.4 7.7 8.6 6.7 5.8</p>
    <p>DVDs 9.5 8.2 8.0 6.6 7.3 8.1</p>
    <p>Electronics 8.2 13.0 9.7 6.8 5.5 5.5</p>
    <p>Kitchen 7.5 8.8 6.5 4.4 3.3 5.6</p>
    <p>Average 8.9 10.6 8.0 6.6 5.7 5.8</p>
    <p>The best version of SCL</p>
    <p>Achieves comparable results with SCL without using any auxiliary features</p>
  </div>
  <div class="page">
    <p>Conclusions and Discussion</p>
    <p>} A new method for domain-adaptation: semi-supervised learning with a latent variable model and constraints on inter-domain variability of the latent representations</p>
    <p>} effective on the sentiment classification task</p>
    <p>} Potentially applicable to arbitrary models with distributed configuration</p>
    <p>} (recurrent) neural networks, factorial HMMs, (incremental) sigmoid belief networks, (temporal) restricted Boltzmann machines</p>
    <p>} these methods have shown state-of-the-art results on a number of NLP problems: document classification, dependency and constituent parsing, semantic role labeling,</p>
    <p>} (See also Garg and Henderson talk tomorrow)</p>
  </div>
</Presentation>
