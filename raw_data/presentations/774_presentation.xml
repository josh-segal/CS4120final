<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cheap and Large CAMs for High Performance Data-Intensive</p>
    <p>Networked Systems</p>
    <p>Ashok Anand, Chitra Muthukrishnan, Steven Kappes, and Aditya Akella</p>
    <p>University of Wisconsin-Madison</p>
    <p>Suman Nath Microsoft Research</p>
  </div>
  <div class="page">
    <p>New data-intensive networked systems</p>
    <p>Large hash tables (10s to 100s of GBs)</p>
  </div>
  <div class="page">
    <p>New data-intensive networked systems</p>
    <p>Data center Branch office</p>
    <p>WAN</p>
    <p>WAN optimizers Object</p>
    <p>Object store (~4 TB) Hashtable (~32GB)</p>
    <p>Look up</p>
    <p>Object</p>
    <p>Chunks(4 KB)</p>
    <p>Key (20 B)</p>
    <p>Chunk pointer Large hash tables (32 GB)</p>
    <p>High speed (~10 K/sec) inserts and evictions</p>
    <p>High speed (~10K/sec) lookups for 500 Mbps link</p>
  </div>
  <div class="page">
    <p>New data-intensive networked systems</p>
    <p>Other systems  De-duplication in storage systems (e.g., Datadomain)</p>
    <p>CCN cache (Jacobson et al., CONEXT 2009)</p>
    <p>DONA directory lookup (Koponen et al., SIGCOMM 2006)</p>
    <p>Cost-effective large hash tables</p>
    <p>Cheap Large cAMs</p>
  </div>
  <div class="page">
    <p>Candidate options</p>
    <p>DRAM 300K $120K+</p>
    <p>Disk 250 $30+</p>
    <p>Random reads/sec</p>
    <p>Cost (128 GB)</p>
    <p>Flash-SSD 10K* $225+</p>
    <p>Random writes/sec</p>
    <p>Too slow Too</p>
    <p>expensive</p>
    <p>* Derived from latencies on Intel M-18 SSD in experiments</p>
    <p>Slow writes</p>
    <p>How to deal with slow writes of Flash SSD</p>
    <p>+Price statistics from 2008-09</p>
  </div>
  <div class="page">
    <p>Our CLAM design</p>
    <p>New data structure BufferHash + Flash</p>
    <p>Key features</p>
    <p>Avoid random writes, and perform sequential writes in a batch</p>
    <p>Sequential writes are 2X faster than random writes (Intel SSD)</p>
    <p>Batched writes reduce the number of writes going to Flash</p>
    <p>Bloom filters for optimizing lookups</p>
    <p>BufferHash performs orders of magnitude better than DRAM based traditional hash tables in ops/sec/$</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and motivation</p>
    <p>CLAM design</p>
    <p>Key operations (insert, lookup, update)</p>
    <p>Eviction</p>
    <p>Latency analysis and performance tuning</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Flash/SSD primer</p>
    <p>Random writes are expensive</p>
    <p>Avoid random page writes</p>
    <p>Reads and writes happen at the granularity of a flash page</p>
    <p>I/O smaller than page should be avoided, if possible</p>
  </div>
  <div class="page">
    <p>Conventional hash table on Flash/SSD</p>
    <p>Flash</p>
    <p>Keys are likely to hash to random locations</p>
    <p>Random writes</p>
    <p>SSDs: FTL handles random writes to some extent; But garbage collection overhead is high</p>
    <p>~200 lookups/sec and ~200 inserts/sec with WAN optimizer workload, &lt;&lt; 10 K/s and 5 K/s</p>
  </div>
  <div class="page">
    <p>Conventional hash table on Flash/SSD</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Cant assume locality in requests  DRAM as cache wont work</p>
  </div>
  <div class="page">
    <p>Our approach: Buffering insertions</p>
    <p>Control the impact of random writes  Maintain small hash table (buffer) in memory  As in-memory buffer gets full, write it to flash</p>
    <p>We call in-flash buffer, incarnation of buffer</p>
    <p>Incarnation: In-flash hash table</p>
    <p>Buffer: In-memory hash table</p>
    <p>DRAM Flash SSD</p>
  </div>
  <div class="page">
    <p>Two-level memory hierarchy</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Incarnation</p>
    <p>Net hash table is: buffer + all incarnations</p>
    <p>Oldest incarnation</p>
    <p>Latest incarnation</p>
  </div>
  <div class="page">
    <p>Lookups are impacted due to buffers</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Lookup key</p>
    <p>In-flash look ups</p>
    <p>Multiple in-flash lookups. Can we limit to only one?</p>
  </div>
  <div class="page">
    <p>Bloom filters for optimizing lookups</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Lookup key</p>
    <p>Bloom filters</p>
    <p>In-memory look ups</p>
    <p>False positive!</p>
    <p>Configure carefully! 4 3 2 1</p>
  </div>
  <div class="page">
    <p>Update: nave approach</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Bloom filters</p>
    <p>Update key</p>
    <p>Update key Expensive random writes</p>
    <p>Discard this nave approach</p>
  </div>
  <div class="page">
    <p>Lazy updates</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Bloom filters</p>
    <p>Update key</p>
    <p>Insert key</p>
    <p>Lookups check latest incarnations first</p>
    <p>Key, new value</p>
    <p>Key, old value</p>
  </div>
  <div class="page">
    <p>Eviction for streaming apps</p>
    <p>Eviction policies may depend on application  LRU, FIFO, Priority based eviction, etc.</p>
    <p>Two BufferHash primitives  Full Discard: evict all items</p>
    <p>Naturally implements FIFO</p>
    <p>Partial Discard: retain few items  Priority based eviction by retaining high priority items</p>
    <p>BufferHash best suited for FIFO  Incarnations arranged by age  Other useful policies at some additional cost</p>
    <p>Details in paper</p>
  </div>
  <div class="page">
    <p>Issues with using one buffer</p>
    <p>Single buffer in DRAM  All operations and</p>
    <p>eviction policies</p>
    <p>High worst case insert latency  Few seconds for 1</p>
    <p>GB buffer</p>
    <p>New lookups stall</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer</p>
    <p>Incarnation table</p>
    <p>Bloom filters</p>
  </div>
  <div class="page">
    <p>Partitioning buffers</p>
    <p>Partition buffers  Based on first few bits</p>
    <p>of key space  Size &gt; page</p>
    <p>Avoid i/o less than page</p>
    <p>Size &gt;= block  Avoid random page</p>
    <p>writes</p>
    <p>Reduces worst case latency</p>
    <p>Eviction policies apply per buffer</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Incarnation table</p>
  </div>
  <div class="page">
    <p>BufferHash: Putting it all together</p>
    <p>Multiple buffers in memory</p>
    <p>Multiple incarnations per buffer in flash</p>
    <p>One in-memory bloom filter per incarnation</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Buffer 1 Buffer K . .</p>
    <p>. .</p>
    <p>Net hash table = all buffers + all incarnations</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and motivation</p>
    <p>Our CLAM design</p>
    <p>Key operations (insert, lookup, update)</p>
    <p>Eviction</p>
    <p>Latency analysis and performance tuning</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Latency analysis</p>
    <p>Insertion latency</p>
    <p>Worst case size of buffer</p>
    <p>Average case is constant for buffer &gt; block size</p>
    <p>Lookup latency</p>
    <p>Average case Number of incarnations</p>
    <p>Average case False positive rate of bloom filter</p>
  </div>
  <div class="page">
    <p>Parameter tuning: Total size of Buffers</p>
    <p>.</p>
    <p>. .</p>
    <p>.</p>
    <p>Total size of buffers = B1 + B2 +  + BN</p>
    <p>Too small is not optimal Too large is not optimal either Optimal = 2 * SSD/entry</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>Given fixed DRAM, how much allocated to buffers</p>
    <p>B1 BN</p>
    <p># Incarnations = (Flash size/Total buffer size)</p>
    <p>Lookup #Incarnations * False positive rate</p>
    <p>False positive rate increases as the size of bloom filters decrease</p>
    <p>Total bloom filter size = DRAM  total size of buffers</p>
  </div>
  <div class="page">
    <p>Parameter tuning: Per-buffer size</p>
    <p>Affects worst case insertion</p>
    <p>What should be size of a partitioned buffer (e.g. B1) ?</p>
    <p>.</p>
    <p>. .</p>
    <p>.</p>
    <p>DRAM</p>
    <p>Flash</p>
    <p>B1 BN</p>
    <p>Adjusted according to application requirement (128 KB  1 block)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background and motivation</p>
    <p>Our CLAM design</p>
    <p>Key operations (insert, lookup, update)</p>
    <p>Eviction</p>
    <p>Latency analysis and performance tuning</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Configuration</p>
    <p>4 GB DRAM, 32 GB Intel SSD, Transcend SSD</p>
    <p>2 GB buffers, 2 GB bloom filters, 0.01 false positive rate</p>
    <p>FIFO eviction policy</p>
  </div>
  <div class="page">
    <p>BufferHash performance</p>
    <p>WAN optimizer workload  Random key lookups followed by inserts</p>
    <p>Hit rate (40%)</p>
    <p>Used workload from real packet traces also</p>
    <p>Comparison with BerkeleyDB (traditional hash table) on Intel SSD</p>
    <p>Average latency BufferHash BerkeleyDB</p>
    <p>Look up (ms) 0.06 4.6</p>
    <p>Insert (ms) 0.006 4.8</p>
    <p>Better lookups!</p>
    <p>Better inserts!</p>
  </div>
  <div class="page">
    <p>Insert performance</p>
    <p>BerkeleyDB</p>
    <p>Bufferhash</p>
    <p>CDF</p>
    <p>Insert latency (ms) on Intel SSD</p>
    <p>Random writes are slow! Buffering effect!</p>
  </div>
  <div class="page">
    <p>Lookup performance</p>
    <p>Bufferhash</p>
    <p>BerkeleyDB</p>
    <p>CDF</p>
    <p>Garbage collection overhead due to writes!</p>
    <p>Lookup latency (ms) for 40% hit workload</p>
  </div>
  <div class="page">
    <p>Performance in Ops/sec/$</p>
    <p>16K lookups/sec and 160K inserts/sec</p>
    <p>Overall cost of $400</p>
    <p>42 lookups/sec/$ and 420 inserts/sec/$</p>
    <p>Orders of magnitude better than 2.5 ops/sec/$ of DRAM based hash tables</p>
  </div>
  <div class="page">
    <p>Other workloads</p>
    <p>Varying fractions of lookups</p>
    <p>Results on Trancend SSD</p>
    <p>Lookup fraction BufferHash BerkeleyDB</p>
    <p>BufferHash ideally suited for write intensive workloads</p>
    <p>Average latency per operation</p>
  </div>
  <div class="page">
    <p>Evaluation summary</p>
    <p>BufferHash performs orders of magnitude better in ops/sec/$ compared to traditional hashtables on DRAM (and disks)</p>
    <p>BufferHash is best suited for FIFO eviction policy  Other policies can be supported at additional cost, details</p>
    <p>in paper</p>
    <p>WAN optimizer using Bufferhash can operate optimally at 200 Mbps, much better than 10 Mbps with BerkeleyDB  Details in paper</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>FAWN (Vasudevan et al., SOSP 2009)</p>
    <p>Cluster of wimpy nodes with flash storage</p>
    <p>Each wimpy node has its hash table in DRAM</p>
    <p>We target</p>
    <p>Hash table much bigger than DRAM</p>
    <p>Low latency as well as high throughput systems</p>
    <p>HashCache (Badam et al., NSDI 2009)</p>
    <p>In-memory hash table for objects stored on disk</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We have designed a new data structure BufferHash for building CLAMs</p>
    <p>Our CLAM on Intel SSD achieves high ops/sec/$ for todays data-intensive systems</p>
    <p>Our CLAM can support useful eviction policies</p>
    <p>Dramatically improves performance of WAN optimizers</p>
  </div>
  <div class="page">
    <p>Thank you</p>
  </div>
</Presentation>
