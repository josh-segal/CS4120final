<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Edelta: A Word-Enlarging Based Fast Delta Compression Approach</p>
    <p>Wen Xia, Chunguang Li, Hong Jiang, Dan Feng, Yu Hua, Leihua Qin, Yucheng Zhang</p>
  </div>
  <div class="page">
    <p>Outline  Background and Problems</p>
    <p>Observation and Motivation</p>
    <p>Edelta Design and Implementation</p>
    <p>Performance Evaluation</p>
    <p>Conclusion and Future Work</p>
  </div>
  <div class="page">
    <p>Dedup vs. Delta Compression  In recent years, dedupication and delta compression are</p>
    <p>gaining increasing attentions</p>
    <p>Dedup vs. Delta Compression</p>
    <p>Data deduplication runs much faster than delta compression</p>
    <p>Delta compression is able to eliminate more redundancy among non-duplicate but similar chunks (about 2-3X more)</p>
    <p>Can delta compression run faster than deduplication?</p>
    <p>Benefits Reduces the storage space requirement Minimizes the network transmission of redundant data</p>
  </div>
  <div class="page">
    <p>State of the Art on Delta Compression</p>
    <p>Delta encoding  Xdelta, Zdelta, Ddelta(Performance14)</p>
    <p>Cache compression  Difference Engine (OSDI08)  I-CASH (HPCA12)</p>
    <p>WAN optimization/backup storage  Dropbox  SIDC (FAST12, HotStorage12) 4</p>
    <p>Delta i b b,iA A+  Reverse deltab,i b iA A+</p>
  </div>
  <div class="page">
    <p>Delta Encoding</p>
    <p>Our Previous Work: Ddelta</p>
    <p>Use Gear-based CDC to fast partition strings (words)</p>
    <p>Encode the Matched /New words into Copy/Insert messages</p>
    <p>About 3X faster than Xdelta, Cloud it be more faster ??</p>
  </div>
  <div class="page">
    <p>Observation and Motivation  Observation I: In Ddelta, 96% of the time overhead is from</p>
    <p>Chunking (~45%), hashing (~16%), and indexing (~35%)  Observation 2: Copy is very long while Insert is short,</p>
    <p>Motivation: Can we exploit word-content locality to reduce some unnecessary computation operations.</p>
    <p>Word-content locality</p>
    <p>Average length of the grouped C/I messages</p>
  </div>
  <div class="page">
    <p>An Example  For those contiguous duplicate words {b8, 5f, a9, c4}, the</p>
    <p>chunking, hashing, and indexing for the words {5f, a9, c4} would be unnecessary by directly enlarging the detected word {b8}, which is just a fast byte-wise comparison.</p>
  </div>
  <div class="page">
    <p>Implementation of Edelta  We implement Edelta on top of our previous work Ddelta</p>
    <p>For the two known or detected similar chunks, Edelta consists of two key steps: Find a matched word and then enlarge the word</p>
  </div>
  <div class="page">
    <p>Continue..  Step (1): Tentatively detects a duplicate word by Ddeltas scheme.  Step (2): Directly enlarge the detected word into a much longer one and</p>
    <p>thus avoid the word-matching operations in the enlarged regions.</p>
    <p>Scheme I only word-enlarges the input data file/chunk  Scheme II word-enlarges both the input and base files/chunks</p>
  </div>
  <div class="page">
    <p>Evaluation  Metrics: Compression ratio and encoding speed  Experimental Setup</p>
    <p>Intel i7 processor, 16GB RAM, two 1TB 7200rpm hard disks, and a 120GB SSD of Kingston VP200S37A120G.</p>
    <p>Two case studies  1. Delta compressing the updated tarred files</p>
    <p>Datasets: linux, GDB, GCC, etc. tarred files  2. Delta compressing the non-duplicate but similar</p>
    <p>Chunks  Datasets: RDB, VM images, Linux  First deduplication, and then resemblance detection, delta</p>
    <p>encoding the detected chunks.</p>
  </div>
  <div class="page">
    <p>Case Study I</p>
    <p>5-20X Improvement</p>
    <p>Compression ratio</p>
    <p>Only 1-2% decrease</p>
  </div>
  <div class="page">
    <p>Case Study II  Post-deduplication delta compression  Dedup factors of the three datasets are 44.7, 2.0, and 22.4 respectively</p>
    <p>More than 400MB/s  2.5-5X Improv. over Delta  Not as high as Case Study I</p>
    <p>Locality missing</p>
  </div>
  <div class="page">
    <p>The hybrid data reduction system performance</p>
    <p>Post-dedupe Delta+GZ data reduction</p>
    <p>Delta+GZIP have the similar compression ratio (Edelta, Xdelta)</p>
    <p>Edelta based solutions have the highest system throughputs.</p>
  </div>
  <div class="page">
    <p>Conclusion and Future Work  Edelta is able to delta encode a 4KB-chunk within 2-10 s  Edelta achieves an encoding speedup of 3-10X over the state</p>
    <p>of-the-art DDelta, Xdelta, and Zdelta without noticeably decreasing the compression ratio</p>
    <p>Future Work  Find more promising application scenarios for Edelta  There are still other bottlenecks for delta compression, such as</p>
    <p>resemblance detection and reading base chunks/file</p>
    <p>Try to make delta compression faster than deduplication</p>
  </div>
  <div class="page">
    <p>Thanks! Q &amp; A</p>
  </div>
</Presentation>
