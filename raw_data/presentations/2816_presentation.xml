<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BLAS-on-flash</p>
    <p>An Efficient Alternative for Scaling ML training and inference with NVMs</p>
    <p>Suhas JS, Harsha Simhadri, Srajan Garg, Anil Kag, Venkatesh B</p>
    <p>Microsoft Research India</p>
  </div>
  <div class="page">
    <p>Scope | Memory-Intensive non-DL workloads</p>
    <p>Classification / Regression</p>
    <p>Topic Modeling</p>
    <p>Matrix Factorizations</p>
    <p>Clustering</p>
    <p>Typical use-cases</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Distributed ML | Current Landscape</p>
    <p>Pros  Terabyte-scale machine learning</p>
    <p>Decent speedups on large clusters</p>
    <p>Widely used in production</p>
    <p>Cons  High setup + maintenance cost</p>
    <p>Code rewrite using specific abstractions</p>
    <p>Platform and programming inefficiencies</p>
  </div>
  <div class="page">
    <p>Scalability | Compact systems</p>
    <p>GraphCHI [Kyrola et al., OSDI12]</p>
    <p>Scalability! But at what COST? [McSherry, Isard, Murray, HotOS2016] - Are big ML platforms really scalable or more useful than single node platforms?</p>
    <p>Ligra [Shun, Blelloch, PPoPP13], Ligra+ .. - Web scale graph processing on a single shared memory machine</p>
    <p>EfficientScalable</p>
  </div>
  <div class="page">
    <p>BLAS-on-flash | Overview Observations</p>
    <p>Legacy code = multi-threaded code + math library calls</p>
    <p>High locality in BLAS-3 operations  PCIe-SSDs bandwidth sufficient</p>
    <p>Contributions</p>
    <p>A library of matrix operations for large SSD-resident matrices (GBs  TBs)</p>
    <p>Link to legacy code via the standard BLAS and sparseBLAS API</p>
    <p>DAG definition + online scheduling to execute data-dependent computation</p>
  </div>
  <div class="page">
    <p>API |In-memory  BLAS-on-flash</p>
    <p>- float *A; + flash_ptr&lt;float&gt; A;</p>
    <p>+ flash ptr&lt;float&gt; mat =</p>
    <p>flash::malloc&lt;float&gt;(len);</p>
    <p>+ flash::sgemm(args, A, B, C);</p>
    <p>+ float* mmap_A = A.ptr;</p>
    <p>+ legacy_fn(mmap_A); // correct, but possibly slow</p>
    <p>- float* mat =</p>
    <p>(float*)malloc(len);</p>
    <p>- sgemm(args, A, B, C);</p>
    <p>- legacy_fn(A);</p>
  </div>
  <div class="page">
    <p>gemm | Task View</p>
  </div>
  <div class="page">
    <p>gemm | Chain View</p>
  </div>
  <div class="page">
    <p>gemm | DAG view</p>
    <p>Accumulate Chains</p>
    <p>???</p>
  </div>
  <div class="page">
    <p>gemm | Kernel  Task Creation</p>
  </div>
  <div class="page">
    <p>gemm | Kernel  DAG Creation</p>
  </div>
  <div class="page">
    <p>gemm | Kernel  DAG Submission</p>
  </div>
  <div class="page">
    <p>gemm | Kernel  Poll Completion</p>
  </div>
  <div class="page">
    <p>gemm | Task  Input/Output</p>
  </div>
  <div class="page">
    <p>gemm | Task  Computation</p>
  </div>
  <div class="page">
    <p>Access Specifier | Block Definition</p>
  </div>
  <div class="page">
    <p>Sector Alignment | Correct vs fast</p>
    <p>Sector-level sharing between adjacent unaligned blocks</p>
    <p>Conflicting writes detected and ordered automatically</p>
    <p>Aligned operations extract highest performance</p>
  </div>
  <div class="page">
    <p>Software Stack | Architecture</p>
    <p>Kernel</p>
    <p>Scheduler</p>
    <p>Schedule I/O + compute</p>
    <p>Tunable inter-task parallelism</p>
    <p>DAG state management</p>
  </div>
  <div class="page">
    <p>Software Stack | Architecture</p>
    <p>Kernel</p>
    <p>Scheduler</p>
    <p>Prioritizer</p>
    <p>Prioritize data reuse</p>
    <p>Heuristic: min # of bytes to prefetch</p>
  </div>
  <div class="page">
    <p>Software Stack | Architecture II</p>
    <p>Program Cache</p>
    <p>(flash_ptr&lt;T&gt;,AS)  T*</p>
    <p>Uniqueness in DRAM contents</p>
    <p>Data-reuse</p>
    <p>Hit/miss queries</p>
  </div>
  <div class="page">
    <p>Software Stack | Architecture II</p>
    <p>Program Cache</p>
    <p>I/O Executor</p>
    <p>Thread-pool + blocking I/O</p>
    <p>Order conflicting writes</p>
  </div>
  <div class="page">
    <p>Software Stack | Architecture II</p>
    <p>Program Cache</p>
    <p>I/O Executor</p>
    <p>File Handle</p>
    <p>Concurrent strided I/O requests</p>
    <p>Linux kernel AIO + libaio</p>
  </div>
  <div class="page">
    <p>Evaluation | Hardware Specifications</p>
    <p>Class Name Processor(s) Cores RAM Disk Read BW Write BW</p>
    <p>Workstation Z840 E5-2620v4 x2 16 32GB 2x 960EVO 1TB 3GB/s 2.2GB/s</p>
    <p>Virtual Machines (VM)</p>
    <p>M64 E7-8890v3 x2 32 1792GB SATA SSD 250MB/s 250MB/s</p>
    <p>L32s E5-2698Bv3 x2 32 256GB 6TB vSSD 1.4GB/s 1.4GB/s</p>
    <p>Bare-Metal Server</p>
    <p>Sandbox Gold 6140 x2 36 512GB 3.2TB PM1725a 4GB/s 1GB/s</p>
    <p>Spark Cluster [x40]</p>
    <p>DS14v2 E5-2673v3 x2 16 112GB SATA SSD 250MB/s 250MB/s</p>
  </div>
  <div class="page">
    <p>Sparse Matrices from bag-of-words representation  Rows  Words</p>
    <p>Columns  Documents</p>
    <p>Value  Frequency</p>
    <p>Datasets used:</p>
    <p>Context: Parameter servers with dozens of nodes process 100--200B tokens</p>
    <p>Name # cols # rows NNZs Tokens File size (CSR)</p>
    <p>Small (Pubmed) 8.15M 140K 428M 650M 10.3GB</p>
    <p>Medium (Bing) 22M 1.56M 6.3B 15B 151GB</p>
    <p>Large (Bing) 81.7M 2.27M 22.2B 65B 533GB</p>
    <p>Evaluation | Datasets</p>
  </div>
  <div class="page">
    <p>Evaluation | Metrics</p>
    <p>Time  Absolute time to completion</p>
    <p>Memory  Maximum DRAM usage</p>
    <p>Time ratio  In-memory : Flash  0.25  Flash version is 0.25x as fast as In-memory</p>
    <p>Memory ratio  Flash : In-memory  0.5  Flash version needs 0.5x as much as In-memorys DRAM</p>
  </div>
  <div class="page">
    <p>gemm | 8GB RAM is all you need ?</p>
    <p>Larger inner dimension  Longer accumulate chains  Lower disk pressure</p>
    <p>Reduction Dimension</p>
    <p>z840</p>
    <p>L32s VM</p>
    <p>sandbox</p>
    <p>Reduction Dimension</p>
    <p>z840</p>
    <p>L32s VM</p>
    <p>sandbox</p>
    <p>T im</p>
    <p>e R</p>
    <p>a ti</p>
    <p>o</p>
  </div>
  <div class="page">
    <p>csrmm | Sparsity ruins the party</p>
    <p>Compute:Communication  Sparsity</p>
    <p>Max out disk bandwidth (read + write)</p>
    <p>im e</p>
    <p>R a</p>
    <p>ti o</p>
    <p>z840</p>
    <p>L32s VM</p>
    <p>sandbox</p>
    <p>Dimensionality reduction, projection operations (e.g. PCA)</p>
    <p>No reuse</p>
  </div>
  <div class="page">
    <p>SVD | Choosing the right algorithm  SVD using symmetric eigensolvers</p>
    <p>Lanczos Algorithm  ARPACK, Spark MLLib</p>
    <p>2 matrix-vector (gemv) calls for  eigenvalues</p>
    <p>Streaming matrix from SSD  bad performance</p>
    <p>DRAM bandwidth  30x Flash bandwidth</p>
    <p>A</p>
    <p>v Av A2v</p>
    <p>Beigs( )</p>
    <p>Krylov Subspace</p>
    <p>A A</p>
  </div>
  <div class="page">
    <p>SVD | Choosing the right algorithm  SVD using symmetric eigensolvers</p>
    <p>Lanczos Algorithm</p>
    <p>Block Krylov-Schur Algorithm [Zhou, Saad, 2008]</p>
    <p>Use  2</p>
    <p>matrix-matrix (gemm) calls for  eigenvalues</p>
    <p>-fold reduction in number of matrix access</p>
    <p>Eigenvalues need to be well separated to get speedups</p>
    <p>A</p>
    <p>M AM A2M</p>
    <p>Beigs( ) Ab bA b</p>
  </div>
  <div class="page">
    <p>Eigenvalues| Text datasets</p>
    <p>Spectrum for text data tapers off</p>
    <p>1</p>
    <p>for some  &gt; 1</p>
    <p>Gap between successive eigenvalues large enough for block methods</p>
  </div>
  <div class="page">
    <p>Eigensolvers | Comparison</p>
    <p>Solve for top-K largest eigenvalues</p>
    <p>Spectra (Lanczos, Eigen + MKL)</p>
    <p>Spark MLlib computeSVD  Shared + dedicated mode</p>
    <p>500 singular values hardcoded limit</p>
    <p>OOM on driver node (&gt;200 singular values)</p>
    <p>Block Krylov-Schur (Block KS)  5000 singular values on Large dataset with</p>
    <p>T im</p>
    <p>e (</p>
    <p>m in</p>
    <p>)</p>
    <p># workers</p>
    <p>Medium (151GB) - 500 Singular Values</p>
    <p>(Dedicated)computeSVD (Shared) computeSVD Spectra Block KS Flash Block KS</p>
  </div>
  <div class="page">
    <p>Eigensolvers | Cost-effectiveness</p>
    <p>Single node vs Distributed solvers  16x fewer processing cores, &gt;73x reduction in DRAM requirement</p>
    <p>70% performance of best distributed solver runtime</p>
    <p>Orders of magnitude better hardware utilization, orders of magnitude cheaper</p>
    <p>T im</p>
    <p>e (</p>
    <p>m in</p>
    <p>)</p>
    <p># workers</p>
    <p>Large (533GB) - 200 Singular Values</p>
    <p>(Dedicated)computeSVD (Shared) computeSVD Block KS Flash Block KS</p>
    <p>M e</p>
    <p>m o</p>
    <p>ry (</p>
    <p>G B</p>
    <p>)</p>
    <p># workers</p>
    <p>Large (533GB) - 200 Singular Values</p>
    <p>(Dedicated)computeSVD Block KS Flash Block KS</p>
  </div>
  <div class="page">
    <p>ISLE | Web-Scale Topic Modeling  Current</p>
    <p>2000-topic model, 533GB input  &gt;1TB DRAM</p>
    <p>Goal  5000+-topic model, &gt;533GB input</p>
    <p>128GB RAM machines in production</p>
    <p>Expensive steps  SVD, Clustering</p>
    <p>ISLE + BLAS-on-Flash  Flash Block KS for SVD</p>
    <p>Flash k-means for clustering</p>
    <p>Custom kernels for other operations</p>
  </div>
  <div class="page">
    <p>ISLE | Larger models, lower costs ?</p>
    <p>In-memory baselines for Large dataset are run on M64 due to high DRAM requirement</p>
    <p>&gt;7x reduction in memory usage with no overheads on large datasets</p>
    <p>z840 sandbox L32s sandbox L32s sandbox L32s</p>
    <p>Pubmed Medium Large</p>
    <p>Runtime Ratio</p>
    <p>z840 sandbox L32s sandbox L32s sandbox L32s</p>
    <p>Pubmed Medium Large</p>
    <p>Memory Ratio</p>
  </div>
  <div class="page">
    <p>XML | Web-scale classification</p>
    <p>Assign a subset of labels to query point from pool of millions of labels</p>
    <p>Decision-tree like approaches for 100M+ labels</p>
    <p>Bing Related Search + Recommendations</p>
    <p>PfastreXML  Depth-First Search traversal</p>
    <p>Large ensemble of fast and inaccurate trees</p>
    <p>Parabel  Breadth-First Beam-Search traversal</p>
    <p>Small ensemble of slow and accurate trees</p>
    <p>q=</p>
  </div>
  <div class="page">
    <p>XML | Web-scale classification</p>
    <p>Weekly inference, infrequent training  250M points inference (500GB) against 14GB trees</p>
    <p>Runs on a cluster of DS14 (112GB) nodes</p>
    <p>Why BLAS-on-Flash?  150GB models exist, unable to run on DS14</p>
    <p>&gt;250GB models foreseeable</p>
    <p>In-memory baseline  Improved existing multi-threaded in-memory code</p>
    <p>6x faster than current production code</p>
    <p>q=</p>
  </div>
  <div class="page">
    <p>XML | Algorithms + Evaluation</p>
    <p>Inference running out of flash uses less DRAM without performance regressions</p>
    <p>Inference on larger models  Better quality predictions</p>
    <p>sandbox PfastreXML (50 trees)</p>
    <p>L32s VM PfastreXML (50 trees)</p>
    <p>L32s VM Parabel (3 trees)</p>
    <p>sandbox Parabel (3 trees)</p>
    <p>In -m</p>
    <p>e m</p>
    <p>o ry</p>
    <p>: F</p>
    <p>la sh</p>
    <p>Runtime Peak Memory</p>
    <p>Algorithm PfastreXML Parabel</p>
    <p>Tree Type Unbalanced Binary Trees</p>
    <p>Balanced Binary Trees</p>
    <p>Traversal Depth First Search (DFS)</p>
    <p># trees 50 3</p>
    <p>Time 440 hours 900 hours</p>
  </div>
  <div class="page">
    <p>In the works</p>
    <p>Decision Trees training (LightGBM)  Train gradient-boosted decision trees on TBs of data</p>
    <p>Out-of-core training for better models at low-cost</p>
    <p>k-Approximate Nearest Neighbor (k-ANN) Search  Serve queries on 100B+ points in few ms each</p>
    <p>DRAM limitations  partition dataset, mirror + aggregate response</p>
    <p>Use disk-resident indexes to increase points-per-node</p>
  </div>
  <div class="page">
    <p>Conclusion We have developed set of math routines utilizing a DAG execution engine</p>
    <p>for large SSD-resident data</p>
    <p>Near in-memory performance</p>
    <p>Drastic reduction in memory usage  larger inputs possible</p>
    <p>Relevant for Optane/NVDIMMs, GraphCore</p>
    <p>NAND ($250/TB)</p>
    <p>DRAM (&gt;$7000/TB)</p>
    <p>$ $$$ $$$$$</p>
    <p>github.com/Microsoft/BLAS-on-flash</p>
  </div>
</Presentation>
