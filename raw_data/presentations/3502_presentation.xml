<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Octavian Suciu Radu Mrginean Yiitcan Kaya Hal Daum III Tudor Dumitra</p>
    <p>University of Maryland, College Park</p>
  </div>
  <div class="page">
    <p>ML - Training a Classifier</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Training Instances Decision Boundary</p>
  </div>
  <div class="page">
    <p>ML - Testing the Classifier</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Testing Instances Labeled Testing Instances</p>
  </div>
  <div class="page">
    <p>Threat Models In Adversarial Machine Larning</p>
    <p>Lots of proposed attack and defense strategies  Various assumptions about adversaries</p>
    <p>We evaluate the practical impact of assumptions on attack effectiveness  This helps design better defense mechanisms</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Benign</p>
  </div>
  <div class="page">
    <p>Practical Attack Example  Lets consider a running example:  Drebin Android malware detector[1]  Support Vector Machine (SVM) classifier  Trained using a public dataset</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>[1] Arp et al. &quot;DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket; 2014</p>
  </div>
  <div class="page">
    <p>Targeted Evasion Attacks</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Target</p>
    <p>Testing Instances Victim Decision Boundary</p>
    <p>Capability: Modify target features</p>
    <p>Evasion: modify target features to cross the decision boundary</p>
  </div>
  <div class="page">
    <p>Poisoning Attack Example  Microsofts Tay chatbot poisoned through tweets*</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>* https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist</p>
  </div>
  <div class="page">
    <p>Poisoning the Drebin Classifier</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning</p>
    <p>Attacks 7</p>
    <p>Victim Decision BoundaryVictim Training Instances Attempt 1: add target with flipped label to the training set</p>
    <p>Assumption: adversarial control over the labeling function[2][3]</p>
    <p>Target</p>
    <p>[3] Koh et al. &quot;Understanding Black-box Predictions via Influence Functions; 2017</p>
    <p>[2] Jagielski et al. &quot;Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning&quot;; 2018</p>
  </div>
  <div class="page">
    <p>Practical Label Assignment</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Labeling Oracle</p>
    <p>Our StingRay attack achieves targeted poisoning without this assumption</p>
  </div>
  <div class="page">
    <p>Attacker Limitations Through Existing Models</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision Boundary Surrogate Attacker Training Instances[4]Victim Training Instances</p>
    <p>White-Box Attacker Decision Boundary[6] Black-Box Attacker Decision Boundary[5]</p>
    <p>[4] rndic et al. &quot;Practical Evasion of a Learning-Based Classifier: A Case Study; 2014</p>
    <p>[6] Papernot et al. &quot;The limitations of deep learning in adversarial settings.; 2015 [5] Papernot et al. &quot;Practical Black-Box Attacks against Machine Learning; 2016</p>
  </div>
  <div class="page">
    <p>Adversarial Models in Practice</p>
    <p>D re</p>
    <p>bi n</p>
    <p>Guessable training algorithm with unknown parameters (linear SVM)</p>
    <p>Partial Algorithm knowledge</p>
    <p>Known and publicly accessible data set (Drebin data set)</p>
    <p>Full Instance knowledge</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Can be captured by existing models</p>
    <p>Transferability measures success of an attack performed on a surrogate model when triggered on the victim [5][6]</p>
    <p>[5] Papernot et al. &quot;Practical Black-Box Attacks against Machine Learning; 2016 [6] Goodfellow et al. &quot;Explaining and Harnessing Adversarial Examples&quot;; 2014</p>
  </div>
  <div class="page">
    <p>Malware Features in Practice  Malware detectors use program analysis features  Derived from code disassembly</p>
    <p>permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.MAIN</p>
    <p>api_call::setWifiEnabled url::c.admob.com</p>
    <p>Features extracted from Android Manifest</p>
    <p>Features extracted using static program analysis</p>
    <p>D re</p>
    <p>bi n</p>
    <p>Static program analysis features might be unknown or hard to modify</p>
  </div>
  <div class="page">
    <p>Adversarial Models in Practice (2)</p>
    <p>Proprietary features from binaries (Program analysis)</p>
    <p>Partial Feature knowledgeD re</p>
    <p>bi n</p>
    <p>Guessable training algorithm with unknown parameters (linear SVM)</p>
    <p>Partial Algorithm knowledge</p>
    <p>Known and publicly accessible data set (Drebin data set)</p>
    <p>Full Instance knowledge</p>
    <p>Hard-to-modify features (Program analysis)</p>
    <p>Partial Leverage</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Can be captured by existing models</p>
    <p>Cannot be captured by existing models</p>
    <p>Assumption: attackers have full knowledge and leverage on all features</p>
  </div>
  <div class="page">
    <p>White-Box 10% Known Training Instances</p>
    <p>SR of the StingRay poisoning attack on Drebin</p>
    <p>Implicit Full Leverage Practical Partial Leverage</p>
    <p>Practical Effectiveness of Poisoning Attacks</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Practical attack success rate is overestimated by 36%</p>
    <p>Hiding 90% of the training set decreases success rate by 50%</p>
    <p>Existing models can capture training set knowledge limitations</p>
    <p>Success Rate (SR): Percentage of attacks that are successful on the victim</p>
  </div>
  <div class="page">
    <p>Contributions  FAIL adversarial model for highlighting realistic</p>
    <p>adversarial capabilities  Represents knowledge and control along: Features,</p>
    <p>Algorithms, Instances, Leverage</p>
    <p>StingRay, a generic targeted poisoning attack  Implemented on four applications and against three</p>
    <p>defenses</p>
    <p>Systematic evaluation of how much adversarial success depends on implicit assumptions  More accurate threat assessment</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>Outline  FAIL  StingRay  Evaluation</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>The FAIL Model  Models adversaries with variable levels of knowledge</p>
    <p>and capabilities across four dimensions:  Features  Algorithms  Instances  Leverage</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>FAIL in Action</p>
    <p>Proprietary features from binaries (Program analysis)</p>
    <p>Partial Feature knowledge</p>
    <p>Guessable training algorithm with unknown parameters (linear SVM)</p>
    <p>Partial Algorithm knowledge</p>
    <p>Known and publicly accessible data set (Drebin data set)</p>
    <p>Full Instance knowledge</p>
    <p>Hard-to-modify features (Program analysis)</p>
    <p>Partial Leverage</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Drebin FAIL</p>
    <p>Adversary Model</p>
    <p>Generalized transferability measures the attacks success rate under realistic knowledge and capabilities assumptions</p>
  </div>
  <div class="page">
    <p>FAIL - Features  Models the degree of knowledge about the</p>
    <p>adversarial features  What features can be kept secret?  Are the exact feature values known?</p>
    <p>Examples:  Unknown program analysis features  Unknown image resolution</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>FAIL - Algorithms  Models the degree of knowledge about the classifier  Is the algorithm class known?  Is the training algorithm known?  Are the model parameters secret?</p>
    <p>Examples:  Unknown linear training algorithm  Unknown neural network architecture</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>FAIL  Instances  Measures the overlap between the attack and the</p>
    <p>victim training sets  Is the entire training set public?  Are some instances known?  Are public instances sufficient to train a robust classifier?</p>
    <p>Examples:  Unknown malware training set  Public image training set</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>FAIL - Leverage  Limits the crafting capabilities of the attacker  Which feature can be modified by the attacker?  Does the attack on some features have side effects?</p>
    <p>Examples:  Hard to modify program analysis features  Watermarked images</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>Outline  FAIL  StingRay  Evaluation</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>Four Target Applications  Drebin[1]: Android malware detector based on SVM  Image classifier: Convolutional Neural Networks  Twitter exploit predictor[7]: SVM classifier  Breach predictor[8]: Random Forests on timeseries</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>[7] Sabottke et al. &quot;Vulnerability disclosure in the age of social media: exploiting Twitter for predicting real-world exploits; 2015 [8] Koh et al. &quot;Understanding Black-box Predictions via Influence Functions; 2017</p>
    <p>[1] Arp et al. &quot;DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket; 2014</p>
  </div>
  <div class="page">
    <p>The Poisoners Dilemma (1)</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision BoundaryVictim Training Instances</p>
    <p>Instance labels cannot be assigned by the attacker</p>
    <p>Target</p>
  </div>
  <div class="page">
    <p>The Poisoners Dilemma (2)</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision BoundaryVictim Training Instances</p>
    <p>Poisoning instances could be detected by existing defenses</p>
    <p>Target</p>
    <p>Poisoning Instances Detected outliers</p>
  </div>
  <div class="page">
    <p>The Poisoners Dilemma (3)</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision BoundaryVictim Training Instances</p>
    <p>Poisoning instances could cause collateral, indiscriminate damage</p>
    <p>Poisoned Decision Boundary</p>
    <p>Target</p>
    <p>Victim Testing Instances</p>
  </div>
  <div class="page">
    <p>The StingRay Apprach</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision Boundary</p>
    <p>StingRay achieves both individual and collective inconspicuousness</p>
    <p>Poisoned Decision Boundary</p>
    <p>Target</p>
    <p>Victim Testing Instances</p>
  </div>
  <div class="page">
    <p>Attack Requirements  StingRay design requirements:  No assumed control over the labeling function  Individually and collectively inconspicuous poisoning  Practical FAIL considerations</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>StingRay High Level Illustration</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Victim Decision Boundary Poisoned Decision BoundaryVictim Testing Instances</p>
    <p>Victim Training Instances Base Instances Poisoning Instances</p>
    <p>Target</p>
  </div>
  <div class="page">
    <p>Crafting Example - Drebin</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>api_call::setWifiEnabled permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.SEARCH intent.action.MAIN</p>
    <p>VirusTotal highlights some features as more suspicious than others</p>
    <p>malicious label</p>
  </div>
  <div class="page">
    <p>StingRay  Choosing a Base Instances</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>api_call::setWifiEnabled permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.SEARCH intent.action.MAIN</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>Choose base instances with some similarity to target</p>
    <p>benign label</p>
  </div>
  <div class="page">
    <p>StingRay  Individual Inconspicuousness</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>api_call::setWifiEnabled permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.SEARCH intent.action.MAIN</p>
    <p>intent::LAUNCHER permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>intent::LAUNCHER permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>Reusing existing instances mitigates lack of leverage on some features</p>
  </div>
  <div class="page">
    <p>StingRay  Collective Inconspicuousness</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>intent.action.MAIN Poison instances bypass three defenses: RONI, targeted RONI and Micromodels</p>
    <p>api_call::setWifiEnabled permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.SEARCH intent.action.MAIN intent.action.SEARCH intent.action.MAIN</p>
  </div>
  <div class="page">
    <p>StingRay  Uncontrolled Labels</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>api_call::setWifiEnabled permission::WRITE_CONTACTS permission.CALL_PHONE permission::ACCESS_WIFI_STATE permission::READ_CONTACTS intent.action.SEARCH intent.action.MAIN</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS intent.action.SEARCH</p>
    <p>api_call::setWifiEnabled permission::ACCESS_WIFI_STATE activity::MainActivity permission::READ_CONTACTS intent.action.MAIN</p>
    <p>unknown label</p>
  </div>
  <div class="page">
    <p>Crafting Example  Neural Networks  Neural Networks learn features from raw data  Adapting JSMA[6] for poisoning  JSMA pushes instances towards class and not an instance  We modify JSMAs objective function to move the</p>
    <p>poisoning instances towards the target</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning</p>
    <p>Attacks 35</p>
    <p>[6] Papernot et al. &quot;The limitations of deep learning in adversarial settings; 2016</p>
  </div>
  <div class="page">
    <p>StingRay - White-Box Performance</p>
    <p>Drebin Image Classifier Exploit Predictor Breach Predictor</p>
    <p>Success Rate of StingRay in white-box setting</p>
    <p>StingRay</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning</p>
    <p>Attacks 36</p>
    <p>Success Rate (SR): Percentage of attacks that are successful on the victim</p>
  </div>
  <div class="page">
    <p>Outline  FAIL  StingRay  Evaluation</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>StingRay - Success on Drebin</p>
    <p>Full Limited F Limited A Limited I Limited L Unconstrained Random 20%</p>
    <p>features known SGD algorithm</p>
    <p>(vs SVM) 10% training set</p>
    <p>known 60% read</p>
    <p>only features</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>Feature secrecy appears to be the most powerful limiting factor</p>
    <p>Some attacks perceived failed on the surrogate model are actually successful on the victim</p>
  </div>
  <div class="page">
    <p>StingRay and JSMA - Success on the Image Classifier</p>
    <p>Full Limited F Limited A Limited I Limited L Unconstrained 30-40% center</p>
    <p>pixels known CNN missing</p>
    <p>one layer 70% training set known</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
    <p>StingRay remains successful on all dimensions, sometimes even with increased efficiency due to a constrained localized strategy JSMA is more effective in white-box settings, but performs poorly on all other dimensions, in contrast to prior observations for Algorithms[9] [9] Papernot et al. &quot;Transferability in machine learning: from phenomena to black-box attacks using adversarial samples; 2016</p>
    <p>StingRay JSMA</p>
  </div>
  <div class="page">
    <p>FAIL Captures Adversaries from Prior Work</p>
    <p>Prior Work F A I L Testing-time attacks</p>
    <p>FGSM Evasion (Goodfellow et al., 2014 ) N/A</p>
    <p>Model Stealing (Tramer et al., 2016)</p>
    <p>Genetic Evasion (Xu et al., 2016)</p>
    <p>Black-box Evasion (Papernot et al., 2017)</p>
    <p>C&amp;W Evasion (Carlini et al., 2017) N/A</p>
    <p>Training-time attacks SVM Poisoning Attack (Biggio et al., 2012) N/A</p>
    <p>DNNs Poisoning (Munoz-Gonzalez et al., 2017)</p>
    <p>NNs Backdoors (Gu et al., 2017 )</p>
    <p>NNs Trojaning (Liu et al., 2017)</p>
    <p>Fully considered Considered, not evaluated Not considered</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning</p>
    <p>Attacks</p>
  </div>
  <div class="page">
    <p>Conclusions  FAIL  Models realistic adversarial assumptions  Captures existing adversaries  Generalizes the notion of transferability  Applicable to both evasion and poisoning</p>
    <p>StingRay  Targeted poisoning attack  Crafts inconspicuous samples  No assumed control over the labeling function  Implemented against four applications</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Octavian Suciu osuciu.com osuciu@umiacs.umd.edu</p>
    <p>FAIL Framework available at:</p>
    <p>ter.ps/fail</p>
    <p>Octavian Suciu - When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks</p>
  </div>
</Presentation>
