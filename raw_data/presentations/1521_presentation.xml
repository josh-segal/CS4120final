<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Tale of Two Erasure Codes in HDFS</p>
    <p>Mingyuan Xia, Mohit Saxena Mario Blaum, David Pease</p>
    <p>IBM Research Almaden &amp; McGill University</p>
    <p>FAST15</p>
  </div>
  <div class="page">
    <p>Really Big Data Today</p>
    <p>UNECE data 1</p>
  </div>
  <div class="page">
    <p>Big Data Storage</p>
    <p>GFS Three-way replicaNon</p>
    <p>Storage overhead:3x</p>
  </div>
  <div class="page">
    <p>Big Data Storage</p>
    <p>GFS Three-way replicaNon</p>
    <p>GFS v2 Reed-Solomon</p>
    <p>Facebook HDFS Reed-Solomon</p>
    <p>MicrosoU Azure Storage LRC</p>
    <p>Facebook HDFS LRC, Hitchiker</p>
    <p>Erasure Coded Storage saves millions of $ for capital cost</p>
    <p>Storage overhead:3x 1.4x~1.5x</p>
  </div>
  <div class="page">
    <p>Erasure Coding 101</p>
    <p>D1 D2 D3</p>
    <p>stripe</p>
    <p>D4 D5</p>
    <p>D6 D7 D8 D9 D10</p>
    <p>Facebook HDFS: Reed Solomon (14,10)</p>
  </div>
  <div class="page">
    <p>Erasure Coding 101</p>
    <p>D1 D2 D3</p>
    <p>P1 P2</p>
    <p>stripe</p>
    <p>D4 D5</p>
    <p>D6</p>
    <p>P3</p>
    <p>D7 D8 D9 D10</p>
    <p>P4</p>
    <p>Facebook HDFS: Reed Solomon (14,10)  Compute 4 pariNes per 10 data blocks  Storage overhead: 1.4x=14/10</p>
  </div>
  <div class="page">
    <p>Erasure Coding 101</p>
    <p>D1 D2 D3</p>
    <p>P1 P2</p>
    <p>stripe</p>
    <p>D4 D5</p>
    <p>D6</p>
    <p>P3</p>
    <p>D7 D8 D9 D10</p>
    <p>P4</p>
    <p>Facebook HDFS: Reed Solomon (14,10)  Compute 4 pariNes per 10 data blocks  Storage overhead: 1.4x=14/10  For a single failure, RS needs any 10 blocks</p>
    <p>over network from other nodes to recover</p>
  </div>
  <div class="page">
    <p>Erasure Coding 101</p>
    <p>D1 D2 D3</p>
    <p>P1 P2</p>
    <p>stripe</p>
    <p>D4 D5</p>
    <p>D6</p>
    <p>P3</p>
    <p>D7 D8 D9 D10</p>
    <p>P4</p>
    <p>Facebook HDFS: Reed Solomon (14,10)  Compute 4 pariNes per 10 data blocks  Storage overhead: 1.4x  For a single failure, RS needs any 10 blocks</p>
    <p>over network from other nodes to recover</p>
    <p>Problems:  Degraded read  Data reconstrucNon</p>
  </div>
  <div class="page">
    <p>Problem 1: Degraded Read</p>
    <p>Data node</p>
    <p>?</p>
    <p>Data node Data node</p>
    <p>HDFS client</p>
    <p>Read excepNon</p>
    <p>HDFS</p>
    <p>Causes  SoUware errors (&gt;90%)</p>
    <p>Rolling updates  Hot-spot effects</p>
    <p>Hardware errors</p>
  </div>
  <div class="page">
    <p>Problem 1: Degraded Read</p>
    <p>Data node</p>
    <p>?</p>
    <p>Data node Data node</p>
    <p>HDFS client</p>
    <p>Extra latency for recovery</p>
    <p>Read excepNon</p>
    <p>HDFS</p>
    <p>Causes  SoUware errors (&gt;90%)</p>
    <p>Rolling updates  Hot-spot effects</p>
    <p>Hardware errors</p>
  </div>
  <div class="page">
    <p>Problem 2: Data ReconstrucNon</p>
    <p>Data node Data node Data node</p>
    <p>MapReduce reconstrucNon job</p>
    <p>HDFS</p>
    <p>Causes  Disk or node failure  Decommissioned nodes</p>
  </div>
  <div class="page">
    <p>Problem 2: Disk/node ReconstrucNon</p>
    <p>Data node Data node Data node</p>
    <p>MapReduce reconstrucNon job</p>
    <p>HDFS</p>
    <p>ProducNon Clusters  New data: 500TB~900TB/day  Failure: lose ~100k blocks/day  ReconstrucNon traffic: 180TB/day</p>
  </div>
  <div class="page">
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Recovery Cost vs. Storage Overhead</p>
    <p>Facebook: RS(14,10)</p>
    <p>Google: RS(9,6)</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Recovery Cost vs. Storage Overhead</p>
    <p>Google: RS(9,6)</p>
    <p>Recover faster? Use less storage?</p>
    <p>Facebook: RS(14,10)</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>HDFS Data Access Skew</p>
    <p>[VLDB12] Chen et al., InteracNve Query Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads</p>
    <p>Four Cloudera customers and one Facebook workload</p>
  </div>
  <div class="page">
    <p>Two Erasure Codes</p>
    <p>Coded with a compact code with high storage efficiency</p>
    <p>Coded with a fast code with low recovery cost</p>
  </div>
  <div class="page">
    <p>AdapNve Coding in HDFS</p>
    <p>Reed Solomon</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>HDFS</p>
    <p>HACFS</p>
    <p>Compact Code</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>Fast Code</p>
    <p>Read cold</p>
    <p>Read hot</p>
  </div>
  <div class="page">
    <p>Popular code families</p>
    <p>Product Code  Local ReconstrucHon Code  Reed-Solomon Code (MDS code)  ParNal MDS Code  HoVer Code</p>
    <p>Low recovery cost codes</p>
  </div>
  <div class="page">
    <p>Popular code families</p>
    <p>Product Code  Local ReconstrucHon Code  Reed-Solomon Code (MDS code)  ParNal MDS Code  HoVer Code</p>
    <p>Low recovery cost codes</p>
  </div>
  <div class="page">
    <p>Fast and Compact Product Codes</p>
    <p>D1 D2 D3 D4 D5 P1 D6 D7 D8 D9 D10 P2 P3 P4 P5 P6 P7 P8</p>
    <p>Fast code (Product Code 2x5)</p>
    <p>Storage overhead: 1.8x (18/10)</p>
  </div>
  <div class="page">
    <p>Fast and Compact Product Codes</p>
    <p>D1 D2 D3 D4 D5 P1 D6 D7 D8 D9 D10 P2 P3 P4 P5 P6 P7 P8</p>
    <p>Fast code (Product Code 2x5) Recovery cost: 2</p>
    <p>Storage overhead: 1.8x (18/10)</p>
  </div>
  <div class="page">
    <p>Fast and Compact Product Codes</p>
    <p>D1 D2 D3 D4 D5 P1 D6 D7 D8 D9 D10 P2 P3 P4 P5 P6 P7 P8</p>
    <p>Fast code (Product Code 2x5) Recovery cost: 2</p>
    <p>Storage overhead: 1.8x</p>
    <p>Compact code (Product Code 6x5) Recovery cost: 5</p>
    <p>Storage overhead: 1.4x</p>
  </div>
  <div class="page">
    <p>HACFS with Product Codes</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Compact code</p>
    <p>Fast code</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>HACFS with Product Codes</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>HACFS with Product Codes</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead Cold data accounts for the majority</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>HACFS with Product Codes</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Hot data is accessed more oUen</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>HACFS with Product Codes</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>OpNmal point: lowest recovery cost and storage overhead</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>Storage Bound</p>
    <p>Re co ve ry C os t</p>
    <p>Storage Overhead</p>
    <p>Google Colossus (1.5x storage)</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>Upcoding/Downcoding</p>
    <p>Reed Solomon</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>HDFS</p>
    <p>HACFS</p>
    <p>Compact Code</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>Fast Code</p>
    <p>Read cold</p>
    <p>Read hot</p>
    <p>Over the bound</p>
    <p>Upcoding</p>
  </div>
  <div class="page">
    <p>Upcoding/Downcoding</p>
    <p>Reed Solomon</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>HDFS</p>
    <p>HACFS</p>
    <p>Compact Code</p>
    <p>Write cold</p>
    <p>Recently created</p>
    <p>Fast Code</p>
    <p>Read cold</p>
    <p>Read hot</p>
    <p>Over the bound</p>
    <p>Below the bound</p>
    <p>Downcoding</p>
    <p>Upcoding</p>
  </div>
  <div class="page">
    <p>Upcoding for Product Codes Fast code Compact code</p>
    <p>P P</p>
    <p>P P P P P P</p>
    <p>P P</p>
    <p>P P P P P P</p>
    <p>P P</p>
    <p>P P P P P P</p>
    <p>P P</p>
    <p>P P P P P P</p>
    <p>P P</p>
    <p>P P</p>
    <p>+</p>
    <p>PC(2x5) PC(6x5)</p>
    <p>Parity-only Conversion  Horizontal parNes require no re-computaNon  VerNcal pariNes require no data block transfer  All parity updates can be done in parallel and in a distributed manner</p>
  </div>
  <div class="page">
    <p>Efficient Up/Down-coding</p>
    <p>Popular code families with efficient up/down- coding  Product Code  Local ReconstrucHon Code  Reed-Solomon Code (MDS code)  ParNal MDS Code  HoVer Code</p>
    <p>HACFS implementaHon</p>
    <p>Applicable to other codes</p>
    <p>as well</p>
  </div>
  <div class="page">
    <p>EvaluaNon  HACFS ImplementaNon  Extension to Facebooks HDFS  3k LOC: three new modules</p>
    <p>Methodology  Five workloads: four Cloudera customers, one Facebook [VLDB2012]</p>
    <p>HDFS cluster: 11 nodes  Each node: 24 cores, 6 disks, 1 Gbps network</p>
  </div>
  <div class="page">
    <p>Experiment metrics</p>
    <p>Degraded read latency  Foreground read request delay  Caused mostly by soUware issues</p>
    <p>ReconstrucNon Nme  Background recovery for failures  Caused mostly by hardware failures</p>
    <p>Storage overhead (bounded)</p>
  </div>
  <div class="page">
    <p>Degraded Read Latency</p>
    <p>Azure Storage</p>
    <p>Facebook HDFS-RAID</p>
    <p>Google Colossus</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>Degraded Read Latency</p>
    <p>HACFS-PC</p>
    <p>Compact PC</p>
    <p>Single Coded Systems</p>
    <p>Fast PC</p>
  </div>
  <div class="page">
    <p>Degraded Read Latency</p>
    <p>HACFS-PC</p>
    <p>Single Coded Systems</p>
    <p>Compact PC</p>
    <p>Google Colossus: 1.5x</p>
  </div>
  <div class="page">
    <p>Degraded Read Latency</p>
    <p>Single Coded Systems</p>
    <p>HACFS-PC HACFS-LRC</p>
  </div>
  <div class="page">
    <p>Degraded Read Latency</p>
    <p>CC1 86% accesses to hot data 94.2% of the data is cold</p>
    <p>Single Coded Systems</p>
    <p>HACFS-PC HACFS-LRC</p>
  </div>
  <div class="page">
    <p>ReconstrucNon Time</p>
    <p>Single Coded Systems</p>
    <p>HACFS-PC HACFS-LRC</p>
    <p>Cold data in CC3: 75.5%</p>
  </div>
  <div class="page">
    <p>ReconstrucNon Time</p>
    <p>Cold data in CC1: 94.2%</p>
    <p>Single Coded Systems</p>
    <p>HACFS-PC HACFS-LRC</p>
    <p>Cold data in CC3: 75.5%</p>
  </div>
  <div class="page">
    <p>System Comparisons</p>
    <p>HACFS using Product Codes Colossus FS FB HDFS Azure</p>
    <p>Degraded Read Latency 25.2% 46.1% 25.4% ReconstrucNon Time 14.3% 43.7% 21.4% Storage Overhead 2.3% -4.7% -10.2%</p>
    <p>HACFS using LRCs Colossus FS FB HDFS Azure</p>
    <p>Degraded Read Latency 21.5% 43.3% 21.2% ReconstrucNon Time -3.1% 32.2% 5.6% Storage Overhead 7.7% 1.1% -4.2%</p>
  </div>
  <div class="page">
    <p>System Comparisons</p>
    <p>HACFS using Product Codes Colossus FS FB HDFS Azure</p>
    <p>Degraded Read Latency 25.2% 46.1% 25.4% ReconstrucNon Time 14.3% 43.7% 21.4% Storage Overhead 2.3% -4.7% -10.2%</p>
    <p>HACFS using LRCs Colossus FS FB HDFS Azure</p>
    <p>Degraded Read Latency 21.5% 43.3% 21.2% ReconstrucNon Time -3.1% 32.2% 5.6% Storage Overhead 7.7% 1.1% -4.2%</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Single Coded Systems</p>
  </div>
  <div class="page">
    <p>A Tale of Two Erasure Codes in HDFS</p>
    <p>Mingyuan Xia, Mohit Saxena Mario Blaum, David Pease</p>
    <p>IBM Research Almaden &amp; McGill University</p>
    <p>FAST15</p>
    <p>Thanks Q/A</p>
  </div>
</Presentation>
