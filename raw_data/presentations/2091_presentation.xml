<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing P. Balaji, Argonne National Laboratory</p>
    <p>W. Feng and J. Archuleta, Virginia Tech</p>
    <p>H. Lin, North Carolina State University</p>
    <p>SC|07 Storage Challenge</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Biological Problems of Significance  Discover missing genes via sequence-similarity computations</p>
    <p>(i.e., mpiBLAST, http://www.mpiblast.org/)  Generate a complete genome sequence-similarity tree to speed</p>
    <p>up future sequence searches</p>
    <p>Our Contributions  Worldwide Supercomputer</p>
    <p>Compute: ~12,000 cores across six U.S. supercomputing centers  Storage: 0.5-petabyte at the Tokyo Institute of Technology</p>
    <p>ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing</p>
    <p>Decouples computation and I/O and drastically reduces I/O overhead  Delivers 90% storage bandwidth utilization</p>
    <p>A 100x improvement over (vanilla) mpiBLAST</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation  Problem Statement  Approach  Results  Conclusion</p>
  </div>
  <div class="page">
    <p>Importance of Sequence Search</p>
    <p>Motivation</p>
    <p>Why sequence search is so important</p>
  </div>
  <div class="page">
    <p>Challenges in Sequence Search</p>
    <p>Observations  Overall size of genomic databases</p>
    <p>doubles every 12 months  Processing horsepower doubles</p>
    <p>only every 18-24 months</p>
    <p>Consequence  The rate at which genomic</p>
    <p>databases are growing is outstripping our ability to compute (i.e., sequence search) on them.</p>
  </div>
  <div class="page">
    <p>Problem Statement #1</p>
    <p>The Case of the Missing Genes  Problem</p>
    <p>Most current genes have been detected by a gene-finder program, which can miss real genes</p>
    <p>Approach  Every possible location along a genome should be</p>
    <p>checked for the presence of genes  Solution</p>
    <p>All-to-all sequence search of all 567 microbial genomes that have been completed to date</p>
    <p>but requires more resources than can be traditionally found at a single supercomputer center 2.63 x 1014 sequence searches!</p>
  </div>
  <div class="page">
    <p>Problem Statement #2</p>
    <p>The Search for a Genome Similarity Tree  Problem</p>
    <p>Genome databases are stored as an unstructured collection of sequences in a flat ASCII file</p>
    <p>Approach  Completely correlate all sequences by matching each</p>
    <p>sequence with every other sequence  Solution</p>
    <p>Use results from all-to-all sequence search to create genome similarity tree</p>
    <p>but requires more resources than can be traditionally found at a single supercomputer center</p>
    <p>Level 1: 250 matches; Level 2: 2502 = 62,500 matches; Level 3: 2503 = 15,625,000 matches</p>
  </div>
  <div class="page">
    <p>Approach: Hardware Infrastructure</p>
    <p>Worldwide Supercomputer  Six U.S. supercomputing institutions (~12,000 processors) and</p>
    <p>one Japanese storage institution (0.5 petabytes), ~10,000 kilometers away</p>
  </div>
  <div class="page">
    <p>Approach: ParaMEDIC Architecture</p>
    <p>ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi ct ure.</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi ct ure.</p>
    <p>Paramedic API (PMAPI)</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>mpiBLASTQ ui ckTi me and a decompressor are needed to see thi s pi ct ure.</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>Data Encryption</p>
    <p>Paramedic Data Tools</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>Data Integrity</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>Communication Services</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi ct ure.</p>
    <p>Direct Network</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi ct ure.</p>
    <p>D i st ri but ed</p>
    <p>Fi l e System</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see t hi s pi cture.</p>
    <p>Application Plugins</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi ct ure.</p>
    <p>Basic data Compression</p>
    <p>plugin</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see t hi s pi cture.</p>
    <p>mpi BLAST</p>
    <p>pl ugi n</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi cture.</p>
    <p>Vi z .</p>
    <p>pl ugi n</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi ct ure.</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed to see thi s pi cture.</p>
    <p>Remote Visualization</p>
    <p>Q ui ckTi me and a decompressor</p>
    <p>are needed t o see t hi s pi cture.</p>
    <p>.</p>
    <p>Applications</p>
    <p>ParaMEDIC API (PMAPI)</p>
    <p>ParaMEDIC Data Tools</p>
    <p>Encryption Data</p>
    <p>Encryption Data</p>
    <p>Data Integrity Data</p>
    <p>Integrity</p>
  </div>
  <div class="page">
    <p>Approach: ParaMEDIC Framework</p>
    <p>Compute Master I/O Master</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Master</p>
    <p>mpiBLAST Worker</p>
    <p>mpiBLAST Worker</p>
    <p>Query Raw Metadata Query</p>
    <p>Write Results</p>
    <p>Generate Temp Database</p>
    <p>Read Temp Database</p>
    <p>I/O Work e rs</p>
    <p>Com pute Work e rs</p>
    <p>I/O Se rve rs hos ting file</p>
    <p>s ys te m</p>
    <p>The ParaMEDIC Framework</p>
  </div>
  <div class="page">
    <p>Preliminary Results: ANL-VT Supercomputer</p>
    <p>ANL to Virginia Tech Encrypted File-system</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>( se</p>
    <p>c)</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
  </div>
  <div class="page">
    <p>Preliminary Results: Teragrid Supercomputer</p>
    <p>Teragrid Infrastructure</p>
    <p>Query Size (KB)</p>
    <p>E xe</p>
    <p>cu tio</p>
    <p>n Ti</p>
    <p>m e</p>
    <p>( se</p>
    <p>c)</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
  </div>
  <div class="page">
    <p>Storage Challenge: Compute Resources</p>
    <p>2200-processor System X cluster (Virginia Tech)  2048-processor BG/L supercomputer (Argonne)  5832-processor SiCortex supercomputer (Argonne)  700-processor Intel Jazz cluster (Argonne)  320+60 processors on TeraGrid (U. Chicago &amp; SDSC)  512-processor Oliver cluster (CCT at LSU)  A few hundred processors on Open Science Grid</p>
    <p>(RENCI)  128-processors on the Breadboard cluster (Argonne)</p>
    <p>Total: ~12,000 Processors</p>
  </div>
  <div class="page">
    <p>Storage Challenge: Storage Resources</p>
    <p>Clients  10 quad-core SunFire X4200  Two 16-core SunFire X4500 systems.</p>
    <p>Object Storage Servers (OSS)  20 SunFire X4500</p>
    <p>Object Storage Targets (OST)  140 SunFire X4500 (each OSS has 7 OSTs)</p>
    <p>RAID configuration for OST  RAID5 with 6 drives</p>
    <p>Network: Gigabit Ethernet  Kernel: 2.6  Lustre Version: 1.6.2</p>
    <p>QuickTime and a TIFF (Uncompressed) decompressor</p>
    <p>are needed to see this picture.</p>
  </div>
  <div class="page">
    <p>Storage Utilization with Lustre</p>
    <p>Storage Utilization with Lustre</p>
    <p>Computation Threads</p>
    <p>T h ro</p>
    <p>u g</p>
    <p>h p u t</p>
    <p>(M b</p>
    <p>p s )</p>
    <p>mpiBLAST ParaMEDIC MPI-IO-Test</p>
  </div>
  <div class="page">
    <p>Storage Utilization Breakdown with Lustre</p>
    <p>ParaMEDIC Compute-I/O breakup (Lustre)</p>
    <p>Computation Threads</p>
    <p>P e rc</p>
    <p>e n ta</p>
    <p>g e</p>
    <p>I/O Percent</p>
    <p>Compute Percent</p>
  </div>
  <div class="page">
    <p>Storage Utilization (Local Disks)</p>
    <p>Storage Utilization with Local Disk</p>
    <p>Computation Threads</p>
    <p>T h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t (M</p>
    <p>b p</p>
    <p>s )</p>
    <p>mpiBLAST</p>
    <p>ParaMEDIC</p>
    <p>MPI-IO-Test</p>
  </div>
  <div class="page">
    <p>Storage Utilization Breakdown (Local Disks)</p>
    <p>ParaMEDIC Compute-I/O breakup (Local Disk)</p>
    <p>Computation Threads</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e I/O Percent</p>
    <p>Compute Percent</p>
  </div>
  <div class="page">
    <p>Conclusion: Biology</p>
    <p>Biological Problems Addressed  Discovering missing genes via sequence-similarity computations</p>
    <p>speed-up future sequence searches.</p>
    <p>Status  Missing Genes</p>
    <p>Now possible!  Ongoing with biologists</p>
    <p>Complete Similarity Tree  Large % of chromosomes</p>
    <p>do not match any other chromosomes</p>
    <p>Percentage Not Matched</p>
    <p>Replicon ID</p>
    <p>P e</p>
    <p>rc e n</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Conclusion: Computer Science</p>
    <p>Contributions  Worldwide supercomputer consisting of ~12,000 processors and</p>
    <p>ParaMEDIC: Parallel Metadata Environment for Distributed I/O and Computing</p>
    <p>Decouples computation and I/O and drastically reduces I/O overhead.</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>Computational Resources  K. Shinpaugh, L. Scharf, G. Zelenka (Virginia Tech)  I. Foster, M. Papka (U. Chicago)  E. Lusk and R. Stevens (Argonne National Laboratory)  M. Rynge, J. McGee, D. Reed (RENCI)  S. Jha and H. Liu (CCT at LSU)</p>
    <p>Storage Resources  S. Matsuoka (Tokyo Inst. of Technology)  S. Ihara, T. Kujiraoka (Sun Microsystems, Japan)  S. Vail, S. Cochrane (Sun Microsystems, USA)</p>
  </div>
</Presentation>
