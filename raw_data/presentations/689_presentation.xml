<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The SCADS Director: Scaling a Distributed Storage System Under</p>
    <p>Stringent Performance Requirements</p>
    <p>Beth Trushkowsky, Peter Bodk, Armando Fox,</p>
    <p>Michael J. Franklin, Michael I. Jordan, David A. Patterson</p>
    <p>FAST 2011</p>
  </div>
  <div class="page">
    <p>elasticity for interactive web apps</p>
    <p>Interactivity Service-Level-Objective: Over any 1-minute interval, 99% of requests are satisfied in less than 100ms</p>
    <p>Targeted systems features: - horizontally scalable - API for data movement - backend for interactive apps</p>
    <p>clients</p>
    <p>web servers</p>
    <p>storage</p>
  </div>
  <div class="page">
    <p>wikipedia workload trace - June 2009</p>
    <p>Michael Jackson dies</p>
  </div>
  <div class="page">
    <p>overprovisioning storage system</p>
    <p>(assuming data stored on ten servers)</p>
    <p>overprovision by 300% to handle spike</p>
  </div>
  <div class="page">
    <p>contributions</p>
    <p>Cloud computing is mechanism for storage elasticity  Scale up when needed  Scale down to save money</p>
    <p>We address the scaling policy  Challenges of latency-based scaling  Model-based approach for elasticity to deal with stringent SLO  Fine-grained workload monitoring aids in scaling up and down  Show elasticity for both a hotspot and a diurnal workload</p>
    <p>pattern</p>
  </div>
  <div class="page">
    <p>SCADS key/value store</p>
    <p>Features  Partitioning (until some minimum data size)  Replication  Add/remove servers</p>
    <p>Properties  Range-based partitioning  Data maintained in memory for performance  Eventually consistent</p>
    <p>(see SCADS: Scale-independent storage for social computing applications, CIDR09)</p>
  </div>
  <div class="page">
    <p>classical closed-loop control for elasticity?</p>
    <p>SCADS cluster</p>
    <p>Controller</p>
    <p>actions</p>
    <p>Action Executor</p>
    <p>actions</p>
    <p>upper %-tile latency</p>
    <p>sampled latency</p>
    <p>sampled latency</p>
    <p>config</p>
  </div>
  <div class="page">
    <p>oscillations from a noisy signal</p>
    <p>time</p>
    <p>Noisy signal Will smoothing help?</p>
  </div>
  <div class="page">
    <p>too much smoothing masks spike</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>variation for smoothing intervals</p>
    <p>smoothing interval [min]</p>
    <p>st an</p>
    <p>da rd</p>
    <p>d ev</p>
    <p>ia tio</p>
    <p>n [m</p>
    <p>s] (l</p>
    <p>og s</p>
    <p>ca le</p>
    <p>)</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>! !</p>
    <p>! !</p>
    <p>! !</p>
    <p>! !</p>
    <p>! ! !</p>
    <p>!</p>
    <p>mean latency</p>
    <p>(SCADS running on Amazon EC2)</p>
    <p>raw 99th %</p>
    <p>raw mean</p>
  </div>
  <div class="page">
    <p>model-predictive control (MPC)  MPC instead of classical closed-loop</p>
    <p>Upper %-tile latency is a noisy signal  Use per-server workload as predictor of upper %-tile latency  Therefore need a model that predicts SLO violations based on</p>
    <p>observed workload</p>
    <p>Reacting with MPC  Use model of the system to determine a sequence of actions</p>
    <p>to change state to meet constraint  Execute first steps, then re-evaluate</p>
    <p>Model workload SLO violation</p>
  </div>
  <div class="page">
    <p>model-predictive control loop</p>
    <p>SCADS cluster</p>
    <p>Controller</p>
    <p>actions</p>
    <p>Action Executor</p>
    <p>actions</p>
    <p>Performance Models</p>
    <p>Workload Histogram</p>
    <p>sampled workload</p>
    <p>smoothed workload</p>
    <p>config upper %-tile</p>
    <p>latency</p>
    <p>sampled latency</p>
    <p>sampled latency</p>
    <p>config</p>
  </div>
  <div class="page">
    <p>building a performance model  Benchmark SCADS servers</p>
    <p>on Amazons EC2</p>
    <p>Steady-state model  Single server capacity  Explore space of possible</p>
    <p>workload  Binary classifier: SLO violation</p>
    <p>or not</p>
    <p>get workload [req/sec]</p>
    <p>pu t w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>[re q/</p>
    <p>se c]</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</p>
    <p>!</p>
    <p>! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</p>
    <p>!</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! !</p>
    <p>get workload [req/sec]</p>
    <p>pu t w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>[re q/</p>
    <p>se c]</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>!</p>
    <p>! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</p>
    <p>!</p>
    <p>! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !</p>
    <p>!</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! ! !</p>
    <p>! ! !</p>
    <p>No violation</p>
    <p>Violation</p>
  </div>
  <div class="page">
    <p>how much data to move?</p>
    <p>time</p>
    <p>workload (requests/sec)</p>
  </div>
  <div class="page">
    <p>finer-granularity workload monitoring  Need fine-grained workload monitoring</p>
    <p>Data movement especially impacts tail of latency distribution  Only move enough data to alleviate performance issues  Move data quickly  Better for scaling down later</p>
    <p>Monitor workload on small units of data (bins)  Move/copy bins between servers</p>
  </div>
  <div class="page">
    <p>summary of approach</p>
    <p>Fine-grained monitoring and performance model  Determine amount of data to move from overloaded server  Estimate how much extra room an underloaded server has  Know when safe to coalesce servers</p>
    <p>Replication for predictability and robustness  See paper and/or tonights poster session</p>
  </div>
  <div class="page">
    <p>controller stages</p>
    <p>Stage 1: Replicate</p>
    <p>W o rk</p>
    <p>lo ad</p>
    <p>threshold</p>
    <p>B in</p>
    <p>s</p>
    <p>Stage 2: Partition</p>
    <p>Stage 3: Allocate servers</p>
    <p>Storage nodes N1 N2 N3 N4 N5 N6 N7</p>
  </div>
  <div class="page">
    <p>controller stages</p>
    <p>W o rk</p>
    <p>lo ad</p>
    <p>threshold</p>
    <p>B in</p>
    <p>s</p>
    <p>destination</p>
    <p>Stage 1: Replicate</p>
    <p>Stage 2: Partition</p>
    <p>Stage 3: Allocate servers</p>
    <p>Storage nodes N1 N2 N3 N4 N5 N6 N7</p>
  </div>
  <div class="page">
    <p>controller stages</p>
    <p>W o rk</p>
    <p>lo ad</p>
    <p>threshold</p>
    <p>B in</p>
    <p>s</p>
    <p>N1 N2 N3 N4 N5 N6 N7</p>
    <p>Stage 1: Replicate</p>
    <p>Stage 2: Partition</p>
    <p>Stage 3: Allocate servers</p>
  </div>
  <div class="page">
    <p>experimental results  Experiment setup</p>
    <p>Up to 20 SCADS servers run on m1.small instances on Amazon EC2  Server capacity: 800MB, due to in-memory restriction  5-10 data bins per server  100ms SLO on read latency</p>
    <p>Workload profiles  Hotspot</p>
    <p>100% workload increase in five minutes on a single data item  Based on spike experienced by CNN.com on 9/11</p>
    <p>Diurnal  Workload increases during the day, decreases at night  Replayed trace at 12x speedup</p>
  </div>
  <div class="page">
    <p>extra workload directed to single data item</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>aggregate request rate re</p>
    <p>qu es</p>
    <p>t r at</p>
    <p>e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>qu es</p>
    <p>t r at</p>
    <p>e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>per-bin request rate</p>
    <p>hot bin</p>
    <p>other 199 bins</p>
    <p>time [min]</p>
  </div>
  <div class="page">
    <p>replicating hot data</p>
    <p>per-bin request rate</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>time [min]</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>qu es</p>
    <p>t r at</p>
    <p>e</p>
    <p>pe r</p>
    <p>bi n</p>
    <p>re qu</p>
    <p>es t r</p>
    <p>at e</p>
    <p>p er</p>
    <p>ce nt</p>
    <p>ile la</p>
    <p>te nc</p>
    <p>y [m</p>
    <p>s]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
  </div>
  <div class="page">
    <p>scaling up and down</p>
    <p>Number of servers  two experiments close</p>
    <p>to ideal</p>
    <p>Over-provisioning tradeoff  Amplify workload by</p>
    <p>Savings  Known peak: 16%  30% headroom: 41%</p>
    <p>simulated time [min] w</p>
    <p>or kl</p>
    <p>oa d</p>
    <p>ra te</p>
    <p>[r eq</p>
    <p>/s ]</p>
    <p>number of servers</p>
    <p>aggregate request rate</p>
    <p>ideal elastic 10%</p>
    <p>elastic 30%</p>
    <p>simulated time [min]</p>
    <p>nu m</p>
    <p>be r o</p>
    <p>f s er</p>
    <p>ve rs</p>
    <p>elastic 0.3 elastic 0.1 ideal</p>
  </div>
  <div class="page">
    <p>cost-risk tradeoff</p>
    <p>Over-provisioning  Allows more time before violation occurs  Cost-risk tradeoff</p>
    <p>Comparing over-provisioning for diurnal experiment  Recall SLO parameters: threshold, percentile, interval  Over-provisioning factor of 30% vs 10%</p>
    <p>Interval Max percentile achieved</p>
  </div>
  <div class="page">
    <p>conclusion  Elasticity for storage servers possible by leveraging cloud</p>
    <p>computing</p>
    <p>Upper percentile too noisy  Model-based approach to build control framework for</p>
    <p>elasticity subject to stringent performance SLO</p>
    <p>Finer-grained workload monitoring  Minimize impact of data movement on performance  Quickly responding to workload fluctuations</p>
    <p>Evaluated on EC2 with hotspot and diurnal workloads</p>
  </div>
  <div class="page">
    <p>increasing replication</p>
    <p>lantecy [ms]</p>
    <p>C D</p>
    <p>F</p>
  </div>
</Presentation>
