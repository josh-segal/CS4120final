<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Jacob Leverich</p>
    <p>and Christos Kozyrakis</p>
    <p>Stanford University</p>
    <p>On the Energy (In)efficiency of Hadoop:</p>
    <p>Scale-down Efficiency</p>
  </div>
  <div class="page">
    <p>The current design of Hadoop precludes</p>
    <p>scale-down of commodity clusters.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Hadoop crash-course</p>
    <p>Scale-down efficiency</p>
    <p>How Hadoop precludes scale-down</p>
    <p>How to fix it</p>
    <p>Did we fix it?</p>
    <p>Future work</p>
  </div>
  <div class="page">
    <p>Hadoop crash-course</p>
    <p>Hadoop == Distributed Processing Framework</p>
    <p>Hadoop MapReduce Google MapReduce</p>
    <p>Tasks are automatically distributed by the framework.</p>
    <p>Hadoop Distributed File System Google File System</p>
    <p>Files divided into large (64MB) blocks; amortizes overheads.</p>
    <p>Blocks replicated for availability and durability management.</p>
  </div>
  <div class="page">
    <p>Scale-down motivation</p>
    <p>HP Proliant DL140 G3</p>
    <p>Typical utilization [Barroso and Holzle, 2007]</p>
  </div>
  <div class="page">
    <p>Scale-down for energy proportionality</p>
    <p>= 4 x P(40%) = 4 x 325W = 1300W</p>
    <p>= 2 x P(80%) = 2 x 365W = 730W</p>
  </div>
  <div class="page">
    <p>The problem: storage consolidation</p>
    <p>Hadoop Distributed File System Consolidate computation? Easy.</p>
    <p>Consolidate storage? Not (as) easy.</p>
    <p>All servers must be available, even during low-load periods. [Barroso and Holzle, 2007]</p>
    <p>Hadoop inherited this feature from Google File System</p>
    <p>:-(</p>
  </div>
  <div class="page">
    <p>HDFS and block replication</p>
    <p>block replication table = replica</p>
    <p>Allocate evenly.</p>
    <p>Replication factor = 3</p>
    <p>Node</p>
    <p>B lo</p>
    <p>ck</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
    <p>F</p>
    <p>G</p>
    <p>H</p>
  </div>
  <div class="page">
    <p>Attempted scale-down</p>
    <p>Node</p>
    <p>B lo</p>
    <p>ck</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
    <p>F</p>
    <p>G</p>
    <p>H</p>
    <p>Problems:</p>
    <p>Scale-down vs. Self-healing Wasted capacity: sleeping replicas != lost replicas</p>
    <p>Flurry of net &amp; disk activity!</p>
    <p>Which nodes to disable?</p>
    <p>Must maintain data availability</p>
  </div>
  <div class="page">
    <p>How to fix it</p>
    <p>Scale-down vs. Self-healing Wasted capacity: sleeping replicas != lost replicas</p>
    <p>Flurry of net &amp; disk activity!</p>
    <p>Which nodes to disable?</p>
    <p>Must maintain data availability</p>
    <p>Self-non-healing</p>
  </div>
  <div class="page">
    <p>Self-non-healing</p>
    <p>Node</p>
    <p>B lo</p>
    <p>ck</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
    <p>F</p>
    <p>G</p>
    <p>H</p>
    <p>Zzzzz</p>
    <p>Coordinate with Hadoop when we put a node to sleep</p>
    <p>Prevent block re-replications</p>
  </div>
  <div class="page">
    <p>New RPCs in HDFS primary node</p>
    <p>sleepNode(String hostname)</p>
    <p>Similar to node decommissioning, but dont replicate blocks</p>
    <p>% hadoop dfsadmin sleepNode 10.10.1.80:50020</p>
    <p>Save blocks to a sleeping blocks map for bookkeeping</p>
    <p>Ignore heartbeats and block reports from this node</p>
    <p>wakeNode(String hostname)</p>
    <p>Watch for heartbeats, force node to send block report</p>
    <p>Execute arbitrary commands (i.e. send wake-on-LAN packet)</p>
    <p>wakeBlock(Block target)</p>
    <p>Wake a sleeping node that has a particular block</p>
  </div>
  <div class="page">
    <p>How to fix it</p>
    <p>Scale-down vs. Self-healing Wasted capacity: sleeping replicas != lost replicas</p>
    <p>Flurry of net &amp; disk activity!</p>
    <p>Which nodes to disable?</p>
    <p>Must maintain data availability</p>
    <p>Self-non-healing</p>
    <p>Covering Subset replication invariant</p>
  </div>
  <div class="page">
    <p>Replication placement invariants</p>
    <p>Hadoop uses simple invariants to direct block placement</p>
    <p>Example: Rack-Aware Block Placement</p>
    <p>Protects against common-mode failures (i.e. switch failure, power delivery failure)</p>
    <p>Invariant: Blocks must have replicas on at least 2 racks.</p>
    <p>Is there some energy-efficient replication invariant?</p>
    <p>Must inform our decision on which nodes we can disable.</p>
  </div>
  <div class="page">
    <p>Covering subset replication invariant</p>
    <p>Goal:</p>
    <p>Maximize the number of servers that can simultaneously sleep.</p>
    <p>Strategy:</p>
    <p>Aggregate live data onto a covering subset of nodes.</p>
    <p>Never turn off a node in the covering subset.</p>
    <p>Invariant:</p>
    <p>Every block must have one replica in the covering subset.</p>
  </div>
  <div class="page">
    <p>Covering subset replication invariant</p>
    <p>Node</p>
    <p>B lo</p>
    <p>ck</p>
    <p>A</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
    <p>F</p>
    <p>G</p>
    <p>H</p>
    <p>Zzzzz</p>
  </div>
  <div class="page">
    <p>How to fix it</p>
    <p>Scale-down vs. Self-healing Wasted capacity: sleeping replicas != lost replicas</p>
    <p>Flurry of net &amp; disk activity!</p>
    <p>Which nodes to disable?</p>
    <p>Must maintain data availability</p>
    <p>Self-non-healing</p>
    <p>Covering Subset replication invariant</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Disable n nodes, compare Hadoop job energy &amp; perf. Individual runs of webdata_sort/webdata_scan from GridMix</p>
    <p>Cluster 36 nodes, HP Proliant DL140 G3</p>
    <p>Energy model Validated estimate based on CPU utilization</p>
    <p>Disabled node = 0 Watts</p>
    <p>Possible to evaluate hypothetical hardware</p>
  </div>
  <div class="page">
    <p>Results: Performance</p>
    <p>It slows down (obviously)</p>
    <p>Peak performance benchmark</p>
    <p>Sort (network intensive) worse off than Scan</p>
    <p>Amdahls Law</p>
  </div>
  <div class="page">
    <p>Results: Energy</p>
    <p>Less energy consumed for same amount of work</p>
    <p>Nodes consume energy more than they improve performance</p>
    <p>Slower systems usually more efficient; high performance is a trade-off!</p>
  </div>
  <div class="page">
    <p>Results: Power</p>
    <p>Excellent knob for clusterlevel power capping</p>
    <p>Much larger dynamic range than tweaking frequency/voltage at the server level</p>
  </div>
  <div class="page">
    <p>Results: The Bottom Line</p>
    <p>Operational Hadoop clusters can scale-down.</p>
    <p>We reduce energy consumption</p>
    <p>at the expense of single-job latency.</p>
  </div>
  <div class="page">
    <p>Continuing Work</p>
  </div>
  <div class="page">
    <p>Covering subset: mechanism vs. policy</p>
    <p>The replication invariant is a mechanism.</p>
    <p>Which nodes constitute a subset is policy (open question).</p>
    <p>Size trade-off</p>
    <p>Too small: Low capacity and performance bottleneck</p>
    <p>Too large: Wasted energy on idle nodes</p>
    <p>How many covering subsets?</p>
    <p>Invariant: Blocks must have a replica in each covering subsets.</p>
  </div>
  <div class="page">
    <p>Quantify Trade-offs</p>
    <p>Random Fault Injection experiments</p>
    <p>What happens when a covering subset node fails?</p>
    <p>How much do you trust idle disks?</p>
    <p>Performance</p>
    <p>Availability</p>
    <p>Energy consumption</p>
    <p>Durability</p>
  </div>
  <div class="page">
    <p>Dynamic Power Management</p>
    <p>Algorithmically decide which nodes to sleep or wakeup</p>
    <p>What signals to use?</p>
    <p>CPU utilization?</p>
    <p>Disk/net utilization?</p>
    <p>Job Queue length?</p>
    <p>MapReduce and HDFS must cooperate</p>
    <p>i.e. idle nodes may host transient Map outputs</p>
  </div>
  <div class="page">
    <p>Workloads</p>
    <p>Benchmarks</p>
    <p>HBase/BigTable vs. MapReduce</p>
    <p>Short, unpredictable data access vs. long streaming access</p>
    <p>Quality of service and throughput are important</p>
    <p>Pig vs. Sort+Scan</p>
    <p>Recorded job traces vs. random job traces</p>
    <p>Peak performance vs. fractional utilization</p>
    <p>What are typical usage patterns?</p>
  </div>
  <div class="page">
    <p>Scale</p>
    <p>Network hierarchy</p>
    <p>Hadoop framework inefficiencies</p>
    <p>Computational overhead (must process many block reports!)</p>
    <p>Experiments on Amazon EC2</p>
    <p>Awarded an Amazon Web Services grant</p>
    <p>Cant measure power! Must use a model.</p>
    <p>Any Amazonians here? Lets make a validated energy model.</p>
  </div>
</Presentation>
