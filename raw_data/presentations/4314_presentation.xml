<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Discourse Marker Augmented Network with Reinforcement Learning</p>
    <p>for Natural Language Inference</p>
    <p>Authors Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang, Deng Cai, Xiaofei He</p>
    <p>Organization Zhejiang University, China</p>
  </div>
  <div class="page">
    <p>What is Natural Language Inference (NLI)?</p>
    <p>Premise Hypothesis</p>
    <p>Entailment</p>
  </div>
  <div class="page">
    <p>What is Natural Language Inference (NLI)?</p>
    <p>?Premise Hypothesis Neutral</p>
  </div>
  <div class="page">
    <p>What is Natural Language Inference (NLI)?</p>
    <p>Premise Hypothesis</p>
    <p>Contradiction</p>
  </div>
  <div class="page">
    <p>Applications  Question Answering  Machine Translation  Semantic Search  Text Summarization</p>
  </div>
  <div class="page">
    <p>Discourse Marker  A discourse marker is a word or a phrase that plays a role in managing the flow and structure of discourse.</p>
    <p>Examples: so, because, and, but, or</p>
  </div>
  <div class="page">
    <p>Discourse Marker &amp; NLI?</p>
    <p>But Because If Although And So</p>
    <p>Entailment Neutral Contradiction</p>
  </div>
  <div class="page">
    <p>Related Works  Datasets</p>
    <p>SNLI (Bowman et al., 2015) MultiNLI (Williams et al., 2017)</p>
    <p>SOTA Neural Network Models CAFE (Tay et al., 2017) KIM (Chen et al., 2017) DIIN (Gong et al., 2018)</p>
  </div>
  <div class="page">
    <p>Related Works  Transfer Learning for NLI</p>
    <p>Skip-thoughts (Vendrov et al., 2016) Cove (McCann et al., 2017)</p>
    <p>Discourse Marker Applications DisSent (Nie et al., 2017)</p>
  </div>
  <div class="page">
    <p>Discourse Marker Prediction (DMP)</p>
    <p>Its rainy outside but we will not take the umbrella</p>
    <p>Its rainy outside + But + We will not take the umbrella</p>
    <p>(S1, S2) Neural Networks M</p>
    <p>So Because But   If</p>
  </div>
  <div class="page">
    <p>Discourse Marker Prediction (DMP)</p>
    <p>Glove</p>
    <p>Glove</p>
    <p>BiLSTM Sentence Representations</p>
    <p>Sentence1</p>
    <p>Sentence2</p>
    <p>Prediction</p>
    <p>Last hidden state</p>
    <p>Max pooling over all the hidden states</p>
    <p>To Be Transferred</p>
  </div>
  <div class="page">
    <p>Discourse Marker Augmented Network (NLI Model)</p>
    <p>Glove</p>
    <p>Glove</p>
    <p>Char</p>
    <p>Char POS</p>
    <p>POS</p>
    <p>NER</p>
    <p>NER EM</p>
    <p>EM</p>
    <p>BiLSTM</p>
    <p>Hypothesis</p>
    <p>Encoding Layer</p>
    <p>Premise</p>
  </div>
  <div class="page">
    <p>Discourse Marker Augmented Network (NLI Model)</p>
    <p>Glove</p>
    <p>Glove</p>
    <p>Char</p>
    <p>Char POS</p>
    <p>POS</p>
    <p>NER</p>
    <p>NER EM</p>
    <p>EM</p>
    <p>BiLSTM</p>
    <p>Hypothesis</p>
    <p>Premise</p>
    <p>BiLSTM Sentence Representations</p>
    <p>Pre-trained DMP Model:</p>
  </div>
  <div class="page">
    <p>Discourse Marker Augmented Network (NLI Model) Interaction ------ Similarity Matrix</p>
    <p>The i-th word of the premise The j-th word of the hypothesis</p>
    <p>The sentence representation of the premise The sentence representation of the hypothesis</p>
  </div>
  <div class="page">
    <p>Discourse Marker Augmented Network (NLI Model)</p>
    <p>Attention Mechanism</p>
    <p>Prediction</p>
    <p>Similarity Matrix</p>
    <p>Modeling vector of the premise</p>
    <p>Modeling vector of the hypothesis</p>
    <p>The sentence representation of the premise The sentence representation of the hypothesis</p>
  </div>
  <div class="page">
    <p>Training Cross Entropy Loss</p>
    <p>Correct Label: neutral</p>
    <p>Original Labels: neutral, neutral, entailment, entailment, neutral</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Previous action policy that predicts the label given P and H.</p>
  </div>
  <div class="page">
    <p>Experiments (Datasets)  Stanford Natural Language Inference (SNLI) (Bowman et al., 2015)</p>
    <p>Multi-Genre Natural Language Inference (MultiNLI) (Williams et al., 2017) 433k human annotated sentences pairs</p>
    <p>BookCorpus (Zhu et al., 2015) 6.5M pairs of sentences for 8 discourse markers</p>
  </div>
  <div class="page">
    <p>Experiments (Results)</p>
    <p>Sentence EncodingBased Models</p>
    <p>Other Neural Network Models</p>
    <p>Ensemble Models</p>
  </div>
  <div class="page">
    <p>Experiments (Analysis)</p>
  </div>
  <div class="page">
    <p>Experiments (Analysis)</p>
    <p>Premise: 3 young man in hoods standing in the middle of a quiet street facing the camera. Hypothesis: Three people sit by a busy street bare-headed.</p>
  </div>
  <div class="page">
    <p>Conclusion  We solve the task of the natural language inference via</p>
    <p>transferring knowledge from another supervised task.</p>
    <p>We propose a new objective function to make full use of the labels information.</p>
    <p>In the future work, we would like to explore some other transfer learning sources.</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
  </div>
</Presentation>
