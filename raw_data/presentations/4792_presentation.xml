<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Automatic Labeling of Multinomial Topic Models</p>
    <p>Qiaozhu Mei, Xuehua Shen, ChengXiang Zhai</p>
    <p>University of Illinois at Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background: statistical topic models</p>
    <p>Labeling a topic model  Criteria and challenge</p>
    <p>Our approach: a probabilistic framework</p>
    <p>Experiments</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Statistical Topic Models for Text Mining</p>
    <p>Text Collections</p>
    <p>Probabilistic Topic Modeling</p>
    <p>web 0.21 search 0.10 link 0.08 graph 0.05</p>
    <p>Subtopic discovery</p>
    <p>Opinion comparison</p>
    <p>Summarization</p>
    <p>Topical pattern analysis</p>
    <p>term 0.16 relevance 0.08 weight 0.07 feedback 0.04 independ. 0.03 model 0.03</p>
    <p>Topic models (Multinomial distributions)</p>
    <p>PLSA [Hofmann 99] LDA [Blei et al. 03]</p>
    <p>Author-Topic [Steyvers et al. 04]</p>
    <p>CPLSA [Mei &amp; Zhai 06]</p>
    <p>Pachinko allocation [Li &amp; McCallum 06]</p>
    <p>Topic over time [Wang et al. 06]</p>
  </div>
  <div class="page">
    <p>Topic Models: Hard to Interpret</p>
    <p>Use top words  automatic, but hard to make sense</p>
    <p>Human generated labels  Make sense, but cannot scale up</p>
    <p>term 0.16 relevance 0.08 weight 0.07 feedback 0.04 independence 0.03 model 0.03 frequent 0.02 probabilistic 0.02 document 0.02</p>
    <p>Retrieval Models</p>
    <p>Question: Can we automatically generate understandable labels for topics?</p>
    <p>Term, relevance, weight, feedback</p>
    <p>insulin foraging foragers collected grains loads collection nectar</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>What is a Good Label?</p>
    <p>Semantically close (relevance)</p>
    <p>Understandable  phrases?</p>
    <p>High coverage inside topic</p>
    <p>Discriminative across topics</p>
    <p>term 0.1599 relevance 0.0752 weight 0.0660 feedback 0.0372 independence 0.0311 model 0.0310 frequent 0.0233 probabilistic 0.0188 document 0.0173</p>
    <p>iPod Nano</p>
    <p>Pseudo-feedback</p>
    <p>Information Retrieval</p>
    <p>Retrieval models</p>
    <p>Mei and Zhai 06: a topic in SIGIR</p>
  </div>
  <div class="page">
    <p>Our Method</p>
    <p>Collection (e.g., SIGIR)</p>
    <p>term 0.16 relevance 0.07 weight 0.07</p>
    <p>feedback 0.04 independence 0.03 model 0.03  filtering 0.21 collaborative 0.15  trec 0.18</p>
    <p>evaluation 0.10</p>
    <p>NLP Chunker Ngram Stat.</p>
    <p>information retrieval, retrieval model, index structure, relevance feedback,</p>
    <p>Candidate label pool1</p>
    <p>Relevance Score</p>
    <p>Information retrieval 0.26 retrieval models 0.19 IR models 0.17 pseudo feedback 0.06</p>
  </div>
  <div class="page">
    <p>Relevance (Task 2): the Zero-Order Score</p>
    <p>Intuition: prefer phrases well covering top words</p>
    <p>Clustering</p>
    <p>dimensional</p>
    <p>algorithm</p>
    <p>birch</p>
    <p>shape</p>
    <p>Latent Topic</p>
    <p>Good Label (l1): clustering algorithm</p>
    <p>body</p>
    <p>Bad Label (l2): body shape</p>
    <p>p(w| )</p>
    <p>p(clustering|) = 0.4</p>
    <p>p(dimensional|) = 0.3</p>
    <p>p(body|) = 0.001</p>
    <p>p(shape|) = 0.01</p>
    <p>&gt;)lg(</p>
    <p>)|lg(</p>
    <p>orithmaclusteringp</p>
    <p>orithmaclusteringp</p>
    <p>)(</p>
    <p>)|(</p>
    <p>shapebodyp</p>
    <p>shapebodyp</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Clustering</p>
    <p>hash</p>
    <p>dimension</p>
    <p>algorithm</p>
    <p>partition</p>
    <p>p(w | clustering algorithm )</p>
    <p>Good Label (l1) clustering algorithm</p>
    <p>Clustering</p>
    <p>hash</p>
    <p>dimension</p>
    <p>key</p>
    <p>algorithm</p>
    <p>p(w | hash join)</p>
    <p>key hash join  code hash table search hash join</p>
    <p>map keyhash algorithmkey</p>
    <p>hashkey tablejoin</p>
    <p>l2: hash join</p>
    <p>Relevance (Task 2): the First-Order Score</p>
    <p>Intuition: prefer phrases with similar context (distribution)</p>
    <p>Clustering</p>
    <p>dimension</p>
    <p>partition</p>
    <p>algorithm</p>
    <p>hash</p>
    <p>Topic</p>
    <p>P(w|)</p>
    <p>w</p>
    <p>rank</p>
    <p>ClwPMIwp )|,()|(</p>
    <p>Score (l,  ) = D(|| l)</p>
  </div>
  <div class="page">
    <p>Discrimination and Coverage (Tasks 3 &amp; 4)</p>
    <p>Discriminative across topic:  High relevance to target topic, low relevance to</p>
    <p>other topics</p>
    <p>High Coverage inside topic:  Use MMR strategy</p>
    <p>)],'(max)1(),([maxarg '</p>
    <p>llSimlScorel SlSLl</p>
    <p>),(),(),(' ,...,1,1,...,1 kiiii lScorelScorelScore</p>
  </div>
  <div class="page">
    <p>Variations and Applications</p>
    <p>Labeling document clusters  Document cluster  unigram language model</p>
    <p>Applicable to any task with unigram language model</p>
    <p>Context sensitive labels  Label of a topic is sensitive to the context</p>
    <p>An alternative way to approach contextual text mining</p>
    <p>tree, prune, root, branch  tree algorithms in CS  ? in horticulture</p>
    <p>? in marketing?</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Datasets:  SIGMOD abstracts; SIGIR abstracts; AP news data</p>
    <p>Candidate labels: significant bigrams; NLP chunks</p>
    <p>Topic models:  PLSA, LDA</p>
    <p>Evaluation:  Human annotators to compare labels generated from</p>
    <p>anonymous systems</p>
    <p>Order of systems randomly perturbed; score average over all sample topics</p>
  </div>
  <div class="page">
    <p>Result Summary</p>
    <p>Automatic phrase labels &gt;&gt; top words</p>
    <p>1-order relevance &gt;&gt; 0-order relevance  Bigram &gt; NLP chunks</p>
    <p>Bigram works better with literature; NLP better with news</p>
    <p>System labels &lt;&lt; human labels  Scientific literature is an easier task</p>
  </div>
  <div class="page">
    <p>Results: Sample Topic Labels</p>
    <p>tree 0.09 trees 0.08 spatial 0.08 b 0.05 r 0.04 disk 0.02 array 0.01 cache 0.01</p>
    <p>north 0.02 case 0.01 trial 0.01 iran 0.01 documents 0.01 walsh 0.009 reagan 0.009 charges 0.007</p>
    <p>the, of, a, and, to, data, &gt; 0.02  clustering 0.02 time 0.01 clusters 0.01 databases 0.01 large 0.01 performance 0.01 quality 0.005</p>
    <p>clustering algorithm clustering structure</p>
    <p>large data, data quality, high data,</p>
    <p>data application,</p>
    <p>iran contra</p>
    <p>r tree b tree</p>
    <p>indexing methods</p>
  </div>
  <div class="page">
    <p>Results: Context-Sensitive Labeling</p>
    <p>sampling estimation approximation histogram selectivity histograms</p>
    <p>selectivity estimation; random sampling;</p>
    <p>approximate answers;</p>
    <p>distributed retrieval; parameter estimation;</p>
    <p>mixture models;</p>
    <p>Context: Database (SIGMOD Proceedings)</p>
    <p>Context: IR (SIGIR Proceedings)</p>
    <p>Explore the different meaning of a topic with different contexts (content switch)</p>
    <p>An alternative approach to contextual text mining</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Labeling: A postprocessing step of all multinomial topic models</p>
    <p>A probabilistic approach to generate good labels  understandable, relevant, high coverage, discriminative</p>
    <p>Broadly applicable to mining tasks involving multinomial word distributions; context-sensitive</p>
    <p>Future work:  Labeling hierarchical topic models</p>
    <p>Incorporating priors</p>
  </div>
  <div class="page">
    <p>Thanks! - Please come to our poster tonight (#40)</p>
  </div>
</Presentation>
