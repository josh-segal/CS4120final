<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Loopy Belief Propagation for Bipartite Maximum Weight</p>
    <p>b-Matching</p>
    <p>Bert Huang and Tony Jebara Computer Science Department</p>
    <p>Columbia University New York, NY 10027</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>On bipartite graph, G = (U, V, E) {u1, . . . , un} ! U</p>
    <p>{v1, . . . , vn} ! V</p>
    <p>E = (ui, vj ), !i!j</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>On bipartite graph,</p>
    <p>A = weight matrix</p>
    <p>s.t. weight of edge</p>
    <p>G = (U, V, E) {u1, . . . , un} ! U</p>
    <p>{v1, . . . , vn} ! V</p>
    <p>E = (ui, vj ), !i!j</p>
    <p>(ui, vj ) = Aij</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>Task: Find the maximum weight subset of E such that each vertex has exactly b neighbors.</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>Task: Find the maximum weight subset of E such that each vertex has exactly b neighbors.</p>
    <p>Example:</p>
    <p>n = 4 b = 2</p>
    <p>Graphical models are powerful probabilistic constructions that describe the dependencies between di!erent dimensions in a probability distribution. In an acyclic graph, a graphical model can be correctly maximized or marginalized by collecting messages from all leaf nodes at some root, then distributing messages back toward the leaf nodes [7].</p>
    <p>However, when loops are present in the graph, belief propagation methods often reach oscillation states or converge to either local maxima or incorrect marginals. Cases with a single loop have been analyzed in [10], which gives an analytical expression relating the steady-state beliefs to the true marginals.</p>
    <p>Previous work related convergence of BP to types of free energy in [8], and [5] describes general su&quot;cient conditions for convergence towards marginals.</p>
    <p>Belief propagation has been generalized into a larger set of algorithms called tree-based reparameterization (TRP) in [9]. These algorithms iteratively reparameterize the distribution without changing it based on various trees in the original graph. Pairwise BP can be interpreted as doing this on the two-node trees of each edge. The set of graphs on which TRP converges subsumes that of BP. However, we use standard belief propagation here because it converges on our graph, it is simpler to implement and has additional benefits such as parallel computation.</p>
    <p>In this work, we provide a proof of convergence based on our specific graphical model and use the topology of our graph to find our convergence time.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>Consider a bipartite graph G = (U, V, E) such that U = {u1, . . . , un}, V = {v1, . . . , vn}, and E = (ui, vj ), !i &quot; {1, . . . , n}, !j &quot; {1, . . . , n}. Let A be the weight matrix of G such that the weight of edge (ui, vj ) is Aij . Let a b-matching be characterized by a function M (ui) or M (vj ) that returns the set of neighbor vertices of the input vertex in the b-matching. The b-matching objective function can then be written as</p>
    <p>max M</p>
    <p>W(M ) =</p>
    <p>max M</p>
    <p>n !</p>
    <p>i=1</p>
    <p>!</p>
    <p>vk!M(ui)</p>
    <p>Aik + n</p>
    <p>!</p>
    <p>j=1</p>
    <p>!</p>
    <p>u!!M(vj )</p>
    <p>A!j</p>
    <p>s.t. |M (ui)| = b, !i &quot; {1, . . . , n} |M (vj )| = b, !j &quot; {1, . . . , n} .</p>
    <p>(1)</p>
    <p>If we define variables xi &quot; X and yj &quot; Y for each vertex such that xi = M (ui), and yj = M (vj ), we can define the following functions:</p>
    <p>!(xi) = exp( !</p>
    <p>vj !xi</p>
    <p>Aij ), !(yj ) = exp( !</p>
    <p>ui!yj</p>
    <p>Aij )</p>
    <p>&quot;(xi, yj ) = (vj &quot; xi # ui &quot; yj ). (2)</p>
    <p>Note that both X and Y have values over &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>configurations. For example, for n = 4 and b = 2, xi could be any entry from {{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}}. Similarly, if we were to crudely view the potential functions !(xi), !(yj ) and &quot;(xi, yj) as</p>
    <p>tables, these tables would be of size &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>, &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>and &quot;</p>
    <p>n b</p>
    <p>#2</p>
    <p>entries respectively. The potential function &quot;(xi, yj ) is a binary function which goes to zero if yj contains a configuration that chooses node ui when xi does not contain a configuration that includes node vj and viceversa. These are zero configurations since they create an invalid b-matching. Otherwise &quot;(xi, yj ) = 1 if xi and yj are in configurations that could agree with a feasible b-matching. In Section 3.2 we will show how to avoid ever directly manipulating these cumbersome tables explicitly.</p>
    <p>Using the potentials and pairwise clique functions, we can write out the weighted b-matching objective as a probability distribution p(X, Y ) $ exp(W(M )) [1].</p>
    <p>p(X, Y ) = 1</p>
    <p>Z</p>
    <p>n $</p>
    <p>i=1</p>
    <p>n $</p>
    <p>j=1</p>
    <p>&quot;(xi, yj ) n</p>
    <p>$</p>
    <p>k=1</p>
    <p>!(xk)!(yk) (3)</p>
    <p>We maximize this probability function using the maxproduct algorithm. The max-product algorithm iteratively passes messages, which are vectors over settings of the variables, between dependent variables and stores beliefs, which are estimates of max-marginals. The following are the update equations for messages from xi to yj. To avoid clutter, we omit the formulas for messages from yj to xi because these update equations for the reverse messages are the same except we swap the x and the y terms. In general, the default range of subscript indices is 1 through n; we only indicate the exceptions.</p>
  </div>
  <div class="page">
    <p>Classical Application: Resource Allocation</p>
    <p>- Manual labor</p>
    <p>- n workers</p>
    <p>- n tasks</p>
    <p>- Team of b workers needed per task.</p>
    <p>- skill of worker at performing task.</p>
    <p>Bipartite Weighted b-Matching</p>
    <p>Aij</p>
    <p>Tasks</p>
    <p>Workers</p>
    <p>Graphical models are powerful probabilistic constructions that describe the dependencies between di!erent dimensions in a probability distribution. In an acyclic graph, a graphical model can be correctly maximized or marginalized by collecting messages from all leaf nodes at some root, then distributing messages back toward the leaf nodes [7].</p>
    <p>However, when loops are present in the graph, belief propagation methods often reach oscillation states or converge to either local maxima or incorrect marginals. Cases with a single loop have been analyzed in [10], which gives an analytical expression relating the steady-state beliefs to the true marginals.</p>
    <p>Previous work related convergence of BP to types of free energy in [8], and [5] describes general su&quot;cient conditions for convergence towards marginals.</p>
    <p>Belief propagation has been generalized into a larger set of algorithms called tree-based reparameterization (TRP) in [9]. These algorithms iteratively reparameterize the distribution without changing it based on various trees in the original graph. Pairwise BP can be interpreted as doing this on the two-node trees of each edge. The set of graphs on which TRP converges subsumes that of BP. However, we use standard belief propagation here because it converges on our graph, it is simpler to implement and has additional benefits such as parallel computation.</p>
    <p>In this work, we provide a proof of convergence based on our specific graphical model and use the topology of our graph to find our convergence time.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>Consider a bipartite graph G = (U, V, E) such that U = {u1, . . . , un}, V = {v1, . . . , vn}, and E = (ui, vj ), !i &quot; {1, . . . , n}, !j &quot; {1, . . . , n}. Let A be the weight matrix of G such that the weight of edge (ui, vj ) is Aij . Let a b-matching be characterized by a function M (ui) or M (vj ) that returns the set of neighbor vertices of the input vertex in the b-matching. The b-matching objective function can then be written as</p>
    <p>max M</p>
    <p>W(M ) =</p>
    <p>max M</p>
    <p>n !</p>
    <p>i=1</p>
    <p>!</p>
    <p>vk!M(ui)</p>
    <p>Aik + n</p>
    <p>!</p>
    <p>j=1</p>
    <p>!</p>
    <p>u!!M(vj )</p>
    <p>A!j</p>
    <p>s.t. |M (ui)| = b, !i &quot; {1, . . . , n} |M (vj )| = b, !j &quot; {1, . . . , n} .</p>
    <p>(1)</p>
    <p>If we define variables xi &quot; X and yj &quot; Y for each vertex such that xi = M (ui), and yj = M (vj ), we can define the following functions:</p>
    <p>!(xi) = exp( !</p>
    <p>vj !xi</p>
    <p>Aij ), !(yj ) = exp( !</p>
    <p>ui!yj</p>
    <p>Aij )</p>
    <p>&quot;(xi, yj ) = (vj &quot; xi # ui &quot; yj ). (2)</p>
    <p>Note that both X and Y have values over &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>configurations. For example, for n = 4 and b = 2, xi could be any entry from {{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}}. Similarly, if we were to crudely view the potential functions !(xi), !(yj ) and &quot;(xi, yj) as</p>
    <p>tables, these tables would be of size &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>, &quot;</p>
    <p>n b</p>
    <p>#</p>
    <p>and &quot;</p>
    <p>n b</p>
    <p>#2</p>
    <p>entries respectively. The potential function &quot;(xi, yj ) is a binary function which goes to zero if yj contains a configuration that chooses node ui when xi does not contain a configuration that includes node vj and viceversa. These are zero configurations since they create an invalid b-matching. Otherwise &quot;(xi, yj ) = 1 if xi and yj are in configurations that could agree with a feasible b-matching. In Section 3.2 we will show how to avoid ever directly manipulating these cumbersome tables explicitly.</p>
    <p>Using the potentials and pairwise clique functions, we can write out the weighted b-matching objective as a probability distribution p(X, Y ) $ exp(W(M )) [1].</p>
    <p>p(X, Y ) = 1</p>
    <p>Z</p>
    <p>n $</p>
    <p>i=1</p>
    <p>n $</p>
    <p>j=1</p>
    <p>&quot;(xi, yj ) n</p>
    <p>$</p>
    <p>k=1</p>
    <p>!(xk)!(yk) (3)</p>
    <p>We maximize this probability function using the maxproduct algorithm. The max-product algorithm iteratively passes messages, which are vectors over settings of the variables, between dependent variables and stores beliefs, which are estimates of max-marginals. The following are the update equations for messages from xi to yj. To avoid clutter, we omit the formulas for messages from yj to xi because these update equations for the reverse messages are the same except we swap the x and the y terms. In general, the default range of subscript indices is 1 through n; we only indicate the exceptions.</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>Alternate uses of b-matching:</p>
    <p>- Balanced k-nearest-neighbors</p>
    <p>- Each node can only be picked k times.</p>
    <p>- Robust to translations of test data.</p>
    <p>- When test data is collected under different conditions (e.g. time, location, instrument calibration).</p>
  </div>
  <div class="page">
    <p>Bipartite Weighted b-Matching</p>
    <p>Classical algorithms solve Max-Weighted bMatching in running time, such as:</p>
    <p>- Blossom Algorithm (Edmonds 1965)</p>
    <p>- Balanced Network Flow</p>
    <p>(Fremuth-Paeger, Jungnickel 1999)</p>
    <p>O(bn3)</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>- Bayati, Shah, and Sharma (2005) formulated the 1-matching problem as a probability distribution.</p>
    <p>- This work generalizes to arbitrary b.</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Variables:</p>
    <p>Each vertex chooses b neighbors.</p>
  </div>
  <div class="page">
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>Example: -ui</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Variables:</p>
    <p>Each vertex chooses b neighbors.</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Variables:</p>
    <p>Each vertex chooses b neighbors.</p>
    <p>For vertex ,</p>
    <p>Similarly, for have variable</p>
    <p>Note: variables have possible settings.</p>
    <p>ui</p>
    <p>vj</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
    <p>Xi ! V, |Xi| = b</p>
    <p>Yj</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Weights as probabilities:</p>
    <p>Since we sum weights but multiply probabilities, exponentiate.</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;!(Xi) = exp( 1</p>
    <p>!</p>
    <p>vj !Xi</p>
    <p>Aij )</p>
    <p>!(Yj ) = exp( 1</p>
    <p>!</p>
    <p>ui!Yj</p>
    <p>Aij )</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Enforce b-matching:</p>
    <p>Neighbor choices must agree</p>
  </div>
  <div class="page">
    <p>Example: Invalid settings</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 4: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 2. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 6: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 6 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to</p>
    <p>u1 u2 u3 u4</p>
    <p>v1</p>
    <p>Figure 2: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>be always zero due to the scale of the plot following the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Enforce b-matching:</p>
    <p>Neighbor choices must agree</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Enforce b-matching:</p>
    <p>Neighbor choices must agree</p>
    <p>Pairwise compatibility function:</p>
    <p>= (vj ! Xi &quot; ui ! Yj ).</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
    <p>!(Xi, Yj ) =</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>!(Xi) = exp( 1</p>
    <p>!</p>
    <p>vj !Xi</p>
    <p>Aij ) !(Yj ) = exp( 1</p>
    <p>!</p>
    <p>ui!Yj</p>
    <p>Aij )</p>
    <p>!(Xi, Yj ) = (vj ! Xi &quot; ui ! Yj ).</p>
    <p>P (X, Y ) = 1</p>
    <p>Z</p>
    <p>n!</p>
    <p>i,j=1</p>
    <p>!(Xi, Yj ) n!</p>
    <p>k=1</p>
    <p>&quot;(Xk)&quot;(Yj )</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Ignore the Z normalization, P(X,Y) is exactly the exponentiated weight of the b-matching.</p>
    <p>!(Xi) = exp( 1</p>
    <p>!</p>
    <p>vj !Xi</p>
    <p>Aij ) !(Yj ) = exp( 1</p>
    <p>!</p>
    <p>ui!Yj</p>
    <p>Aij )</p>
    <p>!(Xi, Yj ) = (vj ! Xi &quot; ui ! Yj ).</p>
    <p>P (X, Y ) = 1</p>
    <p>Z</p>
    <p>n!</p>
    <p>i,j=1</p>
    <p>!(Xi, Yj ) n!</p>
    <p>k=1</p>
    <p>&quot;(Xk)&quot;(Yj )</p>
  </div>
  <div class="page">
    <p>Edge Weights As a Distribution</p>
    <p>Also, since were maximizing, ignore the 1/2 (makes the math more readable).</p>
    <p>!(Xi) = exp( 1</p>
    <p>!</p>
    <p>vj !Xi</p>
    <p>Aij ) !(Yj ) = exp( 1</p>
    <p>!</p>
    <p>ui!Yj</p>
    <p>Aij )</p>
    <p>!(Xi, Yj ) = (vj ! Xi &quot; ui ! Yj ).</p>
    <p>P (X, Y ) = 1</p>
    <p>Z</p>
    <p>n!</p>
    <p>i,j=1</p>
    <p>!(Xi, Yj ) n!</p>
    <p>k=1</p>
    <p>&quot;(Xk)&quot;(Yj )</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Standard Max-Product</p>
    <p>Send messages between variables:</p>
    <p>Fuse messages to obtain beliefs (or estimate of max-marginals):</p>
    <p>mXi (Yj ) = 1</p>
    <p>Z max Xi</p>
    <p>!</p>
    <p>&quot;!(Xi)&quot;(Xi, Yj ) #</p>
    <p>k !=j</p>
    <p>mYk (Xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(Xi) = 1</p>
    <p>Z !(Xi)</p>
    <p>!</p>
    <p>k</p>
    <p>mYk (Xi)</p>
  </div>
  <div class="page">
    <p>Standard Max-Product</p>
    <p>Converges to true maximum on any tree structured graph (Pearl 1986).</p>
    <p>We show that it converges to the correct maximum on our graph.</p>
  </div>
  <div class="page">
    <p>Standard Max-Product</p>
    <p>Converges to true maximum on any tree structured graph (Pearl 1986).</p>
    <p>We show that it converges to the correct maximum on our graph.</p>
    <p>But what about the -length message and belief vectors?</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>- Use algebraic tricks to reduce -length message vectors to scalars.</p>
    <p>- Derive new update rule for scalar messages.</p>
    <p>- Use similar trick to maximize belief vectors efficiently.</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
    <p>Lets speed through the math.</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Take advantage of binary function:</p>
    <p>Message vectors consist of only two values</p>
    <p>!(xi, yj )</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Take advantage of binary function:</p>
    <p>Message vectors consist of only two values</p>
    <p>If we rename these two values we can break up the product.</p>
    <p>!(xi, yj )</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Take advantage of binary function:</p>
    <p>Message vectors consist of only two values</p>
    <p>If we rename these two values we can break up the product.</p>
    <p>!(xi, yj )</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Normalize messages by dividing whole vector by</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>!xiyj</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Normalize messages by dividing whole vector by</p>
    <p>-length vector scalar</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>!xiyj</p>
    <p>!</p>
    <p>n</p>
    <p>b</p>
    <p>&quot;</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Derive update rule:</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Derive update rule:</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>!(xi) ! !</p>
    <p>k!xi</p>
    <p>exp(Aik)</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Derive update rule:</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>Efficient Max-Product</p>
    <p>Derive update rule:</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
  </div>
  <div class="page">
    <p>bth greatest setting of k for the term</p>
    <p>Efficient Max-Product After canceling terms message update simplifies to</p>
    <p>and we maximize beliefs with</p>
    <p>Both these updates take O(bn) time per vertex.</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>mxi (yj ) = 1</p>
    <p>Z max</p>
    <p>xi</p>
    <p>!</p>
    <p>&quot;!(xi)&quot;(xi, yj ) #</p>
    <p>k !=j</p>
    <p>myk (xi)</p>
    <p>$</p>
    <p>%</p>
    <p>b(xi) = 1</p>
    <p>Z !(xi)</p>
    <p>#</p>
    <p>k</p>
    <p>myk (xi)</p>
    <p>Loopy belief propagation on this graph converges to the optimum. However, since there are</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>possible settings for each variable, direct belief propagation is not feasible with larger graphs.</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>-LENGTH MESSAGE VECTORS</p>
    <p>We exploit three peculiarities of the above formulation to fully represent the</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>length messages as scalars.</p>
    <p>First, the &quot; functions are well structured, and their structure causes the maximization term in the message updates to always be one of two values.</p>
    <p>mxi (yj ) ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui &quot; yj</p>
    <p>mxi (yj ) ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>k !=j</p>
    <p>myk (xi), if ui /&quot; yj (4)</p>
    <p>This is because the &quot; function changes based only on whether the setting of yj indicates that vj shares an edge with ui. Furthermore, if we redefine the above message values as two scalars, we can write the messages more specifically as</p>
    <p>xiyj ! max vj &quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki</p>
    <p>#xiyj ! max vj /&quot;xi</p>
    <p>!(xi) #</p>
    <p>uk&quot;xi\vj</p>
    <p>ki #</p>
    <p>uk /&quot;xi\vj</p>
    <p>#ki. (5)</p>
    <p>Second, since the messages are unnormalized probabilities, we can divide any constant from the vectors without changing the result. We divide all entries in the message vector by #xiyj to get</p>
    <p>xiyj = xiyj #xiyj</p>
    <p>and #xiyj = 1 .</p>
    <p>This lossless compression scheme simplifies the storage of message vectors from length</p>
    <p>&amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>to 1.</p>
    <p>We rewrite the ! functions as a product of the exponentiated Aij weights and eliminate the need to exhaustively maximize over all possible sets of size b. Inserting Equation (2) into the definition of xiyj gives</p>
    <p>xiyj = maxj&quot;xi !(xi)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi !(xi) (</p>
    <p>k&quot;xi\j ki</p>
    <p>= maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aik)</p>
    <p>(</p>
    <p>k&quot;xi\j ki</p>
    <p>= exp(Aij ) maxj&quot;xi</p>
    <p>(</p>
    <p>k&quot;xi\j exp(Aik)ki</p>
    <p>maxj /&quot;xi (</p>
    <p>k&quot;xi exp(Aij )ki</p>
    <p>(6)</p>
    <p>We cancel out common terms and are left with the simple message update rule,</p>
    <p>xiyj = exp(Aij )</p>
    <p>exp(Ai!)y!xi .</p>
    <p>Here, the index $ refers to the the bth greatest setting of k for the term exp(Aik)myk(xi), where k #= j. This compressed version of a message update can be computed in O(bn) time.</p>
    <p>We cannot e!ciently reconstruct the entire belief vector but we can e!ciently find its maximum.</p>
    <p>max xi</p>
    <p>b(xi) ! max xi</p>
    <p>!(xi) #</p>
    <p>k&quot;xi</p>
    <p>ykxi</p>
    <p>! max xi</p>
    <p>#</p>
    <p>k&quot;xi</p>
    <p>exp(Aik)ykxi (7)</p>
    <p>Finally, to maximize over xi we enumerate k and greedily select the b largest values of exp(Aik)ykxi .</p>
    <p>The above procedure avoids enumerating all &amp;</p>
    <p>n b</p>
    <p>'</p>
    <p>entries in the belief vector, and instead reshapes the distribution into a b dimensional hypercube. The maximum of the hypercube is found e!ciently by searching each dimension independently. Note that each dimension represents one of the b edges for node ui.</p>
    <p>We begin with the assumption that MG, the maximum weight b-matching on G, is unique. Moreover, W(MG), the weight of MG, is greater than that of any other matching by a constant %.</p>
    <p>% = W(MG) $ max M !=MG</p>
    <p>W(M )</p>
    <p>Let T be the unwrapped graph of G from node ui. An unwrapped graph is a tree that contains representations of all paths of length d in G originating from a single root node without any backtracking. Each node in T maps to a corresponding node in G, and each node in G maps to multiple nodes in T . Nodes and edges in T have the same local connectivity and potential functions as their corresponding nodes in G. Let r be the root of T that corresponds to ui in the original graph</p>
    <p>=</p>
    <p>exp(Aik)myk (xi), s. t. k != j</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
    <p>Assumptions:</p>
    <p>- Optimal b-matching is unique.</p>
    <p>- Weights treated as constants.</p>
    <p>! = difference between weight of best and 2nd best b-matching is constant.</p>
  </div>
  <div class="page">
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>Loopy Belief Propagation for Bipartite Maximum Weight b-Matching</p>
    <p>Bert Huang</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Tony Jebara</p>
    <p>Computer Science Dept. Columbia University New York, NY 10027</p>
    <p>Abstract</p>
    <p>We formulate the weighted b-matching objective function as a probability distribution function and prove that belief propagation (BP) on its graphical model converges to the optimum. Standard BP on our graphical model cannot be computed in polynomial time, but we introduce an algebraic method to circumvent the combinatorial message updates. Empirically, the resulting algorithm is on average faster than popular combinatorial implementations, while still scaling at the same asymptotic rate of O(bn3). Furthermore, the algorithm shows promising performance in machine learning applications.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 1: Example b-matching MG on a bipartite graph G. Dashed lines represent possible edges, solid lines represent b-matched edges. In this case b = 2.</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 3: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [?]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 ! n ! 100 and 1 ! b ! n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Too complex to color...</p>
  </div>
  <div class="page">
    <p>Basic mechanism: Unwrapped Graph, T</p>
    <p>- Construction follows loopy BP messages in reverse.</p>
    <p>- True max-marginals of root node are exactly belief at iteration d.</p>
    <p>- Max of unwrapped graph distribution is the maximum weight b-matching on tree.</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Proof by contradiction:</p>
    <p>What happens if optimal b-matching on T differs from optimal b-matching on G at root?</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Convergence Proof Sketch</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Best b-matching on T</p>
    <p>Best b-matching on G copied onto T</p>
  </div>
  <div class="page">
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>There exists at least one path on T that alternates between edges that are b-matched in each b-matching.</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>There exists at least one path on T that alternates between edges that are b-matched in each b-matching.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Claim: If depth d is great enough, if we replace the blue edges of this path with the red edges in the optimal b-matching on T, we get a new b-matching with greater weight.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Claim: If depth d is great enough, if we replace the blue edges of this path with the red edges in the optimal b-matching on T, we get a new b-matching with greater weight.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>Claim: If depth d is great enough, if we replace the blue edges of this path with the red edges in the optimal b-matching on T, we get a new b-matching with greater weight.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Convergence Proof Sketch</p>
  </div>
  <div class="page">
    <p>We can analyze the change in weight by looking only at edges on path.</p>
    <p>u1</p>
    <p>v1 v2 v3 v4</p>
    <p>u2 u2 u2 u2u3 u3 u3 u3u4 u4 u4 u4</p>
    <p>v2 v3 v4 v2 v3 v4 v2 v3 v4 v1 v3 v4 v1 v3 v4 v1 v3 v4 v1 v2 v4 v1 v2 v4 v1 v2 v4 v1 v2 v3 v1 v2 v3 v1 v2 v3</p>
    <p>Figure 2: Example unwrapped graph T of G at 3 iterations. The matching MT is highlighted based on MG from Figure 1. Note that leaf nodes cannot have perfect b-matchings, but all inner nodes and the root do. One possible path PT is highlighted, which is discussed in Lemma ??.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 1. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>One natural application of b-matching is as an improvement over k-nearest neighbor (KNN) for classification. Using KNN for classification is a quick way of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to ob</p>
    <p>Convergence Proof Sketch</p>
    <p>Modifying optimal b-matching produced better b-matching</p>
    <p>Original contradiction impossible.</p>
  </div>
  <div class="page">
    <p>Loopy BP converges to true maximum weight b-matching in d iterations</p>
    <p>Convergence Proof Sketch</p>
    <p>! = difference between weight of best and 2nd best b-matching.</p>
    <p>d ! n</p>
    <p>! max</p>
    <p>i,j Aij = O(n)</p>
  </div>
  <div class="page">
    <p>Loopy BP converges to true maximum weight b-matching in d iterations</p>
    <p>Running time of full algorithm:</p>
    <p>Convergence Proof Sketch</p>
    <p>! = difference between weight of best and 2nd best b-matching.</p>
    <p>O(bn3)</p>
    <p>d ! n</p>
    <p>! max</p>
    <p>i,j Aij = O(n)</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Running time comparison against GOBLIN graph optimization library.</p>
    <p>- Random weights.</p>
    <p>- Varied graph size n from 3 to 100</p>
    <p>- Varied b from 1 to !n/2&quot;</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Running time comparison against GOBLIN graph optimization library.</p>
    <p>. . .u2v2u4v1u1v3u4v1u3</p>
    <p>First cycle</p>
    <p>(a) PG</p>
    <p>u4</p>
    <p>v3u1</p>
    <p>v1</p>
    <p>(b) c1</p>
    <p>. . .u2v2u4v1u3</p>
    <p>(c) PG \ c1</p>
    <p>Figure 4: (a) One possible extension of PG from Figure 4. This path comes from a deeper T so PG is longer. The first cycle c1 detected is highlighted. (b) Cycle c1 from PG. (c) The remainder of PG when we remove c1. Note the alternation of the matched edges remains consistent even when we cut out cycles in the interior of the path.</p>
    <p>with edges in PT toggled, that has a higher weight than MT . This should be impossible because MT is defined as the optimal b-matching. This occurs when</p>
    <p>d ! n</p>
    <p>!</p>
    <p>!</p>
    <p>max e1,e2!E</p>
    <p>W(e1) &quot; W(e2)</p>
    <p>&quot;</p>
    <p>.</p>
    <p>We can consider ! and the maximum di!erence between weights as constants. If d = &quot;(n), Contradiction 1 always leads to this impossible circumstance, and therefore Theorem 1 is true.</p>
    <p>The bound in this proof is for the worst-case. In our experiments, the maximum and minimum weights had little e!ect and the ! term only changed convergence time noticeably if its value was near zero. Due to finite numerical precision, this may appear as a tie between di!erent settings, which often causes BP to fail [11].</p>
    <p>We elaborate the speed advantages of our method and an application in machine learning where b-matching can improve classification accuracy.</p>
    <p>We compare the performance of our implementation of belief propagation maximum weighted b-matching against the free graph optimization package, GOBLIN. 1</p>
    <p>Classical b-matching algorithms such as the balanced network flow method used by the GOBLIN library run in O(bn3) time [2]. The belief propagation method takes O(bn) time to compute one iteration of message updates for each of the 2n nodes and converges in O(n) iterations. So, its overall running time is also O(bn3).</p>
    <p>We ran both algorithms on randomly generated bipartite graphs of 10 # n # 100 and 1 # b # n/2. We generated the weight matrix with the rand function in MATLAB, which picks each weight independently from a uniform distribution between 0 and 1.</p>
    <p>The GOBLIN library is C++ code and our implementation2 of belief propagation b-matching is in C. Both were run on a 3.00 Ghz. Pentium 4 processor.</p>
    <p>In general, the belief propagation runs hundreds of times faster than GOBLIN. Figure 5 shows various comparisons of their running times. The surface plots show how the algorithms scale with respect to n and b. The line plots show cross sections of these surface plots, with appropriate transformations on the running time to show the scaling (without these transformations, the belief propagation line would appear to be always zero due to the scale of the plot following the GOBLIN line). Since both algorithms have running time O(bn3), when we fix b = 5, we get a cubic curve. When we fix b = n/2, we get a quartic curve because b becomes a function of n.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 5: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
  </div>
  <div class="page">
    <p>Experiments: Translated Test Data On toy data, translation cripples KNN but b-matching makes no classification errors.</p>
    <p>u1 u2 u3 u4</p>
    <p>v1 v2 v3 v4</p>
    <p>Figure 5: The cyclic alternating path PG starting at v1 on G that corresponds to the nodes visited by PT . Edges are numbered to help follow the loopy path.</p>
    <p>b</p>
    <p>BP median running time</p>
    <p>n</p>
    <p>t</p>
    <p>b</p>
    <p>GOBLIN median running time</p>
    <p>n</p>
    <p>t</p>
    <p>n</p>
    <p>t1 /3</p>
    <p>Median Running time when B=5</p>
    <p>n</p>
    <p>t1 /4</p>
    <p>Median Running time when B=! n/2 &quot;</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>GOBLIN</p>
    <p>BP</p>
    <p>Figure 7: Median running time in seconds on randomly weighted graphs for the GOBLIN library and BP. Top Left: BP median running time with respect to n, b. Bottom Left: GOBLIN b-matching solver median running time over n, b. Top Right: Cube root of running time ( 3</p>
    <p>! t) of both</p>
    <p>algorithms when b = 3; note that both scale linearly in this plot, implying a n3 term in running time for both. Bottom Right: Root-4 ( 4</p>
    <p>! t) of running time for various n,</p>
    <p>when b = &quot;n/2#. Both algorithms have quartic running time.</p>
    <p>of computing a reasonable prediction of class, but it is inherently greedy. The algorithm starts by computing an a!nity matrix between all data points. For each test data point, KNN greedily selects its k neighbors regardless of the connectivity or graph this may generate. This invariably leads to some nodes serving as hub nodes and labeling too many unknown examples while other training points are never used as neighbors.</p>
    <p>Instead, using b-matching enforces uniform connectivity across the training and testing points. For example, assume the number of testing points and training points is the same and we perform b-matching. Then, each testing point will be labeled by b training points and each training point contributes to the labeling of b testing points. This means b-matching will do a better job of recreating the distribution of classes seen in the training data when it classifies test data. This is useful when test data is transformed in some way that preserves the shape of the distribution, but it is</p>
    <p>!10 !5 0 5 10 15 !6</p>
    <p>!4</p>
    <p>!2</p>
    <p>Translated Feature 0 10 20 30 40 50</p>
    <p>k</p>
    <p>A c c u ra</p>
    <p>c y</p>
    <p>Accuracy</p>
    <p>Train +1</p>
    <p>Test +1</p>
    <p>Train !1</p>
    <p>Test !1</p>
    <p>b!matching</p>
    <p>k!nearest!neighbor</p>
    <p>Figure 8: Left: Synthetic classification data where the test data is translated to the right along the x-axis. Right: Accuracy for k-nearest neighbor versus b-matching for various k (or b).</p>
    <p>translated or scaled to confuse KNN. This can occur in situations where training data comes from a di&quot;erent source than the testing data.</p>
    <p>In both algorithms, we compute a bipartite a!nity graph between a set of training points and a set of testing points. We fix the cardinalities of these sets to be equal (though if they are unequal we can downsample and run the algorithm in multiple folds to obtain classifications). We use either KNN or weighted b-matching to prune the graph, then we classify each test point based on the majority of the classes of the neighbors. In our experiments, we use negative Euclidean distance as our a!nity metric.</p>
    <p>We created synthetic data by sampling 50 training data points from two spherical Gaussians with means at (3, 3) and (!3, !3). We then sampled 50 testing data points from similar Gaussians, but translated along the x-axis by +8. This is a severe case of translation where it is obvious that KNN could be fooled by the proximity of the testing Gaussian to the training Gaussian of the wrong class. Figure 7 shows the points we sampled from this setup. As expected, KNN could only achieve 65% accuracy on this data for the optimal setting of k, while b-matching classified the points perfectly for all settings of b.</p>
    <p>We sampled the MNIST digits dataset of 28 &quot; 28 grayscale digits. For each of the digits 3, 5, and 8, we sampled 100 training points from the training set and 100 from the testing set. We average accuracy over 20 random samplings to avoid anomalous results. We cross-validate over settings of b and k in the range [1, 300] and save the best accuracy achieved for each algorithm on each sampling.</p>
    <p>We examine the case where training and testing data</p>
    <p>!! &quot; ! #&quot; #! !$</p>
    <p>!%</p>
    <p>!&amp;</p>
    <p>&quot;</p>
    <p>&amp;</p>
    <p>%</p>
    <p>$ '()*+,*-./01*1</p>
  </div>
  <div class="page">
    <p>Experiments MNIST Digits with pseudo-translation</p>
    <p>- Image data with background changes is like translation.</p>
    <p>- Train on MNIST digits 3, 5, and 8.</p>
    <p>- Test on new examples with various bluescreen textures.</p>
    <p>lem. We provided an e!cient method to update the !</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>-length messages. The practical running time of this algorithm is hundreds of times faster than the classical implementations we have found. This makes b-matching a more practical tool in machine learning settings and lets us tackle significantly larger datasets.</p>
    <p>Furthermore, by formulating weighted b-matching in this manner, we can take advantage of the inherent parallel computation of belief propagation. In addition to the empirical evidence of a faster constant on the asymptotic running time, the procedure could be distributed over a number of machines. For example, n servers and n clients could run this algorithm in parallel to optimize throughput.</p>
    <p>Theoretically, the convergence of loopy BP on this graph and the ability to update the</p>
    <p>!</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>length messages are pleasant surprises. We have tried to generalize this method to all graph structures, not just bipartite graphs, but found that there were cases that led to oscillation in the belief updates. This is due to the fact that cycles can be both even and odd in general graphs. Generalizing to such cases is one future direction for this work.</p>
    <p>In addition to being limited to bipartite graphs, belief propagation methods have di!culty with functions that have multiple solutions. This is why we assume there is a unique solution in the proof. In a way, our convergence bound still holds when there are multiple solutions, in the sense that the ! value is in the denominator, so the limit as ! goes to zero implies infinite iterations to convergence. Belief propagation cannot handle these ties in the function due to the fact that the information being passed between variables is only the maximum value, and no information about the actual variable values is sent. Therefore, the algorithm has no mechanism to distinguish between one maximum and the other. We are currently investigating if tree-based reparameterization techniques or other graphical model inference methods can solve the multiple-optimum or non-bipartite b-matching cases.</p>
    <p>Acknowledgments</p>
    <p>We thank Stuart Andrews, Andrew Howard and Blake Shaw for insightful discussions. Supported in part by NSF grant IIS-0347499.</p>
    <p>References</p>
    <p>[1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation. In Proc. of the IEEE International Symposium on Information Theory, 2005.</p>
    <p>[2] C. Fremuth-Paeger and D. Jungnickel. Balanced network flows. i. a unifying framework for design and</p>
    <p>Background texture</p>
    <p>A v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>k</p>
    <p>A v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>Figure 7: (Left) Examples of digits placed on backgrounds: 1-unaltered, 2-diagonal lines, 3-grid, 4-white noise, 5-brushed metal, 6-wood, 7-marble. (Top right) Average accuracies over 20 random samplings for optimal setting of b or k. (Bottom right) Average accuracy on wood background for various settings of b or k.</p>
    <p>analysis of matching algorithms. Networks, 33(1):1 28, 1999.</p>
    <p>[3] B. Frey and D. Dueck. Mixture modeling by a!nity propagation. In Advances in Neural Information Processing Systems 18, pages 379386. MIT Press, 2006.</p>
    <p>[4] T. Jebara and V. Shchogolev. B-matching for spectral clustering. In Proc. of the European Conference on Machine Learning, ECML, 2006.</p>
    <p>[5] J. Mooij and H. Kappen. Su!cient conditions for convergence of loopy belief propagation. In Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 39640, 2005.</p>
    <p>[6] M. Muller-Hannemann and A. Schwartz. Implementing weighted b-matching algorithms: Insights from a computational study. In ACM Journal of Experimental Algorithmics, Volume 5, Article 8, 2000.</p>
    <p>[7] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.</p>
    <p>[8] S. Tatikonda and M. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artificial Intell., vol. 18, 2002.</p>
    <p>[9] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory, 49(5), 2003.</p>
    <p>[10] Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12(1):141, 2000.</p>
    <p>[11] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs. IEEETIT: IEEE Transactions on Information Theory, 47, 2001.</p>
  </div>
  <div class="page">
    <p>Experiments lem. We provided an e!cient method to update the !</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>-length messages. The practical running time of this algorithm is hundreds of times faster than the classical implementations we have found. This makes b-matching a more practical tool in machine learning settings and lets us tackle significantly larger datasets.</p>
    <p>Furthermore, by formulating weighted b-matching in this manner, we can take advantage of the inherent parallel computation of belief propagation. In addition to the empirical evidence of a faster constant on the asymptotic running time, the procedure could be distributed over a number of machines. For example, n servers and n clients could run this algorithm in parallel to optimize throughput.</p>
    <p>Theoretically, the convergence of loopy BP on this graph and the ability to update the</p>
    <p>!</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>length messages are pleasant surprises. We have tried to generalize this method to all graph structures, not just bipartite graphs, but found that there were cases that led to oscillation in the belief updates. This is due to the fact that cycles can be both even and odd in general graphs. Generalizing to such cases is one future direction for this work.</p>
    <p>In addition to being limited to bipartite graphs, belief propagation methods have di!culty with functions that have multiple solutions. This is why we assume there is a unique solution in the proof. In a way, our convergence bound still holds when there are multiple solutions, in the sense that the ! value is in the denominator, so the limit as ! goes to zero implies infinite iterations to convergence. Belief propagation cannot handle these ties in the function due to the fact that the information being passed between variables is only the maximum value, and no information about the actual variable values is sent. Therefore, the algorithm has no mechanism to distinguish between one maximum and the other. We are currently investigating if tree-based reparameterization techniques or other graphical model inference methods can solve the multiple-optimum or non-bipartite b-matching cases.</p>
    <p>Acknowledgments</p>
    <p>We thank Stuart Andrews, Andrew Howard and Blake Shaw for insightful discussions. Supported in part by NSF grant IIS-0347499.</p>
    <p>References</p>
    <p>[1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation. In Proc. of the IEEE International Symposium on Information Theory, 2005.</p>
    <p>[2] C. Fremuth-Paeger and D. Jungnickel. Balanced network flows. i. a unifying framework for design and</p>
    <p>Background texture A</p>
    <p>v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>k</p>
    <p>A v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>Figure 7: (Left) Examples of digits placed on backgrounds: 1-unaltered, 2-diagonal lines, 3-grid, 4-white noise, 5-brushed metal, 6-wood, 7-marble. (Top right) Average accuracies over 20 random samplings for optimal setting of b or k. (Bottom right) Average accuracy on wood background for various settings of b or k.</p>
    <p>analysis of matching algorithms. Networks, 33(1):1 28, 1999.</p>
    <p>[3] B. Frey and D. Dueck. Mixture modeling by a!nity propagation. In Advances in Neural Information Processing Systems 18, pages 379386. MIT Press, 2006.</p>
    <p>[4] T. Jebara and V. Shchogolev. B-matching for spectral clustering. In Proc. of the European Conference on Machine Learning, ECML, 2006.</p>
    <p>[5] J. Mooij and H. Kappen. Su!cient conditions for convergence of loopy belief propagation. In Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 39640, 2005.</p>
    <p>[6] M. Muller-Hannemann and A. Schwartz. Implementing weighted b-matching algorithms: Insights from a computational study. In ACM Journal of Experimental Algorithmics, Volume 5, Article 8, 2000.</p>
    <p>[7] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.</p>
    <p>[8] S. Tatikonda and M. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artificial Intell., vol. 18, 2002.</p>
    <p>[9] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory, 49(5), 2003.</p>
    <p>[10] Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12(1):141, 2000.</p>
    <p>[11] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs. IEEETIT: IEEE Transactions on Information Theory, 47, 2001.</p>
    <p>lem. We provided an e!cient method to update the !</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>-length messages. The practical running time of this algorithm is hundreds of times faster than the classical implementations we have found. This makes b-matching a more practical tool in machine learning settings and lets us tackle significantly larger datasets.</p>
    <p>Furthermore, by formulating weighted b-matching in this manner, we can take advantage of the inherent parallel computation of belief propagation. In addition to the empirical evidence of a faster constant on the asymptotic running time, the procedure could be distributed over a number of machines. For example, n servers and n clients could run this algorithm in parallel to optimize throughput.</p>
    <p>Theoretically, the convergence of loopy BP on this graph and the ability to update the</p>
    <p>!</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>length messages are pleasant surprises. We have tried to generalize this method to all graph structures, not just bipartite graphs, but found that there were cases that led to oscillation in the belief updates. This is due to the fact that cycles can be both even and odd in general graphs. Generalizing to such cases is one future direction for this work.</p>
    <p>In addition to being limited to bipartite graphs, belief propagation methods have di!culty with functions that have multiple solutions. This is why we assume there is a unique solution in the proof. In a way, our convergence bound still holds when there are multiple solutions, in the sense that the ! value is in the denominator, so the limit as ! goes to zero implies infinite iterations to convergence. Belief propagation cannot handle these ties in the function due to the fact that the information being passed between variables is only the maximum value, and no information about the actual variable values is sent. Therefore, the algorithm has no mechanism to distinguish between one maximum and the other. We are currently investigating if tree-based reparameterization techniques or other graphical model inference methods can solve the multiple-optimum or non-bipartite b-matching cases.</p>
    <p>Acknowledgments</p>
    <p>We thank Stuart Andrews, Andrew Howard and Blake Shaw for insightful discussions. Supported in part by NSF grant IIS-0347499.</p>
    <p>References</p>
    <p>[1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation. In Proc. of the IEEE International Symposium on Information Theory, 2005.</p>
    <p>[2] C. Fremuth-Paeger and D. Jungnickel. Balanced network flows. i. a unifying framework for design and</p>
    <p>Background texture</p>
    <p>A v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>k</p>
    <p>A v e</p>
    <p>ra g</p>
    <p>e a</p>
    <p>c c u</p>
    <p>ra c y BM</p>
    <p>KNN</p>
    <p>Figure 7: (Left) Examples of digits placed on backgrounds: 1-unaltered, 2-diagonal lines, 3-grid, 4-white noise, 5-brushed metal, 6-wood, 7-marble. (Top right) Average accuracies over 20 random samplings for optimal setting of b or k. (Bottom right) Average accuracy on wood background for various settings of b or k.</p>
    <p>analysis of matching algorithms. Networks, 33(1):1 28, 1999.</p>
    <p>[3] B. Frey and D. Dueck. Mixture modeling by a!nity propagation. In Advances in Neural Information Processing Systems 18, pages 379386. MIT Press, 2006.</p>
    <p>[4] T. Jebara and V. Shchogolev. B-matching for spectral clustering. In Proc. of the European Conference on Machine Learning, ECML, 2006.</p>
    <p>[5] J. Mooij and H. Kappen. Su!cient conditions for convergence of loopy belief propagation. In Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 39640, 2005.</p>
    <p>[6] M. Muller-Hannemann and A. Schwartz. Implementing weighted b-matching algorithms: Insights from a computational study. In ACM Journal of Experimental Algorithmics, Volume 5, Article 8, 2000.</p>
    <p>[7] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.</p>
    <p>[8] S. Tatikonda and M. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artificial Intell., vol. 18, 2002.</p>
    <p>[9] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory, 49(5), 2003.</p>
    <p>[10] Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12(1):141, 2000.</p>
    <p>[11] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs. IEEETIT: IEEE Transactions on Information Theory, 47, 2001.</p>
    <p>lem. We provided an e!cient method to update the !</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>-length messages. The practical running time of this algorithm is hundreds of times faster than the classical implementations we have found. This makes b-matching a more practical tool in machine learning settings and lets us tackle significantly larger datasets.</p>
    <p>Furthermore, by formulating weighted b-matching in this manner, we can take advantage of the inherent parallel computation of belief propagation. In addition to the empirical evidence of a faster constant on the asymptotic running time, the procedure could be distributed over a number of machines. For example, n servers and n clients could run this algorithm in parallel to optimize throughput.</p>
    <p>Theoretically, the convergence of loopy BP on this graph and the ability to update the</p>
    <p>!</p>
    <p>n b</p>
    <p>&quot;</p>
    <p>length messages are pleasant surprises. We have tried to generalize this method to all graph structures, not just bipartite graphs, but found that there were cases that led to oscillation in the belief updates. This is due to the fact that cycles can be both even and odd in general graphs. Generalizing to such cases is one future direction for this work.</p>
    <p>In addition to being limited to bipartite graphs, belief propagation methods have di!culty with functions that have multiple solutions. This is why we assume there is a unique solution in the proof. In a way, our convergence bound still holds when there are multiple solutions, in the sense that the ! value is in the denominator, so the limit as ! goes to zero implies infinite iterations to convergence. Belief propagation cannot handle these ties in the function due to the fact that the information being passed between variables is only the maximum value, and no information about the actual variable values is sent. Therefore, the algorithm has no mechanism to distinguish between one maximum and the other. We are currently investigating if tree-based reparameterization techniques or other graphical model inference methods can solve the multiple-optimum or non-bipartite b-matching cases.</p>
    <p>Acknowledgments</p>
    <p>We thank Stuart Andrews, Andrew Howard and Blake Shaw for insightful discussions. Supported in part by NSF grant IIS-0347499.</p>
    <p>References</p>
    <p>[1] M. Bayati, D. Shah, and M. Sharma. Maximum weight matching via max-product belief propagation. In Proc. of the IEEE International Symposium on Information Theory, 2005.</p>
    <p>[2] C. Fremuth-Paeger and D. Jungnickel. Balanced network flows. i. a unifying framework for design and</p>
    <p>Background texture</p>
    <p>A v e</p>
    <p>ra g e a</p>
    <p>c c u ra</p>
    <p>c y BM</p>
    <p>KNN</p>
    <p>k</p>
    <p>A v e ra</p>
    <p>g e a</p>
    <p>c c u ra</p>
    <p>c y BM</p>
    <p>KNN</p>
    <p>Figure 7: (Left) Examples of digits placed on backgrounds: 1-unaltered, 2-diagonal lines, 3-grid, 4-white noise, 5-brushed metal, 6-wood, 7-marble. (Top right) Average accuracies over 20 random samplings for optimal setting of b or k. (Bottom right) Average accuracy on wood background for various settings of b or k.</p>
    <p>analysis of matching algorithms. Networks, 33(1):1 28, 1999.</p>
    <p>[3] B. Frey and D. Dueck. Mixture modeling by a!nity propagation. In Advances in Neural Information Processing Systems 18, pages 379386. MIT Press, 2006.</p>
    <p>[4] T. Jebara and V. Shchogolev. B-matching for spectral clustering. In Proc. of the European Conference on Machine Learning, ECML, 2006.</p>
    <p>[5] J. Mooij and H. Kappen. Su!cient conditions for convergence of loopy belief propagation. In Proceedings of the 21th Annual Conference on Uncertainty in Artificial Intelligence (UAI-05), pages 39640, 2005.</p>
    <p>[6] M. Muller-Hannemann and A. Schwartz. Implementing weighted b-matching algorithms: Insights from a computational study. In ACM Journal of Experimental Algorithmics, Volume 5, Article 8, 2000.</p>
    <p>[7] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1988.</p>
    <p>[8] S. Tatikonda and M. Jordan. Loopy belief propagation and Gibbs measures. In Proc. Uncertainty in Artificial Intell., vol. 18, 2002.</p>
    <p>[9] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree-based reparameterization framework for analysis of sum-product and related algorithms. IEEE Transactions on Information Theory, 49(5), 2003.</p>
    <p>[10] Y. Weiss. Correctness of local probability propagation in graphical models with loops. Neural Computation, 12(1):141, 2000.</p>
    <p>[11] Y. Weiss and W. Freeman. On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs. IEEETIT: IEEE Transactions on Information Theory, 47, 2001.</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Provably convergent belief propagation for a new type of graph (b-matchings). + Empirically faster than previous algorithms. + Parallelizeable - Only bipartite case. - Requires unique maximum. Interesting theoretical results coming out of sum-product for approximating marginals.</p>
  </div>
</Presentation>
