<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Understanding and Tackling the Hidden Memory Latency for Edge-based Heterogeneous Platform</p>
    <p>Zhendong Wang, Zhen Wang, Cong Liu, and Yang Hu</p>
    <p>Pervasive and Emerging Architecture Research Lab, UT Dallas</p>
    <p>Presented by Zhendong Wang</p>
    <p>HotEdge 2020 Jun. 25, 2020</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Edge intelligence</p>
    <p>Integrated</p>
    <p>GPU</p>
    <p>Latency</p>
    <p>data</p>
    <p>allocation</p>
    <p>CPU</p>
    <p>data</p>
    <p>initialization</p>
    <p>GPU</p>
    <p>Computation</p>
    <p>kernel</p>
    <p>GPU</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>ML/DNN enables a series of edge applications</p>
    <p>backed by Integrated</p>
    <p>GPU</p>
    <p>Deployment in iGPUs are stymied</p>
    <p>(1) Limited memory space</p>
    <p>e.g., TX2: 8GPU, AGX:16GB</p>
    <p>(2) Application stringent latency requirements</p>
    <p>e.g., driving automation is safety-critical and latency-sensitive</p>
    <p>Widely deployed in integrated CPU/GPU(iGPU) platform</p>
    <p>Weight</p>
    <p>Power</p>
    <p>Size</p>
    <p>Rigorous requirements of</p>
    <p>memory footprint and</p>
    <p>processing latency for iGPU platform</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Unified Memory (UM) Management model has relieved the situation</p>
    <p>(2) Save memory footprint(1) Ease memory management</p>
    <p>CUDA: cudaMallocManaged()</p>
    <p>Is current Unified Memory (UM) model good enough?</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Limits of current Unified Memory (UM) model  hidden latency</p>
    <p>t</p>
    <p>Def</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>UM CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>Alloc</p>
    <p>Alloc</p>
    <p>Data processing flow under Def. and UM memory model</p>
    <p>Def: copy-then-execute memory model</p>
    <p>UM: unified memory model</p>
    <p>DNN YOLO2 YOL03 SSD DAVE-2</p>
    <p>M.O.S 49K 81K 10K 250K</p>
    <p>Autonomous driving workloads  large matrix operation scale (M.O.S.) --- Matric Addition and Matric Multiplication</p>
    <p>Kernel time</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Limits of current Unified Memory (UM) model  hidden latency</p>
    <p>matrix addition matrix multiplication</p>
    <p>UM still spends excessive time on initialization</p>
    <p>Def: init ~50% latency</p>
    <p>UM: init ~90% latency</p>
    <p>Others = H2D copy + D2H copy + kernel time</p>
    <p>Init.: data initialization</p>
    <p>Others = kernel time (No copy)</p>
    <p>Init.: data initialization</p>
    <p>t</p>
    <p>Def</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>UM CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>Alloc</p>
    <p>Alloc</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Limits of current Unified Memory (UM) model  hidden latency</p>
    <p>matrix addition matrix multiplication</p>
    <p>UM also slows down the computation kernel</p>
    <p>Observations: Unnecessary initialize data in</p>
    <p>CPU side</p>
    <p>(1) Save initialization latency</p>
    <p>(2) Benefit kernel /overall application response</p>
    <p>performance</p>
    <p>t</p>
    <p>Def</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>UM CPU</p>
    <p>GPU</p>
    <p>Init</p>
    <p>Execution</p>
    <p>Alloc</p>
    <p>Alloc</p>
    <p>Kernel time</p>
    <p>Kernel time</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Enhanced Unified Memory Management (eUMM)</p>
    <p>(1) Initializing data in GPU side</p>
    <p>Existing mechanism of</p>
    <p>legacy Unified</p>
    <p>Management model</p>
    <p>GPU-side data</p>
    <p>initialization in</p>
    <p>eUMM</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>(2) Prefetch-enhanced GPU-Init performance</p>
    <p>Enhanced Unified Memory Management (eUMM)</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Computation kernel is</p>
    <p>not longer slowed down</p>
    <p>Faster data initialization</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Characterization of legacy unified memory management</p>
    <p>Initialization latency</p>
    <p>Kernel launch latency</p>
    <p>An enhanced data initialization model based on Unified Memory management (eUMM)</p>
    <p>Initializing data in GPU side</p>
    <p>Overlapping page mapping with data initialization to further reduce latency</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Prospect &amp; Future work</p>
    <p>Extend eUMM to a broad spectrum of workloads</p>
    <p>Autonomous driving workloads (object detection, object tracking)</p>
    <p>Reduce the inherent overhead of GPU-side data initialization</p>
    <p>GPU-side data initialization does not outperform when data size is small</p>
    <p>GPUDirect</p>
    <p>Bypass CPU to accelerate the communication between GPU and peripheral storage</p>
  </div>
  <div class="page">
    <p>D E P AR T M E N T O F E L E C T R I C AL AN D C O M P U T E R E N G I N E E R I N G</p>
    <p>Thank You</p>
    <p>If you have any questions, please contact zhendong.wang@utdallas.edu</p>
  </div>
</Presentation>
