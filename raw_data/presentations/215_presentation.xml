<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Supporting the Global Arrays PGAS Model Using MPI One-Sided Communication</p>
    <p>James Dinan, Pavan Balaji, Jeff Hammond, Sriram Krishnamoorthy, and Vinod Tipparaju</p>
    <p>Presented by: James Dinan James Wallace Gives Postdoctoral Fellow Argonne National Laboratory</p>
  </div>
  <div class="page">
    <p>Global Arrays, a Global-View Data Model</p>
    <p>Distributed, shared multidimensional arrays  Aggregate memory of multiple nodes into global data space  Programmer controls data distribution, can exploit locality</p>
    <p>One-sided data access: Get/Put({i, j, k}{i, j, k})  NWChem data management: Large coeff. tables (100GB+)</p>
    <p>Shared</p>
    <p>G lo</p>
    <p>ba l a</p>
    <p>dd re</p>
    <p>ss</p>
    <p>sp ac</p>
    <p>e</p>
    <p>Private</p>
    <p>Proc0 Proc1 Procn</p>
    <p>X[M][M][N]</p>
    <p>X[1..9] [1..9][1..9]</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>ARMCI: The Aggregate Remote Memory Copy Interface</p>
    <p>GA runtime system  Manages shared memory  Provides portability  Native implementation</p>
    <p>One-sided communication  Get, put, accumulate,   Load/store on local data  Noncontiguous operations</p>
    <p>Mutexes, atomics, collectives, processor groups,</p>
    <p>Location consistent data access  I see my operations in issue order</p>
    <p>GA_Put({x,y},{x,y})</p>
    <p>ARMCI_PutS(rank, addr, )</p>
  </div>
  <div class="page">
    <p>Implementing ARMCI</p>
    <p>ARMCI Support  Natively implemented  Sparse vendor support  Implementations lag systems</p>
    <p>MPI is ubiquitous  Support one-sided for 15 years</p>
    <p>Goal: Use MPI RMA to implement ARMCI 1. Portable one-sided communication for NWChem users 2. MPI-2: drive implementation performance, one-sided tools 3. MPI-3: motivate features 4. Interoperability: Increase resources available to application</p>
    <p>ARMCI/MPI share progress, buffer pinning, network and host resources</p>
    <p>Challenge: Mismatch between MPI-RMA and ARMCI</p>
    <p>Native ARMCI-MPI</p>
  </div>
  <div class="page">
    <p>MPI Remote Memory Access Interface</p>
    <p>Active and Passive target Modes  Active: target participates  Passive: target does not participate</p>
    <p>Window: Expose memory for RMA  Logical public and private copies  Conservative data consistency model</p>
    <p>Accesses must occur within an epoch  Lock(window, rank)  Unlock(window, rank)  Access mode can be exclusive or shared  Operations are not ordered within an epoch</p>
    <p>Unlock</p>
    <p>Rank 0 Rank 1</p>
    <p>Get(Y)</p>
    <p>Put(X)</p>
    <p>Lock</p>
    <p>Completion</p>
    <p>Public Copy Public Copy</p>
    <p>Private Copy</p>
    <p>Private Copy</p>
  </div>
  <div class="page">
    <p>MPI-2 RMA Separate Memory Model Concurrent, conflicting accesses are erroneous</p>
    <p>Conservative, but extremely portable  Compatible with non-coherent memory systems</p>
    <p>Public Copy Public Copy</p>
    <p>Private Copy</p>
    <p>Private Copy</p>
    <p>Same source Same epoch Diff. Sources</p>
    <p>X X X X</p>
    <p>load store</p>
  </div>
  <div class="page">
    <p>ARMCI/MPI-RMA Mismatch</p>
  </div>
  <div class="page">
    <p>Translation: Global Memory Regions</p>
    <p>Translate between ARMCI and MPI shared data segment representations  ARMCI: Array of base pointers  MPI: Window object</p>
    <p>Translate between MPI and ARMCI ranks  MPI: Window/comm rank  ARMCI: Absolute rank</p>
    <p>Preserve MPI window semantics  Manage access epochs  Protect shared buffers</p>
    <p>Metadata 0 1  N</p>
    <p>Allocation Metadata</p>
    <p>MPI_Win window; int size[nproc]; ARMCI_Group grp; ARMCI_Mutex rmw;</p>
    <p>Allocation Metadata</p>
    <p>MPI_Win window; int size[nproc]; ARMCI_Group grp; ARMCI_Mutex rmw;</p>
    <p>Absolute Process Id</p>
  </div>
  <div class="page">
    <p>GMR: Shared Segment Allocation</p>
    <p>base[nproc] = ARMCI_Malloc(size, group)</p>
    <p>All ranks can pass different size  Size = 0 yields base[me] = NULL  base[me] must be valid on node  Solution: Allgather base pointers</p>
    <p>ARMCI_Free(ptr)</p>
    <p>Need to find allocation and group  Local process may pass NULL</p>
    <p>Solution: Allreduce to select leader  Leader broadcasts their base pointer  All can lookup the allocation</p>
    <p>Metadata 0 1  N</p>
    <p>Absolute Process Id</p>
  </div>
  <div class="page">
    <p>GMR: Preserving MPI local access semantics ARMCI_Put( src = 0x9e0, dst = 0x39b,</p>
    <p>size = 8 bytes, rank = 1 );</p>
    <p>Problem: Local buffer is also shared  Cant access without epoch  Can we lock it?</p>
    <p>Same window: not allowed  Diff window: can deadlock</p>
    <p>Solution: Copy to private buffer</p>
    <p>src_copy = Lock; memcpy; Unlock</p>
    <p>xrank = GMR_Translate(comm, rank) Lock(win, xrank) Put(src_cpy, dst, size, xrank) Unlock(win, xrank)</p>
    <p>Metadata 0 1  N</p>
    <p>Allocation Metadata</p>
    <p>window = win; Size = { 1024, }; Grp = comm;</p>
    <p>Allocation Metadata</p>
    <p>window = win; Size = { 1024, }; Grp = comm;</p>
    <p>Absolute Process Id</p>
  </div>
  <div class="page">
    <p>ARMCI Noncontiguous Operations: I/O Vector  Generalized noncontiguous transfer</p>
    <p>with uniform segment size: typedef struct {</p>
    <p>void **src_ptr_array; // Source addresses void **dst_ptr_array; // Dest. Addresses int bytes; // Length of all seg. int ptr_array_len; // Number of segments</p>
    <p>} armci_giov_t;</p>
    <p>Three methods to support in MPI 1. Conservative (one epoch): Lock, Put/Get/Acc, Unlock,  2. Batched (multiple epochs): Lock, Put/Get/Acc, , Unlock 3. Direct: Generate MPI indexed datatype for source and destination</p>
    <p>Single operation/epoch: Lock, Put/Get/Acc, Unlock  Handoff processing to MPI</p>
    <p>ARMCI_GetV()</p>
  </div>
  <div class="page">
    <p>ARMCI Noncontiguous Operations: Strided</p>
    <p>Transfer a section of an N-d array into an N-d buffer</p>
    <p>Transfer options:  Translate into an IOV  Generate datatypes</p>
    <p>MPI Subarray datatype:  dim = sl+1  Dims[0] = count[0]  Dims[1..dim-2] = stride[i]/Dims[i-1]  Dims[dim-1] = Count[dim-1]  Index = { 0, 0, 0 }  Sub_dims = Count</p>
    <p>src Source pointer</p>
    <p>dst Destination pointer</p>
    <p>sl Number of stride levels (dim-1)</p>
    <p>Count[sl+1] Number of units in each dim.</p>
    <p>src_stride[sl] Source stride array</p>
    <p>dst_stride[sl] Destination stride array</p>
    <p>ARMCI Strided Specification</p>
  </div>
  <div class="page">
    <p>Avoiding concurrent, conflicting accesses</p>
    <p>Contiguous operations  Dont know what other nodes do  Wrap each call in an exclusive epoch</p>
    <p>Noncontiguous operations  I/O Vector segments may overlap  MPI Error!</p>
    <p>Must detect errors and fall back to conservative mode if needed</p>
    <p>Generate a conflict tree  Sorted, self-balancing AVL tree 1. Search the tree for a match 2. If (match found): Conflict! 3. Else: Insert into the tree</p>
    <p>Merge search/insert steps into a single traversal</p>
  </div>
  <div class="page">
    <p>Additional ARMCI Operations</p>
    <p>MPI-2 RMA doesnt provide any atomic RMW operations  Read, modify, write is forbidden within an epoch</p>
    <p>Mutex implementation: Latham, Ross, &amp; Thakur [IJHPCA 07]  Does not poll over the network, space scales as O(P)</p>
    <p>Mutex_lock, get, modify, put, Mutex_unlock  Slow, best we can do in MPI-2</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>System Nodes Cores Memory Interconnect MPI</p>
    <p>IBM BG/P (Intrepid) 40,960 1 x 4 2 GB 3D Torus IBM MPI</p>
    <p>IB (Fusion) 320 2 x 4 36 GB IB QDR MVAPICH2 1.6</p>
    <p>Cray XT5 (Jaguar) 18,688 2 x 6 16 GB Seastar 2+ Cray MPI</p>
    <p>Cray XE6 (Hopper) 6,392 2 x 12 32 GB Gemini Cray MPI</p>
    <p>Communicaton Benchmarks  Contiguous bandwidth  Noncontiguous Bandwidth</p>
    <p>NWChem performance evaluation  CCSD(T) calculation on water pentamer</p>
    <p>IB, XT5: Native much better than ARMCI-MPI (needs tuning)</p>
  </div>
  <div class="page">
    <p>Impact of Interoperability</p>
    <p>ARMCI: lose performance with MPI buffer, unregistered path  MPI: lose performance with ARMCI buffer, on-demand reg.</p>
    <p>IB Cluster: ARMCI and MPI Get Bandwidth</p>
  </div>
  <div class="page">
    <p>Contiguous Communication Bandwidth (BG/P &amp; XE6)</p>
    <p>BG/P: Native is better for small to medium size messages  Bandwidth regime: get/put are same and acc is ~15% less BW</p>
    <p>XE: ARMCI-MPI is 2x better for get/put  Double precision accumulate, 2x better small, same large xfers</p>
  </div>
  <div class="page">
    <p>Strided Communication Bandwidth (BG/P)</p>
    <p>Segment size 1 kB</p>
    <p>Batched is best  Other methods always pack</p>
    <p>Packing in host CPU slower than injecting into network</p>
    <p>MPI implementation should select this automatically</p>
    <p>Performance is close to native</p>
  </div>
  <div class="page">
    <p>Strided Communication Benchmark (XE6)</p>
    <p>Segment size 1 kB</p>
    <p>Batched is best for Acc  Not clear for others</p>
    <p>Significant performance advantage over current native implementation  Under active development</p>
  </div>
  <div class="page">
    <p>NWChem Performance (BG/P)</p>
  </div>
  <div class="page">
    <p>NWChem Performance (XE6)</p>
  </div>
  <div class="page">
    <p>Looking Forward to MPI-3 RMA</p>
    <p>Unified memory model  Take advantage of coherent hardware  Relaxed synchronization will yield</p>
    <p>better performance  Conflicting accesses</p>
    <p>Localized to locations accessed  Relaxed to undefined  Load/store does not corrupt</p>
    <p>Atomic CAS, and Fetch-and-Add, and new accumulate operations  Mutex space overhead MPI-3: O(1)</p>
    <p>Request-based non-blocking ops  Shared memory (IPC) windows</p>
    <p>Public Copy Public Copy</p>
    <p>Private Copy</p>
    <p>Private Copy</p>
    <p>Unified Copy</p>
    <p>Unified Copy</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>ARMCI-MPI Provides:  Complete, portable runtime system for GA and NWChem  MPI-2 performance driver  MPI-3 feature driver</p>
    <p>Mechanisms to overcome interface and semantic mismatch  Performance is pretty good, dependent on impl. tuning</p>
    <p>Production use on BG/P and BG/Q  MPI-2 wont match native because of separate memory model  MPI-3 designed to close the memory model gap</p>
    <p>Available for download with MPICH2  Integration with ARMCI/GA in progress</p>
    <p>Contact: Jim Dinan &lt;dinan@mcs.anl.gov&gt;</p>
  </div>
  <div class="page">
    <p>Additional Slides</p>
  </div>
  <div class="page">
    <p>Mutex Lock Algorithm</p>
    <p>Mutex is a byte vector on rank p: V[nproc] = { 0 }</p>
    <p>Mutex_lock: 1. MPI_Win_lock(mutex_win, p, EXCL) 2. Put(V[me], 1) 3. Get(V[0..me-1, me+1..nproc-1]) 4. MPI_Win_unlock(mutex_win, p, EXCL) 5. If (V[0..me-1, me+1..nproc-1] == 0)</p>
    <p>Return SUCCESS</p>
    <p>Remote:</p>
    <p>Local: 1</p>
  </div>
  <div class="page">
    <p>Mutex Unlock Algorithm</p>
    <p>Mutex is a byte vector on rank p: V[nproc]</p>
    <p>Mutex_unlock: 1. MPI_Win_lock(mutex_win, p, EXCL) 2. Put(V[me], 0) 3. Get(V[0..me-1, me+1..nproc-1]) 4. MPI_Win_unlock(mutex_win, p, EXCL) 5. For i = 1..nproc</p>
    <p>If (V[(me+i)%nproc] == 1) 1. Send(NOTIFICATION, (me+i)%nproc) 2. Break</p>
    <p>Remote:</p>
    <p>Local: 0</p>
    <p>Notify</p>
  </div>
  <div class="page">
    <p>Additional Data</p>
  </div>
  <div class="page">
    <p>Contiguous Microbenchmark (BG/P and XE6)</p>
    <p>BG/P: Native is better for small to medium size messages  Bandwidth regime: get/put are same and acc is ~15% less BW</p>
    <p>XE: ARMCI-MPI is 2x better for get/put  Similar for double precision accumulate</p>
  </div>
  <div class="page">
    <p>Strided Communication Bandwidth (BG/P)</p>
  </div>
  <div class="page">
    <p>Strided Communication Benchmark (XE6)</p>
  </div>
  <div class="page">
    <p>Contiguous Microbenchmark (IB and XT5)</p>
    <p>IB: Close to native for get/put in the bandwidth regime  Performance is a third of native for accumulate</p>
    <p>XE: Close to native for moderately sized messages  Performance is half of native in the bandwidth regime</p>
  </div>
  <div class="page">
    <p>Strided Bandwidth (IB)</p>
    <p>Direct is the best option in most cases</p>
    <p>IOV-Batched is better for large message accumulate</p>
    <p>Tuning needed!</p>
  </div>
  <div class="page">
    <p>Strided Bandwidth (XT5)</p>
    <p>Direct is best  It should be!</p>
    <p>Tuning needed to better handle large messages and noncontiguous</p>
  </div>
  <div class="page">
    <p>NWChem Performance (XT5)</p>
  </div>
  <div class="page">
    <p>NWChem Performance (IB)</p>
  </div>
</Presentation>
