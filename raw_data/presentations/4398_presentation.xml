<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Neural Hidden Markov Model for Machine Translation</p>
    <p>Weiyue Wang, Derui Zhu, Tamer Alkhouli, Zixuan Gan and Hermann Ney</p>
    <p>{surname}@i6.informatik.rwth-aachen.de</p>
    <p>July 17th, 2018</p>
    <p>Human Language Technology and Pattern Recognition Lehrstuhl fr Informatik 6</p>
    <p>Computer Science Department RWTH Aachen University, Germany</p>
    <p>W. Wang: Neural HMM for MT 1 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>I Attention-based neural translation models</p>
    <p>. attend to specific positions on the source side to generate translation</p>
    <p>. improvements over pure encoder-decoder sequence-to-sequence approach</p>
    <p>I Neural HMM has been successfully applied on top of SMT systems [Wang &amp; Alkhouli+ 17]</p>
    <p>I This work explores its application in standalone decoding</p>
    <p>. end-to-end, only with neural networks  NMT</p>
    <p>. LSTM structures outperform FFNN variants in [Wang &amp; Alkhouli+ 17]</p>
    <p>W. Wang: Neural HMM for MT 2 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Neural Hidden Markov Model</p>
    <p>I Translation</p>
    <p>. source sentence fJ1 = f1...fj...fJ</p>
    <p>. target sentence eI1 = e1...ei...eI</p>
    <p>. alignment i  j = bi I Model translation using an alignment model and a lexicon model:</p>
    <p>p(eI1|fJ1 ) =  bI1</p>
    <p>p(eI1,b I 1|fJ1 ) (1)</p>
    <p>:=  bI1</p>
    <p>I i=1</p>
    <p>p(ei|bi1,ei10 ,fJ1 )   lexicon model</p>
    <p>p(bi|bi11 ,ei10 ,fJ1 )   alignment model</p>
    <p>(2)</p>
    <p>with p(bi|bi11 ,ei10 ,fJ1 ) := p(i|b i1 1 ,e</p>
    <p>i1 0 ,f</p>
    <p>J 1 )</p>
    <p>. predicts the jump i = bi bi1</p>
    <p>W. Wang: Neural HMM for MT 3 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Neural Hidden Markov Model</p>
    <p>f1    fj1 fj fj+1    fJ</p>
    <p>hj</p>
    <p>e0</p>
    <p>e1</p>
    <p>ei1</p>
    <p>ei</p>
    <p>ei+1</p>
    <p>eI</p>
    <p>si1</p>
    <p>si</p>
    <p>hj si1 ei1</p>
    <p>p(ei|hj, si1, ei1)</p>
    <p>I Neural network based lexicon model</p>
    <p>W. Wang: Neural HMM for MT 4 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Neural Hidden Markov Model</p>
    <p>f1    fj1 fj fj+1    fJ</p>
    <p>hj</p>
    <p>e0</p>
    <p>e1</p>
    <p>ei1</p>
    <p>ei</p>
    <p>ei+1</p>
    <p>eI</p>
    <p>si1</p>
    <p>si</p>
    <p>hj si1 ei1</p>
    <p>p(i|hj, si1, ei1)</p>
    <p>I Neural network based alignment model (j = bi1) W. Wang: Neural HMM for MT 4 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>I Training criterion for sentence pairs (Fr,Er),r = 1, ...,R:</p>
    <p>argmax</p>
    <p>{ r</p>
    <p>log p(Er|Fr) }</p>
    <p>(3)</p>
    <p>I Derivative for a single sentence pair (F,E) = (fJ1 ,e I 1):</p>
    <p>log p(E|F) =</p>
    <p>j,j</p>
    <p>i</p>
    <p>pi(j ,j|fJ1 ,eI1; )</p>
    <p>HMM posterior weights</p>
    <p>log p(j,ei|j,ei10 ,fJ1 ; ) (4)</p>
    <p>I Entire training procedure: backpropagation in an EM framework</p>
    <p>W. Wang: Neural HMM for MT 5 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Decoding</p>
    <p>I Search over all possible target strings</p>
    <p>max eI1</p>
    <p>p(eI1|fJ1 ) = max eI1</p>
    <p>bI1</p>
    <p>i</p>
    <p>p(bi,ei|bi1,ei10 ,fJ1 )</p>
    <p>I Extending partial hypothesis from ei10 to e i 0</p>
    <p>Q(i,j; ei0) =  j</p>
    <p>[ p(j,ei|j,ei10 ,fJ1 ) Q(i 1,j; ei10 )</p>
    <p>] (5)</p>
    <p>I Pruning:</p>
    <p>Q(i; ei0) =  j</p>
    <p>Q(i,j; ei0)</p>
    <p>argmax ei</p>
    <p>Q(i; ei0)  select several candidates (6)</p>
    <p>W. Wang: Neural HMM for MT 6 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Decoding</p>
    <p>I No explicit coverage constraints</p>
    <p>. one-to-many alignment cases and unaligned source words</p>
    <p>I Search space in decoding</p>
    <p>. neural HMM: consists of both alignment and translation decisions</p>
    <p>. attention model: consists only of translation decisions</p>
    <p>I Decoding complexity (J = source sentence length, I = target sentence length)</p>
    <p>. neural HMM: O(J2  I)</p>
    <p>. attention model: O(J  I)</p>
    <p>. in practice, neural HMM 3 times slower than attention model</p>
    <p>W. Wang: Neural HMM for MT 7 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>I WMT 2017 GermanEnglish and ChineseEnglish translation tasks I Quality measured with case sensitive BLEU and TER on newstests2017</p>
    <p>I Moses tokenizer and truecasing scripts [Koehn &amp; Hoang+ 07]</p>
    <p>I Jieba1 segmenter for Chinese data</p>
    <p>I 20K byte pair encoding (BPE) operations [Sennrich &amp; Haddow+ 16]</p>
    <p>. joint for GermanEnglish and separate for ChineseEnglish I Attention-based system are trained with Sockeye [Hieber &amp; Domhan+ 17]</p>
    <p>. encoder and decoder embedding layer size 620</p>
    <p>. a bidirectional encoder layer with 1000 LSTMs with peephole connections</p>
    <p>. Adam [Kingma &amp; Ba 15] as optimizer with a learning rate of 0.001</p>
    <p>. batch size 50, 30% dropout</p>
    <p>. beam search with beam size 12</p>
    <p>. model weights averaging</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>I Neural hidden markov model implemented in TensorFlow [Abadi &amp; Agarwal+ 16]</p>
    <p>. encoder and decoder embedding layer size 350</p>
    <p>. projection layer size 800 (400+200+200)</p>
    <p>. three hidden layers of sizes 1000, 1000 and 500 respectively</p>
    <p>. normal softmax layer  lexicon model: large output layer with roughly 25K nodes  alignment model: small output layer with 201 nodes</p>
    <p>. Adam as optimizer with a learning rate of 0.001</p>
    <p>. batch size 20, 30% dropout</p>
    <p>. beam search with beam size 12</p>
    <p>. model weights averaging</p>
    <p>W. Wang: Neural HMM for MT 9 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>WMT 2017 # free GermanEnglish EnglishGerman ChineseEnglish</p>
    <p>parameters BLEU[%] TER[%] BLEU[%] TER[%] BLEU[%] TER[%]</p>
    <p>FFNN-based neural HMM 33M 28.3 51.4 23.4 58.8 19.3 64.8 LSTM-based neural HMM 52M 29.6 50.5 24.6 57.0 20.2 63.7 Attention-based neural network 77M 29.5 50.8 24.7 57.4 20.2 63.8</p>
    <p>I FFNN-based neural HMM: [Wang &amp; Alkhouli+ 17] applied in decoding</p>
    <p>I LSTM-based neural HMM: this work</p>
    <p>I Attention-based neural network: [Bahdanau &amp; Cho+ 15]</p>
    <p>I All models trained without synthetic data</p>
    <p>I Single model used for decoding</p>
    <p>I LSTM models improve FFNN-based system by up to 1.3% BLEU and 1.8% TER</p>
    <p>I Comparable performance with attention-based system</p>
    <p>W. Wang: Neural HMM for MT 10 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>I Apply NNs to conventional HMM for MT</p>
    <p>I End-to-end with a stand-alone decoder</p>
    <p>I Comparable performance with the standard attention-based system</p>
    <p>. significantly outperforms the feed-forward variant</p>
    <p>I Future work</p>
    <p>. Speed up training and decoding</p>
    <p>. Application in automatic post editing</p>
    <p>. Combination with attention or transformer [Vaswani &amp; Shazeer+ 17] model</p>
    <p>W. Wang: Neural HMM for MT 11 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Thank you for your attention</p>
    <p>Weiyue Wang</p>
    <p>wwang@cs.rwth-aachen.de</p>
    <p>http://www-i6.informatik.rwth-aachen.de/</p>
    <p>W. Wang: Neural HMM for MT 12 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Appendix: Motivation</p>
    <p>I Neural HMM compared to attention-based systems</p>
    <p>. recurrent encoder and decoder without attention component</p>
    <p>. replacing attention mechanism by a first-order HMM alignment model  attention levels: deterministic normalized similarity scores  HMM alignments: discrete random variables and must be marginalized</p>
    <p>. separating the alignment model from the lexicon model  more flexibility in modeling and training  avoids propagating errors from one model to another  implies an extended degree of interpretability and control over the model</p>
    <p>W. Wang: Neural HMM for MT 13 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Appendix: Analysis</p>
    <p>er w ollte</p>
    <p>nie an irgendeiner</p>
    <p>A rt</p>
    <p>von A useinandersetzung</p>
    <p>teilnehm en</p>
    <p>. he</p>
    <p>never</p>
    <p>wanted</p>
    <p>to</p>
    <p>be</p>
    <p>in</p>
    <p>any</p>
    <p>kind</p>
    <p>of</p>
    <p>altercation</p>
    <p>.</p>
    <p>Attention-based NMT</p>
    <p>er w ollte</p>
    <p>nie an irgendeiner</p>
    <p>A rt</p>
    <p>von A useinandersetzung</p>
    <p>teilnehm en</p>
    <p>. he</p>
    <p>never</p>
    <p>wanted</p>
    <p>to</p>
    <p>be</p>
    <p>in</p>
    <p>any</p>
    <p>kind</p>
    <p>of</p>
    <p>altercation</p>
    <p>.</p>
    <p>Neural HMM</p>
    <p>I Attention weight and alignment matrices visualized in heat map form</p>
    <p>I Generated by attention NMT baseline and neural HMM</p>
    <p>W. Wang: Neural HMM for MT 14 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Appendix: Analysis</p>
    <p>source 28-jhriger Koch in San Francisco Mall tot aufgefunden reference 28-Year-Old Chef Found Dead at San Francisco Mall attention NMT 28-year-old cook in San Francisco Mall found dead neural HMM 28-year-old cook found dead in San Francisco Mall</p>
    <p>source Frankie hat in GB bereits fast 30 Jahre Gewinner geritten , was toll ist . reference Frankie s been riding winners in the UK for the best part of 30 years which is great to see . attention NMT Frankie has been a winner in the UK for almost 30 years , which is great . neural HMM Frankie has ridden winners in the UK for almost 30 years , which is great .</p>
    <p>source Wer baut Braunschweigs gnstige Wohnungen ? reference Who is going to build Braunschweig s low-cost housing ? attention NMT Who does Braunschweig build cheap apartments ? neural HMM Who builds Braunschweig s cheap apartments ?</p>
    <p>I Sample translations from the WMT GermanEnglish newstest2017 set . underline source words of interest . italicize correct translations . bold-face for incorrect translations</p>
    <p>W. Wang: Neural HMM for MT 15 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[Abadi &amp; Agarwal+ 16] M. Abadi, A. Agarwal, P. Barham et al.: TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. CoRR, Vol. abs/1603.04467, 2016. 9</p>
    <p>[Bahdanau &amp; Cho+ 15] D. Bahdanau, K. Cho, Y. Bengio: Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of the 3rd International Conference on Learning Representations, San Diego, CA, USA, May 2015. 10</p>
    <p>[Hieber &amp; Domhan+ 17] F. Hieber, T. Domhan, M. Denkowski, D. Vilar, A. Sokolov, A. Clifton, M. Post: Sockeye: A Toolkit for Neural Machine Translation. ArXiv e-prints, Vol. abs/1712.05690, December 2017. 8</p>
    <p>[Kingma &amp; Ba 15] D.P. Kingma, J.L. Ba: Adam: A method for stochastic optimization. In Proceedings of the Third International Conference on Learning Representations, San Diego, CA, USA, May 2015. 8</p>
    <p>[Koehn &amp; Hoang+ 07] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst: Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for</p>
    <p>W. Wang: Neural HMM for MT 16 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177180, Prague, Czech Republic, June 2007. 8</p>
    <p>[Sennrich &amp; Haddow+ 16] R. Sennrich, B. Haddow, A. Birch: Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 1715 1725, Berlin, Germany, August 2016. 8</p>
    <p>[Vaswani &amp; Shazeer+ 17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin: Attention Is All You Need. In 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, December 2017. 11</p>
    <p>[Wang &amp; Alkhouli+ 17] W. Wang, T. Alkhouli, D. Zhu, H. Ney: Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pp. 125131, Vancouver, Canada, August 2017. 2, 10</p>
    <p>W. Wang: Neural HMM for MT 17 / 12 July 17th, 2018</p>
  </div>
  <div class="page">
    <p>The Blackslide</p>
    <p>GoBack</p>
  </div>
</Presentation>
