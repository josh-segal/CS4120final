<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Shenango: Achieving High CPU Efficiency for Latency-sensitive Datacenter Workloads</p>
    <p>Hari Balakrishnan</p>
    <p>Jonathan Behrens</p>
    <p>Adam Belay</p>
    <p>Joshua Fried</p>
    <p>Amy Ousterhout</p>
  </div>
  <div class="page">
    <p>Trend #1: Faster Networks</p>
    <p>But todays operating systems add significant overheads to I/O</p>
    <p>Latency: ~5 s Throughput: 100 Gbits/s 100x</p>
  </div>
  <div class="page">
    <p>The Rise of Kernel Bypass</p>
    <p>Dedicate busy-spinning cores  Applications directly poll NIC queues  Enables higher throughput and lower latency</p>
    <p>Kernel</p>
    <p>Kernel Bypass</p>
    <p>Application</p>
    <p>NIC packet queues</p>
    <p>Kernel</p>
    <p>Traditional Approach Application</p>
    <p>NIC packet queues</p>
    <p>core core core core</p>
    <p>core core</p>
  </div>
  <div class="page">
    <p>Trend #2: Slowing of Moores Law</p>
    <p>CPUs only utilized 10-66% today  CPU efficiency becomes increasingly important</p>
    <p>increased demand for servers increased demand for energy</p>
  </div>
  <div class="page">
    <p>Load Variation Makes Efficiency Challenging</p>
    <p>Load variation for datacenter workloads  Days: diurnal cycles  Microseconds: packet bursts, thread bursts</p>
    <p>Peak load requires significantly more cores than average load</p>
    <p>Figure 1: Continuously changing queries per second</p>
    <p>bursts of traces across all software layers without requiring explicit coordination. Unlike traditional sampling or bursty approaches which rely on explicitly maintained counters [19, 6] or propagation of sampling decisions [27, 22], coordinated bursty tracing uses time to coordinate the start and end of bursts. Since all layers collect their bursts at the same time (clock drift has not been a problem in practice), we can reason across the entire stack of our application rather than just a single layer. By collecting many bursts we get a random sampling of the mix of operations which enables us to derive valid conclusions from our performance investigations.</p>
    <p>Second, since interactions between software layers are responsible for many performance problems, we need to be able to connect trace events at one layer with events at another. Vertical context injection solves this problem by making a stylized sequence of innocuous system calls at each high-level event of interest. These system calls insert the system call events into the kernel trace which we can analyze to produce a trace that interleaves both high and low-level events. Unlike prior work (e.g., [27] or [15]) our approach does not require explicit propagation of a trace context through the layers of the software stack.</p>
    <p>To illustrate the above points, this paper presents data from Gmail, a popular email application from Google. However, the authors have used the techniques for many other applications at Google, particularly Google Drive; the lessons in this paper apply equally well to those other applications.</p>
    <p>The primary challenge in performance analysis of cloud applications stems from their constantly varying load. Figure 1 shows scaled queries per second (QPS) across thousands of processes serving tens of millions of users over the course of a week for one deployment of Gmail. By scaled we mean that we have multiplied the actual numbers by a constant to protect Googles proprietary information; since we have multiplied each point by the</p>
    <p>Figure 2: Continuously changing response size</p>
    <p>Figure 3: Continuously changing user behavior</p>
    <p>same constant and each graph is zero based, it allows relative comparisons between points or curves on the same graph.1 The time axes for all graphs in this paper are in US Pacific time and start on a Sunday unless the graph is for a one-off event in which case we pick the time axis most suitable for the event. We see that load on our system changes continuously: from day to day and from hour to hour by more than a factor of two.</p>
    <p>While one expects fluctuations in QPS (e.g., there are more active users during the day than at night), one does not expect the mix of requests to fluctuate significantly. Figure 2 shows one characteristic of requests, the response size per request, over the course of a week. Figure 2 shows that response size per request changes over the course of the week and from hour to hour (by more than a factor of two) which indicates that the actual mix of requests to our system (and not just their count) changes continuously.</p>
    <p>The remainder of this section explores the sources of variation in the mix of requests.</p>
    <p>Performance Analysis of Cloud Applications, NSDI 18 5</p>
  </div>
  <div class="page">
    <p>The Need for Multiplexing</p>
    <p>Two types of applications: latency-sensitive and batchprocessing</p>
    <p>Pack both on the same server  Bing does this on over 90,000 servers</p>
  </div>
  <div class="page">
    <p>Multiplexing with Existing Approaches</p>
    <p>Example: Memcached + batch processing application</p>
    <p>client server</p>
    <p>~1 s</p>
  </div>
  <div class="page">
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Memcached Offered Load (million requests/s)</p>
    <p>Multiplexing with Existing Approaches</p>
    <p>No existing approach provides high network performance and high CPU efficiency</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s)</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Linux Arachne Shenango ZygOS</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>poor latency poor throughput</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Linux Arachne Shenango ZygOS</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s)</p>
    <p>Linux Goal</p>
    <p>ac kg</p>
    <p>ro un</p>
    <p>d O</p>
    <p>ps /s</p>
    <p>poor efficiency</p>
    <p>poor latency poor throughput</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Reconcile the tradeoff between high CPU efficiency and network performance</p>
    <p>Reallocate cores across applications at microsecond granularity  Coarser granularities insufficient for microsecond-scale tasks and</p>
    <p>microsecond-scale bursts</p>
  </div>
  <div class="page">
    <p>Challenges of Fast Reallocations</p>
    <p>How many cores does an application need?  Application-level metrics are too slow  Multiple sources of load: packets and threads</p>
    <p>Overhead of reallocation  Reconfiguring hardware is too slow</p>
    <p>Existing systems dont address these challenges</p>
  </div>
  <div class="page">
    <p>Shenangos Contributions</p>
    <p>Efficient algorithm for determining when an application needs more cores  Based on thread and packet queueing delays</p>
    <p>IOKernel: steers packets in software and allocates cores  Core reallocations take ~5 s</p>
    <p>Cache-aware core selection algorithm  Load balancing of packet protocol (e.g., TCP) handling</p>
  </div>
  <div class="page">
    <p>Shenangos Design</p>
    <p>active core</p>
    <p>App 1</p>
    <p>work stealing</p>
    <p>App 2 app</p>
    <p>thread</p>
    <p>idle core</p>
    <p>IOKernel</p>
    <p>Kernel</p>
    <p>NIC queues</p>
    <p>runtime library</p>
    <p>packet queues</p>
  </div>
  <div class="page">
    <p>How Many Cores Should the IOKernel Allocate?</p>
    <p>active core</p>
    <p>runtime library</p>
    <p>App 1 app</p>
    <p>thread</p>
    <p>IOKernel</p>
    <p>Kernel packet queues</p>
    <p>idle core App 2</p>
    <p>NIC queues</p>
  </div>
  <div class="page">
    <p>Compute Congestion</p>
    <p>Compute congestion: when granting an application an additional core would allow it to complete its work more quickly</p>
    <p>Goal: grant each application as few cores as possible while avoiding compute congestion</p>
    <p>active core</p>
    <p>App 1 app thread</p>
    <p>new thread</p>
  </div>
  <div class="page">
    <p>Queued threads or packets indicate congestion  Any packets or threads queued since the last run (5 s ago)?</p>
    <p>Grant one more core  Ring buffers enable an efficient check</p>
    <p>headt=n-1 &gt; tailt=n implies congestion</p>
    <p>Congestion Detection Algorithm</p>
    <p>tailt=0</p>
    <p>headt=1</p>
    <p>tailt=1</p>
    <p>congested</p>
    <p>headt=0</p>
    <p>tailt=0</p>
    <p>headt=1</p>
    <p>tailt=1</p>
    <p>not congested</p>
    <p>active core</p>
    <p>App 1</p>
    <p>runqueue packet queues</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>IOKernel  Uses DPDK 18.11</p>
    <p>Runtime  UDP and TCP  C++ and Rust bindings</p>
    <p>13,000 lines of code total</p>
  </div>
  <div class="page">
    <p>Evaluation Questions</p>
    <p>How well does Shenango reconcile the tradeoff between CPU efficiency and network performance?</p>
    <p>How does Shenango respond to sudden bursts in load?  How do Shenangos individual mechanisms contribute to its</p>
    <p>overall performance?</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>1 server + 6 clients, 10 Gbits/s NICs  Clients run our open-loop load generator built on Shenango</p>
    <p>Requests follow Poisson arrivals, use TCP</p>
    <p>System Kernel Bypass Networking</p>
    <p>Lightweight Threading</p>
    <p>Balancing Interval</p>
    <p>Linux   4000 s</p>
    <p>System Kernel Bypass Networking</p>
    <p>Lightweight Threading</p>
    <p>Balancing Interval</p>
    <p>Linux   4000 s</p>
    <p>ZygOS (SOSP 17)   N/A</p>
    <p>System Kernel Bypass Networking</p>
    <p>Lightweight Threading</p>
    <p>Balancing Interval</p>
    <p>Linux   4000 s</p>
    <p>ZygOS (SOSP 17)   N/A Arachne (OSDI 18)   50000 s</p>
    <p>System Kernel Bypass Networking</p>
    <p>Lightweight Threading</p>
    <p>Balancing Interval</p>
    <p>Linux   4000 s</p>
    <p>ZygOS (SOSP 17)   N/A Arachne (OSDI 18)   50000 s</p>
    <p>Shenango   5 s</p>
  </div>
  <div class="page">
    <p>CPU Efficiency and Network Performance with Memcached</p>
    <p>Memcached + batch processing application</p>
    <p>Shenango matches ZygOSs tail latency with high CPU efficiency</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Linux Arachne Shenango ZygOS</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Linux Arachne Shenango ZygOS</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Linux Arachne Shenango ZygOS</p>
    <p>B at</p>
    <p>ch O</p>
    <p>ps /s</p>
    <p>kernel bypass networking</p>
    <p>no overprovisioning</p>
    <p>IOKernel is saturated</p>
    <p>client server</p>
  </div>
  <div class="page">
    <p>O ffe</p>
    <p>re d</p>
    <p>Lo ad</p>
    <p>(m ill</p>
    <p>io n</p>
    <p>re qu</p>
    <p>es ts</p>
    <p>/s )</p>
    <p>Shenango is Resilient to Bursts in Load</p>
    <p>TCP requests with 1 s synthetic work + batch processing application</p>
    <p>Increase or decrease the load every 1 s</p>
    <p>reallocates cores 10,000x as often</p>
    <p>Time (s)</p>
    <p>O ffe</p>
    <p>re d</p>
    <p>Lo ad</p>
    <p>(m ill</p>
    <p>io n</p>
    <p>re qu</p>
    <p>es ts</p>
    <p>/s )</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Arachne Shenango</p>
    <p>Time (s)</p>
    <p>O ffe</p>
    <p>re d</p>
    <p>Lo ad</p>
    <p>(m ill</p>
    <p>io n</p>
    <p>re qu</p>
    <p>es ts</p>
    <p>/s )</p>
    <p>% L</p>
    <p>at en</p>
    <p>cy (</p>
    <p>s) Arachne Shenango</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Shenango reconciles the tradeoff between low tail latency and high CPU efficiency</p>
    <p>Reallocates cores at microsecond granularity  Efficient congestion detection algorithm  IOKernel: allocates cores and steers packets in software</p>
    <p>https://github.com/shenango</p>
  </div>
</Presentation>
