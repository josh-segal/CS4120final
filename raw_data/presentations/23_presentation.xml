<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Jun. 28-Jul. 2, 2011ICML2011</p>
    <p>On Information-Maximization Clustering: Tuning Parameter</p>
    <p>Selection and Analytic Solution</p>
    <p>On Information-Maximization Clustering: Tuning Parameter</p>
    <p>Selection and Analytic Solution</p>
    <p>Masashi Sugiyama, Makoto Yamada, Manabu Kimura, and Hirotaka Hachiya</p>
    <p>Department of Computer Science Tokyo Institute of Technology</p>
  </div>
  <div class="page">
    <p>Samples in the same cluster are similar. Samples in other clusters are dissimilar.</p>
    <p>Throughout this talk, we assume is known.</p>
  </div>
  <div class="page">
    <p>A) Clustering B) Tuning parameter optimization</p>
  </div>
  <div class="page">
    <p>K-means Dirichlet process mixture</p>
    <p>Pros and cons:  No tuning parameters.</p>
    <p>Cluster shape depends on pre-defined cluster models ( Gaussian). Initialization is difficult.</p>
    <p>(MacQueen, 1967)</p>
    <p>(Ferguson, 1973)</p>
  </div>
  <div class="page">
    <p>Spectral clustering: K-means after non-linear manifold embedding</p>
    <p>Discriminative clustering: Learn a classifier and cluster labels simultaneously</p>
    <p>Dependence maximization: Determine labels so that dependence on samples is maximized</p>
    <p>Information maximization: Learn a classifier so that some information measure is maximized</p>
    <p>(Shi &amp; Malik, 2000; Ng et al., 2002)</p>
    <p>(Xu et al., 2005; Bach &amp; Harchaoui, 2008)</p>
    <p>(Song et al., 2007; Faivishevsky &amp; Goldberger, 2010)</p>
    <p>(Agakov &amp; Barberu, 2006; Gomes et al., 2010)</p>
  </div>
  <div class="page">
    <p>Kernel/similarity parameter choice is difficult. Initialization is difficult.</p>
  </div>
  <div class="page">
    <p>A) Clustering B) Tuning parameter optimization</p>
  </div>
  <div class="page">
    <p>In the proposed method: A non-parametric kernel classifier is learned so that an information measure is maximized. Tuning parameters are chosen so that an information measure is maximized.</p>
  </div>
  <div class="page">
    <p>As an information measure, we use SMI:</p>
    <p>Ordinary MI is the KL divergence. SMI is the Pearson (PE) divergence. Both KL and PE are f-divergences (thus they have similar properties). Indeed, as ordinary MI, SMI satisfies</p>
  </div>
  <div class="page">
    <p>A) Clustering B) Tuning parameter optimization</p>
  </div>
  <div class="page">
    <p>Learn the classifier so that SMI is maximized.</p>
    <p>Challenge: only is available for training</p>
  </div>
  <div class="page">
    <p>Approximate expectation by sample average:</p>
    <p>Assume cluster-prior is uniform:</p>
    <p>Then we obtain the following SMI approximator: : # clusters</p>
  </div>
  <div class="page">
    <p>Under mutual orthonormality of , a solution is given by principal components of kernel matrix .</p>
    <p>Similar to Ding &amp; He (ICML2004)</p>
  </div>
  <div class="page">
    <p>Adjusting sign of principal components :</p>
    <p>Normalization according to . Rounding-up negative probability estimates to 0.</p>
    <p>Final solution (analytically computable):</p>
    <p>:Vector with all ones</p>
    <p>: -th element of a vector :Vector with all zeros</p>
  </div>
  <div class="page">
    <p>A) Clustering B) Tuning parameter optimization</p>
  </div>
  <div class="page">
    <p>However, is not accurate enough since it is an unsupervised estimator of SMI. In the phase of tuning parameter choice, estimated labels are available!</p>
  </div>
  <div class="page">
    <p>Least-squares mutual information (LSMI): Directly estimate the density ratio</p>
    <p>without going through density estimation. Density-ratio estimation is substantially easier than density estimation ( la Vapnik).</p>
    <p>Suzuki &amp; Sugiyama (AISTATS2010)</p>
    <p>Knowing Knowing</p>
  </div>
  <div class="page">
    <p>Least-squares fitting:</p>
    <p>: Kernel function (We use Gaussian kernel)</p>
  </div>
  <div class="page">
    <p>Global solution can be obtained analytically:</p>
    <p>Kernel and regularization parameter can be determined by cross-validation.</p>
  </div>
  <div class="page">
    <p>SMI approximator is given analytically as</p>
    <p>LSMI achieves a fast non-parametric convergence rate!</p>
    <p>We determine the kernel function in SMIC so that LSMI is maximized.</p>
    <p>Suzuki &amp; Sugiyama (AISTATS2010)</p>
  </div>
  <div class="page">
    <p>Input: Unlabeled samples Kernel candidates</p>
    <p>Output: Cluster labels</p>
    <p>S M</p>
    <p>IC S</p>
    <p>M IC</p>
    <p>LS M</p>
    <p>I LS</p>
    <p>M I</p>
  </div>
  <div class="page">
    <p>A) Clustering B) Tuning parameter optimization</p>
  </div>
  <div class="page">
    <p>Tuning parameter is determined by LSMI maximization.</p>
    <p>: -th neighbor of</p>
    <p>(Zelnik-Manor &amp; Perona, NIPS2004)</p>
  </div>
  <div class="page">
    <p>SMIC with model selection by LSMI works well!</p>
    <p>2 1 0 1 2 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>t</p>
    <p>S M</p>
    <p>I e</p>
    <p>st im</p>
    <p>a te</p>
    <p>t</p>
    <p>S M</p>
    <p>I e st</p>
    <p>im a te</p>
    <p>t</p>
    <p>S M</p>
    <p>I e st</p>
    <p>im a te</p>
    <p>t</p>
    <p>S M</p>
    <p>I e</p>
    <p>st im</p>
    <p>a te</p>
    <p>2 1 0 1 2 2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>2 1 0 1 2 2.5</p>
    <p>2</p>
    <p>1.5</p>
    <p>1</p>
    <p>0.5</p>
    <p>4 2 0 2 4 4</p>
    <p>3</p>
    <p>2</p>
    <p>1</p>
  </div>
  <div class="page">
    <p>MNN: Dependence-maximization clustering based on mean nearest neighbor approximation</p>
    <p>MIC: Information-maximization clustering for kernel logistic models with model selection by maximum-likelihood mutual information</p>
    <p>(Zelnik-Manor &amp; Perona, NIPS2004)</p>
    <p>(MacQueen, 1967)</p>
    <p>(Faivishevsky &amp; Goldberger, ICML2010)</p>
    <p>(Gomes, Krause &amp; Perona, NIPS2010) (Suzuki, Sugiyama, Sese &amp; Kanamori, FSDM2008)</p>
  </div>
  <div class="page">
    <p>Adjusted Rand index (ARI): larger is better Red: Best or comparable by 1%t-test SMIC works well and computationally efficient!</p>
    <p>Digit (d = 256,n = 5000, and c = 10) KM SC MNN MIC SMIC</p>
    <p>ARI 0.42(0.01) 0.24(0.02) 0.44(0.03) 0.63(0.08) 0.63(0.05) Time 835.9 973.3 318.5 84.4[3631.7] 14.4[359.5]</p>
    <p>Face (d = 4096,n = 100, and c = 10)</p>
    <p>KM SC MNN MIC SMIC</p>
    <p>ARI 0.60(0.11) 0.62(0.11) 0.47(0.10) 0.64(0.12) 0.65(0.11)</p>
    <p>Time 93.3 2.1 1.0 1.4[30.8] 0.0[19.3]</p>
    <p>Document (d = 50,n = 700, and c = 7)</p>
    <p>KM SC MNN MIC SMIC</p>
    <p>ARI 0.00(0.00) 0.09(0.02) 0.09(0.02) 0.01(0.02) 0.19(0.03) Time 77.8 9.7 6.4 3.4[530.5] 0.3[115.3]</p>
    <p>Word (d = 50,n = 300, and c = 3) KM SC MNN MIC SMIC</p>
    <p>ARI 0.04(0.05) 0.02(0.01) 0.02(0.02) 0.04(0.04) 0.08(0.05)</p>
    <p>Time 6.5 5.9 2.2 1.0[369.6] 0.2[203.9]</p>
    <p>Accelerometry (d = 5,n = 300, and c = 3)</p>
    <p>KM SC MNN MIC SMIC</p>
    <p>ARI 0.49(0.04) 0.58(0.14) 0.71(0.05) 0.57(0.23) 0.68(0.12)</p>
    <p>Time 0.4 3.3 1.9 0.8[410.6] 0.2[92.6]</p>
    <p>Speech (d = 50, n = 400, and c = 2) KM SC MNN MIC SMIC</p>
    <p>ARI 0.00(0.00) 0.00(0.00) 0.04(0.15) 0.18(0.16) 0.21(0.25) Time 0.9 4.2 1.8 0.7[413.4] 0.3[179.7]</p>
    <p>Sens eval2</p>
  </div>
  <div class="page">
    <p>Cluster initialization is difficult. Tuning parameter choice is difficult.</p>
    <p>SMIC: A new information-maximization clustering method based on squared-loss mutual information (SMI):</p>
    <p>Analytic global solution is available. Objective tuning parameter choice is possible.</p>
    <p>MATLAB code is available from http://sugiyama-www.cs.titech.ac.jp/~sugi/software/SMIC/</p>
  </div>
  <div class="page">
    <p>Feature selection</p>
    <p>Dimensionality reduction</p>
    <p>Independent component analysis</p>
    <p>Independence test</p>
    <p>Causal inference</p>
    <p>Suzuki &amp; Sugiyama (AISTATS2010) Yamada, Niu, Takagi &amp; Sugiyama (ArXiv2011)</p>
    <p>Suzuki &amp; Sugiyama (Neural Comp. 2011)</p>
    <p>Sugiyama &amp; Suzuki (IEICE-ED2011)</p>
    <p>Yamada &amp; Sugiyama (AAAI2010)</p>
    <p>Suzuki, Sugiyama, Sese &amp; Kanamori (BMC Bioinfo. 2009)</p>
  </div>
</Presentation>
