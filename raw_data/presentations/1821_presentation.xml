<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Building a Scalable Multimedia Search Engine Using Infiniband</p>
    <p>Qi Chen</p>
    <p>Yisheng Liao, Christopher Mitchell, Jinyang Li, Zhen Xiao</p>
    <p>Peking University</p>
    <p>NYU</p>
  </div>
  <div class="page">
    <p>Online search must be scalable</p>
    <p>Client</p>
    <p>Example Search Engines: Google, Bing</p>
  </div>
  <div class="page">
    <p>How multimedia search is done</p>
    <p>Feature 1 Feature 2  Feature x</p>
    <p>Img 1 1 1</p>
    <p>Img 2 1 1</p>
    <p>Img n 1 1</p>
    <p>billions of Features</p>
    <p>b il</p>
    <p>li o</p>
    <p>n s</p>
    <p>o f</p>
    <p>im a</p>
    <p>g e</p>
    <p>s</p>
    <p>Indexing (usually done offline)</p>
  </div>
  <div class="page">
    <p>How multimedia search is done</p>
    <p>Feature 1 Feature 2  Feature x</p>
    <p>Img 1 1 1</p>
    <p>Img 2 1 1</p>
    <p>Img n 1 1</p>
    <p>billions of Features</p>
    <p>b il</p>
    <p>li o</p>
    <p>n s</p>
    <p>o f</p>
    <p>im a</p>
    <p>g e</p>
    <p>s</p>
    <p>Search features f1, f2, , fn</p>
    <p>Query image</p>
    <p>Indexing (usually done offline)</p>
  </div>
  <div class="page">
    <p>Two ways to distribute: horizontal partition</p>
    <p>Feature 1 Feature 2  Feature f</p>
    <p>Img 1 1 1</p>
    <p>Img 2 1 1</p>
    <p>Img 3 1</p>
    <p>Img n 1 1</p>
    <p>Not scalable because a query must contact all servers</p>
  </div>
  <div class="page">
    <p>Two ways to distribute: vertical partition</p>
    <p>Feature 1 Feature 2   Feature f</p>
    <p>Img 1 1 1</p>
    <p>Img 2 1 1</p>
    <p>Img n 1 1</p>
    <p>Expensive because a query may look up tens of thousands of features</p>
  </div>
  <div class="page">
    <p>Horizontal vs. vertical: State-of-art and new opportunity</p>
    <p>Horizontal beats vertical partitioning on the Ethernet</p>
    <p>But..</p>
    <p>Ultra-low latency network is coming to data centers</p>
    <p>Infiniband, RoCE</p>
    <p>RTT  10us. (compared to RTT &gt;100us on Ethernet)</p>
    <p>Our insight: Vertical beats horizontal on low-latency networks</p>
    <p>Why latency matters: Use more roundtrips to reduce feature lookups</p>
  </div>
  <div class="page">
    <p>Outlines</p>
  </div>
  <div class="page">
    <p>Overview of VertiCut Image Search</p>
    <p>Indexing</p>
    <p>Offline indexer transforms images to 128-bit binary codes</p>
    <p>Searching</p>
    <p>Online k-nearest-nbr (KNN) algorithm finds k codes with smallest hamming distance to a query code</p>
    <p>Collection of 128-dim vectors</p>
    <p>01011101 10101110</p>
    <p>Collection of 128bit binary codes</p>
    <p>binary transformation</p>
    <p>using Locality Sensitive Hashing</p>
    <p>(LSH)</p>
    <p>feature extraction</p>
    <p>using SIFT</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>T4</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>T4</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>index img list</p>
    <p>T1</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>index img list</p>
    <p>T1</p>
    <p>,Code(x)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>index img list</p>
    <p>T1 T2</p>
    <p>,Code(x)</p>
    <p>index img list</p>
    <p>,Code(x)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>index img list</p>
    <p>T1 T2</p>
    <p>,Code(x)</p>
    <p>index img list</p>
    <p>,Code(x)</p>
    <p>index img list</p>
    <p>,Code(x)</p>
    <p>T3</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>VertiCut uses Multi-index Hashing [CVPR12]</p>
    <p>To index</p>
    <p>Break a 128-bit code into 4 pieces</p>
    <p>Insert i-th piece in hash table Ti</p>
    <p>Code(x) = 011111 000101 000101 001110</p>
    <p>index img list</p>
    <p>T1 T2 T4</p>
    <p>,Code(x)</p>
    <p>index img list</p>
    <p>,Code(x)</p>
    <p>index img list</p>
    <p>,Code(x)</p>
    <p>T3</p>
    <p>index img list</p>
    <p>,Code(x)</p>
  </div>
  <div class="page">
    <p>VertiCut search architecture</p>
    <p>Pilaf DHT [USENIX ATC13]</p>
    <p>Q = 011110</p>
    <p>Get (~10us)</p>
    <p>index img list</p>
    <p>index img list</p>
    <p>index img list</p>
    <p>index img list</p>
    <p>Search nodes</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>query code q</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>For each image code x in C: if distance(x, q) &lt; : add x to result if |result| &gt;= 100: return KNN in result</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>Find KNNs with hamming distance &lt;</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
    <p>For each image code x in C: if distance(x, q) &lt; : add x to result if |result| &gt;= 100: return KNN in result</p>
  </div>
  <div class="page">
    <p>How to do KNN in binary space?</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>For each image code x in C: if distance(x, q) &lt; : add x to result if |result| &gt;= 100: return KNN in result</p>
    <p>query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
    <p>d</p>
  </div>
  <div class="page">
    <p>Optimization #1: approx. KNN</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>For each image code x in C: if distance(x, q) &lt; d: add x to result if |result| &gt;= 100: return KNN in result</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
    <p>For each d = 4, 8, 12, 16, .</p>
    <p>Problem: Large d  numerous (combinatorial) lookups Typically, d=20  #lookups = 165K</p>
  </div>
  <div class="page">
    <p>Optimization #1: approx. KNN</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>For each image code x in C: if distance(x, q) &lt; d: add x to result if |result| &gt;= 100: return KNN in result</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
    <p>For each d = 4, 8, 12, 16, .</p>
    <p>Problem: Large d  numerous (combinatorial) lookups Typically, d=20  #lookups = 165K</p>
    <p>Our insight:  Stop search as soon as the candidate set C is big enough  KNNs in C approximates the true KNNs</p>
  </div>
  <div class="page">
    <p>Optimization #1: approx. KNN</p>
    <p>To search 100 KNNs given a query code q</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx)</p>
    <p>For each d = 4, 8, 12, 16, .</p>
    <p>Our insight:  Stop search as soon as the candidate set C is big enough  KNNs in C approximates the true KNNs</p>
    <p>For each hash table Ti S  Enum indices with distance = For each idx in S: C  CTi.lookup(idx) if |C| &gt;= f * 100: return KNN in result</p>
  </div>
  <div class="page">
    <p>Optimization #1: approx. KNN</p>
    <p>Experiments show:</p>
    <p>To obtain k results, we can stop search when C &gt; 20  k</p>
    <p>Results contain 80% of true KNNs</p>
    <p>Avg. distance of results is close to that of true KNNs (&lt;1)</p>
    <p>Reduces # of lookups by a factor of 40</p>
  </div>
  <div class="page">
    <p>Optimization #2: avoid null lookups</p>
    <p>Pilaf DHT [USENIX ATC13]</p>
    <p>Observation: &gt;90% lookups return empty result</p>
    <p>10011   10011</p>
    <p>10011</p>
    <p>Q = 011110</p>
    <p>Each search node keeps a bitmap for each hash table</p>
    <p>Do a lookup in DHT only after the bitmap returns a hit</p>
    <p>Bitmap size (4*500MB) does not increase with # of images indexed</p>
  </div>
  <div class="page">
    <p>Outlines</p>
  </div>
  <div class="page">
    <p>Experiment Environment</p>
    <p>Experimental Setup</p>
    <p>12 servers connected with 20Gbps Infiniband</p>
    <p>1 billion image descriptors from BIGANN dataset</p>
    <p>Each query retrieves 1000 KNNs</p>
  </div>
  <div class="page">
    <p>Vertical scales better than Horizontal</p>
    <p>~10800 DHT gets ~2700 RTTs</p>
    <p>~22000 DHT gets ~5500 RTTs</p>
  </div>
  <div class="page">
    <p>VertiCut is only feasible on low-latency network</p>
    <p>~2700 RTTs 8 times slower on Ethernet</p>
  </div>
  <div class="page">
    <p>Effects of Optimizations</p>
    <p>No opt Approx Bitmap VertiCut</p>
    <p>reduction</p>
    <p># o</p>
    <p>f D</p>
    <p>H T</p>
    <p>l o</p>
    <p>o k</p>
    <p>u p</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>Outlines</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Bag-of-features based search</p>
    <p>JI et al.[TM13], MARE et al.[MIR10], YAN et al.[SenSys08], MIH[CVPR12], Rankreduce[LSDS-IR10]</p>
    <p>Traditionally use horizontal partition for distribution</p>
    <p>High-dimentional search trees (e.g. KD-tree)</p>
    <p>ALY et al.[BMVC11]</p>
    <p>Build a distributed tree offline  Cannot be incrementally updated</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Ultra low-latency networks allow vertical partition to perform better than traditional horizontal partition</p>
    <p>VertiCut: a scalable image search engine</p>
    <p>Built on top of Pilaf DHT on Infiniband</p>
    <p>Use two optimizations to reduce DHT lookups</p>
    <p>Approximate nearest neighbor search</p>
    <p>Eliminate empty lookups</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
  </div>
</Presentation>
