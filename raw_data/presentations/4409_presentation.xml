<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Influence of Context on Sentence Acceptability Judgements</p>
    <p>Jean-Philippe Bernardy1, Shalom Lappin1, and Jey Han Lau2,3</p>
    <p>July 17, 2018</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>I Sentence acceptability: the extent to which a sentence is natural to native speakers.</p>
    <p>I It encompasses semantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory limitation.</p>
    <p>I Grammaticality, by contrast, is a theoretical concept that measures the syntactic well-formedness of a sentence.</p>
    <p>I Here we are interested in predicting acceptability judgements.</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>I We previously explored using unsupervised probabilistic methods to predict sentence acceptability, and found some success.</p>
    <p>I It provides evidence that linguistic knowledge can be represented as a probabilistic system, addressing foundational questions concerning the categorical nature of grammatical knowledge.</p>
  </div>
  <div class="page">
    <p>Acceptability in Context</p>
    <p>I In previous experiments sentence acceptability was judged (by humans) or predicted (by models) independently of context.</p>
    <p>I Here we extend the research to investigate the impact of context on acceptability.</p>
    <p>I Context is defined as the full document environment surrounding a sentence.</p>
    <p>I Specifically, we want to understand the influence of context on: I Human acceptability ratings I Model prediction of acceptability</p>
  </div>
  <div class="page">
    <p>Human Acceptability Ratings in Context</p>
    <p>I We perform round-trip translation of sentences (e.g. ENFREN) from English Wikipedia to generate a set of sentences with varying degrees of acceptability.</p>
    <p>I We use MTurk to collect acceptability judgements (rated on a 4-point scale).</p>
    <p>I Annotation task was run twice: first without context, and second within the document context.</p>
    <p>I We collect multiple ratings for a sentence and take the mean.</p>
    <p>I Human acceptability ratings: I without context = h; I with context = h+</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>With-context h+ Against Without-context h Ratings</p>
    <p>mean h per sentence</p>
    <p>m ea</p>
    <p>n h +</p>
    <p>p er</p>
    <p>se n</p>
    <p>te n</p>
    <p>ce</p>
  </div>
  <div class="page">
    <p>Observations</p>
    <p>I Pearsons r = 0.80 between h+ and h.</p>
    <p>I Context boosts acceptability ratings most for ill-formed sentences.</p>
    <p>I Surprisingly, context reduces acceptability for the most acceptable sentences.</p>
    <p>I Context compresses distribution of ratings.</p>
    <p>I One-vs-rest correlation, performance of a single annotator against the rest: 0.628 for h and 0.293 for h+.</p>
    <p>I Low correlation is explained by the compression effect of context  good and bad sentences are now less separable.</p>
  </div>
  <div class="page">
    <p>Modelling Acceptability with Unsupervised Models</p>
    <p>I lstm: standard LSTM language model</p>
    <p>I tdlm: a topically driven language model; language model is driven by a topic vector automatically learnt on the document context.</p>
    <p>I 4 variants at test time: I Use only the sentence as input: lstm and tdlm; I Use both sentence and context as input: lstm+ and tdlm+.</p>
    <p>I lstm+ incorporates context by feeding it to the LSTM network and taking the final state as the initial state for the current sentence.</p>
    <p>I Models trained on 100K English Wikipedia articles (40M tokens).</p>
  </div>
  <div class="page">
    <p>Acceptability Measures</p>
    <p>To map sentence probability to acceptability, we compute several acceptability measures, which are designed to normalise sentence length and word frequency.</p>
    <p>SLOR = log P  log U</p>
    <p>L</p>
    <p>I P = probability of the sentence given by a model;</p>
    <p>I U = unigram probability of the sentence;</p>
    <p>I L = sentence length</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>lstm lstm+ tdlm tdlm+</p>
    <p>h 0.584 0.633 0.640 0.653 h+ 0.503 0.546 0.557 0.568</p>
    <p>I Across all models (lstm or tdlm) and human ratings (h or h+), using context at test time improves performance.</p>
    <p>I tdlm consistently outperforms lstm (even tdlm &gt; lstm+).</p>
    <p>I Lower correlation when predicting sentence acceptability judged with context.</p>
    <p>I It suggests h+ ratings are more difficult to predict than h, which corresponds to the low one-vs-rest human performance.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>I Context positively influences acceptability, particularly for ill-formed sentences.</p>
    <p>I But it also has the reverse effect for well-formed sentences.</p>
    <p>I Incorporating context (during training or testing) helps modelling acceptability.</p>
    <p>I Prediction performance declines when tested on acceptability ratings judged with context, due to the compression effect of ratings.</p>
    <p>I Future work: investigate why context reduces acceptability for highly acceptable sentences.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
