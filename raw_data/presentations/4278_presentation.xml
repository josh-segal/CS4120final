<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Conclusions</p>
    <p>Data Augmentation for Low-Resource Neural Machine Translation</p>
    <p>Summary</p>
    <p>Marzieh Fadaee Arianna Bisazza Christof Monz Informatics Institute, University of Amsterdam</p>
    <p>Approach: Translation Data Augmentation (TDA)</p>
    <p>Rare Translation Generation (DEEN)</p>
    <p>NMT Results (BLEU)</p>
    <p>Data Augmentation</p>
    <p>Words in Words in Words in</p>
    <p>generated during translation not generated during translation affected by augmentation</p>
    <p>Vrare \ Vreference Vrare \ Vreference Vrare \ Vreference</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>,</p>
    <p>The generated translation length to reference length ratio is on average 7% higher</p>
    <p>Model testset2014 testset2015 testset2016</p>
    <p>TDAr=1 TDAr1</p>
    <p>Baseline Back-trans1:1</p>
    <p>Back-trans1:1</p>
    <p>Altering only one word per sentence</p>
    <p>Closest work is back-translation of monolingual data (Sennrich et al. ACL 2016)</p>
    <p>Our approach focuses on non meaning-preserving augmentation</p>
    <p>that is compatible with the contextChoosing the best</p>
    <p>translation of s0i</p>
    <p>LS TM</p>
    <p>LS TM</p>
    <p>I had been told that you would voluntarily be speaking today. mir wurde signalisiert, sie wrden heute freiwillig sprechen.</p>
    <p>New sentence pair:</p>
    <p>Altering one or multiple words per sentence</p>
    <p>TDA significantly improves translation quality Substituting several rare words is preferable even though the augmented sentences are likely to be noisier</p>
    <p>DEEN</p>
    <p>I had been told that you would not be speaking today .</p>
    <p>mir wurde signalisiert , sie wrden heute nicht sprechen .</p>
    <p>Model testset2014 testset2015 testset2016</p>
    <p>Baseline</p>
    <p>ENDE</p>
    <p>testset 2014</p>
    <p>testset 2015</p>
    <p>testset 2016</p>
    <p>Baseline</p>
    <p>Baseline</p>
    <p>Baseline</p>
    <p>TDAr1</p>
    <p>TDAr1</p>
    <p>TDAr1</p>
    <p>freiwillig</p>
    <p>t0 j</p>
    <p>= arg max</p>
    <p>t2trans(s0i) P lex</p>
    <p>(t|s0 i</p>
    <p>)P lexinv</p>
    <p>(s0 i</p>
    <p>|t)P LMT</p>
    <p>(t|tj11 )</p>
    <p>LM probability threshold</p>
    <p>voluntarily</p>
    <p>s0i --</p>
    <p>ungefragt freiwillig</p>
    <p>trans(s0i)</p>
    <p>Image Processing</p>
    <p>Has not been done in Natural Language Processing</p>
    <p>One possible approach is paraphrasing which is meaningpreserving</p>
    <p>Flipping, cropping, tilting, altering the RGB channels Neural Machine Translation models perform best when an abundance of parallel data is available Acquiring human translations for low-resource language pairs is costly</p>
    <p>Our approach</p>
    <p>As a result training with the augmented bitext achieves significant BLEU improvements in a simulated low-resource EnglishGerman translation setting</p>
    <p>alters existing parallel sentences targeting low-frequency words augments the data by generating new diverse context for low-frequency words and the corresponding translations</p>
    <p>We present a data augmentation technique to enrich the training data targeting rare words</p>
    <p>Increasing the size of the training data by diversifying the context of rare words yields better translations</p>
    <p>Generation of correct rare words during translation increases</p>
    <p>The attention scores of rare words are on average 8.8% higher than the baseline model</p>
    <p>Hence translation of low-frequency words is difficult and often inaccurate</p>
    <p>t 2 trans(s0i)</p>
    <p>Vrare</p>
    <p>Data 371K 731K 4.5M</p>
    <p>Data</p>
    <p>Simulated low-resource MT setting</p>
    <p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 639.021.646</p>
  </div>
</Presentation>
