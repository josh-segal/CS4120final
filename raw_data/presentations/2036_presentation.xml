<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Non-Data-Communication Overheads in MPI:</p>
    <p>Analysis on Blue Gene/P</p>
    <p>P. Balaji, A. Chan, W. Gropp, R. Thakur, E. Lusk</p>
    <p>Argonne National Laboratory University of Chicago</p>
    <p>University of Illinois, Urbana Champaign</p>
  </div>
  <div class="page">
    <p>Ultra-scale High-end Computing</p>
    <p>Processor speeds no longer doubling every 18-24 months  High-end Computing systems growing in parallelism</p>
    <p>Energy usage and heat dissipation are major issues now  Energy usage is proportional to V2F  Lots of slow cores use lesser energy than one fast core</p>
    <p>Consequence:  HEC systems rely less on the performance of a single core  Instead, extract parallelism out of a massive number of low</p>
    <p>frequency/low-power cores  E.g., IBM Blue Gene/L, IBM Blue Gene/P, SiCortex</p>
  </div>
  <div class="page">
    <p>IBM Blue Gene/P System  Second Generation of the Blue Gene supercomputers  Extremely energy efficient design using low-power chips</p>
    <p>Four 850MHz cores on each PPC450 processor</p>
    <p>Connected using five specialized networks  Two of them (10G and 1G Ethernet) are used for File I/O and</p>
    <p>system management  Remaining three (3D Torus, Global Collective network,</p>
    <p>Global Interrupt network) are used for MPI communication  Point-to-point communication goes through the torus network  Each node has six outgoing links at 425 MBps (total of 5.1</p>
    <p>GBps)</p>
  </div>
  <div class="page">
    <p>Blue Gene/P Software Stack  Three Software Stack Layers:</p>
    <p>System Programming Interface (SPI)  Directly above the hardware  Most efficient, but very difficult to program and not portable !</p>
    <p>Deep Computing Messaging Framework (DCMF)  Portability layer built on top of SPI  Generalized message passing framework  Allows different stacks to be built on top</p>
    <p>MPI  Built on top of DCMF  Most portable of the three layers  Based off of MPICH2 (integrated into MPICH2 as of 1.1a1)</p>
  </div>
  <div class="page">
    <p>Issues with Scaling MPI on the BG/P</p>
    <p>Large scale systems such as BG/P provide the capacity needed for achieving a Petaflop or higher performance</p>
    <p>This system capacity has to be translated to capability for end users</p>
    <p>Depends on MPIs ability to scale to large number of cores  Pre- and post-data-communication processing in MPI</p>
    <p>Simple computations can be expensive on modestly fast 850 MHz CPUs</p>
    <p>Algorithmic Issues  Consider an O(N) algorithm with a small proportionality constant  Acceptable on 100 processors; Brutal on 100,000 processors</p>
  </div>
  <div class="page">
    <p>MPI Internal Processing Overheads</p>
    <p>Application</p>
    <p>MPI</p>
    <p>Application</p>
    <p>MPI</p>
    <p>Pre- and Post-datacommunication overheads</p>
    <p>Application</p>
    <p>MPI</p>
    <p>Application</p>
    <p>MPI</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction</p>
    <p>Issues with Scaling MPI on Blue Gene/P</p>
    <p>Experimental Evaluation</p>
    <p>MPI Stack Computation Overhead</p>
    <p>Algorithmic Inefficiencies</p>
    <p>Concluding Remarks</p>
  </div>
  <div class="page">
    <p>Basic MPI Stack Overhead</p>
    <p>Application</p>
    <p>MPI</p>
    <p>Application</p>
    <p>DCMF DCMF</p>
    <p>MPI</p>
    <p>Application Application</p>
    <p>DCMF DCMF</p>
  </div>
  <div class="page">
    <p>Basic MPI Stack Overhead (Results)</p>
    <p>DCMF</p>
    <p>MPI</p>
    <p>Message size (bytes)</p>
    <p>L a</p>
    <p>te nc</p>
    <p>y (u</p>
    <p>s)</p>
    <p>DCMF</p>
    <p>MPI</p>
    <p>Message size (bytes)</p>
    <p>B a</p>
    <p>nd w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b p</p>
    <p>s)</p>
  </div>
  <div class="page">
    <p>Request Allocation and Queuing</p>
    <p>Blocking vs. Non-blocking point-to-point communication  Blocking: MPI_Send() and MPI_Recv()  Non-blocking: MPI_Isend(), MPI_Irecv() and MPI_Waitall()</p>
    <p>Non-blocking communication potentially allows for better overlap of computation with communication, but  requires allocation, initialization and queuing/de-queuing of</p>
    <p>MPI_Request handles</p>
    <p>What are we measuring?  Latency test using MPI_Send() and MPI_Recv()  Latency test using MPI_Irecv(), MPI_Isend() and MPI_Waitall()</p>
  </div>
  <div class="page">
    <p>Request Allocation and Queuing Overhead</p>
    <p>Blocking</p>
    <p>Non-blocking</p>
    <p>Message size (bytes)</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
    <p>Message size (bytes)</p>
    <p>% O</p>
    <p>ve rh</p>
    <p>e a d</p>
  </div>
  <div class="page">
    <p>Derived Datatype Processing</p>
    <p>MPI Buffers</p>
  </div>
  <div class="page">
    <p>Overheads in Derived Datatype Processing</p>
    <p>Contiguous</p>
    <p>Vector-Char</p>
    <p>Vector-Short</p>
    <p>Vector-Int</p>
    <p>Vector-Double</p>
    <p>Message size (bytes)</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
    <p>Derived Datatype Latency (Short Messages)</p>
    <p>Contiguous</p>
    <p>Vector-Char</p>
    <p>Vector-Short</p>
    <p>Vector-Int</p>
    <p>Vector-Double</p>
    <p>Message size (bytes)</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
  </div>
  <div class="page">
    <p>Copies with Unaligned Buffers</p>
    <p>For 4-byte integer copies:  Buffer alignments of 0-4 means that the entire integer is in</p>
    <p>the same double word  to access an integer, you only need to fetch one double word</p>
    <p>Buffer alignments of 5-7 means that the integer spans two double word boundaries  to access an integer, you need to fetch two double words</p>
    <p>Double Word</p>
    <p>Integer Integer Integer</p>
  </div>
  <div class="page">
    <p>Buffer Alignment Overhead</p>
    <p>Byte alignment</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
    <p>Buffer Alignment Overhead (without 32Kbytes)</p>
    <p>Byte alignment</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
  </div>
  <div class="page">
    <p>Thread Communication</p>
    <p>Multiple threads calling MPI can corrupt the stack  MPI uses locks to serialize access to the stack</p>
    <p>Current locks are coarse grained  protect the entire MPI call  Implies these locks serialize communication for all threads</p>
    <p>MPI</p>
    <p>MPI MPI</p>
    <p>MPI MPI</p>
    <p>Four MPI processes Four threads in one</p>
    <p>MPI process</p>
  </div>
  <div class="page">
    <p>Overhead of Thread Communication</p>
    <p>Threads</p>
    <p>Processes</p>
    <p>Number of Cores</p>
    <p>M e</p>
    <p>s s</p>
    <p>a g</p>
    <p>e R</p>
    <p>a te</p>
    <p>( M</p>
    <p>M P</p>
    <p>S )</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction</p>
    <p>Issues with Scaling MPI on Blue Gene/P</p>
    <p>Experimental Evaluation</p>
    <p>MPI Stack Computation Overhead</p>
    <p>Algorithmic Inefficiencies</p>
    <p>Concluding Remarks</p>
  </div>
  <div class="page">
    <p>Tag and Source Matching</p>
    <p>Search time in most implementations is linear with respect to the number of requests posted</p>
    <p>Source = 1 Tag = 1</p>
    <p>Source = 1 Tag = 2</p>
    <p>Source = 2 Tag = 1</p>
    <p>Source = 0 Tag = 0</p>
  </div>
  <div class="page">
    <p>Overheads in Tag and Source Matching</p>
    <p>Tag Matching Overhead vs. Number of Requests</p>
    <p>Number of Requests</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
    <p>Number of peers</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
  </div>
  <div class="page">
    <p>Unexpected Message Overhead</p>
    <p>Unexpected Message Overhead vs. Number of Requests</p>
    <p>Number of Unexpected Requests</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
    <p>Unexpected Message Overhead vs. Peers</p>
    <p>Number of peers</p>
    <p>L a te</p>
    <p>n c y</p>
    <p>(u s )</p>
  </div>
  <div class="page">
    <p>Multi-Request Operations</p>
    <p>Waitany Time</p>
    <p>Number of requests</p>
    <p>T im</p>
    <p>e (</p>
    <p>u s )</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction</p>
    <p>Issues with Scaling MPI on Blue Gene/P</p>
    <p>Experimental Evaluation</p>
    <p>MPI Stack Computation Overhead</p>
    <p>Algorithmic Inefficiencies</p>
    <p>Concluding Remarks</p>
  </div>
  <div class="page">
    <p>Concluding Remarks</p>
    <p>Systems such as BG/P provide the capacity needed for achieving a Petaflop or higher performance</p>
    <p>System capacity has to be translated to end-user capability  Depends on MPIs ability to scale to large number of cores</p>
    <p>We studied the non-data-communication overheads in MPI on BG/P  Identified several bottleneck possibilities within MPI  Stressed these bottlenecks with benchmarks  Analyzed the reasons behind such overheads</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Contact:</p>
    <p>Pavan Balaji: balaji@mcs.anl.gov Anthony Chan: chan@mcs.anl.gov</p>
    <p>William Gropp: wgropp@illinois.edu</p>
    <p>Rajeev Thakur: thakur@mcs.anl.gov Rusty Lusk: lusk@mcs.anl.gov</p>
    <p>Project Website: http://www.mcs.anl.gov/research/projects/mpich2</p>
  </div>
</Presentation>
