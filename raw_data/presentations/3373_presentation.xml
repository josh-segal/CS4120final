<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Detecting Concept Drift in Malware Classification Models</p>
    <p>Roberto Jordaney+, Kumar Sharad, Santanu Kumar Dash, Zhi Wang, Davide Papini,</p>
    <p>Ilia Nouretdinov+, Lorenzo Cavallaro+</p>
    <p>USENIX Security Symposium</p>
    <p>Thu Aug 17, 2017</p>
    <p>+Royal Holloway, University of London NEC Laboratories Europe University College London Nankai University Elettronica S.p.A.</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification</p>
    <p>Usually, a 2-phase process:</p>
    <p>labeled objects</p>
    <p>of unknown objects</p>
    <p>Objects are described as vectors of features</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification</p>
    <p>Usually, a 2-phase process:</p>
    <p>labeled objects</p>
    <p>of unknown objects</p>
    <p>Objects are described as vectors of features</p>
    <p>Pikachu</p>
    <p>Charmander</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification</p>
    <p>Usually, a 2-phase process:</p>
    <p>labeled objects</p>
    <p>of unknown objects</p>
    <p>Objects are described as vectors of features</p>
    <p>Pikachu</p>
    <p>Charmander</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification Problem: Concept Drift</p>
    <p>Concept drift is the change in the statistical properties of an object in</p>
    <p>unforeseen ways</p>
    <p>Drifted objects will likely be wrongly classified</p>
    <p>Hitmonlee (new pokmon family)</p>
    <p>Raichu (evolution of Pikachu)</p>
    <p>Charmeleon (evolution of Charmander)</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification Problem: Concept Drift</p>
    <p>Concept drift is the change in the statistical properties of an object in</p>
    <p>unforeseen ways</p>
    <p>Drifted objects will likely be wrongly classified</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification Problem: Concept Drift</p>
    <p>Concept drift is the change in the statistical properties of an object in</p>
    <p>unforeseen ways</p>
    <p>Drifted objects will likely be wrongly classified</p>
    <p>Of course, the problem exists in multiclass classification settings. . .</p>
  </div>
  <div class="page">
    <p>Machine Learning Classification Problem: Concept Drift</p>
    <p>Multiclass classification is a generalization of the binary case</p>
  </div>
  <div class="page">
    <p>Concept Drift</p>
    <p>In non-stationary contexts classifiers will suffer from concept drift due to:  malware evolution ,</p>
    <p>new malware families</p>
    <p>Need a way to assess the predictions of classifiers  Ideally classifier-agnostic assessments</p>
    <p>Need to identify objects that fit a model and those drifting away</p>
    <p>Our Contributions</p>
    <p>Conformal Evaluator: statistical evaluation of ML classifiers  Per-class quality threshold to identify reliable and unreliable predictions</p>
  </div>
  <div class="page">
    <p>Concept Drift</p>
    <p>In non-stationary contexts classifiers will suffer from concept drift due to:  malware evolution ,</p>
    <p>new malware families</p>
    <p>Need a way to assess the predictions of classifiers  Ideally classifier-agnostic assessments</p>
    <p>Need to identify objects that fit a model and those drifting away</p>
    <p>Our Contributions</p>
    <p>Conformal Evaluator: statistical evaluation of ML classifiers  Per-class quality threshold to identify reliable and unreliable predictions</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator</p>
    <p>Assesses decisions made by a classifier  Mark each decision as reliable or unreliable</p>
    <p>Builds and makes use of p-value as assessment criteria  Computes per-class thresholds to divide reliable decisions from unreliable ones</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value?</p>
    <p>Used to measure how well a sample fits into a single class  Conformal Evaluator computes a p-value for each class, for each test element</p>
    <p>Definition</p>
    <p>t = Non-conformity score for test element t</p>
    <p>i K,i = Non-conformity score for train element i</p>
    <p>p-value = |{i : i  t}|</p>
    <p>|K| K = Total number of element</p>
    <p>P-value</p>
    <p>Ratio between the number of training elements that are more dissimilar than the</p>
    <p>element under test</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>ML classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>Machine learning classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>Machine learning classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: P-value Example</p>
    <p>Machine learning classifier:</p>
    <p>distance from centroid</p>
    <p>dissimilar than the one under test?</p>
    <p>Lets see how p-values are used within Conformal Evaluator.</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: How Does it Work?</p>
    <p>non-conformity requirement)</p>
    <p>Training dataset</p>
    <p>Decision algorithm Non-conformity measure Threshold analysis</p>
    <p>Threshold for class A</p>
    <p>Threshold for class B . . . . .</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: How Does it Work?</p>
    <p>Training dataset</p>
    <p>Decision algorithm Non-conformity measure Threshold analysis</p>
    <p>Threshold for class A</p>
    <p>Threshold for class B . . . . .</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: How Does it Work?</p>
    <p>Training dataset</p>
    <p>Decision algorithm Non-conformity measure Threshold analysis</p>
    <p>Threshold for class A</p>
    <p>Threshold for class B . . . . .</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: Identifying per-class Thresholds</p>
    <p>Customizable constraints:</p>
    <p>Desired performance (of the predictions marked as reliable)  E.g.: high-level performance will raise the threshold</p>
    <p>Number of unreliable prediction tolerated  E.g.: low number of unreliable prediction will lower the threshold</p>
    <p>Assumptions</p>
    <p>Performance of non-drifted elements are similar to the one declared by the algorithm  Predictions with high confidence will have higher p-values</p>
  </div>
  <div class="page">
    <p>Conformal Evaluator: Identifying per-class Thresholds</p>
    <p>We use the p-values and prediction labels from training samples  From the thresholds that satisfy the constraints we chose the one that maximize</p>
    <p>one or the other</p>
    <p>P-value</p>
    <p>Identified Threshold</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
  </div>
  <div class="page">
    <p>Experimental Results: Case Studies</p>
    <p>Binary case study: Android malware detection algorithm  Reimplemented Drebin1 algorithm with similar results</p>
    <p>(0.95-0.92 precision-recall on malicious apps and 0.99-0.99 precision-recall on benign apps)</p>
    <p>Static features of Android apps, linear SVM (used as NCM)  Concept drift scenario: malware evolution</p>
    <p>Multiclass case study: Microsoft malware classification algorithm  Solution to Microsoft Kaggle competition2, ranked among the top ones  Static features from Windows PE binaries, Random Forest (used as NCM)  Concept drift scenario: family discovery</p>
    <p>Malware in Your Pocket. In 21st Annual Network and Distributed System Security Symposium (NDSS), San Diego, California, USA, February 23-26,</p>
    <p>KAGGLE INC. Microsoft Malware Classification Challenge (BIG 2015). https://www.kaggle.com/c/malware-classification, 2015.</p>
  </div>
  <div class="page">
    <p>Experimental Results: Case Studies</p>
    <p>Binary case study: Android malware detection algorithm  Reimplemented Drebin1 algorithm with similar results</p>
    <p>(0.95-0.92 precision-recall on malicious apps and 0.99-0.99 precision-recall on benign apps)</p>
    <p>Static features of Android apps, linear SVM (used as NCM)  Concept drift scenario: malware evolution</p>
    <p>Multiclass case study: Microsoft malware classification algorithm  Solution to Microsoft Kaggle competition2, ranked among the top ones  Static features from Windows PE binaries, Random Forest (used as NCM)  Concept drift scenario: family discovery</p>
    <p>Malware in Your Pocket. In 21st Annual Network and Distributed System Security Symposium (NDSS), San Diego, California, USA, February 23-26,</p>
    <p>KAGGLE INC. Microsoft Malware Classification Challenge (BIG 2015). https://www.kaggle.com/c/malware-classification, 2015.</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Drebin dataset: samples collected from 2010 to 2012  Marvin dataset3: malware apps collected from 2010 to 2014 (no duplicates)</p>
    <p>We expect some object to drift from objects in the Drebin dataset</p>
    <p>Drebin Dataset</p>
    <p>Type Samples</p>
    <p>Benign 123,435</p>
    <p>Malware 5,560</p>
    <p>Marvin Dataset</p>
    <p>Type Samples</p>
    <p>Benign 9,592</p>
    <p>Malware 9,179</p>
    <p>Static And Dynamic Analysis. In 39th IEEE Annual Computer Software and Applications Conference (COMPSAC), Taichung, Taiwan, July 1-5, 2015.</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Drift Confirmation</p>
    <p>Training dataset: Drebin dataset</p>
    <p>Testing dataset: 4,500 benign and 4,500 malicious random samples from Marvin dataset</p>
    <p>Prediction label</p>
    <p>Original label Benign Malicious Recall</p>
    <p>Benign 4,498 2 1</p>
    <p>Malicious 2,890 1,610 0.36</p>
    <p>Precision 0.61 1</p>
    <p>Marvin malicious app Drebin malicious app</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Drift Confirmation</p>
    <p>Training dataset: Drebin dataset</p>
    <p>Testing dataset: 4,500 benign and 4,500 malicious random samples from Marvin dataset</p>
    <p>Prediction label</p>
    <p>Original label Benign Malicious Recall</p>
    <p>Benign 4,498 2 1</p>
    <p>Malicious 2,890 1,610 0.36</p>
    <p>Precision 0.61 1</p>
    <p>Marvin malicious app Drebin malicious app</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Drift Confirmation</p>
    <p>Training dataset: Drebin dataset</p>
    <p>Testing dataset: 4,500 benign and 4,500 malicious random samples from Marvin dataset</p>
    <p>Prediction label</p>
    <p>Original label Benign Malicious Recall</p>
    <p>Benign 4,498 2 1</p>
    <p>Malicious 2,890 1,610 0.36</p>
    <p>Precision 0.61 1</p>
    <p>Marvin malicious app Drebin malicious app</p>
    <p>Drebin benign app</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Threshold Identification</p>
    <p>Training dataset: Drebin dataset  Testing dataset: 4,500 benign and 4,500 malicious random samples from Marvin dataset  Make use of Conformal Evaluators prediction assessment algorithm</p>
    <p>Constraints: F1-score of 0.99 and 0.76 of elements marked as reliable</p>
    <p>Prediction label</p>
    <p>Original label Benign Malicious Recall</p>
    <p>Benign 4,257 2 1</p>
    <p>Malicious 504 1,610 0.76</p>
    <p>Precision 0.89 1</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Retraining</p>
    <p>Training dataset: Drebin dataset + samples marked as unreliable from previous experiment  Testing dataset: 4,500 benign and 4,500 malicious random samples of Marvin dataset</p>
    <p>(no sample overlap from previous experiment)</p>
    <p>Assigned label</p>
    <p>Sample Benign Malicious Recall</p>
    <p>Benign 4,413 87 0.98</p>
    <p>Malicious 255 4,245 0.94</p>
    <p>Precision 0.96 0.98</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Threshold Comparison</p>
    <p>Compare probability- and p-value-based thresholds  Central tendency and dispersion points of true positive distribution</p>
    <p>Training dataset: Drebin dataset  Testing dataset: 4,500 benign and 4,500 malicious apps from Marvin dataset (random sampling)</p>
    <p>TPR TPR FPR FPR</p>
    <p>(reliable predictions) (unreliable predictions) (reliable predictions) (unreliable predictions)</p>
    <p>p-value probability p-value probability p-value probability p-value probability</p>
    <p>Median 0.8737 0.8061 0.3080 0.3300 0.0000 0.0 0.0008 0.0008</p>
    <p>Mean 0.8737 0.4352 0.3080 0.3433 0.0000 0.0 0.0008 0.0018</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Threshold Comparison</p>
    <p>Compare probability- and p-value-based thresholds  Central tendency and dispersion points of true positive distribution</p>
    <p>Training dataset: Drebin dataset  Testing dataset: 4,500 benign and 4,500 malicious apps from Marvin dataset (random sampling)</p>
    <p>TPR TPR FPR FPR</p>
    <p>(reliable predictions) (unreliable predictions) (reliable predictions) (unreliable predictions)</p>
    <p>p-value probability p-value probability p-value probability p-value probability</p>
    <p>Median 0.8737 0.8061 0.3080 0.3300 0.0000 0.0 0.0008 0.0008</p>
    <p>Mean 0.8737 0.4352 0.3080 0.3433 0.0000 0.0 0.0008 0.0018</p>
  </div>
  <div class="page">
    <p>Experimental Results: Binary Classification (Malware Evolution)</p>
    <p>Experiment: Threshold Comparison</p>
    <p>Compare probability- and p-value-based thresholds  Central tendency and dispersion points of true positive distribution</p>
    <p>Training dataset: Drebin dataset  Testing dataset: 4,500 benign and 4,500 malicious apps from Marvin dataset (random sampling)</p>
    <p>TPR TPR FPR FPR</p>
    <p>(reliable predictions) (unreliable predictions) (reliable predictions) (unreliable predictions)</p>
    <p>p-value probability p-value probability p-value probability p-value probability</p>
    <p>Median 0.8737 0.8061 0.3080 0.3300 0.0000 0.0 0.0008 0.0008</p>
    <p>Mean 0.8737 0.4352 0.3080 0.3433 0.0000 0.0 0.0008 0.0018</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Conformal Evaluator (CE)</p>
    <p>Statistical evaluation to assess predictions of ML classifiers and identify concept drift</p>
    <p>Algorithm Agnostic: Uses non-conformity measure (NCM) from the ML classifier</p>
    <p>Statistical Support: Builds p-values from NCM to statistically-support predictions</p>
    <p>Quality Thresholds: Builds thresholds from p-values to identify unreliable predictions</p>
    <p>We evaluate the proposed solution on different ML classifiers and case studies  Android malware apps in binary classification settings  Windows PE binaries in multi-class classification settings</p>
    <p>Information on CEs python code and dataset availability at:</p>
    <p>https://s2lab.isg.rhul.ac.uk/projects/ce</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Conformal Evaluator (CE)</p>
    <p>Statistical evaluation to assess predictions of ML classifiers and identify concept drift</p>
    <p>Algorithm Agnostic: Uses non-conformity measure (NCM) from the ML classifier</p>
    <p>Statistical Support: Builds p-values from NCM to statistically-support predictions</p>
    <p>Quality Thresholds: Builds thresholds from p-values to identify unreliable predictions</p>
    <p>We evaluate the proposed solution on different ML classifiers and case studies  Android malware apps in binary classification settings  Windows PE binaries in multi-class classification settings</p>
    <p>Information on CEs python code and dataset availability at:</p>
    <p>https://s2lab.isg.rhul.ac.uk/projects/ce</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Conformal Evaluator (CE)</p>
    <p>Statistical evaluation to assess predictions of ML classifiers and identify concept drift</p>
    <p>Algorithm Agnostic: Uses non-conformity measure (NCM) from the ML classifier</p>
    <p>Statistical Support: Builds p-values from NCM to statistically-support predictions</p>
    <p>Quality Thresholds: Builds thresholds from p-values to identify unreliable predictions</p>
    <p>We evaluate the proposed solution on different ML classifiers and case studies  Android malware apps in binary classification settings  Windows PE binaries in multi-class classification settings</p>
    <p>Information on CEs python code and dataset availability at:</p>
    <p>https://s2lab.isg.rhul.ac.uk/projects/ce</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Binary Classification Case Study: Comparison with Probability</p>
    <p>TPR FPR TPR FPR MALICIOUS BENIGN</p>
    <p>of kept elements of kept elements of discarded elements of discarded elements kept elements kept elements</p>
    <p>p-value probability p-value probability p-value probability p-value probability p-value probability p-value probability</p>
    <p>Median 0.8737 0.8061 0.0000 0.0 0.3080 0.3300 0.0008 0.0008 0.0880 0.0584 0.4136 0.4304</p>
    <p>Mean 0.8737 0.4352 0.0000 0.0 0.3080 0.3433 0.0008 0.0018 0.0880 0.1578 0.4136 0.7513</p>
    <p>Table 4: Thresholds comparison between p-value and probability. The results show,</p>
    <p>together with the performance of the sample marked as unreliable, a clear advantage</p>
    <p>of the p-value metric compared to the probability one.</p>
  </div>
  <div class="page">
    <p>P-value vs Probability: situation 1</p>
    <p>P-value Probability</p>
    <p>Red 0.0 0.5</p>
    <p>Green 0.0 0.5</p>
  </div>
  <div class="page">
    <p>P-value vs Probability: situation 2</p>
    <p>P-value Probability</p>
    <p>Red 0.5 0.5</p>
    <p>Green 0.5 0.5</p>
  </div>
  <div class="page">
    <p>Experimental Results: Multiclass classification (new family discovery)</p>
    <p>Dataset: Microsoft Malware Classification Challenge (2015)</p>
    <p>Microsoft Malware Classification Challenge Dataset</p>
    <p>Malware Samples Malware Samples</p>
    <p>Ramnit 1 541 Obfuscator.ACY 1 228</p>
    <p>Lollipop 2 478 Gatak 1 013</p>
    <p>Kelihos ver3 2 942 Kelihos ver1 398</p>
    <p>Vundo 4 75 Tracur 751</p>
  </div>
  <div class="page">
    <p>Experimental Results: Multiclass classification (new family discovery)</p>
    <p>Experiment: Family Discovery</p>
    <p>Training families: Ramnit, Lollipop, Kelihos ver3, Vundo, Obfuscator.ACY, Gatak, Kelihos ver1  Testing family: Tracur</p>
    <p>Classification results:</p>
    <p>Lollipop Kelihos ver3 Vundo Kelihos ver1 Obfuscator.ACY</p>
  </div>
  <div class="page">
    <p>Experimental Results: Multiclass classification (new family discovery)</p>
    <p>P-value distribution for samples of Tracur family; as expected, the values are all close to zero.</p>
    <p>Prediction: Ramnit</p>
    <p>Prediction: Lollipop</p>
    <p>Prediction: Kelihos_ver3</p>
    <p>Prediction: Vundo</p>
    <p>Prediction: Kelihos_ver1</p>
    <p>Prediction: Obfuscator.ACY</p>
    <p>Prediction: Gatak</p>
    <p>Pva</p>
    <p>lu es</p>
    <p>P-values: Ramnit P-values: Lollipop P-values: Kelihos_ver3</p>
    <p>P-values: Vundo P-values: Kelihos_ver1</p>
    <p>P-values: Obfuscator.ACY P-values: Gatak</p>
  </div>
  <div class="page">
    <p>Experimental Results: Multiclass classification (new family discovery)</p>
    <p>Probability distribution for samples of Tracur family; bounded to sum to one, the</p>
    <p>values are different than zero.</p>
    <p>Prediction: Ramnit</p>
    <p>Prediction: Lollipop</p>
    <p>Prediction: Kelihos_ver3</p>
    <p>Prediction: Vundo</p>
    <p>Prediction: Kelihos_ver1</p>
    <p>Prediction: Obfuscator.ACY</p>
    <p>Prediction: Gatak</p>
    <p>P ro</p>
    <p>b a b il it</p>
    <p>ie s</p>
    <p>Probabilities: Ramnit</p>
    <p>Probabilities: Lollipop</p>
    <p>Probabilities: Kelihos_ver3</p>
    <p>Probabilities: Vundo</p>
    <p>Probabilities: Kelihos_ver1</p>
    <p>Probabilities: Obfuscator.ACY</p>
    <p>Probabilities: Gatak</p>
  </div>
</Presentation>
