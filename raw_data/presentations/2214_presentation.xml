<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>HotStorage18</p>
    <p>BIBIM: A Prototype Multi-Partition Aware Heterogeneous New Memory</p>
    <p>Gyuyoung Park 1 , Miryeong Kwon1, Pratyush Mahapatra</p>
    <p>Michael Swift 2 , and Myoungsoo Jung1</p>
    <p>Yonsei University Computer Architecture and MEmory Systems Lab</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Increasing Demands for Bigger Memory  More cores require more memory capacity</p>
    <p>However, there is memory capacity gap  Core count doubling ~ every 2 years  Memory capacity doubling ~ every 3 years</p>
    <p>Moreover, there are many data-intensive applications Ex) Memory caching (MEMCACHED, Redis), In-memory database (DynamoDB, HANA)</p>
    <p>Year</p>
    <p>R e</p>
    <p>la ti</p>
    <p>ve c</p>
    <p>a p</p>
    <p>a ci</p>
    <p>ty</p>
    <p>Core</p>
    <p>Memory (DRAM)</p>
    <p>Need a bigger and larger memory!!</p>
    <p>Source: Lim et al, ISCA09</p>
  </div>
  <div class="page">
    <p>Comparison of Technology Scaling</p>
    <p>Time &amp; Technology node</p>
    <p>Cost Reduction</p>
    <p>Rate</p>
    <p>DRAM</p>
    <p>NAND</p>
    <p>New Memory</p>
    <p>Managed DRAM</p>
    <p>Scaling limitation</p>
    <p>DRAM</p>
    <p>NAND</p>
    <p>New Memory (PRAM, STT-MRAM, ReRAM)</p>
    <p>Suitable for next memory device</p>
    <p>Source: SK Hynix</p>
  </div>
  <div class="page">
    <p>W ri</p>
    <p>te t</p>
    <p>im e</p>
    <p>( s)</p>
    <p>Comparison of Programming Latency</p>
    <p>Courtesy: Motoyuki Ooishi Memory capacity (bit)</p>
    <p>SRAM</p>
    <p>DRAM</p>
    <p>Volatile RAM</p>
    <p>NOR Flash memory</p>
    <p>NAND Flash memory</p>
    <p>NV-RAM w/ erase-before</p>
    <p>write</p>
    <p>PRAM NV-RAM w/o erase-before</p>
    <p>write</p>
    <p>MRAM</p>
    <p>FeRAM Can achieve</p>
    <p>high capacity</p>
    <p>Unfortunately, takes a long time to program</p>
  </div>
  <div class="page">
    <p>Why does PRAM Show Long Write Latency?</p>
    <p>How Significant?</p>
    <p>PRAM exploits the unique behavior of chalcogenide glass (GST). By switching status of GST, it can store data</p>
    <p>PRAM cell</p>
    <p>Top electrode</p>
    <p>Bottom electrode</p>
    <p>225 (GST)</p>
    <p>Heater 0  1 1  0</p>
  </div>
  <div class="page">
    <p>Programming Method of PRAM GST status is changed by heating the material</p>
    <p>Moreover, compared with DRAM, PRAM has asymmetric latency</p>
    <p>Amorphous (Data 0)</p>
    <p>Time</p>
    <p>G ST</p>
    <p>T e</p>
    <p>m p</p>
    <p>e ra</p>
    <p>tu re</p>
    <p>(~300)</p>
    <p>(~600)</p>
    <p>Crystallin (Data 1)</p>
    <p>Long write</p>
    <p>latency</p>
  </div>
  <div class="page">
    <p>General Assumptions of PRAM Latency</p>
    <p>Many previous works assume PRAMs write latency as similar to or slightly worse than DRAM (1.5x)</p>
    <p>HPCA13ISCA09</p>
    <p>HPCA18</p>
    <p>Assumed PRAMDRAM</p>
    <p>W R</p>
    <p>IT E</p>
    <p>l a te</p>
    <p>n c y (</p>
    <p>n s ) 150</p>
  </div>
  <div class="page">
    <p>PRAM Latency Measurement</p>
    <p>Our performance measurement on real 3x nm PRAM exhibits expensive write latency than DRAM (190x)</p>
    <p>Real PRAMDRAM</p>
    <p>W R</p>
    <p>IT E</p>
    <p>l a</p>
    <p>te n c y (</p>
    <p>n s )</p>
  </div>
  <div class="page">
    <p>Then, How Can PRAMs Long Write Latency Be Mitigated?</p>
    <p>Lets borrow the concept of Bibim Bibim means Mix in Korean Mix various ingredients for better taste</p>
  </div>
  <div class="page">
    <p>Hybrid Memory Can Help Us</p>
    <p>Put fast DRAM and slow PRAM together</p>
    <p>PRAMDRAM</p>
    <p>NOTE) Im just used as a write-only inclusive cache of slow PRAM</p>
  </div>
  <div class="page">
    <p>BTW, How Can Hybrid Memory Be Used in Real</p>
    <p>System?</p>
    <p>We design new memory controller, BIBIM, for non-volatile hybrid memory</p>
    <p>BIBIM</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Demo (Track and Field)</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>BIBIM</p>
    <p>Performance comparison (memory</p>
    <p>access with synthetic, readwrite inter-mixed trace)</p>
  </div>
  <div class="page">
    <p>Demo  Slow Version</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>BIBIM</p>
    <p>Host sent 1M memory</p>
    <p>requests</p>
    <p>Memory services</p>
    <p>incoming requests</p>
    <p>Still PRAM servicing memory requests</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>What Are The Considerations To Design Hybrid Memory</p>
    <p>(DRAM+PRAM) Controller?</p>
    <p>To get insights of controller design, lets understand the details of DRAM and PRAM</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>Memory controller</p>
  </div>
  <div class="page">
    <p>DRAMs Multi-bank Architecture</p>
    <p>B a</p>
    <p>n k</p>
    <p>N</p>
    <p>Multiple banks to serve multiple memory requests in parallel  Single row buffer within a bank</p>
    <p>B a</p>
    <p>n k</p>
    <p>R o</p>
    <p>w d</p>
    <p>e co</p>
    <p>d e</p>
    <p>r</p>
    <p>Column decoder</p>
    <p>Row buffer</p>
    <p>B a</p>
    <p>n k</p>
    <p>Row buffer</p>
  </div>
  <div class="page">
    <p>Does PRAM have the Same Internal Architecture with</p>
    <p>DRAM?</p>
    <p>Challenge1: PRAMs write latency is long</p>
    <p>PRAM employs multiple row buffers</p>
    <p>Challenge2: PRAMs asymmetric latency incurs lots of bank conflicts</p>
    <p>PRAM uses multi-partition architecture</p>
  </div>
  <div class="page">
    <p>PRAMs Multi-partition Architecture  Multiple partitions within the bank for partition-level parallelism</p>
    <p>Multiple row buffers to mitigate long write latency</p>
    <p>B a</p>
    <p>n k</p>
    <p>Partition 0</p>
    <p>Partition 1</p>
    <p>Partition 15</p>
    <p>D e</p>
    <p>co d</p>
    <p>e r</p>
    <p>Sense Amp.</p>
    <p>Cell ArrayRAB</p>
    <p>RDB</p>
  </div>
  <div class="page">
    <p>Can A Conventional DRAM Controller Be Aware of MultiPartition? (Inside of A Bank)</p>
    <p>(Revisited) Conventional DRAM scheduler just utilizes bank-level parallelism. Cannot see inside of bank!</p>
    <p>Partition-level parallelism should be supported</p>
  </div>
  <div class="page">
    <p>How Memory Requests Can Be Scheduled By Exploiting</p>
    <p>Multi-Partition Architecture? Limitation of PRAM: In PRAM design, WRITE request blocks whole PRAM bank.</p>
    <p>Partition 0</p>
    <p>Partition 1</p>
    <p>Partition 15</p>
    <p>Decoder</p>
    <p>S e</p>
    <p>n se</p>
    <p>a m</p>
    <p>p .</p>
    <p>B lo</p>
    <p>ck e</p>
    <p>d !</p>
    <p>Request (Write)</p>
  </div>
  <div class="page">
    <p>How Memory Requests Can Be Scheduled By Exploiting</p>
    <p>Multi-Partition Architecture?</p>
    <p>Key insight: Although WRITE cannot be serviced, READs can be serviced if partition number is different</p>
  </div>
  <div class="page">
    <p>Non-Blocking Read Service (NBRS) Solution: Add a register to store partition number of WRITE and compare it with partition number of incoming READ request</p>
    <p>Partition 0 Partition 1</p>
    <p>Partition 15</p>
    <p>Decoder</p>
    <p>S e</p>
    <p>n se</p>
    <p>a m</p>
    <p>p .</p>
    <p>BIBIM controller</p>
    <p>WRITE partition number</p>
    <p>Partition 2</p>
    <p>Same partition READ cant be serviced!</p>
  </div>
  <div class="page">
    <p>BIBIM Design1: Scheduling Support Module for PRAM</p>
    <p>aware New Scheduling Scheme</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
  </div>
  <div class="page">
    <p>Now Requests Are Scheduled. Then, How Then Can It Serve to</p>
    <p>Hybrid Memory?</p>
    <p>Firstly, as is generally known, LPDDR2 is JEDEC standard low-power memory interface (used for DRAM)</p>
  </div>
  <div class="page">
    <p>DRAMs Timing (LPDDR2 by JEDEC) 1) Activation: activate target row &amp; write that data to row buffer 2) Read/Write: accessing row buffer with column address 3) Precharge: charge half-voltage of bit-line</p>
    <p>R o</p>
    <p>w d</p>
    <p>e co</p>
    <p>d e</p>
    <p>r</p>
    <p>Column decoder</p>
    <p>B a</p>
    <p>n k</p>
    <p>a n</p>
    <p>k 1</p>
    <p>B a</p>
    <p>n k</p>
    <p>N</p>
  </div>
  <div class="page">
    <p>Does PRAM have the Same Memory Interface (LPDDR2)</p>
    <p>with DRAM?</p>
    <p>(Revisited) PRAM has a different architecture with DRAM such as Multiple row buffers and More larger capacity</p>
    <p>Different interface is required</p>
  </div>
  <div class="page">
    <p>PRAMs Timing  PRAM requires different timing model from DRAM</p>
    <p>NVM memory space is much larger than a DRAM</p>
    <p>3-Phase addressing (LPDDR2-NVM by JEDEC)</p>
    <p>RAB</p>
    <p>Pre-active</p>
    <p>U p</p>
    <p>p e</p>
    <p>r ro</p>
    <p>w a</p>
    <p>d d</p>
    <p>re ss</p>
    <p>Row Address Buffer (RAB) is selected by the memory controller</p>
    <p>Activate</p>
    <p>Upper Lower</p>
    <p>Lower row address RDB</p>
    <p>PRAM array</p>
    <p>Row Data Buffer (RDB) is also selected by the memory controller</p>
    <p>READ</p>
    <p>Column address</p>
    <p>Data out</p>
  </div>
  <div class="page">
    <p>BIBIM Design2: Heterogeneity Support Module for both LPDDR2 &amp; LPDDR2-NVM</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Our own new physical layer (400MHz) is</p>
    <p>implemented</p>
  </div>
  <div class="page">
    <p>Dont Forget DRAM is For Cache. Then, How Caching Can Be</p>
    <p>Supported?</p>
    <p>Solution: Keep which data exist in DRAM (caching info) in lookup table.</p>
    <p>Moreover, like conventional cache, controller should have algorithms such as DRAM dataline update, eviction, and find empty dataline.</p>
  </div>
  <div class="page">
    <p>Hardware Support for Lookup Table Lookup table do not include data value, includes address information 0</p>
    <p>g in</p>
    <p>d e</p>
    <p>x o</p>
    <p>ff se</p>
    <p>t0</p>
    <p>way2 way3 way4count valid tag</p>
    <p>= = = =</p>
    <p>Mux DRAM hit/miss # way</p>
    <p>With multiple comparators, 4 ways can be</p>
    <p>parallelized</p>
    <p>Only 512KB BRAM is used</p>
    <p>for lookup table</p>
  </div>
  <div class="page">
    <p>BIBIM Design3: Caching Support Module for Use DRAM As Inclusive Cache of PRAM</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Caching Support Module</p>
  </div>
  <div class="page">
    <p>BTW, How Non-Volatility of PRAM Can Be Maintained Although DRAM Is Integrated? (Hybrid)</p>
    <p>Challenge of hybrid memory: Data in DRAM will disappear when there is a power failure</p>
    <p>CPU DRAM</p>
    <p>PRAM</p>
    <p>Memory controller</p>
    <p>Data</p>
    <p>Power failure</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>FLUSH Operation Solution: Provide Flush operation which moves DRAM data to PRAM. Memory controller generates PRAM write request corresponding to the target DRAM row.</p>
    <p>NOTE) PRAM write will be stored in command queue which exists in memory controller. And DRAM dataline is invalidated.</p>
    <p>CPU DRAM</p>
    <p>PRAM</p>
    <p>Memory controller</p>
    <p>Data Flush</p>
    <p>Generate PRAM write request</p>
    <p>WRITE</p>
  </div>
  <div class="page">
    <p>Okay, Data Delivery Is Guaranteed. Is It Good Enough?</p>
    <p>Challenge of flush: User believes data has the latest value. But, the memory controller can reorder the order of memory request</p>
    <p>CPU DRAM</p>
    <p>PRAM</p>
    <p>Memory controller2) Data=2</p>
    <p>reorder</p>
    <p>Process2</p>
  </div>
  <div class="page">
    <p>FENCE Operation Solution: Provide Fence operation to enforce data delivery order of memory requests. The memory controller can simply add fence flag to check fenced or not</p>
    <p>CPU</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>Memory controller</p>
    <p>Fence 1) Data=1</p>
    <p>Fence flag</p>
  </div>
  <div class="page">
    <p>BIBIM Design4: Persistent Support Module to Guarantee Data Delivery &amp; Delivery Order</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Caching Support Module</p>
    <p>Persistent Support Module</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>We Designed Four Modules in BIBIM. BTW, How They Work</p>
    <p>Together?</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>Persistent Support Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Request Scheduling</p>
    <p>Module</p>
    <p>Caching Support Module</p>
  </div>
  <div class="page">
    <p>Specific Example: Evict DRAM Dataline</p>
    <p>[Host] Sends WRITE request to the hybrid memory</p>
    <p>[CACHING] Check DRAM dataline hit or not</p>
    <p>[CACHING] (If it is a miss) Find victim DRAM dataline based on LRU policy, and return victim DRAM address to the SCHEDULING module</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Caching Support Module</p>
    <p>Persistent Support Module</p>
    <p>Host</p>
    <p>Caching Support Module</p>
    <p>Request Scheduling</p>
    <p>ModuleWRITE</p>
    <p>DRAM Addr</p>
  </div>
  <div class="page">
    <p>Specific Example: Evict DRAM Dataline</p>
    <p>[SCHEUDLING] Sends DRAM address to the HETEROGENEITY module</p>
    <p>[HETEROGENITY] Generate DRAM commands</p>
    <p>[DRAM] Return DRAM dataline to the SCHEDULING module</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Caching Support Module</p>
    <p>Persistent Support Module</p>
    <p>Request Scheduling</p>
    <p>Module</p>
    <p>Heterogeneity Support Module</p>
    <p>DRAM Addr DRAM Cmd DRAM Data</p>
  </div>
  <div class="page">
    <p>Specific Example: Evict DRAM Dataline</p>
    <p>[SCHEUDLING] Ask fence flag is set or not to the PERSISTENT module</p>
    <p>[PERSISTENT] Return as fence flag is set</p>
    <p>[SCHEUDLING] Do not serve PRAM WRITE (Eviction of victim DRAM dataline)</p>
    <p>P H</p>
    <p>Y</p>
    <p>DRAM</p>
    <p>PRAM? ?</p>
    <p>?? Request</p>
    <p>Scheduling Module</p>
    <p>Heterogeneity Support Module</p>
    <p>Caching Support Module</p>
    <p>Persistent Support Module</p>
    <p>Request Scheduling</p>
    <p>Module DRAM Data</p>
    <p>Persistent Support Module</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Persistent Memory Workloads WHISPER: Wisconsin-HP Labs Suite for Persistence</p>
  </div>
  <div class="page">
    <p>Wisconsin-HP Labs Suite for Persistence (WHISPER)</p>
    <p>Benchmark Brief description</p>
    <p>echo Scalable, multi-version key-value store</p>
    <p>ycsb H-store like DB. Undo logs for consistency</p>
    <p>tpcc H-store like DB. Undo logs for consistency</p>
    <p>ctree Micro-benchmarks for simulations</p>
    <p>hashmap Micro-benchmarks for simulations</p>
    <p>vacation Online travel reservation system</p>
    <p>Write-intensive (82% of total requests)</p>
  </div>
  <div class="page">
    <p>Latency Comparisons (DRAM-only vs. PRAM-only vs. Hybrid memory w/BIBIM)</p>
  </div>
  <div class="page">
    <p>Average Request Latency</p>
    <p>ech o</p>
    <p>tpcc ycs b</p>
    <p>ctre e has</p>
    <p>hma p vac</p>
    <p>ation 0.0</p>
    <p>DRAM PRAM BIBIM (15%) A</p>
    <p>v g</p>
    <p>. re</p>
    <p>q u</p>
    <p>e s t</p>
    <p>la te</p>
    <p>n c y (u</p>
    <p>s )</p>
    <p>CPU CPU CPU</p>
    <p>Lower is better! DRAM &gt; BIBIM &gt; PRAM</p>
  </div>
  <div class="page">
    <p>Average Request Latency</p>
    <p>ech o</p>
    <p>tpcc ycs b</p>
    <p>ctre e</p>
    <p>has hma</p>
    <p>p vac</p>
    <p>ation 0.0</p>
    <p>DRAM PRAM BIBIM (15%)</p>
    <p>A vg</p>
    <p>. re</p>
    <p>q u e st</p>
    <p>la te</p>
    <p>n cy</p>
    <p>(u s)</p>
    <p>BIBIM reduces the long latency than PRAM by 87%  BIBIM only degrades the perf. than DRAM by 44%  Best performance benefits: vacation; many cache hits (high write fraction)  Small performance benefits: tpcc, ycsb; persistent operation (flush) generates</p>
    <p>many PRAM writes</p>
  </div>
  <div class="page">
    <p>Non-blocking Read Service (NBRS) Analysis Tested on PRAM-only (PRAM)</p>
  </div>
  <div class="page">
    <p>RAW Latency of Non-NBRS PRAM  Read-after-write (RAW)</p>
    <p>ech o</p>
    <p>tpc c</p>
    <p>ycs b</p>
    <p>ctre e</p>
    <p>has hm</p>
    <p>ap vac</p>
    <p>atio n0</p>
    <p>Non-NBRS NBRS</p>
    <p>A v g</p>
    <p>. re</p>
    <p>a d</p>
    <p>-a ft</p>
    <p>e r</p>
    <p>w ri te</p>
    <p>l a</p>
    <p>te n</p>
    <p>c y (</p>
    <p>u s )</p>
    <p>RAW latency of Non-NBRS: 19.8 us on average.</p>
    <p>( PRAM write latency)</p>
    <p>Req1: Write @ partition 0 (cache eviction</p>
    <p>/persistent operation)</p>
    <p>Req2: Read @ partition 1</p>
    <p>Req3: Read @ partition 15</p>
    <p>RAW</p>
    <p>Non-NBRS PRAM bank</p>
    <p>Partition 0</p>
    <p>Partition 1</p>
    <p>Partition 15</p>
    <p>B lo</p>
    <p>ck e</p>
    <p>d !</p>
    <p>Decoder</p>
    <p>S e</p>
    <p>n se</p>
    <p>a m</p>
    <p>p .</p>
  </div>
  <div class="page">
    <p>RAW Latency of NBRS PRAM  Read-after-write (RAW)</p>
    <p>Req1: Write @ partition 0 (cache eviction</p>
    <p>/persistent operation)</p>
    <p>Req2: Read @ partition 1</p>
    <p>Req3: Read @ partition 15</p>
    <p>RAW</p>
    <p>RAR</p>
    <p>ech o</p>
    <p>tpc c</p>
    <p>ycs b</p>
    <p>ctre e</p>
    <p>has hm</p>
    <p>ap vac</p>
    <p>atio n0</p>
    <p>Non-NBRS NBRS</p>
    <p>A v g . re</p>
    <p>a d -a</p>
    <p>ft e r</p>
    <p>w ri te</p>
    <p>l a te</p>
    <p>n c y (</p>
    <p>u s )</p>
    <p>Many writeMany partition conflicts</p>
    <p>RAW latency of NBRS: 52% reduced on average.</p>
    <p>NBRS PRAM bank</p>
    <p>Partition 0</p>
    <p>Partition 1</p>
    <p>Partition 15</p>
    <p>Decoder</p>
    <p>S e</p>
    <p>n se</p>
    <p>a m</p>
    <p>p .</p>
    <p>N B</p>
    <p>R S</p>
    <p>!</p>
  </div>
  <div class="page">
    <p>Demo  Normal Version</p>
    <p>DRAM</p>
    <p>PRAM</p>
    <p>BIBIM</p>
    <p>Performance comparison (memory</p>
    <p>access with synthetic, readwrite inter-mixed trace)</p>
  </div>
  <div class="page">
    <p>Conclusion BIBIM can achieve both high capacity and short latency</p>
    <p>Our main contributions: 1) Multi-partition aware scheduler (NBRS) 2) New physical layer (PHY) 3) Persistent support for hybrid memory</p>
    <p>BIBIMPRAM-Only A</p>
    <p>v g . la</p>
    <p>te n c y (</p>
    <p>u s )</p>
  </div>
</Presentation>
