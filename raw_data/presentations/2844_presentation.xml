<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TCPRDMA: CPU-efficient Remote Storage Access with i10</p>
    <p>Jaehyun Hwang Qizhe Cai Rachit AgarwalAo Tang</p>
  </div>
  <div class="page">
    <p>i10 Motivation: Two trends in Remote I/O</p>
    <p>Move from SCSI to NVMe  Emergence of NVMe SSDs:</p>
    <p>&gt;1M IOPS (read), &gt;400K IOPS (write)</p>
  </div>
  <div class="page">
    <p>What do these trends mean for Remote I/O?</p>
    <p>Bottlenecks pushed back to OS, software stack!</p>
    <p>Software stack (iSCSI) performance  Storage + network overlap</p>
    <p>Long-lived Local I/O Remote I/OP er</p>
    <p>-c or</p>
    <p>e Th</p>
    <p>ro ug</p>
    <p>hp ut</p>
    <p>(k</p>
    <p>IO P</p>
    <p>S )</p>
    <p>TCP/IP Storage Storage</p>
    <p>Remote I/O</p>
    <p>TCP/IP</p>
    <p>Bottlenecks at the boundary of storage and network stacks!</p>
    <p>~30Gbps</p>
  </div>
  <div class="page">
    <p>User-space storage and network stacks  Storage + Remote I/O (user) + DPDK  Performance: Good!</p>
    <p>Storage</p>
    <p>Remote I/O</p>
    <p>TCP/IP</p>
    <p>NIC driver</p>
    <p>User space</p>
    <p>Kernel</p>
    <p>Storage</p>
    <p>Remote I/O</p>
    <p>TCP/IP</p>
    <p>NIC driver</p>
    <p>NIC RDMA-NIC</p>
    <p>NVMe-over-RDMA  Storage + Remote I/O (kernel) + RDMA  Performance: Good!</p>
    <p>Previous Approaches</p>
    <p>In-Kernel (NVMe-over-TCP)  Storage + Remote I/O + TCP (all in the kernel)  Performance: Not-so-good!</p>
    <p>H/W</p>
    <p>User-level stacks</p>
    <p>User-level I/O engines</p>
    <p>Applications</p>
    <p>TCP/IP</p>
    <p>NIC driver Fundamental?</p>
  </div>
  <div class="page">
    <p>CPU Usage (%)</p>
    <p>Remote Storage Access Overheads: TCP vs. RDMA</p>
    <p>Apps Blk TX Blk RX Net TX Net RX Idle Others</p>
    <p>NVMe-over-TCP NVMe-over-RDMA</p>
    <p>Storage stack Network stack</p>
    <p>Storage</p>
    <p>Remote I/O</p>
    <p>TCP/IP</p>
    <p>NIC</p>
    <p>NVMe TCP</p>
    <p>NVMe RDMA</p>
    <p>Network processing overhead! Context switching overhead!</p>
  </div>
  <div class="page">
    <p>i10 Summary  A new remote I/O stack implemented entirely in the kernel</p>
    <p>No changes in apps, no changes in TCP/IP stack, no changes in hardware</p>
    <p>Throughput-per-core similar to NVMe-over-RDMA  Latency within 1.7x of RDMA (for SSD accesses)</p>
    <p>Three simple ideas  i10-lane: dedicated resources  i10-caravans: request and data batching  Delayed doorbells: Interrupt coalescing</p>
    <p>Minimize network processing</p>
    <p>Minimize context switching</p>
  </div>
  <div class="page">
    <p>Block</p>
    <p>i10</p>
    <p>TCP/IP</p>
    <p>i10-lane: Dedicated per-(core, target) pair pipe</p>
    <p>Kernel</p>
    <p>User space</p>
    <p>App.</p>
    <p>I/O syscalls</p>
    <p>NVMe SSD</p>
    <p>i10-lane</p>
    <p>[Target] [Host]</p>
    <p>NIC NIC</p>
    <p>Per-core block queues</p>
    <p>Dedicated i10 queues</p>
    <p>Dedicated TCP buffers</p>
  </div>
  <div class="page">
    <p>Block</p>
    <p>i10</p>
    <p>TCP/IP</p>
    <p>i10-lane: Dedicated per-(core, target) pair pipe</p>
    <p>Kernel</p>
    <p>User space</p>
    <p>App.</p>
    <p>I/O syscalls</p>
    <p>[Target2]</p>
    <p>NVMe SSD</p>
    <p>[Target] [Host]</p>
    <p>NIC NVMe SSDNIC NIC</p>
  </div>
  <div class="page">
    <p>i10-lane: Why per-(core, target) pair lanes?</p>
    <p>Per-target: Too much contention  Per-core: Fewer batching opportunities</p>
    <p>Target2Target1Target1 Target2 Target1 Target2</p>
    <p>All requests in each i10 queue are destined to the same target over the same TCP connection</p>
    <p>Block</p>
    <p>i10</p>
  </div>
  <div class="page">
    <p>i10 Caravans: i10-lanes enable efficient batching</p>
    <p>Block</p>
    <p>i10</p>
    <p>TCP/IP</p>
    <p>Kernel</p>
    <p>User space</p>
    <p>App.</p>
    <p>I/O syscalls</p>
    <p>[Target2]</p>
    <p>NVMe SSD</p>
    <p>[Target] [Host]</p>
    <p>NIC NVMe SSDNIC NIC</p>
    <p>Sockets Allow larger payloads up to 64KB using TSO</p>
    <p>No CPU cycles for packet segmentation</p>
    <p>Significantly reduce per-byte network processing overhead!</p>
    <p>caravan (~64KB)</p>
    <p>one socket call per caravan</p>
  </div>
  <div class="page">
    <p>Block</p>
    <p>i10</p>
    <p>TCP/IP</p>
    <p>Kernel</p>
    <p>User space</p>
    <p>App.</p>
    <p>I/O syscalls</p>
    <p>NVMe SSD</p>
    <p>[Target] [Host]</p>
    <p>NIC NIC</p>
    <p>Context switching in Remote I/O (without i10)</p>
    <p>High thread switching overhead! (1-3us per request)</p>
  </div>
  <div class="page">
    <p>Block</p>
    <p>i10</p>
    <p>TCP/IP</p>
    <p>Delayed doorbells: Minimizing context switching</p>
    <p>Kernel</p>
    <p>User space</p>
    <p>App.</p>
    <p>I/O syscalls</p>
    <p>NVMe SSD</p>
    <p>[Target] [Host]</p>
    <p>NIC NIC</p>
    <p>Ring doorbell only after caravan size worth of requests</p>
    <p>Under low loads, use a timer (e.g., 50us)</p>
  </div>
  <div class="page">
    <p>i10 Evaluation Setup  Two 24-core servers connected directly</p>
    <p>100Gbps Mellanox CX-5  No switches in middle  ensure bottlenecks in the kernel</p>
    <p>NVMe-device at both servers  ~700k IOPS (read), ~400k IOPS (write)  ~100us read latency</p>
    <p>No specialized hardware functionalities used in i10 evaluation  For hardware and software configuration, see the paper.</p>
  </div>
  <div class="page">
    <p>i10 Evaluation: how does i10 performance    compare to NVMe-over-RDMA?</p>
    <p>Metrics of interest: throughput per core, average latency, tail latency   compare to user-space stacks?   vary with different workloads, hardware and applications?</p>
    <p>read/write ratios  Delayed doorbell timer  Aggregation size  I/O Request sizes  Storage device access latency  Real applications  Number of target devices</p>
    <p>scale with number of cores?   depend on various design aspects (lanes, caravans, delayed doorbell)?</p>
    <p>Throughput Latency</p>
    <p>Comparable (or better)</p>
    <p>Not terrible (&lt;1.7X)</p>
    <p>Answer:</p>
    <p>What is the best case performance? What do we expect from TCP  RDMA ? Please see our paper!</p>
  </div>
  <div class="page">
    <p>TCP  RDMA:</p>
    <p>Throughput: Comparable  Tail latency: &lt;1.7X</p>
    <p>Single core performance</p>
    <p>NVMe SSD</p>
    <p>~96k ~213k ~225k ~122k ~228k ~269k</p>
    <p>SSD access latency</p>
    <p>RAM block device</p>
    <p>TCP  RDMA:</p>
    <p>Throughput: Comparable (or better)  Tail latency: +97us</p>
    <p>High load latency: TCP  RDMA</p>
  </div>
  <div class="page">
    <p>Scalability with number of cores</p>
    <p>RAM block device</p>
    <p>TCP  RDMA:</p>
    <p>Throughput: Scales similar (~14 cores) or better</p>
    <p>Seems related to hardware scalability</p>
  </div>
  <div class="page">
    <p>i10-lane TSO/GRO + Jumbo i10 Caravans Delayed Doorbells</p>
    <p>#Cores</p>
    <p>Each of the design component contributes to i10 performance</p>
    <p>Benefits from individual design components</p>
    <p>T hr</p>
    <p>ou gh</p>
    <p>pu t (</p>
    <p>kI O</p>
    <p>P S</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>CPU Usage (%)</p>
    <p>i10 improves over NVMe-over-TCP by using Fewer cycles for network processing (Net Tx/Rx) and scheduling (Others) More cycles for Applications, and block layer operations (Blk Tx/Rx)</p>
    <p>Understanding performance improvement</p>
    <p>Apps Blk TX Blk RX Net TX Net RX Idle Others</p>
    <p>NVMe-over-TCP i10 with caravans i10 with caravans+delayed doorbells</p>
    <p>(scheduling overhead, etc.)</p>
  </div>
  <div class="page">
    <p>Kernel implementation Further evaluation</p>
    <p>Test scripts  All are available at:</p>
    <p>https://github.com/i10-kernel/</p>
  </div>
</Presentation>
