<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Atul Adya  Google John Dunagan  Microso7 Alec Wolman  Microso0 Research</p>
  </div>
  <div class="page">
    <p>Incoming Request (from Device D1): store my current IP = A</p>
    <p>Frontend Web server</p>
    <p>Frontend Web server</p>
    <p>Frontend Web server</p>
    <p>ApplicaGon Server (InMemory)</p>
    <p>ApplicaGon Server (InMemory)</p>
    <p>ApplicaGonServer (InMemory)</p>
    <p>Incoming Request (from Device D2): Tell me D1s current IP addr</p>
    <p>Locate the App Server that stores the contact info for D1</p>
    <p>Store D1s IP addr</p>
    <p>Locate the App Server that stores the contact info for D1</p>
    <p>Read D1s IP addr</p>
    <p>Problems:  How to assign responsibility for items to app servers? (parEEoning)</p>
    <p>How to deal with addiEon, removal, &amp; crashes of app servers?</p>
    <p>How to avoid requests for the same item winding up at different servers? (use leases)</p>
    <p>How to adapt to load changes?</p>
  </div>
  <div class="page">
    <p>Targets class of services with these characterisEcs:  InteracEve (needs low latency)  App servers operate on inmemory state</p>
    <p>ApplicaEon Eer operates on cached data: the truth is hosted on clients or backend storage</p>
    <p>Services use many small objects  Even the most popular object can be handled by one server  ReplicaEon not needed to handle load</p>
  </div>
  <div class="page">
    <p>Prior systems implement leasing and parEEoning separately</p>
    <p>We show that integraEng leasing and parEEoning allows scaling to massive numbers of objects</p>
    <p>This integraEon requires us to rethink the mechanisms and API for leasing  Managerdirected leasing  NontradiEonal API where clients cannot request leases</p>
  </div>
  <div class="page">
    <p>Centrifuge design  Centrifuge internals  Results from live deployment</p>
  </div>
  <div class="page">
    <p>Frontend Lookup Library Centrifuge</p>
    <p>Manager Service</p>
    <p>InMemory Server Owner Library</p>
    <p>InMemory Server Owner Library</p>
    <p>InMemory Server Owner Library</p>
    <p>Frontend Lookup Library</p>
    <p>Frontend Lookup Library</p>
    <p>Lookups: FrontEnd Web Servers</p>
    <p>Owners: Middle Tier ApplicaGon Servers</p>
  </div>
  <div class="page">
    <p>Need to issue leases for very large # of objects  Lease per object will lead to prohibiEve overhead</p>
    <p>Centrifuge manager hands out leases on ranges</p>
    <p>Use consistent hashing to parEEon a flat namespace  Assign leases on conEguous ranges of the hashed namespace</p>
    <p>One lease (one range) per virtual node (64 per server)</p>
    <p>Single mechanism: managerdirected leasing handles both leasing and parEEoning</p>
    <p>Centrifuge Manager Service</p>
    <p>InMemory Server Owner Library</p>
    <p>Lease: 050,100200</p>
  </div>
  <div class="page">
    <p>Lookup API URL Lookup(Key key) void LossNoEficaEonUpcall(KeyRange[] lost)</p>
    <p>Owner API bool CheckLeaseNow(Key key, out LeaseNum leaseNum) bool CheckLeaseConEnuous(Key key, LeaseNum leaseNum)</p>
    <p>Frontend Lookup Library Frontend Lookup Library Frontend Lookup Library</p>
    <p>Incoming Request: Find Device D</p>
    <p>Lookup(D) &gt; hXp://m6/</p>
    <p>Server m2 Owner Library</p>
    <p>Server m1 Owner Library</p>
    <p>Server m6 Owner Library</p>
  </div>
  <div class="page">
    <p>Servers in datacenter environment are stable</p>
    <p>Benefits  Much cheaper to avoid holding mulEple copies in RAM  Avoids complexity/performance issues of quorum protocols  Doesnt add extra complexity:  Need a mechanism to tolerate correlated failures anyway (e.g. security vulnerabiliEes, patch installaEon)</p>
    <p>Cost  When an applicaEon server crashes, items are not available unEl clients republish</p>
  </div>
  <div class="page">
    <p>When applicaEon server crashes, Lookups receive Loss NoEficaEons  Indicates which ranges are lost  Allows the applicaEon to determine which clients should republish their state</p>
    <p>Live Mesh services use this model  Rely on clients to recover state</p>
  </div>
  <div class="page">
    <p>ParEEoning  Manager spreads namespace across Owners by assigning leases</p>
    <p>Consistency  Leases ensure singlecopy guarantee: at any Eme t, for any key at most one Owner node</p>
    <p>Recovery  Loss noEficaEons enable app developer to detect and recover from Owner crashes</p>
    <p>Membership  Owners indicate liveness by requesEng leases</p>
    <p>Load Balancing  Manager rebalances namespace based on reported load</p>
  </div>
  <div class="page">
    <p>Centrifuge design  Centrifuge internals  Results from live deployment</p>
  </div>
  <div class="page">
    <p>Incremental protocol to synchronize Lookup and Manager lease tables</p>
    <p>Lookups are fast: no need to contact Manager and incur delay</p>
    <p>Manager load not dependent on incoming request load to Lookups 13</p>
    <p>Lookup Manager</p>
    <p>Lease Table Current LSN:4 [01:Owner=A] [12:Owner=B] [29:Owner=C] Change Log</p>
    <p>Cached Lease Table Current LSN:2</p>
    <p>I am at LSN 2.</p>
    <p>Here are changes LSN 2&gt;4</p>
  </div>
  <div class="page">
    <p>Robustness: Owners have mulEple opportuniEes to retain their leases:  Leases requested every 15 seconds  Leases last 60 seconds  Takes 3 consecuEve lost/delayed requests to lose the lease</p>
    <p>Safety: owner never thinks it has the lease when the manager disagrees  Similar to previous lease servers, rely on clock rate synchronizaEon</p>
    <p>Owner</p>
    <p>Request Leases</p>
    <p>Manager</p>
    <p>Leases granted/recalled</p>
  </div>
  <div class="page">
    <p>Lookups and Owners</p>
    <p>Leader</p>
    <p>Leader and Standbys</p>
    <p>Paxos Group</p>
    <p>Yes.</p>
    <p>Renew leader lease and commit state update.</p>
    <p>Can I have the leader lease?</p>
    <p>No.</p>
    <p>Standby</p>
    <p>Standby</p>
    <p>Manager Service</p>
  </div>
  <div class="page">
    <p>Centrifuge designed to run in a single datacenter</p>
    <p>Scalability target: ~1000 machines in 1 cluster</p>
    <p>Beyond there, scale by deploying mulEple clusters</p>
  </div>
  <div class="page">
    <p>Centrifuge design  Centrifuge internals  Results from live deployment</p>
  </div>
  <div class="page">
    <p>First deployed in April 2008  Results cover 2.5 months: Dec 08  Mar 09  1000 Lookups, 130 Owners  Manager = 8 servers</p>
  </div>
  <div class="page">
    <p>Is the Centrifuge manager a scalability borleneck in steadystate?</p>
    <p>How well does Centrifuge handle highchurn events?</p>
    <p>How stable are producEon servers?</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>From 1/15/09 through 3/2/09, no patch installaEons  How stable were the owners during this period?</p>
    <p>Servers are very stable: only 10 leaseloss events  7 cases, servers recovered &lt; 10 minutes  3 cases, servers recovered &lt; 1 hour</p>
  </div>
  <div class="page">
    <p>Centrifuge simplifies building scalable applicaEon Eers with inmemory state</p>
    <p>Combining leasing and parEEoning leads to a simple and powerful protocol</p>
    <p>Deployed within Live Mesh since April 2008, in use by 5 different Live Mesh Services</p>
    <p>Data center server stability enables the single copy in RAM w/loss noEficaEons</p>
  </div>
</Presentation>
