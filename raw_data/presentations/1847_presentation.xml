<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>First Prev Page 1 Next Last Go Back Full Screen Close Quit</p>
    <p>Multi-Label Informed Latent Semantic Indexing</p>
    <p>Shipeng Yu12</p>
    <p>Joint work with Kai Yu1 and Volker Tresp1</p>
    <p>August 2005</p>
  </div>
  <div class="page">
    <p>First Prev Page 2 Next Last Go Back Full Screen Close Quit</p>
    <p>Outline</p>
    <p>Motivation</p>
    <p>Latent Semantic Indexing</p>
    <p>Multi-label Informed Latent Semantic Indexing (MLSI)</p>
    <p>Experimental Results</p>
    <p>Conclusion and Future works</p>
  </div>
  <div class="page">
    <p>First Prev Page 3 Next Last Go Back Full Screen Close Quit</p>
    <p>Motivation</p>
    <p>We are dealing with high-dimensional data in information retrieval.</p>
    <p>A typical text corpus has more than 10,000 features (words as features)!</p>
    <p>What are the problems?</p>
    <p>Noisy features: Effective features are small</p>
    <p>Learnability: curse of dimensionality</p>
    <p>Inefficiency: Computational cost is too high</p>
    <p>How to solve these problems? Dimensionality Reduction</p>
    <p>Feature selection: Select part of the features</p>
    <p>Latent semantic indexing (LSI): Learn a feature transformation from high-dimensional input space to a low-dimensional latent space</p>
  </div>
  <div class="page">
    <p>First Prev Page 4 Next Last Go Back Full Screen Close Quit</p>
    <p>Why MLSI</p>
    <p>LSI is unsupervised:</p>
    <p>Unable to use prior knowledge or label information</p>
    <p>The indexing is not necessarily related to classification tasks</p>
    <p>We want to have a feature transformation method that can</p>
    <p>Incorporate label information elegantly</p>
    <p>Derive both linear and non-linear mappings</p>
    <p>Explore the dependency between multiple categories</p>
    <p>This leads to Multi-label informed Latent Semantic Indexing (MLSI).</p>
  </div>
  <div class="page">
    <p>First Prev Page 5 Next Last Go Back Full Screen Close Quit</p>
    <p>Before We Start ...</p>
    <p>Some notations:</p>
    <p>We have N documents</p>
    <p>Document i is denoted as xi  X  RM</p>
    <p>Output for the ith document is denoted as yi  Y  RL</p>
    <p>X  RNM , Y  RNL contain the input and output data as follows:</p>
    <p>X =</p>
    <p>x11    x1M... ... ... xN 1    xN M</p>
    <p>=</p>
    <p>xT1...</p>
    <p>xTN</p>
    <p>, Y =</p>
    <p>y11    y1L... ... ... yN 1    yN L</p>
    <p>=</p>
    <p>yT1...</p>
    <p>yTN</p>
    <p>We aim to derive a mapping  : X 7 V such that V  RK , K &lt; M</p>
  </div>
  <div class="page">
    <p>First Prev Page 6 Next Last Go Back Full Screen Close Quit</p>
    <p>Outline</p>
    <p>Motivation</p>
    <p>Latent Semantic Indexing</p>
    <p>Multi-label Informed Latent Semantic Indexing</p>
    <p>Experimental Results</p>
    <p>Conclusion and Future works</p>
  </div>
  <div class="page">
    <p>First Prev Page 7 Next Last Go Back Full Screen Close Quit</p>
    <p>Latent Semantic Indexing</p>
    <p>LSI finds the best rank-K approximation to the data matrix X.</p>
    <p>This can be equivalently solved by singular value decomposition (SVD) of X:</p>
    <p>X = VUT</p>
    <p>We can sort diagonal entries of  in decreasing order</p>
    <p>U = [u1, . . . , uK] gives the K mapping directions</p>
    <p>Problem: How to incorporate label information into the mappings?</p>
  </div>
  <div class="page">
    <p>First Prev Page 8 Next Last Go Back Full Screen Close Quit</p>
    <p>Optimization Problem of LSI</p>
    <p>Alternatively, LSI minimizes the reconstruction error of input data:</p>
    <p>min A,V</p>
    <p>X  VA2F s.t. VT V = I,</p>
    <p>with V  RNK the latent factors, and A  RKM the factor loadings.</p>
  </div>
  <div class="page">
    <p>First Prev Page 9 Next Last Go Back Full Screen Close Quit</p>
    <p>MLSI</p>
    <p>In MLSI we are minimizing the reconstruction errors of both X and Y:</p>
    <p>min A,B,V</p>
    <p>(1  )X  VA2F + Y  VB 2 F</p>
    <p>s.t. VT V = I, V = XW.</p>
    <p>MLSI is biased by the outputs Y</p>
    <p>MLSI minimizes the inter-correlation between X and Y</p>
    <p>MLSI minimizes the intra-correlation within Y (if multiple outputs)</p>
  </div>
  <div class="page">
    <p>First Prev Page 10 Next Last Go Back Full Screen Close Quit</p>
    <p>Outline</p>
    <p>Motivation</p>
    <p>Latent Semantic Indexing</p>
    <p>Multi-label Informed Latent Semantic Indexing</p>
    <p>Primal form: Linear mappings</p>
    <p>Dual form: Non-linear mappings</p>
    <p>Experimental Results</p>
    <p>Conclusion and Future works</p>
  </div>
  <div class="page">
    <p>First Prev Page 11 Next Last Go Back Full Screen Close Quit</p>
    <p>Solution of MLSI</p>
    <p>The optimization problem is</p>
    <p>min A,B,V</p>
    <p>(1  )X  VA2F + Y  VB 2 F</p>
    <p>s.t. VT V = I, V = XW.</p>
    <p>Following standard Lagrange formulism, we obtain, at the optimum,</p>
    <p>A and B solely depend on V: A = VT X, B = VT Y.</p>
    <p>Denote K := (1  )XXT + YYT , the minimum value is N</p>
    <p>i=K+1 i.</p>
    <p>We only need to optimize W since V = XW.</p>
  </div>
  <div class="page">
    <p>First Prev Page 12 Next Last Go Back Full Screen Close Quit</p>
    <p>MLSI: Primal Form</p>
    <p>Denote W = [w1, . . . , wK], we turn to an equivalent problem w.r.t. w:</p>
    <p>max wRM</p>
    <p>wT XT KXw</p>
    <p>s.t. wT XT Xw = 1.</p>
    <p>This leads to the primal form of the MLSI solution:</p>
    <p>Calculate K = (1  )XXT + YYT ;</p>
    <p>Solve a generalized eigenvalue problem XT KXw = XT Xw, obtain eigenvectors w1, . . . , wK with largest K eigenvalues 1  . . .  K ; Form mapping functions j(x) =</p>
    <p>jw</p>
    <p>T j x,j = 1, . . . ,K, and finally</p>
    <p>(x) = [1(x), . . . ,K(x)] T defines the mapping .</p>
    <p>MLSI recovers LSI when  = 0.</p>
  </div>
  <div class="page">
    <p>First Prev Page 13 Next Last Go Back Full Screen Close Quit</p>
    <p>MLSI: Dual Form</p>
    <p>Dual form is obtained by applying representer theorem and define dual variable  as</p>
    <p>w = XT .</p>
    <p>This leads to the equivalent dual form with respect to :</p>
    <p>max RN</p>
    <p>T KxKKx</p>
    <p>s.t. T K2x = 1.</p>
    <p>Kx = XX T , Ky = YY</p>
    <p>T , K = (1  )Kx + Ky.</p>
    <p>This is a simpler problem for N &lt; M.</p>
  </div>
  <div class="page">
    <p>First Prev Page 14 Next Last Go Back Full Screen Close Quit</p>
    <p>Primal versus Dual</p>
    <p>Which form to choose in real world applications?</p>
    <p>Primal MLSI solves an M  M generalized eigenvalue problem  more efficient when M &lt; N</p>
    <p>can only learn a linear mapping for X</p>
    <p>Dual MLSI solves an N  N generalized eigenvalue problem  more efficient when N &lt; M (usually true for text data)</p>
    <p>can learn non-linear mappings using kernel trick</p>
  </div>
  <div class="page">
    <p>First Prev Page 15 Next Last Go Back Full Screen Close Quit</p>
    <p>Connection to Related Work</p>
    <p>MLSI is more general to other supervised projection methods.</p>
    <p>Fisher Discriminant Analysis (FDA)</p>
    <p>Only deal with binary classification problem</p>
    <p>Can only handle one output</p>
    <p>Canonical Correlation Analysis (CCA)</p>
    <p>Only minimize the correlation between X and Y</p>
    <p>Ignore intrinsic correlations of both X and Y</p>
    <p>Partial Least Square (PLS)</p>
    <p>A penalized CCA</p>
    <p>Can not generalize well to new data</p>
  </div>
  <div class="page">
    <p>First Prev Page 16 Next Last Go Back Full Screen Close Quit</p>
    <p>Outline</p>
    <p>Motivation</p>
    <p>Latent Semantic Indexing</p>
    <p>Multi-label Informed Latent Semantic Indexing</p>
    <p>Experimental Results</p>
    <p>Conclusion and Future works</p>
  </div>
  <div class="page">
    <p>First Prev Page 17 Next Last Go Back Full Screen Close Quit</p>
    <p>Experiment Setup</p>
    <p>The Goal: Evaluate indexing methods for multi-label classification.</p>
    <p>Data sets</p>
    <p>Reuters-21578: 1600 documents with 6076 words, 47 categories</p>
    <p>RCV1: 3588 documents with 5496 words, 79 categories</p>
    <p>Preprocessing</p>
    <p>Take categories with at least 50 documents</p>
    <p>Pick up words that occur at least 5 times in documents</p>
    <p>Use TFIDF features</p>
  </div>
  <div class="page">
    <p>First Prev Page 18 Next Last Go Back Full Screen Close Quit</p>
    <p>Methodology</p>
    <p>We compare three methods:</p>
    <p>Full Features: Use all features to do classification</p>
    <p>LSI: Classification with new unsupervised features</p>
    <p>MLSI: Classification with new supervised features</p>
    <p>We test two settings for each data set:</p>
    <p>Setting (I): We pick up 70% categories for classification and employ 5-fold cross-validation with one fold training and 4 folds testing</p>
    <p>Setting (II): Evaluate the classification performance on the rest 30% categories for previously unseen data with newly derived features</p>
  </div>
  <div class="page">
    <p>First Prev Page 19 Next Last Go Back Full Screen Close Quit</p>
    <p>Results for Reuters-21578</p>
    <p>MLSI is significantly better than LSI.</p>
  </div>
  <div class="page">
    <p>First Prev Page 20 Next Last Go Back Full Screen Close Quit</p>
    <p>Results for RCV1</p>
    <p>MLSI is significantly better than Full Features in setting (II).</p>
  </div>
  <div class="page">
    <p>First Prev Page 21 Next Last Go Back Full Screen Close Quit</p>
    <p>Sensitivity of  for MLSI</p>
    <p>setting (I) setting (II)</p>
  </div>
  <div class="page">
    <p>First Prev Page 22 Next Last Go Back Full Screen Close Quit</p>
    <p>Outline</p>
    <p>Motivation</p>
    <p>Latent Semantic Indexing</p>
    <p>Multi-label Informed Latent Semantic Indexing</p>
    <p>Experimental Results</p>
    <p>Conclusion and Future works</p>
  </div>
  <div class="page">
    <p>First Prev Page 23 Next Last Go Back Full Screen Close Quit</p>
    <p>Conclusion</p>
    <p>MLSI has the following advantages:</p>
    <p>It is supervised and incorporates label information</p>
    <p>It considers both the inter-correlation between X and Y, and the intracorrelation of Y</p>
    <p>Both linear and non-linear mappings are easy to derive</p>
    <p>It handles multiple outputs simultaneously</p>
    <p>It takes LSI as a special case (when  = 0)</p>
    <p>Experimental results are very encouraging.</p>
  </div>
  <div class="page">
    <p>First Prev Page 24 Next Last Go Back Full Screen Close Quit</p>
    <p>Future Works</p>
    <p>Compare with other supervised projection methods</p>
    <p>Automatically set parameter</p>
    <p>Try larger data sets</p>
    <p>Apply the indexing to information retrieval tasks</p>
  </div>
</Presentation>
