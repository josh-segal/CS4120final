<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora</p>
    <p>Stephen Roller, Douwe Kiela, and Maximilian Nickel</p>
  </div>
  <div class="page">
    <p>Hypernymy  Hierarchical relations play a central role in</p>
    <p>knowledge representation (Miller, 1995)</p>
    <p>cat is a feline is a mammal is an animal</p>
    <p>All animals are living things -&gt; cats are living things</p>
    <p>Automatic hypernymy detection approaches:</p>
    <p>Pattern based: high-precision lexico-syntactic patterns (Hearst, 1992)</p>
    <p>Distributional Inclusion: unconstrained word co-occurrences (Zhitomirsky-Geffet and Dagan, 2005)</p>
    <p>/[NP] such as [NP] (and [NP])?/</p>
    <p>animals such as cats and dogs</p>
    <p>animals including cats and dogs</p>
    <p>cats, dogs, and other animals</p>
  </div>
  <div class="page">
    <p>Objectives</p>
    <p>Are Hearst patterns more valuable than distributional information?  Do we learn more from using general semantic contexts, or exploiting highly targeted ones?  Are differences robust across multiple evaluation settings?</p>
    <p>Can we remedy some of Hearst patterns' weaknesses?  Scaling up data and extraction is cheaper and easier today  Do embedding methods help alleviate sparsity?</p>
  </div>
  <div class="page">
    <p>Tasks 10% Validation, 90% Test</p>
    <p>Detection  Distinguish hypernymy pairs from other relations</p>
    <p>Average Precision (AP) across 5 datasets (Shwartz et al., 2017)</p>
    <p>Direction  Identify the direction of entailment (XY or YX?)</p>
    <p>Accuracy across 3 datasets (Kiela et al., 2015)</p>
    <p>2 also contain non-entailments (XY)</p>
    <p>Graded Entailment  Predict the degree of entailment</p>
    <p>Spearman's rho on 1 dataset (Vuli et al., 2017)</p>
    <p>Detection  BLESS (Baroni and Lenci, 2011)</p>
    <p>EVAL (Santus et al., 2015)</p>
    <p>LEDS (Baroni et al., 2012)</p>
    <p>Shwartz (Shwartz et al., 2016)</p>
    <p>WBLESS (Weeds et al., 2014)</p>
    <p>Direction  BLESS (Baroni and Lenci, 2011)</p>
    <p>WBLESS (Weeds et al., 2014)</p>
    <p>BiBless (Kiela et al., 2015)</p>
    <p>Graded Entailment  Hyperlex (Vuli et al., 2017)</p>
  </div>
  <div class="page">
    <p>Hearst Pattern Extraction</p>
    <p>Preprocessing  10 Hearst patterns</p>
    <p>Gigaword + Wikipedia</p>
    <p>Lemmatized, POS tagged</p>
    <p>Matches were aggregated and filtered:</p>
    <p>Pair must match 2 distinct patterns</p>
    <p>431K distinct pairs covering 243K unique types</p>
  </div>
  <div class="page">
    <p>Hearst Pattern Models</p>
    <p>Count transformation  PPMI(x, y): transform counts using</p>
    <p>Positive Pointwise Mutual Information</p>
    <p>Simple embedding (Truncated SVD)  SPMI(x, y): apply truncated SVD to PPMI counts</p>
    <p>Select k using validation set</p>
    <p>Related to Cederberg and Widdows (2003)</p>
    <p>Fr eq</p>
    <p>ue nc</p>
    <p>y (lo</p>
    <p>g sc</p>
    <p>al e)</p>
  </div>
  <div class="page">
    <p>Distributional Methods</p>
    <p>Cosine baseline</p>
    <p>Selected 3 high performing, unsupervised methods based on Shwartz et al. (2017)</p>
    <p>WeedsPrec (Weeds et al., 2004); invCL (Lenci and Benotto, 2012); SLQS (Santus et al., 2014)</p>
    <p>Use strong distributional space from Shwartz et al. (2017)</p>
    <p>Wikipedia + UkWaC</p>
    <p>POS tagged and lemmatized</p>
    <p>Dependency contexts (Pado and Lapata, 2007; Levy and Goldberg, 2014)</p>
    <p>Tune hyperparameters on validation</p>
  </div>
  <div class="page">
    <p>Distr. methods have trouble with global calibration (AP)</p>
    <p>Pattern has mixed performance</p>
    <p>SPMI model best on 4/5 datasets.</p>
    <p>Embedding Hearst patterns helps overcome sparsity  Fills in gaps  Downweights outliers</p>
    <p>Detection</p>
    <p>A ve</p>
    <p>ra ge</p>
    <p>P re</p>
    <p>ci si</p>
    <p>on</p>
    <p>BLESS Shwartz EVAL LEDS WBLESS</p>
    <p>Cosine Best Distributional PPMI SPMI</p>
  </div>
  <div class="page">
    <p>Detection + Direction difficult for distributional methods</p>
    <p>Patterns outperform distr. methods on 2/3  BLESS pathologically difficult</p>
    <p>for cosine and PPMI</p>
    <p>SPMI significantly better  Embedding patterns</p>
    <p>overcomes sparsity</p>
    <p>Direction</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>BLESS WBLESS BiBless</p>
    <p>Cosine Best Distributional PPMI SPMI</p>
  </div>
  <div class="page">
    <p>Pattern based methods outperform distr.</p>
    <p>Embedding hurts...  Spearman's rho doesn't</p>
    <p>punish ties (many 0s)  Add small noise (10-6) to</p>
    <p>PPMI model to break ties randomly</p>
    <p>SPMI best after adjustment</p>
    <p>Graded Entailment</p>
    <p>Sp ea</p>
    <p>rm an</p>
    <p>'s rh</p>
    <p>o</p>
    <p>Hyperlex</p>
    <p>Cosine Best Distributional PPMI SPMI</p>
    <p>Sp ea</p>
    <p>rm an</p>
    <p>'s rh</p>
    <p>o</p>
    <p>Hyperlex</p>
    <p>Cosine Best Distributional PPMI SPMI</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Pattern-based approaches outperform distributional methods</p>
    <p>Targeted Hearst contexts are more valuable than semantic similarity gains</p>
    <p>Embedding Hearst patterns works well</p>
    <p>Helps substantially with sparsity issues</p>
    <p>We open source our experiments and evaluation framework: https://github.com/facebookresearch/hypernymysuite</p>
  </div>
  <div class="page">
    <p>Thank you! Questions?</p>
  </div>
</Presentation>
