<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Should Security Researchers Experiment More and Draw More Inferences?</p>
    <p>Kevin Killourhy with Roy Maxion</p>
    <p>Carnegie Mellon University CSET 2011 (August 8)</p>
    <p>* With thanks to Walter Tichys Should Computer Scientists Experiment More? (1998)</p>
    <p>*</p>
  </div>
  <div class="page">
    <p>Should Security Researchers Experiment More and Draw More Inferences?</p>
    <p>YES!</p>
  </div>
  <div class="page">
    <p>Security researchers rarely conduct experiments and draw inferences</p>
    <p>101 keystroke dynamics papers surveyed  80 papers evaluated a classifier</p>
    <p>Similar experience in IDS and Insider-Threat research</p>
    <p>Comparative experiments: 43 / 80 (53.75%) Inferential statistics: 6 / 80 (7.5%)</p>
    <p>http://www.cs.cmu.edu/~keystroke/cset-2011</p>
  </div>
  <div class="page">
    <p>One-off evaluations confound detector and data</p>
    <p>Researcher Detector Data Set Error Rate (percentage)</p>
    <p>Alice A 1 20</p>
    <p>Bob B 2 15</p>
    <p>Carol C 3 10</p>
    <p>Dave D 4 5</p>
  </div>
  <div class="page">
    <p>A 20</p>
    <p>B 15</p>
    <p>C 10</p>
    <p>D 5</p>
    <p>D et</p>
    <p>ec to</p>
    <p>r Data Set</p>
    <p>One-off evaluations reveal diagonals of a matrix</p>
  </div>
  <div class="page">
    <p>A 20 20 20 20</p>
    <p>B 15 15 15 15</p>
    <p>C 10 10 10 10</p>
    <p>D 5 5 5 5</p>
    <p>D et</p>
    <p>ec to</p>
    <p>r Data Set</p>
    <p>Case 1: No Data Effect</p>
  </div>
  <div class="page">
    <p>A 20 10 0 0</p>
    <p>B 25 15 5 0</p>
    <p>C 30 20 10 0</p>
    <p>D 35 25 15 5</p>
    <p>D et</p>
    <p>ec to</p>
    <p>r Data Set</p>
    <p>Case 2: Data Effect</p>
  </div>
  <div class="page">
    <p>A 20 10 5 15</p>
    <p>B 5 15 20 10</p>
    <p>C 10 5 10 20</p>
    <p>D 15 20 15 5</p>
    <p>D et</p>
    <p>ec to</p>
    <p>r Data Set</p>
    <p>Case 3: Data/Detector Interaction</p>
  </div>
  <div class="page">
    <p>Which case holds for security research?</p>
    <p>A Case 3: Data/Detector</p>
    <p>Interaction</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>A Case 1: No Data Effect</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>A Case 2: Data Effect</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
    <p>Keystroke dynamics: Worm detection:</p>
    <p>(Cho et al., 2000) (Killourhy &amp; Maxion, 2009)</p>
    <p>(Stafford &amp; Li, 2010)</p>
  </div>
  <div class="page">
    <p>Keystroke Dynamics:  Timing features  Keyboard  Amount of training  Different kinds of typists  Practice effects  Typing task  Injury or distraction</p>
    <p>Security technologies do not have an error rate; they have many error rates, depending on</p>
    <p>factors in the operating environment.</p>
    <p>Inferential statistics focus our efforts</p>
    <p>Worm Detection:  Type of network  Size of network  Traffic rate  Topology  Scanning rate  Targeting strategy  Payload characteristics</p>
    <p>Malware Scanning:  Operating system  File format  Packer  Environment</p>
    <p>(home/office)  Web browser  User habits</p>
    <p>The number of potentially important factors can be overwhelming</p>
  </div>
  <div class="page">
    <p>Empirical averages only tell part of the story</p>
    <p>Factor (value)</p>
    <p>Error Rate (percentage)</p>
    <p>X 5</p>
    <p>Y 10</p>
    <p>Z 15</p>
    <p>X Y Z</p>
    <p>Er ro</p>
    <p>r R at</p>
    <p>e</p>
    <p>X Y Z</p>
    <p>Er ro</p>
    <p>r R at</p>
    <p>e</p>
    <p>Is the factor important or not?</p>
    <p>Important Negligible</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>What?  Security researchers rarely conduct experiments and draw inferences.</p>
    <p>So What?  Current results are not very meaningful.  They cannot answer important research questions.  There is no direction for future work.  A lot of research effort is wasted.</p>
    <p>Now What? (Issues)  Gathering and sharing good data  Establishing a standard methodology  Security-specific challenges  Changing the culture  Beyond experiments and inferences</p>
  </div>
  <div class="page">
    <p>Gathering and sharing good data</p>
    <p>Gathering and sharing good data is hard!  Ground truth, artifacts, and realism are recurring problems  Confidential or sensitive information limit willingness to share</p>
    <p>Good science without comparative experiments is also hard  The problem does not go away because the solution is inconvenient.</p>
    <p>Possible solutions:  Repositories like PREDICT can protect shared data  Testbeds like DETER can generate non-sensitive data  One shared data set, even if perfect, would not be enough  Detectors could be shared instead of data</p>
    <p>A Case 3: Data/Detector</p>
    <p>Interaction</p>
    <p>B</p>
    <p>C</p>
    <p>D</p>
  </div>
  <div class="page">
    <p>Choosing the right inferential technique can be hard!  Statistical hypothesis tests vs. confidence intervals  Threshold significance levels vs. p-values  Classical, non-parametric, or Bayesian methods</p>
    <p>They may disagree on the details, but all statisticians make inferences  Additional thoughts:</p>
    <p>Practically, different techniques lead to similar conclusions  Consult with statisticians and discuss the right techniques for our data or domain  My suggestion is to start with classical methods and confidence intervals</p>
    <p>Establishing a standard methodology</p>
    <p>X Y Z Er</p>
    <p>ro r R</p>
    <p>at e</p>
    <p>X Y Z</p>
    <p>Er ro</p>
    <p>r R at</p>
    <p>e</p>
  </div>
  <div class="page">
    <p>Dealing with a malicious and intelligent adversary is hard!  A lot of other sciences deal with averages; we deal with worst cases</p>
    <p>Possible solutions:  Identify where experiments and inferences would be useful; start doing them  Establish the ratio of useful to difficult (e.g., 80:20, 50:50, 20:80)  Study adversaries and build a model (possibly using experiments and inferences)</p>
    <p>Security-specific challenges</p>
    <p>For certain areas of computer security, experiments seem useful, and the community will benefit from better experimental</p>
    <p>infrastructure, datasets, and methods. For other areas, it seems difficult to do meaningful experiments without developing a</p>
    <p>way to model a sophisticated, creative adversary. (Stolfo, Bellovin, &amp; Evans, 2011)</p>
  </div>
  <div class="page">
    <p>Changing the culture</p>
    <p>Fine! We could and should do experiments and inferences. How?  Despite the magnitude of the problem, inertia is strong  Comparative experiments are sometimes done, inferences never</p>
    <p>Change starts at home  Where home is our own research and peer reviews</p>
    <p>Additional thoughts:  Conferences can and do offer a carrot for shared data  Perhaps a stick is sometimes necessary (e.g., archival journals)  Reviewer guidelines for what constitute acceptable methods  Decide when promising exploratory work is acceptable</p>
  </div>
  <div class="page">
    <p>Beyond experiments and inferences</p>
    <p>The limits of comparative experiments and inferences  Is it enough to do comparative experiments and inferential statistics?</p>
    <p>Experiments and inferences are necessary, not sufficient:  Invalid experiments that test the wrong things  Unrealistic evaluation data  Research that cannot be reproduced  Inferential techiques that are inappropriate for the data</p>
    <p>Bad science can be done with experiments and inferences. Can good science be done without them?</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>NSF, CyLab, ARO, CERT, and USENIX</p>
    <p>David Banks, Shing-hon Lao, Soojung Ha, Chao Shen, and Pat Loring</p>
    <p>CSET organizers, reviewers, and participants</p>
  </div>
  <div class="page">
    <p>Related efforts</p>
    <p>Tichy (1998): Computer science lags behind others in experimental methodology</p>
    <p>Kurkowski et al. (2005): Similar problems exist in mobile network research</p>
    <p>Peisert and Bishop (2007): Security experiments should be falsifiable, controlled, and reproducible</p>
    <p>Somayaji et al. (2009): Adapted particular experimental and statistical methods (clinical trials) to security research</p>
    <p>Sommer and Paxson (2010): More advice when using machine-learning in security domains</p>
  </div>
  <div class="page">
    <p>In closing</p>
    <p>In bioinformatics, researchers are trained to do comparative experiments and statistical inferences</p>
    <p>Government funding and journal publication require that the research data be shared and that statistical tests be significant</p>
    <p>The expectation is that someone can download researchers data and scripts and reproduce all the tables and figures in their paper.</p>
    <p>For particularly promising results, forensic statisticians test this expectation.</p>
    <p>They often dont succeed:  Data sets contain duplicated and missing subjects  Class labels (e.g., diseased vs. healthy) have been reversed  Off-by-one errors identify the wrong factor as significant  Many times the failure cannot be adequately explained</p>
  </div>
  <div class="page">
    <p>In closing</p>
  </div>
  <div class="page">
    <p>In closing</p>
    <p>In a field where   comparative experiments are the status quo  inferential statistics are taught in research-methods courses  bad research is severely penalized</p>
    <p>they still discover problems.</p>
    <p>How concerned should we be about security research?</p>
  </div>
</Presentation>
