<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events</p>
    <p>Elahe Rahimtoroghi, Ernesto Hernandez and Marilyn A Walker</p>
    <p>Natural Language and Dialogue Systems Lab</p>
    <p>Department of Computer Science</p>
    <p>University of California Santa Cruz Santa Cruz, CA 95064, USA</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Goal u Capture common-sense knowledge about the</p>
    <p>fine-grained events of everyday experience</p>
    <p>u opening a fridge enabling preparing food</p>
    <p>u getting out of bed being triggered by an alarm going off</p>
    <p>Contingency relation between events (Cause and Condition)</p>
    <p>PDTB</p>
  </div>
  <div class="page">
    <p>Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives</p>
    <p>u Rich with common-sense knowledge about contingent relations between events</p>
    <p>u placing a tarp, setting up a tent</p>
    <p>u the hurricane made landfall, the wind blew, a tree fell</p>
    <p>u started cleaning up, cut up the trees, raking</p>
    <p>Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events</p>
    <p>Elahe Rahimtoroghi, Ernesto Hernandez and Marilyn A Walker Natural Language and Dialogue Systems Lab</p>
    <p>Department of Computer Science, University of California Santa Cruz Santa Cruz, CA 95064, USA</p>
    <p>elahe@soe.ucsc.edu, eherna23@ucsc.edu, mawalker@ucsc.edu</p>
    <p>Abstract</p>
    <p>Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topicspecific datasets enables learning finergrained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.</p>
    <p>The original idea behind scripts as introduced by Schank was to capture knowledge about the finegrained events of everyday experience, such as opening a fridge enabling preparing food, or the event of getting out of bed being triggered by an alarm going off (Schank and Abelson, 1977; Mooney and DeJong, 1985) This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than</p>
    <p>Camping Trip We packed all our things on the night before Thu (24 Jul) except for frozen food. We brought a lot of things along. We woke up early on Thu and JS started packing the frozen marinatinated food inside the small cooler... In the end, we decided the best place to set up the tent was the squarish ground thats located on the right. Prior to setting up our tent, we placed a tarp on the ground. In this way, the underneaths of the tent would be kept clean. After that, we set the tent up. Storm I dont know if I wouldve been as calm as I was without the radio, as the hurricane made landfall in Galveston at 2:10AM on Saturday. As the wind blew, branches thudded on the roof or trees snapped, it was helpful to pinpoint the place... A tree fell on the garage roof, but its minor damage compared to what couldve happened. We then started cleaning up, despite Sugar Land implementing a curfew until 2pm; I didnt see any policemen enforcing this. Luckily my dad has a gas saw (as opposed to electric), so we helped cut up three of our neighbors trees. I did a lot of raking, and theres so much debris in the garbage.</p>
    <p>Figure 1: Excerpts of two stories in the blogs corpus on the topics of Camping Trip and Storm.</p>
    <p>newswire (Hu et al., 2013; Manshadi et al., 2008; Beamer and Girju, 2009). News articles (obviously) cover newsworthy topics such as bombing, explosions, war and killing so the knowledge learned is limited to those types of events. However, much of the user-generated content on</p>
    <p>social media is provided by ordinary people telling stories about their daily lives. These stories are rich with common-sense knowledge. For example, the Camping Trip story in Fig. 1 contains implicit common-sense knowledge about contingent (causal and conditional) relations between campingrelated events, such as setting up a tent and placing a tarp. The Storm story contains implicit knowledge about events such as the hurricane made landfall,</p>
    <p>This fine-grained knowledge is simply not found in previous work on narrative event collections</p>
  </div>
  <div class="page">
    <p>A Brief Look at Previous Work</p>
    <p>u Much of the previous work is not focused on a particular relation between events (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Manshadi et al., 2008; Nguyen et al., 2015; Balasubramanian et al., 2013; Pichotta and Mooney, 2014)</p>
    <p>u Main focus is on newswire</p>
    <p>u Evaluation criteria: narrative cloze test</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 4</p>
    <p>Contingency</p>
    <p>Personal stories</p>
    <p>New evaluation method as well as previous work</p>
  </div>
  <div class="page">
    <p>Challenge: Personal stories provide both advantages and disadvantages u Told in chronological order</p>
    <p>u Temporal order between events is a strong cue to contingency</p>
    <p>u Their structure is more similar to oral narrative (Labov and Waletzky, 1967; Labov, 1997) than to newswire</p>
    <p>u Only about a third of the sentences in a personal narrative describe actions (Rahimtoroghi et al., 2014; Swanson et al., 2014)</p>
    <p>u Novel methods are needed to find useful relationships between events</p>
  </div>
  <div class="page">
    <p>Event Representation and Extraction</p>
    <p>u Multi-argument representation is richer, capable of capturing interactions between multiple events (Pichotta and Mooney, 2014)</p>
    <p>u Event extraction</p>
    <p>u Stanford dependency parser</p>
    <p>u Stanford NER</p>
    <p>Verb Lemma (subj:Subject Lemma, dobj:Direct Object Lemma, prt:Particle)</p>
    <p>Table 3 shows example sentences describing an event from the Camping topic along with their event structure. The examples show how including the arguments often change the meaning of an event. In Row 1 the direct object and particle are required to completely understand the event in this sentence. Row 2 shows another example where the verb have cannot implicate what event is happening and the direct object oatmeal is needed to understand what has occurred in the story. We parse each sentence and extract every verb</p>
    <p>lemma with its arguments using Stanford dependencies (Manning et al., 2014). For each verb, we extract the nsubj, dobj, and prt dependency relations if they exist, and use their lemma in the event representation. To generalize the event representations, we use the types identified by Stanfords Named Entity Recognizer and map each argument to its named entity type if available, e.g., in Row 3 of Table 3, the Lost Valley River Campground is represented by its type LOCATION. We use abstract types for named entities such as PERSON, ORGANIZATION, TIME and DATE. We also represent each pronoun by the abstract type PERSON, e.g. Row 5 in Table 3.</p>
    <p>and Girju (2009) as a way to measure the tendency</p>
    <p># Sentence ! Event Representation 1 but it wasnt at all frustrating putting up the tent and</p>
    <p>setting up the first night ! put (dobj:tent, prt:up) 2 The next day we had oatmeal for breakfast</p>
    <p>! have (subj:PERSON, dobj:oatmeal) 3 by the time we reached the Lost River Valley Camp</p>
    <p>ground, it was already past 1 pm ! reach (subj:PERSON, dobj:LOCATION)</p>
    <p>Table 3: Event representation examples from Camping Trip topic.</p>
    <p>of an event pair to encode a causal relation, where event pairs with high CP have a higher probability of occurring in a causal context. We calculate CP for every pair of adjacent events in each topic-specific dataset. We used a 2-skip bigram model which considers two events to be adjacent if the second event occurs within two or less events after the first one. We use skip-2 bigram in order to capture the fact</p>
    <p>that two related events may often be separated by a non-essential event, because of the oral-narrative nature of our data (Rahimtoroghi et al., 2014). In contrast to the verbs that describe an event (e.g., hike, climb, evacuate, drive), some verbs describe private states such as as belong, depend, feel, know. We filter out clauses that tend to be associated with private states (Wiebe, 1990). A pilot evaluation showed that this improves the results. Equation 1 shows the formula for calculating</p>
    <p>Causal Potential of a pair consisting of two events: (e1, e2). Here P denotes probability and P(e1 ! e2) is the probability of e2 occurring after e1 in the adjacency window which is equal to 3 due to the skip-2 bigram model. P(e2|e1) is the conditional probability of e2 given that e1 has been seen in the adjacency window. This is equivalent to the EventBigram model described in Sec. 3.3.</p>
    <p>CP(e1, e2) = log P(e2|e1) P(e2)</p>
    <p>+log P(e1 ! e2) P(e2 ! e1)</p>
    <p>(1)</p>
    <p>To calculate CP, we need to compute event counts from the corpus and thus we need to define when two events are considered equal. The simplest approach is to define two events to be equal when their verb and arguments exactly match. However, with a close look at the data this approach does not</p>
    <p>Event: Verb Lemma (subj:Subject Lemma, dobj:Direct Object Lemma, prt:Particle)</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>u Generate topic-sorted personal stories using bootstrapping</p>
    <p>u Direct comparison of topic-specific data vs. general-domain stories</p>
    <p>u Learn more fine-grained and richer knowledge from topic-specific corpus</p>
    <p>u Even with less amount of data</p>
    <p>u Two sets of experiments</p>
    <p>u Directly compare to previous work</p>
    <p>u Introduce new evaluation methods</p>
  </div>
  <div class="page">
    <p>Semi-Supervised Algorithm for Generating Topic-Specific Dataset</p>
    <p>Corpus</p>
    <p>AutoSlog-TS</p>
    <p>Event-patterns</p>
    <p>Labeled data</p>
    <p>small set ( 200-300) of stories on the topic</p>
    <p>NP-Prep-(NP):CAMPING-IN (subj)-ActVB-Dobj:WENT-CAMPING</p>
    <p>Camping: 299 Storm: 361</p>
  </div>
  <div class="page">
    <p>Causal Potential (Beamer and Girju 2009)</p>
    <p>u Unsupervised distributional measure</p>
    <p>u Tendency of an event pair to encode a causal relation</p>
    <p>u Probability of occurring in a causal context</p>
    <p>u Calculate CP for every pair of adjacent events</p>
    <p>u Skip-2 bigram model</p>
    <p>u Two related events may often be separated by a non-event sentences</p>
    <p>Verb Lemma (subj:Subject Lemma, dobj:Direct Object Lemma, prt:Particle)</p>
    <p>Table 3 shows example sentences describing an event from the Camping topic along with their event structure. The examples show how including the arguments often change the meaning of an event. In Row 1 the direct object and particle are required to completely understand the event in this sentence. Row 2 shows another example where the verb have cannot implicate what event is happening and the direct object oatmeal is needed to understand what has occurred in the story. We parse each sentence and extract every verb</p>
    <p>lemma with its arguments using Stanford dependencies (Manning et al., 2014). For each verb, we extract the nsubj, dobj, and prt dependency relations if they exist, and use their lemma in the event representation. To generalize the event representations, we use the types identified by Stanfords Named Entity Recognizer and map each argument to its named entity type if available, e.g., in Row 3 of Table 3, the Lost Valley River Campground is represented by its type LOCATION. We use abstract types for named entities such as PERSON, ORGANIZATION, TIME and DATE. We also represent each pronoun by the abstract type PERSON, e.g. Row 5 in Table 3.</p>
    <p>and Girju (2009) as a way to measure the tendency</p>
    <p># Sentence ! Event Representation 1 but it wasnt at all frustrating putting up the tent and</p>
    <p>setting up the first night ! put (dobj:tent, prt:up) 2 The next day we had oatmeal for breakfast</p>
    <p>! have (subj:PERSON, dobj:oatmeal) 3 by the time we reached the Lost River Valley Camp</p>
    <p>ground, it was already past 1 pm ! reach (subj:PERSON, dobj:LOCATION)</p>
    <p>Table 3: Event representation examples from Camping Trip topic.</p>
    <p>of an event pair to encode a causal relation, where event pairs with high CP have a higher probability of occurring in a causal context. We calculate CP for every pair of adjacent events in each topic-specific dataset. We used a 2-skip bigram model which considers two events to be adjacent if the second event occurs within two or less events after the first one. We use skip-2 bigram in order to capture the fact</p>
    <p>that two related events may often be separated by a non-essential event, because of the oral-narrative nature of our data (Rahimtoroghi et al., 2014). In contrast to the verbs that describe an event (e.g., hike, climb, evacuate, drive), some verbs describe private states such as as belong, depend, feel, know. We filter out clauses that tend to be associated with private states (Wiebe, 1990). A pilot evaluation showed that this improves the results. Equation 1 shows the formula for calculating</p>
    <p>Causal Potential of a pair consisting of two events: (e1, e2). Here P denotes probability and P(e1 ! e2) is the probability of e2 occurring after e1 in the adjacency window which is equal to 3 due to the skip-2 bigram model. P(e2|e1) is the conditional probability of e2 given that e1 has been seen in the adjacency window. This is equivalent to the EventBigram model described in Sec. 3.3.</p>
    <p>CP(e1, e2) = log P(e2|e1) P(e2)</p>
    <p>+log P(e1 ! e2) P(e2 ! e1)</p>
    <p>(1)</p>
    <p>To calculate CP, we need to compute event counts from the corpus and thus we need to define when two events are considered equal. The simplest approach is to define two events to be equal when their verb and arguments exactly match. However, with a close look at the data this approach does not</p>
    <p>Temporal order</p>
  </div>
  <div class="page">
    <p>Evaluations</p>
    <p>u Narrative cloze test</p>
    <p>u Sequence of narrative events in a document from which one event has been removed</p>
    <p>u Predict the missing event</p>
    <p>u Unigram model results nearly as good as other complicated models (Pichotta and Mooney, 2014)</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 10</p>
  </div>
  <div class="page">
    <p>Automatic Two-Choice Test</p>
    <p>u Automatically generated set of two-choice questions with the answers</p>
    <p>u Modeled after the COPA task (An Evaluation of Commonsense Causal Reasoning, Roemmele et al., 2011)</p>
    <p>u From held-out test sets for each dataset</p>
    <p>u Each question consists of one event and two choices</p>
    <p>Question event: arrange (dobj:outdoor)</p>
    <p>Choice 1: help (dobj:trip) Choice 2: call (subj:PERSON)</p>
    <p>u Predict which of the two choices is more likely to have a contingency relation with the event in the question</p>
  </div>
  <div class="page">
    <p>Comparison to Previous Work: Rel-gram Tuples (Balasubramanian et al., 2013)</p>
    <p>u Rel-grams: Generate pairs of relational tuples of events</p>
    <p>u Use co-occurrence statistics based on Symmetric Conditional Probability</p>
    <p>u Publicly available through an online search interface</p>
    <p>u Outperform the previous work</p>
    <p>u Two experiments:</p>
    <p>u Content of the learned event knowledge</p>
    <p>u Method: one of the baselines on our data</p>
    <p>seem adequate. For example, consider the following events:</p>
    <p>go (subj:PERSON, dobj:camp) go (subj:family, dobj:camp) go (dobj:camp)</p>
    <p>They encode the same action although their representations do not exactly match and differ in the subject. Our intuition is that when we count the number of events represented as go (subj:PERSON, dobj:camp) we should also include the count of go (dobj:camp). To be able to generalize over the event structure and take into account these nuances, we consider two events to be equal if they have the same verb lemma and share at least one argument other than the subject.</p>
    <p>P(e2|e1) = Count(e1, e2)</p>
    <p>Count(e1) (2)</p>
    <p>Event-SCP. We use the Symmetric Conditional Probability between event tuples (Rel-grams) used in (Balasubramanian et al., 2013) as another baseline method. The Rel-gram model is the most relevant previous work to our method and outperforms the previous state of the art on generating narrative event schema. This metric combines bigram probability considering both directions:</p>
    <p>SCP(e1, e2) = P(e2|e1)  P(e1|e2) (3)</p>
    <p>Like Event-Bigram, we used MLE for estimating Event-SCP from the corpus.</p>
    <p>Label Rel-gram Tuples Contingent &amp; Strongly Relevant 7 % Contingent &amp; Somewhat Relevant 0 % Contingent &amp; Not Relevant 35 % Total Contingent 42 %</p>
    <p>Table 4: Evaluation of Rel-gram tuples on AMT.</p>
    <p>We conducted three sets of experiments to evaluate different aspects of our work. First, we compare the content of our topic-specific event pairs to current state of the art event collections to show that the fine-grained knowledge we learned about everyday events does not exist in previous work focused on the news genre. Second, we run an automatic evaluation test, modeled after the COPA task (Roemmele et al., 2011), on a held-out test set to evaluate the event pair collections that we have extracted from both General-Domain and Topic-Specific datasets, in terms of contingency relations. We hypothesize that the contingent event pairs can be used as basic elements for generating coherent event chains and narrative schema. So, in the third part of the experiments, we extract topic-indicative contingent event pairs from our Topic-Specific dataset and run an experiment on Amazon Mechanical Turk (AMT) to evaluate the top N pairs with respect to their contingency relation and topic-relevance.</p>
    <p>and does not consider the causal relation between events for inducing event schema. We compare the content of what we learned from our topic-specific corpus to the Rel-gram tuples to show that the finegrained type of knowledge that we learn is not found in their events collection. We also applied the cooccurrence statistics that they used on our data as a</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>u Event-Unigram</p>
    <p>u Produce a distribution of normalized frequencies for events</p>
    <p>u Event-Bigram</p>
    <p>u Bigram probability of every pair of adjacent events using skip-2 bigram model</p>
    <p>u Event-SCP</p>
    <p>u Symmetric Conditional Probability between event tuples (Balasubramanian et al., 2013)</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>u General-domain dataset</p>
    <p>u Train (4,000 stories)</p>
    <p>u Held-out test (200 stories)</p>
    <p>u Topic-specific dataset</p>
    <p>Topic Dataset # Docs Camping Hand-labeled held-out test 107 Trip Hand-labeled train (Train-HL) 192</p>
    <p>Train-HL + Bootstrap (Train-HL-BS) 1,062 Storm Hand-labeled held-out test 98</p>
    <p>Hand-labeled train (Train-HL) 263 Train-HL + Bootstrap (Train-HL-BS) 1,234</p>
    <p>Table 5: Number of stories in the train and test sets from topic-specific dataset.</p>
    <p>baseline (Event-SCP) for comparison to our method and present the results in Sec. 4.2. In this experiment we compare the event pairs ex</p>
    <p>tracted from our Camping Trip topic to the Rel-gram tuples. The Rel-gram tuples are not sorted by topic. To find tuples relevant to Camping Trip, we used our top 10 indicative events and extracted all the Rel-gram tuples that included at least one event corresponding to one of the Camping Trip indicative events. For example, for go(dobj:camp), we pulled out all the tuples that included this event from the Rel-grams collection. The indicative events for each topic were automatically generated during the bootstrapping using AutoSlog-TS (Sec. 2). Then we applied the same sorting and filtering</p>
    <p>methods presented in the Rel-grams work and removed any tuple with frequency less than 25 and sorted the rest by the total symmetrical conditional probability. These numbers are publicly available as a part of the Rel-grams collection. We evaluated the top N = 100 tuples of this list using the Mechanical Turk task described later in Sec. 4.3. The evaluation results presented in Table 4 show that 42% of the Rel-gram pairs were labeled as contingent by the annotators and only 7% were both contingent and topic-relevant. We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.</p>
    <p>Model Accuracy Event-Unigram 0.478 Event-Bigram 0.481 Event-SCP (Rel-gram) 0.477 Causal Potential 0.510</p>
    <p>Table 6: Automatic two-choice test results for General-Domain dataset.</p>
    <p>Topic Model Train Dataset Accuracy Camping Event-Unigram Train-HL-BS 0.507 Trip Event-Bigram Train-HL-BS 0.510</p>
    <p>Event-SCP Train-HL-BS 0.508 Causal Potential Train-HL 0.631 Causal Potential Train-HL-BS 0.685</p>
    <p>Storm Event-Unigram Train-HL-BS 0.510 Event-Bigram Train-HL-BS 0.523 Event-SCP Train-HL-BS 0.516 Causal Potential Train-HL 0.711 Causal Potential Train-HL-BS 0.887</p>
    <p>Table 7: Automatic two-choice test results for Topic-Specific dataset.</p>
    <p>occurring in the test set. The following is an example of a question from the Camping Trip test set:</p>
    <p>Question event: arrange (dobj:outdoor) Choice 1: help (dobj:trip) Choice 2: call (subj:PERSON)</p>
    <p>In this example, arrange (dobj:outdoor) is followed by the event help (dobj:trip) in a document from the test set and call (subj:PERSON) was randomly generated. The model is supposed to predict which of the two choices is more likely to have a contingency relation with the event in the question. We argue that a strong contingency model should be able to choose the correct answer (the one that is adjacent to the question event) and the accuracy achieved on the test questions is an indication of the models robustness. For the General-Domain dataset, we split the data</p>
    <p>into train (4,000 stories) and held-out test (200 stories) sets. For each topic-specific set, we divided the hand-labeled data into a train (Train-HL) and heldout test, and created a second train set consisting of Train-HL and the data collected by bootstrapping (Train-HL-BS) as shown in Table 5. We automatically created a question for every event occurring in the test data which resulted in 3,123 questions for General-Domain data, 2,058 for the Camping and 2,533 questions for the Storm topic. For each dataset, we applied the baseline meth</p>
    <p>ods and Causal Potential model on the train sets to</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>u CP results stronger than all the baselines</p>
    <p>u Results on topic-specific dataset is significantly stronger than general-domain narratives</p>
    <p>u More training data collected by bootstrapping improves the accuracy</p>
    <p>Topic Dataset # Docs Camping Hand-labeled held-out test 107 Trip Hand-labeled train (Train-HL) 192</p>
    <p>Train-HL + Bootstrap (Train-HL-BS) 1,062 Storm Hand-labeled held-out test 98</p>
    <p>Hand-labeled train (Train-HL) 263 Train-HL + Bootstrap (Train-HL-BS) 1,234</p>
    <p>Table 5: Number of stories in the train and test sets from topic-specific dataset.</p>
    <p>baseline (Event-SCP) for comparison to our method and present the results in Sec. 4.2. In this experiment we compare the event pairs ex</p>
    <p>tracted from our Camping Trip topic to the Rel-gram tuples. The Rel-gram tuples are not sorted by topic. To find tuples relevant to Camping Trip, we used our top 10 indicative events and extracted all the Rel-gram tuples that included at least one event corresponding to one of the Camping Trip indicative events. For example, for go(dobj:camp), we pulled out all the tuples that included this event from the Rel-grams collection. The indicative events for each topic were automatically generated during the bootstrapping using AutoSlog-TS (Sec. 2). Then we applied the same sorting and filtering</p>
    <p>methods presented in the Rel-grams work and removed any tuple with frequency less than 25 and sorted the rest by the total symmetrical conditional probability. These numbers are publicly available as a part of the Rel-grams collection. We evaluated the top N = 100 tuples of this list using the Mechanical Turk task described later in Sec. 4.3. The evaluation results presented in Table 4 show that 42% of the Rel-gram pairs were labeled as contingent by the annotators and only 7% were both contingent and topic-relevant. We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.</p>
    <p>Model Accuracy Event-Unigram 0.478 Event-Bigram 0.481 Event-SCP (Rel-gram) 0.477 Causal Potential 0.510</p>
    <p>Table 6: Automatic two-choice test results for General-Domain dataset.</p>
    <p>Topic Model Train Dataset Accuracy Camping Event-Unigram Train-HL-BS 0.507 Trip Event-Bigram Train-HL-BS 0.510</p>
    <p>Event-SCP Train-HL-BS 0.508 Causal Potential Train-HL 0.631 Causal Potential Train-HL-BS 0.685</p>
    <p>Storm Event-Unigram Train-HL-BS 0.510 Event-Bigram Train-HL-BS 0.523 Event-SCP Train-HL-BS 0.516 Causal Potential Train-HL 0.711 Causal Potential Train-HL-BS 0.887</p>
    <p>Table 7: Automatic two-choice test results for Topic-Specific dataset.</p>
    <p>occurring in the test set. The following is an example of a question from the Camping Trip test set:</p>
    <p>Question event: arrange (dobj:outdoor) Choice 1: help (dobj:trip) Choice 2: call (subj:PERSON)</p>
    <p>In this example, arrange (dobj:outdoor) is followed by the event help (dobj:trip) in a document from the test set and call (subj:PERSON) was randomly generated. The model is supposed to predict which of the two choices is more likely to have a contingency relation with the event in the question. We argue that a strong contingency model should be able to choose the correct answer (the one that is adjacent to the question event) and the accuracy achieved on the test questions is an indication of the models robustness. For the General-Domain dataset, we split the data</p>
    <p>into train (4,000 stories) and held-out test (200 stories) sets. For each topic-specific set, we divided the hand-labeled data into a train (Train-HL) and heldout test, and created a second train set consisting of Train-HL and the data collected by bootstrapping (Train-HL-BS) as shown in Table 5. We automatically created a question for every event occurring in the test data which resulted in 3,123 questions for General-Domain data, 2,058 for the Camping and 2,533 questions for the Storm topic. For each dataset, we applied the baseline meth</p>
    <p>ods and Causal Potential model on the train sets to</p>
    <p>Topic Dataset # Docs Camping Hand-labeled held-out test 107 Trip Hand-labeled train (Train-HL) 192</p>
    <p>Train-HL + Bootstrap (Train-HL-BS) 1,062 Storm Hand-labeled held-out test 98</p>
    <p>Hand-labeled train (Train-HL) 263 Train-HL + Bootstrap (Train-HL-BS) 1,234</p>
    <p>Table 5: Number of stories in the train and test sets from topic-specific dataset.</p>
    <p>baseline (Event-SCP) for comparison to our method and present the results in Sec. 4.2. In this experiment we compare the event pairs ex</p>
    <p>tracted from our Camping Trip topic to the Rel-gram tuples. The Rel-gram tuples are not sorted by topic. To find tuples relevant to Camping Trip, we used our top 10 indicative events and extracted all the Rel-gram tuples that included at least one event corresponding to one of the Camping Trip indicative events. For example, for go(dobj:camp), we pulled out all the tuples that included this event from the Rel-grams collection. The indicative events for each topic were automatically generated during the bootstrapping using AutoSlog-TS (Sec. 2). Then we applied the same sorting and filtering</p>
    <p>methods presented in the Rel-grams work and removed any tuple with frequency less than 25 and sorted the rest by the total symmetrical conditional probability. These numbers are publicly available as a part of the Rel-grams collection. We evaluated the top N = 100 tuples of this list using the Mechanical Turk task described later in Sec. 4.3. The evaluation results presented in Table 4 show that 42% of the Rel-gram pairs were labeled as contingent by the annotators and only 7% were both contingent and topic-relevant. We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.</p>
    <p>Model Accuracy Event-Unigram 0.478 Event-Bigram 0.481 Event-SCP (Rel-gram) 0.477 Causal Potential 0.510</p>
    <p>Table 6: Automatic two-choice test results for General-Domain dataset.</p>
    <p>Topic Model Train Dataset Accuracy Camping Event-Unigram Train-HL-BS 0.507 Trip Event-Bigram Train-HL-BS 0.510</p>
    <p>Event-SCP Train-HL-BS 0.508 Causal Potential Train-HL 0.631 Causal Potential Train-HL-BS 0.685</p>
    <p>Storm Event-Unigram Train-HL-BS 0.510 Event-Bigram Train-HL-BS 0.523 Event-SCP Train-HL-BS 0.516 Causal Potential Train-HL 0.711 Causal Potential Train-HL-BS 0.887</p>
    <p>Table 7: Automatic two-choice test results for Topic-Specific dataset.</p>
    <p>occurring in the test set. The following is an example of a question from the Camping Trip test set:</p>
    <p>Question event: arrange (dobj:outdoor) Choice 1: help (dobj:trip) Choice 2: call (subj:PERSON)</p>
    <p>In this example, arrange (dobj:outdoor) is followed by the event help (dobj:trip) in a document from the test set and call (subj:PERSON) was randomly generated. The model is supposed to predict which of the two choices is more likely to have a contingency relation with the event in the question. We argue that a strong contingency model should be able to choose the correct answer (the one that is adjacent to the question event) and the accuracy achieved on the test questions is an indication of the models robustness. For the General-Domain dataset, we split the data</p>
    <p>into train (4,000 stories) and held-out test (200 stories) sets. For each topic-specific set, we divided the hand-labeled data into a train (Train-HL) and heldout test, and created a second train set consisting of Train-HL and the data collected by bootstrapping (Train-HL-BS) as shown in Table 5. We automatically created a question for every event occurring in the test data which resulted in 3,123 questions for General-Domain data, 2,058 for the Camping and 2,533 questions for the Storm topic. For each dataset, we applied the baseline meth</p>
    <p>ods and Causal Potential model on the train sets to</p>
    <p>General-Domain Stories</p>
  </div>
  <div class="page">
    <p>Compare Camping Trip Event Pairs against the Rel-gram tuples</p>
    <p>u Find tuples relevant to Camping Trip</p>
    <p>u Used our top 10 indicative event-patterns, generated and ranked during the bootstrapping</p>
    <p>u Apply filtering and ranking</p>
    <p>u Evaluate top N = 100</p>
    <p>go (dobj: camping)</p>
  </div>
  <div class="page">
    <p>Evaluation on Mechanical Turk</p>
    <p>u New method for evaluating topic-specific contingent event pairs</p>
    <p>u Rate each pair 0: The events are not contingent 1: The events are contingent but not relevant to the specified topic 2: The events are contingent and somewhat relevant to the specified topic 3: The events are contingent and strongly relevant to the specified topic</p>
    <p>u More readable representation for annotators:</p>
    <p>Subject - Verb Particle - Direct Object pack (subj:PERSON, dobj:car, prt: up)  person  pack up - car</p>
  </div>
  <div class="page">
    <p>Rel-gram Evaluation Results</p>
    <p>Label &gt;2: Contingent &amp; strongly topic-relevant Label = 2: Contingent &amp; somewhat topic-relevant 1  Label &lt; 2: Contingent &amp; not topic-relevant Label &lt; 1: Not contingent</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 18</p>
    <p>seem adequate. For example, consider the following events:</p>
    <p>go (subj:PERSON, dobj:camp) go (subj:family, dobj:camp) go (dobj:camp)</p>
    <p>They encode the same action although their representations do not exactly match and differ in the subject. Our intuition is that when we count the number of events represented as go (subj:PERSON, dobj:camp) we should also include the count of go (dobj:camp). To be able to generalize over the event structure and take into account these nuances, we consider two events to be equal if they have the same verb lemma and share at least one argument other than the subject.</p>
    <p>P(e2|e1) = Count(e1, e2)</p>
    <p>Count(e1) (2)</p>
    <p>Event-SCP. We use the Symmetric Conditional Probability between event tuples (Rel-grams) used in (Balasubramanian et al., 2013) as another baseline method. The Rel-gram model is the most relevant previous work to our method and outperforms the previous state of the art on generating narrative event schema. This metric combines bigram probability considering both directions:</p>
    <p>SCP(e1, e2) = P(e2|e1)  P(e1|e2) (3)</p>
    <p>Like Event-Bigram, we used MLE for estimating Event-SCP from the corpus.</p>
    <p>Label Rel-gram Tuples Contingent &amp; Strongly Relevant 7 % Contingent &amp; Somewhat Relevant 0 % Contingent &amp; Not Relevant 35 % Total Contingent 42 %</p>
    <p>Table 4: Evaluation of Rel-gram tuples on AMT.</p>
    <p>We conducted three sets of experiments to evaluate different aspects of our work. First, we compare the content of our topic-specific event pairs to current state of the art event collections to show that the fine-grained knowledge we learned about everyday events does not exist in previous work focused on the news genre. Second, we run an automatic evaluation test, modeled after the COPA task (Roemmele et al., 2011), on a held-out test set to evaluate the event pair collections that we have extracted from both General-Domain and Topic-Specific datasets, in terms of contingency relations. We hypothesize that the contingent event pairs can be used as basic elements for generating coherent event chains and narrative schema. So, in the third part of the experiments, we extract topic-indicative contingent event pairs from our Topic-Specific dataset and run an experiment on Amazon Mechanical Turk (AMT) to evaluate the top N pairs with respect to their contingency relation and topic-relevance.</p>
    <p>and does not consider the causal relation between events for inducing event schema. We compare the content of what we learned from our topic-specific corpus to the Rel-gram tuples to show that the finegrained type of knowledge that we learn is not found in their events collection. We also applied the cooccurrence statistics that they used on our data as a</p>
  </div>
  <div class="page">
    <p>Topic-Specific Contingent Event Pairs</p>
    <p>u Two filtering methods</p>
    <p>u Selected the frequent pairs for each topic and removed the ones that occur less than 5 times</p>
    <p>u Used the indicative event-patterns for each topic and extracted the pairs that at least included one of these patterns</p>
    <p>u Rank by Causal Potential scores to identify the highly contingent ones</p>
    <p>u Evaluated the top N = 100 pairs on Mechanical Turk task</p>
  </div>
  <div class="page">
    <p>Topic-Specific Pairs Evaluation Results</p>
    <p>u Inter-annotator reliability</p>
    <p>u average kappa = 0.73 (substantial agreement)</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 20</p>
    <p>Figure 2: Examples of event pairs with high CP scores extracted from General-Domain stories.</p>
    <p>learn contingent event pairs and tested the pair collections on the questions generated from held-out test set. We extracted about 418K contingent event pairs from General-Domain train set, 437K from Storm Train-HL-BS and 630K pairs from Camping Trip Train-HL-BS set using Causal Potential model. We used our automatic test approach to evaluate these event pair collections. The results for GeneralDomain and Topic-Specific datasets are shown in Table 6 and Table 7 respectively. The Causal Potential model trained on Train-HL</p>
    <p>BS dataset achieved accuracy of 0.685 on Camping Trip and 0.887 on Storm topic which is significantly stronger than all the baselines. Our experiments indicate that having more training data collected by bootstrapping improves the accuracy of the model in predicting contingency relation between events. Additionally, the Causal Potential results on Topic-Specific dataset is significantly stronger than General-Domain narratives indicating that using a topic-sorted dataset improves learning causal knowledge about events. Fig. 2 shows some examples of event pairs with high CP scores extracted from general-Domain set. In the following section we extract topic-indicative contingent event pairs and show that Topic-Specific data enables learning of finer-grained event knowledge that pertain to a particular theme.</p>
    <p>We identify contingent event pairs that are highly indicative of a particular topic. We hypothesize that these event pairs serve as building blocks of coherent event chains and narrative schema since they encode contingency relation and correspond to a specific theme. We evaluate the pairs on Amazon Mechanical Turk (AMT). To identify event sequences that have a strong</p>
    <p>correlation to a topic (topic-indicative pairs) we applied two filtering methods. First, we selected the frequent pairs for each topic and removed the ones</p>
    <p>Label Camping Storm Contingent &amp; Strongly Relevant 44 % 33 % Contingent &amp; Somewhat Relevant 8 % 20 % Contingent &amp; Not Relevant 30 % 24 % Total Contingent 82 % 77 %</p>
    <p>Table 8: Results of evaluating indicative contingent event pairs on AMT.</p>
    <p>that occur less than 5 times in the corpus. Second, we used the indicative event-patterns for each topic and extracted the pairs that at least included one of these patterns. Indicative event-patterns are automatically generated during the bootstrapping using AutoSlog-TS and mapped to their corresponding event representation as described in Sec. 2. Then we used the Causal Potential scores from our contingency model for ranking the topic-indicative event pairs to identify the highly contingent ones. We sorted the pairs based on the Causal Potential score and evaluated the top N pairs in this list.</p>
    <p>Evaluations and Results. We evaluate the indicative contingent event pairs using human judgment on Amazon Mechanical Turk (AMT). Narrative schema consists of chains of events that are related in a coherent way and correspond to a common theme. Consequently, we evaluate the extracted pairs based on two main criteria:</p>
    <p>Contingency: Two events in the pair are likely to occur together in the given order and the second event is contingent upon the first one.</p>
    <p>Topic Relevance: Both events strongly correspond to the specified topic.</p>
    <p>We have designed one task to assess both criteria since if an event pair is not contingent, it cannot be used in narrative schema for not satisfying the required coherence (even if it is topic-relevant). We asked the AMT annotators to rate each pair on a scale of 0-3 as follows:</p>
    <p>To ensure that the Amazon Mechanical Turk annotations are reliable, we designed a Qualification</p>
  </div>
  <div class="page">
    <p>Examples of Event Pairs</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 21</p>
    <p>Topic-Specific Dataset General-Domain Dataset</p>
    <p>person - go  go down - trail</p>
    <p>person - find - fellow  go back</p>
    <p>person - see - gun  see - police</p>
    <p>person - go  person - walk down</p>
    <p>climb  person - find - rock</p>
    <p>person - pack up - car  head out</p>
    <p>wind - blow - transformer  power - go out</p>
    <p>tree - fall - eave  crush</p>
    <p>hit - location  evacuate - person</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>u Learned new type of knowledge</p>
    <p>u Common-sense knowledge about everyday events focused on contingency relation</p>
    <p>u Data collection</p>
    <p>u Semi-supervised bootstrapping approach create topic-sorted dataset</p>
    <p>u New evaluation methods</p>
    <p>u Two-choice test and Mechanical Turk task</p>
    <p>u Results</p>
    <p>u On topic-specific dataset is significantly stronger than general-domain</p>
    <p>u Method used on the news genre do not work as well on personal stories</p>
    <p>u Fine-grained relations we learn are not found in existing event collections</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 22</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Natural Language and Dialougue Systems UC Santa Cruz 23</p>
  </div>
</Presentation>
