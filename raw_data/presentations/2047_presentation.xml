<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Toward Efficient Support for Multithreaded MPI Communication</p>
    <p>Pavan Balaji1, Darius Buntinas1, David Goodell1, William Gropp2, and Rajeev Thakur1</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Multicore processors are ubiquitous</p>
    <p>Heading towards 10's or 100's of cores per chip  Programming models for multicore clusters</p>
    <p>Pure message-passing  Across cores and nodes  Leads to large process count</p>
    <p>Problem may not scale to that many processes  Resource constraints</p>
    <p>Memory per process  TLB entries</p>
    <p>Hybrid  Shared-memory across cores &amp; message passing between nodes  Fewer processes  Use threads: POSIX threads, OpenMP  MPI functions can be called from multiple threads</p>
  </div>
  <div class="page">
    <p>Motivation (cont'ed)</p>
    <p>Providing thread support in MPI is essential</p>
    <p>Implementing this is non-trivial  At larger scale, just providing thread support is not sufficient</p>
    <p>Providing efficient thread support is critical  Concurrency within the library</p>
    <p>We describe our efforts to optimize performance of threads in MPICH2  Four approaches</p>
    <p>Decreasing critical-section granularity  Increasing concurrency</p>
    <p>Send side</p>
    <p>Evaluate each approach using message-rate benchmark</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Thread Support in MPI  Lock Granularity</p>
    <p>Levels of Granularity  Analyzing Impact of Granularity on Message Rate  Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Thread Support in MPI</p>
    <p>User specifies what level of thread safety is desired</p>
    <p>MPI_THREAD_SINGLE, _FUNNELED, _SERIALIZED or _MULTIPLE  _FUNNELED and _SERIALIZED</p>
    <p>Trivial to implement once you have _SINGLE*  No performance overhead over _SINGLE*</p>
    <p>MPI_THREAD_MULTIPLE  MPI library implementation needs to be thread safe</p>
    <p>Serialize access to shared structures, avoid races  Typically done using locks</p>
    <p>Locks can have a significant performance impact  Lock overhead can increase latency of each call  Locks can impact concurrency</p>
    <p>*usually</p>
  </div>
  <div class="page">
    <p>Coarse Grained Locking</p>
    <p>Single global mutex  Mutex is held between entry and exit of most MPI_ functions  No concurrency in communication</p>
  </div>
  <div class="page">
    <p>Lock Granularity</p>
    <p>Using mutexes can affect concurrency between threads</p>
    <p>Coarser granularity  less concurrency  Finer granularity  more concurrency</p>
    <p>Fine grained locking</p>
    <p>Decreasing size of critical sections increases concurrency  Only hold mutex when you need it</p>
    <p>Using multiple mutexes can also increase concurrency  Use separate mutexes for different critical sections</p>
    <p>Acquiring and releasing mutexes has overhead  Increasing granularity increases overhead</p>
    <p>Shrinking CS, using multiple mutexes increases complexity  Checking for races is more difficult  Need to worry about deadlocks</p>
  </div>
  <div class="page">
    <p>Levels of Granularity</p>
    <p>Global  Use a single global mutex, held from function enter to exit  Existing implementation</p>
    <p>Brief Global  Use a single global mutex, but reduce the size of the critical section</p>
    <p>as much as possible  Per-Object</p>
    <p>Use one mutex per data object: lock data not code sections  Lock-Free</p>
    <p>Use no mutexes  Use lock-free algorithms  Future work</p>
  </div>
  <div class="page">
    <p>Analyzing Impact of Granularity on Message Rate</p>
    <p>Measured message rate for multithreaded sender to single-threaded receivers  100,000 times:</p>
    <p>Sender: MPI_Send() 128 messages, MPI_Recv() ack  Receiver: MPI_Irecv() 128 messages, MPI_Send() ack</p>
    <p>Threads Processes</p>
  </div>
  <div class="page">
    <p>Global Granularity</p>
    <p>Single global mutex  Mutex is held between entry and exit of most MPI_ functions  This prevents any thread concurrency in communication</p>
  </div>
  <div class="page">
    <p>Global Granularity</p>
    <p>Holding mutex</p>
    <p>Waiting for mutex</p>
    <p>MPI_Send()</p>
    <p>Access data structure 1</p>
    <p>Access data structure 2</p>
  </div>
  <div class="page">
    <p>Brief Global Granularity</p>
    <p>Single global mutex  Reduced the size of the critical section</p>
    <p>Acquire mutex just before first access to shared data  Release mutex just after last access to shared data</p>
    <p>If no shared data is accessed, no need to lock  Sending to MPI_PROC_NULL accesses no shared data</p>
    <p>Reduced time mutex is held  Still using global mutex</p>
  </div>
  <div class="page">
    <p>Brief Global Granularity: Sending to MPI_PROC_NULL</p>
    <p>Holding mutex</p>
    <p>Waiting for mutex</p>
    <p>MPI_Send()</p>
    <p>Access data structure 1</p>
  </div>
  <div class="page">
    <p>Sending to MPI_PROC_NULL</p>
    <p>No actual communication performed</p>
    <p>No shared data is accessed  No need to lock mutex  Brief global avoids locking unnecessarily</p>
    <p>Performs as well as Processes</p>
  </div>
  <div class="page">
    <p>Brief Global Granularity: Real Communication</p>
    <p>Holding mutex</p>
    <p>Waiting for mutex</p>
    <p>MPI_Send()</p>
    <p>Access data structure 1</p>
    <p>Access data structure 2</p>
  </div>
  <div class="page">
    <p>Blocking Send: Real Communication</p>
    <p>Perform real communication</p>
    <p>Shared data accessed  Brief global still uses global lock</p>
    <p>Performs poorly</p>
  </div>
  <div class="page">
    <p>Per-Object Granularity</p>
    <p>Multiple mutexes  Lock data objects not code sections</p>
    <p>One mutex per object</p>
    <p>Threads can access different objects concurrently  Different data object for each destination</p>
    <p>Further reduced time mutex is held  Acquire data object's mutex just before accessing it  Then immediately release it</p>
    <p>Reduced time mutex is held  Concurrent access to separate data  Increased lock overhead  Can still contend on globally accessed data structures</p>
  </div>
  <div class="page">
    <p>Per-Object Granularity</p>
    <p>Holding mutex 1 Holding mutex 2</p>
    <p>Waiting for mutex</p>
    <p>MPI_Send()</p>
    <p>Access data structure 1</p>
    <p>Access data structure 2</p>
  </div>
  <div class="page">
    <p>Blocking Sends</p>
    <p>Perform real communication  Shared data is accessed, but not concurrently</p>
    <p>Brief global performs as poorly as global: contention on single lock  Per-Object: one mutex per VC  no contention on mutex</p>
  </div>
  <div class="page">
    <p>What About Globally Shared Objects?</p>
    <p>Per-Object can still have contention on global structures</p>
    <p>Allocate from global request pool  MPI_COMM_WORLD ref counting</p>
    <p>Use thread-local request pool  Each thread allocates and frees from a private pool of requests</p>
    <p>Also use atomic asm instructions for updating reference counts  Ref count updates are short but frequent  Atomic increment  Atomic test-and-decrement</p>
  </div>
  <div class="page">
    <p>Per-Object Granularity: Non-Blocking Sends</p>
    <p>Holding mutex 1</p>
    <p>Access data structure 3</p>
    <p>Holding mutex 2</p>
    <p>Waiting for mutex</p>
    <p>MPI_Isend()</p>
    <p>Access data structure 1</p>
    <p>Holding mutex 3</p>
    <p>Access data structure 2</p>
  </div>
  <div class="page">
    <p>Non-Blocking Sends</p>
    <p>Non-blocking sends access global data  Requests are allocated from a global request pool</p>
    <p>Alloc and free of req requires updating reference count on communicator  Per-object tlp: thread-local request pool  Per-object tlp atom: use tlp and atomic ref count updates  Still some contention exists</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We implemented and evaluated different levels of lock granularity</p>
    <p>Finer granularity levels allow more concurrency  Finer granularity increases complexity</p>
    <p>Per-object uses multiple locks  Hard to check we're holding the right lock  Deadlocks</p>
    <p>With Global, a routine might always have been called with a lock held  With Per-object, not always true  Recursive locks</p>
    <p>Can't check for double locking  Lock-free solutions are critical for performance</p>
    <p>Global objects must be accessed  Atomic ref count updates</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Address receive side</p>
    <p>Receive queues  Atomically search unexpected queue then enqueue on posted  Lock free or speculative approaches  Per source receive queues</p>
    <p>Multithreaded receive  Only one thread can call poll() or recv()  How to efficiently dispatch incoming messages to different threads?  Ordering</p>
    <p>Lock free</p>
    <p>Eliminate locks altogether  Increases complexity</p>
    <p>Eliminate locks but increase instruction count  Verification  We have started work on a portable atomics library</p>
  </div>
  <div class="page">
    <p>For more information...</p>
    <p>http://www.mcs.anl.gov/research/projects/mpich2</p>
    <p>{balaji, buntinas, goodell, thakur}@mcs.anl.gov  wgropp@illinois.edu</p>
  </div>
</Presentation>
