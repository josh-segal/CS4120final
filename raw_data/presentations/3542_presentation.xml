<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</p>
    <p>Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli</p>
    <p>Usenix Security Symposium 2019, Aug. 14-16, Santa Clara, California, USA</p>
    <p>Northeastern University,</p>
    <p>Boston, MA, USA</p>
    <p>Pattern Recognition and Applications Lab</p>
    <p>University of Cagliari, Italy</p>
  </div>
  <div class="page">
    <p>Attacks against machine learning</p>
  </div>
  <div class="page">
    <p>Evasion: add minimum amount of perturbation to a test point to change prediction</p>
    <p>Poisoning: add a fraction of poisoning points in training to degrade model accuracy (availability attack)</p>
    <p>Attacker Knowledge</p>
    <p>White box: full knowledge of the ML system</p>
    <p>Black-box: query access to the model</p>
    <p>Threat model</p>
  </div>
  <div class="page">
    <p>Why study transferability?</p>
    <p>Transferability: the ability of an attack, crafted against a surrogate model, to be effective against a different, unknown target model [1,2]</p>
    <p>Open problems:  What are the factors behind the transferability of evasion and</p>
    <p>poisoning attacks?  When and why do adversarial attacks transfer?</p>
    <p>target model</p>
    <p>surrogate model</p>
    <p>is the attack effective?</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Optimization framework for evasion and poisoning attacks</p>
    <p>Transferability definition and theoretical bound</p>
    <p>Metric 1: Size of the input gradient</p>
    <p>Metric 2: Gradient alignment</p>
    <p>Metric 3: Variability of the loss landscape</p>
    <p>Comprehensive experimental evaluation of transferability</p>
    <p>Study the relationship between transferability and model complexity</p>
  </div>
  <div class="page">
    <p>Why complexity may influence transferability?</p>
    <p>Model complexity: The capacity of the classifier to fit the training data (can be controlled through regularization)</p>
  </div>
  <div class="page">
    <p>Loss attained by the target on an adversarial point crafted against the surrogate</p>
    <p>Our definition for transferability</p>
    <p>target</p>
    <p>target model</p>
    <p>surrogate model</p>
  </div>
  <div class="page">
    <p>Loss attained by the target on an adversarial point crafted against the surrogate</p>
    <p>Our definition for transferability</p>
    <p>surrogate 7</p>
    <p>Gradient-based optimization:</p>
    <p>Evasion: [Biggio et al. 13],</p>
    <p>[Szegedy et al. 14], [Goodfellow et al. 14],</p>
    <p>[Carlini and Wagner 17], [Madry et al. 18]</p>
    <p>Poisoning: [Biggio et al. 12, Suciu et al. 18]</p>
  </div>
  <div class="page">
    <p>Loss attained by the target on an adversarial point crafted against the surrogate</p>
    <p>Our definition for transferability</p>
    <p>target</p>
    <p>surrogate 7</p>
    <p>target model</p>
    <p>surrogate model</p>
  </div>
  <div class="page">
    <p>Loss attained by the target on an adversarial point crafted against the surrogate</p>
    <p>Our definition for transferability</p>
    <p>target</p>
    <p>surrogate 7</p>
    <p>=</p>
    <p>=</p>
    <p>R: gradient alignment</p>
    <p>measures black-box to</p>
    <p>white-box loss</p>
    <p>increment ratio</p>
    <p>S: size of input</p>
    <p>gradients measures</p>
    <p>white-box loss</p>
    <p>increment</p>
    <p>Poisoning attacks follow a similar derivation</p>
  </div>
  <div class="page">
    <p>Evaluates the loss increment  incurred by the target classifier under attack  Intuition: to capture sensitivity of the loss function to input</p>
    <p>perturbations, as also highlighted in previous work (at least for evasion attacks [1,2,3])</p>
    <p>Metric 1: Size of input gradients</p>
    <p>deep neural networks by regularizing their input gradients, AAAI 2018 3. C. J. Simon-Gabriel et al., Adversarial vulnerability of neural networks increases with input</p>
    <p>dimension, arXiv 2018</p>
  </div>
  <div class="page">
    <p>Evaluates the ratio</p>
    <p>between the loss increment incurred in the</p>
    <p>black-box case and that incurred in the white-box case</p>
    <p>Metric 2: Gradient alignment</p>
    <p>Black-box attack against the surrogate model</p>
    <p>White-box attack against the target model</p>
    <p>Gradient alignment</p>
  </div>
  <div class="page">
    <p>This metric evaluates the variability of the surrogate classifier under training data resampling</p>
    <p>Metric 3: Variability of the surrogate loss landscape</p>
  </div>
  <div class="page">
    <p>Experimental setup</p>
    <p>Datasets:</p>
    <p>Evasion: Drebin (Android Malware Detection)</p>
    <p>Poisoning: LFW (Face Verification task 1 vs 5)</p>
    <p>Evasion &amp; Poisoning: MNIST89</p>
    <p>Classifiers (8 surrogates, 12 target models):</p>
    <p>ridge, logistic regression, linear/RBF SVM, neural networks, random forests</p>
    <p>Experiments:</p>
    <p>White-box security evaluation</p>
    <p>Black-box security evaluation (all combinations of targets and surrogates)</p>
    <p>Correlation between the proposed metrics, transferability and model complexity</p>
    <p>Statistical tests</p>
  </div>
  <div class="page">
    <p>RQ1: Are target classifiers with larger input gradients more vulnerable?  How does model complexity affect the size of input gradients?</p>
    <p>Transferability of evasion attacks</p>
    <p>Complexity</p>
    <p>SVM-RBF H SVM-RBF L</p>
    <p>Gradient Size</p>
  </div>
  <div class="page">
    <p>RQ1: Are target classifiers with larger input gradients more vulnerable?  How does model complexity affect the size of input gradients?</p>
    <p>Higher complexity models have larger gradients</p>
    <p>Target with larger gradients are more vulnerable</p>
    <p>Transferability of evasion attacks</p>
  </div>
  <div class="page">
    <p>Transferability of evasion attacks</p>
    <p>RQ2: Is the gradient alignment correlated with the difference of the perturbations computed considering the target and the surrogate models?</p>
    <p>The gradient alignment metric is heavily correlated with the correlation between the perturbations</p>
    <p>gradient alignment perturbation correlation</p>
  </div>
  <div class="page">
    <p>Does model complexity impact poisoning?</p>
    <p>The findings are similar to evasion for input gradient and variability of loss landscape</p>
    <p>Differences from evasion:</p>
    <p>For poisoning the best surrogates are the ones with similar level of model complexity</p>
    <p>SVM L SVM H SVM-RBF L SVM-RBF H</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Transferability definition and metrics to investigate connections between attack transferability and complexity of target and surrogate models</p>
    <p>Extensive experiments on 3 datasets and 12 classifiers have shown that:  High-complexity models are more vulnerable to both evasion and</p>
    <p>poisoning attacks  Low-complexity models are better surrogates to perform evasion attacks  The complexity of the best surrogate is the same as the one of the target</p>
    <p>for availability poisoning</p>
    <p>Open-source code available within the Python library SecML:  Code: https://gitlab.com/secml/secml  Docs: https://secml.gitlab.io</p>
  </div>
</Presentation>
