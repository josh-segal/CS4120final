<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>N-best Reranking by</p>
    <p>Multitask Learning</p>
    <p>Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki Isozaki, Masaaki Nagata</p>
    <p>NTT Communication Science Laboratories</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 2</p>
    <p>Our Goal</p>
    <p>Incorporate millions of features into MT without overfitting!</p>
    <p>Features.</p>
    <p>The Computational Linguist</p>
    <p>Machine Translation</p>
    <p>System</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 3</p>
    <p>Main Ideas</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 4</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 5</p>
    <p>Background</p>
    <p>Goal: given f, score translations e based on:</p>
    <p>Were interested in systems employing millions of features</p>
    <p>N-best List FeaturesTrained weights</p>
    <p>Note: Here we focus on N-best reranking but extension to 1st-pass training is possible</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 6</p>
    <p>Sparse features for MT</p>
    <p>[Watanabe2007] proposed heavily-lexicalized features, e.g.</p>
    <p>Never used if input sentence does not contain Monsieur</p>
    <p>Many reordering possibilities  many potential features said Smith Mr., Smith Mr. said,..</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 7</p>
    <p>Why does overfitting occur?</p>
    <p>Because there exist very little feature overlap between any two N-best lists.</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 8</p>
    <p>Visualizing feature overlap (or lack thereof)</p>
    <p>Feature Growth Rate Definition: ratio of new-feature to active feature In the limit, 45% of active features are never seen before!</p>
    <p>Conditions for this long-tail behavior -Feature templates are heavily-lexicalized -Input (f) has high variability -Output (e) has high variability</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 9</p>
    <p>Outline</p>
    <p>What is multitask learning  How N-best can be viewed as multitask problem</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 10</p>
    <p>What is Multitask Learning?</p>
    <p>A set of machine learning techniques for exploiting heterogeneous training data</p>
    <p>Contrasts with i.i.d. assumption of traditional setup  Instead assumes some underlying commonality</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 11</p>
    <p>Examples of Tasks  Multiple domains:</p>
    <p>Multiple related problems:</p>
    <p>News Europarl Web</p>
    <p>God dag (Swedish)</p>
    <p>Hello (English)</p>
    <p>Goddag (Danish)</p>
    <p>Guten Tag (German)</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 12</p>
    <p>N-bests with sparse features can be viewed as a</p>
    <p>Multitask problem</p>
    <p>Nbest List 1</p>
    <p>Nbest List 2</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>Nbest List 3</p>
    <p>features</p>
    <p>features</p>
    <p>Feature Histogram</p>
    <p>Train a single weight w ? Is data i.i.d. across N-bests?</p>
    <p>Task 1</p>
    <p>Task 2</p>
    <p>Task 3</p>
    <p>NO! Data is heterogenous. Treat as multitask!</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 13</p>
    <p>Our Meta-Algorithm</p>
    <p>Nbest List 1</p>
    <p>Nbest List 2</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>STEP 1: Train weights independently for each N-best</p>
    <p>STEP 2: Find commonality among weights (and iterate)</p>
    <p>STEP 3: Train conventional reranker on discovered common features</p>
    <p>w1</p>
    <p>w2</p>
    <p>New Feature Representation</p>
    <p>Plug in your favorite Multitask Learning method</p>
    <p>Conventional Reranker</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 14</p>
    <p>L1/L2 Joint Regularization (one example multitask learning method)</p>
    <p>||W||1,2 computed by 1. Stacking the weights into a matrix 2. Take L2 norm on columns, then L1 norm on result Effect: encourage sharing of features</p>
    <p>=</p>
    <p>+ I</p>
    <p>i</p>
    <p>ii</p>
    <p>www</p>
    <p>WnbestwLoss I</p>
    <p>,..,,</p>
    <p>||||),(minarg 21</p>
    <p>Exercise: which is the better solution?</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 15</p>
    <p>Many multitask methods are available!</p>
    <p>Joint Regularization:  L1/L2 [Obozinski09, Argyriou08]  L1/L-infinity [Quattoni09]</p>
    <p>Bayesian Prior: [Daume09, Finkel09]</p>
    <p>Shared Feature Subspace:  SVD-based [Ando05]  Neural network [Caruana97]  Deep Learning [Collobert08]</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 16</p>
    <p>Outline</p>
    <p>Data  Results</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 17</p>
    <p>Data EnglishJapanese translation of PubMed abstracts</p>
    <p>Phrase-based Decoder</p>
    <p>Phrase table: 17k sentence pairs Language model: 800k sentences</p>
    <p>Nbest List 1</p>
    <p>Nbest List 2</p>
    <p>N=100 500 lists for train 100 lists for tune 500 lists for test</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 18</p>
    <p>Experiment comparison</p>
    <p>What is best feature representation?</p>
    <p>Specifics:  Base reranker is RankSVM, similar to [Shen04]  Original: 2.4 million features  Tune multitask feature dimension: {250,500,1000}</p>
    <p>Baselines:</p>
    <p>Features discovered by Multitask:</p>
    <p>VS.</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 19</p>
    <p>Results</p>
    <p>Test BLEU</p>
    <p>Train BLEU</p>
    <p>No. of features</p>
    <p>Feature Representation</p>
    <p>Improvements in red are statistically significant by bootstrap test (p&lt;0.05)</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 20</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 21</p>
    <p>Contributions</p>
    <p>N-best Lists with sparse features may be cast as multitask problem</p>
    <p>Proposed meta-algorithm uses multitask methods to learn better features for reranking</p>
    <p>Nbest List 1</p>
    <p>Nbest List 2</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>features</p>
    <p>Feature Histogram</p>
    <p>w1</p>
    <p>w2</p>
    <p>New Feature Representation</p>
    <p>Conventional Reranker</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 22</p>
    <p>Final Words</p>
    <p>Features.</p>
    <p>The Computational Linguist</p>
    <p>Machine Translation</p>
    <p>System</p>
    <p>But we must avoid overfitting: 1. Careful definition of features:</p>
    <p>e.g. [Chiang09,Marton08] 2. Feature mining</p>
    <p>[This work]</p>
    <p>MORE FEATURES IS THE WAY TO GO: Translation is a delicate process requiring many fine-grained knowledge</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 23</p>
    <p>Thanks! Questions? Suggestions?  Citations:</p>
    <p>[Ando05]: A framework for learning predictive structures from multiple tasks, JMLR</p>
    <p>[Argyriou08]: Convex multitask feature learning, MLJ  [Chiang09]: 11,001 new features for SMT, NAACL  [Collobert08]: A unified architecture for NLP: deep neural networks with</p>
    <p>multitask learning, ICML  [Daume09]: Bayesian multitask learing with latent hierarchices, UAI  [Marton08]: Soft syntactic constraints for hierarchical phrase based</p>
    <p>translation, ACL  [Finkel09]: Hierarchical Bayesian domain adaptation, NAACL  [Quattoni09]: An efficient projection for L1-Linf regularization, ICML  [Shen04]: Discriminative reranking for MT, NAACL  [Watanabe07]: Online large margin training for SMT, EMNLP</p>
    <p>Acknowledgments:  We thank Jun Suzuki, Shinji Watanabe, Albert Au Yeung and the three</p>
    <p>reviewers for their valuable comments!</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 24</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 25</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 26</p>
    <p>Open Questions</p>
    <p>Interactive feature engineering?</p>
    <p>Different partition of tasks?</p>
    <p>Multitask on lattices or larger N-bests?</p>
    <p>Comparison to online learning?</p>
  </div>
  <div class="page">
    <p>N-best Reranking by Multitask Learning 27</p>
    <p>A Bayesian perspective</p>
    <p>f is task-specific parameter  P(e|f) is common across tasks</p>
    <p>P(e|f)</p>
    <p>P(e|f=f1)</p>
    <p>f1</p>
    <p>Nbest List 1</p>
    <p>P(e|f=f2)</p>
    <p>f2</p>
    <p>Nbest List 2</p>
    <p>P(e|f=f3)</p>
    <p>f3</p>
    <p>Nbest List 3</p>
  </div>
</Presentation>
