<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Interactive Planning-based Cognitive Assistance on the Edge</p>
    <p>Zhiming Hu, Maayan Shvo, Allan Jepson and Iqbal Mohomed Samsung AI Centre, Toronto</p>
  </div>
  <div class="page">
    <p>What is cognitive assistance?  One of the most exciting applications in AR Glasses</p>
    <p>Google Glass, HoloLens 2</p>
    <p>Helpful in a myriad of tasks</p>
    <p>Health care education and training</p>
    <p>Industrial tool for remote support</p>
    <p>Cooking assistant and fitness coach</p>
    <p>2 Image source for HoloLens 2: https://commons.wikimedia.org/wiki/File:HoloLens_2.jpeg, https://creativecommons.org/licenses/by/2.0/legalcode, changes are not made on the image.</p>
  </div>
  <div class="page">
    <p>How to build a cognitive assistant?  Lots of existing work on building cognitive assistance [1,2,3,4]</p>
    <p>Perception module</p>
    <p>Determine the current task state</p>
    <p>Cognitive module</p>
    <p>Generate the next step</p>
    <p>3</p>
    <p>[1] VideoPipe: Building Video Stream Processing Pipelines at the Edge, Middleware 2019 [2] https://github.com/cmusatyalab/gabriel-sandwich [3] Mohan, S., Ramea, K., Price, B., Shreve, M., Eldardiry, H., &amp; Nelson, L. (2019). Building Jarvis-A Learner-Aware Conversational Trainer. In IUI Workshops. [4] Laird, John E. The Soar cognitive architecture. MIT press, 2012.</p>
  </div>
  <div class="page">
    <p>The motivation  While it is simple to build a state machine to guide a user to complete</p>
    <p>some tasks, there are several issues</p>
    <p>The state machine needs to be pre-defined</p>
    <p>It cannot list all the possible user errors, thus cannot recover from such failure cases.</p>
    <p>4</p>
    <p>Bread Ham Lettuce Tomato Bread</p>
    <p>Bread ?</p>
  </div>
  <div class="page">
    <p>How about a planner?  Benefits</p>
    <p>Flexible, can recover from any user errors</p>
    <p>Challenges</p>
    <p>Need to calculate accurate current task state (CTS)</p>
    <p>Not as computationally efficient as state machines.</p>
    <p>5</p>
  </div>
  <div class="page">
    <p>A planning problem  A planning problem may be encoded in PDDL by defining the domain, initial state,</p>
    <p>and goal state.</p>
    <p>\</p>
    <p>If all of the ingredients are clear and on the table, one possible solution is  = stack(ham,bread1),stack(lettuce,ham),stack(bread2,lettuce), stack(tomato,bread2),stack(bread3,tomato).</p>
    <p>6</p>
    <p>Classifier for the Top Object on the Sandwich</p>
    <p>Sequence of Classification Results: Bread -&gt; Ham -&gt; Bread</p>
    <p>Stack Bread on Ham OR</p>
    <p>Unstack Ham from Bread</p>
    <p>Figure 2: A case of ambiguity when deciding the current task state based on the information from the perceptual module.</p>
    <p>In the edge server, the frames will first go through a perceptual module, which could be a classifier or an object detector. The classification/detection results will then be used to compute the current task state. If there is any ambiguity in this step, our system will initiate a dialogue with the user to resolve the ambiguity. Finally, given the up-to-date current task state, the planner and the state tracker will generate the corresponding next step instruction to the user.</p>
    <p>We will use a sandwich assembly task as an example for the rest of the paper. This is the same task used in the Gabriel cognitive assistant and is a good toy example to consider the key aspects of the system. The user is given a set of ingredients (e.g. tomato, lettuce, bread) and given explicit instructions to assemble a particular sandwich.</p>
    <p>In the above case, the system can resolve the ambiguity in current task state by requesting for clarification from the user. Previous work [20] suggests that a clarification request should list all uncertain options when there exist no more than two options (e.g., Did you &lt;add/remove&gt; &lt;item1&gt; or &lt;item2&gt;?. Otherwise, humans prefer to be asked a Wh-question (e.g., What item did you &lt;add/remove&gt;?).</p>
    <p>generate a partially or totally ordered sequence of actions  a plan  which transforms some initially specified state of the world to a desired (goal) state. Research in the field, spanning many decades, has strived to produce general solutions to this problem, leading to planners which are not domain-specific. That is, general-purpose planners have been created, which are agnostic to problem and domain specific peculiarities and return a solution, a plan, given some input specified in a generic and standard format (e.g., the Planning Domain Definition Language (PDDL) [15]).</p>
    <p>Definition 1 (Planning Problem). A planning problem is a tuple of the form P = (F,A,I,G), where F is a finite set of fluent symbols, A is a set of actions, I  F defines the initial state, and G  F defines the goal state. Each action a 2 A is associated with a precondition, Prea, add effects, eff</p>
    <p>+ a</p>
    <p>, delete</p>
    <p>effects, eff  a</p>
    <p>, and non-negative action costs, COST(a).</p>
    <p>A state, s, is a set of fluents that are true (a fluent is a condition that can change over time). An action a 2 A is executable in a state s if Prea  s. The successor state is defined as d(a,s) =((s\ eff</p>
    <p>a ) [ eff +</p>
    <p>a ) for the executable actions. The</p>
    <p>sequence of actions p = [a1,...,an] is executable in s if the state s0 = d(an,d(an1,...,d(a1,s))) is defined. Moreover, p is the solution to a planning problem P if it is executable from I and G  d(an,d(an1,...,d(a1,I))).</p>
    <p>Given a user goal, G, we instantiate a planning problem P = (F,A,I,G) where I, A and F are predefined. We make assumptions pertaining to the initial state I that may or may not hold in the world. To illustrate, we partially model our sandwich example as a planning problem:</p>
    <p>stack(x,y) 2 A</p>
    <p>Prestack = {clear(x),clear(y),ontable(x)}  eff +</p>
    <p>stack = {on(x,y)} (note: x is on y)</p>
    <p>eff  stack</p>
    <p>= {clear(y)}</p>
    <p>G = {onTable(bread1),on(ham,bread1),on(lettuce,ham), on(bread2,lettuce),on(tomato,bread2),on(bread3,tomato)}</p>
    <p>Given that initially all of the ingredients are clear and on the table, one possible solution to the above problem is a plan p = stack(ham,bread1),stack(lettuce,ham),stack(bread2,lettuce), stack(tomato,bread2),stack(bread3,tomato).</p>
    <p>The system guides the user through the execution of p by providing her with the plan incrementally, one step at a time. In the beginning, we generate the initial plan p and build a state machine based on the plan. After that, the vision system is assumed to observe actions performed by the user (e.g., stack(lettuce,ham)) which are then used to update the current state of the world. More specifically, when the vision system observes some action a in some state s, the new state will be</p>
    <p>The key to get the correct plan is to obtain accurate current task state</p>
  </div>
  <div class="page">
    <p>Ambiguity Resolving  We keep track of the current task state by recognizing the actions taken</p>
    <p>since the beginning of the interaction.</p>
    <p>However, we may encounter ambiguous cases where we cannot determine which action was performed by the user.</p>
    <p>7</p>
    <p>Classifier for the Top Object on the Sandwich</p>
    <p>Sequence of Classification Results: Bread -&gt; Ham -&gt; Bread</p>
    <p>Stack Bread on Ham OR</p>
    <p>Unstack Ham from Bread</p>
    <p>Figure 2: A case of ambiguity when deciding the current task state based on the information from the perceptual module.</p>
    <p>In the edge server, the frames will first go through a perceptual module, which could be a classifier or an object detector. The classification/detection results will then be used to compute the current task state. If there is any ambiguity in this step, our system will initiate a dialogue with the user to resolve the ambiguity. Finally, given the up-to-date current task state, the planner and the state tracker will generate the corresponding next step instruction to the user.</p>
    <p>We will use a sandwich assembly task as an example for the rest of the paper. This is the same task used in the Gabriel cognitive assistant and is a good toy example to consider the key aspects of the system. The user is given a set of ingredients (e.g. tomato, lettuce, bread) and given explicit instructions to assemble a particular sandwich.</p>
    <p>In the above case, the system can resolve the ambiguity in current task state by requesting for clarification from the user. Previous work [20] suggests that a clarification request should list all uncertain options when there exist no more than two options (e.g., Did you &lt;add/remove&gt; &lt;item1&gt; or &lt;item2&gt;?. Otherwise, humans prefer to be asked a Wh-question (e.g., What item did you &lt;add/remove&gt;?).</p>
    <p>generate a partially or totally ordered sequence of actions  a plan  which transforms some initially specified state of the world to a desired (goal) state. Research in the field, spanning many decades, has strived to produce general solutions to this problem, leading to planners which are not domain-specific. That is, general-purpose planners have been created, which are agnostic to problem and domain specific peculiarities and return a solution, a plan, given some input specified in a generic and standard format (e.g., the Planning Domain Definition Language (PDDL) [15]).</p>
    <p>Definition 1 (Planning Problem). A planning problem is a tuple of the form P = (F,A,I,G), where F is a finite set of fluent symbols, A is a set of actions, I  F defines the initial state, and G  F defines the goal state. Each action a 2 A is associated with a precondition, Prea, add effects, eff</p>
    <p>+ a</p>
    <p>, delete</p>
    <p>effects, eff  a</p>
    <p>, and non-negative action costs, COST(a).</p>
    <p>A state, s, is a set of fluents that are true (a fluent is a condition that can change over time). An action a 2 A is executable in a state s if Prea  s. The successor state is defined as d(a,s) =((s\ eff</p>
    <p>a ) [ eff +</p>
    <p>a ) for the executable actions. The</p>
    <p>sequence of actions p = [a1,...,an] is executable in s if the state s0 = d(an,d(an1,...,d(a1,s))) is defined. Moreover, p is the solution to a planning problem P if it is executable from I and G  d(an,d(an1,...,d(a1,I))).</p>
    <p>Given a user goal, G, we instantiate a planning problem P = (F,A,I,G) where I, A and F are predefined. We make assumptions pertaining to the initial state I that may or may not hold in the world. To illustrate, we partially model our sandwich example as a planning problem:</p>
    <p>stack(x,y) 2 A</p>
    <p>Prestack = {clear(x),clear(y),ontable(x)}  eff +</p>
    <p>stack = {on(x,y)} (note: x is on y)</p>
    <p>eff  stack</p>
    <p>= {clear(y)}</p>
    <p>G = {onTable(bread1),on(ham,bread1),on(lettuce,ham), on(bread2,lettuce),on(tomato,bread2),on(bread3,tomato)}</p>
    <p>Given that initially all of the ingredients are clear and on the table, one possible solution to the above problem is a plan p = stack(ham,bread1),stack(lettuce,ham),stack(bread2,lettuce), stack(tomato,bread2),stack(bread3,tomato).</p>
    <p>The system guides the user through the execution of p by providing her with the plan incrementally, one step at a time. In the beginning, we generate the initial plan p and build a state machine based on the plan. After that, the vision system is assumed to observe actions performed by the user (e.g., stack(lettuce,ham)) which are then used to update the current state of the world. More specifically, when the vision system observes some action a in some state s, the new state will be</p>
  </div>
  <div class="page">
    <p>Dynamic State Tracking  A planner with state machines</p>
    <p>The planner will only be called when an unexpected action is detected</p>
    <p>8</p>
    <p>Stack Ham</p>
    <p>on Bread</p>
    <p>Stack Lettuce</p>
    <p>on Ham</p>
    <p>Stack Bread</p>
    <p>on Lettuce</p>
    <p>End</p>
    <p>Start</p>
    <p>Stack Lettuce</p>
    <p>on Ham</p>
    <p>Stack Bread</p>
    <p>on Lettuce</p>
    <p>End</p>
    <p>Stack Tomato</p>
    <p>on Ham</p>
    <p>Observed Activity</p>
    <p>Replanning</p>
    <p>Unstack Tomato</p>
    <p>from Ham</p>
    <p>Figure 3: State tracking with a planner and state machines. The green box shows the current expected action.</p>
    <p>the successor state d(a,s). Following the observation of a user action, we check whether the observed action a matches the current step of the plan that was given to the user. If yes, we will provide the next instruction directly based on the state machine and change the current state in the state machine. However, if not, we generate a new planning problem where I is the updated state d(a,s) and solve it to obtain p0. For instance, if the user made a mistake such as stacking tomato on ham instead of stacking lettuce on ham, p0 will correct the users mistake and instruct her to remove the tomato and place the lettuce instead. An illustrative figure is shown in Fig. 3. With the new p0, a new state machine is built based on the new list of instructions. The approach of integrating the planner with a state machine is advantageous since the system will not trigger the planner every time as long as the user is following the plan. Note that we employ here a simple approach to plan execution and monitoring; the rich body of related work offers a myriad of solutions (e.g., [7, 8]).</p>
    <p>We built our system on top of the gabriel-sandwich project [9] with a edge server architecture and adopted the WebSocket to transfer frames and resulting instructions between the client and edge server. We have implemented a new Hololens 2 client and added our image classification module, the activity recognition module and the interactive state tracking module in the edge server. A video demonstration is available [2].</p>
    <p>For the client, as we can see in Fig. 1, it also acts as the camera source. In our system, we only fetch new frames from the camera source if there are less than two frames that are under processing in the edge server. All the other frames will be dropped at the camera source directly.</p>
    <p>In the edge server, for the image classification module, we adopt the transfer learning method and build our model based on a lightweight MobileNet v2 backbone network [1], which could be deployed on a lot of edge devices. We have five classes in our case, which are bread, tomato, lettuce, ham and a label for the background class. To fine tuning the network,</p>
    <p>we take pictures on the objects in a sandwich toy and prepare around 60 images for each label. It achieves the accuracy of more than 99% for both the training set and the validation set after 20 iterations. In the edge server, we also smooth the detection results and only report the detection of an object if it has been detected for four consecutive frames.</p>
    <p>As shown in Fig. 2, the top object cannot fully represent the current task state because different actions may lead to the same top object. Therefore, we propose a simple activity recognition method based on the changes of the top objects and the objects that we already stacked in the pile. For instance, if we have stacked A and B in sequence, we will know that C is put on B if C is detected right after B. This method allows us to infer a temporal relation (e.g., cheese stacked on bread) by leveraging a classifier running on a sequence of frames. However, if A is detected after B, we need to involve the user to resolve the ambiguity and figure out whether the user has put another A on B or it has removed the B on top.</p>
    <p>Given the users goal (e.g., make a ham sandwich), we call the fast-downward planning system [10] to generate a plan, which achieves the goal. The user is then provided with the first instruction. Following this, when a user action is observed, we check whether the action matches the current step of the plan. As described previously, we only call the planner if the observed action does not match the current instruction. Otherwise, we directly track the state transitions based on a state machine derived from the plan. Whenever a user action is observed, we update the state of the world by modifying the PDDL file to reflect effects of the action.</p>
    <p>(a) Runtime for the planner. (b) Runtime for the classifier</p>
    <p>Figure 4: Runtime for the planner and the classifier.</p>
    <p>In this section, we show the characteristics of running the planner and the classifier in our experiments.</p>
  </div>
  <div class="page">
    <p>Runtime of the planner and classifier</p>
    <p>9</p>
    <p>Stack Ham</p>
    <p>on Bread</p>
    <p>Stack Lettuce</p>
    <p>on Ham</p>
    <p>Stack Bread</p>
    <p>on Lettuce</p>
    <p>End</p>
    <p>Start</p>
    <p>Stack Lettuce</p>
    <p>on Ham</p>
    <p>Stack Bread</p>
    <p>on Lettuce</p>
    <p>End</p>
    <p>Stack Tomato</p>
    <p>on Ham</p>
    <p>Observed Activity</p>
    <p>Replanning</p>
    <p>Unstack Tomato</p>
    <p>from Ham</p>
    <p>Figure 3: State tracking with a planner and state machines. The green box shows the current expected action.</p>
    <p>the successor state d(a,s). Following the observation of a user action, we check whether the observed action a matches the current step of the plan that was given to the user. If yes, we will provide the next instruction directly based on the state machine and change the current state in the state machine. However, if not, we generate a new planning problem where I is the updated state d(a,s) and solve it to obtain p0. For instance, if the user made a mistake such as stacking tomato on ham instead of stacking lettuce on ham, p0 will correct the users mistake and instruct her to remove the tomato and place the lettuce instead. An illustrative figure is shown in Fig. 3. With the new p0, a new state machine is built based on the new list of instructions. The approach of integrating the planner with a state machine is advantageous since the system will not trigger the planner every time as long as the user is following the plan. Note that we employ here a simple approach to plan execution and monitoring; the rich body of related work offers a myriad of solutions (e.g., [7, 8]).</p>
    <p>We built our system on top of the gabriel-sandwich project [9] with a edge server architecture and adopted the WebSocket to transfer frames and resulting instructions between the client and edge server. We have implemented a new Hololens 2 client and added our image classification module, the activity recognition module and the interactive state tracking module in the edge server. A video demonstration is available [2].</p>
    <p>For the client, as we can see in Fig. 1, it also acts as the camera source. In our system, we only fetch new frames from the camera source if there are less than two frames that are under processing in the edge server. All the other frames will be dropped at the camera source directly.</p>
    <p>In the edge server, for the image classification module, we adopt the transfer learning method and build our model based on a lightweight MobileNet v2 backbone network [1], which could be deployed on a lot of edge devices. We have five classes in our case, which are bread, tomato, lettuce, ham and a label for the background class. To fine tuning the network,</p>
    <p>we take pictures on the objects in a sandwich toy and prepare around 60 images for each label. It achieves the accuracy of more than 99% for both the training set and the validation set after 20 iterations. In the edge server, we also smooth the detection results and only report the detection of an object if it has been detected for four consecutive frames.</p>
    <p>As shown in Fig. 2, the top object cannot fully represent the current task state because different actions may lead to the same top object. Therefore, we propose a simple activity recognition method based on the changes of the top objects and the objects that we already stacked in the pile. For instance, if we have stacked A and B in sequence, we will know that C is put on B if C is detected right after B. This method allows us to infer a temporal relation (e.g., cheese stacked on bread) by leveraging a classifier running on a sequence of frames. However, if A is detected after B, we need to involve the user to resolve the ambiguity and figure out whether the user has put another A on B or it has removed the B on top.</p>
    <p>Given the users goal (e.g., make a ham sandwich), we call the fast-downward planning system [10] to generate a plan, which achieves the goal. The user is then provided with the first instruction. Following this, when a user action is observed, we check whether the action matches the current step of the plan. As described previously, we only call the planner if the observed action does not match the current instruction. Otherwise, we directly track the state transitions based on a state machine derived from the plan. Whenever a user action is observed, we update the state of the world by modifying the PDDL file to reflect effects of the action.</p>
    <p>C D</p>
    <p>)</p>
    <p>(a) Runtime for the planner.</p>
    <p>C D</p>
    <p>)</p>
    <p>(b) Runtime for the classifier</p>
    <p>Figure 4: Runtime for the planner and the classifier.</p>
    <p>In this section, we show the characteristics of running the planner and the classifier in our experiments.</p>
    <p>It is feasible to run both the planner and classifier on the edge.</p>
  </div>
  <div class="page">
    <p>Demo  The video for our demo is available here.</p>
    <p>10</p>
  </div>
  <div class="page">
    <p>Future Work  Personalized instructions</p>
    <p>Resource management for multiple cognitive assistance agents</p>
    <p>Applications that only need partial order</p>
    <p>Linear Temporal Logic (LTL)</p>
    <p>11</p>
  </div>
  <div class="page">
    <p>Summary  We have proposed an architecture for cognitive assistants on the edge</p>
    <p>Ambiguous task states are prevalent and we need to deal with them</p>
    <p>We should combine the planner with state machines to enjoy both of the benefits.</p>
    <p>12</p>
  </div>
  <div class="page">
    <p>Thanks! zhiming.hu@samsung.com</p>
    <p>13</p>
  </div>
</Presentation>
