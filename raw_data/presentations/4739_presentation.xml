<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Performance Models for Evaluation and Automatic Tuning of Symmetric Sparse Matrix-Vector Multiply</p>
    <p>University of California, Berkeley Berkeley Benchmarking and Optimization Group (BeBOP)</p>
    <p>http://bebop.cs.berkeley.edu</p>
    <p>Benjamin C. Lee, Richard W. Vuduc, James W. Demmel, Katherine A. Yelick University of California, Berkeley</p>
  </div>
  <div class="page">
    <p>Performance Tuning Challenges</p>
    <p>Computational Kernels  Sparse Matrix-Vector Multiply (SpMV): y = y + Ax</p>
    <p>A : Sparse matrix, symmetric ( i.e., A = AT )  x, y : Dense vectors</p>
    <p>Sparse Matrix-Multiple Vector Multiply (SpMM): Y = Y + AX  X, Y : Dense matrices</p>
    <p>Performance Tuning Challenges  Sparse code characteristics</p>
    <p>High bandwidth requirements (matrix storage overhead)  Poor locality (indirect, irregular memory access)  Poor instruction mix (low ratio of flops to memory operations)</p>
    <p>SpMV performance less than 10% of machine peak  Performance depends on kernel, matrix, and architecture</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Blocking (1/3)</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Blocking (2/3)</p>
    <p>BCSR with uniform, aligned grid</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Blocking (3/3)</p>
    <p>Fill-in zeros: Trade extra flops for better blocked efficiency</p>
  </div>
  <div class="page">
    <p>Optimizations: Matrix Symmetry</p>
    <p>Symmetric Storage  Assume compressed sparse row (CSR) storage  Store half the matrix entries (e.g., upper triangle)</p>
    <p>Performance Implications  Same flops  Halves memory accesses to the matrix  Same irregular, indirect memory accesses</p>
    <p>For each stored non-zero A(i, j)  y ( i ) += A ( i , j ) * x ( j )  y ( j ) += A ( i , j ) * x ( i )</p>
    <p>Special consideration of diagonal elements</p>
  </div>
  <div class="page">
    <p>Optimizations: Multiple Vectors</p>
    <p>Performance Implications  Reduces loop overhead  Amortizes the cost of reading A for v vectors</p>
    <p>A</p>
    <p>X</p>
    <p>v</p>
    <p>k</p>
    <p>Y</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Usage (1/3)</p>
    <p>Register Blocking  Assume column-wise unrolled block multiply  Destination vector elements in registers ( r )</p>
    <p>A</p>
    <p>x</p>
    <p>y</p>
    <p>r</p>
    <p>c</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Usage (2/3)</p>
    <p>Symmetric Storage  Doubles register usage ( 2r )</p>
    <p>Destination vector elements for stored block  Source vector elements for transpose block</p>
    <p>A</p>
    <p>x</p>
    <p>y</p>
    <p>r</p>
    <p>c</p>
  </div>
  <div class="page">
    <p>Optimizations: Register Usage (3/3)</p>
    <p>Vector Blocking  Scales register usage by vector width ( 2rv )</p>
    <p>A</p>
    <p>X</p>
    <p>v</p>
    <p>k</p>
    <p>Y</p>
  </div>
  <div class="page">
    <p>Evaluation: Methodology</p>
    <p>Three Platforms  Sun Ultra 2i, Intel Itanium 2, IBM Power 4</p>
    <p>Matrix Test Suite  Twelve matrices  Dense, Finite Element, Linear Programming, Assorted</p>
    <p>Reference Implementation  No symmetry, no register blocking, single vector multiplication</p>
    <p>Tuning Parameters  SpMM code characterized by parameters ( r , c , v )</p>
    <p>Register block size : r x c  Vector width : v</p>
  </div>
  <div class="page">
    <p>Evaluation: Exhaustive Search</p>
    <p>Performance  2.1x max speedup (1.4x median) from symmetry (SpMV)</p>
    <p>{Symm BCSR Single Vector} vs {Non-Symm BCSR Single Vector}</p>
    <p>2.6x max speedup (1.1x median) from symmetry (SpMM)  {Symm BCSR Multiple Vector} vs {Non-Symm BCSR Multiple Vector}</p>
    <p>7.3x max speedup (4.2x median) from combined optimizations  {Symm BCSR Multiple Vector} vs {Non-Symm CSR Single Vector}</p>
    <p>Storage  64.7% max savings (56.5% median) in storage</p>
    <p>Savings &gt; 50% possible when combined with register blocking</p>
    <p>9.9% increase in storage for a few cases  Increases possible when register block size results in significant fill</p>
  </div>
  <div class="page">
    <p>Performance Results: Sun Ultra 2i</p>
  </div>
  <div class="page">
    <p>Performance Results: Sun Ultra 2i</p>
  </div>
  <div class="page">
    <p>Performance Results: Sun Ultra 2i</p>
  </div>
  <div class="page">
    <p>Performance Results: Intel Itanium 2</p>
  </div>
  <div class="page">
    <p>Performance Results: IBM Power 4</p>
  </div>
  <div class="page">
    <p>Automated Empirical Tuning</p>
    <p>Exhaustive search infeasible  Cost of matrix conversion to blocked format</p>
    <p>Parameter Selection Procedure  Off-line benchmark</p>
    <p>Symmetric SpMM performance for dense matrix D in sparse format  { Prcv(D) | 1  r,c  bmax and 1  v  vmax }, Mflop/s</p>
    <p>Run-time estimate of fill  Fill is number of stored values divided by number of original non-zeros  { frc(A) | 1  r,c  bmax }, always at least 1.0</p>
    <p>Heuristic performance model  Choose ( r , c , v ) to maximize estimate of optimized performance  maxrcv { Prcv(A) = Prcv (D) / frc (A) | 1  r,c  bmax and 1  v  min( vmax , k ) }</p>
  </div>
  <div class="page">
    <p>Evaluation: Heuristic Search</p>
    <p>Heuristic Performance  Always achieves at least 93% of best performance from</p>
    <p>exhaustive search  Ultra 2i, Itanium 2</p>
    <p>Always achieves at least 85% of best performance from exhaustive search</p>
    <p>Power 4</p>
  </div>
  <div class="page">
    <p>Performance Results: Sun Ultra 2i</p>
  </div>
  <div class="page">
    <p>Performance Results: Intel Itanium 2</p>
  </div>
  <div class="page">
    <p>Performance Results: IBM Power 4</p>
  </div>
  <div class="page">
    <p>Performance Models</p>
    <p>Model Characteristics and Assumptions  Considers only the cost of memory operations  Accounts for minimum effective cache and memory latencies  Considers only compulsory misses (i.e., ignore conflict misses)  Ignores TLB misses</p>
    <p>Execution Time Model  Loads and cache misses</p>
    <p>Analytic model (based on data access patterns)  Hardware counters (via PAPI)</p>
    <p>Charge ai for hits at each cache level  T = (L1 hits) a1 + (L2 hits) a2 + (Mem hits) amem  T = (Loads) a1 + (L1 misses) (a2  a1) + (L2 misses) (amem  a2)</p>
  </div>
  <div class="page">
    <p>Evaluation: Performance Bounds</p>
    <p>Measured Performance vs. PAPI Bound  Measured performance is 68% of PAPI bound, on average  FEM applications are closer to bound than non-FEM matrices</p>
  </div>
  <div class="page">
    <p>Performance Results: Sun Ultra 2i</p>
  </div>
  <div class="page">
    <p>Performance Results: Intel Itanium 2</p>
  </div>
  <div class="page">
    <p>Performance Results: IBM Power 4</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Matrix Symmetry Optimizations  Symmetric Performance: 2.6x speedup (1.1x median)  Overall Performance: 7.3x speedup (4.15x median)  Symmetric Storage: 64.7% savings (56.5% median)  Cumulative performance effects</p>
    <p>Automated Empirical Tuning  Always achieves at least 85-93% of best performance from</p>
    <p>exhaustive search</p>
    <p>Performance Modeling  Models account for symmetry, register blocking, multiple vectors  Measured performance is 68% of predicted performance (PAPI)</p>
  </div>
  <div class="page">
    <p>Current &amp; Future Directions</p>
    <p>Parallel SMP Kernels  Multi-threaded versions of optimizations  Extend performance models to SMP architectures</p>
    <p>Self-Adapting Sparse Kernel Interface  Provides low-level BLAS-like primitives  Hides complexity of kernel-, matrix-, and machine-specific tuning  Provides new locality-aware kernels</p>
  </div>
  <div class="page">
    <p>Appendices</p>
    <p>Berkeley Benchmarking and Optimization Group  http://bebop.cs.berkeley.edu</p>
    <p>Conference Paper: Performance Models for Evaluation and Automatic Tuning of Symmetric Sparse Matrix-Vector Multiply</p>
    <p>http://www.cs.berkeley.edu/~blee20/publications/lee2004-icpp-symm.pdf</p>
    <p>Technical Report: Performance Optimizations and Bounds for Sparse Symmetric Matrix-Multiple Vector Multiply</p>
    <p>http://www.cs.berkeley.edu/~blee20/publications/lee2003-tech-symm.pdf</p>
  </div>
  <div class="page">
    <p>Appendices</p>
  </div>
  <div class="page">
    <p>Performance Results: Intel Itanium 1</p>
  </div>
</Presentation>
