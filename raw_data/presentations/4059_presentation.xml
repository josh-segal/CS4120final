<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Cocaine Noodles: Exploiting the Gap between Human and</p>
    <p>Machine Speech Recognition</p>
    <p>Tavish Vaidya, Yuankai Zhang, Micah Sherr and Clay Shields</p>
    <p>Georgetown University</p>
    <p>Presented at WOOT15</p>
  </div>
  <div class="page">
    <p>Voice input is near ubiquitous</p>
  </div>
  <div class="page">
    <p>Voice input is near ubiquitous</p>
  </div>
  <div class="page">
    <p>Can the differences between human and machine understanding of speech lead to attacks?</p>
    <p>Ok Google</p>
    <p>Cocaine Noodles</p>
  </div>
  <div class="page">
    <p>Attack 1</p>
    <p>Open malicious webpage - Serve drive-by-download or malware - Open up attack surface for further attacks</p>
  </div>
  <div class="page">
    <p>Attack 2</p>
    <p>Send text message to particular number - Monetize the attack using reverse SMS billing or</p>
    <p>premium SMS service numbers as destination</p>
  </div>
  <div class="page">
    <p>Attack 3</p>
    <p>Enumerate devices in an area (e.g. those belonging to dissidents attending a rally)</p>
  </div>
  <div class="page">
    <p>Other Attacks</p>
    <p>Denial-of-Service - E.g., use public announcement</p>
    <p>systems to turn on airplane mode</p>
    <p>Sending/forging email</p>
    <p>Sending/forging messages on social media</p>
  </div>
  <div class="page">
    <p>Attacker Goals</p>
    <p>Execute commands on target device by exploiting its speech recognition system</p>
    <p>Minimize the possibility of alerting the user of the attack - Produce mangled commands that are understood by the device</p>
    <p>but not the user</p>
  </div>
  <div class="page">
    <p>(Non)Assumptions</p>
    <p>Non-assumption: we make no assumption about target speech recognition system</p>
    <p>Speech recognition model and process are treated as black boxes</p>
    <p>Attacks are agnostic to particular AI/ML used by target device</p>
    <p>Adversary is able to play audio to target devices  E.g., from an elevator speaker, youtube video, LRAD etc.</p>
    <p>Target devices do not apply biometrics or attempt to authenticate users/speakers</p>
    <p>Target devices are always listening to voice input</p>
  </div>
  <div class="page">
    <p>Background: Speech Recognition Overview</p>
    <p>Pre-processing  Background noise removal</p>
    <p>Speech/non-speech segmentation</p>
    <p>Feature Extraction  Acoustic features useful for</p>
    <p>recognizing speech</p>
    <p>Mel-frequency cepstral coefficients (MFCC) for representing acoustic features</p>
    <p>Preprocessing</p>
    <p>Input Audio Signal</p>
    <p>Feature Extraction</p>
    <p>(MFCC)</p>
    <p>Filtered Audio Signal</p>
    <p>Speech Recognition System</p>
  </div>
  <div class="page">
    <p>Background: Speech Recognition Overview</p>
    <p>Model Based Prediction  Extracted acoustic features of input</p>
    <p>signal matched against existing models</p>
    <p>Models typically constructed using statistical approaches</p>
    <p>Post-processing  Optionally, rank generated predictions</p>
    <p>using additional information</p>
    <p>E.g., enforcing grammar rules, subject matter, locality of words, etc.</p>
    <p>Preprocessing</p>
    <p>Feature Extraction</p>
    <p>(MFCC)</p>
    <p>Input Audio Signal</p>
    <p>Filtered Audio Signal</p>
    <p>Model-Based Prediction</p>
    <p>Acoustic Features</p>
    <p>Postprocessing Text</p>
    <p>Predictions</p>
    <p>Text Output</p>
    <p>Speech Recognition System</p>
  </div>
  <div class="page">
    <p>Mel-Frequency Cepstral Coefficients (MFCCs)</p>
    <p>- Cepstral coefficients represent acoustic features in audio signal</p>
    <p>- MFCC closely approximates human response to auditory sensation</p>
    <p>- Allows for better representation of sound</p>
    <p>Amplitudes of resulting spectrum are the MFCCs</p>
    <p>DCT of each mel log powers</p>
    <p>Take log of powers for each frequency on mel scale</p>
    <p>Mapping power of obtained spectrum onto mel scale</p>
    <p>FFT of windowed excerpt</p>
    <p>Select a time window</p>
  </div>
  <div class="page">
    <p>Attack Overview</p>
    <p>Ok Google</p>
    <p>Audio Mangler</p>
    <p>MFCC Parameters</p>
    <p>Enough acoustic</p>
    <p>information for target to understand</p>
  </div>
  <div class="page">
    <p>Generating attack commands</p>
    <p>Machines Understanding</p>
    <p>s</p>
    <p>Feedback</p>
    <p>Un-mangled open evil.com</p>
    <p>Mangled open evil.com</p>
    <p>Audio Mangler</p>
    <p>Humans (i.e., attackers) Understanding</p>
  </div>
  <div class="page">
    <p>MFCC Tuning</p>
    <p>- MFCC computation has various parameters</p>
    <p>- We modify 4 independent parameters: 1. wintime</p>
    <p>- Experimentally observed the effect of changing each parameter</p>
    <p>- Perceived quality of mangled audio varies with different parameter values</p>
    <p>- Used Googles Speech-to-Text Speech Recognition API to narrow down parameters</p>
    <p>Google Speech-To</p>
    <p>Text API</p>
    <p>Good, Bad, Error</p>
    <p>Predictions</p>
    <p>s</p>
    <p>Feedback</p>
    <p>Audio Mangler open</p>
    <p>evil.com</p>
    <p>MFCC Parameters</p>
    <p>Mangled open</p>
    <p>evil.com</p>
  </div>
  <div class="page">
    <p>Feature Extraction with Tuned MFCC Parameters</p>
    <p>- Tuned parameters are used for computing MFCC</p>
    <p>- MFCC computation is lossy - Signal is considered statistically constant over a</p>
    <p>small time window</p>
    <p>- Energy level of closely spaced frequencies are aggregated in various frequency regions on mel frequency scale</p>
    <p>- MFCCs do not retain all information about the original input</p>
    <p>- Tuned MFCC parameters are intended to further increase this loss</p>
    <p>Google Speech-To</p>
    <p>Text API</p>
    <p>Good, Bad, Error</p>
    <p>Predictions</p>
    <p>s</p>
    <p>Feedback</p>
    <p>open evil.com</p>
    <p>MFCC Parameters</p>
    <p>Acoustic features Mangled</p>
    <p>open evil.com</p>
    <p>Audio Mangler</p>
    <p>Feature Extraction with</p>
    <p>Tuned MFCC parameters</p>
  </div>
  <div class="page">
    <p>Inverse MFCC Computation</p>
    <p>- Extracted audio features converted back to audio signal</p>
    <p>- MFCC computation steps are reversed</p>
    <p>- White noise added to (re)construct mangled audio command Google</p>
    <p>Speech To Text API</p>
    <p>Good, Bad, Error</p>
    <p>Predictions</p>
    <p>s</p>
    <p>Feedback</p>
    <p>Open evil.com</p>
    <p>MFCC Parameters</p>
    <p>Acoustic features Mangled</p>
    <p>open evil.com</p>
    <p>Audio Mangler</p>
    <p>Feature Extraction with</p>
    <p>Tuned MFCC parameters</p>
    <p>Inverse MFCC</p>
  </div>
  <div class="page">
    <p>Mangled commands are crafted to contain acoustic information for a targeted speech recognition system to work, but the human brain doesnt work the same way as machine speech recognition systems!</p>
    <p>http://www.ucsf.edu/news/2014/01/111506/ucsfteam-reveals-how-brain-recognizes-speech-sounds</p>
    <p>Pre-processing, FFT, Model ..</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Goal: Determine that mangled commands 1) activate functionality on phone (comprehension by machine); and</p>
    <p>Consider 4 types of commands:  activating the voice command input (i.e., OK Google)</p>
    <p>calling a number</p>
    <p>sending a text message to a number</p>
    <p>opening a website (tested against two websites)</p>
  </div>
  <div class="page">
    <p>Comprehension by Machine</p>
    <p>Experimental setup  Tested the audio commands against Google Now</p>
    <p>Samsung Galaxy S4 smartphone with Android version 4.4.2</p>
    <p>Commands were played via speakers placed ~30 cm from phone</p>
    <p>Baseline</p>
    <p>(un-mangled commands)</p>
    <p>Un-mangled versions of all commands were played</p>
    <p>All candidates successfully activated functionality on the device</p>
    <p>Attack</p>
    <p>(mangled commands)</p>
    <p>500 potential candidates filtered using Googles STT</p>
    <p>105 candidates manually chosen by 2 authors</p>
    <p>All selected attack candidates successfully activated functionality on the device</p>
  </div>
  <div class="page">
    <p>Non-comprehension by Human Listeners</p>
    <p>Experimental setup  Amazon Mechanical Turk user study</p>
    <p>Task: Evaluators given 4 unique audio commands to transcribe</p>
    <p>Asked to provide their best guess</p>
    <p>Given bonus ($$$) for correct transcriptions</p>
    <p>Audio samples included both mangled and unmangled commands</p>
    <p>Conservative test: evaluators could replay audio, listen under ideal conditions, etc.</p>
    <p>Evaluation Metric  Levenshtein edit distance (of phonemes) between</p>
    <p>correct and human-provided transcriptions</p>
    <p>Normalized w.r.t. length of correct transcription</p>
    <p>ok google</p>
    <p>cookie coo coo</p>
    <p>OW1 K EY1 . G UW1 G AH0 L</p>
    <p>K UH1 K IY0 . K UW1 . K UW1 Filter</p>
    <p>Levenshtein distance</p>
    <p>Result</p>
  </div>
  <div class="page">
    <p>Human Understanding of Mangled Commands</p>
    <p>Example transcripts: - cookie coo coo - Oh ee oh ah ah - ha he ho ha - Seek approval - puchee poo poo</p>
    <p>Very few understood the audio correctly!</p>
    <p>Very few understood the audio correctly!</p>
    <p>~70% had completely wrong</p>
    <p>interpretation</p>
    <p>~70% had completely wrong</p>
    <p>interpretation</p>
  </div>
  <div class="page">
    <p>Human Understanding of Mangled Commands</p>
    <p>Example transcripts: - sh facebook got it - how do you spell .com - Small hairs button dot car - for place spectrum.com - essa tres quatro dot come</p>
    <p>Less than 10% interpreted mangled</p>
    <p>audio correctly</p>
    <p>Less than 10% interpreted mangled</p>
    <p>audio correctly</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Voice command input systems are ubiquitous, but lack security</p>
    <p>There exists a gap in the ability of humans and machines to understand audio signals</p>
    <p>We examined the possibility of exploiting this gap on voice command inputs</p>
    <p>Preliminary results show that this gap can be exploited</p>
    <p>Cocaine Noodles: Exploiting the Gap between Human and Machine Speech Recognition</p>
    <p>Tavish Vaidya, Yuankai Zhang, Micah Sherr and Clay Shields</p>
    <p>Georgetown University</p>
  </div>
</Presentation>
