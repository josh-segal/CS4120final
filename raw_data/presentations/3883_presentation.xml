<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Water Fountain vs. the Fire hose: An Examination and</p>
    <p>Comparison of Two Large Enterprise Mail Service</p>
    <p>Migrations</p>
    <p>Craig Stacey, IT Manager</p>
    <p>Max Trefonides, Systems Administrator</p>
    <p>Mathematics and Computer Science Division</p>
    <p>Tim Kendall, Systems Administrator</p>
    <p>Materials Science Division</p>
    <p>Brian Finley, Deputy Manager  Unix, Storage, and Operations</p>
    <p>Computing and Information Systems Division</p>
    <p>Introductions: Me, Max, Tim, Brian.</p>
    <p>I know a number of stand-up comics, and theres this truism that is known throughout the business. Every comic loves to hear a great bomb story. They just love to hear about some other comedians worst night on stage. Its a kind of schaudenfraud that makes you feel better about your own situation. Kind of like how watching Springer or Cops makes everything in your world seem so much happier.</p>
    <p>Im happy to see this is alive and well in the world of systems administration! You all are about to hear to woeful tale of what led up to the worst weekend of my professional career as a sysadmin. So prepare to bask in the tale of poor decisions, rushed implementations, and mistyped config files, and be happy it wasnt you!</p>
  </div>
  <div class="page">
    <p>Laboratory Overview of Services  Central IT Services</p>
    <p>provided by Computing and Information Systems division</p>
    <p>Programmatic divisions often have IT needs outside this scope</p>
    <p>Occasionally, divisionspecific IT groups provide services that overlap with CIS.</p>
    <p>Argonnes central IT Services group (CIS) provides services for both the Operations and Programmatic sides of the laboratory. Any of the labs divisions and groups can use these services, typically with no additional cost. Of course, these services dont always overlap with the needs of the programmatic sides of the laboratory.</p>
    <p>So, many of the Programmatic divisions also maintain their own IT staffs of varying sizes to support mission-specific computing needs. These groups will also, in some cases, provide general IT services.</p>
    <p>Here we see simple Venn diagram that demonstrates two concepts in one. You can look at the three areas as the IT services provided by each group, or you can look at them as representing the IT needs of the groups customers. In either case, theres an overlap of services provided or needed. If we focus on this center intersection (click), we can find the service were going to talk about today -- e-mail.</p>
    <p>For clarity, when I say operations, Im referring to the business of running Argonne National Laboratory. The groups who are concerned with the day to day operation of the lab, and not involved in research. When I say programmatic, Im referring to the divisions who do the actual research funded by the various programs.</p>
    <p>MCS (Mathematics and Computer Science) and MSD (Materials Science Division) are two such divisions, each has its own IT group, and each provided e-mail services for their users.</p>
    <p>MCS maintains an IT staff of 7-12 people depending on what you consider IT staff and how many students we have.</p>
    <p>MSD maintains an IT staff of 3 people.</p>
  </div>
  <div class="page">
    <p>Laboratory Mail diagram</p>
    <p>Talking about e-mail, lets look at how things work at the lab, in general.</p>
    <p>The labs central mail service provides everything from external-facing mail relays to mailbox services.</p>
    <p>For a long time, the only real production mail service offered by CIS was Exchange, though in 2008 production-level Zimbra support was offered.</p>
    <p>Mail is scanned at the relay cluster for spam and malware, then passed on to the routing servers for distribution to mailboxes, divisional mail servers, or list servers.</p>
  </div>
  <div class="page">
    <p>MCS Mail Migration</p>
    <p>The title of this paper is the Water Fountain vs. the Firehose. Were going to talk about MCSs approach first, and, well, you can guess which approach we used.</p>
  </div>
  <div class="page">
    <p>MCS Mail Delivery Overview &amp; Diagram</p>
    <p>MCSs mail infrastructure had always been historically outside the scope of ANLs mail system. At the time of the transition, this is how it looked. Pretty straightforward.</p>
    <p>The key part of this diagram, and the focus of the next little bit of this talk is the bottom box, the IMAP server. It was an IBM RS/ 6000 PowerPC 604e AIX box, installed into production service in 1998, running an older version of Cyrus IMAP.</p>
  </div>
  <div class="page">
    <p>Timelines (the long view and the short view)</p>
    <p>Dinosaurs</p>
    <p>Sun explodes</p>
    <p>Stuff happens</p>
    <p>Okay, too long.</p>
    <p>We start with a long view of our timeline. &lt;click&gt; At the far left, &lt;click&gt; we have the Jurassic period, and on the right, the end times. The important stuff is in the middle. &lt;click&gt;</p>
  </div>
  <div class="page">
    <p>Timelines (the long view and the short view)</p>
    <p>Late 2006 into 2007  Planning, Prep, Emergencies</p>
    <p>Cleanup</p>
    <p>Early 2008  Plan shifts</p>
    <p>In 1998, we stood up a successful IMAP server. Too successful, it turns out, since it never really crept onto our radar again until 8 years later, when we began a new project to replace it. Unfortunately, emergencies kept interrupting the planning for this endeavor, and thus it still sat on the back burner. During this time, the Lab stood up a Zimbra server as a pilot program. We were intrigued by its group calendar functionality, as our users were requesting just this very service. And because it wasnt tied to running outlook, our heavy base of Linux users could make use of it.</p>
    <p>By 2008, the pilot switched to production, and so did our planning. We switched our strategy to employ the Zimbra service for user mailboxes as well, and began a new plan on how to get the existing data from our old server (cliff) to the new one (zimbra). By this time, getting off the old mail server was getting higher and higher a priority. Services were failing, mailboxes were too big, and loads were climbing.</p>
    <p>Ill go over this shorter timeline in the next few slides, but what I wanted to point out is how wed gone from a very stretched timeline to a very compressed one. And as well learn later in the talk, it got even more compressed.</p>
  </div>
  <div class="page">
    <p>Research</p>
    <p>We did the research on this. All of our reading indicated this was going to be a simple operation. After all, it was all mailbox data, both the old and the new servers spoke IMAP. We could, with enough lead time, move all the data in advance of the switch to the new server.</p>
    <p>We would be heroes.</p>
    <p>This was going to be simple.</p>
  </div>
  <div class="page">
    <p>I think we all know where this line of thought was heading.</p>
  </div>
  <div class="page">
    <p>The Plan</p>
    <p>Plan A: Use imapsync to move</p>
    <p>user data.</p>
    <p>CliffZimbra</p>
    <p>owney</p>
    <p>Plan A was underway. We began the imapsync process. It was not without its pitfalls. First up, the age of the old mail server precluded us from running the imapsync scripts on it  its perl was old precluding it from being able to open an SSL IMAP connection on Zimbra. Also, it was overtaxed as it was, so we had a newer linux box handle the imapsync process. In order to avoid bringing the mail service to a crawl while we were working on the sync, we found the optimum number of concurrent syncs. Unfortunately, that number was two. Any more, and the mail server was slowing to a crawl or refusing connections.</p>
    <p>Thus, it was a very slow process. Indications were that the actual sync would not finish in anything near an acceptable time period. So, while the syncs continued, we looked into other methods of moving the data.</p>
  </div>
  <div class="page">
    <p>The Plan</p>
    <p>Plan B: rsync!</p>
    <p>Thus was born Plan B. We would rsync the data from our mail server onto a disk the new server could mount, and then use zimbras import tools to convert them into user mailboxes. After the data sync, we would use imapsync to get any new messages and set flags on all messages.</p>
    <p>Because we didnt want to bog down the production zimbra service, we rsynced to a test server first and implemented our mailbox conversion tools on that. Once were were done, we would mount the disk on the production server, and perform the import there.</p>
    <p>Surely, this plan could not fail.</p>
  </div>
  <div class="page">
    <p>Again with the learning.</p>
  </div>
  <div class="page">
    <p>Nothing is Easy</p>
    <p>Despite what Staples would have you believe, there is no easy button. Everything is more complex beneath the surface.</p>
    <p>Wed find some plan that worked on paper, but in implementation did not hold up as expected. A partial laundry list of failures:</p>
    <p>The disk to which we rsynced was slower than we expected. We felt the bottleneck was because the disk was NFS mounted instead of directly mounted. We planned to mount the disk directly on the production server as a logical volume from the SAN. Alas, the architecture of the SAN prevented us from doing this, requiring us to NFS mount this data on the production server as well. Some mailboxes produced name collisions with other mailboxes. Because Zimbra uses a flat namespace for its mailbox and calendar folders, you cannot have a mailbox and calendar with the same name. rsyncs were aborted and restarted at various points in the process due to filling disks, log files run amuck, network issues, machine crashes, etc. In the end, instead of a 4 month window in which to execute this transfer, various aborts and restarts put us into a situation wherein we had roughly two weeks to accomplish the migration and validation of data. Its worth noting here that the April 25th deadline was not completely arbitrary  it was already scheduled to be a maintenance weekend lab-wide, so users had an expectation of downtime. Also, our old mail servers ability to continue to provide service for more than another month was seriously doubted.</p>
  </div>
  <div class="page">
    <p>April (wherein we become wellacquainted with the 2x4 of knowledge) Flipped the switch the morning of the 26th, sync would finish throughout the weekend. imapsync was deleting messages despite our belief if was configured to not do that. Estimates of completion were horribly skewed, as our largest mailbox (over 20GB) was among the last to be migrated. Large mailboxes caused imapsync to time out. Timeouts resulted in only partial mailboxes, since imapsync moved on. Some mailboxes contained corrupt data Other random screwups. By Monday, it was evident the prep and sync was for naught  we were back to square one.</p>
    <p>Starting from square one yet again, we had our method down. The rsynced data was freshly imported onto the production server, and our imapsync scripts were running, picking up the stragglers. Spot checks showed things were going as expected, though slowly. Users with large mailboxes were taking significantly longer than other users, but this was to be expected.</p>
    <p>As the final weekend in April approached, we went into the morning of Saturday, April 26th with optimism that wed overcome the pitfalls wed been seeing.</p>
    <p>&lt;click&gt; That morning, we discovered the imapsync was taking much longer than wed hoped. We made the judgment call that wed flip the delivery switch on delivery, get all the pieces in place, and continue the imapsync through the weekend. With all the pieces in place, mail was now being delivered into the new mailboxes, and we restarted the sync process.</p>
    <p>&lt;click&gt; We were once again to be visited by the 2 x 4 of knowledge, as spot checking the logs later that night showed some behavior we should not be seeing. We had configured the imapsync script to be nondestructive  no mail would be deleted from the destination mailbox, only new messages would be added. However, it turns out that configuration was not working, and mail that had been delivered throughout the day would get deleted once that users imapsync was run.</p>
    <p>&lt;click&gt; Our estimation of when we would complete the syncs were skewed because they were based on alphabetical progress through the user list. Alas, our largest mailbox was the second-to-last mailbox to be synced, and many other of the largest mailboxes were skewed toward the end of the alphabet.</p>
    <p>&lt;click&gt; IMAP would time out on the larger mailboxes, causing partial mailbox migration and flag setting, aborting the user early and moving on to the next one.</p>
    <p>&lt;click&gt; Likewise, some mailboxes would only partially transfer as the sync would abort on the first corrupt message.</p>
    <p>&lt;click&gt; We let users have access to their new mailboxes on Sunday evening by dropping a message in their old mailboxes containing instructions on how to reconfigure their mail clients. A long sleepless weekend led to a typo in that message, causing us to have to change configs such that the incorrect instructions would work.</p>
    <p>&lt;click&gt; On Monday, after seeing the complaints, we sent notice to the users that the previously synced mailboxes were not likely to be current, and that wed give them access to their old mailboxes so that they could move their data by hand. We would assist any user who wanted help.</p>
  </div>
  <div class="page">
    <p>May</p>
    <p>In May, we finished the migration. Largely, its what we should have done from the start. Specifically, we should have simply switched delivery, and let the users move any mail they wanted to keep.</p>
    <p>There were other lessons we learned after MSDs migration, which Ill get into later in the talk.</p>
  </div>
  <div class="page">
    <p>MSD Mail Migration</p>
    <p>This part of the talk is much shorter, mainly because it went largely without incident.</p>
  </div>
  <div class="page">
    <p>MSD Delivery Overview</p>
    <p>As noted here, MSD was largely using the labs mail infrastructure, just not for user mailboxes. Ultimately, all they were looking to do was switching from running their own mailbox server to using the labs services.</p>
    <p>They had considered running their own mailbox server, but based off of MCSs experience and faith in the new Zimbra service, they felt enough of a comfort level to go that route as well.</p>
    <p>While a handful of users would ultimately end up on the Exchange server, the only real change in the before and after for this diagram is moving the final red arrow from the divisional mail server to the ANL Zimbra server.</p>
  </div>
  <div class="page">
    <p>The Importance of Learning From the Mistakes of Others</p>
    <p>Meetings were held prior to Zimbra decision.</p>
    <p>Discussions took place after MCSs migration.</p>
    <p>Prior to any migrations, MSD, MCS, and CIS got together at MSDs behest to discuss using Zimbra as a production mail service. At this point, MCS had already committed to moving to Zimbra, but had yet to perform the switch. We all got together again after MCS had finished its migration and discussed what wed learned in the process.</p>
    <p>MCS unequivocally recommended a phased approach if at all possible, for obvious reasons.</p>
  </div>
  <div class="page">
    <p>The Plan</p>
    <p>Use new tools available in Zimbra to do the bulk of the heavy lifting.  Add external IMAP account to Zimbra account  Let Zimbra server slurp up the tasty messages  Drag imported mailboxes up into the main folder to preserve structure</p>
    <p>A minority of users were migrated to Exchange instead, to facilitate planning with groups outside the division who use Exchange.  These users were migrated piecemeal, by hand, and on a separate</p>
    <p>schedule.</p>
    <p>In the intervening weeks between MCSs and MSDs migration, a new version of Zimbra appeared which contained the ability to download mail from other IMAP accounts. This new feature made MSDs path clear  create a mailbox on the new server, have the user add their old account to it, and let the server do the work. Once the mail was downloaded via IMAP, mailboxes could be trivially dragged from one account to the next, replicating the users folder structure.</p>
    <p>Beautiful in its simplicity, it allowed MSD to migrate users very gradually, at their own pace, largely without problem.</p>
  </div>
  <div class="page">
    <p>Successes and Pitfalls</p>
    <p>Things went very well overall.</p>
    <p>Attachment indexing ate CPU cycles.  Some mailbox corruption was present and had to be handled by the IT</p>
    <p>Operations team by hand.  Misaddressed mail needed handling.</p>
    <p>The plan went off largely as expected, with some minor pitfalls.</p>
    <p>When MCS handled its migration, they were the only real users. Any negative impact on the Zimbra service was only going to be felt by MCS.</p>
    <p>MSD joined in the fun after hundreds of mailboxes were already on the system. Other issues began to arise because of this. Some examples:</p>
    <p>Indexing of attachments was turned on, however as MSD added more and more users, we began to see load averages climb and the system bordered on being unusable for a time. Turning off attachment indexing resolved this issue. Some users had corrupt mailboxes, requiring intervention before their mailboxes could be imported. Some users were receiving mail addressed for the fully qualified divisional address instead of the @anl.gov alias. Because MSD does not run their own relay, these mails would not reach their users once the old mail server was turned off.</p>
    <p>Generally, however, this was a success, and went pretty much as planned.</p>
  </div>
  <div class="page">
    <p>Comparing the two experiences</p>
    <p>Hindsight is 20/20. Its easy to say what we did wrong in MCS. From the surface, in fact, it would seem all we needed to do was wait it out and this new Zimbra feature would have made our work so much easier.</p>
    <p>Deeper examination shows this not to be the case, however. The Zimbra tool MSD used relied on IMAP, the same way imapsync does. Testing after the fact confirmed that the problems we experienced with timeouts in imapsync were also present in this server-side IMAP poll from Zimbra. Certainly, smaller mailboxes would have been transferred this way very easily, but larger ones would still time out, and corrupt mailboxes would still need handholding.</p>
    <p>Likewise our urgencies were different. MSD was battling against a filling disk, whereas in MCS we were looking at the possibility of our aging server not surviving the transition.</p>
  </div>
  <div class="page">
    <p>Lessons and Takeaways</p>
    <p>Your testing must not have been thorough enough.  I know! Im in the future now, too!*</p>
    <p>Phased approaches are preferred, but not always possible Stay on top of hardware and software refreshes Dont try to be too heroic. Letting the users play a large part in a mail migration provides an</p>
    <p>impetus for housecleaning as well.</p>
    <p>*With apologies to Mike Birbiglia</p>
    <p>One of the comments we commonly received on this paper basically boils down to &lt;click&gt; You obviously didnt have thorough enough testing. To which my response is &lt;click&gt;I know. Im in the future now, too!</p>
    <p>&lt;click&gt;Its not unexpected that a slower paced, staged migration is preferred. Alas, sometimes life does not provide this option, and you can occasionally be faced with a much more compressed schedule.</p>
    <p>&lt;click&gt;The biggest lesson learned in MCS is to not let a well-functioning system allow you to be complacent in your hardware and software refreshes. Putting off the inevitable is only going to worsen the matter. In our case, it wasnt simply a complacency, but a larger confluence of events including limited funding, turbulent staffing changes, reorganizations, and, as always, a lack of time, that allowed the situation to reach the point it did. But the longer a system is left alone because its running just fine increases the likelihood that it will grow roots and become far too embedded to be extricated smoothly. Regardless of budgets and purchases, stay on top of documentation, keep the care and feeding of all your systems in everyones consciousness so no one piece gets left behind in a sea of undocumented kludges.</p>
    <p>&lt;click&gt; Dont be a hero. We can get this Scotty on the Enterprise thing going, wherein we have the reputation of being a miracle worker. But, in reality, every once in awhile Scotty should have just told Kirk to cram it, or else the exceptional becomes the norm, and anything less is unacceptable. Have some faith in your users and their ability to deal with things. I take every opportunity I can to brag about our users. Weve got such a fantastic group of people we support in MCS, they genuinely understand Systems Administration is not easy, and that a large portion of what we do goes on under the hood and isnt noticed. This is most probably not universal, so it may not be a lesson for others to take away, but it is one for us.</p>
    <p>&lt;click&gt; By eventually having the users lead their own migrations, we accomplished some cleanup as well. Certainly beneficial for us.</p>
  </div>
  <div class="page">
    <p>Epilogue &amp; Datacenter Pr0n</p>
    <p>We took the lessons learned from this paper, and the other papers cited within, and applied it to a physical migration we just endured. We moved from a 3000 square foot datacenter to a shiny new 25,000 square foot facility. The move went very well, in a nicely staged manner. We even got to devise a prototype for datacenter air hockey.</p>
    <p>The mail server we were so afraid would crash in April of 2008 was turned off for good in October. It will never bother us again. This is its main CPU board, which will hang as a trophy of conquest in my office.</p>
  </div>
</Presentation>
