<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SENIC: Scalable NIC for End-Host Rate Limiting</p>
    <p>Sivasankar Radhakrishnan</p>
    <p>Yilong Geng, Vimalkumar Jeyakumar,</p>
    <p>Abdul Kabbani, George Porter, Amin Vahdat</p>
    <p>USENIX NSDI 2014 4 April 2014</p>
  </div>
  <div class="page">
    <p>Consolidation of Servers</p>
    <p>Network resource management and allocation is crucial</p>
  </div>
  <div class="page">
    <p>Scalable rate limiting is required Thousands of rate limiters per server</p>
    <p>Network Resource Allocation</p>
    <p>! Performance isolation: Oktopus, Seawall, EyeQ</p>
    <p>! Congestion control: QCN, RCP, D3, DCTCP, HULL</p>
    <p>Rely on programmable rate limiters</p>
  </div>
  <div class="page">
    <p>Rate Limiter Options</p>
    <p>Software Hardware</p>
    <p>Scales to many classes</p>
    <p>Works at high link speeds</p>
    <p>Low CPU overhead</p>
    <p>Accurate and precise</p>
    <p>Supports hypervisor bypass</p>
    <p>SENIC</p>
    <p>Reorganize responsibilities of the NIC and operating system</p>
  </div>
  <div class="page">
    <p>Current NIC Design</p>
    <p>Host (DRAM)</p>
    <p>Wire</p>
    <p>Typically 8-32 rings</p>
    <p>. . .</p>
    <p>. . . Qdisc queues</p>
    <p>TX buffers</p>
    <p>Packet Scheduler</p>
    <p>NIC (SRAM) 2</p>
    <p>Tx buffers 3. Rate limit NIC Tx ring buffers</p>
  </div>
  <div class="page">
    <p>Current NIC Design</p>
    <p>Wire</p>
    <p>Typically 8-32 rings</p>
    <p>. . .</p>
    <p>. . . Qdisc queues</p>
    <p>TX buffers</p>
    <p>Packet Scheduler</p>
    <p>Host (DRAM)</p>
    <p>NIC (SRAM)</p>
  </div>
  <div class="page">
    <p>Current NIC Design</p>
    <p>Wire</p>
    <p>Typically 8-32 rings</p>
    <p>. . .</p>
    <p>. . . Qdisc queues</p>
    <p>TX buffers</p>
    <p>Packet Scheduler</p>
    <p>Host (DRAM)</p>
    <p>NIC (SRAM)</p>
    <p>Host DRAM is cheap and</p>
    <p>abundant</p>
  </div>
  <div class="page">
    <p>Current NIC Design</p>
    <p>Wire</p>
    <p>Typically 8-32 rings</p>
    <p>. . .</p>
    <p>. . . Qdisc queues</p>
    <p>TX buffers</p>
    <p>Packet Scheduler</p>
    <p>Host (DRAM)</p>
    <p>NIC (SRAM)</p>
    <p>Hardware is good at per-packet scheduling</p>
  </div>
  <div class="page">
    <p>Current NIC Design</p>
    <p>Wire</p>
    <p>Typically 8-32 rings</p>
    <p>. . .</p>
    <p>. . . Qdisc queues</p>
    <p>TX buffers</p>
    <p>Packet Scheduler</p>
    <p>Host (DRAM)</p>
    <p>NIC (SRAM)</p>
    <p>Can we get rid of these large buffers?</p>
    <p>Expensive and limited</p>
  </div>
  <div class="page">
    <p>SENIC Design</p>
    <p>Host RAM</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>. . . FIFO queues</p>
    <p>(or ring buffers)</p>
    <p>Packet Scheduler</p>
    <p>Many Tx queues</p>
    <p>class queues stored in host RAM</p>
  </div>
  <div class="page">
    <p>SENIC Design</p>
    <p>Host RAM</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>. . . FIFO queues</p>
    <p>(or ring buffers)</p>
    <p>Packet Scheduler</p>
    <p>Many Tx queues</p>
    <p>class queues stored in host RAM</p>
  </div>
  <div class="page">
    <p>SENIC Design</p>
    <p>Host RAM</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>. . . FIFO queues</p>
    <p>(or ring buffers)</p>
    <p>Packet Scheduler</p>
    <p>Many Tx queues</p>
    <p>Scalability</p>
    <p>Precision and Low CPU overhead</p>
    <p>CPU handles control plane operations</p>
    <p>(Configuring queues, rate limits, packet classification)</p>
  </div>
  <div class="page">
    <p>SENIC Prototypes</p>
    <p>! NetFPGA 10G hardware prototype ! Demonstrates feasibility ! Implements simple token bucket scheduler ! Late binding of DMA transfers from host memory</p>
    <p>! Software prototype ! Dedicated CPU core for network scheduling ! Works with any existing NIC</p>
  </div>
  <div class="page">
    <p>NetFPGA 10G Microbenchmarks</p>
    <p>! Synthesized at 100MHz with 1000 rate limiters</p>
  </div>
  <div class="page">
    <p>! Synthesized at 100MHz with 1000 rate limiters</p>
    <p>! Inter-packet delay for a traffic class</p>
    <p>! Average: within 0.038% of ideal pacer delay</p>
    <p>! Standard deviation: 1.7% of inter-packet delay</p>
    <p>Pkt 1 Pkt 2 Pkt 3 1500B packets</p>
    <p>Is it Accurate?</p>
  </div>
  <div class="page">
    <p>Is it Fast?</p>
    <p>! Scheduling decision latency: ! 5 SRAM lookups (50 ns)</p>
    <p>! 1500B packet at 40Gb/s: 300ns budget</p>
    <p>! Smaller packets: schedule a burst at a time</p>
  </div>
  <div class="page">
    <p>Memcached One-to-All</p>
    <p>UDP All-to-All</p>
    <p>X 10 tenants</p>
    <p>X 8 machines</p>
    <p>SENIC</p>
    <p>Macrobenchmark: Tenant Isolation</p>
  </div>
  <div class="page">
    <p>! Metrics: 1. Memcached tail latency 2. UDP throughput</p>
    <p>! Compare SENIC to: 1. Hierarchical Token Buckets (HTB) 2. Parallel Token Buckets (PTB)</p>
    <p>! Varying memcached tenant load</p>
    <p>Macrobenchmark: Tenant Isolation</p>
  </div>
  <div class="page">
    <p>Memcached Tail Latency</p>
    <p>HTB PTB</p>
    <p>SENIC</p>
    <p>(Lower is better)</p>
  </div>
  <div class="page">
    <p>UDP Tenant Throughput (Closer to 3Gb/s configured limit is better)</p>
    <p>HTB</p>
    <p>PTB</p>
    <p>SENIC</p>
  </div>
  <div class="page">
    <p>UDP Tenant Throughput (Closer to 3Gb/s configured limit is better)</p>
    <p>HTB</p>
    <p>PTB</p>
    <p>SENIC</p>
  </div>
  <div class="page">
    <p>SENIC Supports Other NIC Features</p>
  </div>
  <div class="page">
    <p>TCP Segmentation Offload</p>
    <p>Host Memory</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>Header cached on NIC</p>
  </div>
  <div class="page">
    <p>DMA header and payload for each MTU sized packet</p>
    <p>Host Memory</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>SENIC  TSO</p>
  </div>
  <div class="page">
    <p>Host Memory</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>SENIC  TSO</p>
    <p>No Problem!</p>
  </div>
  <div class="page">
    <p>Host Memory</p>
    <p>NIC</p>
    <p>Wire</p>
    <p>SENIC  TSO</p>
    <p>! 40Gb/s, 1500B MTU: 6.5M DMA transfers per second</p>
    <p>! Measurement from a Mellanox Connect-X3 NIC: ! 13  14M DMA transfers per second supported</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>! Delivers vision of scalable rate limiting</p>
    <p>! Accurate and precise</p>
    <p>! Easily implementable in hardware and software</p>
    <p>Code @ http://sivasankar.me/senic/</p>
  </div>
</Presentation>
