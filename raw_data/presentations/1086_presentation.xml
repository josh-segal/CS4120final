<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Effectively Mitigating I/O Inactivity in vCPU Scheduling</p>
    <p>Weiwei Jia12, Cheng Wang1, Xusheng Chen1, Jianchen Shan2, Xiaowei Shang2, Heming Cui1, Xiaoning Ding2, Luwei Cheng3, Francis C. M. Lau1, Yuexuan Wang1,</p>
    <p>Yuangang Wang4</p>
    <p>Hong Kong University1, New Jersey Institute of Technology2, Facebook3, Huawei4</p>
  </div>
  <div class="page">
    <p>VM consolidation is pervasive in clouds</p>
    <p>Consolidation benefits:  Ease management  Save energy  Improve resource utilization and system throughput</p>
    <p>Virtual Machine Monitor (VMM)</p>
    <p>Virtual Machine (VM 1)</p>
    <p>Physical Machine (PM 1)</p>
    <p>VM 2</p>
    <p>PM n</p>
    <p>. . .</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>VM 2</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
    <p>VM 2</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  Multiple vCPUs sharing one pCPU is often</p>
    <p>E.g., VMWARE suggests 8-10 vCPUs to share one pCPU 5</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  vCPU scheduler schedules and deschedules vCPUs periodically</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  vCPU scheduler schedules and deschedules vCPUs periodically</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  vCPU scheduler schedules and deschedules vCPUs periodically</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>vCPU1 in VM1</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>vCPU1 in VM2 inactive</p>
    <p>active</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  vCPU scheduler schedules and deschedules vCPUs periodically</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>vCPU1 in VM1</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>vCPU1 in VM2 inactive</p>
    <p>active</p>
  </div>
  <div class="page">
    <p>Multiple vCPUs are scheduled to time-share a pCPU</p>
    <p>Physical CPU (pCPU): hardware resources  Virtual CPU (vCPU): processors in VM, threads in VMM  vCPU scheduler schedules and deschedules vCPUs periodically</p>
    <p>VMM</p>
    <p>VM 1</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 2 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>vCPU1 in VM1</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>vCPU1 in VM2 inactive</p>
    <p>active</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>An understudied problem: I/O inactivity</p>
    <p>VM1 vCPU1 activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>HDFS I/O activity</p>
    <p>on vCPU1</p>
    <p>I/O request</p>
    <p>active</p>
    <p>inactive</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>An understudied problem: I/O inactivity</p>
    <p>VM1 vCPU1 activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>HDFS I/O activity</p>
    <p>on vCPU1</p>
    <p>I/O request</p>
    <p>active</p>
    <p>inactive</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>An understudied problem: I/O inactivity</p>
    <p>VM1 vCPU1 activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0</p>
    <p>HDFS I/O activity</p>
    <p>on vCPU1</p>
    <p>I/O request</p>
    <p>active</p>
    <p>inactive</p>
    <p>No I/O requests can be issued on an inactive vCPU Underutilization</p>
    <p>of I/O device</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>VM1 can only use 50% of I/O bandwidth even when VM2 does not use I/O device.  I/O throughput of HDFS in VM1 is only 55% of that on bare-metal.</p>
    <p>I/O inactivity causes low I/O performance in VMs</p>
  </div>
  <div class="page">
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
    <p>Operating System</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
  </div>
  <div class="page">
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
    <p>Operating System</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
  </div>
  <div class="page">
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
    <p>Operating System</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
  </div>
  <div class="page">
    <p>Two I/O-intensive applications (HDFS and MongoDB) show similar I/O throughput on bare-metal</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
    <p>Operating System</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1 pCPU4pCPU3</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4</p>
    <p>VM 1</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>VM1 and VM2 are assigned with the same I/O priority, but MongoDB (VM2) achieves much higher I/O throughput.</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>VM1 and VM2 are assigned with the same I/O priority, but MongoDB (VM2) achieves much higher I/O throughput.</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>VM1 and VM2 are assigned with the same I/O priority, but MongoDB (VM2) achieves much higher I/O throughput.</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
    <p>VM1 and VM2 are assigned with the same I/O priority, but MongoDB (VM2) achieves much higher I/O throughput.</p>
    <p>vCPUs running I/O tasks in MongoDB in VM2 have more time slice to run.  vCPUs running HDFS in VM1 have less time slice to run</p>
    <p>I/O scheduler cannot effectively enforce fairness between VMs</p>
  </div>
  <div class="page">
    <p>Key: decouping I/O activities from vCPU scheduling</p>
    <p>vSlicer[HPDC12]</p>
  </div>
  <div class="page">
    <p>Blue line: vCPU activity</p>
    <p>Black line: I/O activity</p>
    <p>Key: decouping I/O activities from vCPU scheduling</p>
    <p>Vanilla KVM</p>
    <p>active</p>
    <p>inactive</p>
    <p>I/O request</p>
    <p>vSlicer[HPDC12]</p>
    <p>active</p>
    <p>inactive</p>
  </div>
  <div class="page">
    <p>Blue line: vCPU activity</p>
    <p>Black line: I/O activity</p>
    <p>Key: decouping I/O activities from vCPU scheduling</p>
    <p>Vanilla KVM</p>
    <p>active</p>
    <p>inactive</p>
    <p>I/O request</p>
    <p>vSlicer[HPDC12]</p>
    <p>active</p>
    <p>inactive</p>
    <p>inactive inactive inactive inactive</p>
  </div>
  <div class="page">
    <p>Blue line: vCPU activity</p>
    <p>Black line: I/O activity</p>
    <p>Key: decouping I/O activities from vCPU scheduling</p>
    <p>Vanilla KVM</p>
    <p>active</p>
    <p>inactive</p>
    <p>I/O request</p>
    <p>vSlicer[HPDC12]</p>
    <p>xBalloon[SoCC17]</p>
    <p>active</p>
    <p>inactive</p>
    <p>inactive inactive inactive inactive</p>
    <p>inactive inactive</p>
  </div>
  <div class="page">
    <p>Blue line: vCPU activity</p>
    <p>Black line: I/O activity</p>
    <p>Key: decouping I/O activities from vCPU scheduling</p>
    <p>Vanilla KVM</p>
    <p>active</p>
    <p>inactive</p>
    <p>I/O request</p>
    <p>vSlicer[HPDC12]</p>
    <p>xBalloon[SoCC17]</p>
    <p>active</p>
    <p>inactive</p>
    <p>inactive inactive inactive inactive</p>
    <p>inactive inactive</p>
    <p>I/O activity bare-metal</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
    <p>Key observation: each VM often has active vCPUs</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
    <p>Key observation: each VM often has active vCPUs</p>
    <p>Solution: keep I/O tasks on active vCPUs</p>
    <p>- Migrate an I/O task when its vCPU is about to be descheduled</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
    <p>Key observation: each VM often has active vCPUs</p>
    <p>Solution: keep I/O tasks on active vCPUs</p>
    <p>- Migrate an I/O task when its vCPU is about to be descheduled</p>
    <p>- Migrate the I/O task to a vCPU that is NOT to be descheduled soon.</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
    <p>Key observation: each VM often has active vCPUs</p>
    <p>Solution: keep I/O tasks on active vCPUs</p>
    <p>- Migrate an I/O task when its vCPU is about to be descheduled</p>
    <p>- Migrate the I/O task to a vCPU that is NOT to be descheduled soon.</p>
    <p>Benefits: I/O tasks can make continuous progress like on bare-metal</p>
  </div>
  <div class="page">
    <p>Can we decouple I/O activities from vCPU scheduling?</p>
    <p>Our answer is YES!</p>
    <p>Key observation: each VM often has active vCPUs</p>
    <p>Solution: keep I/O tasks on active vCPUs</p>
    <p>- Migrate an I/O task when its vCPU is about to be descheduled</p>
    <p>- Migrate the I/O task to a vCPU that is NOT to be descheduled soon.</p>
    <p>Benefits: I/O tasks can make continuous progress like on bare-metal</p>
    <p>- Migration can be efficient because I/O tasks usually have small working sets.</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>I/O-bound Task</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
    <p>vCPU3</p>
    <p>activity</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
    <p>NOT to be descheduled soon.</p>
    <p>vCPU3</p>
    <p>activity</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
    <p>NOT to be descheduled soon.</p>
    <p>vCPU3</p>
    <p>activity</p>
  </div>
  <div class="page">
    <p>vMigrater: a user-level design</p>
    <p>Hypervisor</p>
    <p>VM vCPU Monitor</p>
    <p>vMigrater</p>
    <p>Task Migrater</p>
    <p>vCPU Scheduler</p>
    <p>pCPU1 pCPU2</p>
    <p>CPU-bound Task</p>
    <p>vCPU1 vCPU2 vCPU3</p>
    <p>Task Detector</p>
    <p>I/O-bound Task</p>
    <p>vCPU1</p>
    <p>activity</p>
    <p>active</p>
    <p>inactive</p>
    <p>Time</p>
    <p>T1 T2T0 T1</p>
    <p>To be descheduled</p>
    <p>NOT to be descheduled soon.</p>
    <p>vCPU3</p>
    <p>activity</p>
  </div>
  <div class="page">
    <p>Challenges with the user-level design (1/3) - How to detect I/O tasks quickly?</p>
    <p>Existing resource monitors cannot respond to execution phase changes quickly</p>
    <p>- eg., Linux top and iotop refresh measurements periodically every a few seconds</p>
    <p>- Applications often have bursty I/O phases finished within a refreshing period.</p>
  </div>
  <div class="page">
    <p>Challenges with the user-level design (1/3) - How to detect I/O tasks quickly?</p>
    <p>Existing resource monitors cannot respond to execution phase changes quickly</p>
    <p>- eg., Linux top and iotop refresh measurements periodically every a few seconds</p>
    <p>- Applications often have bursty I/O phases finished within a refreshing period.</p>
    <p>Event-driven method in vMigrator</p>
    <p>- Monitor I/O events time at OS block I/O layer</p>
    <p>- Calculate the fraction between the I/O events time and the whole period</p>
    <p>- respond quickly when task becomes I/O intensive (&lt;1 millisecond)</p>
  </div>
  <div class="page">
    <p>Implementation challenges (2/3) - when to migrate?</p>
    <p>When the vCPUs are to be inactive</p>
    <p>Nave approach: monitor inactive/active vCPUs in hypervisor layer: not secure and portable</p>
    <p>Our approach</p>
    <p>- a heartbeat-like mechanism: timer events as heartbeats</p>
    <p>- a vCPU cannot process timer events when it is inactive</p>
    <p>- vCPU time slice: timer differences between the start timer and end timer when the vCPU is active</p>
  </div>
  <div class="page">
    <p>Challenges with the user-level design (3/3) - migrate to which vCPU(s)?</p>
    <p>Migrate to vCPUs with enough remaining time slice  Estimation of time slice still relies on the heartbeat-like mechanism</p>
    <p>Nave approach: consolidate all I/O tasks to the vCPU with the longest remaining time slice</p>
    <p>- Problem: the vCPU may be overloaded</p>
    <p>Our approach: distribute I/O tasks to vCPUs based on I/O workload and remaining time slice.</p>
    <p>- tasks with heavier I/O workload on vCPUs with more time slice 53</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>Dell PowerEdge R430 with 12 cores, a 1TB HDD, and a 1TB SSD</p>
    <p>Both VMs and VMM (linux QEMU/KVM) use Ubuntu Linux 16.04</p>
    <p>Each VM has 12 vCPUs and 4GB DRAM</p>
    <p>Compared with vSlicer [HPDC12] and xBalloon [SoCC17]</p>
  </div>
  <div class="page">
    <p>Evaluation applications and workloads</p>
    <p>Application Workload</p>
    <p>HDFS Sequentially read 16GB with HDFS TestDFSIO</p>
    <p>LevelDB Randomly scan table with db_bench</p>
    <p>MediaTomb Concurrent requests on teanscoding a 1.1GB video</p>
    <p>HBase Randomly read 1GB with Hbase PerfEval</p>
    <p>PostMark Concurrent requests on a mail server</p>
    <p>Nginx Concurrent requests on watermarking images</p>
    <p>MongDB Sequentially scan records with YCSB</p>
  </div>
  <div class="page">
    <p>Evaluation questions</p>
    <p>How much performance improvement can be achieved with vMigrater, compared with vanilla KVM and two related systems?</p>
    <p>Can vMigrater help the I/O scheduler in the VMM to achieve fairness between VMs?</p>
    <p>How robust is vMigrater to varying workloads?</p>
    <p>What is the overhead incurred by vMigrater?</p>
    <p>What is vMigraters performance when the workload in a VM varies over time</p>
    <p>How does vMigrater scale to the number of shared vCPUs on a pCPU?</p>
  </div>
  <div class="page">
    <p>Throughput on seven applications (4 vCPUs sharing per pCPU)</p>
    <p>vMigraters throughput is 192% higher than Vanilla KVM, 75% higher than vSlicer, 84% higher than xBalloon on average</p>
  </div>
  <div class="page">
    <p>HDFS performance analysis</p>
    <p>HDFS Vanilla vSlicer xBalloon vMigrater</p>
    <p>I/O inactivity time (seconds)</p>
  </div>
  <div class="page">
    <p>MediaTomb performance analysis</p>
    <p>HDFS Vanilla vSlicer xBalloon vMigrater</p>
    <p>I/O inactivity time (seconds)</p>
    <p>MediaTomb Vanilla vSlicer xBalloon vMigrater</p>
    <p>I/O inactivity time (seconds)</p>
    <p>vMigrater has big I/O inactivity periods on MediaTomb  MediaTomb combine computation and I/O in each thread so the migration cost</p>
    <p>is higher</p>
  </div>
  <div class="page">
    <p>Fairness of I/O Scheduler in VMM</p>
    <p>VMM</p>
    <p>Physical Machine</p>
    <p>pCPU2pCPU1</p>
    <p>vCPU1</p>
    <p>pCPU4pCPU3</p>
    <p>vCPU2 vCPU3 vCPU4 vCPU1 vCPU2 vCPU3 vCPU4</p>
    <p>VM 2VM 1</p>
  </div>
  <div class="page">
    <p>Robustness to varying workloads</p>
  </div>
  <div class="page">
    <p>Robustness to varying workloads</p>
  </div>
  <div class="page">
    <p>Robustness to varying workloads</p>
  </div>
  <div class="page">
    <p>Robustness to varying workloads</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>I/O inactivity problem</p>
    <p>- Performance degradation</p>
    <p>- I/O scheduler unfairness</p>
    <p>vMigrater: effectively mitigating I/O inactivity problem</p>
    <p>- Performance is close to bare-metal</p>
    <p>- Regain fairness</p>
    <p>vMigraters source code: https://github.com/hku-systems/vMigrater</p>
  </div>
  <div class="page">
    <p>Thank you</p>
    <p>Questions?</p>
  </div>
</Presentation>
