<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>PAC Model-Free</p>
    <p>Reinforcement Learning</p>
    <p>Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, Michael L. Littman</p>
    <p>RL3, Rutgers University CSE, Univ. of California, San Diego</p>
    <p>TTI Chicago  Yahoo! Research</p>
    <p>Presenter: Lihong Li</p>
    <p>With thanks to: Sham Kakade, Yishay Mansour, Ali Nouri, Satinder Singh, and Tom Walsh.</p>
  </div>
  <div class="page">
    <p>WARNING: This is a theoretical work about complexity results.</p>
    <p>Someone told me that each equation I included in the book would halve the sales. I therefore resolved not to have any equations at all.</p>
    <p>Stephen Hawking (A Brief History of Time, 1988)</p>
    <p>BUT we are computer scientists.</p>
    <p>SO Im going to use three equations.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Consider reinforcement learning</p>
    <p>of a single agent  in a fully observable environment  based on a single thread of experience (no resets or generative models)</p>
    <p>Theoretical contributions: Delayed Q-learning which</p>
    <p>is model-free  improves on previous complexity results</p>
    <p>space complexity  per-step computational complexity  sample complexity (of exploration)</p>
    <p>answers the open question of efficient model-free RL affirmatively</p>
  </div>
  <div class="page">
    <p>Talk Outline</p>
    <p>Introduction</p>
    <p>Delayed Q-learning</p>
    <p>Proof Sketch</p>
    <p>Future Directions</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>. Introduction Delayed Q-learning</p>
    <p>Main Results</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Notation</p>
    <p>Consider finite Markov decision processes (MDPs) with</p>
    <p>state space S,  action space A,  discount factor   [0, 1),  transition function T (s|sa), and  bounded rewards R(s, a)  [0, 1].</p>
    <p>A deterministic Markov policy  : S 7 A. Given a trajectory: s1, a1, r1, s2, a2, r2,    , st, at, rt,    . Value functions:</p>
    <p>V (s) := E{r1 + r2 +  2r3 +    | s1 = s, }</p>
    <p>Q(s, a) := E{r1 + r2 +  2r3 +    | s1 = s, a1 = a, }</p>
    <p>V (s) := V</p>
    <p>(s) = max</p>
    <p>V (s)</p>
    <p>Q(s, a) := Q</p>
    <p>(s, a) = max</p>
    <p>Q(s, a)</p>
  </div>
  <div class="page">
    <p>Reinforcement Learning</p>
    <p>Objective</p>
    <p>to learn the optimal policy or value function  based on sampling of (or interaction with) the environment  without knowing T and R.</p>
    <p>Challenges:</p>
    <p>exploration vs. exploitation  temporal credit assignment  scaling up  generalization</p>
  </div>
  <div class="page">
    <p>Performance Criteria</p>
    <p>We often trade one factor for another:</p>
    <p>per-step computational complexity</p>
    <p>space complexity</p>
    <p>model-free: o(S2A)  model-based: (S2A)</p>
    <p>sample complexity</p>
    <p>(Kakade, 2003): #timesteps that the algorithm does not behave -optimally.</p>
    <p>An algorithm is PAC-MDP if w.h.p. its sample complexity is bounded by a polynomial in relevant quantities.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>PAC-MDP non-PAC-MDP/unknown</p>
    <p>modelfree</p>
    <p>Q-learning, Sarsa</p>
    <p>modelbased</p>
    <p>E3, Rmax, MBIE Dyna-Q, prioritized sweeping, certainty equivalence, adaptive RTDP</p>
    <p>computation space (best) sample</p>
    <p>E3 (S2A) (S2A) polynomial</p>
    <p>Rmax (S2A) (S2A) O (</p>
    <p>S2A 3(1)6</p>
    <p>)</p>
    <p>MBIE (S2A) (S2A) O (</p>
    <p>S2A 3(1)6</p>
    <p>)</p>
    <p>Q-learning O(log(A)) (SA) can be EXP Sarsa O(log(A)) (SA) can be EXP</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>PAC-MDP non-PAC-MDP/unknown</p>
    <p>modelfree</p>
    <p>Delayed Q-learning Q-learning, Sarsa</p>
    <p>modelbased</p>
    <p>E3, Rmax, MBIE Dyna-Q, prioritized sweeping, certainty equivalence, adaptive RTDP</p>
    <p>computation space (best) sample</p>
    <p>E3 (S2A) (S2A) polynomial</p>
    <p>Rmax (S2A) (S2A) O (</p>
    <p>S2A 3(1)6</p>
    <p>)</p>
    <p>MBIE (S2A) (S2A) O (</p>
    <p>S2A 3(1)6</p>
    <p>)</p>
    <p>Q-learning O(log(A)) (SA) can be EXP Sarsa O(log(A)) (SA) can be EXP</p>
    <p>Delayed Q-learning O(log(A)) (SA) O (</p>
    <p>SA</p>
    <p>4(1)8</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Delayed Q-learning</p>
    <p>Introduction</p>
    <p>. Delayed Q-learning Main Results</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Algorithm Overview</p>
    <p>During execution</p>
    <p>Maintain Q-values for all (s, a), denoted by Qt(s, a) at time t;  Define Vt(s) = maxa Qt(s, a).</p>
    <p>Delayed Q-learning:</p>
    <p>). 3. At time t = 1, 2, 3,    :</p>
    <p>(a) selects greedy action: at  arg maxa Qt(st, a); (b) observes immediate reward rt and next state st+1; (c) one-step lookahead backup value: rt +  maxa Qt(st+1, a); (d) updates Qt(st, at):</p>
  </div>
  <div class="page">
    <p>Raw Update Rule</p>
    <p>Suppose (s, a) is visited m times since last update:</p>
    <p>time k1 k2 ki t=km</p>
    <p>an attempted update</p>
    <p>collected at km</p>
    <p>intermediate visits</p>
    <p>(s,a)= ( , )</p>
    <p>The respective backup values: rk1 + Vk1 (sk1+1), rk2 + Vk2 (sk2+1),    , rkm + Vkm (skm+1)</p>
    <p>Q-learning at time ki:</p>
    <p>Qki+1(s, a)  (1  )Qki (s, a) +  (rki + Vki (ski+1)) .</p>
    <p>The delayed update rule at time km:</p>
    <p>Qt+1(s, a)  1</p>
    <p>m</p>
    <p>m</p>
    <p>i=1</p>
    <p>(</p>
    <p>rki + Vki (ski+1) )</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>Refined Update Rule</p>
    <p>The raw update rule: Qt+1(s, a)  1 m</p>
    <p>m i=1</p>
    <p>(</p>
    <p>rki + Vki (ski+1) )</p>
    <p>.</p>
    <p>To prove PAC-MDP-ness, make several changes:</p>
    <p>Add a bonus 1 = (</p>
    <p>Qt+1(s, a)  1</p>
    <p>m</p>
    <p>m</p>
    <p>i=1</p>
    <p>(</p>
    <p>rki + Vki (ski+1) )</p>
    <p>+1.</p>
    <p>Update of Q(s, a) succeeds only when</p>
    <p>it results in a minimum decrease of 1, and  some Q(, ) is changed since last update of Q(s, a).</p>
    <p>If update unsuccessful</p>
    <p>keep current Q-values,  discard these m samples, and  start collecting another m samples.</p>
  </div>
  <div class="page">
    <p>Comparison to Q-learning</p>
    <p>Similarities:</p>
    <p>model-free, learns Q-values, algorithmic structure, online, etc.</p>
    <p>Differences:</p>
    <p>optimistic initialization</p>
    <p>updates</p>
    <p>delayed until m samples  no learning rates  may fail  finite #updates  Q-values monotonically decrease</p>
    <p>always chooses greedy actions</p>
    <p>never has exponential sample complexity</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>Introduction</p>
    <p>Delayed Q-learning</p>
    <p>. Main Results Conclusions</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>Set</p>
    <p>m =</p>
    <p>log (</p>
    <p>SA (1)</p>
    <p>)</p>
    <p>2(1  )4</p>
    <p>.</p>
    <p>Then Delayed Q-learning enjoys provable efficiency:</p>
    <p>Per-step computational complexity: O(log(A))</p>
    <p>Space complexity: O(SA)</p>
    <p>Sample complexity: O(SA)</p>
    <p>Similar results for finite-horizon cases.</p>
  </div>
  <div class="page">
    <p>Known State-Actions</p>
    <p>Known State-Actions:</p>
    <p>Kt =</p>
    <p>{</p>
    <p>(s, a)</p>
    <p>Qt(s, a)</p>
    <p>(</p>
    <p>R(s, a) +</p>
    <p>s</p>
    <p>T (s|sa)Vt(s )</p>
    <p>)</p>
    <p>31</p>
    <p>}</p>
    <p>Escape probability:</p>
    <p>p = Pr</p>
    <p>{</p>
    <p>escape Kt in H = O</p>
    <p>(</p>
    <p>(</p>
    <p>(1  )</p>
    <p>))</p>
    <p>steps</p>
    <p>}</p>
    <p>.</p>
    <p>),( as</p>
    <p>tK AS</p>
  </div>
  <div class="page">
    <p>Proof Sketch</p>
    <p>because of the refined update rule  allows Hoeffdings bound be used in our proof below</p>
    <p>p small = Bellman residuals small w.h.p.  = actual value functions are close to V</p>
    <p>= near-optimal policies</p>
    <p>(s, a) / Kk1 and is visited m times = Q(s, a) is updated at time km w.h.p.</p>
    <p>but #updates is bounded by P  = bound #occurrences of this undesired case</p>
    <p>Conclusion: Delayed Q-learning is PAC-MDP.</p>
  </div>
  <div class="page">
    <p>Related Work on Sample Complexity</p>
    <p>Model-based</p>
    <p>(Fiechter, COLT94): assumes a reset  E3 (Kearns-Singh ICML98): explicitly explores or exploits  Rmax (Brafman-Tennenholtz IJCAI01) / MBIE (Strehl-Littman</p>
    <p>ICML05): optimism in the face of uncertainty  RTDP-RMAX and RTDP-IE (Strehl-Li-Littman UAI06): O(S log A)</p>
    <p>computational complexity</p>
    <p>Model-free</p>
    <p>Phased Q-learning (Kearns-Singh NIPS99): averaging updates to simulate Bellman backups</p>
    <p>(Even-dar-Mansour JMLR03): assumes an efficient exploration policy</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Introduction</p>
    <p>Delayed Q-learning</p>
    <p>Main Results</p>
    <p>. Conclusions</p>
  </div>
  <div class="page">
    <p>Future Directions</p>
    <p>Closing the gap between upper and lower bounds of sample complexity.</p>
    <p>best known lower bound (Kakade 2003): ( SA (1)2</p>
    <p>)</p>
    <p>Extending results to possibly infinite MDPs</p>
    <p>generalization</p>
    <p>Employing structures</p>
    <p>factored representations (e.g., factored E3)  state abstraction</p>
  </div>
  <div class="page">
    <p>Take-Home Messages</p>
    <p>Solved the open question of efficient model-free RL  Delayed Q-learning: the first algorithm that is</p>
    <p>model free  proven to be efficient, and  without resets or generative models</p>
    <p>Sample complexity (O(SA)) is less than MDP description complexity (O(S2A))</p>
    <p>only O(SA) quantities are to be estimated;  MDP representations are not compact in the sense of efficiently learning</p>
    <p>near-optimal behavior.</p>
    <p>Sample complexity does not increase significantly compared to deterministic MDPs (O(SA)).</p>
  </div>
</Presentation>
