<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Optimizing Flash-based Key-value Cache Systems</p>
    <p>The 8th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage16)</p>
    <p>Department of Computing, Hong Kong Polytechnic University</p>
    <p>Computer Science &amp; Engineering, Louisiana State University</p>
    <p>Zhaoyan Shen, Feng Chen, Yichen Jia, Zili Shao</p>
  </div>
  <div class="page">
    <p>Key-value Information</p>
    <p>Key-value access is dominant in web services</p>
    <p>Many apps simply store and retrieve key-value pairs</p>
    <p>Key-value cache is the first line of defense  Benefits: Improve throughput, reduce latency, reduce server load</p>
    <p>In-memory KV cache is popular (Memcache)</p>
    <p>High speed but has cost, power, capacity problem</p>
  </div>
  <div class="page">
    <p>Key-value Slabs (Flash LBA)</p>
    <p>Log-Structured Logical Flash Space</p>
    <p>Flash based Key-value Cache</p>
    <p>+</p>
    <p>Slab (Slab, Slot)Key</p>
    <p>Hash Index (Memory)</p>
    <p>SHA-1(Key_1)</p>
    <p>Key-value cache Flash SSD</p>
    <p>In-memory hash map to track key-to-value mapping</p>
    <p>Slabs are used in a log-structured way</p>
    <p>Updated value item written to a new location and old values recycled later</p>
  </div>
  <div class="page">
    <p>Critical Issues</p>
    <p>Redundant mapping</p>
    <p>Double garbage collection</p>
    <p>Over-over-provisioning</p>
  </div>
  <div class="page">
    <p>Critical Issue 1: Redundant Mapping</p>
    <p>Redundant mapping at application- and FTL-level  KVC: An in-memory hash table (Key  Slab, Offset)</p>
    <p>FTL: An on-device page mapping table (LBA  PBA)</p>
    <p>Problems  Two mapping structures (unnecessarily) co-exist at two levels</p>
    <p>A significant waste of on-device DRAM space (e.g., 1GB for 1TB)  The on-device DRAM buffer is costly, unreliable, and could be used for buffering.</p>
    <p>SHA-1(Key)</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Slab SpaceHash table</p>
    <p>(1, 2)K1</p>
    <p>Mapping Table</p>
    <p>AAA, BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>PBALBA</p>
    <p>Critical Issues</p>
    <p>KVC software Mapping FTL Mapping in hardware</p>
  </div>
  <div class="page">
    <p>Critical Issue 2: Double Garbage Collection</p>
    <p>Garbage collection (GC) at app- and FTL- levels  KVC: Recycle deleted or changed key-value items</p>
    <p>FTL: Recycle trimmed or changed pages</p>
    <p>Problems  Semantic validity of a key-value entry is not seen at FTL</p>
    <p>Redundant data copy operation</p>
    <p>SHA-1(Key)</p>
    <p>Hash table</p>
    <p>(1, 2)K1</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>PBASlab Space</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>LBA</p>
    <p>CCC,DDD</p>
    <p>Mapping Table</p>
    <p>(2, 1) AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>CCC,DDD</p>
    <p>FTL-level GCKVC-level GC</p>
    <p>Critical Issues</p>
  </div>
  <div class="page">
    <p>Critical Issue 3: Over-over-provisioning</p>
    <p>Over-provisioning at FTL-level  FTL has a portion (20-30%) of flash as Over-Provisioning Space (OPS)</p>
    <p>OPS space is invisible and unusable to the host applications</p>
    <p>Problems  OPS is reserved for dealing the worst-case situation, as a storage</p>
    <p>Over-over provisioning for Key-value caches  Key-value caches are dominated by read (GET) traffic, not writes</p>
    <p>Key-value cache hit ratio is highly sensitive to usable cache size  If 20-30% space can be released, the cache hit ratio can be greatly improved</p>
    <p>SHA-1(Key)</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Slab SpaceHash table</p>
    <p>(1, 2)K1</p>
    <p>LBA</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>PBA</p>
    <p>FFF,GGG HHH,III JJJ, KKK</p>
    <p>Mapping Table</p>
    <p>Unusable</p>
    <p>Critical Issues</p>
  </div>
  <div class="page">
    <p>Semantic Gap Problem</p>
    <p>Semantic Gap</p>
    <p>Fine-grained GC  Key-to-value mapping  Validity of slab entries</p>
    <p>Physical data layout on flash  Direct flash memory control  Proper mapping granularity</p>
    <p>In the current SW/HW architecture, we can do little to address these three issues.</p>
    <p>Key-value cache Flash SSD</p>
    <p>Critical Issues</p>
  </div>
  <div class="page">
    <p>Optimization Goals</p>
    <p>Redundant mapping  Single mapping</p>
    <p>Double garbage collection  App-driven GC</p>
    <p>Over-over-provisioning  Dynamic OPS</p>
  </div>
  <div class="page">
    <p>Design Overview</p>
    <p>An enhanced flash-aware key-value cache</p>
    <p>A thin intermediate library layer (libssd)</p>
    <p>A specialized flash memory PCI-E SSD hardware</p>
    <p>Flash SSD</p>
    <p>Garbage Collection</p>
    <p>Page Mapping</p>
    <p>Bad Block Mgm.</p>
    <p>Flash Operations</p>
    <p>Wear Leveling</p>
    <p>OPS Mgm.</p>
    <p>Operating Systems</p>
    <p>Page Cache Device Driver</p>
    <p>I/O Scheduling</p>
    <p>Key-value Cache</p>
    <p>K/V Mapping</p>
    <p>Slab Mgm. Cache Mgm.</p>
    <p>Open-channel SSD</p>
    <p>Flash Operations</p>
    <p>Operating Systems</p>
    <p>Page Cache Device Driver</p>
    <p>I/O Scheduling</p>
    <p>Library (libssd)</p>
    <p>Operation Conversion</p>
    <p>Slab/Flash Translation</p>
    <p>Bad Block Mgm.</p>
    <p>Key-value Cache</p>
    <p>Key/Slab Mapping</p>
    <p>KVC-driven GC</p>
    <p>OPS Management</p>
    <p>Slab Manager Cache Manager</p>
  </div>
  <div class="page">
    <p>An Enhanced Flash-aware Key-value Cache</p>
    <p>Slab management</p>
    <p>Unified direct mapping</p>
    <p>Garbage collection</p>
    <p>OPS management</p>
    <p>Key-value Cache</p>
  </div>
  <div class="page">
    <p>Our choice  Directly use a flash block as a slab (8MB)</p>
    <p>Slab buffer: An in-memory slab maintained for each class</p>
    <p>Parallelize and asynchronize the slab write I/Os to the flash memory</p>
    <p>Round-robin allocation of in-flash slab for load-balancing across channels</p>
    <p>In-Memory Slab Buffer</p>
    <p>Slab Management</p>
    <p>Class i Class i+1 Class nClass i+2</p>
    <p>Key/Value</p>
    <p>Channel #0 Channel #1 Channel #2 Channel #3 Channel #4</p>
    <p>Key-value Cache</p>
    <p>Flash SSD</p>
  </div>
  <div class="page">
    <p>SHA-1(Key)</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Slab SpaceHash table</p>
    <p>(1, 2)K1</p>
    <p>Mapping Table</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Flash Memory</p>
    <p>Unified Direct Mapping</p>
    <p>Collapse multiple levels of indirect mapping to only one  Prior mapping: KeySlabLBAPBA</p>
    <p>Current mapping: KeySlab (i.e., Flash Block)</p>
    <p>Benefits  Removes the time overhead for lookup intermediate layers (Speed+)</p>
    <p>Only one single must-have in-memory hash table is needed (Cost-)</p>
    <p>On-device RAM space can be completely removed (or for other uses)</p>
    <p>Key-value Cache</p>
    <p>SHA-1(Key)</p>
    <p>Hash table</p>
    <p>(Slab, Offset)K1</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Flash Memory</p>
    <p>Slab #1</p>
    <p>Slab #2</p>
    <p>CDC, CDC</p>
  </div>
  <div class="page">
    <p>One single GC is driven directly by key-value cache system  All slab writes are in units of blocks (no need for device-level GC)</p>
    <p>GC is directly triggered and controlled by application-level KVC</p>
    <p>Two GC policies  Greedy GC: the least occupied slab is evicted to move minimum slots</p>
    <p>Quick clean: the LRU slab is immediately dropped recycling valid slots</p>
    <p>Adaptively used for different circumstances (busy or idle)</p>
    <p>Target Slab</p>
    <p>App-driven Garbage Collection</p>
    <p>Key-value Cache</p>
    <p>Greedy GC</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>copy CCC,DDD</p>
    <p>Victim Slab</p>
    <p>AAA,BBB BBB,CCC CCC,DDD DDD,EEE</p>
    <p>Quick Clean</p>
    <p>eee,fff ccc,ddd bbb,ccc aaa,bbb</p>
    <p>LRU Slab MRU Slab</p>
  </div>
  <div class="page">
    <p>Over-Provisioning Space Management</p>
    <p>Dynamically tuning OPS space online  Rationale  KVC is typically read-intensive and OPS can be small</p>
    <p>Goal  keep just enough OPS space (adaptive to intensity of writes)</p>
    <p>OPS management policies  Heuristic method: An OPS window (WL and WH) to estimate size</p>
    <p>Low watermark hit  Trigger quick clean, raise the OPS window</p>
    <p>High watermark hit  OPS is over-allocated, lower the OPS window</p>
    <p>Key-value Cache</p>
    <p>O P</p>
    <p>S</p>
    <p>Time</p>
    <p>WH</p>
    <p>WL</p>
    <p>Max OSP</p>
    <p>Quick clean initiated</p>
  </div>
  <div class="page">
    <p>Preliminary Experiments</p>
    <p>Implementation  Key-value cache on Twitters Fatcache to fit hardward</p>
    <p>Libssd Library (621 lines of code in C)</p>
    <p>Experimental Setup  Intel Xeon E-1225, 32GB Memory, 1TB Disk, Open-Channel SSD</p>
    <p>Ubuntu 14.04 LTS, Linux 3.17.8, Ext4 filesystem</p>
    <p>Hardware: Open-channel SSD  A PCI-E based with 12 channel, and 192 LUNs</p>
    <p>Direct control to the device (via ioctl interface)</p>
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>SET  Throughput and Latency</p>
    <p>SET Workloads: 40milliom requests of 1KB key/value pairs</p>
    <p>Both set throughput/latency from our scheme are the best</p>
    <p>Experiments</p>
    <p>Our Scheme</p>
    <p>Stock Fatcache</p>
    <p>Our Scheme Stock Fatcache</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>KV stores become critical as they are one of the most important building blocks in modern web infrastructures and high-performance data-intensive applications.</p>
    <p>We build a highly efficient flash-based cache system which enables three benefits, namely a unified single-level direct mapping, a cache-driven fine-grained garbage collection, and an adaptive over-provisioning scheme</p>
    <p>We are implementing a prototype on the Open-Channel SSD hardware and our preliminary results show that it is highly promising</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>GET  Throughput and Latency</p>
    <p>GET performance is largely determined by the raw speed</p>
    <p>GET latencies are among the lowest in the set of SSDs</p>
    <p>Our Scheme</p>
    <p>Stock Fatcache</p>
    <p>Our Scheme Stock Fatcache</p>
  </div>
  <div class="page">
    <p>SET Latencies  A Zoom-in View</p>
    <p>The direct control successfully removes high cost GC effects</p>
    <p>Quick clean removes long I/Os under high write pressure</p>
    <p>Our Scheme Stock Fatchche + KingSpec</p>
  </div>
  <div class="page">
    <p>Block Erase Count</p>
    <p>Trace collected with running Fatcache on Samsung SSD</p>
    <p>Block trace is replayed on MSRs SSD simulator for erase count</p>
    <p>Our solution reduces erase count by 30%</p>
    <p>Erase Count</p>
    <p>SSDSim Open-channel SSD</p>
    <p>Quick Clean kicks in</p>
  </div>
  <div class="page">
    <p>Effect of the In-memory Slab Buffer</p>
    <p>10x buffer size difference does not affect latency significantly</p>
    <p>A small (56MB) in-memory slab buffer is sufficient for use</p>
  </div>
</Presentation>
