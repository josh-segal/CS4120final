<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>KyotoEBMT System Descrip3on for the</p>
    <p>John Richardson Raj Dabre Chenhui Chu Fabien Cromires Toshiaki Nakazawa Sadao Kurohashi</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Overview of the system</p>
    <p>Improvements since WAT2014</p>
    <p>Results for WAT2015</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Overview of Kyoto-EBMT</p>
  </div>
  <div class="page">
    <p>KyotoEBMT Overview  Example-Based MT paradigm  Need parallel corpus  Few language-specific assumpJons</p>
    <p>sJll a few language-specific rules</p>
    <p>Tree-to-Tree Machine TranslaJon  Maybe the least commonly used variant of x-to-x  SensiJve to parsing quality of both source and target languages  Maximize the chances of preserving informaJon</p>
    <p>Dependency trees  Less commonly used than ConsJtuent trees  Most natural for Japanese  Should contain all important semanJc informaJon</p>
  </div>
  <div class="page">
    <p>KyotoEBMT pipeline</p>
    <p>Somehow classic pipeline  1- Preprocessing of the parallel corpus  2- Processing of input sentence  3- Decoding/Tuning/Reranking</p>
    <p>Tuning and reranking done with kbMira  seems to work beVer than PRO for us</p>
  </div>
  <div class="page">
    <p>Other specifici3es</p>
    <p>No phrase-table  all translaJon rules computed on-the-fly for each input  cons:</p>
    <p>possibly slower (but not so slow)  compuJng significance/ sparse features more complicated</p>
    <p>pros:  full-context available for compuJng features  no limit on the size of matched rules  possibility to output perfect translaJon when input is very similar to an example</p>
    <p>Flexible translaJon rules  OpJonal words  AlternaJve inserJon posiJons  Decoder can process flexible rules more efficiently than a long list of alternaJve rules</p>
    <p>some flexible rules may actually encode &gt;millions of standard rules</p>
  </div>
  <div class="page">
    <p>Flexible Rules Extracted on-the-fly</p>
    <p>(plasJc)</p>
    <p>(for example)</p>
    <p>the</p>
    <p>hydrogen is</p>
    <p>produced from</p>
    <p>natural gas and</p>
    <p>petroleum at</p>
    <p>present</p>
    <p>raw</p>
    <p>X(plasJc) is</p>
    <p>petroleum</p>
    <p>produced</p>
    <p>from</p>
    <p>Y(for example)</p>
    <p>?</p>
    <p>Y(for example)</p>
    <p>Y(for example)</p>
    <p>raw* opJonal word</p>
    <p>Input:</p>
    <p>Matched Example:</p>
    <p>Flexible translaJon rule created on-the-fly:</p>
    <p>X: Simple case (X has an equivalent in the source example) Y: ambiguous inserJon posiJon raw: null-aligned -&gt; opJonal</p>
    <p>(encode many translaJon opJons at once)</p>
  </div>
  <div class="page">
    <p>Other specifici3es</p>
    <p>No phrase-table  all translaJon rules computed on-the-fly for each input  cons:</p>
    <p>possibly slower (but not so slow)  compuJng significance/ sparse features more complicated</p>
    <p>pros:  full-context available for compuJng features  no limit on the size of matched rules  possibility to output perfect translaJon when input is very similar to an example</p>
    <p>Flexible translaJon rules  OpJonal words  AlternaJve inserJon posiJons  Decoder can process flexible rules more efficiently than a long list of alternaJve rules</p>
    <p>some flexible rules may actually encode &gt;millions of standard rules</p>
  </div>
  <div class="page">
    <p>Improvements since WAT2014</p>
  </div>
  <div class="page">
    <p>KyotoEBMT improvements</p>
    <p>Our system is very sensiJve to parsing errors  ConJnuous improvements to  Juman  KNP  SKP</p>
    <p>Added support for parse forests  (compact representaJons)</p>
    <p>Forest parses</p>
  </div>
  <div class="page">
    <p>Forest Input</p>
    <p>A parJal soluJon to the issues of Tree-to-Tree MT  Can help with parsing errors  Can help with syntacJc divergences</p>
    <p>In WAT2014,  we used 20-best input parses  n-best list of all inputs merged and reranked</p>
    <p>Now, with forest:  an exponenJal number of input parses can be encoded  the selecJon of parses is done during decoding</p>
  </div>
  <div class="page">
    <p>KyotoEBMT improvements</p>
    <p>System is also very sensiJve to alignment errors  We used to correct alignments by using dependency trees (Nakazawa and Kurohashi, 2012)  Now we further improve them with Nile (Riesa et al., 2011)</p>
    <p>Forest parses</p>
  </div>
  <div class="page">
    <p>Alignment Improvements  Used Nile (Riesa et al., 2011) to improve the alignment  As suggested by (Neubig and Duh, 2014)  Require us to parse into consJtuent trees as well</p>
    <p>Ckylark parser for Japanese (Oda+, 2015)  Berkeley Parser for Chinese/English</p>
    <p>Nile becomes the third element of an alignment pipeline</p>
    <p>Giza++ (Nakazawa and Kurohashi, 2012) Nile</p>
    <p>with dependency trees with consJtuent trees</p>
    <p>F:0.63 F:0.69 F:0.75 JC alignment F -&gt;</p>
    <p>(Giza++ / Nile only -&gt; F:0.70)</p>
  </div>
  <div class="page">
    <p>KyotoEBMT improvements</p>
    <p>Many small improvements  BeVer handling of flexible rules  Bug fixes</p>
    <p>10 new features  alignment score  context similarity score based on word2vec vectors</p>
    <p>Forest parses</p>
  </div>
  <div class="page">
    <p>KyotoEBMT improvements</p>
    <p>Reranking  Previously used features:  7-gram language model  RNNLM language model</p>
    <p>Now also using a Neural MT based bilingual Language Model</p>
    <p>Forest parses</p>
  </div>
  <div class="page">
    <p>Bilingual Neural Network Language Model</p>
    <p>Combine Neural MT with EBMT  We use the state-of-the-art model described by (Bahdanau et al., 2015)</p>
    <p>Model seen as a Language Model condiJonalized on the input  Remarks:</p>
    <p>Processing Japanese and Chinese as sequences of characters gave good results  No need to limit vocabulary (~4000/6000 characters for J/C)  Avoid segmentaJon issues  Faster training</p>
    <p>Neural MT models alone produced bad translaJons  eg. Character BLEU for C-&gt;J almost half that of KyotoEBMT</p>
    <p>Reranking performances saturates before MT performances</p>
    <p>Reranked BLEU/ NeuralMT char-BLEU vs Epochs for J-&gt;C</p>
    <p>Neural MT cBLEU reranked BLEU</p>
    <p>KyotoEBMT cBLEU</p>
  </div>
  <div class="page">
    <p>KyotoEBMT improvements</p>
    <p>Improved working methods (that maVers!)</p>
    <p>automaJc nightly tesJng for variaJons in BLEU/ asserJon errors/ memory leaks</p>
    <p>Overall improvements across all the pipeline  EsJmaJng the global contribuJon of each element is tough, but here are the final results,</p>
    <p>Forest parses</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>Results for WAT2015</p>
    <p>Reranking BLEU RIBES HUMAN</p>
    <p>J-&gt;E NO 21.31 (+0.71) 70.65 (+0.53) 16.50 YES 22.89 (+1.82) 72.46 (+2.56) 32.50</p>
    <p>E-&gt;J NO 30.69 (+0.92) 76.78 (+1.57) 40.50 YES 33.06 (+1.97) 78.95 (+2.99) 51.00</p>
    <p>J-&gt;C NO 29.99 (+2.78) 80.71 (+1.58) 16.00 YES 31.40 (+3.83) 82.70 (+3.87) 12.50</p>
    <p>C-&gt;J NO 36.30 (+2.73) 81.97 (+1.87) 16.75 YES 38.53 (+3.78) 84.07 (+3.81) 18.50</p>
  </div>
  <div class="page">
    <p>Results for WAT2015</p>
    <p>Reranking BLEU RIBES HUMAN</p>
    <p>J-&gt;E NO 21.31 (+0.71) 70.65 (+0.53) 16.50 YES 22.89 (+1.82) 72.46 (+2.56) 32.50</p>
    <p>E-&gt;J NO 30.69 (+0.92) 76.78 (+1.57) 40.50 YES 33.06 (+1.97) 78.95 (+2.99) 51.00</p>
    <p>J-&gt;C NO 29.99 (+2.78) 80.71 (+1.58) 16.00 YES 31.40 (+3.83) 82.70 (+3.87) 12.50</p>
    <p>C-&gt;J NO 36.30 (+2.73) 81.97 (+1.87) 16.75 YES 38.53 (+3.78) 84.07 (+3.81) 18.50</p>
    <p>The various improvements lead to good changes in BLEU. Almost +4 BLEU for the JC/CJ</p>
  </div>
  <div class="page">
    <p>Results for WAT2015</p>
    <p>Reranking BLEU RIBES HUMAN</p>
    <p>J-&gt;E NO 21.31 (+0.71) 70.65 (+0.53) 16.50 YES 22.89 (+1.82) 72.46 (+2.56) 32.50</p>
    <p>E-&gt;J NO 30.69 (+0.92) 76.78 (+1.57) 40.50 YES 33.06 (+1.97) 78.95 (+2.99) 51.00</p>
    <p>J-&gt;C NO 29.99 (+2.78) 80.71 (+1.58) 16.00 YES 31.40 (+3.83) 82.70 (+3.87) 12.50</p>
    <p>C-&gt;J NO 36.30 (+2.73) 81.97 (+1.87) 16.75 YES 38.53 (+3.78) 84.07 (+3.81) 18.50</p>
    <p>Mystery! Only for J-&gt;C, we find that reranking decreased</p>
    <p>Human EvaluaJon score. (While sJll improving BLEU/RIBES)</p>
  </div>
  <div class="page">
    <p>Results for WAT2015</p>
    <p>Reranking BLEU RIBES HUMAN</p>
    <p>J-&gt;E NO 21.31 (+0.71) 70.65 (+0.53) 16.50 YES 22.89 (+1.82) 72.46 (+2.56) 32.50</p>
    <p>E-&gt;J NO 30.69 (+0.92) 76.78 (+1.57) 40.50 YES 33.06 (+1.97) 78.95 (+2.99) 51.00</p>
    <p>J-&gt;C NO 29.99 (+2.78) 80.71 (+1.58) 16.00 YES 31.40 (+3.83) 82.70 (+3.87) 12.50</p>
    <p>C-&gt;J NO 36.30 (+2.73) 81.97 (+1.87) 16.75 YES 38.53 (+3.78) 84.07 (+3.81) 18.50</p>
  </div>
  <div class="page">
    <p>Code is available and Open-sourced</p>
    <p>Version 1.0 released  1 year arer version 0.1  2 years arer development started</p>
    <p>Downloadable at: hVp://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/  GPL Licence</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>KyotoEBMT is a (Dependency) Tree-to-Tree MT system with state-of- the-art results  Open-sourced (hVp://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/)  Improvements across the whole pipeline lead us to close to +4 BLEU improvements  Some future works:  Make more use of the target structure  Use of deep learning features in the decoder</p>
    <p>eg. as in (Devlin et al., 2014)</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
