<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Local Model Poisoning Attacks to Byzantine-Robust Federated Learning</p>
    <p>Minghong Fang*, Xiaoyu Cao* , Jinyuan Jia, and Neil Gong</p>
  </div>
  <div class="page">
    <p>Federated Learning: Collaborative Learning with Decentralized Data</p>
    <p>Aggregation</p>
    <p>Global model</p>
    <p>Local models</p>
  </div>
  <div class="page">
    <p>Real-world Applications</p>
  </div>
  <div class="page">
    <p>A single malicious worker can arbitrarily manipulate the global model.</p>
    <p>Federated Learning is Vulnerable to Attacks</p>
    <p>Weighted mean</p>
  </div>
  <div class="page">
    <p>Byzantine-robust Federated Learning</p>
    <p>Byzantine-robust aggregation rules as a defense.  Learn a good global model given bounded Byzantine workers.</p>
    <p>Main idea:  Filter out statistical outliers.</p>
    <p>Existing methods:  Krum, Trimmed-mean, coordinate-wise median, Bulyan,</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>Asymptotic bounds  Order-optimal bounds on parameters.  No guarantee on classification results.</p>
    <p>Strong assumptions  IID data  Strongly-convex</p>
  </div>
  <div class="page">
    <p>Our work</p>
    <p>Increase classification error rates of global models by sending manipulated local models.</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>Attackers goal:  High classification error rate.</p>
    <p>Attackers capability:  Controls some workers.  Sends arbitrary local models.</p>
    <p>Attackers knowledge:  Compromised workers: everything.  Server: know aggregation or not.  Benign workers: know model/data or not.</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Honest</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11 Aggregation</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Threat Model</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Full-knowledge Attack</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Partial-knowledge Attack</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Aggregation Rule is Known</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>Aggregation Rule is Unknown</p>
    <p>c m-c</p>
    <p>+2 1</p>
    <p>1  +1+2 1</p>
    <p>+11</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>2</p>
    <p>Our Idea</p>
    <p>1  No attack: global model changes</p>
    <p>along some direction.</p>
  </div>
  <div class="page">
    <p>Our Idea</p>
    <p>2</p>
    <p>1</p>
    <p>Deviate global model the most towards inverse of the direction.</p>
    <p>1</p>
  </div>
  <div class="page">
    <p>Formulate Optimization Problem</p>
    <p>max 1  ,,</p>
    <p>(  )</p>
    <p>Subject to  =  1,  , , +1,  ,</p>
    <p>= (1  ,  ,  , +1,  , )</p>
    <p>where  is a column vector of the changing directions of .</p>
    <p>Our formula is general to any aggregation rule .</p>
  </div>
  <div class="page">
    <p>Solving the Optimization Problem</p>
    <p>Full knowledge:  1,  , , +1,  ,  are known.  Directly solve the optimization problem.</p>
    <p>Partial knowledge:  Only 1,  ,  are known.  Estimate  with 1,  , .</p>
    <p>Aggregation unknown:  Take a guess on .</p>
  </div>
  <div class="page">
    <p>Experimental Settings</p>
    <p>100 workers  20 compromised by default</p>
    <p>Non-IID data distribution</p>
    <p>Datasets:  MNIST (default)  Fashion-MNIST  CH-MNIST  Breast Cancer Wisconsin (Diagnostic)</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
    <p>There is no attack</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
    <p>Adding gaussian random noise to local models</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
    <p>Flip labels of local training data</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
    <p>Our attack, partial knowledge</p>
  </div>
  <div class="page">
    <p>Our Attack is Effective</p>
    <p>NoAttack Gaussian LabelFlip Partial Full Krum 0.11 0.10 0.10 0.75 0.77 Trimmed Mean 0.06 0.07 0.07 0.14 0.23 Median 0.06 0.06 0.16 0.28 0.32</p>
    <p>Our attacks can effectively increase error rates</p>
    <p>Our attack, full knowledge</p>
  </div>
  <div class="page">
    <p>More Compromised, Better Attack</p>
    <p>Our attacks are more effective with more compromised workers</p>
  </div>
  <div class="page">
    <p>More Non-IID, Better Attack</p>
    <p>Our attacks are more effective when data are more Non-IID among workers</p>
  </div>
  <div class="page">
    <p>Our Attacks Transfer between Aggregation Rules</p>
    <p>Krum Trimmed mean Median No attack 0.14 0.12 0.13</p>
    <p>Krum attack 0.70 0.15 0.18 Trimmed mean attack 0.14 0.25 0.20</p>
  </div>
  <div class="page">
    <p>Comparing with Data Poisoning Attacks</p>
    <p>NoAttack DataPoisoning Partial Full Mean 0.10 0.11 0.54 0.69 Krum 0.23 0.24 0.85 0.89 Trimmed Mean 0.12 0.12 0.27 0.32 Median 0.13 0.13 0.19 0.21</p>
    <p>Our attacks are much more effective than data poisoning attacks</p>
  </div>
  <div class="page">
    <p>Possible Defenses</p>
    <p>Error Rate based Rejection (ERR):  Remove local models based on validation error rates.</p>
    <p>Loss Function based Rejection (LFR):  Remove local models based on validation loss.</p>
    <p>Union</p>
  </div>
  <div class="page">
    <p>Defense Results</p>
    <p>The defenses are effective in some cases while not in others</p>
    <p>We need more advanced defenses against our attacks</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We propose a general local model poisoning attack for any Byzantinerobust federated learning.</p>
    <p>Our attack can increase error rates of global models in Byzantinerobust federated learning.</p>
    <p>New defenses are needed to defend against our attacks.</p>
  </div>
  <div class="page">
    <p>For any questions, please email  xiaoyu.cao@duke.edu  myfang@iastate.edu</p>
  </div>
</Presentation>
