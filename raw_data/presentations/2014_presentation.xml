<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Communication Analysis of Parallel</p>
    <p>Large Blue Gene Systems</p>
    <p>A. Chan, P. Balaji, W. Gropp, R. Thakur</p>
    <p>Math. and Computer Science, Argonne National Lab</p>
    <p>University of Illinois, Urbana Champaign</p>
  </div>
  <div class="page">
    <p>Fast Fourier Transform</p>
    <p>One of the most popular and widely used numerical methods in scientific computing</p>
    <p>Forms a core building block for applications in many fields, e.g., molecular dynamics, many-body simulations, montecarlo simulations, partial differential equation solvers</p>
    <p>1D, 2D, 3D data grids FFTs are all used  Represents the dimensionality of the data being operated on</p>
    <p>2D process grids are popular  Represents the logical layout of the processes  E.g., Used by P3DFFT</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Parallel 3D FFT with P3DFFT  Relative new implementation of 3DFFT from SDSC  Designed for massively parallel systems</p>
    <p>Reduces synchronization overheads compared to other 3D FFT implementations</p>
    <p>Communicates along row and column in the 2D process grid  Internally utilizes sequential 1D FFT libraries and</p>
    <p>performance data grid transforms to collect the required data</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>P3DFFT for Flat Cartesian Meshes  Lot of prior work to improve 3D FFT performance  Mostly focuses on regular 3D cartesian meshes</p>
    <p>All sides of the mesh are of (almost) equal size</p>
    <p>Flat 3D cartesian meshes are becoming popular  Good tool for studying quasi-2D systems that occur during</p>
    <p>the transition of 3D systems to 2D systems  E.g., superconducting condensate, Quantum-Hall effect, and</p>
    <p>Turbulence theory in geophysical studies  Failure of P3DFFT for such systems is a known problem</p>
    <p>Objective: Understand the communication characteristics of P3DFFT, especially with respect to flat 3D meshes</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Communication overheads in P3DFFT</p>
    <p>Experimental Results and Analysis</p>
    <p>Concluding Remarks and Future Work</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>BG/L Network Overview</p>
    <p>BG/L has five different networks  Two of them (1G Ethernet and 100M Ethernet with JTAG</p>
    <p>interface) are used for file I/O and system management  3D Torus: Used for point-to-point MPI communication (as</p>
    <p>well as collectives for large message sizes)  Global Collective Network: Used for collectives using small</p>
    <p>messages and regular communication patterns  Global Interrupt Network: Used for barrier and other process</p>
    <p>synchronization routines</p>
    <p>For Alltoallv (in P3DFFT), the 3D Torus network is used  175MB/s bandwidth per link per direction (total 1.05 GB/s)</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Mapping 2D Process Grid to BG/L</p>
    <p>A 512 process system:  By default broken into a 32x16 logical process grid (provided</p>
    <p>by MPI_Dims_create)  Forms a 8x8x8 physical process grid on the BG/L</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Communication Characterization of P3DFFT</p>
    <p>Consider a process grid of P = Prow x Pcol and a data grid of N = nx x ny x nz</p>
    <p>P3DFFT performs a two-step process (forward transform and reverse transform)  The first step requires nz / Pcol Alltoallvs over the row sub</p>
    <p>communicator with message size mrow = N / (nz x Prow 2)</p>
    <p>The second step requires one Alltoallv over the column subcommunicator with message size mcol = N . Prow / P</p>
    <p>Total time =</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Trends in P3DFFT Performance</p>
    <p>Total communication time impacted by three variables:  Message size</p>
    <p>Too small message size implies network bandwidth is not fully utilized</p>
    <p>Too large message size is OK, but that implies the other communicators message size will be too small</p>
    <p>Communicator size  The lesser the better</p>
    <p>Communicator topology (and corresponding congestion)  This part increases quadratically with communicator size, so</p>
    <p>will have a large impact on large-scale systems</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Communication overheads in P3DFFT</p>
    <p>Experimental Results and Analysis</p>
    <p>Concluding Remarks and Future Work</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Alltoallv Bandwidth on Small Systems</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Alltoallv Bandwidth on Large Systems</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Communication Analysis on Small Systems</p>
    <p>Small Prow and small nz provide the best performance for small-scale systems  This is the exact opposite of MPIs default behavior !</p>
    <p>It tries to keep Prow and Pcol as close as possible; we need them to be as far away as possible</p>
    <p>Difference of up to 10%</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Evaluation on Large Systems (16 racks)</p>
    <p>Small Prow still performs the best</p>
    <p>Unlike small systems, large nz is better for large systems</p>
    <p>Increasing congestion plays an important role</p>
    <p>Difference as much as 48% Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction</p>
    <p>Communication overheads in P3DFFT</p>
    <p>Experimental Results and Analysis</p>
    <p>Concluding Remarks and Future Work</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Concluding Remarks and Future Work</p>
    <p>We analyzed the communication in P3DFFT on BG/L and identified the parameters that impact performance  Evaluated the impact of the different parameters and</p>
    <p>identified trends in performance  Found that while uniform process grid topologies are ideal</p>
    <p>for uniform 3D data grids, for flat cartesian grids, nonuniform process grid topologies are ideal</p>
    <p>Shown up to 48% improvement in performance by utilizing our understanding to tweak parameters</p>
    <p>Future Work: Intend to do this on Blue Gene/P (performance counters make this a lot more interesting)</p>
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>(HiPC: 12/19/2008)</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Contacts:</p>
    <p>Emails: {chan, balaji, thakur}@mcs.anl.gov</p>
    <p>wgropp@illinois.edu</p>
    <p>Web Link:</p>
    <p>http://www.mcs.anl.gov/research/projects/mpich2</p>
  </div>
</Presentation>
