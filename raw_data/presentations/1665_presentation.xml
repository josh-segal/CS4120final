<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Two-level Throughput and Latency</p>
    <p>I/O Control for Parallel File Systems</p>
    <p>Yiqi Xu, Ming Zhao Florida International University</p>
    <p>School of Computing and Information Sciences</p>
    <p>http://visa.cis.fiu.edu</p>
    <p>yxu006@cis.fiu.edu, ming@cis.fiu.edu</p>
  </div>
  <div class="page">
    <p>Background  Parallel Storages</p>
    <p>Parallel File System in High Performance Computing</p>
    <p>Distribute data on multiple storage nodes</p>
    <p>Aggregate throughput from multiple, parallel storage nodes</p>
    <p>Components</p>
    <p>Server side: data &amp; meta-data server daemon</p>
    <p>Client side: MPI library, client daemon</p>
    <p>Compute</p>
    <p>nodes</p>
    <p>APP</p>
    <p>Storage</p>
    <p>nodes</p>
    <p>P a ra</p>
    <p>ll e l</p>
    <p>F il e</p>
    <p>S y st</p>
    <p>e m</p>
    <p>Meta-data nodes</p>
    <p>Data nodes</p>
    <p>PFS</p>
  </div>
  <div class="page">
    <p>Motivation (1)</p>
    <p>Parallel storage is commonly shared</p>
    <p>Applications have different I/O demands</p>
    <p>Their I/Os interfere with each other</p>
    <p>Compute</p>
    <p>nodes APP1</p>
    <p>APP2</p>
    <p>APPn</p>
    <p>Storage</p>
    <p>nodes</p>
    <p>WRF[10]</p>
    <p>S3D[12]</p>
    <p>mpiBlast[11]</p>
    <p>Frequent update of</p>
    <p>Gigabytes of output</p>
    <p>Periodic check-pointing</p>
    <p>Gigabytes of data</p>
    <p>Load Gigabytes of</p>
    <p>data before execution</p>
  </div>
  <div class="page">
    <p>Background  vPFS</p>
    <p>Enhanced distributed SFQ scheduler</p>
    <p>Global bandwidth proportional sharing with low overhead</p>
    <p>App</p>
    <p>App</p>
    <p>App</p>
    <p>PFS Proxy</p>
    <p>Virtual PFS1</p>
    <p>Virtual PFS2</p>
    <p>HPC application 1</p>
    <p>HPC application 2</p>
    <p>Fixed Queue Depth</p>
  </div>
  <div class="page">
    <p>Motivation (2)</p>
    <p>Two representative parallel application: BTIO[9]/IOR[8]</p>
    <p>Limited performance improvement from vPFS[5]</p>
    <p>Throughput alone is not enough to satisfy applications</p>
    <p>performance needs</p>
    <p>Large BTIO (1MB/IO) Small BTIO (320B/IO)</p>
    <p>B T</p>
    <p>IO T</p>
    <p>h r o u g h p u t</p>
    <p>(M B /s</p>
    <p>) Native - Standalone</p>
    <p>Native - with IOR</p>
    <p>vPFS - DSFQ</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Problem</p>
    <p>HPC applications requiring throughput or latency (or both)</p>
    <p>guarantees interfere with each other on the parallel storage</p>
    <p>vPFS enforcement on bandwidth sharing is NOT enough to</p>
    <p>satisfy different applications needs</p>
    <p>Solution</p>
    <p>Use vPFS to create a new scheduler to recognize and regulate</p>
    <p>I/Os with awareness of both throughput and latency needs</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background, Motivation &amp; Overview</p>
    <p>Two-Level Parallel I/O Scheduler</p>
    <p>Architecture</p>
    <p>Algorithm</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Two-Level QoS</p>
    <p>(T, D): A tuple for both Throughput and Latency</p>
    <p>T is the agreed throughput upper bound limit from the</p>
    <p>application</p>
    <p>D is the guaranteed the latency (deadline) upper bound</p>
    <p>from the storage</p>
    <p>When T is violated, D is not guaranteed any more</p>
  </div>
  <div class="page">
    <p>Architecture</p>
    <p>High level provides throughput control as well as</p>
    <p>service synchronization</p>
    <p>Low level monitors the device and adjusts #</p>
    <p>outstanding requests of the device[13]</p>
    <p>C1 C2 C3  Cm</p>
    <p>High</p>
    <p>Level</p>
    <p>Low</p>
    <p>Level EDF</p>
    <p>Monitor Controller Dispatcher</p>
    <p>vPFS-DSFQ Parallel Storage</p>
    <p>Nodes</p>
    <p>Depth</p>
  </div>
  <div class="page">
    <p>High Level Throughput Control</p>
    <p>Efficient parallel storage synchronization: total</p>
    <p>service proportional sharing of bandwidth</p>
    <p>Strict fair sharing using SFQ-based algorithm: better</p>
    <p>utilization</p>
    <p>C1 C2 C3  Cm</p>
    <p>D*w1 D*w2 D*W3 D*Wm</p>
    <p>Low Level</p>
    <p>vPFS-DSFQ Parallel Storage</p>
    <p>Nodes</p>
    <p>vPFS-DSFQ</p>
    <p>vPFS-DSFQ</p>
    <p>D* (1-w2-w3) credits used D credits in each time window</p>
  </div>
  <div class="page">
    <p>High Level Throughput Control</p>
    <p>Total-service proportional sharing: parallel storage</p>
    <p>synchronization</p>
    <p>Strict fair sharing of using SFQ-based algorithm:</p>
    <p>better utilization</p>
    <p>C1 C2 C3  Cm</p>
    <p>r1 r4</p>
    <p>Low Level</p>
    <p>vPFS-DSFQ Parallel Storage</p>
    <p>Nodes</p>
    <p>vPFS-DSFQ</p>
    <p>vPFS-DSFQ</p>
    <p>Credits claimed only when requests arrive: r1+r4=D</p>
  </div>
  <div class="page">
    <p>Low Level Latency Control</p>
    <p>Final dispatching of requests to storage device</p>
    <p>A feedback-control loop for adjusting the device depth</p>
    <p>C1 C2 C3  Cm High Level</p>
    <p>Low Level</p>
    <p>Monitor Controller Dispatcher</p>
    <p>EDF</p>
    <p>LO</p>
    <p>Mean EDF Wait Time</p>
  </div>
  <div class="page">
    <p>Low Level Bounds and Terms</p>
    <p>Three bounds to predict the future</p>
    <p>For class i, the maximum depth LRT O allowed without violating</p>
    <p>the deadline</p>
    <p>The lower bound depth Ll O to ensure any request whose deadline</p>
    <p>is in the current time window is completed</p>
    <p>The upper bound depth Lu O if the latency need is continuously</p>
    <p>met and utilization should be raised</p>
    <p>Terms</p>
    <p>X  # requests completed in last time window</p>
    <p>LO  current window queue depth</p>
    <p>LOmax  maximum # outstanding requests in current window</p>
  </div>
  <div class="page">
    <p>Low Level Feedbacks</p>
    <p>LO scaled by 3 coefficients to derive 3 threshold bounds</p>
    <p>:</p>
    <p>The more time left to complete a request, the larger</p>
    <p>The smaller actual device latency, the larger</p>
    <p>or :</p>
    <p>(about throughput)</p>
    <p>Low Level at the end of current time window</p>
    <p>Monitor Controller Dispatcher</p>
    <p>EDF Waiting Time MTi e</p>
    <p>Time Ti o, X, Lmax</p>
    <p>O</p>
    <p>=</p>
    <p>=</p>
    <p>, e.g.</p>
    <p>,</p>
    <p>=</p>
    <p>, e.g.</p>
    <p>From (, )</p>
  </div>
  <div class="page">
    <p>Controlling LO :Underloaded Case</p>
    <p>=</p>
    <p>;   =   ;</p>
    <p>=</p>
    <p>&lt;</p>
    <p>&lt;</p>
    <p>or</p>
  </div>
  <div class="page">
    <p>Controlling LO :Overloaded Case</p>
    <p>Over all classes, a minimum of all selected queue</p>
    <p>threshold is chosen</p>
    <p>&lt;</p>
    <p>(,  )</p>
    <p>=</p>
    <p>ver all classes, a minimum of all</p>
    <p>selected queue threshold is chosen</p>
  </div>
  <div class="page">
    <p>Clients</p>
    <p>Low level idleness detection</p>
    <p>If      0.9</p>
    <p>Idleness updated on the lower level:</p>
    <p>When a request is dispatched or completed</p>
    <p>At the beginning of an overloaded time window</p>
    <p>High level credit replenishment</p>
    <p>When the lower level reports idleness</p>
    <p>When no remaining credits</p>
    <p>But new requests query and find the idleness</p>
    <p>When credit replenishment time window elapsed</p>
    <p>Cooperation between Two Levels</p>
    <p>credits</p>
    <p>idleness</p>
    <p>Monitor</p>
    <p>vPFS-DSFQ Clients</p>
    <p>Clients</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Hardware</p>
    <p>1 Client with 64 processes</p>
    <p>1 Server</p>
    <p>One gigabit switch</p>
    <p>Software</p>
    <p>PVFS 2.8.2</p>
    <p>IOR 2.10.3</p>
    <p>Experiments</p>
    <p>Adaptation of storage queue size</p>
    <p>Handling of overloaded storage</p>
  </div>
  <div class="page">
    <p>IORs Issue Rates</p>
    <p>One on-off pattern, with one constantly increasing</p>
    <p>Storage capacity is about 50MB/s</p>
    <p>App1 QoS: (40MB/s, 100ms); App2 QoS: (20MB/s, 300ms)</p>
  </div>
  <div class="page">
    <p>Adaptation of Queue Length</p>
    <p>Accurate transition between over- and under-load</p>
    <p>Good depth obtained for adequate throughput</p>
  </div>
  <div class="page">
    <p>Latency Differentiation</p>
    <p>Storage is overloaded when both Apps are on</p>
    <p>App1 conforms to 100ms 10 times than App2</p>
    <p>App1s overall 95th percentile latency is smaller than App2</p>
    <p>th P</p>
    <p>e rc</p>
    <p>e n ti le</p>
    <p>R e s p o n s e T</p>
    <p>im e (</p>
    <p>m s )</p>
    <p>App2 (20MB/s, 300ms)</p>
    <p>App1(40MB/s, 100ms)</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Future Work</p>
    <p>Two-level I/O control for parallel storage</p>
    <p>Two-level scheduler can effectively respect the latency</p>
    <p>needs of different applications</p>
    <p>Latency can be managed using a feedback-control loop for</p>
    <p>a black box storage device</p>
    <p>Future work</p>
    <p>Manage I/Os of different sizes</p>
    <p>Create distributed versions of EDF</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>[1] PVFS2. http://www.pvfs.org/pvfs2/.</p>
    <p>[2] PanFS. http://www.panasas.com .</p>
    <p>[3] GPFS. http://www.ibm.com/systems/software/gpfs .</p>
    <p>[4] Lustre. http://www.lustre.org .</p>
    <p>[5] Y. Xu, D. Arteaga, M. Zhao, Y. Liu, R. Figueiredo, S. Seelam, vPFS: Bandwidth Virtualization of</p>
    <p>Parallel Storage Systems, IEEE 28th Symposium on Mass Storage Systems and Technologies, 2012</p>
    <p>[6] Yin Wang and Arif Merchant, Proportional-share scheduling for distributed storage systems,</p>
    <p>In Proceedings of the 5th USENIX conference on File and Storage Technologies (FAST07). USENIX</p>
    <p>Association, Berkeley, CA, USA, 4-4.</p>
    <p>[7] W. Jin, J. S. Chase, and J. Kaur, Interposed Proportional Sharing For A Storage Service Utility,</p>
    <p>SIGMETRICS, 2004.</p>
    <p>[8] IOR HPC Benchmark, http://sourceforge.net/projects/ior-sio/.</p>
    <p>[9] NASA Parallel Benchmark, http://www.nas.nasa.gov/publications/npb.html .</p>
    <p>[10] P. Welsh, P. Bogenschutz, Weather Research and Forecast (WRF) Model: Precipitation</p>
    <p>Prognostics from the WRF Model during Recent Tropical Cyclones, Interdepartmental Hurricane</p>
    <p>Conference, 2005.</p>
    <p>[11] A. Darling, L. Carey, and W. Feng, The Design, Implementation, and Evaluation of mpiBLAST,</p>
    <p>ClusterWorld Conf. and Expo, 2003.</p>
    <p>[12] R. Sankaran, et al., Direct Numerical Simulations of Turbulent Lean Premixed Combustion,</p>
    <p>Journal of Physics Conference Series, 2006.</p>
    <p>[13] J. Zhang, A. Sivasubramaniam, Q. Wang, A. Riska, and E. Riedel, Storage performance</p>
    <p>virtualization via throughput and latency control. Trans. Storage 2 (August 2006), 283308.</p>
  </div>
  <div class="page">
    <p>Acknowledgement</p>
    <p>Sponsor: National Science Foundation</p>
    <p>VISA lab: http://visa.cs.fiu.edu</p>
    <p>More information: http://visa.cis.fiu.edu/hecura</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
</Presentation>
