<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Void: A fast and light voice liveness detection system</p>
    <p>Muhammad Ejaz Ahmed, Il-youp Kwak, Jun Ho Huh, Iljoo Kim, Taekyung Oh, and</p>
    <p>Hyoungshick Kim</p>
    <p>CSIRO Data61, Australia</p>
    <p>Sungkyunkwan University, Korea</p>
    <p>Samsung Research, Korea</p>
    <p>Chung-Ang University, Korea</p>
    <p>KAIST, Korea</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Targets</p>
    <p>speech model Input text Synthesized speech</p>
    <p>Voice replay attack</p>
    <p>Voice synthesis attack</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Voice liveness detection</p>
    <p>HumanSpoof</p>
    <p>Voice liveness detection system</p>
  </div>
  <div class="page">
    <p>Requirements</p>
    <p>Latency and model size  Processing delay must be less than 100 milliseconds.</p>
    <p>A single GPU may be expected to concurrently process 100 or more voice sessions.</p>
    <p>On-device implementation without need to communicate with remote servers.</p>
    <p>Detection accuracy  Around 10% or below EER to be considered as a usable solution.</p>
  </div>
  <div class="page">
    <p>Our contributions</p>
    <p>Void: a fast, lightweight and easily implementable in commercial voice assistants.  Provide key insights for attack detection.</p>
    <p>Single classification model with just 97 features.</p>
    <p>Void is robust under numerous environmental settings.</p>
    <p>Evaluation using two large datasets consisting of:  255,173 voice samples collected from 120 participants. EER achieved is 0.3%.</p>
    <p>18,030 ASVspoof competition voice samples collected from 42 participants. EER achieved 11.6% (second best-performing approach).</p>
    <p>Void is about 8 times faster and uses 153 times less memory in detection compared to best-performing.</p>
    <p>Resilient against adversarial attacks. We evaluated it on:  Hidden voice attack: Accuracy 99.7%</p>
    <p>Inaudible voice command (Dolphin attack): accuracy 100%</p>
    <p>Voice synthesis attacks: accuracy 90.2%</p>
    <p>Equalization manipulation attacks: accuracy 86.3%</p>
  </div>
  <div class="page">
    <p>Key insights</p>
  </div>
  <div class="page">
    <p>Key insight 1: Decay patterns in spectral power</p>
    <p>Fig. 1. Live-human voice sample Fig. 2. Replayed using smartphone loudspeaker</p>
  </div>
  <div class="page">
    <p>Key insight 2 : Peak patterns in spectral power</p>
  </div>
  <div class="page">
    <p>High level overview of Void</p>
  </div>
  <div class="page">
    <p>Data collection</p>
  </div>
  <div class="page">
    <p>Dataset contain three sets (training, development, and evaluation).</p>
    <p>Voice sample were collected from numerous environments such as balcony,</p>
    <p>bedroom, canteen, home, office, and lab space.</p>
    <p>120 participants recruited for data collection. 53% of the participants were male.</p>
    <p>50 commands from a prepared list of real-world voice assistant commands.</p>
    <p>Participants were in the 40-49 (13%), 30-39 (62%), and 20-29 (25%) age groups.</p>
    <p>Our dataset</p>
    <p>ASVspoof 2017 competition dataset</p>
  </div>
  <div class="page">
    <p>Datasets</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Overall performance</p>
  </div>
  <div class="page">
    <p>Lightweight nature of Void</p>
  </div>
  <div class="page">
    <p>Void</p>
    <p>ASVspoof 2017 competition results [https://www.asvspoof.org/ slides_ASVspoof2017_Interspeech.pdf].</p>
  </div>
  <div class="page">
    <p>Void as an ensemble solution</p>
    <p>+VoidEER = 11.6% MFCC-GMMEER = 25.5%</p>
    <p>Void + MFCC-GMM EER = 8.7%</p>
  </div>
  <div class="page">
    <p>Adversarial attacks against Void</p>
  </div>
  <div class="page">
    <p>Voids resilience against adversarial attacks</p>
    <p>Hidden voice command: Hidden voice commands refer to commands that can not be interpreted by</p>
    <p>human ears but can be interpreted and processed by voice assistant services.</p>
    <p>Inaudible voice command (Dolphin attack): Inaudible voice command attacks involve playing an</p>
    <p>ultrasound signal with spectrum above 20kHz, which would be inaudible to human ears.</p>
    <p>Voice synthesis attack: Open source voice modeling tools called Tacotron and Deepvoice 2 to train</p>
    <p>a user voice model with 13,100 publicly available voice samples.</p>
    <p>We then used the trained model to generate 1,300 synthesis voice attack samples by feeding in</p>
    <p>commands as text inputs.</p>
    <p>EQ manipulation attacks: EQ manipulation is a process commonly used for altering the frequency response of an</p>
    <p>audio system by leveraging linear filters.</p>
    <p>By leveraging audio equalization, an attacker could intentionally manipulate the power of certain frequencies to mimic spectrum patterns observed in live-human voices.</p>
  </div>
  <div class="page">
    <p>Voids resilience against adversarial attacks</p>
  </div>
  <div class="page">
    <p>Limitations of Void</p>
    <p>Void performance against high-quality speakers may degrade.</p>
    <p>EQ attack results show that carefully crafted voice samples can bypass Void. However, such attack would require strong signal processing expertise.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Lightweight:  Void runs on single efficient classification model with 97 features and does not</p>
    <p>require addiction hardware.</p>
    <p>Void is 8 times faster and 153 times lighter than top performing solution of ASVspoof competition.</p>
    <p>On average Void took 35 milliseconds to classify a voice sample and just 1.98MB memory.</p>
    <p>On-device implementation possible.</p>
    <p>Efficient:  Our evaluation on two large datasets, Void achieves 0.3% and 11.6% EER,</p>
    <p>respectively.</p>
    <p>Void is also resilient against various adversarial attacks.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
</Presentation>
