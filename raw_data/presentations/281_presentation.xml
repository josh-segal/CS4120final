<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Dialog Management for Rapid-Prototyping of</p>
    <p>Speech-Based Training AgentsVictor Hung, Avelino Gonzalez,</p>
    <p>Ronald DeMara University of Central Florida</p>
  </div>
  <div class="page">
    <p>Introduction Approach Evaluation Results Conclusions</p>
    <p>Agenda</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>General Problem  Elevate the level of speech-based discourse to a</p>
    <p>new level of naturalness in Embodied Conversation Agents (ECA) carrying an open-domain dialog</p>
    <p>Specific Problem  Overcome Automatic Speech Recognition (ASR)</p>
    <p>limitations  Domain-independent knowledge management</p>
    <p>Training Agent Design  Conversational input with robustness to ASR and</p>
    <p>adaptable knowledge base</p>
  </div>
  <div class="page">
    <p>Approach</p>
    <p>Build a dialog manager that:  Handles ASR limitations  Manages domain-independent knowledge  Provides open dialog</p>
    <p>CONtext-driven Corpus-based Utterance Robustness (CONCUR)  Input Processor  Knowledge Manager  Discourse ModelI/O</p>
    <p>Dialog Manager</p>
    <p>User Input</p>
    <p>Agent Response</p>
    <p>Input Processor Discourse</p>
    <p>Model</p>
    <p>Knowledge Manager</p>
  </div>
  <div class="page">
    <p>CONCUR</p>
    <p>Input Processor  Pre-process knowledge corpus via</p>
    <p>keyphrasing  Break down user utterance</p>
    <p>Input Processor</p>
    <p>Corpus</p>
    <p>Data</p>
    <p>Keyphrase Extractor WordNet</p>
    <p>NLP Toolkit</p>
    <p>User Utteranc e</p>
    <p>Knowledge Manager  3 data bases  Encyclopedia-entry</p>
    <p>style corpus  Context-driven</p>
  </div>
  <div class="page">
    <p>CONCUR</p>
    <p>CxBR Discourse Model  Goal Bookkeeper</p>
    <p>Goal Stack (Branting et al, 2004)</p>
    <p>Inference Engine</p>
    <p>Context Topology  Agent Goals  User Goals</p>
  </div>
  <div class="page">
    <p>Detailed CONCUR Block Diagram</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Plagued by subjectivity  Gathering of both objective and subjective metrics</p>
    <p>Qualitative and quantitative metrics:  Efficiency metrics</p>
    <p>Total elapsed time  Number of user turns  Number of system turns  Total elapsed time per turn  Word-Error Rate (WER)</p>
    <p>Quality metrics  Out-of-corpus</p>
    <p>misunderstandings  General misunderstandings  Errors  Total number of user goals  Total number of user goals</p>
    <p>fulfilled  Goal completion accuracy  Conversational accuracy</p>
    <p>Survey data  Naturalness  Usefulness</p>
  </div>
  <div class="page">
    <p>Evaluation Instrument</p>
    <p>Nine statements, judged on a 1-to-7 scale based on level of agreement</p>
    <p>Naturalness  If I told someone the character in this tool was real</p>
    <p>they would believe me.  The character on the screen seemed smart.  I felt like I was having a conversation with a real</p>
    <p>person.  This did not feel like a real interaction with another</p>
    <p>person.  Usefulness</p>
    <p>I would be more productive if I had this system in my place of work.</p>
    <p>The tool provided me with the information I was looking for.</p>
    <p>I found this to be a useful way to get information.  This tool made it harder to get information than talking</p>
    <p>to a person or using a website.  This does not seem like a reliable way to retrieve</p>
    <p>information from a database.</p>
  </div>
  <div class="page">
    <p>Data Acquisition</p>
    <p>General data set acquisition procedure:  User asked to interact with agent</p>
    <p>Natural, information-seeking  Voice recording</p>
    <p>User asked to complete survey  Data analysis process:</p>
    <p>Voice transcriptions, ASR transcripts, internal data, and surveys analyzed</p>
    <p>Data Set Dialog Manager Agent Style</p>
    <p>Domain</p>
    <p>Surveys/ Transcript</p>
    <p>s Collected</p>
    <p>NSF I/UCRC 30/30</p>
    <p>NSF I/UCRC 30/20</p>
  </div>
  <div class="page">
    <p>Data Acquisition</p>
    <p>LifeLike Avatar</p>
    <p>Speech Recognizer</p>
    <p>CONCUR Dialog Manager</p>
    <p>Agent Externals</p>
    <p>ASR String</p>
    <p>Response String</p>
    <p>MicUser Voice</p>
    <p>Speaker</p>
    <p>Monitor</p>
    <p>Agent Voice</p>
    <p>Agent Image</p>
    <p>Monitor</p>
    <p>CONCUR Chatbot</p>
    <p>CONCUR Dialog Manager</p>
    <p>Jabber-based Agent</p>
    <p>Agent Text Output</p>
    <p>Keyboar d</p>
    <p>User Text Input</p>
    <p>ECA</p>
    <p>Chatbot</p>
  </div>
  <div class="page">
    <p>Survey Baseline</p>
    <p>Agent Naturalness User</p>
    <p>Rating Usefulness User</p>
    <p>Rating</p>
    <p>Data Set 1: AlexDSS Avatar</p>
    <p>Data Set 2: CONCUR Avatar</p>
    <p>Amani (Gandhe et al, 2009)</p>
    <p>Hassan (Gandhe et al, 2009)</p>
    <p>1. Both LifeLike Avatars established user assessments that exceeded other ECA efforts</p>
    <p>2. Both avatar-based systems in the speech-based data sets established similar scores in Naturalness and Usefulness</p>
    <p>Question 1: What are the expectations of naturalness and usefulness for the conversation agents in this study?</p>
    <p>Question 2: How differently did users rate the AlexDSS Avatar with the CONCUR Avatar?</p>
  </div>
  <div class="page">
    <p>Survey Baseline</p>
    <p>3. ECA-based systems were judged similarly, both better than chatbot</p>
    <p>Question 3: How differently did users rate the ECA systems with the chatbot system?</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Data Set 1: AlexDSS Avatar</p>
    <p>Data Set 2: CONCUR</p>
    <p>Avatar Efficiency</p>
    <p>Metrics WER 60.85% 58.48%</p>
    <p>Quantitative Analysis</p>
    <p>Out-of-Corpus Misunderstanding Rate</p>
    <p>Goal Completion Accuracy 63.29% 60.48%</p>
    <p>Question 1: Can a speech-based CONCUR Avatars goal completion accuracy measure up to the AlexDSS Avatar under a high WER?</p>
    <p>1. A Speech-based CONCUR Avatars goal completion accuracy measures up to AlexDSS avatar with similarly high WER</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Data Set 2: CONCUR</p>
    <p>Avatar</p>
    <p>Data Set 3: CONCUR Chatbot</p>
    <p>Efficiency Metrics</p>
    <p>WER 58.48% 0.00%</p>
    <p>Quantitative Analysis</p>
    <p>Out-of-Corpus Misunderstanding</p>
    <p>Rate 6.37% 6.77%</p>
    <p>Goal Completion Accuracy 60.48% 68.48%</p>
    <p>Question 2: How does improving WER affect CONCURs goal completion accuracy?</p>
    <p>2. Improved WER does not increase CONCURs goal completion accuracy because no new user goals were identified or corrected with the better recognition</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Agent Average</p>
    <p>WER</p>
    <p>Goal Completion Accuracy</p>
    <p>Data Set 2: CONCUR Avatar</p>
    <p>Digital Kyoto (Misu and Kawahara, 2007)</p>
    <p>Question 3: Can CONCURs goal completion accuracy measure up to other conversation agents in lieu of high WER?</p>
    <p>3: CONCURs goal completion accuracy is similar to that of the Digital Kyoto system, with twice the WER.</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Data Set 1: AlexDSS Avatar</p>
    <p>Data Set 2: CONCUR</p>
    <p>Avatar Efficiency</p>
    <p>Metrics WER 60.85% 58.48%</p>
    <p>Quantitative Analysis</p>
    <p>General Misunderstanding Rate</p>
    <p>Error Rate 8.71% 21.81%</p>
    <p>Conversational Accuracy 81.78% 64.22%</p>
    <p>Question 4: Can a speech-based CONCUR Avatars conversational accuracy measure up to the AlexDSS avatar under a high WER?</p>
    <p>4. Speech-based CONCURs conversational accuracy does not measure up to an AlexDSS Avatar with similarly high WER. This can be attributed to general misunderstandings and errors caused by misheard user requests or specific question answering requests not common with menu-driven discourse models</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Data Set 2: CONCUR</p>
    <p>Avatar</p>
    <p>Data Set 3: CONCUR Chatbot</p>
    <p>Efficiency Metrics</p>
    <p>WER 58.48% 0.00%</p>
    <p>Quantitative Analysis</p>
    <p>General Misunderstanding</p>
    <p>Rate 14.12% 7.48%</p>
    <p>Error Rate 21.81% 16.68%</p>
    <p>Goal Completion Accuracy 60.48% 68.48%</p>
    <p>Conversational Accuracy 64.22% 75.31%</p>
    <p>Question 5: How does improving WER affect CONCURs conversational accuracy?</p>
    <p>5. Improved WER increases CONCURs conversational accuracy by decreasing general misunderstandings</p>
  </div>
  <div class="page">
    <p>ASR Resilience</p>
    <p>Agent Average</p>
    <p>WER Conversation al Accuracy</p>
    <p>Data Set 2: CONCUR Avatar</p>
    <p>TARA (Schumaker et al, 2007)</p>
    <p>Question 6: Can CONCURs conversational accuracy measure up to other conversation agents in lieu of high WER?</p>
    <p>6: CONCURs conversational accuracy surpasses that of the TARA system, which is text-based.</p>
  </div>
  <div class="page">
    <p>Domain-Independence</p>
    <p>Data Set 2: NSF I/UCRC Avatar</p>
    <p>Data Set 3: NSF I/UCRC</p>
    <p>Chatbot</p>
    <p>Data Set 4: Current Events</p>
    <p>Chatbot</p>
    <p>Quantitative Analysis</p>
    <p>Out-Of-Corpus Misunderstanding</p>
    <p>Rate 6.15% 6.77% 17.45%</p>
    <p>Goal Completion Accuracy</p>
    <p>Question 1: Can CONCUR maintain goal completion accuracy after changing to a less specific domain corpus?</p>
    <p>1. CONCURs goal completion accuracy does not remain consistent after a change to a generalized domain corpus. Changing domain expertise may increase out-of-corpus requests, which decreases goal completion</p>
  </div>
  <div class="page">
    <p>Domain-Independence</p>
    <p>Data Set 2: NSF I/UCRC Avatar</p>
    <p>Data Set 3: NSF I/UCRC</p>
    <p>Chatbot</p>
    <p>Data Set 4: Current Events</p>
    <p>Chatbot</p>
    <p>Quantitative Analysis</p>
    <p>General Misunderstanding</p>
    <p>Rate 14.49% 7.48% 0.00%</p>
    <p>Error Rate 21.81% 16.68% 16.46% Conversational</p>
    <p>Accuracy 64.22% 75.34% 83.54%</p>
    <p>Question 2: Can CONCUR maintain conversational accuracy after changing to a less specific domain corpus?</p>
    <p>2. After changing to a general domain corpus, CONCUR is capable of maintaining its conversational accuracy</p>
  </div>
  <div class="page">
    <p>Domain-Independence</p>
    <p>Dialog System Method Turnover</p>
    <p>Time CONCUR Corpus-based 3 Days</p>
    <p>Marve (Babu et al, 2006)</p>
    <p>Wizard-of-Oz 18 Days</p>
    <p>Amani (Gandhe et al, 2009)</p>
    <p>Question-Answer Pairs Weeks</p>
    <p>AlexDSS Expert System Weeks Sergeant Blackwell (Robinson et al, 2008)</p>
    <p>Wizard-of-Oz 7 Months</p>
    <p>Sergeant Star (Artstein et al, 2009)</p>
    <p>Question-Answer Pairs 1 Year</p>
    <p>HMIHY (Bchet et al, 2004)</p>
    <p>Hand-modeled 2 Years</p>
    <p>Hassan (Gandhe et al, 2009)</p>
    <p>Question-Answer Pairs Years</p>
    <p>3. CONCURs Knowledge Manager enables a shortened knowledge development turnover time as compared to other conversation agent knowledge management systems</p>
    <p>Question 3: Can CONCUR provide a quick method of providing agent knowledge?</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Building Training Agents  Agent Design</p>
    <p>ECA preference over Chatbot format  ASR</p>
    <p>ASR improvements leads to better conversationlevel processing</p>
    <p>High ASR not necessarily an obstacle for ECA design  Knowledge Management</p>
    <p>Tailoring domain expertise for an intended audience is more effective than a generalized corpus</p>
    <p>Separation of domain knowledge from agent discourse helps to maintain conversational accuracy and speed up agent development times</p>
  </div>
</Presentation>
