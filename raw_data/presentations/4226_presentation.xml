<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Ozan Caglayan1, Pranava Madhyastha2, Lucia Specia2, Loc Barrault1</p>
    <p>Probing the Need for Visual Context in Multimodal Machine Translation</p>
  </div>
  <div class="page">
    <p>Multimodal Machine Translation (MMT)</p>
    <p>Better machine translation approaches by leveraging multiple modalities  Dataset  Multi30K (Elliott et al., 2016)</p>
    <p>Multilingual extension of Flickr30K (Young et al., 2014)  Images, English descriptions, French, German and Czech translations.</p>
    <p>Language grounding  Sense disambiguation  river bank vs. financial bank  Grammatical gender disambiguation  Learning concepts</p>
    <p>Potential benefit</p>
  </div>
  <div class="page">
    <p>Example: grammatical gender</p>
    <p>A baseball player in a black shirt just tagged</p>
    <p>a player in a white shirt. Un joueur de baseball en</p>
    <p>maillot noir vient de toucher un joueur en maillot blanc.</p>
    <p>Une joueuse de baseball en maillot noir vient de toucher une joueuse en maillot blanc.</p>
    <p>Source Sentence (EN)</p>
    <p>Candidate Translations (FR)</p>
    <p>Female baseball player</p>
    <p>Male baseball player</p>
  </div>
  <div class="page">
    <p>Example: grammatical gender</p>
    <p>A baseball player in a black shirt just tagged</p>
    <p>a player in a white shirt. Un joueur de baseball en</p>
    <p>maillot noir vient de toucher un joueur en maillot blanc.</p>
    <p>Une joueuse de baseball en maillot noir vient de toucher une joueuse en maillot blanc.</p>
    <p>Source Sentence (EN)</p>
    <p>Candidate Translations (FR)</p>
    <p>Female baseball player</p>
    <p>Male baseball player</p>
    <p>Visual context disambiguates the gender</p>
  </div>
  <div class="page">
    <p>Where are we?</p>
    <p>Benefit of current approaches is not evident - WMT18 (Barrault et al., 2018):  Largest gain from external corpora, not from images (Grnroos et al., 2018)</p>
  </div>
  <div class="page">
    <p>Where are we?</p>
    <p>Benefit of current approaches is not evident:  Adversarially attacking MMT</p>
    <p>marginally influences the scores (Elliott 2018)</p>
    <p>METEOR (EN-DE) Congruent Incongruent</p>
    <p>Dec-init 57.0 56.8</p>
    <p>Trg-mul 57.3 57.3</p>
    <p>Fusion-conv 55.0 53.3</p>
  </div>
  <div class="page">
    <p>Why dont images help?</p>
    <p>Pre-trained CNN features may not be good enough for MMT  ImageNet has very limited set of objects</p>
    <p>Current multimodal models may not be effective   Multi30K dataset may be</p>
    <p>Too simple; language is enough  Too small to generalise visual features</p>
  </div>
  <div class="page">
    <p>Why dont images help?</p>
    <p>Pre-trained CNN features may not be good enough for MMT  ImageNet has very limited set of objects</p>
    <p>Current multimodal models may not be effective   Multi30K dataset may be</p>
    <p>Too simple; language is enough  Too small to generalise visual features</p>
  </div>
  <div class="page">
    <p>This paper</p>
    <p>We degrade source language  Systematically mask source words at training and inference times</p>
    <p>Hypothesis 1: MMT models should perform better than text-only models if image is effectively taken into account  Image features  Multimodal models</p>
    <p>Hypothesis 2: More sophisticated MMT models should perform better</p>
    <p>than simpler MMT models</p>
  </div>
  <div class="page">
    <p>Types of degradation</p>
    <p>Source sentence a lady in a blue dress singing</p>
  </div>
  <div class="page">
    <p>Types of degradation (1)</p>
    <p>Color Masking a lady in a [v] dress singing</p>
    <p>Source sentence a lady in a blue dress singing</p>
    <p>Very small-scale masking  3.3% of source words are removed</p>
  </div>
  <div class="page">
    <p>Types of degradation (2)</p>
    <p>Color Masking a lady in a [v] dress singing</p>
    <p>Entity Masking a [v] in a blue [v] singing</p>
    <p>Source sentence a lady in a blue dress singing</p>
    <p>Uses Flickr30K entity annotations (Plummer et al., 2015)  26% of source words are removed (3.4 blanks / sent)</p>
  </div>
  <div class="page">
    <p>Types of degradation (3)</p>
    <p>Color Masking a lady in a [v] dress singing</p>
    <p>Entity Masking a [v] in a blue [v] singing</p>
    <p>Progressive Masking (k=4) a lady in a [v] [v] [v]</p>
    <p>Progressive Masking (k=2) a lady [v] [v] [v] [v] [v]</p>
    <p>Progressive Masking (k=0) [v] [v] [v] [v] [v] [v] [v]</p>
    <p>Source sentence a lady in a blue dress singing</p>
    <p>Removal of any words  16 variants with  MMT task becomes multimodal sentence completion/captioning</p>
  </div>
  <div class="page">
    <p>Settings</p>
    <p>2-layer GRU-based encoder/decoder NMT  400D hidden units, 200D embeddings</p>
    <p>Visual features  ResNet-50 CNN pretrained on ImageNet  2048D pooled vectoral representations  2048x8x8 convolutional feature maps</p>
    <p>Multi30K dataset  Primary language pair: English  French</p>
  </div>
  <div class="page">
    <p>Simple grounding  Tied INITialization of encoders and decoders</p>
    <p>(Calixto and Liu, 2017), (Caglayan et al., 2017)</p>
    <p>MMT methods</p>
    <p>Linear Layer</p>
    <p>Decoder</p>
    <p>EncoderH id</p>
    <p>d e</p>
    <p>n S</p>
    <p>ta te</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>Source Word Encodings</p>
    <p>Multimodal attention  DIRECT fusion uses modality specific attention layers and concatenates their</p>
    <p>output (Caglayan et al., 2016), (Calixto et al., 2016)</p>
    <p>MMT methods</p>
    <p>V isu</p>
    <p>a l</p>
    <p>A tte</p>
    <p>n tio</p>
    <p>n Te</p>
    <p>x tu</p>
    <p>a l</p>
    <p>A tte</p>
    <p>n tio</p>
    <p>n</p>
    <p>D e</p>
    <p>co d</p>
    <p>e r</p>
    <p>Source Sentence</p>
  </div>
  <div class="page">
    <p>Source Word Encodings</p>
    <p>Multimodal attention  HIERarchical fusion applies a third attention layer instead of concatenation</p>
    <p>(Libovick and Helcl, 2017)</p>
    <p>MMT methods</p>
    <p>V isu</p>
    <p>a l</p>
    <p>A tte</p>
    <p>n tio</p>
    <p>n Te</p>
    <p>x tu</p>
    <p>a l</p>
    <p>A tte</p>
    <p>n tio</p>
    <p>n</p>
    <p>Source Sentence</p>
    <p>D e</p>
    <p>co d</p>
    <p>e r</p>
    <p>H ie</p>
    <p>ra rch</p>
    <p>ica l</p>
    <p>A tte</p>
    <p>n tio</p>
    <p>n</p>
  </div>
  <div class="page">
    <p>Mean and standard deviation (3 runs) of METEOR scores  Statistical significance testing with MultEval (Clark et al., 2011)</p>
    <p>Adversarial evaluation  Shuffled (incongruent) image features (Elliott 2018)  Incongruent decoding: Incongruent features at inference time-only  Blinding: Incongruent features at training and inference times</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>Method Baseline</p>
    <p>METEOR</p>
    <p>NMT 70.6  0.5</p>
    <p>INIT 70.7  0.2</p>
    <p>HIER 70.9  0.3</p>
    <p>DIRECT 70.9  0.2</p>
    <p>Upper bound - no masking</p>
    <p>MMTs slightly better than NMT on average</p>
  </div>
  <div class="page">
    <p>Method Baseline</p>
    <p>METEOR Masked</p>
    <p>METEOR</p>
    <p>NMT 70.6  0.5 68.4  0.1</p>
    <p>INIT 70.7  0.2</p>
    <p>HIER 70.9  0.3</p>
    <p>DIRECT 70.9  0.2</p>
    <p>Color masking</p>
    <p>Masked NMT suffers a substantial 2.2 drop</p>
  </div>
  <div class="page">
    <p>Color masking</p>
    <p>Masked NMT suffers a substantial 2.2 drop  Masked MMT significantly better than masked NMT</p>
    <p>Method Baseline</p>
    <p>METEOR Masked</p>
    <p>METEOR</p>
    <p>NMT 70.6  0.5 68.4  0.1</p>
    <p>INIT 70.7  0.2 68.9  0.1</p>
    <p>HIER 70.9  0.3 69.0  0.3</p>
    <p>DIRECT 70.9  0.2 68.8  0.3</p>
  </div>
  <div class="page">
    <p>Color masking</p>
    <p>Method Baseline</p>
    <p>METEOR Masked</p>
    <p>METEOR Masked color Accuracy (%)</p>
    <p>NMT 70.6  0.5 68.4  0.1 32.5</p>
    <p>INIT 70.7  0.2 68.9  0.1 36.5</p>
    <p>HIER 70.9  0.3 69.0  0.3 44.5</p>
    <p>DIRECT 70.9  0.2 68.8  0.3 44.5</p>
    <p>Masked NMT suffers a substantial 2.2 drop  Masked MMT significantly better than masked NMT  Accuracy in color translation much better in attentive MMT</p>
  </div>
  <div class="page">
    <p>Color masking</p>
  </div>
  <div class="page">
    <p>Entity masking</p>
    <p>NMT suffers &gt; 20 points drop</p>
  </div>
  <div class="page">
    <p>Entity masking</p>
    <p>NMT suffers &gt; 20 points drop</p>
    <p>Up to 4.2 METEOR recovered by MMT</p>
  </div>
  <div class="page">
    <p>Entity masking</p>
    <p>NMT suffers &gt; 20 points drop</p>
    <p>Up to 4.2 METEOR recovered by MMT</p>
    <p>Models are visually sensitive: Up to ~10 METEOR drop with</p>
    <p>incongruent decoding</p>
  </div>
  <div class="page">
    <p>Entity masking (all languages)</p>
    <p>MMT Gain over NMT</p>
    <p>English  INIT HIER DIRECT Average</p>
    <p>Czech 1.4 1.7 1.7 1.6</p>
    <p>German 2.1 2.5 2.7 2.4</p>
    <p>French 3.4 3.9 4.2 3.8</p>
    <p>Average 2.3 2.7 2.9</p>
    <p>All languages benefit from visual context</p>
    <p>French benefits the most (less morphology)</p>
    <p>Multimodal attention better than INIT, Direct</p>
    <p>fusion slightly better than hierarchical</p>
  </div>
  <div class="page">
    <p>Entity masking (attention)</p>
    <p>A typo in the source (song) translated to chanson</p>
    <p>Visual attention barely changes</p>
  </div>
  <div class="page">
    <p>Entity masking (attention)</p>
    <p>mother, song and day are masked</p>
    <p>Textual attention is less confident, visual attention works!</p>
  </div>
  <div class="page">
    <p>Entity masking</p>
    <p>MMT is attentive, INC is incongruent decoding</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>As more information is removed, all MMT models leverage visual context,</p>
    <p>up to 7 METEOR points</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Attentive models perform better than INIT</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Upper bound: ~7 METEOR when all words are masked</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Original k=12 k=4</p>
    <p>NMT 70.6 63.9 28.6</p>
    <p>Compare two degraded variants to original Multi30K</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Original k=12 k=4</p>
    <p>NMT 70.6 63.9 28.6</p>
    <p>DIRECT MMT + 0.3 + 0.6 + 3.7</p>
    <p>Compare two degraded variants to original Multi30K  MMT improves over NMT as linguistic information (k) is removed</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Original k=12 k=4</p>
    <p>NMT 70.6 63.9 28.6</p>
    <p>DIRECT MMT + 0.3 + 0.6 + 3.7</p>
    <p>Incongruent Dec. - 0.7 - 1.4 - 6.4</p>
    <p>Compare two degraded variants to original Multi30K  MMT improves over NMT as linguistic information (k) is removed</p>
    <p>It also becomes sensitive to the visual incongruence</p>
    <p>(Relative to DIRECT MMT)</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>Original k=12 k=4</p>
    <p>NMT 70.6 63.9 28.6</p>
    <p>DIRECT MMT + 0.3 + 0.6 + 3.7</p>
    <p>Incongruent Dec. - 0.7 - 1.4 - 6.4</p>
    <p>Blinding 70.6 64.1 28.4</p>
    <p>Compare two degraded variants to original Multi30K  MMT improves over NMT as linguistic information (k) is removed</p>
    <p>It also becomes sensitive to the visual incongruence  MMT that never sees correct features converges to text-only NMT</p>
    <p>MMT improvements are not random</p>
    <p>(Relative to DIRECT MMT)</p>
  </div>
  <div class="page">
    <p>Progressive masking</p>
    <p>MMT is attentive, INC is incongruent decoding</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Hypothesis 1: MMT models should perform better than text-only models if image is effectively taken into account</p>
    <p>Visual info is taken into account if modalities are complementary rather than redundant</p>
    <p>Incorrect visual info harms performance substantially more   Hypothesis 2: More sophisticated MMT models should perform better</p>
    <p>than simpler MMT models  Attentive MMT better than simple INIT grounding  Attentive MMT recovers more from impact of substantial masking</p>
  </div>
  <div class="page">
    <p>Future work</p>
    <p>Grounding as a way to reduce biases and improve robustness to errors   Better models to balance complementary and redundant information   Multimodality to resolve unknown words</p>
    <p>O cachorro corre no campo cheio de florzinhas brancas.</p>
    <p>The dachshund is running in the fields full of little white flowers.</p>
    <p>O UNK corre no campo cheio de florzinhas brancas.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. 2016. Multi30k: Multilingual english-german image descriptions. In Proceedings of the 5th Workshop on Vision and Language. Association for Computational Linguistics, Berlin, Germany, pages 7074.</p>
    <p>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics 2:6778.</p>
    <p>Chiraag Lala, Pranava Swaroop Madhyastha, Carolina Scarton, and Lucia Specia. 2018. Sheffield submissions for WMT18 multimodal translation shared task. In Proceedings of the Third Conference on Machine Translation. Association for Computational Linguistics, Belgium, Brussels, pages 630637.</p>
    <p>Desmond Elliott. 2018. Adversarial evaluation of multimodal machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, pages 29742978.</p>
    <p>Stig-Arne Grnroos, Benoit Huet, Mikko Kurimo, Jorma Laaksonen, Bernard Merialdo, Phu Pham, Mats Sjberg, Umut Sulubacak, Jrg Tiedemann, Raphael Troncy, and Ral Vzquez. 2018. The MeMAD submission to the WMT18 multimodal translation task. In Proceedings of the Third Conference on Machine Translation. Association for Computational Linguistics, Belgium, Brussels, pages 609617.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Desmond Elliott, Stella Frank, Loc Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers. Association for Computational Linguistics, Copenhagen, Denmark, pages 215233.</p>
    <p>Loc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the third shared task on multimodal machine translation. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers. Association for Computational Linguistics, Belgium, Brussels, pages 308327.</p>
    <p>Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In 2015 IEEE International Conference on Computer Vision (ICCV). pages 26412649.</p>
    <p>Iacer Calixto and Qun Liu. 2017. Incorporating global visual features into attention based neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Copenhagen, Denmark, pages 9921003.</p>
    <p>Ozan Caglayan, Walid Aransa, Adrien Bardet, Mercedes Garca-Martnez, Fethi Bougares, Loc Barrault, Marc Masana, Luis Herranz, and Joost van de Weijer. 2017. LIUM-CVC submissions for WMT17 multimodal translation task. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers. Association for Computational Linguistics, Copenhagen, Denmark, pages 432439.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Ozan Caglayan, Loc Barrault, and Fethi Bougares. 2016. Multimodal attention for neural machine translation. Computing Research Repository arXiv:1609.03976.</p>
    <p>Iacer Calixto, Desmond Elliott, and Stella Frank. 2016. DCU-UvA multimodal MT system report. In Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 634638.</p>
    <p>Jindich Libovick and Jindich Helcl. 2017. Attention strategies for multi-source sequence-to-sequence learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, pages 196202.</p>
    <p>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2. Association for Computational Linguistics, Stroudsburg, PA, USA, HLT 11, pages 176181.</p>
  </div>
</Presentation>
