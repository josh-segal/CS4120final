<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling</p>
    <p>Structure Makes Them Better Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark,</p>
    <p>and Phil Blunsom</p>
  </div>
  <div class="page">
    <p>Language exhibits hierarchical structure</p>
    <p>but LSTMs work so well without explicit notions of structure.</p>
    <p>Motivation</p>
    <p>[[The cat [that he adopted]] [sleeps]]</p>
  </div>
  <div class="page">
    <p>Number Agreement</p>
    <p>Number agreement is a cognitively-motivated probe to distinguish hierarchical theories from purely sequential ones.</p>
    <p>Number agreement example with two attractors (Linzen et al., 2016)</p>
  </div>
  <div class="page">
    <p>Number Agreement is Sensitive to Syntactic Structure</p>
    <p>Number agreement reflects the dependency relation</p>
    <p>between subjects and verbs Models that can capture</p>
    <p>headedness should do better at number agreement</p>
  </div>
  <div class="page">
    <p>Revisit the prior work of Linzen et al. (2016) that argues LSTMs trained on language modelling objectives fail to learn such dependencies.</p>
    <p>Investigate whether models that explicitly incorporate syntactic structure can do better, and how syntactic information should be encoded.</p>
    <p>Demonstrate that how the structure is built affects number agreement generalisation.</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>Number Agreement Dataset Overview</p>
    <p>Train Test</p>
    <p>Sentences 141,948 1,211,080</p>
    <p>Types 10,025 10,025</p>
    <p>Tokens 3,159,622 26,512,851</p>
    <p>Number agreement dataset is derived from dependency-parsed</p>
    <p>Wikipedia corpus</p>
    <p>All intervening nouns must be of the same number</p>
  </div>
  <div class="page">
    <p>Number Agreement Dataset Overview # Attractors # Instances %</p>
    <p>Instances</p>
    <p>n=0 1,146,330 94.7%</p>
    <p>n=1 52,599 4.3%</p>
    <p>n=2 9,380 0.77%</p>
    <p>n=3 2,051 0.17%</p>
    <p>n=4 561 0.05%</p>
    <p>n=5 159 0.01%</p>
    <p>The vast majority of number agreement dependencies are</p>
    <p>sequential</p>
    <p>All intervening nouns must be of the same number</p>
  </div>
  <div class="page">
    <p>First Part: Can LSTMs Learn Number Agreement Well?</p>
    <p>Revisit the same question as Linzen et al. (2016):</p>
    <p>To what extent are LSTMs able to learn non-local syntax-sensitive dependencies in natural language?</p>
    <p>The model is trained with language modelling</p>
    <p>objectives</p>
  </div>
  <div class="page">
    <p>Linzen et al. LSTM Number Agreement Error Rates</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Small LSTM Number Agreement Error Rates</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Larger LSTM Number Agreement Error Rates</p>
    <p>Capacity matters for capturing non-local</p>
    <p>structural dependencies</p>
    <p>Despite this, relatively minor perplexity</p>
    <p>difference (~10%) between H=50 and</p>
    <p>H=150Lower is better</p>
  </div>
  <div class="page">
    <p>LSTM Number Agreement Error Rates</p>
    <p>Capacity and size of training corpus are not</p>
    <p>the full story</p>
    <p>Domain and training settings matter too</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Can Character LSTMs Learn Number Agreement Well?</p>
    <p>Character LSTMs have been used in various tasks, including machine translation, language modelling, and many others. + It is easier to exploit morphological cues. - Model has to resolve dependencies between sequences of</p>
    <p>tokens. - The sequential dependencies are much longer.</p>
  </div>
  <div class="page">
    <p>Character LSTM Agreement Error Rates</p>
    <p>State-of-the-art character LSTM (Melis et al., 2018) model on Hutter Prize, with 27M parameters.</p>
    <p>Trained, validated, and tested on the same data.</p>
    <p>Strong character LSTM model performs much worse for</p>
    <p>multiple attractor cases</p>
    <p>Consistent with earlier work (Sennrich, 2017) and potential</p>
    <p>avenue for improvement</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>LSTM language models are able to learn number agreement to a much larger extent than suggested by earlier work.  Independently confirmed by Gulordava et al. (2018).  We further identify model capacity as one of the reasons for the</p>
    <p>discrepancy.  Model tuning is important.</p>
    <p>A strong character LSTM language model performs much worse for number agreement with multiple attractors.</p>
    <p>First Part Quick Recap</p>
  </div>
  <div class="page">
    <p>Two Ways of Modelling Sentences</p>
  </div>
  <div class="page">
    <p>Three Concrete Alternatives for Modeling Sentences</p>
    <p>(NP the hungry cat)(S (VP meows</p>
    <p>RNNG (Dyer et al., 2016)</p>
    <p>(S cat meows(NP the hungry</p>
    <p>Sequential LSTMs with Syntax (Choe and Charniak, 2016)</p>
    <p>)NP (VP</p>
    <p>cat meowsthe hungry</p>
    <p>Sequential LSTMs without Syntax</p>
    <p>P(x, y)</p>
    <p>P(x)</p>
    <p>P(x, y) Hierarchical inductive bias</p>
  </div>
  <div class="page">
    <p>Evidence of Headedness in the Composition Function</p>
    <p>Kuncoro et al. (2017) found evidence of syntactic headedness in RNNGs</p>
    <p>(Dyer et al., 2016)</p>
    <p>The discovery of syntactic heads would be useful for number agreement</p>
    <p>Inspection of composed representation through the attention weights</p>
  </div>
  <div class="page">
    <p>Experimental Settings</p>
    <p>(NP the hungry cat)(S (VP meows</p>
    <p>All models are trained, validated, and tested on the same dataset.</p>
    <p>On the training split, the syntactic models are trained using predicted phrase-structure trees from the Stanford parser.</p>
    <p>At test time, we run the incremental beam search (Stern et al., 2017) procedure up to the main verb for both verb forms, and take the highest-scoring tree.</p>
    <p>? meow The most probable tree might potentially be</p>
    <p>different for the correct/incorrect verbs</p>
  </div>
  <div class="page">
    <p>Experimental Findings</p>
    <p>Performance differences are</p>
    <p>significant (p &lt; 0.05)</p>
    <p>n=5</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Perplexity</p>
    <p>Dev ppl.</p>
    <p>LSTM LM 72.6</p>
    <p>Seq. Syntactic LSTM 79.2</p>
    <p>RNNGs 77.9</p>
    <p>Perplexity for syntactic models are obtained with importance sampling (Dyer et al., 2016)</p>
    <p>LSTM LM has the best perplexity despite worse number</p>
    <p>agreement performance</p>
  </div>
  <div class="page">
    <p>Further Remarks: Confound in the Dataset</p>
    <p>LSTM language models largely succeed in number agreement</p>
    <p>In around 80% of cases with multiple attractors, the agreement controller coincides with the first noun.</p>
    <p>Key question: How do LSTMs succeed in this task? Identifying the syntactic structure Memorising the first noun</p>
    <p>Kuncoro et al., L2HM 2018</p>
  </div>
  <div class="page">
    <p>Control condition breaks the correlation between the first noun and agreement controller</p>
    <p>Confounded by first nouns</p>
    <p>Lower is better</p>
    <p>Control Condition Experiments for LSTM LM</p>
    <p>Much less likely to affect human experiments</p>
  </div>
  <div class="page">
    <p>Control Condition Experiments for RNNG</p>
    <p>Lower is better</p>
    <p>Same y-axis scale as LSTM LM</p>
    <p>Control for cues that artificial learners can exploit in a cognitive task.</p>
    <p>Adversarial evaluation can better distinguish between models with correct generalisation and those that overfit to surface cues.</p>
  </div>
  <div class="page">
    <p>Related Work  Augmenting our models with a hierarchical inductive bias</p>
    <p>is not the only way to achieve better number agreement.</p>
    <p>Another alternative is to make relevant past information more salient, such as through memory architectures or attention mechanism.  Yogatama et al. (2018) found that both attention mechanism and</p>
    <p>memory architectures outperform standard LSTMs.  They found that a model with a stack-structured memory</p>
    <p>performs best, also demonstrating that a hierarchical, nested inductive bias is important for capturing syntactic dependencies.</p>
  </div>
  <div class="page">
    <p>Second Part Quick Recap</p>
    <p>RNNGs considerably outperform LSTM language model and sequential syntactic LSTM for number agreement with multiple attractors.  Syntactic annotation alone has little impact on number agreement</p>
    <p>accuracy.  RNNGs success is due to the hierarchical inductive bias.  The RNNGs performance is a new state of the art on this dataset</p>
    <p>(previous best from Yogatama et al. (2018) for n=5 is 88.0% vs 91.8%)</p>
    <p>Perplexity is only loosely correlated with number agreement.  Independently confirm the finding of Tran et al. (2018).</p>
  </div>
  <div class="page">
    <p>Different Tree Traversals</p>
    <p>RNNGs operate according to a top-down, left-to-right traversal</p>
    <p>Here we propose two alternative tree construction orders for RNNGs: left-corner and bottom-up traversals.</p>
    <p>x: the flowers in the vase are/is [blooming]</p>
    <p>(NP (NP the flowers) (PP in (NP the vase)))(S (VP are/is ?</p>
    <p>(NP (NP the flowers) (PP in (NP the vase))) are/is ?</p>
    <p>(NP (NP the flowers) (PP in (NP the vase)))(S are/is ?</p>
    <p>Top-down</p>
    <p>Bottom-up</p>
    <p>Left-corner</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Top-Down</p>
    <p>S START</p>
    <p>TOP-DOWN</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Top-Down</p>
    <p>NP</p>
    <p>S START</p>
    <p>TOP-DOWN</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Top-Down</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>S START</p>
    <p>TOP-DOWN</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Top-Down</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>S START</p>
    <p>TOP-DOWNVP</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Left-Corner</p>
    <p>The</p>
    <p>START</p>
    <p>LEFT-CORNE R</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Left-Corner</p>
    <p>NP</p>
    <p>The</p>
    <p>LEFT-CORNE R</p>
    <p>START</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Left-Corner</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>LEFT-CORNE R</p>
    <p>START</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Left-Corner</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>S</p>
    <p>LEFT-CORNE R</p>
    <p>START</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Bottom-Up</p>
    <p>The</p>
    <p>START</p>
    <p>BOTTOM-UP</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Bottom-Up</p>
    <p>The</p>
    <p>START</p>
    <p>BOTTOM-UP</p>
    <p>hungry cat</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Bottom-Up</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>START</p>
    <p>BOTTOM-UP</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Bottom-Up</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>START</p>
    <p>BOTTOM-UP</p>
    <p>meows</p>
  </div>
  <div class="page">
    <p>Quick Illustration of the Differences: Bottom-Up</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>START</p>
    <p>BOTTOM-UP</p>
    <p>meows</p>
    <p>VP</p>
  </div>
  <div class="page">
    <p>Why Does The Build Order Matter?</p>
    <p>Machine learning  The three different strategies yield different intermediate states during</p>
    <p>the generation process and impose different biases on the learner.</p>
    <p>Cognitive  Earlier work in parsing has characterised the strategies plausibility in</p>
    <p>human sentence processing (Johnson-Laird, 1983; Pulman, 1986; Resnik, 1992). We evaluate these strategies as models of generation (Manning and Carpenter, 1997) in terms of number agreement accuracy.</p>
  </div>
  <div class="page">
    <p>Bottom-up Traversal</p>
    <p>The</p>
    <p>The</p>
    <p>Topmost stack element</p>
    <p>x, y: (S (NP the hungry cat) (VP meows))</p>
    <p>Action: GEN(The)</p>
  </div>
  <div class="page">
    <p>Bottom-Up Traversal</p>
    <p>The hungry cat</p>
    <p>The</p>
    <p>hungry</p>
    <p>cat</p>
    <p>Topmost stack element</p>
    <p>Action: GEN(hungry), GEN(cat)</p>
    <p>x, y: (S (NP the hungry cat) (VP meows))</p>
  </div>
  <div class="page">
    <p>Bottom-Up Traversal</p>
    <p>NP</p>
    <p>The hungry cat</p>
    <p>(NP The hungry cat)</p>
    <p>Topmost stack element</p>
    <p>Action: REDUCE-3-NP</p>
    <p>x, y: (S (NP the hungry cat) (VP meows))</p>
  </div>
  <div class="page">
    <p>Bottom-Up Traversal</p>
    <p>NP</p>
    <p>The hungry cat meows</p>
    <p>(NP the hungry cat)</p>
    <p>meows</p>
    <p>Topmost stack element</p>
    <p>x, y: (S (NP the hungry cat) (VP meows))</p>
    <p>Action: REDUCE-1-VP</p>
    <p>(NP the hungry cat)</p>
    <p>(VP meows)</p>
    <p>Topmost stack element</p>
  </div>
  <div class="page">
    <p>Bottom-Up Traversal: After REDUCE-1-VP</p>
    <p>NP</p>
    <p>The hungry cat meows</p>
    <p>(NP the hungry cat)</p>
    <p>(VP meows)</p>
    <p>Topmost stack element</p>
    <p>Action: REDUCE-1-VP</p>
    <p>VP</p>
    <p>x, y: (S (NP the hungry cat) (VP meows))</p>
  </div>
  <div class="page">
    <p>Bottom-Up Parameterisation of Constituent Extent</p>
    <p>Stick-breaking construction</p>
  </div>
  <div class="page">
    <p>Summary Statistics</p>
    <p>Avg. Stack Depth Dev ppl. p(x, y)</p>
    <p>Top-Down 12.29 94.9</p>
    <p>Left-Corner 11.45 95.9</p>
    <p>Bottom-Up 7.41 96.5</p>
    <p>Near-identical perplexity for each variant</p>
    <p>Bottom-up has the shortest stack depth</p>
  </div>
  <div class="page">
    <p>Different Traversal Number Agreement Error Rates</p>
    <p>n=2 n=3 n=4</p>
    <p>Our LSTM (H=350) 5.8 9.6 14.1</p>
    <p>Top-Down 5.5 7.8 8.9</p>
    <p>Left-Corner 5.4 8.2 9.9</p>
    <p>Bottom-Up 5.7 8.5 9.7</p>
    <p>Top-down performs best for n=3 and n=4</p>
    <p>For n=4 this is significant (p &lt; 0.05)</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Part Three Recap and Outlook  We proposed two new RNNG variants with different tree construction</p>
    <p>orders: left-corner and bottom-up RNNGs.</p>
    <p>Top-down construction still performs best in number agreement.  It is the most anticipatory (Marslen-Wilson, 1973; Tanenhaus et al.,</p>
    <p>We can apply the three strategies to parsing and as linking hypothesis to human brain signal during comprehension (Hale et al., 2018).</p>
  </div>
  <div class="page">
    <p>Conclusion  LSTM language models with enough capacity can learn number agreement</p>
    <p>well, while a strong character LSTM performs much worse.</p>
    <p>Explicitly modelling the syntactic structure with RNNGs that have a hierarchical inductive bias leads to much better number agreement.  Syntactic annotation alone does not help if the model is still sequential.</p>
    <p>Top-down construction order outperforms left-corner and bottom-up variants in difficult number agreement cases.</p>
    <p>Perplexity does not completely correlate with number agreement.</p>
  </div>
  <div class="page">
    <p>The end &amp; thank you</p>
  </div>
</Presentation>
