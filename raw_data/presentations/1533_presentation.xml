<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Estimating Unseen Deduplication  from Theory to Practice</p>
    <p>Danny Harnik, Ety Khaitzin, Dmitry Sotnikov</p>
    <p>IBM Research labs - Haifa</p>
  </div>
  <div class="page">
    <p>Deduplication and Estimation</p>
    <p>Deduplication = removing duplicates in storage</p>
    <p>Deduplication Estimation: Given large a large dataset, understand the potential benefit from deduplication.</p>
    <p>Why estimate?</p>
    <p>Sizing: Different data reduction ratio  different amount of storage to buy ($$$!)</p>
    <p>Data placement</p>
    <p>Choosing storage</p>
    <p>Customers are asking for help</p>
    <p>Rule of thumb estimations exist, but:</p>
    <p>The savings for similar data types can vary greatly between users</p>
    <p>To really know you must look at the actual data.</p>
  </div>
  <div class="page">
    <p>Deduplication Estimation</p>
    <p>Obvious solution: scan the entire dataset</p>
    <p>Extremely taxing for very large datasets</p>
    <p>Want to estimate without seeing all of the data!</p>
    <p>Easy for compression [HKMST13]</p>
    <p>Hard for deduplication [HMNSV12]</p>
    <p>Hardness stems from the global nature of deduplication</p>
    <p>Bottom line:</p>
    <p>Need to sample a very large fraction in order to be accurate.</p>
    <p>[VV11]  need (N / log N) samples</p>
    <p>Talk plan:</p>
    <p>Note 1: Discussing deduplication in general</p>
    <p>Could be Fixed size or variable sized chunking</p>
    <p>our tests use fixed 4KB chunks</p>
    <p>Estimate potential  perfect deduplication.</p>
    <p>Note 2: Important related questions</p>
    <p>Distinct elements (data bases)</p>
    <p>How many different species? (Biology)</p>
  </div>
  <div class="page">
    <p>Solutions (in practice)</p>
    <p>Low memory streaming algorithms</p>
    <p>Deduplication specific solutions [HMNSV12], [XCS13]</p>
    <p>Heuristic approaches:</p>
    <p>Ratio on sample</p>
    <p>Statistical estimators</p>
    <p>[S81]</p>
    <p>[C84]</p>
    <p>[HNSS95], [HS98]</p>
    <p>[CCMN00]</p>
  </div>
  <div class="page">
    <p>Solutions (in theory)</p>
    <p>Estimating the Unseen, Valiant &amp; Valiant [VV11], [VV13]</p>
    <p>Algorithm with provable accuracy with a sample of size O(N / log N)</p>
    <p>Matches the lower bound</p>
    <p>But lots of problems moving from theory to practice</p>
  </div>
  <div class="page">
    <p>Our results</p>
    <p>What sample size is enough?</p>
    <p>Present a new method to gauge the accuracy of the estimation</p>
    <p>How to sample? No clear speedup in random reads</p>
    <p>Show how to get accuracy when sampling at a 1MB granularity</p>
    <p>Present a new simple and stateless sampling algorithm</p>
    <p>High memory requirements from the algorithm</p>
    <p>Propose 2 low memory variants of the algorithm</p>
    <p>What about compression?</p>
    <p>Devise a combined deduplication and compression estimation method.</p>
  </div>
  <div class="page">
    <p>Duplication Frequency Histogram (DFH)</p>
    <p>The DFH x of a data set is a histogram {x1, x2, x3,}  xi how many chunks have duplication exactly i</p>
    <p>Examples:  N unique values (no duplication):</p>
    <p>All values appear exactly twice</p>
    <p>Half of the set are unique values</p>
    <p>and half appear 4 times</p>
    <p>A small representation</p>
    <p>Fully characterizes the duplications in the data set</p>
    <p>Can take a DFH of a full set or of a sample of the set</p>
    <p>N 0 0 0 0</p>
    <p>N/2 0 0 N/8 0</p>
  </div>
  <div class="page">
    <p>Map between full DFHs and sample DFHs</p>
    <p>Full DFHs Sample DFHs</p>
    <p>x</p>
    <p>Random sample</p>
    <p>yExpected sample</p>
    <p>Random sample of a full DFH induces a</p>
    <p>distribution on sample DFHs</p>
    <p>Expected sample of a full DFH is the</p>
    <p>expectancy of the sample DFHs in the distribution</p>
  </div>
  <div class="page">
    <p>Full DFHs Sample DFHs</p>
    <p>x</p>
    <p>y</p>
    <p>Expected sample</p>
    <p>The Unseen Algorithm [VV13]</p>
    <p>Input: Observed sample DFH y</p>
    <p>Find a DFH x for which the expected y has minimal distance from y</p>
    <p>Output: dedupe ratio according to x</p>
    <p>Use Linear Programming to find x</p>
    <p>y</p>
  </div>
  <div class="page">
    <p>Example of 30 independent tests at each sample size</p>
    <p>Same hypervisor data from previous slide</p>
    <p>Especially when</p>
    <p>compared to previous</p>
    <p>we converged???</p>
    <p>Cannot run</p>
    <p>multiple tests</p>
  </div>
  <div class="page">
    <p>Range Unseen</p>
    <p>returns upper and lower bounds rather than estimation</p>
    <p>W.h.p. the actual ratio lies inside the range</p>
  </div>
  <div class="page">
    <p>Full DFHs Sample DFHs</p>
    <p>The Range Unseen algorithm - definitions</p>
    <p>y</p>
    <p>Plausible</p>
    <p>DFHs</p>
    <p>Expected sample</p>
    <p>Expected sample</p>
    <p>Dont look at the minimal distance to y but rather at reasonable distances from y</p>
    <p>Plausible DFHs of a sample DFH y:</p>
    <p>All the full DFH for which the expected sample</p>
    <p>is within distance  of y</p>
  </div>
  <div class="page">
    <p>Full DFHs Sample DFHs</p>
    <p>The Range Unseen algorithm</p>
    <p>y</p>
    <p>Plausible</p>
    <p>DFHs</p>
    <p>Expected sample</p>
    <p>Expected sample</p>
    <p>xmax</p>
    <p>The Range Unseen Algorithm</p>
    <p>Input: Observed sample DFH y</p>
    <p>Of all the plausible DFH of y</p>
    <p>find the DFH xmin with the minimal dedupe ratio</p>
    <p>Find the DFH xmax with the maximal dedupe ratio</p>
    <p>Output: dedupe ratios of xmin and xmax</p>
    <p>Use Linear Programming to find xmin and xmax</p>
    <p>xmin</p>
  </div>
  <div class="page">
    <p>Our results</p>
    <p>What sample size is enough?</p>
    <p>Present a new method to gauge the accuracy of the estimation</p>
    <p>How to sample? No clear speedup in random reads</p>
    <p>Show how to get accuracy when sampling at a 1MB granularity</p>
    <p>Present a new simple and stateless sampling algorithm</p>
    <p>High memory requirements from the algorithm</p>
    <p>Propose 2 low memory variants of the algorithm</p>
    <p>What about compression?</p>
    <p>Devise a combined deduplication and compression estimation method.</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>includes low memory and reading at 1MB super-chunks</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>includes low memory and reading at 1MB super-chunks</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>includes low memory and reading at 1MB super-chunks</p>
  </div>
  <div class="page">
    <p>includes low memory and reading at 1MB super-chunks</p>
  </div>
  <div class="page">
    <p>Putting it all together</p>
    <p>includes low memory and reading at 1MB super-chunks</p>
  </div>
  <div class="page">
    <p>Can stop with 1-2% sample when dedupe ratio is very high or very low</p>
    <p>Otherwise need 8% or more</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>New fast, accurate, low memory estimation for data reduction</p>
    <p>Speed up of at least 3X over state of the art</p>
    <p>Assumes 15% sampling and data on rotating disks</p>
    <p>Will be much faster (15X and more) when:</p>
    <p>Reduction ratio is high or low  can stop early</p>
    <p>Data on flash  faster random access</p>
  </div>
  <div class="page">
    <p>Thank You !</p>
  </div>
</Presentation>
