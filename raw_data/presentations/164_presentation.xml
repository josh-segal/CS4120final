<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>2005 Andreas Haeberlen, Rice University 1</p>
    <p>Glacier: Highly durable, decentralized storage despite massive correlated failures</p>
    <p>Andreas Haeberlen</p>
    <p>Alan Mislove</p>
    <p>Peter Druschel</p>
    <p>Rice University Houston, TX</p>
    <p>Boston, MA May 2-4, 2005</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Many distributed applications require storage</p>
    <p>Cooperative storage: Aggregate storage on participating nodes</p>
    <p>Advantages:  Resilient  Highly scalable</p>
    <p>Examples: Farsite, PAST, OceanStore</p>
    <p>Structured overlay network</p>
  </div>
  <div class="page">
    <p>Motivation  Common assumption:</p>
    <p>High node diversity  Failure independence</p>
    <p>Unrealistic! Node population may have low diversity (e.g. OS)</p>
    <p>Worms can cause large-scale correlated Byzantine failures</p>
    <p>Reactive systems are too slow to prevent data loss</p>
  </div>
  <div class="page">
    <p>Related Work  Phoenix, OceanStore use</p>
    <p>introspection:  Build failure model  Store data on nodes</p>
    <p>with low correlation  Limitations:</p>
    <p>Model must reflect all possible correlations</p>
    <p>Even small inaccuracies may lead to data loss</p>
    <p>Users have an incentive to report incorrect data</p>
  </div>
  <div class="page">
    <p>Our Approach: Glacier</p>
    <p>Create massive redundancy to ensure that data survives any correlated failure with high probability</p>
    <p>Assumption: Magnitude of the failure can be bounded by fraction fmax</p>
    <p>Challenges:  Minimize storage</p>
    <p>and bandwidth requirements</p>
    <p>Withstand attacks, Byzantine failures</p>
  </div>
  <div class="page">
    <p>Glacier: Insertion  When a new object is</p>
    <p>inserted: 1. Apply erasure code 2. Attach manifest with</p>
    <p>hashes of fragments 3. Send each fragment</p>
    <p>to a different node  No remote delete</p>
    <p>operation, but lifetime of objects can be limited</p>
    <p>Storage is lease-based; reclaims unused storage</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>Glacier: Maintenance</p>
    <p>Nodes with distance store similar fragments</p>
    <p>Periodic maintenance:  Ask a peer node for</p>
    <p>its list of fragments  Compare with local</p>
    <p>list, recover any missing fragments</p>
    <p>Fragments remain on their nodes during offline periods</p>
    <p>Xk ?</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>Glacier: Recovery</p>
    <p>During a failure, some fragments are damaged or lost  Communication may not be possible  Unaffected nodes do not take any special action:</p>
    <p>Failed nodes are eventually repaired  Maintenance gradually restores lost fragments</p>
    <p>Time</p>
    <p>Insert Correlated failure</p>
    <p>Tfail Offline period</p>
  </div>
  <div class="page">
    <p>Glacier: Durability</p>
    <p>Example configuration:  48 fragments, any 5 sufficient for recovery  Bad news: Storage overhead 9.6x  Good news: Survives 60% correlated failure</p>
    <p>with P=0.999999 (single object)</p>
    <p>fmax Durabilit y</p>
    <p>Code Fragments Storage</p>
    <p>M o re</p>
    <p>st o ra</p>
    <p>g e</p>
    <p>H ig</p>
    <p>h e r d</p>
    <p>u ra</p>
    <p>b ili ty</p>
  </div>
  <div class="page">
    <p>Aggregation  If objects are small:</p>
    <p>Huge number of fragments</p>
    <p>High overhead for storage, management</p>
    <p>Solution: Aggregate objects before storing them in Glacier</p>
    <p>Challenges:  Untrusted</p>
    <p>environment  Aggregates must be</p>
    <p>self-authenticating</p>
    <p>App</p>
    <p>Glacier</p>
    <p>App</p>
    <p>Aggreg.</p>
    <p>Glacier</p>
  </div>
  <div class="page">
    <p>Aggregation: Links  Mapping from objects to</p>
    <p>aggregates is crucial!  Need durability  Need authentication</p>
    <p>Solution: Link aggregates  Result: DAG  Can recover mapping</p>
    <p>by traversing the DAG  DAG forms a hash</p>
    <p>tree; easy to authenticate</p>
    <p>Top-level pointer is kept in Glacier itself</p>
  </div>
  <div class="page">
    <p>Evaluation  Two sets of experiments:</p>
    <p>Trace-driven simulations (scalability, churn, ...)  Actual deployment: ePOST</p>
    <p>ePOST: A cooperative, serverless e-mail system  In production use: Initially 17 users, 20 nodes  Based on FreePastry, PAST, Scribe, POST  Added Glacier for durability</p>
    <p>Glacier configuration in ePOST:  48 fragments, 0.2 encoding  fmax=0.6, P=0.999999</p>
    <p>140 days of practical experience (incl. some failures)</p>
  </div>
  <div class="page">
    <p>Evaluation: Storage</p>
    <p>Inherent storage overhead: 48/5=9.6  17 GB of on-disk storage for 1.3GB of data  Actual storage overhead on disk: About 12.6</p>
  </div>
  <div class="page">
    <p>Evaluation: Network load</p>
    <p>During stable periods, traffic is comparable to PAST  In the ePOST experiment, a misconfiguration</p>
    <p>caused frequent traffic spikes  Long off-line periods were mistaken for failures</p>
  </div>
  <div class="page">
    <p>Evaluation: Recovery</p>
    <p>Experiment: Created a 'clone' of the ePOST ring with only 13 of the 31 nodes (a 58% failure!)</p>
    <p>Started recovery process on a freshly installed node:  User entered e-mail address and date of last use  Glacier located head of aggregate tree, recovered it  System was again ready for use; no data loss</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Large-scale correlated failures are a realistic threat to distributed storage systems</p>
    <p>Glacier provides hard durability guarantees with minimal assumptions about the failure model</p>
    <p>Glacier transforms abundant but unreliable disk space into reliable storage</p>
    <p>Bandwidth cost is low</p>
    <p>Thank you!</p>
  </div>
  <div class="page">
    <p>Glacier is available!</p>
    <p>Download: www.epostmail.org</p>
    <p>Serverless, secure e-mail  Easy to set up  Uses Glacier for durability</p>
  </div>
</Presentation>
