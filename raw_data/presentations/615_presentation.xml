<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>UC Berkeley</p>
    <p>Ari Rabkin and Randy Katz UC Berkeley</p>
    <p>USENIX LISA 2010</p>
    <p>Chukwa: a scalable log collector</p>
    <p>With thanks to Eric Yang, Jerome Boulon, Bill Graham, Corbin Hoenes, and all the other Chukwa developers, contributors, and users</p>
  </div>
  <div class="page">
    <p>Why collect logs?</p>
    <p>Many uses  Need logs to monitor/debug systems  Machine learning is getting increasingly good</p>
    <p>at detecting anomalies automatically.  Web log analysis is key to many businesses</p>
    <p>Easier to process if centralized</p>
  </div>
  <div class="page">
    <p>Three Bets 1. MapReduce processing</p>
    <p>is necessary at scale. 2. Reliability matters for log</p>
    <p>collection 3. Should use Hadoop, not</p>
    <p>re-write storage and processing layers</p>
  </div>
  <div class="page">
    <p>Leveraging Hadoop</p>
    <p>Really want to use HDFS for storage and MapReduce for processing. + Highly scalable, highly robust + Good integrity properties.</p>
    <p>HDFS has quirks - Files should be big - No concurrent appends - Weak synchr onization semantics</p>
  </div>
  <div class="page">
    <p>HDFS</p>
    <p>MapReduce Jobs</p>
    <p>The architecture</p>
    <p>Data Sink (5 minutes)</p>
    <p>Archival Storage (Indefinitely)</p>
    <p>Data App1 log</p>
    <p>App2 log</p>
    <p>Metrics</p>
    <p>Agent (seconds) Agent</p>
    <p>(seconds) Agent (seconds)</p>
    <p>One Per Node</p>
    <p>Collector (seconds) Collector</p>
    <p>(seconds)</p>
    <p>Per 100 nodes</p>
    <p>SQL DB (or HBase)</p>
  </div>
  <div class="page">
    <p>Design envelope</p>
    <p>Data Rate per host (bytes/sec)</p>
    <p>Chukwa not needed  clients should write direct to HDFS</p>
    <p>N um</p>
    <p>be r</p>
    <p>of H</p>
    <p>os ts</p>
    <p>Dont need Chukwa: use NFS instead</p>
    <p>Need better FS!</p>
    <p>Need more aggressive batching or fan-in control</p>
  </div>
  <div class="page">
    <p>Respecting boundaries</p>
    <p>Architecture captures the boundary between monitoring and production services  Important in practice!  Particularly nice in cloud context</p>
    <p>ds) s) Agent Co Collector Data Sink</p>
    <p>Structured Storage</p>
    <p>App1 log</p>
    <p>App2 log</p>
    <p>Metrics</p>
    <p>Monitoring system System being monitored</p>
    <p>Control Protocol</p>
  </div>
  <div class="page">
    <p>Comparison</p>
    <p>Amazon CloudWatch Metrics</p>
    <p>Logs</p>
  </div>
  <div class="page">
    <p>Data sources</p>
    <p>We optimize for the case of logs on disk  Supports legacy systems  Writes to local disk almost always succeed  Kept in memory in practice  fs caching</p>
    <p>Can also handle other data sources  adaptors are pluggable  Support syslog, other UDP, JMS messages.</p>
  </div>
  <div class="page">
    <p>Reliability</p>
    <p>Agents can crash  Record how much data from each source</p>
    <p>has been written successfully.  Resume at that point after crash  Fix duplicates in the storage layer</p>
    <p>Data Sent and committed not committed</p>
  </div>
  <div class="page">
    <p>Collector Agent HDFS</p>
    <p>Incorporating Asynchrony</p>
    <p>What about collector crashes?</p>
    <p>Want to tolerate asynchronous HDFS writes without blocking agent</p>
    <p>Solution: async. acks  Tell agent where data</p>
    <p>will be written if write succeeds.</p>
    <p>Uses single-writer aspect of HDFS</p>
    <p>ls</p>
    <p>Data</p>
    <p>In Foo.done @ 3000</p>
    <p>Length of Foo.done = 3000</p>
    <p>Data</p>
    <p>Foo.done@ 3000 .</p>
    <p>Committed</p>
    <p>Query</p>
  </div>
  <div class="page">
    <p>Fast path</p>
    <p>HDFS</p>
    <p>MapReduce Jobs</p>
    <p>Data Sink (5 minutes)</p>
    <p>Cleaned</p>
    <p>Data Storage (Indefinitely)</p>
    <p>Data App1 log</p>
    <p>App2 log</p>
    <p>Metrics</p>
    <p>Agent (seconds) Agent</p>
    <p>(seconds) Agent (seconds)</p>
    <p>One Per Node</p>
    <p>Collector (seconds) Collector</p>
    <p>(seconds)</p>
    <p>Per 100 nodes</p>
    <p>Fast-path clients (seconds)</p>
  </div>
  <div class="page">
    <p>Two modes</p>
    <p>Robust delivery  Data visible in minutes  Collects everything  Stores to HDFS  Will resend after a crash  Facilitates MapReduce  Used for bulk analysis</p>
    <p>Prompt delivery  Data visible in seconds  User-specified filter  Written over a socket  Delivered at most once  Facilitates near-real-time</p>
    <p>monitoring  Used for real-time</p>
    <p>graphing</p>
  </div>
  <div class="page">
    <p>Overhead [with Cloudstone]</p>
    <p>Without Chukwa With Chukwa</p>
    <p>O p</p>
    <p>s p</p>
    <p>e r</p>
    <p>se c</p>
  </div>
  <div class="page">
    <p>Collection rates</p>
    <p>Tested on EC2  Able to write 30MB/</p>
    <p>sec/collector  Note: data is about</p>
    <p>C o</p>
    <p>ll e c to</p>
    <p>r w</p>
    <p>ri te</p>
    <p>r a te</p>
    <p>( M</p>
    <p>B /s</p>
    <p>e c )</p>
    <p>Agent send rate (MB/sec)</p>
  </div>
  <div class="page">
    <p>Collection rates</p>
    <p>Scales linearly  Able to saturate</p>
    <p>underlying FS</p>
    <p>T o</p>
    <p>ta l</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>e c )</p>
    <p>Collectors</p>
  </div>
  <div class="page">
    <p>Experiences</p>
    <p>Currently in use at:  UC Berkeley's RAD Lab, to monitor Cloud</p>
    <p>experiments  CBS Interactive, Selective Media, and</p>
    <p>Tynt for web log analysis  Dozens of machines  Gigabytes to Terabytes per day</p>
    <p>Other sites toowe don't have a census</p>
  </div>
  <div class="page">
    <p>Related Work Handles logs</p>
    <p>Crash recovery?</p>
    <p>Metadata Interface Agent-side control</p>
    <p>Ganglia/ Nagios/ other NMS</p>
    <p>No No No UDP No Scribe Yes No No RPC Yes Flume Yes Yes Yes flexible No Chukwa Yes Yes Yes flexible Yes</p>
  </div>
  <div class="page">
    <p>Next steps</p>
    <p>Tighten security, to make Chukwa suitable for world-facing deployments</p>
    <p>Adjustable durability  Should be able to buffer arbitrary non-file data</p>
    <p>for reliability  HBase for near-real-time metrics display  Built-in indexing  Your idea here: Exploit open source!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Chukwa is a distributed log collection system that is</p>
    <p>Practical  In use at several sites</p>
    <p>Scalable  Builds on Hadoop for storage and processing</p>
    <p>Reliable  Able to tolerate multiple concurrent failures</p>
    <p>without losing or mangling data  Open Source</p>
    <p>Former Hadoop subproject, currently in Apache incubation, enroute to top level project.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>vs Splunk</p>
    <p>Significant overlap with Splunk.  Splunk uses syslog for transport.  Recently shifted towards MapReduce for</p>
    <p>evaluation.  Chukwa on its own doesnt [yet] do</p>
    <p>indexing or analysis.  Chukwa helps extract data from systems</p>
    <p>Reliably  Customizably</p>
  </div>
  <div class="page">
    <p>Assumptions about App</p>
    <p>Processing should happen off-node. (Production hosts are sacrosanct)</p>
    <p>Data should be available within minutes  Sub-minute delivery a non-goal.</p>
    <p>Data rates between 1 and 100KB/sec/node  Architecture tuned for these cases, but Chukwa</p>
    <p>could be adapted to handle lower/higher rates.  No assumptions about data format  Administrator or app needs to tell Chukwa</p>
    <p>where logs live.  Support for directly streaming logs as well.</p>
  </div>
  <div class="page">
    <p>On the back end</p>
    <p>Chukwa has a notion of parsed records, with complex schemas  Can put into structured storage  Display with HICC, a portal-style web interface.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Not storage, not processing</p>
    <p>Chukwa is a collection system.  Not responsible for storage:</p>
    <p>Use HDFS.  Our model is store-everything, prune late</p>
    <p>Not responsible for processing  Use MapReduce, or custom layer on HDFS</p>
    <p>Responsible for facilitating storage and processing</p>
    <p>Framework for processing collected data  Includes Pig support</p>
  </div>
  <div class="page">
    <p>Goal: Low Footprint</p>
    <p>Wanted minimal footprint on system and minimal changes to user workflow.  Application logging need not change.  Local logs stay put, Chukwa just copies them.  Can either specify filenames in static config, or</p>
    <p>else do some dynamic discovery.  Minimal human-produced metadata</p>
    <p>We track what data source + host a chunk came from. Can store additional tags.</p>
    <p>Chunks are numbered; can reconstruct order.  No schemas required to collect data</p>
  </div>
  <div class="page">
    <p>MapReduce and Hadoop</p>
    <p>Major motivation for Chukwa was storing and analyzing Hadoop logs.  At Yahoo!, common to dynamically allocate</p>
    <p>hundreds of nodes for a particular task.  This can generate MBs of logs per second.  Log analysis becomes difficult</p>
  </div>
  <div class="page">
    <p>Why Ganglia doesnt do this</p>
    <p>Many systems for metrics collection  Ganglia particularly well-known.  Many similar systems, including network</p>
    <p>management systems like OpenView  Focus on collecting and aggregating metrics in</p>
    <p>scalable low-cost way  But logs arent metrics. Want to archive</p>
    <p>everything, not summarize aggressively.  Really want reliable delivery; missing key</p>
    <p>parts of logs might make rest useless</p>
  </div>
  <div class="page">
    <p>Clouds</p>
    <p>Log processing needs to be scalable, since apps can get big quickly</p>
    <p>This used to be a problem for the Microsofts and Googles of the world. Now it affects many more.</p>
    <p>Cant rely on local storage  Nodes are ephemeral  Need to move logs off-node</p>
    <p>Cant do analysis on single host  The data is too big</p>
  </div>
  <div class="page">
    <p>Questions about Goals</p>
    <p>How many nodes? How much data?</p>
    <p>What data sources and delivery semantics?</p>
    <p>Processing expressiveness?</p>
    <p>Storage?</p>
  </div>
  <div class="page">
    <p>Chukwa goals</p>
    <p>How many nodes? How much data?  Scale to thousands of nodes. Hundreds of KB/</p>
    <p>sec/node on average, bursts above that OK  What data sources and delivery semantics?</p>
    <p>Console Logs and Metrics. Reliable delivery (as much as possible.) Minutes of delay are OK.</p>
    <p>Processing expressiveness?  MapReduce</p>
    <p>Storage?  Should be able to store data indefinitely.</p>
    <p>Support petabytes of stored data.</p>
  </div>
  <div class="page">
    <p>In contrast</p>
    <p>Ganglia, Network Management systems, and Amazons CloudWatch are all metricsoriented.  Goal is collecting and disseminating numerical</p>
    <p>metrics data in a scalable way.  Significantly different problem.</p>
    <p>Metrics have well defined semantics  Can tolerate data loss  Easy to aggregate/compress for archiving  Often time-critical</p>
    <p>Chukwa can serve these purposes, but isnt optimized for it.</p>
  </div>
  <div class="page">
    <p>Real-time Chukwa</p>
    <p>Chukwa was originally designed to support batch processing of logs  Minutes of latency OK.</p>
    <p>But we can do [best effort] real-time for free  Watch data go past at the collector  Check chunks against a search pattern, forward</p>
    <p>matching ones to a listener via TCP.  Dont need long-term storage or reliable delivery</p>
    <p>(do those via the regular data path)  Director uses this real-time path.</p>
  </div>
  <div class="page">
    <p>Related work summary</p>
    <p>Ganglia (and traditional NMS) dont do large data volumes or data rates</p>
    <p>Facebooks Scribe+Hive  Scribe is streaming, not batch  Hive is batch, and atop Hadoop  Doesn't do collection or visualization.  Doesnt have strong reliability properties</p>
    <p>Flume (from Cloudera)  Very similar to Chukwa  Emphasis on centralized management</p>
  </div>
</Presentation>
