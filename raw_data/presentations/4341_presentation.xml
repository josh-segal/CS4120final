<?xml version="1.0" ?>
<Presentation>
  <div class="page"/>
  <div class="page">
    <p>Motivation</p>
    <p>Good translation preserves the meaning of the sentence.  Neural MT learns to represent the sentence.</p>
    <p>Is the representation meaningful in some sense?</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Evaluating sentence representations</p>
    <p>Evaluation through classification.  Evaluation through similarity.  Evaluation using paraphrases.</p>
    <p>SentEval (Conneau et al., 2017)  prediction tasks for evaluating sentence embeddings  focus on semantics (recently, linguistics task added, too).</p>
    <p>HyTER paraphrases (Dreyer and Marcu, 2014)</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Evaluation through similarity</p>
    <p>7 similarity tasks: pairs of sentences + human judgement</p>
    <p>with training set, sent. similarity predicted by regression,  without training set, cosine similarity used as sent. sim.,  ultimately, the predicted sent. similarity is correlated with</p>
    <p>the golden truth.  In sum, we report them as AvgSim.</p>
    <p>I think it probably depends on your money. It depends on your country. 0</p>
    <p>Yes, you should mention your experience. Yes, you should make a resume 2</p>
    <p>Hope this is what you are looking for. Is this the kind of thing you're looking for? 4</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Classification task 1. Remove some</p>
    <p>points from the clusters.</p>
    <p>?</p>
    <p>?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Sequence-to-sequence with attention</p>
    <p>Bahdanau et al. (2014)  ij: weight of the j</p>
    <p>th encoder state for the ith decoder state</p>
    <p>no sentence embedding</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Multi-head inner attention</p>
    <p>Liu et al. (2016), Li et al. (2016), Lin et al. (2017)</p>
    <p>ij: weight of the j th encoder</p>
    <p>state for the ith column of MT</p>
    <p>concatenate columns of MT  sentence embedding</p>
    <p>linear projection of columns to control embedding size</p>
  </div>
  <div class="page">
    <p>decoder selects components of embedding</p>
    <p>decoder operates on entire embedding</p>
    <p>Proposed NMT architectures</p>
    <p>ATTN-CTX ATTN-ATTN (compound att.)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Evaluated NMT models</p>
    <p>model architectures:  FINAL, FINAL-CTX: no attention  AVGPOOL, MAXPOOL: pooling instead of attention  ATTN-CTX: inner attention, constant context vector  ATTN-ATTN: inner attention, decoder attention  TRF-ATTN-ATTN: Transformer with inner attention</p>
    <p>translation from English (to Czech or German), evaluating embeddings of English (source) sentences  encs: CzEng 1.7 (Bojar et al., 2016)  ende: Multi30K (Elliott et al., 2016; Helcl and Libovick, 2017)</p>
  </div>
  <div class="page">
    <p>Sample Results  translation quality encs Model Heads BLEU</p>
    <p>Manual (&gt; other)</p>
    <p>Manual ( other)</p>
    <p>Bahdanau ATTN  22.2 50.9 93.8</p>
    <p>compound attention</p>
    <p>ATTN-ATTN 8 18.4 42.5 88.6</p>
    <p>ATTN-ATTN 4 17.1</p>
    <p>inner attention + Cho</p>
    <p>ATTN-CTX 4 16.1 31.7 77.9</p>
    <p>Cho FINAL-CTX  15.5</p>
    <p>ATTN-ATTN 1 14.8 27.3 71.7</p>
    <p>Sutskever FINAL  10.8</p>
    <p>Selected models trained for translation from English to Czech. The embedding size is 1000 (except ATTN).</p>
  </div>
  <div class="page">
    <p>Sample Results  translation quality encs Model Heads BLEU</p>
    <p>Manual (&gt; other)</p>
    <p>Manual ( other)</p>
    <p>Bahdanau ATTN  22.2 50.9 93.8</p>
    <p>compound attention</p>
    <p>ATTN-ATTN 8 18.4 42.5 88.6</p>
    <p>ATTN-ATTN 4 17.1</p>
    <p>inner attention + Cho</p>
    <p>ATTN-CTX 4 16.1 31.7 77.9</p>
    <p>Cho FINAL-CTX  15.5</p>
    <p>ATTN-ATTN 1 14.8 27.3 71.7</p>
    <p>Sutskever FINAL  10.8</p>
    <p>Selected models trained for translation from English to Czech. The embedding size is 1000 (except ATTN).</p>
    <p>BLEU is consistent</p>
    <p>with human evaluation.</p>
  </div>
  <div class="page">
    <p>Sample Results  translation quality encs</p>
    <p>Selected models trained for translation from English to Czech. The embedding size is 1000 (except ATTN).</p>
    <p>Attention in the encoder</p>
    <p>helps translation</p>
    <p>quality.</p>
    <p>Model Heads BLEU Manual (&gt; other)</p>
    <p>Manual ( other)</p>
    <p>Bahdanau ATTN  22.2 50.9 93.8</p>
    <p>compound attention</p>
    <p>ATTN-ATTN 8 18.4 42.5 88.6</p>
    <p>ATTN-ATTN 4 17.1</p>
    <p>inner attention + Cho</p>
    <p>ATTN-CTX 4 16.1 31.7 77.9</p>
    <p>Cho FINAL-CTX  15.5</p>
    <p>ATTN-ATTN 1 14.8 27.3 71.7</p>
    <p>Sutskever FINAL  10.8</p>
  </div>
  <div class="page">
    <p>Sample Results  translation quality encs</p>
    <p>Selected models trained for translation from English to Czech. The embedding size is 1000 (except ATTN).</p>
    <p>More attention heads</p>
    <p>better translation</p>
    <p>quality.</p>
    <p>Model Heads BLEU Manual (&gt; other)</p>
    <p>Manual ( other)</p>
    <p>Bahdanau ATTN  22.2 50.9 93.8</p>
    <p>compound attention</p>
    <p>ATTN-ATTN 8 18.4 42.5 88.6</p>
    <p>ATTN-ATTN 4 17.1</p>
    <p>inner attention + Cho</p>
    <p>ATTN-CTX 4 16.1 31.7 77.9</p>
    <p>Cho FINAL-CTX  15.5</p>
    <p>ATTN-ATTN 1 14.8 27.3 71.7</p>
    <p>Sutskever FINAL  10.8</p>
  </div>
  <div class="page">
    <p>Sample Results  representation eval. encs</p>
    <p>Model Size Heads SentEval AvgAcc</p>
    <p>SentEval AvgSim</p>
    <p>Paraphrases class. accuracy (COCO)</p>
    <p>InferSent 4096  81.7 0.70 31.58</p>
    <p>GloVe bag-of-words 300  75.8 0.59 34.28</p>
    <p>FINAL-CTX (Cho) 1000  74.4 0.60 23.20</p>
    <p>ATTN-ATTN 1000 1 73.4 0.54 21.54</p>
    <p>ATTN-CTX 1000 4 72.2 0.45 14.60</p>
    <p>ATTN-ATTN 1000 4 70.8 0.39 10.84</p>
    <p>ATTN-ATTN 1000 8 70.0 0.36 10.24</p>
    <p>Selected models trained for translation from English to Czech. InferSent and GloVeBOW are trained on monolingual (English) data.</p>
  </div>
  <div class="page">
    <p>Sample Results  representation eval. encs</p>
    <p>Selected models trained for translation from English to Czech. InferSent and GloVeBOW are trained on monolingual (English) data.</p>
    <p>Baselines are hard to</p>
    <p>beat.</p>
    <p>Model Size Heads SentEval AvgAcc</p>
    <p>SentEval AvgSim</p>
    <p>Paraphrases class. accuracy (COCO)</p>
    <p>InferSent 4096  81.7 0.70 31.58</p>
    <p>GloVe bag-of-words 300  75.8 0.59 34.28</p>
    <p>FINAL-CTX (Cho) 1000  74.4 0.60 23.20</p>
    <p>ATTN-ATTN 1000 1 73.4 0.54 21.54</p>
    <p>ATTN-CTX 1000 4 72.2 0.45 14.60</p>
    <p>ATTN-ATTN 1000 4 70.8 0.39 10.84</p>
    <p>ATTN-ATTN 1000 8 70.0 0.36 10.24</p>
  </div>
  <div class="page">
    <p>Sample Results  representation eval. encs</p>
    <p>Model Size Heads SentEval AvgAcc</p>
    <p>SentEval AvgSim</p>
    <p>Paraphrases class. accuracy (COCO)</p>
    <p>InferSent 4096  81.7 0.70 31.58</p>
    <p>GloVe bag-of-words 300  75.8 0.59 34.28</p>
    <p>FINAL-CTX (Cho) 1000  74.4 0.60 23.20</p>
    <p>ATTN-ATTN 1000 1 73.4 0.54 21.54</p>
    <p>ATTN-CTX 1000 4 72.2 0.45 14.60</p>
    <p>ATTN-ATTN 1000 4 70.8 0.39 10.84</p>
    <p>ATTN-ATTN 1000 8 70.0 0.36 10.24</p>
    <p>Selected models trained for translation from English to Czech. InferSent and GloVeBOW are trained on monolingual (English) data.</p>
    <p>Attention harms the</p>
    <p>performance.</p>
  </div>
  <div class="page">
    <p>Sample Results  representation eval. encs</p>
    <p>Model Size Heads SentEval AvgAcc</p>
    <p>SentEval AvgSim</p>
    <p>Paraphrases class. accuracy (COCO)</p>
    <p>InferSent 4096  81.7 0.70 31.58</p>
    <p>GloVe bag-of-words 300  75.8 0.59 34.28</p>
    <p>FINAL-CTX (Cho) 1000  74.4 0.60 23.20</p>
    <p>ATTN-ATTN 1000 1 73.4 0.54 21.54</p>
    <p>ATTN-CTX 1000 4 72.2 0.45 14.60</p>
    <p>ATTN-ATTN 1000 4 70.8 0.39 10.84</p>
    <p>ATTN-ATTN 1000 8 70.0 0.36 10.24</p>
    <p>Selected models trained for translation from English to Czech. InferSent and GloVeBOW are trained on monolingual (English) data.</p>
    <p>More heads  worse results.</p>
  </div>
  <div class="page">
    <p>Full Results  correlations</p>
    <p>BLEU vs. other metrics: 0.57  0.31 (encs) 0.36  0.29 (ende)</p>
    <p>Pairwise average (except BLEU): 0.78  0.32 (encs) 0.57  0.23 (ende)</p>
    <p>en cs</p>
    <p>en de</p>
  </div>
  <div class="page">
    <p>BLEU vs. other metrics: 0.57  0.31 (encs) 0.54  0.27 (ende)</p>
    <p>Pairwise average (except BLEU): 0.78  0.32 (encs) 0.62  0.23 (ende)</p>
    <p>Full Results  correlations excluding Transformer</p>
    <p>en cs</p>
    <p>en de</p>
  </div>
  <div class="page">
    <p>Compound attention interpretation</p>
    <p>ATTN-ATTN en-cs model with 8 heads</p>
  </div>
  <div class="page">
    <p>Compound attention interpretation</p>
    <p>ATTN-ATTN en-cs model with 8 heads</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>relative position in encoder</p>
    <p>in n</p>
    <p>e r</p>
    <p>a tt</p>
    <p>e n</p>
    <p>tio n</p>
    <p>w</p>
    <p>e ig</p>
    <p>h t</p>
    <p>Average attention weight by position</p>
  </div>
  <div class="page">
    <p>relative position in encoder</p>
    <p>in n</p>
    <p>e r</p>
    <p>a tt</p>
    <p>e n</p>
    <p>tio n</p>
    <p>w</p>
    <p>e ig</p>
    <p>h t</p>
    <p>Average attention weight by position</p>
    <p>Heads divide the sentence</p>
    <p>equidistantly, not based on syntax or</p>
    <p>semantics.</p>
  </div>
  <div class="page">
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed NMT architecture combining the benefit of attention and one $&amp;!#* vector representing the whole sentence.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed NMT architecture combining the benefit of attention and one $&amp;!#* vector representing the whole sentence.</p>
    <p>Evaluated the obtained sentence embeddings using a wide range of semantic tasks.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed NMT architecture combining the benefit of attention and one $&amp;!#* vector representing the whole sentence.</p>
    <p>Evaluated the obtained sentence embeddings using a wide range of semantic tasks.</p>
    <p>The better the translation, the worse performance in meaning representation.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed NMT architecture combining the benefit of attention and one $&amp;!#* vector representing the whole sentence.</p>
    <p>Evaluated the obtained sentence embeddings using a wide range of semantic tasks.</p>
    <p>The better the translation, the worse performance in meaning representation.</p>
    <p>Heads divide sentence equidistantly, not logically.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Proposed NMT architecture combining the benefit of attention and one $&amp;!#* vector representing the whole sentence.</p>
    <p>Evaluated the obtained sentence embeddings using a wide range of semantic tasks.</p>
    <p>The better the translation, the worse performance in meaning representation.</p>
    <p>Heads divide sentence equidistantly, not logically.Join our JNLE Special Issue on Sentence Representations:</p>
    <p>http://ufal.mff.cuni.cz/jnle-on-sentence-representation</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Bibliography Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. In ICLR. Ondej Bojar et al. 2016. CzEng 1.6: Enlarged Czech-English parallel corpus with processing tools dockered. In Text, Speech, and Dialogue (TSD), number 9924 in LNAI, pages 231238. Kyunghyun Cho, Bart van Merrienboer, aglar Gulehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP. David L. Davies and Donald W. Bouldin. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-1:224227, 1979. Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. 2016. Multi30k: Multilingual English-German image descriptions. CoRR, abs/1605.00459. Jindich Helcl and Jindich Libovick. 2017. CUNI System for the WMT17 Multimodal Traslation Task.</p>
  </div>
  <div class="page">
    <p>Bibliography Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. In NIPS Vol. 2, NIPS15. Markus Dreyer and Daniel Marcu. 2014. HyTER networks of selected OpenMT08/09 sentences. Linguistic Data Consortium. LDC2014T09. Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. 2016. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering. CoRR, abs/1607.06275. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. CoRR, abs/1405.0312. Zhouhan Lin, Minwei Feng, Ccero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. CoRR, abs/1703.03130. Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning natural language inference using bidirectional LSTM model and inner-attention. CoRR, abs/1605.09090.</p>
  </div>
  <div class="page">
    <p>Bibliography Holger Schwenk and Matthijs Douze. 2017. Learning joint multilingual sentence representations with neural machine translation. CoRR, abs/1704.04154. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS.</p>
  </div>
</Presentation>
