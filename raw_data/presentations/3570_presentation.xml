<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Task SVM Feature Selection</p>
    <p>... or Convex Meta Learning for Discriminatively Finding Features and Kernels</p>
    <p>Tony Jebara (Columbia) &amp; Tommi Jaakkola (MIT)</p>
    <p>+ + + + +</p>
    <p>+ + + + +</p>
  </div>
  <div class="page">
    <p>Meta Learning: (Caruana, Thrun, Baxter) Use multiple related tasks to improve learning Typically implemented in Neural Nets (local minima) with a shared representation layer and input layer</p>
    <p>SVMs: Solve for a single classification/regression model Can we combine multi SVMs for different tasks yet with a shared input space and learn a common representation</p>
    <p>MED: A probabilistic approach to SVMs permitting many extensions Solves multiple SVM models sharing by selecting a representation via a convex program with unique solution</p>
    <p>Selection: The representations we consider here with MED: Linear Feature Selection (Jebara, Weston) Nonlinear Kernel Selection (Lanckriet, Cristianini)</p>
    <p>Multi Task Representations</p>
  </div>
  <div class="page">
    <p>Classification - Discriminative SVMs</p>
    <p>Given training examples: binary (+/- 1) labels: discriminant function:</p>
    <p>Minimize penalty function: with classification constraints:</p>
    <p>Solve QP Get widest margin model * BUT: not probabilistic, no priors, no flexible models</p>
    <p>x x</p>
    <p>x x</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x x x</p>
    <p>x x</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x</p>
    <p>x x</p>
    <p>x x</p>
    <p>Regularization Theory (Poggio, Girosi)</p>
  </div>
  <div class="page">
    <p>Maximum Entropy Discrimination Approach</p>
    <p>Many solutions may be valid.</p>
    <p>Solve for distribution P() over all good  (instead of  *). Find that mins subject to constraints:</p>
    <p>For non-separable, integrate over distribution over models &amp; margins (favoring large margins)</p>
    <p>The Admissible Set</p>
    <p>Information Projection</p>
  </div>
  <div class="page">
    <p>Maximum Entropy Discrimination Solution</p>
    <p>Analytic and Unique:</p>
    <p>partition function: dual objective to max:</p>
    <p>+ve Lagrange multipliers:</p>
    <p>Gaussian mean prior and linear L(X; ) gives back SVM</p>
    <p>MED Generalization Guarantees: Sparsity, VC-Dimension, PAC-Bayes</p>
  </div>
  <div class="page">
    <p>Feature Selection</p>
    <p>Purpose: pick 100 of 10000 features to get largest margin classifier (NP)</p>
    <p>Turn features on/off via binary switches</p>
    <p>Switch Prior: Bernoulli distribution rho parameter varies pruning level</p>
    <p>MED uniquely &amp; efficiently finds discriminative feature subset, analytic partition fn:</p>
    <p>The Admissible Set</p>
    <p>xx</p>
    <p>xx</p>
    <p>x</p>
    <p>x x</p>
    <p>x xx</p>
  </div>
  <div class="page">
    <p>Feature Selection</p>
    <p>Example: Intron-Exon Protein Classification: UCI: 240 dims; 200 train, 1300 test</p>
    <p>Objective Function for Classification</p>
    <p>Epsilon-Tube Regression also straightforward</p>
  </div>
  <div class="page">
    <p>Meta Feature Selection</p>
    <p>Given series of tasks: map inputs to binary: using M discriminants with 1 feature selection vector:</p>
    <p>Subject to MED classification constraints:</p>
    <p>Solve by optimizing joint objective function for all Lagrange</p>
  </div>
  <div class="page">
    <p>Meta Feature Selection Example: To ensure coupled tasks, turn multi-class data set</p>
    <p>into multiple 1 versus many tasks</p>
    <p>UCI Dermatology Dataset: 200 trains, 166 tests, 33 features, 6 classes</p>
    <p>Variable Feature Selection &amp; Regularization Levels</p>
    <p>Cross-validating over Regularization Levels</p>
  </div>
  <div class="page">
    <p>Meta Feature Selection for Regression</p>
    <p>D. Ross Cancer Data: 67 expression level feats. Use subset of 800 genes to predict all others</p>
    <p>Compared with random feature selection</p>
    <p>GENE</p>
    <p>CELL LINE CELL LINE</p>
  </div>
  <div class="page">
    <p>Kernel Selection</p>
    <p>Purpose: pick mixture of subset of Kernel matrices to get largest margin classifier, (learn the Gram matrix)</p>
    <p>Turn kernels on/off via binary switches</p>
    <p>Switch Prior: Bernoulli distribution</p>
    <p>Discriminant uses N models with multiple nonlinear mappings of datum</p>
    <p>MED solution has analytic concave objective fn:</p>
    <p>!</p>
  </div>
  <div class="page">
    <p>Meta Kernel Selection</p>
    <p>Given series of tasks with common (unknown) kernel matrix: using M discriminants with 1 feature selection vector:</p>
    <p>Subject to MED classification constraints:</p>
    <p>Solve by optimizing joint objective function for all Lagrange</p>
    <p>!</p>
  </div>
  <div class="page">
    <p>Meta Kernel Selection</p>
    <p>UCI Isolet data set (letter recognition from audio) 26 Classes used as 1 to Many Binary Classification</p>
    <p>SVM (Xs) Kernel Selection (red) Meta Kernel Selection (blue)</p>
    <p>Used rho = 0.1 Used rho = 0.01 Used rho = 0.001</p>
  </div>
  <div class="page">
    <p>Meta Feature Segmentation (Current Work)</p>
    <p>Given single task, single model, but each of the T points has its own feature selection configuration use 1 discriminant with T feature selection vectors:</p>
    <p>Subject to MED classification constraints:</p>
    <p>Solve transductively by computing distribution over unlabeled</p>
  </div>
  <div class="page">
    <p>Conclusions and Ongoing Work Meta Learning and Representation Learning can be applied to</p>
    <p>SVMs for both classification &amp; regression</p>
    <p>MED permits unique SVM solution with Learning feature selection Learning kernel selection</p>
    <p>MED permits straightforward Meta learning extensions Meta Learning feature selection Meta Learning kernel selection</p>
    <p>Feature selection helps performance Metalearning can help performance if tasks are coupled</p>
    <p>Segmentation inverts multiplicity: single model, multiple selections</p>
  </div>
</Presentation>
