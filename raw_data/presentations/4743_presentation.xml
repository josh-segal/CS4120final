<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Optimizing 27-point Stencil on</p>
    <p>Multicore Kaushik Datta, Samuel Williams, Vasily Volkov,</p>
    <p>Jonathan Carter, Leonid Oliker, John Shalf, and</p>
    <p>Katherine Yelick</p>
    <p>CRD/NERSC, Berkeley Lab</p>
    <p>EECS, University of California, Berkeley</p>
    <p>JTCarter@lbl.gov</p>
    <p>iWAPT 2009</p>
    <p>October 1-2 2009</p>
  </div>
  <div class="page">
    <p>Expanding Set of Manycore</p>
    <p>Architectures</p>
    <p>Potential to deliver most performance for space and power for HPC</p>
    <p>Server and PC commodity  Intel and AMD x86, Sun</p>
    <p>UltraSparc</p>
    <p>Graphics Processors &amp; Gaming  NVIDIA GTX280, STI</p>
    <p>Cell</p>
    <p>Embedded  Intel Atom, ARM (cell</p>
    <p>phone, etc.)</p>
    <p>Picochip DSP 1 GPP core 248 ASPs</p>
    <p>Cisco CRS-1 188 Tensilica GPPs</p>
    <p>Sun Niagara 8 GPP cores (32 threads)</p>
    <p>Intel</p>
    <p>XScale</p>
    <p>Core 32K IC</p>
    <p>MEv 2</p>
    <p>MEv 2</p>
    <p>MEv 2</p>
    <p>MEv</p>
    <p>MEv</p>
    <p>MEv</p>
    <p>Rbuf</p>
    <p>Tbuf</p>
    <p>Hash</p>
    <p>h 16KB</p>
    <p>QDR</p>
    <p>SRAM</p>
    <p>QDR</p>
    <p>SRAM</p>
    <p>RDRA</p>
    <p>M 1</p>
    <p>RDRA</p>
    <p>M 3</p>
    <p>RDRA</p>
    <p>M 2</p>
    <p>G</p>
    <p>A S</p>
    <p>K</p>
    <p>E</p>
    <p>T</p>
    <p>PCI</p>
    <p>(64b)</p>
    <p>MHz</p>
    <p>IXP280IXP280</p>
    <p>S</p>
    <p>P</p>
    <p>I</p>
    <p>r</p>
    <p>C</p>
    <p>S</p>
    <p>I X</p>
    <p>Stripe</p>
    <p>E/D Q E/D Q</p>
    <p>QDR</p>
    <p>SRAM</p>
    <p>MEv 2</p>
    <p>MEv</p>
    <p>MEv 2</p>
    <p>MEv 2</p>
    <p>MEv 2</p>
    <p>MEv</p>
    <p>MEv</p>
    <p>MEv</p>
    <p>MEv 2</p>
    <p>MEv</p>
    <p>CSRs</p>
    <p>-Fast_wr</p>
    <p>-UART</p>
    <p>-Timers</p>
    <p>-GPIO</p>
    <p>BootROM/S lowPort</p>
    <p>QDR</p>
    <p>SRAM</p>
    <p>Intel Network Processor 1 GPP Core</p>
    <p>STI Cell</p>
  </div>
  <div class="page">
    <p>Auto-tuning</p>
    <p>Problem: want to obtain and compare best potential performance of diverse architectures, avoiding  Non-portable code</p>
    <p>Labor-intensive user optimizations for each specific architecture</p>
    <p>A Solution: Auto-tuning  Automate search across a</p>
    <p>complex optimization space</p>
    <p>Achieve performance far beyond current compilers</p>
    <p>Achieve performance portability for diverse architectures Reference</p>
    <p>Best: 4x2</p>
    <p>Mflop/s</p>
    <p>Mflop/s</p>
    <p>For finite element problem (BCSR)</p>
    <p>[Im, Yelick, Vuduc, 2005]</p>
  </div>
  <div class="page">
    <p>Maximizing</p>
    <p>Memory Bandwidth</p>
    <p>Exploit NUMA</p>
    <p>Hide memory latency</p>
    <p>Satisfy Littles Law</p>
    <p>?memoryaffinity ?SWprefetch ?DMAlists</p>
    <p>?unit-stridestreams</p>
    <p>?TLBblocking</p>
    <p>Optimization Categorization</p>
    <p>Minimizing</p>
    <p>Memory Traffic</p>
    <p>Eliminate:</p>
    <p>Capacity misses</p>
    <p>Conflict misses</p>
    <p>Compulsory misses</p>
    <p>Write allocate behavior</p>
    <p>?cacheblocking?arraypadding ?compressdata ?</p>
    <p>streaming</p>
    <p>stores</p>
    <p>Maximizing</p>
    <p>In-core Performance</p>
    <p>Exploit in-core parallelism</p>
    <p>(ILP, DLP, etc)</p>
    <p>Good (enough)</p>
    <p>floating-point balance</p>
    <p>?unroll &amp;jam ?explicitSIMD ?reorder</p>
    <p>?eliminatebranches</p>
  </div>
  <div class="page">
    <p>Optimization Categorization</p>
    <p>Maximizing</p>
    <p>In-core Performance</p>
    <p>Minimizing</p>
    <p>Memory Traffic</p>
    <p>Exploit in-core parallelism</p>
    <p>(ILP, DLP, etc)</p>
    <p>Good (enough)</p>
    <p>floating-point balance</p>
    <p>?unroll &amp;jam ?explicitSIMD ?reorder</p>
    <p>?eliminatebranches</p>
    <p>Eliminate:</p>
    <p>Capacity misses</p>
    <p>Conflict misses</p>
    <p>Compulsory misses</p>
    <p>Write allocate behavior</p>
    <p>?cacheblocking?arraypadding ?compressdata ?</p>
    <p>streaming</p>
    <p>stores</p>
    <p>Maximizing</p>
    <p>Memory Bandwidth</p>
    <p>Exploit NUMA</p>
    <p>Hide memory latency</p>
    <p>Satisfy Littles Law</p>
    <p>?memoryaffinity ?SWprefetch ?DMAlists</p>
    <p>?unit-stridestreams</p>
    <p>?TLBblocking</p>
  </div>
  <div class="page">
    <p>Optimization Categorization</p>
    <p>Maximizing</p>
    <p>In-core Performance</p>
    <p>Maximizing</p>
    <p>Memory Bandwidth</p>
    <p>Exploit in-core parallelism</p>
    <p>(ILP, DLP, etc)</p>
    <p>Good (enough)</p>
    <p>floating-point balance</p>
    <p>?unroll &amp;jam ?explicitSIMD ?reorder</p>
    <p>?eliminatebranches</p>
    <p>Exploit NUMA</p>
    <p>Hide memory latency</p>
    <p>Satisfy Littles Law</p>
    <p>?memoryaffinity ?SWprefetch ?DMAlists</p>
    <p>?unit-stridestreams</p>
    <p>?TLBblocking</p>
    <p>Minimizing</p>
    <p>Memory Traffic</p>
    <p>Eliminate:</p>
    <p>Capacity misses</p>
    <p>Conflict misses</p>
    <p>Compulsory misses</p>
    <p>Write allocate behavior</p>
    <p>?cacheblocking?arraypadding ?compressdata ?</p>
    <p>streaming</p>
    <p>stores</p>
  </div>
  <div class="page">
    <p>Optimization Categorization</p>
    <p>Maximizing</p>
    <p>In-core Performance</p>
    <p>Minimizing</p>
    <p>Memory Traffic</p>
    <p>Maximizing</p>
    <p>Memory Bandwidth</p>
    <p>Exploit in-core parallelism</p>
    <p>(ILP, DLP, etc)</p>
    <p>Good (enough)</p>
    <p>floating-point balance</p>
    <p>?unroll &amp;jam ?explicitSIMD ?reorder</p>
    <p>?eliminatebranches</p>
    <p>Exploit NUMA</p>
    <p>Hide memory latency</p>
    <p>Satisfy Littles Law</p>
    <p>?memoryaffinity ?SWprefetch ?DMAlists</p>
    <p>?unit-stridestreams</p>
    <p>?TLBblocking</p>
    <p>Eliminate:</p>
    <p>Capacity misses</p>
    <p>Conflict misses</p>
    <p>Compulsory misses</p>
    <p>Write allocate behavior</p>
    <p>?cacheblocking?arraypadding ?compressdata ?</p>
    <p>streaming</p>
    <p>stores</p>
  </div>
  <div class="page">
    <p>Optimization Categorization</p>
    <p>Maximizing</p>
    <p>In-core Performance</p>
    <p>Minimizing</p>
    <p>Memory Traffic</p>
    <p>Maximizing</p>
    <p>Memory Bandwidth</p>
    <p>Exploit in-core parallelism</p>
    <p>(ILP, DLP, etc)</p>
    <p>Good (enough)</p>
    <p>floating-point balance</p>
    <p>?unroll &amp;jam ?explicitSIMD ?reorder</p>
    <p>?eliminatebranches</p>
    <p>Exploit NUMA</p>
    <p>Hide memory latency</p>
    <p>Satisfy Littles Law</p>
    <p>?memoryaffinity ?SWprefetch ?DMAlists</p>
    <p>?unit-stridestreams</p>
    <p>?TLBblocking</p>
    <p>Eliminate:</p>
    <p>Capacity misses</p>
    <p>Conflict misses</p>
    <p>Compulsory misses</p>
    <p>Write allocate behavior</p>
    <p>?cacheblocking?arraypadding ?compressdata ?</p>
    <p>streaming</p>
    <p>stores</p>
    <p>Eac h op</p>
    <p>timi zati</p>
    <p>on h as</p>
    <p>a la rge</p>
    <p>para met</p>
    <p>er s pac</p>
    <p>e</p>
    <p>Wha t are</p>
    <p>the opt</p>
    <p>ima l pa</p>
    <p>ram eter</p>
    <p>s?</p>
  </div>
  <div class="page">
    <p>Traversing the Parameter Space</p>
    <p>Opt. #1 Parameters</p>
    <p>O p</p>
    <p>t. #</p>
    <p>a ra</p>
    <p>m e</p>
    <p>te rs</p>
    <p>O pt</p>
    <p>. # 3</p>
    <p>P ar</p>
    <p>am et</p>
    <p>er s</p>
    <p>Exhaustive search of these complex layered</p>
    <p>optimizations is impossible</p>
    <p>To make problem tractable, we:</p>
    <p>order the optimizations</p>
    <p>applied them consecutively</p>
    <p>Every platform had its own set of best parameters</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 10</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 11</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>Chip</p>
    <p>MultiThreaded</p>
    <p>(CMT)</p>
    <p>x86</p>
    <p>Superscalar</p>
    <p>x86</p>
    <p>Superscalar/</p>
    <p>CMT</p>
    <p>PPC</p>
    <p>Dual-issue</p>
    <p>in-order IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 12</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 13</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 14</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Multicore Architectures</p>
    <p>Intel Nehalem (Gainestown) Intel Clovertown</p>
    <p>Sun Niagara2 (Victoria Falls) 15</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>Core</p>
    <p>FSB</p>
    <p>Core Core Core</p>
    <p>shared L2</p>
    <p>shared L2 4MB</p>
    <p>shared L2</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>shared L2</p>
    <p>Core Core</p>
    <p>Chipset (2x128b controllers)</p>
    <p>IBM PPC 450</p>
    <p>(BG/P)</p>
  </div>
  <div class="page">
    <p>Stencil Code Overview</p>
    <p>For a given point, a stencil is a fixed subset of nearest neighbors</p>
    <p>A stencil code updates every point in a regular grid by applying a stencil</p>
    <p>Used in iterative PDE solvers like Jacobi, Multigrid, and AMR</p>
    <p>Focus on a out-of-place 3D 27point stencil sweeping over a 2563 grid</p>
    <p>Problem size &gt; Cache size</p>
    <p>Stencil codes characteristics</p>
    <p>Long unit-stride memory accesses</p>
    <p>Some reuse of each grid point</p>
    <p>30 flops per grid point</p>
    <p>Arithmetic Intensity 0.75-1.88 Adaptive Mesh Refinement (AMR)</p>
  </div>
  <div class="page">
    <p>Nave Stencil Code</p>
    <p>We wish to exploit multicore resources</p>
    <p>Simple parallel stencil code:</p>
    <p>Use pthreads</p>
    <p>Parallelize in least contiguous grid dimension</p>
    <p>Thread affinity for scaling: multithreading, then multicore,</p>
    <p>then multisocket</p>
    <p>x</p>
    <p>y</p>
    <p>z (unit-stride)</p>
    <p>Thread 0</p>
    <p>Thread 1</p>
    <p>Thread n</p>
  </div>
  <div class="page">
    <p>Nave Performance</p>
    <p>Compiler delivers</p>
    <p>poor performance</p>
    <p>icc for Intel</p>
    <p>gcc for VF</p>
    <p>xlc for BG/P</p>
    <p>No parallel scaling</p>
    <p>for two architectures</p>
    <p>Low performance as</p>
    <p>compared with</p>
    <p>stream bandwidth</p>
    <p>prediction</p>
    <p>Reasonably high</p>
    <p>AI means that</p>
    <p>other bottlenecks</p>
    <p>likely exist</p>
  </div>
  <div class="page">
    <p>NUMA Optimization</p>
    <p>! All DRAMs are highlighted in red</p>
    <p>! Co-located data on same socket as</p>
    <p>thread processing it</p>
  </div>
  <div class="page">
    <p>Array Padding Optimization</p>
    <p>Conflict misses may occur on low-associativity</p>
    <p>caches</p>
    <p>Each array was padded by a tuned amount to</p>
    <p>minimize conflicts</p>
    <p>x</p>
    <p>y</p>
    <p>z (unit-stride)</p>
    <p>Thread 0</p>
    <p>Thread 1</p>
    <p>Thread n</p>
    <p>pa dd</p>
    <p>in g</p>
  </div>
  <div class="page">
    <p>Performance</p>
    <p>+ Array Padding</p>
    <p>+ NUMA</p>
    <p>Naive</p>
  </div>
  <div class="page">
    <p>Problem Decomposition</p>
    <p>+Y</p>
    <p>+Z</p>
    <p>Decomposition of the Grid</p>
    <p>into a Chunk of Core Blocks</p>
    <p>+X</p>
    <p>(unit stride)NY N</p>
    <p>Z</p>
    <p>NX</p>
    <p>Large chunks enable</p>
    <p>efficient NUMA Allocation</p>
    <p>Small chunks exploit LLC</p>
    <p>shared caches</p>
    <p>Decomposition into</p>
    <p>Thread Blocks</p>
    <p>CY</p>
    <p>C Z</p>
    <p>CX</p>
    <p>TYTX</p>
    <p>Exploit caches shared</p>
    <p>among threads within a</p>
    <p>core</p>
    <p>Decomposition into</p>
    <p>Register Blocks</p>
    <p>RY</p>
    <p>TY</p>
    <p>C Z</p>
    <p>TX</p>
    <p>RX RZ</p>
    <p>Make DLP/ILP explicit</p>
    <p>Make register reuse</p>
    <p>explicit</p>
    <p>This decomposition is universal across all examined architectures</p>
    <p>Decomposition does not change data structure</p>
    <p>Need to choose best block sizes for each hierarchy level</p>
  </div>
  <div class="page">
    <p>Performance</p>
    <p>+ Thread Blocking</p>
    <p>+ Register Blocking</p>
    <p>+ Core Blocking</p>
    <p>+ Array Padding</p>
    <p>+ NUMA</p>
    <p>Naive</p>
  </div>
  <div class="page">
    <p>ISA Specific Optimizations</p>
    <p>Software prefetch</p>
    <p>Explicit SIMD  PPC SIMD loads do</p>
    <p>not improve performance due to unaligned data</p>
    <p>Cache Bypass  Initial values in write</p>
    <p>array not used</p>
    <p>Eliminate write array cache fills with intrinsics</p>
    <p>Reduces memory traffic from 24 B/point to 16 B/point</p>
    <p>Write</p>
    <p>Array</p>
    <p>DRAM</p>
    <p>Read</p>
    <p>ArrayChip</p>
  </div>
  <div class="page">
    <p>Performance</p>
    <p>+ Cache Bypass</p>
    <p>+ SIMD</p>
    <p>+ Thread Blocking</p>
    <p>+ Software Prefetch</p>
    <p>+ Register Blocking</p>
    <p>+ Core Blocking</p>
    <p>+ Array Padding</p>
    <p>+ NUMA</p>
    <p>Naive</p>
    <p>Optimizations effect</p>
    <p>architectures in different</p>
    <p>ways</p>
  </div>
  <div class="page">
    <p>Common Subexpression</p>
    <p>Elimination Optimization</p>
    <p>Common computation exists between</p>
    <p>different stencil updates</p>
    <p>Compiler does not recognize this</p>
    <p>Reduce number of flops from 30 to 18</p>
  </div>
  <div class="page">
    <p>CSE Version Performance</p>
    <p>+ Cache Bypass</p>
    <p>+ SIMD</p>
    <p>+ Thread Blocking</p>
    <p>+ Software Prefetch</p>
    <p>+ Register Blocking</p>
    <p>+ Core Blocking</p>
    <p>+ Array Padding</p>
    <p>+ NUMA</p>
    <p>Naive</p>
    <p>+ CSE</p>
  </div>
  <div class="page">
    <p>Is Performance Acceptable?</p>
    <p>A model (e.g. Roofline) could be used to</p>
    <p>predict best performance</p>
    <p>Use a two-pass greedy algorithm</p>
  </div>
  <div class="page">
    <p>Second Pass Performance</p>
    <p>+ Cache Bypass</p>
    <p>+ SIMD</p>
    <p>+ Thread Blocking</p>
    <p>+ Software Prefetch</p>
    <p>+ Register Blocking</p>
    <p>+ Core Blocking</p>
    <p>+ Array Padding</p>
    <p>+ NUMA</p>
    <p>Naive</p>
    <p>+ CSE</p>
    <p>+ Second Pass</p>
  </div>
  <div class="page">
    <p>Tuning Speedup</p>
    <p>Speedup at maximum</p>
    <p>concurrency</p>
  </div>
  <div class="page">
    <p>Parallel Speedup</p>
    <p>Speedup going from a single</p>
    <p>core to maximum</p>
    <p>concurrency</p>
    <p>All architectures now scale</p>
  </div>
  <div class="page">
    <p>Effect of compilers</p>
    <p>icc is consistently</p>
    <p>better than gcc</p>
    <p>For single socket gcc +</p>
    <p>register blocking has</p>
    <p>equivalent performance</p>
    <p>to icc</p>
    <p>Core blocking improves</p>
    <p>icc performance, but</p>
    <p>not gcc</p>
    <p>Inferior code</p>
    <p>generation hides</p>
    <p>memory bottleneck?</p>
  </div>
  <div class="page">
    <p>Performance Comparison</p>
    <p>Intel Nehalem best</p>
    <p>in absolute</p>
    <p>performance</p>
    <p>Normalize for low</p>
    <p>power, BG/P</p>
    <p>solution is much</p>
    <p>more attractive</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Compiler alone achieves poor performance</p>
    <p>Low fraction possible performance</p>
    <p>Often no parallel scaling</p>
    <p>Autotuning is essential to achieving good</p>
    <p>performance</p>
    <p>1.8x-3.6x speedups across diverse architectures</p>
    <p>Automatic tuning is necessary for scalability</p>
    <p>Most optimization with the same code base</p>
    <p>Clovertown required SIMD (hampers productivity) for</p>
    <p>best performance</p>
    <p>When power consumption is taken into account,</p>
    <p>BG/P performs well</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
    <p>UC Berkeley</p>
    <p>RADLab Cluster (Nehalem)</p>
    <p>PSI cluster(Clovertown)</p>
    <p>Sun Microsystems</p>
    <p>Niagara2 donations</p>
    <p>ASCR Office in the DOE Office of Science</p>
    <p>contract DE-AC02-05CH11231</p>
  </div>
</Presentation>
