<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Copyright  2009 VMware, Inc. All rights reserved.</p>
    <p>PARDA: Proportional Allocation of Resources for Distributed Storage Access</p>
    <p>Ajay Gulati, Irfan Ahmad, Carl Waldspurger</p>
    <p>Resource Management Team</p>
    <p>VMware Inc.</p>
    <p>USENIX FAST 09 Conference  February 26, 2009</p>
  </div>
  <div class="page">
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>The Problem</p>
    <p>Storage Array LUN</p>
  </div>
  <div class="page">
    <p>What you see</p>
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>The Problem</p>
    <p>Storage Array LUN</p>
  </div>
  <div class="page">
    <p>What you see</p>
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>The Problem</p>
    <p>online store</p>
    <p>Data Mining (low priority)</p>
    <p>What you want to see</p>
    <p>Storage Array LUNStorage Array LUN</p>
  </div>
  <div class="page">
    <p>Distributed Storage Access</p>
    <p>VMs running across multiple hosts</p>
    <p>Hosts share LUNs using a cluster filesystem</p>
    <p>No central control on IO path</p>
    <p>Issues</p>
    <p>Hosts adversely affect each other</p>
    <p>Difficult to respect per-VM allocations</p>
    <p>Proportional shares (aka weights)</p>
    <p>Specify relative priority</p>
    <p>Goal: Provide isolation while maximizing array utilization</p>
  </div>
  <div class="page">
    <p>Host-local Scheduling Inadequate</p>
    <p>Local SFQ schedulers respect VM shares</p>
    <p>BUT not across hosts</p>
    <p>OLTP OLTP OLTP OLTP Iomtr Iomtr</p>
    <p>VM Shares</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Hosts</p>
    <p>Host Shares</p>
  </div>
  <div class="page">
    <p>Host-local Scheduling Inadequate</p>
    <p>Hosts get equal IOPS</p>
    <p>IOPS dependent on VM placement!</p>
    <p>Local SFQ schedulers respect VM shares</p>
    <p>BUT not across hosts</p>
    <p>OLTP OLTP OLTP OLTP Iomtr Iomtr</p>
    <p>VM Shares</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>Hosts</p>
    <p>Host Shares</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem Context</p>
    <p>Analogy to Network Flow Control</p>
    <p>Control Mechanism</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Analogy to Network Flow Control</p>
    <p>Network is a black box Array is a black box</p>
    <p>Network congestion detected using RTT, packet loss</p>
    <p>Estimate array congestion using IO latency</p>
    <p>Packet loss very unlikely</p>
    <p>TCP adapts window size Adapt number of pending IO</p>
    <p>requests (a.k.a. window size)</p>
    <p>TCP ensures fair sharing</p>
    <p>FAST TCP* proportional sharing</p>
    <p>Adapt window size based on</p>
    <p>shares/weights</p>
    <p>Networks Storage</p>
    <p>* Low et. al. FAST TCP: Motivation, Architecture, Algorithms, Performance. Proc. IEEE INFOCOM 04</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem Context</p>
    <p>Analogy to Network Flow Control</p>
    <p>Control Mechanism</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>PARDA Architecture</p>
    <p>SFQ</p>
    <p>SFQ</p>
    <p>SFQ</p>
    <p>Host-Level</p>
    <p>Issue Queues</p>
    <p>Storage Array</p>
    <p>Array Queue</p>
    <p>Queue lengths varied dynamically</p>
  </div>
  <div class="page">
    <p>Per-Host Control Algorithm</p>
    <p>Adjust window size w(t) using cluster-wide average latency L(t)</p>
    <p>LLLL : latency threshold, operating point for IO latency</p>
    <p>:proportional to aggregate VM shares for host  : smoothing parameter between 0 and 1</p>
    <p>Motivated by FAST TCP mechanism</p>
    <p>++=+  )(</p>
    <p>)( )()1()1( tw</p>
    <p>tL twtw</p>
    <p>L</p>
  </div>
  <div class="page">
    <p>Control Algorithm Features</p>
    <p>Maintain high utilization at the array</p>
    <p>Overall array queue proportional to Throughput x L</p>
    <p>Ability to allocate queue size in proportion to hosts shares</p>
    <p>At equilibrium, host window sizes are proportional to  Ability to control overall latency of a cluster</p>
    <p>Cluster operates close to Lor below</p>
    <p>++=+  )(</p>
    <p>)( )()1()1( tw</p>
    <p>tL twtw</p>
    <p>L</p>
  </div>
  <div class="page">
    <p>Unit of Allocation</p>
    <p>Two main units exist in literature</p>
    <p>Bandwidth (MB/s)</p>
    <p>Throughput (IOPS )</p>
    <p>Both have problems</p>
    <p>Using bandwidth may hurt workloads with large IO sizes</p>
    <p>Using IOPS may hurt VMs with sequential IOs</p>
    <p>PARDA: allocate queue slots at array</p>
    <p>Carves out array queue among VMs</p>
    <p>Workloads can recycle queue slots faster or slower</p>
    <p>Maintains high efficiency</p>
  </div>
  <div class="page">
    <p>Storage-Specific Issues</p>
    <p>Issues</p>
    <p>Throughput varies by 10x depending on workload characteristics</p>
    <p>IO sizes may vary by 1000x (512B  512KB)</p>
    <p>Array complexity: caching, different paths for read vs. write</p>
    <p>Hosts may observe different latencies</p>
    <p>PARDA Solutions</p>
    <p>Latency normalization for large IO sizes</p>
    <p>Compute cluster-wide average latency using a shared file</p>
  </div>
  <div class="page">
    <p>Handling BurstsTwo Time Scales</p>
    <p>VM 1</p>
    <p>VM 2</p>
    <p>VM 3</p>
    <p>VM 4</p>
    <p>PARDA Slots Assignment</p>
    <p>All VMs active</p>
  </div>
  <div class="page">
    <p>Handling BurstsTwo Time Scales</p>
    <p>Workload variation over short time periods</p>
    <p>Handled by existing local SFQ scheduler</p>
    <p>No strict partitioning of host-level queue</p>
    <p>VM 1</p>
    <p>VM 2</p>
    <p>VM 3</p>
    <p>VM 4</p>
    <p>PARDA Slots Assignment</p>
    <p>VM2 is idling</p>
    <p>Other VMs take advantage</p>
  </div>
  <div class="page">
    <p>Handling BurstsTwo Time Scales</p>
    <p>Workload variation over short time periods</p>
    <p>Handled by existing local SFQ scheduler</p>
    <p>No strict partitioning of host-level queue</p>
    <p>VM idleness over longer term</p>
    <p>Re-compute  per host based on VM activity  Effectively scale VM shares based on its utilization</p>
    <p>Utilization computed as (# outstanding IOs) / (VM window size)</p>
    <p>VM 1</p>
    <p>VM 2</p>
    <p>VM 3</p>
    <p>VM 4</p>
    <p>PARDA Slots Assignment</p>
    <p>VM2 is idling</p>
    <p>Other VMs take advantage</p>
    <p>Long-term, scale shares</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem Context</p>
    <p>Analogy to Network Flow Control</p>
    <p>Control Mechanism</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>VMware Cluster</p>
    <p>Each host: 2 Intel Xeon dual cores, 8 GB RAM</p>
    <p>FC-SAN attached Storage</p>
    <p>EMC CLARiiON CX3-40 storage array</p>
    <p>Similar results on NetApp FAS6030</p>
    <p>Two volumes used</p>
  </div>
  <div class="page">
    <p>PARDA: Proportional Sharing</p>
    <p>Setup:</p>
    <p>4 Hosts, shares  1 : 1 : 2 : 2</p>
    <p>Latency threshold L = 25ms</p>
    <p>Workload  16KB, 100% reads, 100% random IO</p>
    <p>Aggregate IOPS with/without PARDA (3400 vs 3360)</p>
    <p>Window sizes are in proportion to shares</p>
    <p>Latency close to 25 ms IOPS match shares but affected by other factors</p>
  </div>
  <div class="page">
    <p>PARDA: Dynamic Load Adaptation</p>
    <p>Setup:</p>
    <p>6 Hosts, equal shares, uniform workload</p>
    <p>Latency threshold L = 30ms</p>
    <p>Three VMs are stopped at 145, 220 and 310 sec</p>
    <p>Aggregate IOPS with/without PARDA (3090 vs 3155)</p>
    <p>PARDA adapts to load Latency close to 30 ms IOPS increase with increase in window size</p>
  </div>
  <div class="page">
    <p>PARDA: End-to-End Control</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (I</p>
    <p>O P</p>
    <p>S )</p>
    <p>With PARDA:</p>
    <p>Shares are respected</p>
    <p>independent of VM placement</p>
    <p>Hosts</p>
    <p>Setup:</p>
    <p>Latency threshold L = 25ms 20 10 10 10 20 10</p>
    <p>OLTP OLTP OLTP OLTP Iomtr Iomtr</p>
    <p>VM Shares</p>
    <p>Host Shares</p>
  </div>
  <div class="page">
    <p>PARDA: Handling Bursts</p>
    <p>Setup:  One VM idles from 140 sec to 310 sec</p>
    <p>Result:</p>
    <p>PARDA is able to adjust  values at host</p>
    <p>No undue advantage given to VMs sharing the host with idle VM</p>
  </div>
  <div class="page">
    <p>PARDA: SQL Workloads</p>
    <p>Setup:</p>
    <p>2 Hosts, 2 Windows VMs running SQL server (250 GB data disk, 50 GB log disk)</p>
    <p>Shares 1 : 4</p>
    <p>Latency threshold L = 15ms</p>
    <p>With PARDA:  Shares are respected across hosts</p>
    <p>Host 1,2 with shares 1:4 get 6952 and 12356 OPM</p>
    <p>Without PARDA: No Fairness Across hosts</p>
    <p>Both hosts get ~600 IOPS</p>
  </div>
  <div class="page">
    <p>PARDA: Burst Handling</p>
    <p>Result:</p>
    <p>PARDA adjusts  values at host with the change in pending IO count</p>
    <p>VM2 with high shares is unable to fill its window</p>
    <p>IOPS and OPM are differentiated</p>
    <p>based on  values</p>
    <p>Latency</p>
    <p>Window</p>
    <p>size</p>
    <p>Beta</p>
    <p>values</p>
  </div>
  <div class="page">
    <p>Evaluation Recap</p>
    <p>Effectiveness of PARDA mechanism</p>
    <p>Fairness</p>
    <p>Load or throughput variations</p>
    <p>Burst handling</p>
    <p>End-to-end control</p>
    <p>Enterprise workloads</p>
    <p>Evaluation of control parameters (without PARDA)</p>
    <p>Latency variation with workload</p>
    <p>Latency variation with overall load</p>
    <p>Queue length as control mechanism for fairness</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Problem Context</p>
    <p>Analogy to Network Flow Control</p>
    <p>Control Mechanism</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>PARDA contributions</p>
    <p>Efficient distributed IO resource management</p>
    <p>without any support from storage array</p>
    <p>Fair end-to-end VM allocation, proportional to VM shares</p>
    <p>Control on overall cluster latency</p>
    <p>Future work</p>
    <p>Latency threshold estimation or adaptation?</p>
    <p>Detect and handle uncontrolled (non-PARDA) hosts</p>
    <p>NFS adaptation</p>
  </div>
  <div class="page">
    <p>Questions ...</p>
    <p>{agulati,irfan,carl}@vmware.com</p>
    <p>Storage in Virtualization BoF tonight @7:30 pm</p>
  </div>
</Presentation>
