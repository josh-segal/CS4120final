<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Primary Data Deduplication  Large</p>
    <p>Scale Study and System Design</p>
    <p>A. El-Shimi, R. Kalach, A. Kumar, J. Li, A. Oltean, S. Sengupta</p>
    <p>Microsoft Corporation, Redmond (USA)</p>
  </div>
  <div class="page">
    <p>Primary Data Deduplication for</p>
    <p>File-based Storage</p>
    <p>Relatively recent interest vs. backup data dedup</p>
    <p>Driving forces</p>
    <p>50% year-over-year growth in file based data</p>
    <p>#1 technology feature when choosing a storage solution</p>
    <p>Technology challenges</p>
    <p>Continue to serve primary workload from same copy of data</p>
    <p>Balance resource consumption (CPU/memory/disk I/O), dedup</p>
    <p>space savings, and dedup throughput</p>
    <p>Slide 3</p>
  </div>
  <div class="page">
    <p>Key Requirements for</p>
    <p>Primary Data Deduplication</p>
    <p>Optimize for unique data</p>
    <p>More than 50% of data could be unique (vs. 90+% duplication rates</p>
    <p>in backup data)</p>
    <p>Primary workload friendly</p>
    <p>Maintain efficient access to data (both sequential and random I/O)</p>
    <p>Deduplication cannot assume dedicated resources and must yield</p>
    <p>to primary workload</p>
    <p>Broadly used platform</p>
    <p>Must run well on a low-end server</p>
    <p>Huge variability in workloads and hardware platforms</p>
    <p>Slide 4</p>
  </div>
  <div class="page">
    <p>Key Design Decisions</p>
    <p>Post-processing deduplication</p>
    <p>Preserve latency/throughput of primary data access</p>
    <p>Flexibility in scheduling dedup as background job on cold data</p>
    <p>Deduplication granularity and data chunking</p>
    <p>Chunk-level: variable sized chunking, large chunk size (~80KB)</p>
    <p>Modifications to Rabin fingerprint based chunking to achieve more</p>
    <p>uniform chunk size distribution</p>
    <p>Deduplication resource usage scaling slowly with data size</p>
    <p>Reduced chunk metadata</p>
    <p>RAM frugal chunk hash index</p>
    <p>Data partitioning</p>
    <p>Slide 5</p>
  </div>
  <div class="page">
    <p>Large Scale Study of Primary Datasets</p>
    <p>Used to drive key design</p>
    <p>decisions</p>
    <p>About 7TB of data spread</p>
    <p>across 15 globally distributed</p>
    <p>servers in a large enterprise</p>
    <p>Data crawled and chunked at</p>
    <p>different average chunk sizes</p>
    <p>Using Rabin fingerprint based</p>
    <p>variable sized chunker</p>
    <p>SHA-1 hash, size, compressed</p>
    <p>size, offset in file, file information</p>
    <p>logged for each chunk</p>
    <p>Slide 6</p>
  </div>
  <div class="page">
    <p>Key Design Decisions</p>
    <p>Post-processing deduplication</p>
    <p>Preserve latency/throughput of primary data access</p>
    <p>Flexibility in scheduling dedup as background job on cold data</p>
    <p>Deduplication granularity and data chunking</p>
    <p>Chunk-level: variable sized chunking, large chunk size (~80KB)</p>
    <p>Modifications to Rabin fingerprint based chunking to achieve more</p>
    <p>uniform chunk size distribution</p>
    <p>Deduplication resource usage scaling slowly with data size</p>
    <p>Reduced chunk metadata</p>
    <p>RAM frugal chunk hash index</p>
    <p>Data partitioning</p>
    <p>Slide 7</p>
  </div>
  <div class="page">
    <p>Average Chunk Size</p>
    <p>Compression compensates</p>
    <p>for savings decrease with</p>
    <p>higher chunk size</p>
    <p>Compression is more efficient</p>
    <p>on larger chunks</p>
    <p>Use larger chunk size of</p>
    <p>~64KB</p>
    <p>Without sacrificing dedup</p>
    <p>savings</p>
    <p>Reduce chunk metadata in</p>
    <p>the system</p>
    <p>Slide 9</p>
    <p>Dedup savings loss</p>
    <p>w/o compression</p>
    <p>Dedup savings preserved</p>
    <p>w/ compression</p>
    <p>GFS-US dataset</p>
  </div>
  <div class="page">
    <p>Chunk Reference Count</p>
    <p>Majority of duplicate bytes reside in middle portion of distribution</p>
    <p>Not sufficient to dedup just high ref count chunks</p>
    <p>System needs to deduplicate all chunks that appear more than once</p>
    <p>Implications on the chunk hash index design</p>
    <p>Slide 10</p>
    <p>GFS-Japan-1 dataset</p>
  </div>
  <div class="page">
    <p>Basic version of fingerprint based</p>
    <p>chunking</p>
    <p>Skewed chunk size</p>
    <p>distribution</p>
    <p>Small chunk size =&gt;</p>
    <p>increase in chunk</p>
    <p>metadata in the system</p>
    <p>Large chunks =&gt; reduced</p>
    <p>dedup savings, benefit of</p>
    <p>caching</p>
    <p>Slide 11</p>
    <p>x</p>
    <p>x</p>
    <p>F re</p>
    <p>q u</p>
    <p>e n</p>
    <p>c y d</p>
    <p>is tr</p>
    <p>ib u</p>
    <p>ti o</p>
    <p>n</p>
    <p>Chunk size min max</p>
    <p>Forced chunk boundaries</p>
    <p>Forced boundary at max chunk size is content independent, hence</p>
    <p>may reduce dedup savings</p>
  </div>
  <div class="page">
    <p>Regression Chunking Algorithm</p>
    <p>Goal 1: To obtain uniform chunk size distribution</p>
    <p>Goal 2: Reduce forced chunk boundaries at max size</p>
    <p>Basic idea</p>
    <p>When max chunk size is reached, relax match condition to some</p>
    <p>suffix of bit pattern P</p>
    <p>Match |P| - i bits of P, with decreasing priority for i=0,1, , k</p>
    <p>Reduces probability of forced boundary at max size</p>
    <p>2x10-3 for k=1, 10-14 for k=4</p>
    <p>Slide 12</p>
  </div>
  <div class="page">
    <p>Regression Chunking Algorithm contd.</p>
    <p>Maintains chunking throughput performance</p>
    <p>Core matching loop checks against smallest prefix, break out only if</p>
    <p>match occurs</p>
    <p>Single pass over data: remember match position for each relaxed</p>
    <p>suffix match</p>
    <p>Slide 13</p>
  </div>
  <div class="page">
    <p>Regression Chunking Performance</p>
    <p>Slide 14</p>
    <p>Uniform chunk size distribution</p>
    <p>GFS-US dataset</p>
    <p>Dedup savings improvement</p>
  </div>
  <div class="page">
    <p>Key Design Decisions</p>
    <p>Post-processing deduplication</p>
    <p>Preserve latency/throughput of primary data access</p>
    <p>Flexibility in scheduling dedup as background job on cold data</p>
    <p>Deduplication granularity and data chunking</p>
    <p>Chunk-level: variable sized chunking, large chunk size (~80KB)</p>
    <p>Modifications to Rabin fingerprint based chunking to achieve more</p>
    <p>uniform chunk size distribution</p>
    <p>Deduplication resource usage scaling slowly with data size</p>
    <p>Reduced chunk metadata</p>
    <p>RAM frugal chunk hash index</p>
    <p>Data partitioning</p>
    <p>Slide 15</p>
  </div>
  <div class="page">
    <p>Chunk Indexing</p>
    <p>Log-structured organization</p>
    <p>Chunk metadata organized in</p>
    <p>log-structured manner on disk</p>
    <p>Insertions aggregated in write</p>
    <p>buffer in RAM and appended to</p>
    <p>log in single I/O</p>
    <p>Low RAM footprint index</p>
    <p>Specialized hash table using</p>
    <p>variant of cuckoo hashing</p>
    <p>2-byte signature, 4-byte pointer</p>
    <p>per entry =&gt; 6-bytes of RAM</p>
    <p>per indexed chunk</p>
    <p>Slide 16</p>
  </div>
  <div class="page">
    <p>Chunk Indexing contd.</p>
    <p>Prefetch Cache</p>
    <p>Prefetch chunk mappings for next 100-1000 chunks in same I/O</p>
    <p>Exploit sequential predictability of chunk hash lookups</p>
    <p>Locality expected to be less than in backup workloads</p>
    <p>Prefetch cache sized at 100,000 entries (5MB of RAM)</p>
    <p>About 1% of index lookups hitting disk (on all datasets evaluated)</p>
    <p>Hash table acts as a bloom filter on new chunk lookups</p>
    <p>Slide 17</p>
  </div>
  <div class="page">
    <p>Data Partitioning and Reconciliation</p>
    <p>Two-phase deduplication</p>
    <p>Divide the data into disjoint partitions, and perform deduplication</p>
    <p>within each partition</p>
    <p>Reconcile duplicates across partitions</p>
    <p>Reconciliation algorithm</p>
    <p>Iterative procedure</p>
    <p>Grow the set of reconciled partitions by considering some number</p>
    <p>of unreconciled partitions at a time</p>
    <p>Reconciliation Strategy</p>
    <p>Selective reconciliation</p>
    <p>Delayed reconciliation</p>
    <p>Slide 18</p>
  </div>
  <div class="page">
    <p>Reconciliation of Data Partitions</p>
    <p>Slide 19</p>
    <p>Reconciled partitions Unreconciled partitions (k)</p>
    <p>(indexed in RAM)</p>
    <p>Compare unreconciled partitions with</p>
    <p>each partition in reconciled set</p>
    <p>k = #unreconciled partitions considered per iteration; provides</p>
    <p>trade-off between memory usage and reconciliation speed</p>
  </div>
  <div class="page">
    <p>Efficient partitioning strategies</p>
    <p>Partition data and dedup within</p>
    <p>each partition</p>
    <p>How close is dedup savings within</p>
    <p>partitions to that of global dedup?</p>
    <p>Partitioning by file type</p>
    <p>Dedup savings almost as good as</p>
    <p>with global dedup</p>
    <p>Slide 20</p>
    <p>Partitioning by file path</p>
    <p>Partition by directory sub-trees (each partition  10% of total bytes)</p>
    <p>Not as effective as partitioning by file type for preserving dedup</p>
    <p>savings</p>
    <p>Partitioning by system/volume</p>
    <p>Dedup amenable to</p>
    <p>partitioned processing</p>
  </div>
  <div class="page">
    <p>System Overview</p>
    <p>Data path</p>
    <p>Dedup filter</p>
    <p>Chunk cache</p>
    <p>File stub tx update</p>
    <p>Deduplication pipeline</p>
    <p>Data chunking</p>
    <p>Index lookups + insertions</p>
    <p>Chunk Store insertions</p>
    <p>Background jobs</p>
    <p>Garbage collection (in Chunk Store)</p>
    <p>Data scrubbing</p>
    <p>Slide 21</p>
  </div>
  <div class="page">
    <p>Stream</p>
    <p>Map</p>
    <p>Data</p>
    <p>Chunk</p>
    <p>Phase I  Identify the duplicate data</p>
    <p>Phase II  Optimize the target files</p>
    <p>Deduplication and on-disk structures</p>
    <p>PA</p>
    <p>GE</p>
  </div>
  <div class="page">
    <p>Reduce latency for small</p>
    <p>writes to large files (e.g. OS</p>
    <p>image patching scenario)</p>
    <p>Recall granularity grows with</p>
    <p>file size</p>
    <p>Incremental Dedup will later</p>
    <p>optimize the new data</p>
    <p>GC cleans up unreferenced</p>
    <p>chunks (chunk D in</p>
    <p>example)</p>
    <p>Post Write File Layout</p>
    <p>Pre write File Layout</p>
    <p>Write path to Optimized File</p>
    <p>Write flow</p>
  </div>
  <div class="page">
    <p>Perf. Improvement: Chunk Compression</p>
    <p>Compression/decompression can</p>
    <p>have a significant perf impact</p>
    <p>Compression savings is skewed</p>
    <p>50% of unique chunks responsible</p>
    <p>for 86% of compression savings</p>
    <p>31% of chunks do not compress at</p>
    <p>all</p>
    <p>Solution: selective compression</p>
    <p>Slide 24</p>
    <p>GFS-US dataset</p>
    <p>Reduces cost of compression for large fraction of chunks</p>
    <p>While preserving most of compression savings</p>
    <p>Reduces decompression costs (reduce CPU pressure during heavy reads)</p>
    <p>Also: use a cache for decompressed data (important for hotspots)</p>
    <p>Heuristics for determining which chunks should not be compressed</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Throughput</p>
    <p>Quad-core Intel Xeon 2.27GHz machine, 12GB RAM</p>
    <p>Four scenarios, from combinations of</p>
    <p>Index type (pure in-memory vs. memory/disk)</p>
    <p>Data partitioning (off or on)</p>
    <p>Deduplication throughput</p>
    <p>25-30 MB/s (single thread performance)</p>
    <p>Only about 10% decrease from baseline to least memory case</p>
    <p>Three orders of magnitude higher than typical data ingestion rates</p>
    <p>of 0.03 MB/sec (Leung, et al.)</p>
    <p>Slide 25</p>
    <p>GFS-US dataset</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Resource Usage</p>
    <p>RAM frugality</p>
    <p>Index memory usage</p>
    <p>reduction of 24x vs. baseline</p>
    <p>Low CPU utilization</p>
    <p>30-40% per core</p>
    <p>Enough room available for</p>
    <p>serving primary workload in multi-core modern file servers</p>
    <p>Low disk usage</p>
    <p>Median disk queue depth is zero in all cases</p>
    <p>At 75-th percentile, increase by 2-3; impact of index lookups going</p>
    <p>to disk and/or reconciliation</p>
    <p>Slide 26</p>
  </div>
  <div class="page">
    <p>Performance Evaluation  Parallelizability</p>
    <p>Parallel processing across datasets and CPU cores/disks</p>
    <p>Disk diverse datasets</p>
    <p>One session per volume in current implementation</p>
    <p>One CPU core allocated per dedup session</p>
    <p>One process and thread per deduplication session</p>
    <p>No cross-dependencies in deduplication sessions (each session uses a</p>
    <p>separate index)</p>
    <p>Aggregate dedup throughput scales as expected with number of cores</p>
    <p>(provided sufficient RAM is available)</p>
    <p>Workload scheduler</p>
    <p>Assigns jobs (deduplication, GC, scrubbing) with CPU cores</p>
    <p>Allocates memory per job</p>
    <p>Keeps track of job activity (cancel jobs on memory or CPU pressure)</p>
    <p>Slide 27</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Large scale study of primary data dedup</p>
    <p>7TB of data across 15 globally distributed servers in a large enterprise</p>
    <p>Primary data deduplication in Windows Server 2012</p>
    <p>Design decisions driven by data analysis findings</p>
    <p>Primary workload friendly</p>
    <p>Scale deduplication processing resource usage with data size</p>
    <p>CPU/memory/disk IO</p>
    <p>Data chunking and compression</p>
    <p>Chunk indexing</p>
    <p>Data partitioning and reconciliation</p>
    <p>Primary data serving, reliability, and resiliency aspects not</p>
    <p>covered in this paper</p>
  </div>
</Presentation>
