<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FlashVM: Virtual Memory Management on Flash</p>
    <p>Mohit Saxena</p>
    <p>Michael M. Swift</p>
    <p>University of WisconsinMadison</p>
  </div>
  <div class="page">
    <p>Is Virtual Memory Relevant?</p>
    <p>There is never enough DRAM  Price, power and DIMM slots limit amount  Application memory footprints are everincreasing</p>
    <p>VM is no longer DRAM+Disk  New memory technologies: Flash, PCM, Memristor .</p>
  </div>
  <div class="page">
    <p>Flash and Virtual Memory</p>
    <p>DRAM</p>
    <p>DRAM is expensive</p>
    <p>Disk is slow</p>
    <p>Flash is cheap and fast</p>
    <p>Flash for Virtual Memory</p>
    <p>Storage + VM</p>
    <p>Disk</p>
    <p>VMStorage</p>
    <p>Flash</p>
  </div>
  <div class="page">
    <p>In this talk</p>
    <p>Flash for Virtual Memory  Does it improve system price/performance?  What OS changes are required?</p>
    <p>FlashVM  System architecture using dedicated flash for VM  Extension to core VM subsystem in the Linux kernel  Improved performance, reliability and garbage collection</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction  Background</p>
    <p>Flash and VM  Design  Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>Flash 101</p>
    <p>Flash is not disk  Faster random access performance: 0.1 vs. 23 ms for disk  No inplace modify: write only to erased location</p>
    <p>Flash blocks wear out  Erasures limited to 10,000100,000 per block  Reliability dropping with increasing MLC flash density</p>
    <p>Flash devices age  Logstructured writes leave few clean blocks after extensive use</p>
    <p>Performance drops by up to 85% on some SSDs  Requires garbage collection of free blocks</p>
  </div>
  <div class="page">
    <p>Virtual Memory 101</p>
    <p>DiskVM</p>
    <p>FlashVM</p>
    <p>Memory Size M</p>
    <p>Ex ec u ti o n T im</p>
    <p>e T</p>
    <p>M Reduced DRAM Same performance Lower system price</p>
    <p>T Faster execution No additional DRAM Similar system price</p>
    <p>No Locality</p>
    <p>Unused Memory</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction  Background  Design</p>
    <p>Performance  Reliability  Garbage Collection</p>
    <p>Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>FlashVM Hierarchy</p>
    <p>Block Device Driver</p>
    <p>Page Swapping</p>
    <p>DRAM</p>
    <p>FlashVM Manager: Performance, Reliability and Garbage Collection</p>
    <p>Dedicated Flash Costeffective for VM</p>
    <p>Reduced FS interference Disk</p>
    <p>Disk Scheduler</p>
    <p>Block Layer</p>
    <p>VM Memory Manager</p>
    <p>Dedicated Flash</p>
    <p>MLC NAND</p>
  </div>
  <div class="page">
    <p>VM Performance</p>
    <p>Challenge  VM systems optimized for disk performance  Slow random reads, high access and seek costs, symmetrical read/write performance</p>
    <p>FlashVM dediskifies VM:  Page write back  Page scanning  Disk scheduling  Page prefetching</p>
    <p>Parameter Tuning</p>
  </div>
  <div class="page">
    <p>Page Prefetching</p>
    <p>Linux</p>
    <p>swap map</p>
    <p>FREE</p>
    <p>FREE</p>
    <p>BAD</p>
    <p>FlashVM</p>
    <p>VM assumption</p>
    <p>Seek and rotational delays are longer than the transfer cost of extra blocks</p>
    <p>Linux sequential prefetching Minimize costly disk seeks</p>
    <p>Delimited by free and bad blocks</p>
    <p>FlashVM prefetching</p>
    <p>Exploit fast flash random reads and spatial locality in reference pattern</p>
    <p>Seek over free and bad blocks</p>
    <p>Request</p>
  </div>
  <div class="page">
    <p>Stride Prefetching</p>
    <p>FlashVM uses stride prefetching  Exploit temporal locality in the reference pattern</p>
    <p>Exploit cheap seeks for fast random access</p>
    <p>Fetch two extra blocks in the stride</p>
    <p>Request</p>
    <p>Request</p>
    <p>Request</p>
    <p>Request</p>
    <p>Request</p>
    <p>swap map</p>
  </div>
  <div class="page">
    <p>The Reliability Problem</p>
    <p>Challenge: Reduce the number of writes  Flash chips lose durability after 10,000  100,000 writes  Actual writelifetime can be two orders of magnitude less  Past solutions:</p>
    <p>Diskbased write caches for streamed I/O  Deduplication and compression for storage</p>
    <p>FlashVM uses knowledge of page content and state  Dirty Page sampling  Zero Page sharing</p>
  </div>
  <div class="page">
    <p>Page Sampling</p>
    <p>Dirty?</p>
    <p>Dirty</p>
    <p>Clean</p>
    <p>Inactive LRU Page List</p>
    <p>free_pages</p>
    <p>sample</p>
    <p>Disk</p>
    <p>Linux Writeback all evicted</p>
    <p>dirty pages</p>
    <p>Flash</p>
    <p>FlashVM Prioritize young clean over old dirty pages</p>
    <p>Free Page List</p>
    <p>sR</p>
  </div>
  <div class="page">
    <p>Adaptive Sampling</p>
    <p>Challenge: Reference pattern variations  Writemostly: Many dirty pages  Readmostly: Many clean pages</p>
    <p>FlashVM adapts sampling rate  Maintain a moving average for the write rate  Low write rate Increase sR</p>
    <p>Aggressively skip dirty pages  High write rate Converge to native Linux</p>
    <p>Evict dirty pages to relieve memory pressure</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction  Why FlashVM?  Design</p>
    <p>Performance  Reliability  Garbage Collection</p>
    <p>Evaluation  Conclusions</p>
  </div>
  <div class="page">
    <p>write cluster A at block 0 write cluster B at block 100</p>
    <p>Flash Cleaning</p>
    <p>All writes to flash go to a new location</p>
    <p>Discard command notifies SSD that blocks are unused</p>
    <p>Benefits:  More free blocks for writing</p>
    <p>Avoids copying data for partial overwrites</p>
    <p>free cluster A &amp; discard</p>
    <p>write cluster D at block 0</p>
    <p>write cluster A at block 0</p>
    <p>free cluster A write cluster B at block 100 free cluster B write cluster C at block 200</p>
  </div>
  <div class="page">
    <p>read 4KB</p>
    <p>write 4KB</p>
    <p>erase 128KB</p>
    <p>discard 4KB</p>
    <p>discard 1MB</p>
    <p>discard 10MB</p>
    <p>discard 100MB</p>
    <p>discard 1GB</p>
    <p>Operation</p>
    <p>La te n cy ( m s)</p>
    <p>Discard is Expensive</p>
    <p>OCZVertex, Indilinx controller</p>
    <p>Operation Latency</p>
    <p>&lt; 0.5 ms 2 ms</p>
  </div>
  <div class="page">
    <p>Discard and VM</p>
    <p>Native Linux VM has limited discard support  Invokes discard before reusing free page clusters  Pays high fixed cost for small sets of pages</p>
    <p>FlashVM optimizes to reduce discard cost  Avoid unnecessary discards: dummy discard  Discard larger sizes to amortize cost: merged discard</p>
  </div>
  <div class="page">
    <p>Dummy Discard</p>
    <p>Observation: Overwriting a block  notifies SSD it is empty  after discarding it, uses the free space made available by discard</p>
    <p>FlashVM implements dummy discard  Monitors rate of allocation  Virtualize discard by reusing blocks likely to be overwritten soon</p>
    <p>Overwrite</p>
    <p>DiscardOverwrite</p>
  </div>
  <div class="page">
    <p>Merged Discard</p>
    <p>Native Linux invokes discard once per page cluster  Result: 55 ms latency for freeing 32 pages (128K)</p>
    <p>FlashVM batch many free pages  Defer discard until 100 MB of free pages available</p>
    <p>Pages discarded may be noncontiguous</p>
    <p>Discard</p>
    <p>Discard</p>
    <p>Discard</p>
    <p>Discard</p>
  </div>
  <div class="page">
    <p>Design Summary</p>
    <p>Performance improvements  Parameter Tuning: page write back, page scanning, disk scheduling</p>
    <p>Improved/stride prefetching  Reliability improvements</p>
    <p>Reduced writes: page sampling and sharing  Garbage collection improvements</p>
    <p>Merged and Dummy discard</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction  Motivation  Design  Evaluation</p>
    <p>Performance and memory savings  Reliability and garbage collection</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>System and Devices  2.5 GHz Intel Core 2 Quad, Linux 2.6.28 kernel  IBM, Intel X25M, OCZVertex trimcapable SSDs</p>
    <p>Application Workloads  ImageMagick  resizing a large JPEG image by 500%  Spin  model checking for 10 million states  SpecJBB  16 concurrent warehouses  memcached server  keyvalue store for 1 million keys</p>
  </div>
  <div class="page">
    <p>Application Performance and Memory Savings</p>
    <p>ImageMagick Spin SpecJBB memcachedstore</p>
    <p>memcachedlookup</p>
    <p>Applications</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>/M em</p>
    <p>or y</p>
    <p>S av</p>
    <p>in gs</p>
    <p>Runtime Memory Use</p>
    <p>Const Memory 94% less execution time</p>
    <p>Const Performance 84% memory savings</p>
  </div>
  <div class="page">
    <p>Write Reduction</p>
    <p>Uniform Page Sampling</p>
    <p>Adaptive Page Sampling</p>
    <p>Page Sharing</p>
    <p>Write Reduction Technique</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>/W ri</p>
    <p>te s</p>
    <p>Performance Writes</p>
    <p>ImageMagick</p>
    <p>Spin</p>
  </div>
  <div class="page">
    <p>Garbage Collection</p>
    <p>ImageMagick Spin memcached</p>
    <p>Application</p>
    <p>E la</p>
    <p>p se</p>
    <p>d T</p>
    <p>im e</p>
    <p>(s )</p>
    <p>Linux/Discard FlashVM Linux/No Discard</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>FlashVM: Virtual Memory Management on Flash  Dedicated flash for paging  Improved performance, reliability and garbage collection</p>
    <p>More opportunities and challenges for OS design  Scaling FlashVM to massive memory capacities (terabytes!)</p>
    <p>Future memory technologies: PCM and Memristors</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>FlashVM: Virtual Memory Management on Flash</p>
    <p>Mohit Saxena Michael M. Swift</p>
    <p>University of WisconsinMadison http://pages.cs.wisc.edu/~msaxena/FlashVM.html</p>
  </div>
</Presentation>
