<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Iterative Search for Weakly Supervised Semantic Parsing</p>
    <p>Pradeep Dasigi</p>
    <p>Matt Gardner</p>
    <p>Shikhar Murty</p>
    <p>Luke Zettlemoyer</p>
    <p>Ed Hovy</p>
  </div>
  <div class="page">
    <p>This talk in one slide</p>
    <p>Training semantic parsing with denotation-only supervision is challenging because of spuriousness: incorrect logical forms can yield correct denotations.</p>
    <p>Two solutions:  Iterative training: Online search with initialization  MML over offline search output  Coverage during online search</p>
    <p>State-of-the-art single model performances:  WikiTableQuestions with comparable supervision  NLVR semantic parsing with significantly less supervision</p>
  </div>
  <div class="page">
    <p>Semantic Parsing for Question Answering</p>
    <p>Athlete Nation Olympics Medals</p>
    <p>Gillis Grafstrm</p>
    <p>Sweden (SWE) 19201932 4</p>
    <p>Kim Soo-Nyung</p>
    <p>South Korea (KOR)</p>
    <p>Evgeni Plushenko</p>
    <p>Russia (RUS) 20022014 4</p>
    <p>Kim Yu-na South Korea</p>
    <p>(KOR) 20102014 2</p>
    <p>Patrick Chan Canada (CAN) 2014 2</p>
    <p>Question: Which athlete was from South Korea after the year 2010?</p>
    <p>Answer: Kim Yu-Na</p>
    <p>Reasoning: 1) Get rows where Nation is South Korea 2) Filter rows where value in Olympics &gt; 2010. 3) Get value from Athlete column</p>
    <p>Program: (select_string (filter</p>
    <p>in (filter</p>
    <p>&gt; all_rows olympics 2010)</p>
    <p>south_korea) athlete)</p>
    <p>WikiTableQuestions, Pasupat and Liang (2015)</p>
  </div>
  <div class="page">
    <p>Weakly Supervised Semantic Parsing</p>
    <p>xi: Which athlete was from South Korea after 2010?</p>
    <p>yi: (select_string (filter in(filter&gt; all_rows olympics 2010)south_korea) athlete) zi: Kim Yu-Na</p>
    <p>wi:</p>
    <p>Test: Given find such that</p>
    <p>Athlete Nation Olympics Medals</p>
    <p>Kim Yu-na South Korea 20102014 2</p>
    <p>Tenley Albright</p>
    <p>United States</p>
    <p>Train on</p>
  </div>
  <div class="page">
    <p>Challenge: Spurious logical forms Which athletes are from South Korea after 2010? Kim Yu-Na</p>
    <p>Logical forms that lead to answer:</p>
    <p>((reverse athlete)(and(nation south_korea)(year ((reverse date) (&gt;= 2010-mm-dd)))</p>
    <p>((reverse athlete)(and(nation south_korea)(medals 2)))</p>
    <p>((reverse athlete)(row.index (min ((reverse row.index) (medals 2)))))</p>
    <p>((reverse athlete) (row.index 4))</p>
    <p>Athlete Nation Olympics Medals</p>
    <p>Gillis Grafstrm</p>
    <p>Sweden (SWE) 19201932 4</p>
    <p>Kim Soo-Nyung</p>
    <p>South Korea (KOR)</p>
    <p>Evgeni Plushenko</p>
    <p>Russia (RUS) 20022014 4</p>
    <p>Kim Yu-na South Korea</p>
    <p>(KOR) 20102014 2</p>
    <p>Patrick Chan Canada (CAN) 2014 2</p>
    <p>Athlete from South Korea with 2 medals</p>
    <p>First athlete in the table with 2 medals</p>
    <p>Athlete in row 4</p>
    <p>Athlete from South Korea after 2010</p>
  </div>
  <div class="page">
    <p>Challenge: Spurious logical forms</p>
    <p>There is exactly one square touching the bottom of a box True</p>
    <p>Logical forms that lead to answer:</p>
    <p>(count_equals(square (touch_bottom all_objects)) 1)</p>
    <p>(count_equals (yellow (square all_objects)) 1)</p>
    <p>(object_exists (yellow (triangle (all_objects))))</p>
    <p>(object_exists all_objects)</p>
    <p>Count of squares touching bottom of boxes is 1</p>
    <p>Count of yellow squares is 1</p>
    <p>There exists a yellow triangle</p>
    <p>There exists an object</p>
    <p>Due to binary denotations, 50% of logical forms give correct answer!</p>
    <p>Cornell Natural Language Visual Reasoning, Suhr et al., 2017</p>
  </div>
  <div class="page">
    <p>Training Objectives</p>
    <p>Reward/Cost -based approaches</p>
    <p>Eg.: Neelakantan et al. (2016), Liang et al. (2017, 2018), and others</p>
    <p>Minimum Bayes Risk training: Minimize the expected value of a cost</p>
    <p>but random initialization can cause the search to get stuck in the exponential search space</p>
    <p>Maximum Marginal Likelihood</p>
    <p>Eg.: Liang et al. (2011), Berant et al. (2013), Krishnamurthy et al. (2017), and others</p>
    <p>Maximize the marginal likelihood of an approximate set of logical forms</p>
    <p>but we need a good set of approximate logical forms</p>
    <p>Proposal: Alternate between the two objectives while gradually increasing the search space!</p>
  </div>
  <div class="page">
    <p>Spuriousness solution 1: Iterative search</p>
    <p>Limited depth exhaustive search</p>
    <p>Max logical form depth = k</p>
    <p>Step 0: Get seed set of logical forms till depth k</p>
    <p>LSTMLSTM LSTM LSTM</p>
  </div>
  <div class="page">
    <p>Spuriousness solution 1: Iterative search</p>
    <p>Limited depth exhaustive search</p>
    <p>Step 0: Get seed set of logical forms till depth k</p>
    <p>LSTMLSTM LSTM LSTM</p>
    <p>Maximum Marginal Likelihood</p>
    <p>Step 1: Train model using MML on seed setMax logical form</p>
    <p>depth = k</p>
  </div>
  <div class="page">
    <p>Spuriousness solution 1: Iterative search</p>
    <p>Limited depth exhaustive search</p>
    <p>Step 2: Train using MBR on all data till a greater depth k + s</p>
    <p>LSTMLSTM LSTM LSTM</p>
    <p>Minimum Bayes Risk training till depth k + s</p>
    <p>Step 0: Get seed set of logical forms till depth k</p>
    <p>Step 1: Train model using MML on seed set</p>
  </div>
  <div class="page">
    <p>Spuriousness solution 1: Iterative search</p>
    <p>Step 3: Replace offline search with trained MBR and update seed set</p>
    <p>LSTMLSTM LSTM LSTM</p>
    <p>Minimum Bayes Risk training till depth k + s</p>
    <p>Max logical form depth = k + s</p>
    <p>Step 0: Get seed set of logical forms till depth k</p>
    <p>Step 1: Train model using MML on seed set</p>
    <p>Step 2: Train using MBR on all data till a greater depth k + s</p>
  </div>
  <div class="page">
    <p>Spuriousness solution 1: Iterative search</p>
    <p>k : k + s; Go to Step 1</p>
    <p>Iterate till dev. accuracy stops increasing</p>
    <p>LSTMLSTM LSTM LSTM</p>
    <p>Maximum Marginal Likelihood</p>
    <p>Step 3: Replace offline search with trained MBR and update seed set</p>
    <p>Step 0: Get seed set of logical forms till depth k</p>
    <p>Step 1: Train model using MML on seed set</p>
    <p>Step 2: Train using MBR on all data till a greater depth k + s</p>
  </div>
  <div class="page">
    <p>Spuriousness Solution 2: Coverage guidance</p>
    <p>There is exactly one square touching the bottom of a box.</p>
    <p>(count_equals (square (touch_bottom all_objects)) 1)</p>
    <p>Insight: There is a significant amount of trivial overlap</p>
    <p>Solution: Use overlap as a measure guide search</p>
  </div>
  <div class="page">
    <p>Spuriousness Solution 2: Coverage guidance There is exactly one square touching the bottom.</p>
    <p>Target symbols triggered by rules: count_equals 1 square touch_bottom</p>
    <p>Coverage cost is the number of triggered symbols that do not appear in the logical form</p>
    <p>Lexicon there is a box  box_exists there is a [other]  object_exists box  blue  color_blue box  black  color_black box  yellow  color_yellow box  square  shape_square box  circle  shape_circle box  triangle  shape_triangle not  negate_filter contains  object_in_box touch  top  touch_top touch  bottom  touch_bottom touch  corner  touch_corner touch  right  touch_right touch  left  touch_left touch  wall  touch_edge</p>
    <p>top  top bottom  bottom above  above below  below square  square circle  circle triangle  triangle yellow  yellow black  black blue  blue big  big small  small medium  medium</p>
    <p>Example:</p>
    <p>Sentence: There is exactly one square touching the bottom of a box.</p>
    <p>Triggered target symbols: {count_equals, square, 1, touch_bottom}</p>
    <p>Coverage costs of candidate logical forms:</p>
    <p>Logical form Coverage</p>
    <p>(count_equals (square (touch_bottom all_objects)) 1)</p>
    <p>(count_equals (square all_objects) 1) 1</p>
    <p>(object_exists all_objects) 4</p>
  </div>
  <div class="page">
    <p>Training with Coverage Guidance</p>
    <p>Augment the reward-based objective:</p>
    <p>now is defined a linear combination of coverage and denotation costs</p>
  </div>
  <div class="page">
    <p>Results of training with iterative search on NLVR*</p>
    <p>* using structured representations</p>
  </div>
  <div class="page">
    <p>Results of training with iterative search on WikiTableQuestions</p>
  </div>
  <div class="page">
    <p>Results of using coverage guided training on NLVR*</p>
    <p>when trained from scratch when model initialized from an MML model trained on a seed set of offline searched paths</p>
    <p>* using structured representations</p>
    <p>Model does not learn without coverage! Coverage helps even with strong initialization</p>
  </div>
  <div class="page">
    <p>Comparison with previous approaches on NLVR*</p>
    <p>* using structured representations</p>
    <p>MaxEnt, BiAttPonter are not semantic parsers</p>
    <p>Abs. supervision + Rerank uses manually labeled abstractions of utterance - logical form pairs to get training data for a supervised system, and reranking</p>
    <p>Our work outperforms Goldman et al., 2018 with fewer resources</p>
  </div>
  <div class="page">
    <p>Comparison with previous approaches on WikiTableQuestions</p>
    <p>Non-neural models Reinforcement Learning models</p>
    <p>Non-RL Neural Models</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Spuriousness is a challenge in training semantic parsers with weak supervision</p>
    <p>Two solutions:  Iterative training: Online search with initialization  MML over offline search output  Coverage during online search</p>
    <p>SOTA single model performances:  WikiTableQuestions: 44.3%  NLVR semantic parsing: 82.9%</p>
    <p>Thank you! Questions?</p>
  </div>
</Presentation>
