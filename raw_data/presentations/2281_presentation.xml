<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sockets vs. RDMA Interface over</p>
    <p>Analysis of the Memory Traffic Bottleneck</p>
    <p>Pavan Balaji Hemal V. Shah D. K. Panda</p>
    <p>Network Based Computing Lab</p>
    <p>Computer Science and Engineering</p>
    <p>Ohio State University</p>
    <p>Embedded IA Division</p>
    <p>Intel Corporation</p>
    <p>Austin, Texas</p>
  </div>
  <div class="page">
    <p>Introduction and Motivation  Advent of High Performance Networks</p>
    <p>Ex: InfiniBand, 10-Gigabit Ethernet, Myrinet, etc.</p>
    <p>High Performance Protocols: VAPI / IBAL, GM</p>
    <p>Good to build new applications</p>
    <p>Not so beneficial for existing applications</p>
    <p>Built around portability: Should run on all platforms</p>
    <p>TCP/IP based sockets: A popular choice</p>
    <p>Several GENERIC optimizations proposed and implemented for TCP/IP</p>
    <p>Jacobson Optimization: Integrated Checksum-Copy [Jacob89]</p>
    <p>Header Prediction for Single Stream data transfer</p>
    <p>[Jacob89]: An analysis of TCP Processing Overhead, D. Clark, V. Jacobson, J. Romkey and H. Salwen. IEEE Communications</p>
  </div>
  <div class="page">
    <p>Generic Optimizations Insufficient!</p>
    <p>Processor Speed DOES NOT scale with Network Speeds</p>
    <p>Protocol processing too expensive for current day systems</p>
    <p>http://www.intel.com/research/silicon/mooreslaw.htm</p>
  </div>
  <div class="page">
    <p>Network Specific Optimizations  Sockets can utilize some network features</p>
    <p>Hardware support for protocol processing</p>
    <p>Interrupt Coalescing (can be considered generic)</p>
    <p>Checksum Offload (TCP stack has to modified)</p>
    <p>Insufficient!</p>
    <p>Network Specific Optimizations</p>
    <p>High Performance Sockets [shah99, balaji02]</p>
    <p>TCP Offload Engines (TOE)</p>
    <p>[shah99]: High Performance Sockets and RPC over Virtual Interface (VI) Architecture, H. Shah, C. Pu, R. S. Madukkarumukumana, In CANPC 99</p>
    <p>[balaji02]: Impact of High Performance Sockets on Data Intensive Applications, P. Balaji, J. Wu, T. Kurc, U. Catalyurek, D. K. Panda, J. Saltz, In HPDC 03</p>
  </div>
  <div class="page">
    <p>Memory Traffic Bottleneck  Offloaded Transport Layers provide some performance gains</p>
    <p>Protocol processing is offloaded; lesser host CPU overhead</p>
    <p>Better network performance for slower hosts</p>
    <p>Quite effective for 1-2 Gigabit networks</p>
    <p>Effective for faster (10-Gigabit) networks in some scenarios</p>
    <p>Memory Traffic Constraints</p>
    <p>Offloaded Transport Layers rely on the sockets interface</p>
    <p>Sockets API forces memory access operations in several scenarios</p>
    <p>Transactional protocols such as RPC, File I/O, etc.</p>
    <p>For 10-Gigabit networks memory access operations can limit network performance !</p>
  </div>
  <div class="page">
    <p>Recently released as a successor in the Ethernet family</p>
    <p>Some adapters support TCP/IP checksum and Segmentation offload</p>
    <p>InfiniBand</p>
    <p>Open Industry Standard</p>
    <p>Interconnect for connecting compute and I/O nodes</p>
    <p>Provides High Performance</p>
    <p>Offloaded Transport Layer; Zero-Copy data-transfer</p>
    <p>Provides one-sided communication (RDMA, Remote Atomics)</p>
    <p>Becoming increasingly popular</p>
    <p>An example RDMA capable 10-Gigabit network</p>
  </div>
  <div class="page">
    <p>Objective</p>
    <p>New standards proposed for RDMA over IP</p>
    <p>Utilizes an offloaded TCP/IP stack on the network adapter</p>
    <p>Supports additional logic for zero-copy data transfer to the application</p>
    <p>Compatible with existing Layer 3 and 4 switches</p>
    <p>Whats the impact of an RDMA interface over TCP/IP?</p>
    <p>Implications on CPU Utilization</p>
    <p>Implications on Memory Traffic</p>
    <p>Is it beneficial?</p>
    <p>We analyze these issues using InfiniBands RDMA capabilities!</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction and Motivation</p>
    <p>TCP/IP Control Path and Memory Traffic</p>
    <p>10-Gigabit network performance for TCP/IP</p>
    <p>10-Gigabit network performance for RDMA</p>
    <p>Memory Traffic Analysis for 10-Gigabit networks</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>TCP/IP Control Path (Sender Side)</p>
    <p>Application Buffer</p>
    <p>Socket Buffer</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>write()</p>
    <p>Checksum and Copy</p>
    <p>Post TX Kick Driver</p>
    <p>Return to Application</p>
    <p>Post Descriptor</p>
    <p>INTR on transmit success</p>
    <p>DMA</p>
    <p>Checksum, Copy and DMA are the data touching portions in TCP/IP</p>
    <p>Offloaded protocol stacks avoid checksum at the host; copy and DMA are still</p>
    <p>present</p>
    <p>Packet Leaves</p>
  </div>
  <div class="page">
    <p>TCP/IP Control Path (Receiver Side)</p>
    <p>Application Buffer</p>
    <p>Socket Buffer</p>
    <p>NIC</p>
    <p>Driver</p>
    <p>Wait for read()</p>
    <p>read()</p>
    <p>Application gets data</p>
    <p>Copy</p>
    <p>DMA</p>
    <p>Packet Arrives</p>
    <p>INTR on Arrival</p>
    <p>Data might need to be buffered on the receiver side</p>
    <p>Pick-and-Post techniques force a memory copy on the receiver side</p>
  </div>
  <div class="page">
    <p>North Bridge</p>
    <p>Application and Socket buffers fetched to L2 $</p>
    <p>Application Buffer written back to memory</p>
    <p>Memory Bus Traffic for TCP</p>
    <p>CPU</p>
    <p>NIC</p>
    <p>FSB Memory Bus</p>
    <p>I/O B</p>
    <p>u s</p>
    <p>Each network byte requires 4 bytes</p>
    <p>to be transferred on the Memory</p>
    <p>Bus (unidirectional traffic)</p>
    <p>Assuming 70% memory efficiency,</p>
    <p>TCP can support at most 4-5Gbps</p>
    <p>bidirectional on 10Gbps</p>
    <p>(400MHz/64bit FSB)</p>
    <p>Appln. Buffer</p>
    <p>Socket Buffer</p>
    <p>L2 $ Memor y</p>
    <p>Appln. Buffer</p>
    <p>Socket BufferData DMAData</p>
    <p>Copy</p>
  </div>
  <div class="page">
    <p>Network to Memory Traffic Ratio</p>
    <p>Application Buffer Fits in Cache</p>
    <p>Application Buffer Doesnt fit in Cache</p>
    <p>Transmit (Worst Case) 1-4 2-4</p>
    <p>Transmit (Best Case) 1 2-4</p>
    <p>Receive (Worst Case) 2-4 4</p>
    <p>Receive (Best Case) 2 4</p>
    <p>This table shows the minimum memory traffic associated with network data</p>
    <p>In reality socket buffer cache misses, control messages and noise traffic may cause these to be higher</p>
    <p>Details of other cases present in the paper</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction and Motivation</p>
    <p>TCP/IP Control Path and Memory Traffic</p>
    <p>10-Gigabit network performance for TCP/IP</p>
    <p>10-Gigabit network performance for RDMA</p>
    <p>Memory Traffic Analysis for 10-Gigabit networks</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Test-bed (10-Gig Ethernet)</p>
    <p>Two Dell2600 Xeon 2.4GHz 2-way SMP node</p>
    <p>1GB main memory (333MHz, DDR)</p>
    <p>Intel E7501 Chipset</p>
    <p>32K L1, 512K L2, 400MHz/64bit FSB</p>
    <p>PCI-X 133MHz/64bit I/O bus</p>
    <p>Intel 10GbE/Pro 10-Gigabit Ethernet adapters</p>
    <p>8 P4 2.0 GHz nodes (IBM xSeries 305; 8673-12X)</p>
    <p>Intel Pro/1000 MT Server Gig-E adapters</p>
    <p>256K main memory</p>
  </div>
  <div class="page">
    <p>TCP/IP achieves a latency of 37us (Win Server 2003)  20us on Linux</p>
    <p>About 50% CPU utilization on both platforms</p>
    <p>Peak Throughput of about 2500Mbps; 80-100% CPU Utilization</p>
    <p>Application buffer is always in Cache !!</p>
  </div>
  <div class="page">
    <p>TCP Stack Pareto Analysis (64 byte) Sender Receiver</p>
    <p>Kernel, Kernel Libraries and TCP/IP contribute to the Offloadable TCP/IP stack</p>
  </div>
  <div class="page">
    <p>TCP Stack Pareto Analysis (16K byte) Sender Receiver</p>
    <p>TCP and other protocol overhead takes up most of the CPU</p>
    <p>Offload is beneficial when buffers fit into cache</p>
  </div>
  <div class="page">
    <p>TCP Stack Pareto Analysis (16K byte) Sender Receiver</p>
    <p>TCP and other protocol overhead takes up most of the CPU</p>
    <p>Offload is beneficial when buffers fit into cache</p>
  </div>
  <div class="page">
    <p>Throughput (Fan-in/Fan-out) SB = 128K; MTU=9K</p>
    <p>Peak throughput of 3500Mbps for Fan-In and 4200Mbps for Fan-out</p>
  </div>
  <div class="page">
    <p>Bi-Directional Throughput</p>
    <p>Not the traditional Bi-directional Bandwidth test</p>
    <p>Fan-in with half the nodes and Fan-out with the other half</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction and Motivation</p>
    <p>TCP/IP Control Path and Memory Traffic</p>
    <p>10-Gigabit network performance for TCP/IP</p>
    <p>10-Gigabit network performance for RDMA</p>
    <p>Memory Traffic Analysis for 10-Gigabit networks</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Test-bed (InfiniBand)</p>
    <p>8 SuperMicro SUPER P4DL6 nodes  Xeon 2.4GHz 2-way SMP nodes</p>
    <p>512MB main memory (DDR)</p>
    <p>PCI-X 133MHZ/64bit I/O bus</p>
    <p>Mellanox InfiniHost MT23108 DualPort 4x HCA  InfiniHost SDK version 0.2.0</p>
    <p>HCA firmware version 1.17</p>
    <p>Mellanox InfiniScale MT43132 8-port switch (4x)</p>
    <p>Linux kernel version 2.4.7-10smp</p>
  </div>
  <div class="page">
    <p>InfiniBand RDMA: Latency and Bandwidth</p>
    <p>Performance improvement due to hardware support and zero-copy data transfer</p>
    <p>Near zero CPU Utilization at the data sink for large messages</p>
    <p>Performance limited by PCI-X I/O bus</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction and Motivation</p>
    <p>TCP/IP Control Path and Memory Traffic</p>
    <p>10-Gigabit network performance for TCP/IP</p>
    <p>10-Gigabit network performance for RDMA</p>
    <p>Memory Traffic Analysis for 10-Gigabit networks</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Throughput test: Memory Traffic</p>
    <p>Sockets can force up to 4 times more memory traffic compared to the network traffic</p>
    <p>RDMA allows has a ratio of 1 !!</p>
  </div>
  <div class="page">
    <p>Multi-Stream Tests: Memory Traffic</p>
    <p>Memory Traffic is significantly higher than the network traffic</p>
    <p>Comes to within 5% of the practically attainable peak memory bandwidth</p>
  </div>
  <div class="page">
    <p>Presentation Outline</p>
    <p>Introduction and Motivation</p>
    <p>TCP/IP Control Path and Memory Traffic</p>
    <p>10-Gigabit network performance for TCP/IP</p>
    <p>10-Gigabit network performance for RDMA</p>
    <p>Memory Traffic Analysis for 10-Gigabit networks</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Conclusions  TCP/IP performance on High Performance Networks</p>
    <p>High Performance Sockets</p>
    <p>TCP Offload Engines</p>
    <p>10-Gigabit Networks</p>
    <p>A new dimension of complexity  Memory Traffic</p>
    <p>Sockets API can require significant memory traffic</p>
    <p>Up to 4 times more than the network traffic</p>
    <p>Allows saturation on less than 35% of the network bandwidth</p>
    <p>Shows potential benefits of providing RDMA over IP</p>
    <p>Significant benefits in performance, CPU and memory traffic</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Memory Traffic Analysis for 64-bit systems</p>
    <p>Potential of the L3-Cache available in some systems</p>
    <p>Evaluation of various applications</p>
    <p>Transactional (SpecWeb)</p>
    <p>Streaming (Multimedia Services)</p>
  </div>
  <div class="page">
    <p>For more information, please visit the</p>
    <p>http://nowlab.cis.ohio-state.edu</p>
    <p>Network Based Computing Laboratory,</p>
    <p>The Ohio State University</p>
    <p>Thank You!</p>
    <p>NBC Home Page</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
</Presentation>
