<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Physical Adversarial Examples for Object Detectors</p>
    <p>Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, Tadayoshi Kohno, Dawn Song</p>
  </div>
  <div class="page">
    <p>Deep Neural Networks are Useful</p>
    <p>Speech Recognition,</p>
    <p>Machine XLAT Automated Game Playing</p>
    <p>Fast Brain Lesion Segmentation Image Courtesy: Nvidia/Imperial College</p>
  </div>
  <div class="page">
    <p>Deep Neural Networks are Useful, But Vulnerable</p>
    <p>panda 57.7% confidence</p>
    <p>gibbon 99.3% confidence</p>
    <p>Image Credit: OpenAI</p>
    <p>=+</p>
    <p>Goodfellow et al., Explaining and Harnessing Adversarial Examples, arXiv 1412.6572, 2015</p>
    <p>f(x) = y f(x)  y f(x) = y* where x = x +</p>
  </div>
  <div class="page">
    <p>Deep Neural Networks are Useful, But Vulnerable</p>
    <p>panda 57.7% confidence</p>
    <p>gibbon 99.3% confidence</p>
    <p>Image Credit: OpenAI</p>
    <p>=+</p>
    <p>Can we physically &amp; robustly perturb real objects, in ways that cause misclassifications in a DNN?</p>
  </div>
  <div class="page">
    <p>Current State of Physical Attacks</p>
    <p>Whats the dominant object in this image?</p>
    <p>Classification</p>
    <p>Eykholt et al., Robust Physical-World Attacks on Deep Learning Visual Classification, CVPR 2018</p>
    <p>Kurakin et al., Adversarial Examples in the Physical World, arXiv 1607.02533, 2016 Athalye et al., Synthesizing Robust Adversarial Examples, ICML 2018 Brown et al., Adversarial Patch, arXiv 1712.09665 Sharif et al., Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition, CCS 2016</p>
    <p>Our prior work</p>
  </div>
  <div class="page">
    <p>Different types of Deep Learning Models</p>
    <p>Whats the dominant object in this image?</p>
    <p>Classification</p>
    <p>What are the objects in this scene, and where are they?</p>
    <p>Object Detection</p>
    <p>What are the precise shapes and locations of objects?</p>
    <p>Semantic Segmentation</p>
    <p>Focus of this paper</p>
  </div>
  <div class="page">
    <p>Challenges in Attacking Detectors</p>
    <p>The location of the target object within the scene can vary widely</p>
    <p>Detectors process entire scene, allowing them to use contextual information</p>
    <p>Not limited to producing a single labeling, instead labels all objects in the scene</p>
    <p>We will start with an algorithm to attack classifiers and modify it to attack detectors</p>
  </div>
  <div class="page">
    <p>Review: Robust Physical Perturbations (RP2)</p>
    <p>A distance function The target class</p>
    <p>Perturbation/Noise Matrix Lp norm (L-0, L-1, L-2, )</p>
    <p>Loss Function</p>
    <p>Challenge: How can we make the perturbations robust to changing environmental conditions?</p>
  </div>
  <div class="page">
    <p>Modeling the Effects of the Environment</p>
    <p>Sample from the distribution Xv by:  Taking real images varying physical conditions (e.g., distances and angles)</p>
    <p>Using synthetic transformations</p>
  </div>
  <div class="page">
    <p>Optimizing Spatial Constraints</p>
    <p>Subtle Poster</p>
    <p>Camouflage Sticker</p>
    <p>Approximate vandalism</p>
    <p>Example Masks</p>
  </div>
  <div class="page">
    <p>How To Choose A Mask?</p>
    <p>We had very good success with the octagonal mask Possibility: Mask surface area should be large or should be focused on sensitive regions</p>
    <p>Use L-1</p>
  </div>
  <div class="page">
    <p>Process of Creating a Useful Sticker Attack</p>
    <p>L-1 Perturbation Result Mask Sticker Attack!</p>
  </div>
  <div class="page">
    <p>Adapting RP2: Translational Invariance</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>Adapting RP2: Adversarial Loss Function</p>
    <p>P(object) Cx Cy w h</p>
    <p>P(Stop sign) P(person)</p>
    <p>P(cat)</p>
    <p>P(vase)</p>
    <p>S x S grid cells</p>
    <p>Output of YOLO, 19 x 19 x 425 tensor</p>
    <p>Input scene</p>
    <p>Prob. of object being class y</p>
    <p>Minimize the probability of Stop sign among all predictions</p>
  </div>
  <div class="page">
    <p>Poster and Sticker Attack</p>
  </div>
  <div class="page">
    <p>Evaluation Method &amp; Data</p>
    <p>Record a video while moving towards a sign</p>
    <p>Sample video frames</p>
    <p>Count number of frames in which Stop sign was NOT detected</p>
    <p>White-box Black-box</p>
  </div>
  <div class="page">
    <p>Poster Attack on YOLO v2</p>
  </div>
  <div class="page">
    <p>Sticker Attack on YOLO v2</p>
  </div>
  <div class="page">
    <p>Black-box transfer</p>
    <p>to Faster-RCNN</p>
  </div>
  <div class="page">
    <p>Creation Attacks</p>
    <p>Cause the detector to hallucinate</p>
    <p>A meaningless-to-humans object but detected as an attacker-chosen class</p>
    <p>Threshold after which we stop optimizing for box confidence, set to 0.2</p>
    <p>Y is the class that the attacker wants the detector to see</p>
    <p>YOLO labels this as a Stop sign</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Adversarial examples generalize to varied environmental conditions</p>
    <p>With modest changes to our prior work on attacking classifiers, adversarial examples generalize to the richer class of object detection models  We introduced a new type of adversarial loss (disappearance, creation)</p>
    <p>We also introduced the Translational Invariance property</p>
    <p>Our evaluation based on the state-of-the-art YOLO v2 detector shows that physical attacks are possible up to distances of ~30 feet</p>
    <p>Do these attacks have system wide effects?</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Adversarial examples generalize to varied environmental conditions</p>
    <p>With modest changes to our prior work on attacking classifiers, adversarial examples generalize to the richer class of object detection models  We introduced a new type of adversarial loss (disappearance, creation)</p>
    <p>We also introduced the Translational Invariance property</p>
    <p>Our evaluation based on the state-of-the-art YOLO v2 detector shows that physical attacks are possible up to distances of ~30 feet</p>
    <p>Do these attacks have system wide effects?</p>
    <p>Earlence Fernandes, earlence@cs.washington.edu, earlence.com</p>
  </div>
</Presentation>
