<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>PUBCRAWL: Protecting Users and Businesses from CRAWLers</p>
    <p>Gregoire Jacob1,3, Engin Kirda2, Christopher Kruegel1, Giovanni Vigna1</p>
    <p>* image from viralpatel.net</p>
    <p>Fri Aug 10 2012</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 1 / 25</p>
  </div>
  <div class="page">
    <p>Introduction: are crawlers a threat?</p>
    <p>What do web crawlers/spiders do?</p>
    <p>Browse the web in an automatic and systematic fashion</p>
    <p>Various types:</p>
    <p>- web indexers for search engines, - link checkers for site verification, - but also scrapers to harvest the content of sites</p>
    <p>When does crawling become an abuse?</p>
    <p>Unauthorized large-scale crawls over web sites or social networks</p>
    <p>Use the collected data for competing products or services e.g., American Airlines-2003, Ryanair-2008, Facebook-2010</p>
    <p>Use the collected data for social engineering or targeted attacks e.g., StudiVZ-2009</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 2 / 25</p>
  </div>
  <div class="page">
    <p>Introduction: crawler prevention</p>
    <p>How can we prevent crawlers from accessing a web site?</p>
    <p>Robot Exclusion Protocol:</p>
    <p>Access rules to limit crawlers to certain parts of a site  Cooperation of the crawler is required</p>
    <p>User Authentication:</p>
    <p>Precise tracking of the user activity  Forcing user to login might not comply with the business model</p>
    <p>CAPTCHAs and Traps:</p>
    <p>Tests/Traps that are easily solved/avoided by humans but remain hard for computers</p>
    <p>Potential usability issues and reduced user satisfaction</p>
    <p>Crawlers need to be identified first to trigger prevention</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 3 / 25</p>
  </div>
  <div class="page">
    <p>Introduction: crawler prevention</p>
    <p>Why is the problem of detecting crawlers hard?</p>
    <p>IP-based identification of traffic sources</p>
    <p>- User-agent strings are unreliable - A same IP can host multiple users (proxy) - Authentication not necessarily available</p>
    <p>Passive detection is constraining</p>
    <p>- Request logs only contains basic information: timing, HTTP header and URL information</p>
    <p>- Request logs contains huge amounts of data to handle</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 4 / 25</p>
  </div>
  <div class="page">
    <p>Introduction: crawler prevention</p>
    <p>How are crawlers currently detected?</p>
    <p>Learning techniques to extract crawlers properties</p>
    <p>HTTP header artifacts:</p>
    <p>Betraying user-agent, missing referrer, ignored cookies  Stealthier crawlers already handle these shortcomings</p>
    <p>Simple traffic statistics:</p>
    <p>Large request volume, short inter-arrival time, night traffic  Large number of users behind a proxy show similar statistics  These statistics become inadequate with distributed crawling</p>
    <p>Need robust properties to distinguish: - large proxies hosting a large number of users - stealthy crawlers mimicking browsers - distributed crawlers over multiple sources</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 5 / 25</p>
  </div>
  <div class="page">
    <p>Introduction: containing crawlers</p>
    <p>Our approach: PUBCRAWL</p>
    <p>Hypotheses on the traffic shape:</p>
    <p>- User traffic: high versatility on the short-term, daily regularity. - Crawler traffic: high stability/versatility on the long-term. - Distributed crawler traffic: high synchronization across sources.</p>
    <p>Traffic model based on time series</p>
    <p>- Rational 1: Independently, time-series can model the traffic shape of a source in a way that is independent of the traffic volume.</p>
    <p>- Rational 2: Combined, time-series offer valuable information about the synchronization of sources.</p>
    <p>Machine learning to extract distinctive characteristics of crawler traffic</p>
    <p>Clustering to identify synchronized traffic from crawling campaigns</p>
    <p>Automatic configuration of a containment strategy accordingly</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 6 / 25</p>
  </div>
  <div class="page">
    <p>System: PUBCRAWL overview</p>
    <p>System architecture</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 7 / 25</p>
  </div>
  <div class="page">
    <p>System: heuristic detection</p>
    <p>Detection of suspicious request content</p>
    <p>Input:</p>
    <p>- Suspicious values in the HTTP header fields - Suspicious visit patterns in the URLs</p>
    <p>Heuristics:</p>
    <p>- Original set of heuristics from the state of the art - Extended heuristics adapted to social networks - Decision by majority vote over heuristics</p>
    <p>Source Heuristic Suspicious check</p>
    <p>HTTP High error rate 404 errors HTTP Suspicious referrer none or directory query HTTP Unbalanced traffic 99% of traffic coming from a single-user agent HTTP Ignored cookies new sequence number at every request URL No parameter use no language choice depending on the profile URL Low page revisit low overlap in the visited profiles URL Profile sequence alphabetical order of visited profiles</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 8 / 25</p>
  </div>
  <div class="page">
    <p>System: traffic shape detection</p>
    <p>Detection of crawlers by traffic classification:</p>
    <p>Input: time-series with normalized amplitude and common time origin</p>
    <p>Classification: - Features: Auto-correlation and decomposition analyses - Classifiers: + Naive Bayes, Association Rules and SVM classifiers</p>
    <p>+ Training over user and crawler traffic + Decision by majority vote over the classifiers output</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 9 / 25</p>
  </div>
  <div class="page">
    <p>System: traffic shape detection</p>
    <p>Time series generation</p>
    <p>Counting Process: volume of requests per time interval</p>
    <p>Fundamental differences in traffic shape between users and crawlers</p>
    <p>Crawlers: very stable, sudden or no shift</p>
    <p>Users: locally noisy, daily regularity, slow shifts</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 10 / 25</p>
  </div>
  <div class="page">
    <p>System: traffic shape detection</p>
    <p>Auto-correlation Analysis: Sample Auto-Correlation function (SAC)</p>
    <p>Crawlers: linear decay, strong correlation at small lags, single</p>
    <p>oscillation, no local spike</p>
    <p>Users: cut-off decay, multiple oscillation, local</p>
    <p>spikes at day lags</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 11 / 25</p>
  </div>
  <div class="page">
    <p>System: traffic shape detection</p>
    <p>Decomposition Analysis: Trend, Season and Noise components (STL)</p>
    <p>Crawlers: stable or square trend, predominant trend</p>
    <p>Users: dispersed trend, predominant season</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 12 / 25</p>
  </div>
  <div class="page">
    <p>System: campaign attribution</p>
    <p>Detection of crawling campaigns by traffic clustering:</p>
    <p>Input: time-series with normalized amplitude and common time origin</p>
    <p>Clustering: - Similarity: Inverse of the squared Euclidean distance - Clustering: + Incremental clustering around series medoids</p>
    <p>+ Cluster generation controlled by a minimal intra-cluster similarity + Candidate clusters selected by medoids of similar amplitude and deviation</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 13 / 25</p>
  </div>
  <div class="page">
    <p>System: campaign attribution</p>
    <p>Time series synchronization</p>
    <p>Distributed crawlers from a same campaign are highly synchronized</p>
    <p>Malicious Crawler A Malicious Crawler B</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 14 / 25</p>
  </div>
  <div class="page">
    <p>System: proactive containment</p>
    <p>Strategy to trigger active responses:</p>
    <p>Reduce the impact of active responses over users while limiting the quantity of information possibly leaked by crawlers</p>
    <p>Minimal volume sources:</p>
    <p>- Traffic: user traffic with high probability - Minimal amount of information possibly leaked</p>
    <p>Above average volume sources:</p>
    <p>- Traffic: traffic coming from crawlers or users behind a proxy - Detected crawlers are blacklisted - Legitimate crawlers and stable proxies are whitelisted</p>
    <p>Low to average volume sources:</p>
    <p>- Traffic: mixed traffic between users and distributed crawlers - Time to collect sufficient traffic to apply classification - Active responses (e.g., CAPTCHAs) to slow down potential leaks</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 15 / 25</p>
  </div>
  <div class="page">
    <p>System: proactive containment</p>
    <p>Rational</p>
    <p>Empirical trade-off between: - the acquired knowledge about this source and - the potential amount of information it may leak - the number of impacted sources</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 16 / 25</p>
  </div>
  <div class="page">
    <p>System: proactive containment</p>
    <p>Rational</p>
    <p>Empirical trade-off between: - the acquired knowledge about this source and - the potential amount of information it may leak - the number of impacted sources</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 17 / 25</p>
  </div>
  <div class="page">
    <p>System: proactive containment</p>
    <p>Rational</p>
    <p>Empirical trade-off between: - the acquired knowledge about this source and - the potential amount of information it may leak - the number of impacted sources</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 18 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: dataset presentation</p>
    <p>Social network traffic logs</p>
    <p>Anonymized traffic from a large social network: public profiles accesses (no authentication required)</p>
    <p>Request logs: time, encrypted source IP, encrypted subnet, user-agent, target URL, server response, referrer, cookie</p>
    <p>Filters: - obvious crawlers (more than 500,000 requests per day) - insufficient traffic (less than 1,000 requests per day)</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 19 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: dataset presentation</p>
    <p>Training and testing sets</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 20 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: training the system</p>
    <p>System configuration</p>
    <p>Configuring heuristic thresholds:</p>
    <p>Accuracy Former features New features Combined features</p>
    <p>Training traffic shape classifiers:</p>
    <p>Accuracy: Bayes Rules SVM Vote</p>
    <p>Cross validation 98.39% 96.36% 98.55% 98.99% Two third split 97.45% 96.19% 95.11% 96.90%</p>
    <p>Tuning the intra-cluster similarity threshold:</p>
    <p>Precision Recall Accuracy</p>
    <p>Tuning the proactive containment strategy:</p>
    <p>Minimal Vol. Sufficient Vol. Impacted IPs Impacted Requests</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 21 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: testing the system</p>
    <p>System evaluation</p>
    <p>Heuristics detection: Accuracy Former features New features Combined features</p>
    <p>Accuracy 38.84% 86.34% 74.19%</p>
    <p>Traffic shape classification:</p>
    <p>Accuracy Bayes Rules SVM Vote</p>
    <p>Global 93.05% 87.55% 94.36% 94.89% Legitimate crawlers 92.54% 87.10% 97.18% 93.95% Unauthorized crawlers 88.89% 96.27% 100.00% 100.00% Masquerading crawlers 98.27% 86.71% 98.84% 98.84% Crawlers (TP/FN) 93.66% 87.68% 97.79% 95.58% Users (TN/FP) 82.50% 85.00% 32.50% 82.50%</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 22 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: testing the system</p>
    <p>System evaluation</p>
    <p>Campaign attribution:</p>
    <p>Precision Recall Accuracy</p>
    <p>Agent #Clust. #ClassC #IP Req/day</p>
    <p>Legitimate crawlers</p>
    <p>Bingbot 5 11 211 6 million Googlebot + Feedfetcher 9 11 113 4 million Yahooslurp 4 9 71 500 thousand Baiduspider 1 1 23 50 thousand Voilabot 3 3 20 19 thousand Facebookexternalhit/1.1 1 1 8 14 thousand</p>
    <p>Crawlers with suspicious agent strings</p>
    <p>2 16 22 330 thousand Python-urllib/1.17 2 51 54 140 thousand Mozilla(compatible;ICS) 1 10 10 70 thousand EventMachine HTTP Client 1 3 3 3 thousand Gogospider 1 2 3 2 thousand</p>
    <p>Masquerading crawlers</p>
    <p>Gecko/200805906 FireFox 1 10 73 350 thousand Gecko/20100101 FireFox 9 12 25 60 thousand MSIE6 NT5.2 TencentTraveler 1 1 30 7 thousand Mozilla(compatible; Mac OS X) 1 1 4 8 thousand googlebot(crawl@google.com) 1 1 4 1 thousand</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 23 / 25</p>
  </div>
  <div class="page">
    <p>Evaluation: system evasion</p>
    <p>Evasion techniques and remediation</p>
    <p>Evasion Potential remediation</p>
    <p>Browser header mimicry traffic-shape detection Traffic shape randomization traffic-shape detection Traffic shape engineering model of user traffic needed Distributed crawling campaign attribution Traffic de-synchronization robust similarity measure</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 24 / 25</p>
  </div>
  <div class="page">
    <p>Conclusion: PUBCRAWL</p>
    <p>Contributions</p>
    <p>Solution to the detection and prevention of crawlers</p>
    <p>Traffic model relying on time-series: - More robust than traditional HTTP field values</p>
    <p>- More robust than simpler statistics over the traffic volume and speed</p>
    <p>Detection based on learning techniques over traffic shape features</p>
    <p>Identification of crawling campaigns by clustering of distributed traffic</p>
    <p>Optimization of the containment strategy according to detection</p>
    <p>Large scale deployment in production at a well-known social network</p>
    <p>G. Jacob (UCSB) Fri Aug 10 2012 25 / 25</p>
  </div>
</Presentation>
