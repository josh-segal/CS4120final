<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Efficient Value-Function Approximation via Online Linear Regression</p>
    <p>Lihong Li Michael L. Littman</p>
    <p>Rutgers Laboratory for Real-Life Reinforcement Learning (RL3) Department of Computer Science</p>
    <p>Rutgers University</p>
    <p>AI &amp; Math, 2008 Fort Lauderdale, FL</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 1/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Overview</p>
    <p>We reduce reinforcement learning w/ linear value-function approx.</p>
    <p>to a form of linear regression.</p>
    <p>Therefore, may study reinforcement learning by studying a simpler problem.</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 2/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Outline</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 3/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Reinforcement Learning (RL)</p>
    <p>RL: A learning paradigm for sequential decision making.</p>
    <p>Lots of Successful Applications TD-Gammon Job-shop scheduling Elevator scheduling Dynamic channel allocation Helicopter control ...</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 4/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Function Approximation (FA) for Scaling Up RL</p>
    <p>Most real-life applications of interests are large in size. Bellmans curse of dimensionality E.g., Gos state space: roughly 31919</p>
    <p>FA employs (non-)parametric compact representations: linear approximation neural networks regression trees locally weighted regression, etc.</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 5/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Exploration/Exploitation Tradeoff</p>
    <p>A Key Difference between RL &amp; Supervised Learning (SL)</p>
    <p>SL: receives IID samples passively RL: has partial control over sample collection</p>
    <p>A tradeoff between Exploitation: behave optimally to maximize utility Exploration: behave suboptimally to rule out mistakes</p>
    <p>Intelligent exploration is key to efficient RL</p>
    <p>The Question We Study How to balance exploration and exploitation when linear approximation is used.</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 6/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>Notation</p>
    <p>Finite-Horizon Markov Decision Process M = S, A, P, R, H, s1:</p>
    <p>S: set of states A: finite set of actions P: transition probabilities R: reward function H: horizon s1: start state</p>
    <p>Policy Deterministic policy : S{1, 2,    , H} 7 A.</p>
    <p>s1 s2</p>
    <p>)1,( 11 sa =</p>
    <p>),( 111 asRr = s3 sH sH+1</p>
    <p>time ),|(~ 112 asPs</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 8/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>Optimal Value Function and Optimal Policy</p>
    <p>Optimal Value Function Q</p>
    <p>Measures the maximum expected cumulative reward: Define QH+1(s, a) = 0 Bellman equation for stage h:</p>
    <p>Qh(s, a) = R(s, a) + EsT (|s,a)[max a</p>
    <p>Qh+1(s , a)]</p>
    <p>expected Q-value in stage h + 1</p>
    <p>Optimal Policy Optimal policy is greedy w.r.t. Q:</p>
    <p>(s, h) = arg max a</p>
    <p>Qh(s, a)</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 9/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>Linear Value Function Approximation</p>
    <p>Representing Q by a table is impractical when S is large</p>
    <p>Linear Approximation</p>
    <p>Representation: Qh(s, a) = wh  (s, a) for given features</p>
    <p>(s, a) = [1(s, a), 2(s, a),    , d (s, a)]  [1, 1]d</p>
    <p>Learning: adapting wh s.t. Qh  Qh Has enjoyed much success</p>
    <p>Where Are Features From Often provided by domain experts May be generated automatically [PPLL07]</p>
    <p>We assume i are given</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 10/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>Exploration with Linear Value Function Approximation</p>
    <p>Practice Often use -greedy, Boltzmann, etc. Little general guidelines for turning parameters Sometimes poor performance</p>
    <p>Theory Open</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 11/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>KWIK Online Linear Regression</p>
    <p>KWIK Protocol At time t = 1, 2, 3,</p>
    <p>ENVIRONMENT picks xt  Rd and yt  [1, 1] AGENT observes xt , and has two options:</p>
    <p>predicts yt  [1, 1], or signals yt =  (meaning I dont know)</p>
    <p>AGENT observes yt</p>
    <p>KWIK vs. Online Learning</p>
    <p>Similarity: no I.I.D. assumption Difference: KWIK is self-aware of prediction accuracy</p>
    <p>KWIK = Knows What It Knows [SL08] Li &amp; Littman Efficient Value-Function Approximation 13/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>MDP and RL KWIK Online Linear Regression</p>
    <p>KWIK Online Linear Regression (II)</p>
    <p>Assumptions</p>
    <p>Bounded-Input: xt  1 Semi-Linearity: , w s.t. |E[yt|xt ]  w  xt|   X</p>
    <p>Y</p>
    <p>y=w*x y=E[Y|x]</p>
    <p>Admissible KWIK Algorithms A0 is admissible if, with prob.  1  ,</p>
    <p>every valid prediction yt is ( + )-close to E[yt|xt ], and # of  is bounded by a polynomial, 0(d ,</p>
    <p>)</p>
    <p>0 may also depend on</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 14/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>The Reduction Theoretical Results</p>
    <p>The Reduction</p>
    <p>Overview Use H weight vectors: Qh(s, a) = wh  (s, a), 1  h  H H copies of A0 are run: A0 in stage h predicts Qh  Qh</p>
    <p>Action Selection Let sh be the state in stage h:</p>
    <p>if A0 predicts  for any (sh, a), then choose a otherwise, act greedily: ah = arg maxa A0(sh, a)</p>
    <p>Weight Update Rule Let ah be chosen in stage h:</p>
    <p>if A0(sh, ah) = q 6= , update wh1 with sample: (sh1, ah1), rh1 + q otherwise, do nothing</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 16/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>The Reduction Theoretical Results</p>
    <p>An Illustration</p>
    <p>S1</p>
    <p>S2</p>
    <p>S3</p>
    <p>q1,L =  q1,R = 2.5</p>
    <p>q2,L = 1.3 q2,R = 1.7</p>
    <p>q3,L = 0.8 q3,R =</p>
    <p>r1 = 0.3</p>
    <p>r2 = 0.9</p>
    <p>r3 = 0.1</p>
    <p>S4</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 17/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>The Reduction Theoretical Results</p>
    <p>Total Number of I Dont Know</p>
    <p>Run A0(h, h) in stage h.</p>
    <p>Theorem (I) Total number of  is at most</p>
    <p>H h=1</p>
    <p>( h  0</p>
    <p>( d ,</p>
    <p>, 1 h</p>
    <p>))</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 19/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>The Reduction Theoretical Results</p>
    <p>Approximation Error Bounds</p>
    <p>Run A0(h, h) in stage h.</p>
    <p>Theorem (II)</p>
    <p>Then with probability at least 1  H</p>
    <p>l=1 l ,</p>
    <p>Qh  Qh  H  H</p>
    <p>l=h</p>
    <p>(l + l )</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 20/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>The Reduction Theoretical Results</p>
    <p>Sample Complexity of Exploration</p>
    <p>Theorem (III) Given sufficiently good features, the policy is -optimal except</p>
    <p>O (</p>
    <p>H 3</p>
    <p>0</p>
    <p>( d ,</p>
    <p>H 3</p>
    <p>, H</p>
    <p>)  log</p>
    <p>) episodes, with probability at least 1  .</p>
    <p>So, the algorithm is PAC-MDP [SLW+06].</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 21/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Discussions</p>
    <p>When Can We Have A0? If the target function is exactly linear (i.e., E[y|x] = w  x), A0 has been found [SL08] But, impossible in general</p>
    <p>An example where the number of  has to be  Open question: milder sufficient conditions to allow existence of A0</p>
    <p>Extensions to -discounted MDPs</p>
    <p>Approximation by O (</p>
    <p>) -horizon MDPs</p>
    <p>Open question: a more efficient way without such an approximation</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 22/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Related Work</p>
    <p>Similar analysis for tabular representation [SLW+06] We generalize it to the linear FA case</p>
    <p>KWIK-based exploration for model-based RL [SL08] Our model-free approach enjoys lower time complexity</p>
    <p>Online learning of linear functions (e.g., [Lon97]) Our problem involves the exploration/exploitation tradeoff</p>
    <p>Associative reinforcement learning (e.g., [Aue02]) Our problem involves sequential decision making</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 23/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Future Work</p>
    <p>Identify (mild) sufficient conditions for existence of A0 More efficient KWIK-based solutions to discounted RL Empirical evaluation</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 24/26</p>
  </div>
  <div class="page">
    <p>Motivation Notation</p>
    <p>Take-Home Messages</p>
    <p>What We Did RL w/ linear FA may be reduced to a form of online linear regression The reduction is provably efficient</p>
    <p>Conclusion We may approach efficient RL via studying a simpler prediction problem</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 25/26</p>
  </div>
  <div class="page">
    <p>Appendix For Further Reading</p>
    <p>References</p>
    <p>Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3:397422, 2002.</p>
    <p>Philip M. Long. On-line evaluation and prediction using linear functions. In Proceedings of the Tenth Annual Conference on Computational Learning Theory (COLT-97), pages 2131, 1997.</p>
    <p>Ronald Parr, Christopher Painter-Wakefield, Lihong Li, and Michael L. Littman. Analyzing feature generation for value-function approximation. In Proceedings of the Twenty-Fourth International Conference on Machine Learning (ICML-07), pages 737744, 2007.</p>
    <p>Alexander L. Strehl and Michael L. Littman. Online linear regression and its application to model-based reinforcement learning. In Advances in Neural Information Processing Systems 20 (NIPS-07), 2008.</p>
    <p>Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC model-free reinforcement learning. In Proceedings of the Twenty-Third International Conference on Machine Learning (ICML-06), pages 881888, 2006.</p>
    <p>Li &amp; Littman Efficient Value-Function Approximation 26/26</p>
  </div>
</Presentation>
