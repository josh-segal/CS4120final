<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>coIOMMU: A Virtual IOMMU with Cooperative DMA Buffer</p>
    <p>Tracking for Efficient Memory Management in Direct I/O</p>
    <p>Kun Tian, Yu Zhang (presenter), Luwei Kang, Yan Zhao, Yaozu Dong</p>
    <p>Intel Corporation</p>
  </div>
  <div class="page">
    <p>Legal Disclaimer</p>
    <p>No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document.</p>
    <p>Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a</p>
    <p>particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in</p>
    <p>trade.</p>
    <p>This document contains information on products, services and/or processes in development. All information provided here is subject to</p>
    <p>change without notice. Contact your Intel representative to obtain the latest forecast, schedule, specifications and roadmaps.</p>
    <p>The products and services described may contain defects or errors known as errata which may cause deviations from published</p>
    <p>specifications. Current characterized errata are available on request.</p>
    <p>Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or</p>
    <p>by visiting www.intel.com/design/literature.htm.</p>
    <p>Intel and the Intel logo are trademarks of Intel Corporation or its subsidiaries in the U.S. and/or other countries.</p>
    <p>*Other names and brands may be claimed as the property of others</p>
    <p>Intel Corporation.</p>
  </div>
  <div class="page">
    <p>Direct I/O</p>
    <p>The best performant I/O virtualization method,</p>
    <p>widely deployed in cloud and data centers.</p>
    <p>Guest directly interacts with I/O devices,</p>
    <p>eliminating the host intervention.</p>
    <p>Hardware IOMMU provides inter-guest</p>
    <p>protection with IOMMU page table (IOPT).</p>
    <p>App</p>
    <p>Device A Device B</p>
    <p>IOMMU</p>
    <p>Hypervisor</p>
    <p>Virtual Machine (0)</p>
    <p>Device</p>
    <p>Driver</p>
    <p>App</p>
    <p>Virtual Machine (n)</p>
    <p>Device</p>
    <p>Driver</p>
    <p>App</p>
    <p>Memory</p>
  </div>
  <div class="page">
    <p>Static Pinning in Direct I/O</p>
    <p>Most devices do not support DMA page fault.</p>
    <p>DMA buffers need be pinned in the IOMMU.</p>
    <p>Hypervisor has no visibility of guest DMA</p>
    <p>activities.</p>
    <p>VM</p>
    <p>Device</p>
    <p>IOMMU</p>
    <p>Hypervisor</p>
  </div>
  <div class="page">
    <p>Static Pinning in Direct I/O</p>
    <p>Pre-allocate and pin the entire guest memory</p>
    <p>before guest DMA starts.</p>
    <p>E.g. at VM creation time.</p>
    <p>VM</p>
    <p>Device</p>
    <p>IOMMU</p>
    <p>Hypervisor</p>
  </div>
  <div class="page">
    <p>The Problem of Static Pinning</p>
    <p>Much increased VM creation time</p>
    <p>Up to 73x longer time observed for a VM with 128GB memory.</p>
    <p>Greatly reduced memory utilization</p>
    <p>Prevent many memory optimizations (overcommitment, late</p>
    <p>allocation, swap, etc.).</p>
    <p>Guest Memory Size (GB)</p>
    <p>No Direct I/O</p>
    <p>Direct I/O</p>
    <p>G u</p>
    <p>e s t</p>
    <p>C re</p>
    <p>a ti</p>
    <p>o n</p>
    <p>T im</p>
    <p>e (</p>
    <p>S e c o</p>
    <p>n d</p>
    <p>s )</p>
    <p>VM creation time increases with guest</p>
    <p>memory size in static pinning.</p>
  </div>
  <div class="page">
    <p>Virtual IOMMU (vIOMMU)</p>
    <p>Primary purpose: intra-guest protection</p>
    <p>E.g. protection with virtual DMA remapping against bogus</p>
    <p>guest drivers.</p>
    <p>Side-effect: fine-grained pinning</p>
    <p>Guest uses vIOMMU to map/unmap DMA buffers.</p>
    <p>vIOMMU requests hypervisor to pin/unpin guest DMA buffers.</p>
    <p>A vIOMMU could be emulated or para-virtualized.</p>
    <p>VM</p>
    <p>Device</p>
    <p>IOMMU</p>
    <p>Map/unmap</p>
    <p>DMA buffers</p>
    <p>Fine-grained</p>
    <p>pinning</p>
    <p>Hypervisor</p>
    <p>DMA</p>
    <p>remapping</p>
    <p>vIOMMU</p>
  </div>
  <div class="page">
    <p>The Problem</p>
    <p>Emulation cost of established vIOMMUs could be significant!</p>
    <p>E.g. 96.6% performance downgrade in memcached through 40Gbps NIC.</p>
    <p>SLA violation if forcing all tenants to turn on vIOMMU.</p>
    <p>Aggressive optimizations may compromise security!</p>
    <p>E.g. side-core emulation [8], map cache [52], etc.</p>
    <p>Security Performance</p>
  </div>
  <div class="page">
    <p>The Reality</p>
    <p>Virtual DMA remapping is disabled in established vIOMMUs by most guest OSes.</p>
    <p>Users may opt in when security requirement is over performance concern.</p>
    <p>E.g. Linux uses passthrough by default, leaving strict/lazy for user opt-in.</p>
    <p>The guest security requirement varies. E.g.</p>
    <p>when an untrusted device is plugged in;</p>
    <p>when a device is assigned to untrusted userspace.</p>
    <p>Established vIOMMUs are not suitable as a reliable way for fine-grained pinning!</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>vIOMMU provides an architectural way for learning guest DMA buffers.</p>
    <p>However, mixing the requirements of protection and pinning, through the same</p>
    <p>costly DMA remapping interface, is needlessly constraining.</p>
    <p>Protection is an OPTIONAL guest-side requirement.</p>
    <p>Fine-grained pinning is a GENERAL host-side requirement.</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Decouple DMA tracking and DMA remapping in vIOMMU.</p>
    <p>vIOMMUDMA Remapping DMA Tracking</p>
    <p>Intra-guest Protection</p>
    <p>Fined-grained Pinning</p>
    <p>Guest</p>
    <p>Hypervisor</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Decouple DMA tracking and DMA remapping in vIOMMU.</p>
    <p>vIOMMUDMA Remapping DMA Tracking</p>
    <p>Intra-guest Protection</p>
    <p>Fined-grained Pinning</p>
    <p>Guest</p>
    <p>Hypervisor</p>
    <p>Goals</p>
    <p>Orthogonal to remapping</p>
    <p>Low cost</p>
    <p>Non-intrusive</p>
    <p>Widely applicable</p>
    <p>Extensible</p>
  </div>
  <div class="page">
    <p>Cooperative DMA Buffer Tracking</p>
    <p>Bi-directional shared DMA buffer information</p>
    <p>To guest  whether a page is pinned in the IOMMU.</p>
    <p>To host  whether a page is mapped for DMA.</p>
    <p>A lightweight tracking interface for fine-grained</p>
    <p>pinning</p>
    <p>Minimize VM-exits when mapping DMA pages</p>
    <p>Eliminate VM-exits when unmapping DMA pages</p>
    <p>Enable flexible host memory management policies</p>
    <p>Guest</p>
    <p>Host</p>
    <p>Shared</p>
    <p>Memory</p>
    <p>Mapping status</p>
    <p>Pinning status</p>
    <p>Updated by</p>
    <p>guest</p>
    <p>Updated by</p>
    <p>host</p>
    <p>Read by guest to decide</p>
    <p>whether a pinning</p>
    <p>request is required</p>
    <p>Read by host to decide</p>
    <p>whether a page can be</p>
    <p>asynchronously</p>
    <p>unpinned</p>
  </div>
  <div class="page">
    <p>coIOMMU Architecture</p>
    <p>DMA Tracking Table (DTT)</p>
    <p>Hold shared DMA buffer info.</p>
    <p>coIOMMU driver</p>
    <p>Hook to guest DMA API layer.</p>
    <p>coIOMMU backend</p>
    <p>DMA remapping engine (remapEngine)</p>
    <p>DMA tracking engine (trackEngine)</p>
    <p>Page pinning manager (pManager)</p>
    <p>Guest</p>
    <p>Device</p>
    <p>IOMMU HW</p>
    <p>System Memory</p>
    <p>IOMMU</p>
    <p>driver</p>
    <p>Host</p>
    <p>trackEngine</p>
    <p>remapEngine</p>
    <p>pManager</p>
    <p>coIOMMU Backend vIOPT</p>
    <p>DTT</p>
    <p>Device</p>
    <p>driver</p>
    <p>DMA</p>
    <p>API</p>
    <p>coIOMMU</p>
    <p>driver</p>
  </div>
  <div class="page">
    <p>coIOMMU Architecture</p>
    <p>remapEngine  Same as established DMA remapping</p>
    <p>interface.</p>
    <p>trackEngine  Holds base address of the DTT.</p>
    <p>Emulates a doorbell register for notifying the host.</p>
    <p>pManager  Implements fine-grained pinning policy.</p>
    <p>Invisible to guest.</p>
    <p>Guest</p>
    <p>Device</p>
    <p>IOMMU HW</p>
    <p>System Memory</p>
    <p>IOMMU</p>
    <p>driver</p>
    <p>Host</p>
    <p>trackEngine</p>
    <p>remapEngine</p>
    <p>pManager</p>
    <p>coIOMMU Backend vIOPT</p>
    <p>DTT</p>
    <p>Device</p>
    <p>driver</p>
    <p>DMA</p>
    <p>API</p>
    <p>coIOMMU</p>
    <p>driver</p>
  </div>
  <div class="page">
    <p>DMA Tracking Table (DTT)</p>
    <p>A multi-level paging structure  Shared between host &amp; guest.</p>
    <p>Indexed by guest page frame numbers (GFNs).</p>
    <p>TU - Tracking Unit for each guest page frame number(GFN)  M (mapped)  set/cleared by guest.</p>
    <p>P (pinned) - set/cleared by host.</p>
    <p>A (accessed)  set by guest, cleared by host.</p>
    <p>Extensible through 5 reserved bits  E.g. add a D (dirty) bit to assist dirty page</p>
    <p>tracking in live migration.</p>
    <p>Reserved</p>
    <p>&lt;&lt;3</p>
    <p>+</p>
    <p>DTT Base Pointer L4 Table</p>
    <p>&lt;&lt;3</p>
    <p>+</p>
    <p>&lt;&lt;3</p>
    <p>+</p>
    <p>L3 Table</p>
    <p>L2 Table</p>
    <p>&lt;&lt;3</p>
    <p>+</p>
    <p>L1 Table</p>
    <p>DTE TU0 TU1 TU2 TU3 TU4 TU5 TU6 TU7</p>
    <p>R R R R R A P M</p>
    <p>Tracking Unit (TU)</p>
    <p>M: mapped P: pinned A: accessed R: Reserved</p>
    <p>Guest Physical Address (GPA)</p>
  </div>
  <div class="page">
    <p>Fine-grained Pinning</p>
    <p>Smart pinning  Instant pinning  pinning must be instantly done before any mapped page is used for DMA.</p>
    <p>Precise notification  only notify the hypervisor for pages not pinned.</p>
    <p>Speculative pinning  pManager speculatively pins frequently used pages.</p>
    <p>Lazy unpinning  Asynchronously done by pManager.</p>
    <p>Only tries to unpin the pages that are no longer mapped.</p>
    <p>Unpinned pages are reclaimable.</p>
  </div>
  <div class="page">
    <p>Guest Mapping Operations</p>
    <p>Instant Pinning</p>
    <p>GuestHost</p>
    <p>trackEngine</p>
    <p>pManager</p>
    <p>DTT coIOMMU</p>
    <p>driver</p>
    <p>Sets mapped flag</p>
    <p>Precise notification</p>
    <p>(only when pinned is 0)</p>
    <p>Up to 99.9992% notifications can be avoided due to</p>
    <p>DMA buffer locality!</p>
    <p>GuestHost</p>
    <p>trackEngine</p>
    <p>pManager</p>
    <p>DTT coIOMMU</p>
    <p>driver</p>
    <p>Clears mapped flag</p>
    <p>No notification is required</p>
    <p>Guest DMA map Guest DMA unmap</p>
  </div>
  <div class="page">
    <p>Lazy Unpinning &amp; Speculative Pinning</p>
    <p>Host asynchronously manages pinning &amp; unpinning in a separate thread.</p>
    <p>GuestHost</p>
    <p>DTT</p>
    <p>coIOMMU</p>
    <p>driverPeriodically</p>
    <p>checks the</p>
    <p>mapped &amp;</p>
    <p>accessed flag</p>
    <p>For pages that are not mapped pManager</p>
    <p>accessed?</p>
    <p>Lazy unpinning Speculative pinning</p>
    <p>N</p>
    <p>Y</p>
  </div>
  <div class="page">
    <p>DMA Tracking vs. DMA Remapping</p>
    <p>When DMA remapping is disabled by guest (the majority case).</p>
    <p>DMA tracking is an efficient solution to achieve fine-grained pinning.</p>
    <p>When DMA remapping is conditionally enabled.</p>
    <p>E.g. only for selective devices (e.g. untrusted), or only in specific period (e.g. when the device is assigned to</p>
    <p>userspace).</p>
    <p>However, hypervisor requires full visibility of guest DMA activities for the entire VM life-cycle.</p>
    <p>In such cases, DMA tracking helps provide a reliable way for fine-grained pinning.</p>
    <p>When DMA remapping is always enabled (for all devices at all times).</p>
    <p>DMA tracking provides a consistent tracking interface as other two categories, with negligible cost.</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Based on KVM/QEMU.</p>
    <p>Extend existing virtual Intel VT-d.  Reused the remapping logic in vIOMMU as remapEngine.</p>
    <p>Developed pManager and trackEngine from scratch.</p>
    <p>Extended guest intel-iommu driver to support DMA tracking.</p>
    <p>Applicable to all kinds of direct I/O usages.  No ad-hoc changes in hardware or device drivers.</p>
    <p>Applicable to other OSes.  As long as a generic DMA API layer is afforded.</p>
    <p>Applicable to other vIOMMUs.  New tracking interface is vendor-agnostic and self-contained.</p>
    <p>New/Changed LOC</p>
    <p>Guest Intel VT-d driver</p>
    <p>Host QEMU</p>
    <p>trackEngine 131 new</p>
    <p>pManager 552 new</p>
    <p>Less than 700 LOC in QEMU.</p>
    <p>Less than 1000 LOC in guest.</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Huge page mappings</p>
    <p>The DTT tracks guest pages in 4KB granularity.</p>
    <p>pManager is optimized to conduct 2MB page pinning by merging continuous guest pages.</p>
    <p>Sub-page mappings</p>
    <p>Multiple DMA buffers may co-locate in the same 4KB guest page (e.g. network packets).</p>
    <p>Guest coIOMMU driver tracks the mapping count of each mapped page.</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Kernel Bypassing</p>
    <p>Kernel bypass APIs require userspace to pre-register a trunk of memory.</p>
    <p>Pre-registered memory is mapped through kernel driver, thus still trackable in coIOMMU.</p>
    <p>Concurrency  coIOMMU must properly handle concurrent pinning/unpinning requests between multiple vCPU</p>
    <p>threads and the unpinning thread.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Evaluation targets</p>
    <p>Performance overhead imposed by coIOMMU.</p>
    <p>Memory footprint in various direct I/O usages.</p>
    <p>The desired performance and security under different intra</p>
    <p>guest protection policies.</p>
    <p>Evaluated modes  coIOMMU vs. virtual VT-d</p>
    <p>Passthrough mode: no DMA remapping</p>
    <p>PT-N (coIOMMU) vs. PT-O (virtual VT-d)</p>
    <p>Strict mode: full protection with DMA remapping</p>
    <p>ST-N (coIOMMU) vs. ST-O (virtual VT-d)</p>
    <p>Lazy mode: relaxed protection with DMA remapping</p>
    <p>LA-N (coIOMMU) vs. LA-O (virtual VT-d)</p>
    <p>mode abbr. DMA</p>
    <p>remapping DMA buffer</p>
    <p>tracking pinning model</p>
    <p>protection</p>
    <p>passthrough (virtual VT-d) PT-O unused n/a static no</p>
    <p>passthrough (coIOMMU) PT-N unused used fine-grained no</p>
    <p>strict (virtual VT-d) ST-O used n/a fine-grained full</p>
    <p>strict (coIOMMU) ST-N used used fine-grained full</p>
    <p>lazy (virtual VT-d) LA-O used n/a fine-grained relaxed</p>
    <p>lazy (coIOMMU) LA-N used used fine-grained relaxed</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Three direct I/O usages: NIC/NVMe/GPU.</p>
    <p>Benchmarks</p>
    <p>Netperf: Aggregated throughput reported, for 16 concurrent Netperf</p>
    <p>instances running stream RX &amp; TX tests.</p>
    <p>Nginx: Requests/second reported, for 16 concurrent requests to</p>
    <p>Nginx server installed in guest.</p>
    <p>Memcached: Requests/second reported, for 16*8 concurrent</p>
    <p>requests to Memcached installed in guest.</p>
    <p>FIO: IO requests/second reported, for 16 concurrent fio threads,</p>
    <p>each performing asynchronous direct random reads to NVMe.</p>
    <p>Open Arena: Frame-per-second (fps) reported as benchmark</p>
    <p>VM configuration</p>
    <p>Direct I/O device vCPU number RAM size</p>
    <p>Intel XL710 40Gbps</p>
    <p>NIC 16 32GB</p>
    <p>Intel 760P series 1TB</p>
    <p>NVMe SSDs 16 32GB</p>
    <p>Intel Iris Plus</p>
    <p>graphics 650 GPU 4 4GB</p>
  </div>
  <div class="page">
    <p>Performance T</p>
    <p>h ro</p>
    <p>u g h</p>
    <p>p u</p>
    <p>t (h</p>
    <p>ig h</p>
    <p>e r</p>
    <p>is b</p>
    <p>e tt</p>
    <p>e r)</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>netperf stream rx (Gbps)</p>
    <p>netperf stream tx (Gbps)</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>nginx (req/sec)</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>memcached (req/sec)</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>fio (iops)</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>P T</p>
    <p>-O P</p>
    <p>T -N</p>
    <p>S T</p>
    <p>-O S T</p>
    <p>-N L A</p>
    <p>-O L A</p>
    <p>-N</p>
    <p>throughput cpu [%]</p>
    <p>C P</p>
    <p>U [%</p>
    <p>] (l</p>
    <p>o w</p>
    <p>e r</p>
    <p>is b</p>
    <p>e tt</p>
    <p>e r)</p>
    <p>openarena (fps)</p>
    <p>No observable performance impact with DMA Tracking!</p>
    <p>(even in mixed netperf/fio scenario  data not shown here)</p>
  </div>
  <div class="page">
    <p>The number of pinned pages sampled in 3 second interval, taken from the beginning of the benchmarks to 6</p>
    <p>seconds after their completion. max indicates the total pages of guest memory.</p>
    <p>Memory Footprint</p>
    <p>Entire 32GB guest memory is statically pinned</p>
    <p>w/o DMA tracking</p>
    <p>Fine-grained pinning with DMA Tracking,</p>
    <p>with only 0.5% of guest memory pinned All four DMA remapping modes pin the</p>
    <p>minimal number of pages.</p>
  </div>
  <div class="page">
    <p>Memory Overcommitment</p>
    <p>Test setup</p>
    <p>Host: 64GB RAM size.</p>
    <p>VM1: 32GB RAM, running sysbench(no assigned device).</p>
    <p>VM2: 48GB RAM, assigned with Intel XL710 40Gbps NIC,</p>
    <p>running Netperf.</p>
    <p>Performance compared with running each benchmark alone.</p>
    <p>Results</p>
    <p>PT-O: Sysbench suffers 25+% performance drop, frequent</p>
    <p>page swaps.</p>
    <p>PT-N: No performance drop, with 49GB free memory.</p>
    <p>The impact of memory overcommitment:</p>
    <p>static pinning (PT-O) vs. fine-grained pinning (PT-N)</p>
  </div>
  <div class="page">
    <p>Guest User Space Driver</p>
    <p>Run DPDK with coIOMMU and with the virtual VT-d respectively.</p>
    <p>virtual VT-d coIOMMU</p>
    <p>Throughput (Gbps) cpu [%]</p>
    <p>virtual VT-d coIOMMU ratio</p>
    <p>Spent Cycles</p>
    <p>Create VM</p>
    <p>Assign NIC</p>
    <p>Deassign NIC</p>
    <p>virtual VT-d coIOMMU ratio</p>
    <p>Pinned Pages</p>
    <p>Before DPDK</p>
    <p>In DPDK</p>
    <p>After DPDK</p>
    <p>No need to allocate and pin the entire guest memory in</p>
    <p>coIOMMU.</p>
    <p>No need to unpin the entire guest memory in coIOMMU.</p>
    <p>Likewise, static-pinning is avoided in coIOMMU when</p>
    <p>assigning NIC back to kernel driver.</p>
    <p>coIOMMU always adapts to the actual DMA buffer</p>
    <p>requirement, while virtual VT-d fails to do so when DMA</p>
    <p>remapping is off.</p>
  </div>
  <div class="page">
    <p>DMA Temporal Locality</p>
    <p>time [seconds]</p>
    <p>ever been used for DMA</p>
    <p>currently pinned for DMA</p>
    <p>T h</p>
    <p>e n</p>
    <p>u m</p>
    <p>b e</p>
    <p>r o</p>
    <p>f p</p>
    <p>a g e</p>
    <p>s</p>
    <p>DMA temporal locality when running Netperf</p>
    <p>with dd</p>
    <p>Test setup</p>
    <p>16 Netperf TX instances ran for 15 minutes.</p>
    <p>dd the virtual disk to /dev/zero, to contend the page</p>
    <p>allocation with the networking stack.</p>
    <p>Conclusion</p>
    <p>DMA temporal locality stays good, even in stressed</p>
    <p>scenario.</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Co-work with DMA page faults</p>
    <p>Help reduce the number of DMA faults by proactively pre-pin hot pages.</p>
    <p>Mitigate non-faultable data paths if a device (e.g. many GPUs) only partially supports DMA page faults.</p>
    <p>Guest cooperation</p>
    <p>A selfish guest may choose to not cooperate, e.g., by deliberately report fake DMA pages.</p>
    <p>A quota mechanism can be applied, based on the service level agreement.</p>
    <p>Support two-level IOMMU address translation.  Hardware optimization to reduce virtual IOTLB invalidations.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Established vIOMMUs cannot reliably eliminate static pinning in direct I/O.</p>
    <p>coIOMMU offers a reliable approach to achieve fine-grained pinning, with a cooperative DMA buffer tracking method.</p>
    <p>coIOMMU  dramatically improves the efficiency of memory management in wide direct I/O usages with negligible cost;</p>
    <p>meanwhile sustains the desired security as required in specific protection usage;</p>
    <p>can be easily applied in various vIOMMU implementations.</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>kevin.tian@intel.com yu.c.zhang@intel.com luwei.kang@intel.com yan.y.zhao@intel.com eddie.dong@intel.com</p>
  </div>
</Presentation>
