<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Weblio Pre-reordering SMT System Zhongyuan Zhu</p>
    <p>(People call me Raphael Shu) @raphaelshu</p>
  </div>
  <div class="page">
    <p>Overview of pre-reordering systems  Reorder input text before translation</p>
    <p>Pre-reordering Decoder</p>
    <p>f</p>
    <p>ef</p>
    <p>John hits a ball</p>
    <p>John va_nsubj a ball va_obj hits</p>
  </div>
  <div class="page">
    <p>Approaches of pre-reordering  Syntactic pre-reordering with parse trees</p>
    <p>- Rule-based - Head-finalization (Isozaki et al., 2010)</p>
    <p>- Supervised learning with word alignments - Automatically learning Rewrite Patterns (Xia and</p>
    <p>McCord, 2004)  Syntactic pre-reordering without parse tree</p>
    <p>- LADER (Neubig et al., 2012)</p>
  </div>
  <div class="page">
    <p>Pre-reordering model in our system</p>
  </div>
  <div class="page">
    <p>Overview of our pre-reordering system</p>
    <p>Tree Restructuring</p>
    <p>f</p>
    <p>CFG</p>
    <p>DEP</p>
    <p>HRCFG</p>
    <p>Reordering</p>
    <p>f</p>
    <p>Input</p>
    <p>CFG Parse Tree</p>
    <p>Dependency Parse Tree</p>
    <p>Head-restructured CFG Parse Tree</p>
    <p>Pre-reordering Result</p>
  </div>
  <div class="page">
    <p>Head-restructured CFG Parse Tree  Problem of CFG parse tree</p>
    <p>- Hard to capture long-distance reordering patterns</p>
    <p>Problem of Dependency parse tree - Fully lexicalized parse tree leads to a</p>
    <p>sparse reordering model</p>
  </div>
  <div class="page">
    <p>Head-restructured CFG Parse Tree  Our approach</p>
    <p>- Restructure a CFG parse tree to inject head information into it</p>
    <p>CFG Parse Tree</p>
    <p>John ball</p>
    <p>hits</p>
    <p>a</p>
    <p>dobjnsubj</p>
    <p>det</p>
    <p>Dependency Parse Tree</p>
    <p>+ Head-restructured CFG Parse Tree</p>
    <p>(HRCFG)</p>
    <p>=</p>
    <p>Head word is always lexicalized</p>
  </div>
  <div class="page">
    <p>Learning reordering model based on LM</p>
    <p>S</p>
    <p>nsubj aux auxp ass</p>
    <p>calcul ated</p>
    <p>prep_ by</p>
    <p>NP PP</p>
    <p>detail s</p>
    <p>can be</p>
    <p>by NP</p>
    <p>DERS</p>
    <p>DERS</p>
    <p>Head-restructured CFG parse tree</p>
    <p>Target-side Sentence</p>
    <p>Alignments</p>
    <p>nsubj prep_by calculated auxpass aux</p>
    <p>Golden order</p>
    <p>NP by</p>
    <p>Extract tag sequences in golden order</p>
    <p>LM</p>
    <p>Train a language model on reordered tag sequences</p>
  </div>
  <div class="page">
    <p>Finding golden order with word alignments  Given a bilingual sentence pair, source-side parse tree and</p>
    <p>word alignments, the golden order of a node layer is defined as</p>
    <p>n1 n2 n3</p>
    <p>t1 1 t1</p>
    <p>w2 3w1</p>
    <p>Non-terminals</p>
    <p>Terminals</p>
    <p>Target-side words</p>
    <p>Alignments</p>
    <p>Average position (Ranked)</p>
    <p>{{ {</p>
    <p>a1 =1 a3 = 2 a2 = 3</p>
    <p>Golden order:</p>
    <p>o = (a1,a2,...,ak)</p>
    <p>Initial order:</p>
    <p>o0 = (1,2,...,k)</p>
    <p>For nodes (n1,n2 ,...,nk )</p>
  </div>
  <div class="page">
    <p>Reordering a input parse tree</p>
    <p>nsubj dobj hits dobj nsubj hits hits nsubj dobj hits dobj nsubj dobj hits nsubj nsubj hits dobj</p>
    <p>nsubj dobj hits</p>
  </div>
  <div class="page">
    <p>N-best reordering</p>
    <p>All 12 possible combinations here</p>
    <p>Selected N-best results by accumulated scores (Cube Pruning is applied in the practice)</p>
    <p>Reordered treelets with LM scores</p>
  </div>
  <div class="page">
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>In-house experiments</p>
    <p>BLEU RIBES 1-best parse + 1 best reorder 34.46 0.7817</p>
    <p>N-best parse + 1 best reorder 34.80 0.7851</p>
    <p>N-best parse + N- best reorder 35.10 0.7887</p>
    <p>For N-best reorder, 10 candidate reordering results are considered.  For N-best parse, 30 candidate parse trees are considered.  We select the final translation by the sum of translation score (given by decoder)</p>
    <p>and the score of pre-reordering.</p>
  </div>
  <div class="page">
    <p>N-best reordering &amp; N-best parse tree inputs</p>
    <p>Incorporating multiple reordering results and parse trees benefits automatic scores.</p>
    <p>BLEU RIBES</p>
  </div>
  <div class="page">
    <p>Official evaluation results</p>
    <p>BLEU RIBES HUMAN</p>
    <p>N-best reorder 34.87 0.7869 +43.25</p>
    <p>N-best reorder + N-best parse 35.04 0.7900 +36.00</p>
    <p>BASELINE PBMT 29.80 0.6919 0.00</p>
  </div>
  <div class="page">
    <p>Official evaluation results</p>
  </div>
  <div class="page">
    <p>Effect of pre-ordering  Identical ordered sentences increases to 15%</p>
    <p>Closer in order</p>
  </div>
  <div class="page">
    <p>Example of pre-reordering</p>
    <p>the life of the improvement va_nsubjpass the practical application of a large problem is .</p>
    <p>Restructured parse tree</p>
    <p>the improvement of the life is a large problem of the practical application. Original input</p>
    <p>Reordered input</p>
    <p>Reference</p>
  </div>
  <div class="page">
    <p>Review  Language model is just a quick solution to</p>
    <p>the reordering problem, sometimes it fails in simple cases. - Sparseness problem</p>
    <p>To gain more from forest input, its necessary to integrate it inside the pre-reordering model.</p>
  </div>
  <div class="page">
    <p>Online demonstrations</p>
    <p>Head-restructured CFG parse tree</p>
    <p>Pre-reordering</p>
    <p>http://raphael.uaca.com/demos/hdtree</p>
    <p>http://raphael.uaca.com/demos/raphreorder</p>
  </div>
  <div class="page">
    <p>Thanks.</p>
  </div>
</Presentation>
