<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Tradeoffs in Scalable Data Routing for Deduplication Clusters Wei Dong, Fred Douglis, Kai Li</p>
    <p>Hugo Patterson, Sazzala Reddy, Philip Shilane</p>
    <p>EMC, Princeton University</p>
  </div>
  <div class="page">
    <p>Short backup window  Exponential growth of data</p>
    <p>Exabyt es</p>
    <p>Data Created Worldwide</p>
    <p>Source: IDC 2008</p>
    <p>Daily backup window</p>
    <p>Increasing Backup Demands</p>
    <p>Capacity and throughput requirements are increasing</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Backup requirements exceed the scale of an individual high-end appliance</p>
    <p>Build a deduplication cluster leveraging existing highthroughput single-node systems</p>
  </div>
  <div class="page">
    <p>Bloom</p>
    <p>Filter FP cache</p>
    <p>Background: Content-Based Chunking</p>
    <p>Smaller chunks  better deduplication  10X to 30X data reduction in typical deployment</p>
    <p>Throughput bottleneck: fingerprint lookups  Stream-Informed Segment Layout: cache locality and disk</p>
    <p>avoidance via containers and Bloom filters [Zhu et al 2008]</p>
    <p>Updates</p>
    <p>Backup 1</p>
    <p>Backup 2</p>
  </div>
  <div class="page">
    <p>?</p>
    <p>Bloom</p>
    <p>Filter FP cache</p>
    <p>Background: Content-Based Chunking</p>
    <p>Smaller chunks  better deduplication  10X to 30X data reduction in typical deployment</p>
    <p>Throughput bottleneck: fingerprint lookups  Stream-Informed Segment Layout: cache locality and disk</p>
    <p>avoidance via containers and Bloom filters [Zhu et al 2008]</p>
    <p>Updates</p>
    <p>Backup 1</p>
    <p>Backup 2</p>
    <p>= duplicates</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Stream-Informed Segment Layout [Zhu et al 2008]  HYDRAStor [Dubnicki et al 2009]</p>
    <p>Routing chunks to nodes according to content  Good performance  Worse deduplication rate due to 64 KB chunks</p>
    <p>Extreme Binning [Bhagwat et al 2009]  Routing files according to minimal chunk hash  Dedupe primarily against earlier version of same file</p>
  </div>
  <div class="page">
    <p>Our Approach: Routing Super-Chunks</p>
    <p>Super-chunks: consecutive groups of chunks  Scalable capacity</p>
    <p>Deduplication node-by-node at chunk level  The same chunk may appear on multiple nodes</p>
    <p>Balanced storage usage across cluster nodes</p>
    <p>Scalable throughput  Locality preserved within a super-chunk  Routing super-chunks provides better throughput than</p>
    <p>routing chunks due to caching effects</p>
  </div>
  <div class="page">
    <p>Architecture and Data Flow</p>
    <p>Backup Server</p>
    <p>Chunks</p>
  </div>
  <div class="page">
    <p>Super-chunks</p>
    <p>Architecture and Data Flow</p>
    <p>A C D E F G H IB IA C D E F G HBB</p>
    <p>Node 1 Node 2 Node 3</p>
    <p>IA C D E F G HBBA</p>
    <p>Backup Server</p>
    <p>: logical bins</p>
  </div>
  <div class="page">
    <p>H GBB</p>
    <p>E</p>
    <p>I</p>
    <p>A</p>
    <p>D</p>
    <p>Architecture and Data Flow</p>
    <p>A C D E F G H IB IA C D E F G HBB</p>
    <p>Bin Manager</p>
    <p>Node 1 Node 2 Node 3</p>
    <p>A</p>
    <p>Backup Server</p>
    <p>F</p>
    <p>C</p>
  </div>
  <div class="page">
    <p>H GBB</p>
    <p>E</p>
    <p>I</p>
    <p>A</p>
    <p>D</p>
    <p>Architecture and Data Flow</p>
    <p>A C D E F G H IB IA C D E F G HBB</p>
    <p>Bin Manager</p>
    <p>Node 1 Node 2 Node 3</p>
    <p>A</p>
    <p>Backup Server</p>
    <p>F</p>
    <p>C</p>
  </div>
  <div class="page">
    <p>Design Questions</p>
    <p>Super-chunk creation: consistent formation in the face of small changes to content</p>
    <p>How to group chunks into super-chunks?  How large should super-chunks be?</p>
    <p>Consistent super-chunk routing to optimize deduplication and load balance</p>
    <p>What super-chunk feature to use?  Whether to use information on previously stored data</p>
    <p>(stateful routing)?</p>
  </div>
  <div class="page">
    <p>Super-Chunk Formation</p>
    <p>Anchoring (similar to chunking)  Produce a feature for each chunk  Insert a boundary when feature matches a pattern</p>
    <p>Choosing the right granularity</p>
    <p>Good dedupe</p>
    <p>Low skew</p>
    <p>Bad dedupe</p>
    <p>High skew</p>
    <p>Low Throughput High Throughput</p>
    <p>Sweet Spot</p>
  </div>
  <div class="page">
    <p>Stateless Super-Chunk Routing</p>
    <p>Super Chunk</p>
    <p>Chunk Features</p>
    <p>Super-Chunk Features 20 (first) or 4 (min)</p>
    <p>Bin ID Node ID (w/ Migration)</p>
    <p>Hash of</p>
    <p>first 64 bytes hash(64)</p>
    <p>whole chunk hash(*)</p>
    <p>mod N</p>
    <p>* Real chunk features are full 32-bit integers</p>
  </div>
  <div class="page">
    <p>Load Balancing with Bin Migration</p>
    <p>Migrate when a node exceeds 105% of average disk usage</p>
    <p># bins &gt;&gt; # nodes, so bins are usually small  Limitation</p>
    <p>If a bin becomes much larger than an average node, even migration cannot balance things out!</p>
  </div>
  <div class="page">
    <p>Stateful Super-Chunk Routing</p>
    <p>Goal: improve deduplication while avoiding skew  Approach</p>
    <p>Route super-chunk to node where it matches the best  Avoid flooding a node by matching some chunks but adding</p>
    <p>more and more new chunks</p>
    <p>Algorithm 1. Count how many chunks are matched on each node 2. Discount matches of heavily loaded nodes 3. Route to the one that is significantly better than others</p>
    <p>Or if no clear winner, fall back to stateless routing</p>
    <p>Bin migration no longer needed!</p>
  </div>
  <div class="page">
    <p>Tracking Content of Each Node</p>
    <p>Maintain a Bloom filter for each node  Option 1: Backup server has all Bloom filters</p>
    <p>Memory overhead  Option 2: Use Bloom filters on each backend node</p>
    <p>Communication overhead</p>
    <p>Reduce overhead with sampling (rate = 1/8)</p>
  </div>
  <div class="page">
    <p>Evaluation Goals</p>
    <p>What is the best super-chunk feature &amp; granularity?  Impact on deduplication  Impact on performance</p>
    <p>Is stateless or stateful routing preferable?  How does bin migration help?</p>
  </div>
  <div class="page">
    <p>Datasets</p>
    <p>Trace data collected from production environment  3 large backup sets with multiple data types  5 smaller ones with individual data types  1 synthesized by blending the 5 smaller ones</p>
    <p>Size (TB)</p>
  </div>
  <div class="page">
    <p>Evaluation Metrics</p>
    <p>Capacity (local compression ignored)</p>
    <p>ED normalized for easy comparison</p>
    <p>Throughput  # of on-disk fingerprint index lookups</p>
    <p>total dedupe = original size dedupe size</p>
    <p>data skew = max node utilization avg node utilization</p>
    <p>effective dedupe (ED) = total dedupe data skew</p>
  </div>
  <div class="page">
    <p>Overall Effectiveness  Hash(64) works well for small clusters  Bin migration helps to reduce skew and increase ED  Stateful routing offers further improvements</p>
    <p>Representative</p>
    <p>dataset</p>
  </div>
  <div class="page">
    <p>Exchange: Anomalous Dataset</p>
    <p>A frequent pattern in data happens to qualify as superchunk boundary and routing feature</p>
    <p>A bin becomes much larger than an average node</p>
  </div>
  <div class="page">
    <p>Overall Effectiveness</p>
  </div>
  <div class="page">
    <p>Overall Effectiveness real-world</p>
    <p>deployments</p>
    <p>Home dirs skew at</p>
  </div>
  <div class="page">
    <p>Feature Selection</p>
    <p>All super-chunk routing methods are similarly effective  First of Hash(64) is preferred due to simplicity  HYDRAstor: routing 64K chunks (w/o super-chunks)</p>
  </div>
  <div class="page">
    <p>Feature Selection</p>
    <p>All super-chunk routing methods are similarly effective  First of Hash(64) is preferred due to simplicity  HYDRAstor: routing 64K chunks (w/o super-chunks)  Exchange is the extreme case with large clusters</p>
  </div>
  <div class="page">
    <p>Cache Locality and Throughput</p>
    <p>Throughput measured by max # fingerprint lookup  Lower value suggests higher throughput</p>
    <p>Larger super-chunk  better throughput</p>
    <p>Throughput</p>
    <p>studies</p>
    <p>performed</p>
    <p>@ 32</p>
    <p>nodes</p>
  </div>
  <div class="page">
    <p>Cache Locality and Throughput</p>
    <p>Exchange: anomaly again</p>
    <p>Logical Skew: max(size before dedupe) / avg (size before dedupe)</p>
  </div>
  <div class="page">
    <p>Effect of Bin Migration  Deduplication improves after migration but may drop between</p>
    <p>migration intervals</p>
  </div>
  <div class="page">
    <p>Product: Global Deduplication Array</p>
    <p>Two-node configuration, each with  2x6 cores @ 2.8GHz, 96 GB memory, 10-Gb</p>
    <p>Ethernet  285 TB storage in 12 shelves, 2+2 RAID-6</p>
    <p>Performance numbers  Logical capacity: 11.4PB (with 20x dedupe)  Write throughput: 7.3 GB/s</p>
    <p>Prototype scaled performance linearly from 1-4 nodes</p>
    <p>Recently upgraded, different from paper</p>
  </div>
  <div class="page">
    <p>Super-chunk routing for building deduplication clusters  Scalable throughput  Maximizing deduplication</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Stateless Stateful</p>
    <p>Small clusters</p>
    <p>Large clusters</p>
    <p>All</p>
    <p>Deduplication</p>
    <p>Data Skew</p>
    <p>Migration Cost</p>
    <p>Overhead</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Investigate conditions causing data skew  Examine scalability over broad range of clusters  Support cluster reconfiguration with bin migration  Build a prototype cluster with stateful routing</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>Thanks to the many engineers in EMC Data Domain who created the GDA product</p>
    <p>Thanks to our shepherd, Cristian Ungureanu, for his extensive comments, and the many others who provided feedback or helped with experiments: Dhanabal Ekambaram, Paul Jardetzky, Ed Lee, Dheer Moghe, Naveen Rastogi, Pratap Singh, and Grant Wallace</p>
  </div>
  <div class="page"/>
</Presentation>
