<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Analysis of Network-Partitioning Failures in Cloud Systems</p>
    <p>Ahmed Alquraan, Hatem Takruri, Mohammed Alfatafta, Samer Al-Kiswany</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>Network-partitioning failures are catastrophic, silent, and deterministic</p>
    <p>Surprisingly, partial partitions cause large number of failures</p>
    <p>Debunk two common presumptions 1. Admins believe that systems can tolerate network partitions 2. Designers believe isolating one side of the partition is enough</p>
    <p>NEAT: a network partitioning testing framework  Tested 7 systems  32 failures</p>
  </div>
  <div class="page">
    <p>Motivation  High availability: systems should tolerate infrastructure failures</p>
    <p>(Devices, nodes, network, data centers)</p>
    <p>We focus on network partitioning  Partitioning faults are common</p>
    <p>(once every two weeks at Google[1], 70% of downtime at Microsoft[2], once every 4 days at CENIC[3])</p>
    <p>Complex to handle</p>
    <p>What is the impact of network partitions on modern systems?</p>
    <p>[1] Govindan et al, &quot;Evolve or Die: High-Availability Design Principles Drawn from Googles Network Infrastructure, ACM SIGCOMM 2016 [2] Gill et al, Understanding network failures in data centers: measurement, analysis, and implications, ACM SIGCOMM 2011 [3] Turner et al, California fault lines: understanding the causes and impact of network failures, ACM SIGCOMM 2010</p>
  </div>
  <div class="page">
    <p>In-depth analysis of production failures</p>
    <p>Studied end-to-end failure sequence</p>
    <p>Study the impact of failures</p>
    <p>Characterize conditions and sequence of events</p>
    <p>Identify opportunities to improve fault tolerance</p>
    <p>New system configuration</p>
    <p>Network Partition System reaction (Leader election, reconfig, )</p>
    <p>Failure Visible to users</p>
    <p>User workload</p>
  </div>
  <div class="page">
    <p>Studied 136 high-impact network-partitioning failures from 25 systems  104 failures are user-reported failures</p>
    <p>32 failures are discovered by NEAT</p>
    <p>Studied failure report, discussion, logs, code, and tests</p>
    <p>Reproduced 24 failures to understand intricate details</p>
    <p>Methodology</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>Network partitioning failures are catastrophic, silent, and easy to manifest</p>
    <p>Surprisingly, partial partitions cause large number of failures</p>
    <p>Debunk two common presumptions 1. Admins believe that systems can tolerate network partitions 2. Designers believe isolating one side of the partition is enough</p>
    <p>NEAT: a network partitioning testing framework  Tested 7 systems  32 failures</p>
  </div>
  <div class="page">
    <p>Example  Dirty read in VoltDB</p>
    <p>Master Replica</p>
    <p>Network partition</p>
    <p>read (key)</p>
    <p>key = Y ReplicaMaster</p>
    <p>Dirty read</p>
    <p>Update locally</p>
    <p>Leader election</p>
    <p>Event1: Network partition</p>
    <p>Event2: Write to minority</p>
    <p>Event3: Read from minority</p>
    <p>key X key Xkey Xkey Y</p>
  </div>
  <div class="page">
    <p>Event 1: Network partition</p>
    <p>Event 2: Write to minority</p>
    <p>Event 3: Read from minorityMajority (80%) of the failures are catastrophic</p>
    <p>Catastrophic failure  Data loss, dirty read, broken locks,</p>
    <p>double dequeue, corruption</p>
    <p>Majority (90%) of the failures are silent</p>
    <p>Dirty read</p>
    <p>Master Replica</p>
    <p>Network partition</p>
    <p>read (key)</p>
    <p>key = Y</p>
    <p>ReplicaMaster</p>
    <p>Update locally</p>
    <p>key Y key X</p>
    <p>Failure impact</p>
  </div>
  <div class="page">
    <p>Surprisingly, partition failures are deterministic, silent, and catastrophic</p>
    <p>Dirty read</p>
    <p>Master Replica</p>
    <p>Network partition</p>
    <p>read (key)</p>
    <p>key = Y</p>
    <p>ReplicaMaster</p>
    <p>Update locally</p>
    <p>Require 3 events</p>
    <p>Timing: should occur before the old master shuts down</p>
    <p>Old master shuts down</p>
    <p>key Y key X</p>
    <p>Multiple events should happen in a specific order</p>
    <p>Majority (80%) are deterministic or have known timing constraints</p>
    <p>Timing and ordering</p>
    <p>ti m</p>
    <p>e o</p>
    <p>u t</p>
    <p>Event 1: Network partition</p>
    <p>Event 2: Write to minority</p>
    <p>Event 3: Read from minority</p>
  </div>
  <div class="page">
    <p>Configuration change</p>
    <p>Data consolidation</p>
    <p>Request routing</p>
    <p>Replication protocol</p>
    <p>Two leaders</p>
    <p>Bad leader</p>
    <p>Double voting</p>
    <p>Conflicting election</p>
    <p>Leader election</p>
    <p>Others</p>
    <p>Failures 59% of the failures are due to design flaws</p>
    <p>Early design reviews can help  High-impact area that needs</p>
    <p>further research</p>
    <p>Failure source</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>Network partitioning failures are catastrophic, silent, and easy to manifest</p>
    <p>Surprisingly, partial partitions cause large number of failures</p>
    <p>Debunk two common presumptions 1. Admins believe that systems can tolerate network partitions 2. Designers believe isolating one side of the partition is enough</p>
    <p>NEAT: a network partitioning testing framework  Tested 7 systems  32 failures</p>
  </div>
  <div class="page">
    <p>Group 1 Group 2</p>
    <p>Group 3</p>
    <p>Network partition</p>
    <p>Partial network partitioning</p>
    <p>Network partition types  Complete  Partial  Simplex</p>
  </div>
  <div class="page">
    <p>Partial network partition - double execution in MapReduce</p>
    <p>Task</p>
    <p>Resource Manager</p>
    <p>User</p>
    <p>NodeMgr</p>
    <p>NodeMgr</p>
    <p>AppMaster</p>
    <p>NodeMgr</p>
    <p>start AppMaster</p>
  </div>
  <div class="page">
    <p>Partial network partition - double execution in MapReduce</p>
    <p>Resource Manager</p>
    <p>User</p>
    <p>AppMaster</p>
    <p>NodeMgr</p>
    <p>NodeMgr</p>
    <p>Partition</p>
    <p>Start Another AppMaster</p>
    <p>AppMaster</p>
    <p>Double execution and data corruption</p>
    <p>NodeMgr</p>
    <p>AppMaster has failed</p>
  </div>
  <div class="page">
    <p>Partial network partition - double execution in MapReduce</p>
    <p>Resource Manager</p>
    <p>User</p>
    <p>AppMaster</p>
    <p>NodeMgr</p>
    <p>NodeMgr</p>
    <p>AppMaster</p>
    <p>Double execution and data corruption  Confuses the user</p>
    <p>NodeMgr</p>
    <p>Partition</p>
  </div>
  <div class="page">
    <p>Leads to inconsistent view of system state  Partial partitions are poorly understood and tested</p>
    <p>Partial partitioning leads to 28% of the failures</p>
    <p>Partial network partitioning</p>
    <p>Affects leader election, scheduling, data placement, and configuration change</p>
    <p>Group 1 Group 2</p>
    <p>Group 3</p>
    <p>Network partition</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>Network partitioning failures are catastrophic, silent, and easy to manifest</p>
    <p>Surprisingly, partial partitions cause large number of failures</p>
    <p>Debunk two common presumptions 1. Admins believe that systems can tolerate network partitions 2. Designers believe isolating one side of the partition is enough</p>
    <p>NEAT: a network partitioning testing framework  Tested 7 systems  32 failures</p>
  </div>
  <div class="page">
    <p>Debunks two presumptions</p>
    <p>Admins believe systems with data redundancy can tolerate partitioning</p>
    <p>Action: low priority for repairing ToR switches[1]</p>
    <p>Systems restrict client access to one side to eliminate failures</p>
    <p>Reality: 83% of the failures occur by isolating a single node</p>
    <p>Reality: 64% of the failures require no client access or access to one side only</p>
  </div>
  <div class="page">
    <p>Other findings</p>
    <p>Failures in proven protocols are due to optimizations</p>
    <p>Majority (83%) of the failures can be reproduced with 3 nodes</p>
    <p>Majority (93%) of the failures can be reproduced through tests</p>
  </div>
  <div class="page">
    <p>Highlights</p>
    <p>Network partitioning failures are catastrophic, silent, and easy to manifest</p>
    <p>Surprisingly, partial partitions cause large number of failures</p>
    <p>Debunk two common presumptions 1. Admins believe that systems can tolerate network partitions 2. Designers believe isolating one side of the partition is enough</p>
    <p>NEAT: a network partitioning testing framework  Tested 7 systems  32 failures</p>
  </div>
  <div class="page">
    <p>NEtwork pArtitioning Testing framework (NEAT)</p>
    <p>Supports all types of network partitions</p>
    <p>Simple API</p>
    <p>client1.createSemaphore(1)</p>
    <p>side1 = asList(S1, S2, client1);</p>
    <p>side2 = asList(S3, client2);</p>
    <p>netPart = Partitioner.complete(side1, side2);</p>
    <p>assertTrue(client1.sem_trywait());</p>
    <p>assertFalse(client2.sem_trywait());</p>
    <p>Partitioner.heal(netPart);</p>
    <p>S2 S3S1</p>
    <p>Client1 Client2</p>
    <p>Network partition</p>
    <p>acquire() acquire()</p>
    <p>Apache Ignite double locking failure</p>
  </div>
  <div class="page">
    <p>NEAT design</p>
    <p>Client 1 Client 2</p>
    <p>Server 1</p>
    <p>Server 2</p>
    <p>Server 3</p>
    <p>Test Engine</p>
    <p>N e</p>
    <p>t P</p>
    <p>a rt</p>
    <p>it io</p>
    <p>n e</p>
    <p>r Run target system</p>
    <p>Issue client operations</p>
    <p>C li</p>
    <p>e n</p>
    <p>t D</p>
    <p>ri ve</p>
    <p>r</p>
    <p>Orders client operations  Injects and heals partitions</p>
    <p>OpenFlow  iptables</p>
  </div>
  <div class="page">
    <p>Testing with NEAT</p>
    <p>We tested 7 systems using NEAT</p>
    <p>Discovered 32 failures  30 catastrophic  Confirmed: 12</p>
    <p>System # failures found</p>
    <p>ActiveMQ 2 Ceph 2 Ignite 15</p>
    <p>Infinispan 1 Terracotta 9 MooseFS 2</p>
    <p>DKron 1</p>
  </div>
  <div class="page">
    <p>Concluding remarks</p>
    <p>Further research is needed for network partition fault tolerance</p>
    <p>Specially partial partitions</p>
    <p>Highlight the danger of using unreachability as an indicator of node crash</p>
    <p>Identify ordering, timing, network characteristics to simplify testing</p>
    <p>Identify common pitfalls for developers and admins</p>
    <p>NEAT: network partitioning testing framework</p>
    <p>https://dsl.uwaterloo.ca/projects/neat/ 24</p>
  </div>
</Presentation>
