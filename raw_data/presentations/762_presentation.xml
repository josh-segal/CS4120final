<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>volley: automated data placement for geo-distributed cloud services</p>
    <p>sharad agarwal, john dunagan, navendu jain, stefan saroiu, alec wolman, harbinder bhogan</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 2 4/29/2010</p>
    <p>very rapid pace of datacenter rollout</p>
    <p>April 2007  Microsoft opens DC in Quincy, WA</p>
    <p>September 2008  Microsoft opens DC in San Antonio, TX</p>
    <p>July 2009  Microsoft opens DC in Dublin, Ireland</p>
    <p>July 2009  Microsoft opens DC in Chicago, IL</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 3 4/29/2010</p>
    <p>geo-distribution is here</p>
    <p>major cloud providers have tens of DCs today that are geographically dispersed  cloud service operators want to leverage multiple DCs to serve each user from best DC</p>
    <p>user wants lower latency</p>
    <p>cloud service operator wants to limit cost  two major sources of cost: inter-DC traffic and provisioned capacity in each DC</p>
    <p>if your service hosts dynamic data (e.g. frequently updated wall in social networking), and cost is a major concern  partitioning data across DCs is attractive because you dont consume inter-DC WAN traffic for replication</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 4 4/29/2010</p>
    <p>research contribution</p>
    <p>major unmet challenge: automatically placing user data or other dynamic application state  considering both user latency and service operator cost, at cloud scale</p>
    <p>we show: can do a good job of reducing both user latency and operator cost</p>
    <p>our research contribution  define this problem</p>
    <p>devise algorithm and implement system that outperforms heuristics we consider in our evaluation</p>
    <p>exciting challenge  scale: O(100million) data items</p>
    <p>need practical solution that also addresses costs that operators face</p>
    <p>important for multiple cloud services today; trends indicate many more services with dynamic data sharing</p>
    <p>all the major cloud providers are building out geo-distributed infrastructure</p>
  </div>
  <div class="page">
    <p>overview how do users share data? volley evaluation</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 6 4/29/2010</p>
    <p>data sharing is common in cloud services</p>
    <p>many can be modeled as pub-sub  social networking</p>
    <p>Facebook, LinkedIn, Twitter, Live Messenger</p>
    <p>business productivity</p>
    <p>MS Office Online, MS Sharepoint, Google Docs</p>
    <p>Live Messenger  instant messaging application</p>
    <p>O(100 million) users</p>
    <p>O(10 billion) conversations / month</p>
    <p>Live Mesh  cloud storage, file synchronization, file sharing, remote access</p>
    <p>john</p>
    <p>johns</p>
    <p>wall</p>
    <p>johns</p>
    <p>news feed</p>
    <p>sharad</p>
    <p>sharads</p>
    <p>wall</p>
    <p>sharads</p>
    <p>news feed</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 7 4/29/2010</p>
    <p>PLACING ALL DATA ITEMS IN ONE PLACE IS REALLY BAD FOR LATENCY</p>
    <p>users scattered geographically (Live Messenger)</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 8 4/29/2010</p>
    <p>ALGORITHM NEEDS TO HANDLE USER LOCATIONS THAT CAN VARY</p>
    <p>users travel</p>
    <p>% o</p>
    <p>f d</p>
    <p>e v ic</p>
    <p>e s o</p>
    <p>r u</p>
    <p>s e rs</p>
    <p>max distance from centroid (x1000 miles)</p>
    <p>% of Mesh devices</p>
    <p>% of Messenger users</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 9 4/29/2010</p>
    <p>ALGORITHM NEEDS TO HANDLE DATA ITEMS THAT ARE ACCESSED AT SAME TIME BY USERS IN DIFFERENT LOCATIONS</p>
    <p>users share data across geographic distances</p>
    <p>% o</p>
    <p>f in</p>
    <p>s ta</p>
    <p>n c e s</p>
    <p>distance from device to sharing centroid (x1000 miles)</p>
    <p>% of Messenger conversations</p>
    <p>% of Mesh notification sessions</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 10 4/29/2010</p>
    <p>sharing of data makes partitioning difficult</p>
    <p>data placement is challenging because  complex graph of data inter-dependencies</p>
    <p>users scattered geographically</p>
    <p>data sharing across large geographic distances</p>
    <p>user behavior changes, travels or migrates</p>
    <p>application evolves over time</p>
    <p>john</p>
    <p>johns</p>
    <p>wall</p>
    <p>johns</p>
    <p>news feed</p>
    <p>sharad</p>
    <p>sharads</p>
    <p>wall</p>
    <p>sharads</p>
    <p>news feed</p>
  </div>
  <div class="page">
    <p>overview how do users share data? volley evaluation</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 12 4/29/2010</p>
    <p>DC Z</p>
    <p>DC Y DC X</p>
    <p>simple example</p>
    <p>transaction1: user updates wall A with two subscribers C,D  IP1  A</p>
    <p>A  C</p>
    <p>A  D</p>
    <p>transaction2: user updates wall A with one subscriber C  IP1  A</p>
    <p>A  C</p>
    <p>transaction3: user updates wall B with one subscriber D  IP2,  B</p>
    <p>B  D</p>
    <p>IP 2</p>
    <p>data B</p>
    <p>data C</p>
    <p>IP 1</p>
    <p>data A</p>
    <p>data D</p>
    <p>frequency of operations</p>
    <p>can be weighted by importance</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 13 4/29/2010</p>
    <p>proven algorithms do not apply to this problem</p>
    <p>how to partition this graph among DCs while considering  latency of transactions (impacted by distance between users and dependent data)</p>
    <p>WAN bandwidth (edges cut between dependent data)</p>
    <p>DC capacity (size of subgraphs)</p>
    <p>sparse cut algorithms  models data-data edges</p>
    <p>but not clear how to incorporate users, location / distance</p>
    <p>facility location  better fit than sparse cut and models users-data edges</p>
    <p>but not clear how to incorporate edges and edge costs between data items</p>
    <p>standard commercial optimization packages  can formulate as an optimization</p>
    <p>but dont know how to scale to O(100 million) objects</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 14 4/29/2010</p>
    <p>instead, we design a heuristic</p>
    <p>want heuristic that allows a highly parallelizable implementation  to handle huge scales of modern cloud services</p>
    <p>many cloud services centralize logs into large compute clusters, e.g. Hadoop, Map-Reduce, Cosmos</p>
    <p>use logs to build a fully populated graph  fixed nodes are IP addresses from which client transactions originated</p>
    <p>data items are nodes that can move anywhere on the planet (Earth)</p>
    <p>pull together or mutually attract nodes that frequently interact  reduces latency, and if co-located, will also reduce inter-DC traffic</p>
    <p>fixed nodes prevent all nodes from collapsing onto one point</p>
    <p>not knowing optimal algorithm, we rely on iterative improvement  but iterative algorithms can take a long time to converge</p>
    <p>starting at a reasonable location can reduce search space, number of iterations, job completion time</p>
    <p>constants in update at each iteration will determine convergence</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 15 4/29/2010</p>
    <p>volley algorithm</p>
    <p>phase1: calculate geographic centroid for each data  considering client locations, ignoring data inter-dependencies</p>
    <p>highly parallel</p>
    <p>phase2: refine centroid for each data iteratively  considering client locations, and data inter-dependencies</p>
    <p>using weighted spring model that attracts data items</p>
    <p>but on a spherical coordinate system</p>
    <p>phase3: confine centroids to individual DCs  iteratively roll over least-used data in over-subscribed DCs</p>
    <p>(as many iterations as number of DCs is enough in practice)</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 16 4/29/2010</p>
    <p>volley system overview</p>
    <p>consumes network cost model, DC capacity and locations, and request logs  most apps store this, but require custom translations</p>
    <p>request log record</p>
    <p>timestamp, source entity, destination entity, request size (B), transaction ID</p>
    <p>entity can be client IP address or another data items GUID</p>
    <p>runs on large compute cluster with distributed file system</p>
    <p>hands placement to app-specific migration mechanism  allows Volley to be used by many apps</p>
    <p>computing placement on 1 week  16 wall-clock hours</p>
    <p>10 phase-2 iterations</p>
    <p>400 machine-hours of work</p>
    <p>app servers</p>
    <p>in DC n</p>
    <p>Cosmos store</p>
    <p>in DC y</p>
    <p>Volley</p>
    <p>analysis job</p>
    <p>app-specific</p>
    <p>migration</p>
    <p>mechanism</p>
    <p>app servers</p>
    <p>in DC 2</p>
    <p>app servers</p>
    <p>in DC 1</p>
  </div>
  <div class="page">
    <p>overview how do users share data? volley evaluation</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 18 4/29/2010</p>
    <p>methodology</p>
    <p>inputs  Live Mesh traces from June 2009</p>
    <p>compute placement on week 1, evaluate placement on weeks 2,3,4</p>
    <p>12 geographically diverse DC locations (where we had servers)</p>
    <p>evaluation  analytic evaluation using latency model (Agarwal SIGCOMM09)</p>
    <p>based on 49.9 million measurements across 3.5 million end-hosts</p>
    <p>live experiments using Planetlab clients</p>
    <p>metrics  latency of user transactions</p>
    <p>inter-DC traffic: how many messages between data in different DCs</p>
    <p>DC utilization: e.g. no more than 10% of data in each of 12 DCs</p>
    <p>staleness: how long is the placement good for?</p>
    <p>frequency of migration: how much data migrated and how often?</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 19 4/29/2010</p>
    <p>other heuristics for comparison</p>
    <p>hash  static, random mapping of data to DCs</p>
    <p>optimizes for meeting any capacity constraint for each DC</p>
    <p>oneDC  place all data in one DC</p>
    <p>optimizes for minimizing (zero) traffic between DCs</p>
    <p>commonIP  pick DC closest to IP that most frequently uses data</p>
    <p>optimizes for latency by keeping data items close to user</p>
    <p>firstIP  (didnt work as well as commonIP)</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 20 4/29/2010</p>
    <p>INCLUDES SERVER-SERVER (SAME DC OR CROSS-DC) AND SERVER-USER</p>
    <p>user transaction latency (analytic evaluation)</p>
    <p>u s e r</p>
    <p>tr a n</p>
    <p>s a c ti</p>
    <p>o n</p>
    <p>l a te</p>
    <p>n c y (</p>
    <p>m s )</p>
    <p>percentile of total user transactions</p>
    <p>hash oneDC commonIP volley</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 21 4/29/2010</p>
    <p>WAN TRAFFIC IS A MAJOR SOURCE OF COST FOR OPERATORS</p>
    <p>inter-DC traffic (analytic evaluation)</p>
    <p>oneDC</p>
    <p>hash</p>
    <p>commonIP</p>
    <p>volley</p>
    <p>fraction of messages that are inter-DC</p>
    <p>p la</p>
    <p>c e m</p>
    <p>e n</p>
    <p>t</p>
    <p>real money</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 22 4/29/2010</p>
    <p>COMPARED TO FIRST WEEK</p>
    <p>how many objects are migrated every week</p>
    <p>week2 week3 week4</p>
    <p>p e rc</p>
    <p>e n</p>
    <p>ta g</p>
    <p>e o</p>
    <p>f o</p>
    <p>b je</p>
    <p>c ts</p>
    <p>old objects with</p>
    <p>different placement</p>
    <p>old objects with</p>
    <p>same placement</p>
    <p>new objects</p>
  </div>
  <div class="page">
    <p>sharad.agarwal@microsoft.com PAGE 23 4/29/2010</p>
    <p>summary</p>
    <p>Volleys data partitioning  simultaneously reduces user latency and operator cost</p>
    <p>reduces datacenter capacity skew by over 2X</p>
    <p>reduces inter-DC traffic by over 1.8X</p>
    <p>reduces user latency by 30% at 75th percentile</p>
    <p>runs in under 16 clock-hours for 400 machine-hours computation across 1 week of traces</p>
    <p>Volley solves a real, increasingly important need  partitioning user data or other application state across DCs</p>
    <p>simultaneously reducing operator cost and user latency</p>
    <p>more cloud services built around sharing data between users (both friends &amp; employees)</p>
    <p>cloud providers continue to deploy more DCs</p>
  </div>
  <div class="page">
    <p>thanks!</p>
    <p>sharad agarwal john dunagan navendu jain stefan saroiu alec wolman harbinder bhogan</p>
  </div>
</Presentation>
