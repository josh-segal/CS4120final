<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Dynamic Optimization and Learning for Renewal Systems</p>
    <p>Michael J. Neely, University of Southern California Asilomar Conference on Signals, Systems, and Computers, Nov. 2010</p>
    <p>PDF of paper at: http://ee.usc.edu/stochastic-nets/docs/renewal-systems-asilomar2010.pdf Sponsored in part by the NSF Career CCF-0747525, ARL Network Science Collaborative Tech. Alliance</p>
    <p>t T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R Network</p>
    <p>Coordinator Network</p>
    <p>Coordinator</p>
    <p>Task 1Task 1</p>
    <p>Task 2Task 2</p>
    <p>Task 3Task 3</p>
    <p>T[0] T[1] T[2]</p>
  </div>
  <div class="page">
    <p>A General Renewal System</p>
    <p>t T[0] T[1] T[2]</p>
    <p>y[2]y[1]y[0]</p>
    <p>Renewal Frames r in {0, 1, 2, }. [r] = Policy chosen on frame r. P = Abstract policy space ([r] in P for all r). Policy [r] affects frame size and penalty vector on frame r.</p>
    <p>[r] y[r] = [y0([r]), y1([r]), , yL([r])]</p>
    <p>T[r] = T([r]) = Frame Duration</p>
  </div>
  <div class="page">
    <p>A General Renewal System</p>
    <p>t T[0] T[1] T[2]</p>
    <p>y[2]y[1]y[0]</p>
    <p>Renewal Frames r in {0, 1, 2, }. [r] = Policy chosen on frame r. P = Abstract policy space ([r] in P for all r). Policy [r] affects frame size and penalty vector on frame r. These are random functions of [r] (distribution depends on [r]):</p>
    <p>[r] y[r] = [y0([r]), y1([r]), , yL([r])]</p>
    <p>T[r] = T([r]) = Frame Duration</p>
  </div>
  <div class="page">
    <p>A General Renewal System</p>
    <p>t T[0] T[1] T[2]</p>
    <p>y[2]y[1]y[0]</p>
    <p>Renewal Frames r in {0, 1, 2, }. [r] = Policy chosen on frame r. P = Abstract policy space ([r] in P for all r). Policy [r] affects frame size and penalty vector on frame r. These are random functions of [r] (distribution depends on [r]):</p>
    <p>[r] y[r] = [1.2, 1.8, , 0.4]</p>
    <p>T[r] = 8.1 = Frame Duration</p>
  </div>
  <div class="page">
    <p>A General Renewal System</p>
    <p>t T[0] T[1] T[2]</p>
    <p>y[2]y[1]y[0]</p>
    <p>Renewal Frames r in {0, 1, 2, }. [r] = Policy chosen on frame r. P = Abstract policy space ([r] in P for all r). Policy [r] affects frame size and penalty vector on frame r. These are random functions of [r] (distribution depends on [r]):</p>
    <p>[r] y[r] = [0.0, 3.8, , -2.0]</p>
    <p>T[r] = 12.3 = Frame Duration</p>
  </div>
  <div class="page">
    <p>A General Renewal System</p>
    <p>t T[0] T[1] T[2]</p>
    <p>y[2]y[1]y[0]</p>
    <p>Renewal Frames r in {0, 1, 2, }. [r] = Policy chosen on frame r. P = Abstract policy space ([r] in P for all r). Policy [r] affects frame size and penalty vector on frame r. These are random functions of [r] (distribution depends on [r]):</p>
    <p>[r] y[r] = [1.7, 2.2, , 0.9]</p>
    <p>T[r] = 5.6 = Frame Duration</p>
  </div>
  <div class="page">
    <p>Example 1: Opportunistic Scheduling</p>
    <p>S[r] = (S1[r], S2[r], S3[r])</p>
    <p>All Frames = 1 Slot S[r] = (S1[r], S2[r], S3[r]) = Channel States for Slot r Policy p[r]: On frame r: First observe S[r], then choose a channel to serve (i.,e, {1, 2, 3}). Example Objectives: thruput, energy, fairness, etc.</p>
  </div>
  <div class="page">
    <p>Example 2: Markov Decision Problems</p>
    <p>M(t) = Recurrent Markov Chain (continuous or discrete) Renewals are defined as recurrences to state 1. T[r] = random inter-renewal frame size (frame r). y[r] = penalties incurred over frame r. [r] = policy that affects transition probs over frame r.</p>
    <p>Objective: Minimize time average of one penalty subj. to time average constraints on others.</p>
  </div>
  <div class="page">
    <p>Example 3: Task Processing over Networks</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>Network Coordinator</p>
    <p>Network Coordinator</p>
    <p>Infinite Sequence of Tasks. E.g.: Query sensors and/or perform computations. Renewal Frame r = Processing Time for Frame r. Policy Types:</p>
    <p>Low Level: {Specify Transmission Decisions over Net}  High Level: {Backpressure1, Backpressure2, Shortest Path}</p>
    <p>Example Objective: Maximize quality of information per unit time subject to per-node power constraints.</p>
    <p>Task 1Task 1Task 2Task 2Task 3Task 3 T/R</p>
  </div>
  <div class="page">
    <p>Quick Review of Renewal-Reward Theory (Pop Quiz Next Slide!)</p>
    <p>Define the frame-average for y0[r]:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>y0= lim R</p>
    <p>R</p>
    <p>r=1</p>
    <p>y0[r]</p>
    <p>Time Avg. = lim R</p>
    <p>R r=1y0[r] R r=1T[r]</p>
    <p>= lim R</p>
    <p>R r=1y0[r]</p>
    <p>R r=1T[r]</p>
    <p>= y0 T</p>
    <p>lim R</p>
    <p>R r=1y0[r] R r=1T[r]</p>
    <p>= lim R</p>
    <p>R r=1y0[r]</p>
    <p>R r=1T[r]</p>
    <p>= y0 T</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>y0= lim R</p>
    <p>R</p>
    <p>r=1</p>
    <p>y0[r]</p>
    <p>Time Avg. = lim R</p>
    <p>R r=1y0[r] R r=1T[r]</p>
    <p>= lim R</p>
    <p>R r=1y0[r]</p>
    <p>R r=1T[r]</p>
    <p>= y0 T</p>
    <p>lim R</p>
    <p>R r=1y0[r] R r=1T[r]</p>
    <p>= lim R</p>
    <p>R r=1y0[r]</p>
    <p>R r=1T[r]</p>
    <p>= y0 T</p>
    <p>The time-average for y0[r] is then:</p>
    <p>*If i.i.d. over frames, by LLN this is the same as E{y0}/E{T}.</p>
  </div>
  <div class="page">
    <p>Pop Quiz: (10 points)</p>
    <p>Let y0[r] = Energy Expended on frame r. Time avg. power = (Total Energy Use)/(Total Time) Suppose (for simplicity) behavior is i.i.d. over frames.</p>
    <p>To minimize time average power, which one should we minimize?</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>E y0[r] T[r]</p>
    <p>E {y0[r]} E {T[r]}</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>E y0[r] T[r]</p>
    <p>E {y0[r]} E {T[r]}</p>
    <p>(a) (b)</p>
  </div>
  <div class="page">
    <p>Pop Quiz: (10 points)</p>
    <p>Let y0[r] = Energy Expended on frame r. Time avg. power = (Total Energy Use)/(Total Time) Suppose (for simplicity) behavior is i.i.d. over frames.</p>
    <p>To minimize time average power, which one should we minimize?</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>E y0[r] T[r]</p>
    <p>E {y0[r]} E {T[r]}</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>E y0[r] T[r]</p>
    <p>E {y0[r]} E {T[r]}</p>
    <p>(a) (b)</p>
  </div>
  <div class="page">
    <p>Two General Problem Types:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Minimize: y0/T Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...}</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Maximize: (y1/T,y2/T,...,yL/T) Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...}</p>
  </div>
  <div class="page">
    <p>Solving the Problem (Type 1):</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J . Neely</p>
    <p>Minimize: y0/T Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...}</p>
    <p>Define a Virtual Queue for each inequality constraint:</p>
    <p>Zl[r]Zl[r] clT[r]yl[r]</p>
    <p>Zl[r+1] = max[Zl[r]  clT[r] + yl[r], 0]</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Queue Stable iff yl clT iff yl/T cl</p>
  </div>
  <div class="page">
    <p>Lyapunov Function and Drift-Plus-Penalty Ratio:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Minimize: y0/T Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...} Z2(t)</p>
    <p>Z1(t)</p>
    <p>L[r] = Z1[r] 2 + Z2[r]</p>
    <p>(Z[r]) = E{L[r+1]  L[r] | Z[r]} = Frame-Based Lyap. Drift</p>
    <p>Scalar measure of queue sizes:</p>
    <p>Algorithm Technique: Every frame r, observe Z1[r], , ZL[r]. Then choose a policy [r] in P to minimize:</p>
    <p>(Z[r]) + VE{y0[r]|Z[r]}</p>
    <p>E{T|Z[r]} Drift-Plus-Penalty Ratio =</p>
  </div>
  <div class="page">
    <p>The Algorithm Becomes:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Minimize: y0/T Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...}</p>
    <p>Observe Z[r] = (Z1[r], , ZL[r]). Choose [r] in P to solve:</p>
    <p>Then update virtual queues:</p>
    <p>(Z[r]) + VE{y0[r]|Z[r]}</p>
    <p>E{T|Z[r]}</p>
    <p>Zl[r+1] = max[Zl[r]  clT[r] + yl[r], 0]</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>E Vy0([r]) + L l=1Zl[r]yl([r])|Z[r]</p>
    <p>E T([r])|Z[r]</p>
  </div>
  <div class="page">
    <p>Theorem: Assume the constraints are feasible. Then under this algorithm, we achieve:</p>
    <p>(Z[r]) + VE{y0[r]|Z[r]}</p>
    <p>E{T|Z[r]}</p>
    <p>DPP Ratio:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>limsup R</p>
    <p>R r=1yl[r]</p>
    <p>R r=1T[r]</p>
    <p>cl l {1,...,L} (w.p.1)</p>
    <p>R r=1 E {y0[r]}</p>
    <p>R r=1 E {T[r]}</p>
    <p>ratioopt +O(1/V)</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>limsup R</p>
    <p>R r=1yl[r]</p>
    <p>R r=1T[r]</p>
    <p>cl l {1,...,L} (w.p.1)</p>
    <p>R r=1 E {y0[r]}</p>
    <p>R r=1 E {T[r]}</p>
    <p>ratioopt +O(1/V)</p>
    <p>(a)</p>
    <p>(b)</p>
    <p>For all frames r in {1, 2, 3, }</p>
  </div>
  <div class="page">
    <p>Solving the Problem (Type 2):</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Maximize: (y1/T,y2/T,...,yL/T) Subject to: (1) yl/T cl l {1,...,L}</p>
    <p>(2) [r]P r {0,1,2,...}</p>
    <p>We reduce it to a problem with the structure of Type 1 via:  Auxiliary Variables [r] = (1[r], , L[r]).  The following variation on Jensens Inequality:</p>
    <p>For any concave function (x1, .., xL) and any (arbitrarily correlated) vector of random variables (x1, x2, , xL, T), where T&gt;0, we have:</p>
    <p>E{T(X1, , XL)}</p>
    <p>E{T} E{T(X1, , XL)}</p>
    <p>E{T} ( )</p>
  </div>
  <div class="page">
    <p>The Algorithm (type 2) Becomes:</p>
    <p>On frame r, observe Z[r] = (Z1[r], , ZL[r]). (Auxiliary Variables) Choose 1[r], , L[r] to max the below deterministic problem:</p>
    <p>(Policy Selection) Choose [r] in P to minimize:</p>
    <p>Then update virtual queues: Zl[r+1] = max[Zl[r]  clT[r] + yl[r], 0], Gl[r+1] = max[Gl[r] + l[r]T[r] - yl[r], 0]</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J . Neely</p>
    <p>Maximize: V(1[r],...,M [r])  L l=1Gl[r]l[r]</p>
    <p>Subject to: min l[r]max l {1,...,L}</p>
    <p>E Ll=1Zl[r]yl([r])  L l=1Gl[r]yl([r])|Z[r]</p>
    <p>E T([r])|Z[r]</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>Maximize: V(1[r],...,L [r])  L l=1Gl[r]l[r]</p>
    <p>Subject to: min l[r]max l {1,...,L}</p>
    <p>E Ll=1Zl[r]yl([r])  L l=1Gl[r]yl([r])|Z[r]</p>
    <p>E T([r])|Z[r]</p>
  </div>
  <div class="page">
    <p>Example Problem  Task Processing:</p>
    <p>T/R T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>T/R</p>
    <p>Network Coordinator</p>
    <p>Network Coordinator</p>
    <p>Task 1Task 1Task 2Task 2Task 3Task 3</p>
    <p>Every Task reveals random task parameters [r]: [r] = [(qual1[r], T1[r]), (qual2[r], T2[r]), , (qual5[r], T5[r])] Choose [r] = [which node to transmit, how much idle] in {1,2,3,4,5} X [0, Imax] Transmissions incur power We use a quality distribution that tends to be better for higher-numbered nodes. Maximize quality/time subject to pav 0.25 for all nodes.</p>
    <p>SetupSetup TransmitTransmit Idle I[r] Frame r</p>
  </div>
  <div class="page">
    <p>Minimizing the Drift-Plus-Penalty Ratio:</p>
    <p>Minimizing a pure expectation, rather than a ratio, is typically easier (see Bertsekas, Tsitsiklis Neuro-DP).</p>
    <p>Define:</p>
    <p>Bisection Lemma:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>val()= inf P</p>
    <p>E {a(,) b(,)}</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>= inf P</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}= 0</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&lt; 0 if &gt;</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&gt; 0 if &lt;</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>val()= inf P</p>
    <p>E {a(,) b(,)}</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>= inf P</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}= 0</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&lt; 0 if &gt;</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&gt; 0 if &lt;</p>
  </div>
  <div class="page">
    <p>Learning via Sampling from the past:</p>
    <p>Suppose randomness characterized by: {1, 2, ..., W} (past random samples)</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>val()= inf P</p>
    <p>E {a(,) b(,)}</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>= inf P</p>
    <p>E {a(,)} E {b(,)}</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}= 0</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&lt; 0 if &gt;</p>
    <p>inf P</p>
    <p>E {a(,) b(,)}&gt; 0 if &lt;</p>
    <p>val()= 1 W</p>
    <p>W</p>
    <p>w=1</p>
    <p>inf P</p>
    <p>E {a(,w) b(,w)|w}</p>
    <p>Want to compute (over unknown random distribution of ):</p>
    <p>Approximate this via W samples from the past:</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>val()= 1 W</p>
    <p>W</p>
    <p>w=1</p>
    <p>inf P</p>
    <p>[a(,w) b(,w)]</p>
  </div>
  <div class="page">
    <p>Simulation:</p>
    <p>Sample Size W</p>
    <p>Q ua</p>
    <p>lit y</p>
    <p>of In</p>
    <p>fo rm</p>
    <p>ati on</p>
    <p>/ U</p>
    <p>ni t</p>
    <p>Ti m</p>
    <p>e</p>
    <p>Drift-Plus-Penalty Ratio Alg. With Bisection</p>
    <p>Alternative Alg. With Time Averaging</p>
  </div>
  <div class="page">
    <p>Concluding Sims (values for W=10):</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>y1/T = 0.182335 0.25 y2/T = 0.249547 0.25 y3/T = 0.250018 0.25 y4/T = 0.250032 0.25 y5/T = 0.250046 0.25</p>
    <p>q.o.i/T = 0.852950 , Idle= 1.421260</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>y1/T = 0.182335 0.25</p>
    <p>y2/T = 0.249547 0.25 y3/T = 0.250018 0.25 y4/T = 0.250032 0.25 y5/T = 0.250046 0.25</p>
    <p>q.o.i/T = 0.852950 , Idle= 1.421260</p>
    <p>DRAFT 1</p>
    <p>Renewal Slides Michael J. Neely</p>
    <p>y1/T = 0.182335 0.25</p>
    <p>y2/T = 0.249547 0.25 y3/T = 0.250018 0.25 y4/T = 0.250032 0.25 y5/T = 0.250046 0.25</p>
    <p>q.o.i/T = 0.852950 , Idle= 1.421260</p>
    <p>Quick Advertisement: New Book: M. J. Neely, Stochastic Network Optimization with Application to Communication and Queueing Systems. Morgan &amp; Claypool, 2010.</p>
    <p>http://www.morganclaypool.com/doi/abs/10.2200/S00271ED1V01Y201006CN T007</p>
    <p>PDF also available from Synthesis Lecture Series (on digital library)  Lyapunov Optimization theory (including these renewal system problems)  Detailed Examples and Problem Set Questions.</p>
  </div>
</Presentation>
