<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Measuring and Modeling the Label Dynamics of Online Anti-Malware Engines</p>
    <p>Shuofei Zhu1, Jianjun Shi1,2, Limin Yang3</p>
    <p>Boqin Qin1,4, Ziyi Zhang1,5, Linhai Song1, Gang Wang3</p>
  </div>
  <div class="page">
    <p>VirusTotal</p>
    <p>The largest online anti-malware scanning service  Applies 70+ anti-malware engines</p>
    <p>Provides analysis reports and rich metadata</p>
    <p>Widely used by researchers in the security community</p>
    <p>Submitter Sample</p>
    <p>Metadata</p>
    <p>Reports</p>
    <p>scan API</p>
    <p>report API</p>
  </div>
  <div class="page">
    <p>Challenges of Using VirusTotal</p>
    <p>Q1: When VirusTotal labels are trustworthy?</p>
    <p>Sample</p>
    <p>TimeDay 1 2 3 4 5 6</p>
    <p>McAfee</p>
  </div>
  <div class="page">
    <p>Challenges of Using VirusTotal</p>
    <p>Q1: When VirusTotal labels are trustworthy?</p>
    <p>Q2: How to aggregate labels from different engines?</p>
    <p>Q3: Are different engines equally trustworthy?</p>
    <p>Engines Results</p>
    <p>McAfee</p>
    <p>Microsoft</p>
    <p>Kaspersky</p>
    <p>Avast</p>
  </div>
  <div class="page">
    <p>Challenges of Using VirusTotal</p>
    <p>Q1: When VirusTotal labels are trustworthy?</p>
    <p>Q2: How to aggregate labels from different engines?</p>
    <p>Q3: Are different engines equally trustworthy?</p>
    <p>Equally trustworthy?</p>
  </div>
  <div class="page">
    <p>Literature Survey on VirusTotal Usages</p>
    <p>Surveyed 115 top-tier conference papers that use VirusTotal</p>
    <p>Our findings:  Q1: rarely consider label changes</p>
    <p>Q2: commonly use threshold-based aggregation methods</p>
    <p>Q3: often treat different VirusTotal engines equally</p>
    <p>Yes</p>
    <p>No</p>
    <p>Threshold-Based Method</p>
    <p>Yes</p>
    <p>No</p>
    <p>Reputable Engines</p>
    <p>Yes</p>
    <p>No</p>
    <p>Consider Label Changes</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Overview</p>
    <p>Equally trustworthy?</p>
    <p>has ground-truthno ground-truth</p>
    <p>PE</p>
    <p>APK</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Outline</p>
    <p>Equally trustworthy?</p>
    <p>PE</p>
    <p>APK</p>
    <p>Main Dataset</p>
    <p>has ground-truthno ground-truth</p>
  </div>
  <div class="page">
    <p>Data Collection of the Main Dataset</p>
    <p>We chose fresh files without prior VirusTotal history  Sampled 14,423 files submitted for the first-time on 08/31/2018</p>
    <p>Roughly half were labeled as benign by all engines on day-1</p>
    <p>The rest were labeled as &quot;malicious&quot; by at least 1 engine on day-1</p>
    <p>We collected daily VirusTotal labels over one year  Use rescan API to force VirusTotal to scan the samples everyday</p>
    <p>Data collection window: 08/31/2018  09/30/2019</p>
    <p>Data Preprocessing  341+ million data points from 65 engines</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Outline</p>
    <p>Equally trustworthy?</p>
    <p>PE</p>
    <p>APK</p>
    <p>has ground-truthno ground-truth</p>
    <p>Main Dataset</p>
  </div>
  <div class="page">
    <p>Label Change or Flip</p>
    <p>We model the label dynamics by sequences of 0 and 1  (benign): 0 (malicious):1</p>
    <p>A Flip: 0  1 or 1  0  hazard flip: temporary, lasts only one day</p>
    <p>non-hazard flip: long term, lasts at least two days</p>
    <p>a hazard flip a non-hazard flip</p>
    <p>days1 5 10 15</p>
  </div>
  <div class="page">
    <p>Characteristics of Flips</p>
    <p>normalized flips per file</p>
    <p>% o</p>
    <p>f fi</p>
    <p>le s</p>
    <p>weeks</p>
    <p>N o</p>
    <p>. o f</p>
    <p>fl ip</p>
    <p>s (1</p>
    <p>)</p>
    <p>normalized flips per engine</p>
    <p>% o</p>
    <p>f e n g</p>
    <p>in e s</p>
    <p>hazard flips</p>
    <p>all flips</p>
    <p>Both flips and hazard flips widely exist across scan dates, engines and files.</p>
  </div>
  <div class="page">
    <p>% o</p>
    <p>f st</p>
    <p>a b le</p>
    <p>d f</p>
    <p>il e s</p>
    <p>How long to wait for a files labels to become stable?</p>
    <p>Stable file: all engines' labels on the file do not change any more</p>
    <p>Individual Label Stabilization</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Outline</p>
    <p>Equally trustworthy?</p>
    <p>PE</p>
    <p>APK</p>
    <p>has ground-truthno ground-truth</p>
    <p>Main Dataset</p>
  </div>
  <div class="page">
    <p>Aggregated Label Stabilization</p>
    <p>McAfee 1 1 1 1 0</p>
    <p>Microsoft 1 1 1 0 0</p>
    <p>Kaspersky 0 1 0 0 0</p>
    <p>(62 engines)</p>
    <p>Aggregated labels 1 1 1 0 0  13</p>
    <p>Day 1 2 3 4 5</p>
    <p>(t = 2)</p>
    <p>Many researchers use a threshold (t) to aggregate engines' labels  A file is considered as malicious, when  t engines detect the file</p>
    <p>How flips impact this aggregation policy?  Influenced files: files with both benign and malicious aggregated labels</p>
    <p>Measure % of influenced files for different t</p>
    <p>(all 0)</p>
  </div>
  <div class="page">
    <p>Aggregated Label Stabilization</p>
    <p>benign +</p>
    <p>malicious</p>
    <p>only benign</p>
    <p>only malicious</p>
    <p>Many researchers use a threshold (t) to aggregate engines' labels  A file is considered as malicious, when  t engines detect the file</p>
    <p>How flips impact this aggregation policy?  Influenced files: files with both benign and malicious aggregated labels</p>
    <p>Measure % of influenced files for different t</p>
    <p>Flips can heavily influence labeling aggregation results when threshold t is too small or too large.</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Outline</p>
    <p>Equally trustworthy?</p>
    <p>PE</p>
    <p>APK</p>
    <p>has ground-truthno ground-truth</p>
    <p>Main Dataset</p>
  </div>
  <div class="page">
    <p>How to compute the similarity between engines A and B?  Compute the similarity between the two labeling sequences for each file</p>
    <p>Compute the average sequence-level similarity over all the files</p>
    <p>An example for sequence-level similarity</p>
    <p>Temporary Labeling Similarity</p>
    <p>Divide time sequence into bins (size=?)</p>
    <p>(1, 1, 1, 5)(1, 1, 1, 5, 0, 0, 0, 7, )</p>
    <p>engine A on file X:</p>
    <p>engine B on file X: 0000000001000000100</p>
    <p>(0, 0, 0, 7, 1, 1, 1, 4, )</p>
    <p>cosine</p>
    <p>( )</p>
    <p>(0, 0, 0, 7)</p>
    <p>( )</p>
  </div>
  <div class="page">
    <p>Label Correlations Between Engines</p>
    <p>Avast AVG K7GW</p>
    <p>K7AntiVirus</p>
    <p>Gdata ESET-NOD32 BitDefender Ad-Aware Emsisoft MicroWorld-eScan</p>
  </div>
  <div class="page">
    <p>Label Correlations Between Engines</p>
    <p>Avast AVG K7GW</p>
    <p>K7AntiVirus</p>
    <p>Gdata ESET-NOD32 BitDefender Ad-Aware Emsisoft MicroWorld-eScan</p>
    <p>MicroWorld-eScan</p>
    <p>Emsisoft</p>
    <p>GData</p>
    <p>Ad-Aware</p>
    <p>BitDefender</p>
  </div>
  <div class="page">
    <p>Label Correlations Between Engines</p>
    <p>Avast AVG K7GW</p>
    <p>K7AntiVirus</p>
    <p>Gdata ESET-NOD32 BitDefender Ad-Aware Emsisoft MicroWorld-eScan</p>
    <p>MicroWorld-eScan</p>
    <p>Emsisoft</p>
    <p>GData</p>
    <p>Ad-Aware</p>
    <p>BitDefender</p>
    <p>There are groups of engines with strong correlations in labeling decisions.</p>
  </div>
  <div class="page">
    <p>Q1: the impact of label changes (label flips)</p>
    <p>Q2: threshold-based label aggregation methods</p>
    <p>Q3: the correlation between VirusTotal engines</p>
    <p>Outline</p>
    <p>Equally trustworthy?</p>
    <p>PE</p>
    <p>APK</p>
    <p>has ground-truthno ground-truth</p>
    <p>Ground-Truth Dataset</p>
  </div>
  <div class="page">
    <p>How we create fresh ground-truth samples?  Obfuscating ransomware to create malware</p>
    <p>Obfuscation + compiling open-source software to create goodware</p>
    <p>Findings:  Obfuscation brings many false positives</p>
    <p>Even for high-reputation engines</p>
    <p>3    15 can produce good aggregation results  As long as the benign files are not obfuscated</p>
    <p>Inconsistency exists between the desktop and the VirusTotal versions</p>
    <p>Ground Truth Dataset</p>
    <p>More results in our paper</p>
  </div>
  <div class="page">
    <p>Conclusion and Takeaways</p>
    <p>A paper survey on how researchers use VirusTotal</p>
    <p>Data-driven methods to validate labeling methodologies</p>
    <p>Takeaways and suggestions  Data preprocessing</p>
    <p>Submit the same files in 3 consecutive days to detect hazards</p>
    <p>No need to wait over long time</p>
    <p>Threshold-based label aggregation  Stable: when t is within a reasonable range (2-20)</p>
    <p>Correctness: t = 3 to 15 when benign files are not obfuscated</p>
    <p>Correlation and causality exists between engines</p>
    <p>High-reputation Engines are not always accurate 20</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Also thanks to my collaborators</p>
    <p>Contact  sfzhu@psu.edu</p>
    <p>Artifact  https://sfzhu93.github.io/projects/vt/index.html</p>
  </div>
</Presentation>
