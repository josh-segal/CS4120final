<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Exalt: Empowering Researchers to Evaluate Large-Scale Storage Systems</p>
    <p>Yang Wang, Manos Kapritsos, Lara Schmidt, Lorenzo Alvisi, and Mike Dahlin The University of Texas at Austin</p>
  </div>
  <div class="page">
    <p>We need to evaluate our prototypes</p>
    <p>Design Implementation</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>We need to evaluate our prototypes</p>
    <p>Design Implementation</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Industrial deployment: tens of PBs thousands of nodes</p>
    <p>Researchers: hundreds of TBs hundreds of nodes</p>
    <p>Salus (Wang et al. NSDI 13): 108 servers  Eiger (Lloyd et al. NSDI 13): 256 servers  Spanner (Corbett et al. OSDI 12): Hundreds of servers</p>
    <p>How does one validate the scalability of a storage system?</p>
  </div>
  <div class="page">
    <p>Extrapolation?</p>
    <p>Measure with a small cluster  Predict the behavior at full scale  Assumption:  Resource consumption grows linearly with scale</p>
    <p>CPU</p>
    <p>Network</p>
    <p>Extrapolate: The system can scale to 1,000 nodes.</p>
    <p>Scale</p>
    <p>Resource uHlizaHon</p>
  </div>
  <div class="page">
    <p>Extrapolation?</p>
    <p>Measure with a small cluster  Predict the behavior at full scale  Assumption: May not hold  Resource consumption grows linearly with scale</p>
    <p>CPU</p>
    <p>Network</p>
    <p>Scale</p>
    <p>Resource uHlizaHon</p>
  </div>
  <div class="page">
    <p>Can we run prototypes at full scale?</p>
    <p>Processes</p>
    <p>Machines</p>
  </div>
  <div class="page">
    <p>Can we run prototypes at full scale?</p>
    <p>Colocate multiple processes on one node</p>
    <p>Processes</p>
    <p>Machines</p>
  </div>
  <div class="page">
    <p>Can we run prototypes at full scale?</p>
    <p>Colocate multiple processes on one node</p>
    <p>Processes</p>
    <p>Machines</p>
    <p>Problem: Limited I/O resource</p>
  </div>
  <div class="page">
    <p>Data content doesnt affect system behavior</p>
    <p>!</p>
    <p>Clients can write/read synthetic data !</p>
    <p>Abstract away data on I/O devices !</p>
    <p>Reduce resource requirement of each process</p>
  </div>
  <div class="page">
    <p>How to abstract away data?</p>
    <p>Discard data? (David, Agrawal et al. FAST 2011)  Doesnt work with large-scale storage systems</p>
    <p>Upper layer (Bigtable, HBase, )</p>
    <p>Lower layer (GFS, HDFS, )</p>
    <p>Data</p>
    <p>Metadata</p>
    <p>Our approach: Compress data</p>
    <p>Treat all bytes as data</p>
  </div>
  <div class="page">
    <p>Requirements of compression</p>
    <p>CPU efficient  General-purpose algorithms (e.g. Gzip) are CPU heavy !</p>
    <p>High compression ratio !</p>
    <p>Lossless compression !</p>
    <p>Be able to work with mixed data and metadata</p>
  </div>
  <div class="page">
    <p>Challenge: Data mixed with metadata</p>
    <p>System may add metadata  System may split data (possibly nondeteministically)</p>
    <p>Metadata Client data</p>
    <p>Key: Locate metadata inside data</p>
  </div>
  <div class="page">
    <p>Make data distinguishable from metadata Flag: sequence of bytes that does not appear in metadata</p>
    <p>Efficiently locate metadata: Follow sorted pattern Marker: number of remaining bytes to the end</p>
    <p>Flag Marker</p>
    <p>Solution: Tardis data pattern</p>
    <p>Tardis</p>
  </div>
  <div class="page">
    <p>Tardis compression</p>
    <p>Search for flag</p>
    <p>Retrieve marker Skip 504 bytes</p>
    <p>Search for flag again</p>
    <p>Retrieve marker Skip 1016 bytes: Hit the end of chunk</p>
    <p>Tardis 102416</p>
    <p>Tardis 512:512</p>
    <p>Starting point Length</p>
    <p>Original data</p>
    <p>Compressed data</p>
  </div>
  <div class="page">
    <p>How to find an appropriate flag?</p>
    <p>Scan all metadata: Expensive !</p>
    <p>Observation: Tardis is only used for testing !</p>
    <p>A randomly chosen 8-byte flag works  HDFS  HBase</p>
  </div>
  <div class="page">
    <p>Testing with Tardis  Run potential bottleneck nodes in real mode.  Run most nodes in emulated mode.</p>
    <p>Clients</p>
    <p>Process</p>
    <p>Network</p>
    <p>Bottleneck</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Emulated devices: disk, network, and memory !</p>
    <p>Disk and network: Transparent emulation  Byte code instrumentation (BCI)  Usage: java -Xbootclasspath exalt.jar &lt;original app&gt; !</p>
    <p>Memory: Require code modification  None for HDFS; 71 LOC for HBase</p>
  </div>
  <div class="page">
    <p>Case studies</p>
    <p>Apply our emulator to HDFS and HBase  Measure their scalability  When we find a problem, analyze its root cause, and fix it !</p>
    <p>Testbed:  Texas Advanced Computing Center (TACC)</p>
  </div>
  <div class="page">
    <p>Scalability of HDFS</p>
    <p>Increase number of RPC threads</p>
    <p>Put debug information in tmpfs</p>
    <p>Same as reported by HDFS developers</p>
    <p>Disable sync Put metadata in tmpfs</p>
  </div>
  <div class="page">
    <p>One problem of HDFS: Big files</p>
    <p>HDFS performance degradation as file grows large.</p>
    <p>long[] computeContentSummary(long[] summary) { long bytes = 0; for(Block blk : blocks) { bytes += blk.getNumBytes(); } summary[0] += bytes;  }</p>
  </div>
  <div class="page">
    <p>Applying Exalt more broadly</p>
    <p>CPU intensive systems?  DieCast (Gupta et al. NSDI 2008) !</p>
    <p>Data sensitive applications/benchmarks?  Record (on a large testbed) and replay (on a small one) !</p>
    <p>The target system modifies data?  Ad-hoc solutions for de-duplication, encryption, etc</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Industry</p>
    <p>Researchers</p>
    <p>https://code.google.com/p/exalt/</p>
  </div>
</Presentation>
