<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>-PAPER PRESENTATION</p>
    <p>Privacy Preserving OLAPPrivacy Preserving OLAP</p>
    <p>Rakesh Agrawal - IBM AlmadenRakesh Agrawal - IBM Almaden Ramakrishnan Srikant - IBM AlmadenRamakrishnan Srikant - IBM Almaden Dilys Thomas - Stanford UniversityDilys Thomas - Stanford University</p>
    <p>CMPE 521 PRINCIPLES of DATABASE SYSTEMS</p>
    <p>By ERGUN BACI</p>
  </div>
  <div class="page">
    <p>CONTENT  Overview of the papers subject  Introduction  Privacy preserving computation</p>
    <p>approach / Basic idea  Related Work Done by Authors  Data Perturbation (Randomization)  Reconstruction and Reconstruction Algorithms  Guarantees against Privacy Breaches (Gvenlik</p>
    <p>Aklar)  Extensions  Summary and Future Work  Questions...</p>
  </div>
  <div class="page">
    <p>OVERVIEW</p>
    <p>Briefly, I will present techniques for privacypreserving computation of multidimensional aggregates on data which is partitioned across</p>
    <p>multiple clients.</p>
  </div>
  <div class="page">
    <p>More detail...  Data from different clients is perturbed (randomized) in</p>
    <p>order to preserve privacy before it is integrated at the server.</p>
    <p>Authors develop formal notions of privacy obtained from data perturbation and show that their perturbation provides guarantees against privacy breaches (gvenlik aklar).</p>
    <p>They develop and analyze algorithms for reconstructing counts of subcubes over perturbed data to an estimated answer.</p>
    <p>They also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of this approach.</p>
    <p>OVERVIEW</p>
  </div>
  <div class="page">
    <p>INTRODUCTION  Today, On-line analytical processing (OLAP) is a</p>
    <p>key technology employed in businessintelligence systems.</p>
    <p>The computation of multidimensional aggregates is the essence of on-line analytical processing.</p>
    <p>Throughout this presentation, I will present techniques for computing multidimensional count aggregates in a privacy-preserving way.</p>
    <p>Note that, all domains in this study are numeric.</p>
  </div>
  <div class="page">
    <p>INTRODUCTION Now lets see the query ran on server S...  The server runs aggregate queries of the form</p>
    <p>The basic idea is that every client Ci perturbs its row ri before sending it to the server S .</p>
    <p>The randomness used in perturbing the values ensures information-theoretic row-level privacy</p>
    <p>S runs queries on the resultant perturbed table T.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>INTRODUCTION  The query meant for the original table T is translated into a</p>
    <p>set of queries on the perturbed table T.  The answers to these queries are then reconstructed to</p>
    <p>obtain the result to the original query with bounded error.</p>
    <p>Authors show that their techniques are safe against privacy breaches.</p>
  </div>
  <div class="page">
    <p>Estimated answer!</p>
    <p>Architecture</p>
  </div>
  <div class="page">
    <p>Now lets go over on some properties of the approach:  The perturbation algorithm is publicly known BUT the actual</p>
    <p>random numbers used in the perturbation are hidden.  To allow clients to operate independently, local</p>
    <p>perturbations is used so that the perturbed value of a data element depends only on its initial value and not on those of the other data elements.</p>
    <p>Different columns of a row are perturbed independently.  Authors use retention replacement schemes where an</p>
    <p>element is decided to be retained with probability p or replaced with an element selected from a probability distribution function (p.d.f.)</p>
  </div>
  <div class="page">
    <p>RELATED WORK  In one study:Slightly different perturbation technique</p>
    <p>(Additive perturbation) is used with a different p.d.f., that the authors feel difficult to prove its security.</p>
    <p>In another: Privacy preserving association rule mining is studied but only on boolean data.</p>
    <p>In one other study: A model is built for a single server, unlike our problem where data is integrated from multiple clients.</p>
    <p>And... In another related study, model is designed for a small number clients.</p>
    <p>A nice contribution of previous literature survey is: Adaptation of Formal definitions of privacy breaches in this study from a previous paper...</p>
  </div>
  <div class="page">
    <p>DATA PERTURBATION</p>
    <p>Lets begin with Definitions...  A single record of the table is referred</p>
    <p>to as a row, while an attribute is referred to as a column.</p>
    <p>A single column from a single row is the granularity of perturbation and is called a data element.</p>
  </div>
  <div class="page">
    <p>DATA PERTURBATION DEFINITION 1</p>
    <p>Perturbation Algorithm: A perturbation algorithm is a randomized algorithm that given a table T creates a table T having the same number of rows and columns.</p>
    <p>T : Original table  T: Perturbed table  Note that, table is perturbated column by</p>
    <p>column!</p>
  </div>
  <div class="page">
    <p>DATA PERTURBATION DEFINITION 2</p>
    <p>Retention Replacement Perturbation:  Retention replacement perturbation is a</p>
    <p>perturbation algorithm, where each element in column j is retained with probability pj, and with probability (1- pj) replaced with an element selected from the replacing p.d.f. (in this study, Uniform distribution is used)</p>
  </div>
  <div class="page">
    <p>Randomization Approach Overview</p>
    <p>...</p>
    <p>Randomizer Randomizer</p>
    <p>Reconstruct distribution</p>
    <p>of Age</p>
    <p>Reconstruct distribution</p>
    <p>of Salary</p>
    <p>Data Mining Algorithms</p>
    <p>Model</p>
  </div>
  <div class="page">
    <p>Single Attribute Example What is the fraction of people in a shoping</p>
    <p>mall with age between 30 and 50?  Assume age range is between 0-80  Whenever a person enters the building flips</p>
    <p>a coin of bias p=0.3 for heads say.  Heads -- report true age  Tails -- random number uniform in 0-80</p>
    <p>reported</p>
    <p>Totally 100 randomized numbers collected.  Of these 26 are 30-50.  How many among the original are 30-50?</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Out of 100 : 70 perturbed (0.7 fraction), 30 retained (0.3 fraction)</p>
  </div>
  <div class="page">
    <p>Perturbed ,</p>
    <p>NOT Age[30-50] 17,5 Perturbed, Age[30-50]</p>
    <p>(50-30)/(80-0)= 0.25.....Rate of Age[30-50] for uniform p.d.f.</p>
  </div>
  <div class="page">
    <p>Since there were 26 randomized rows in [30-50]. 26 - 17,5 = 8,5 of them come from the 30 retained rows.</p>
    <p>NOT Age[30-50]</p>
  </div>
  <div class="page">
    <p>Scaling up</p>
    <p>Total Rows</p>
    <p>Age[30-50] (Retained)</p>
    <p>Thus 28,3 people had age 30-50 in expectation.</p>
    <p>This gives intuition about how we reconstruct!</p>
  </div>
  <div class="page">
    <p>RECONSTRUCTION DEFINITION 3</p>
    <p>Reconstructible Function: Given a perturbation  converting table T to T , a numeric function f on T is said to be (n,,) reconstructible by a function f ' , if f ' can be evaluated on the perturbed table T' so that | f  - f | &lt; max(o, f ) with probability greater than (1- ) whenever the table T has more than n rows. The probability is over the random choices made by .</p>
    <p>dea is: An aggregate function on the original table T, must be reconstructed by accessing the perturbed table T .</p>
  </div>
  <div class="page">
    <p>RECONSTRUCTION Result of DEFINITION 3</p>
    <p>The accuracy of the reconstruction algorithm is formalized by the notion of approximate probabilistic reconstructability.</p>
    <p>At the end, 2k number of queries formed, where k is the number of columns/range predicates, in the form of: count(P1 .P2 .. . . Pk), count(P1 .P2 .. . . Pk), count(P1 .P2 .. . . Pk), count(P1.P2.. . . Pk) . . . count(P1.P2.. . . Pk)</p>
    <p>These queries are evaluated on the perturbed table .</p>
    <p>The answers on T are reconstructed into estimated answers for the same queries on T, which include the answer to the original query.</p>
  </div>
  <div class="page">
    <p>Reconstruction Problem</p>
    <p>Original values x1, x2, ..., xn  from probability distribution X (unknown)</p>
    <p>To hide these values, we use y1, y2, ..., yn  from probability distribution Y</p>
    <p>Given</p>
    <p>x1+y1, x2+y2, ..., xn+yn  the probability distribution of Y</p>
    <p>Estimate the probability distribution of X.</p>
  </div>
  <div class="page">
    <p>Intuition (Reconstruct single point)</p>
    <p>Use Bayes' rule for density functions</p>
    <p>V</p>
    <p>Original distribution for Age</p>
    <p>Probabilistic estimate of original value of V</p>
  </div>
  <div class="page">
    <p>Intuition (Reconstruct single point)</p>
    <p>Original Distribution for Age</p>
    <p>Probabilistic estimate of original value of V</p>
    <p>V</p>
    <p>Use Bayes' rule for density functions</p>
  </div>
  <div class="page">
    <p>Reconstructing the Distribution</p>
    <p>Combine estimates of where point came from for all the points:  Gives estimate of original distribution.</p>
  </div>
  <div class="page">
    <p>Reconstructing Single Column Aggregates</p>
    <p>Consider the uniform retention replacement perturbation with retention probability p applied on a database with n rows and a single column, C, with domain [min, max]. Consider the predicate P = C[low, high]. Given the perturbed table T , lets see how to estimate an answer to the query count(P) on T. Let tables T, T each have n rows. Let nr = count(P) evaluated on table T , while no = count(P) estimated for table T. Given nr we estimate no as:</p>
  </div>
  <div class="page">
    <p>Reconstructing Single Column Aggregates</p>
    <p># of perturbated rows</p>
    <p># of perturbated rows in the range</p>
    <p># of perturbated OR non-perturbed rows,</p>
    <p>BUT in the range</p>
    <p>Probability of being in the range</p>
    <p>To scale</p>
    <p>Can be used as MLE(Maximum Likelihood Estimator) and as a good reconstructor</p>
    <p>no nr estimation!</p>
  </div>
  <div class="page">
    <p>Reconstructing Single Column Aggregates</p>
    <p>Here, a theorem is given</p>
    <p>According to this, an estimator x is given as:</p>
    <p>where A is a transition matrix to reconstruct...</p>
  </div>
  <div class="page">
    <p>Reconstructing Multiple Column Aggregates</p>
    <p>Basic idea...  For multiple column, reconstruction is similar, but, in multiple case</p>
    <p>k X 2 transition matrix is formed  2k aggregate queries are run on perturbed table</p>
    <p>A simple example:</p>
  </div>
  <div class="page">
    <p>Reconstruction Multiple Column Aggregates</p>
    <p>Two main techniques are used:  Matrix transition technique  If pr is the retention probability for the rth column, we calculate vector x from vector y as x = yA-1. The transition matrix A, which is with 2k rows and 2k columns, can be calculated as the tensor product of matrices</p>
    <p>where the matrix Ar, for 1 &lt; r &lt; k is the transition matrix for column r.  Briefly, the idea is to form a transition matrix for each COLUMN.  With tensor product of matrices, matrix A is formed.</p>
    <p>So, we have split the space of possible evaluations of a ROW into 2k states.</p>
  </div>
  <div class="page">
    <p>Iterative bayesian technique</p>
    <p>Basic idea is: Initializing the vector, x0 = y, and iterating until two consecutive x iterates do not differ much.</p>
  </div>
  <div class="page">
    <p>Errors in Reconstruction  Due to the randomization in the perturbation</p>
    <p>algorithm there are errors in the transition probabilities in matrix A. This causes y, the posteriori distribution after perturbation calculated from T , to have errors. Hence the reconstructed x will have errors.  The error decreases as the number of rows, n, increases.  The error in reconstruction increases as the number of</p>
    <p>reconstructed columns, k, increases, and the probability of retention, p, decreases.</p>
  </div>
  <div class="page">
    <p>Guarantees Against Privacy Breaches</p>
    <p>Two types of privacy breaches are mentioned:</p>
  </div>
  <div class="page">
    <p>(1, 2) privacy breach</p>
    <p>It means,If original (a priori) probability of a condition is smaller or equal to 1 and perturbated probaility (posteriori) is greater or equal to 2, there is (1, 2) privacy breach...</p>
    <p>This breach is referred to another paper and no any quantitative value is given in both, as the measure of the breach.</p>
  </div>
  <div class="page">
    <p>Second one is the extended version of the first formed by the authors.</p>
    <p>They considered a database of purchases made by individuals.</p>
    <p>They think that: Many people buy bread, but not many buy the same prescription medicine.</p>
    <p>Attention!  The new metric is more concerned about whether an</p>
    <p>adversary can infer from the randomized row which medicine a person bought, and is less concerned about the adversary determining with high probability that the original row had bread, as most individuals buy bread and it does not distinguish the individual from the rest of the crowd.</p>
    <p>(s, 1, 2) privacy breach</p>
  </div>
  <div class="page">
    <p>Guarantees Against Privacy Breaches  A security condition!</p>
    <p>Single Column Perturbation (Theorem) Let p be the probability of retention, then uniform perturbation applied to a single column is secure against a (s, 1, 2) breach, if</p>
    <p>Where s is equal to ps/ms.  ps is a priori probablity (p(age&lt;10)=0.2)  ms is replacing probablity of p.d.f. used</p>
    <p>For Multiple column perturbation, idea is same but calculation is more complicated...</p>
  </div>
  <div class="page">
    <p>Theorem for s value</p>
  </div>
  <div class="page">
    <p>EXTENSIONS</p>
    <p>Three different types of extensions can be made :</p>
  </div>
  <div class="page">
    <p>Categorical data  For categorical data, extension of algorithms</p>
    <p>are extremely simple:  Define: Consider a categorical column, C,</p>
    <p>having discrete domain D. Let . A predicate P, on column C, using S is defined as</p>
    <p>Use: Use this as retention replacement probability</p>
  </div>
  <div class="page">
    <p>Alternative retention replacement schemes</p>
    <p>selected uniformly among all data elements (i.e. The replacing p.d.f. is the same as the a priori distribution).</p>
    <p>permuted amongst themselves.</p>
  </div>
  <div class="page">
    <p>Application to Classification</p>
    <p>Aggregate queries on multiple columns can be used for privacy preserving construction of decision trees.</p>
    <p>Example:A decision tree is built on randomized table T with schema (age, salary, house-rent, classvariable) to predict the column class variable.</p>
    <p>Here, idea is : For the first split, say on (age &lt; 30), the gini index is calculated using the estimated answers of the four queries: count(age[0-30] and Q), count(age[0,30] and Q), count(age[0-30] and Q ) and count(age[0,30] and Q ) on T.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>EXPERIMENTS  Both real data and synthetic data is used  For real data, authors used the Adult</p>
    <p>dataset, from the UCI Machine Learning Repository.</p>
    <p>Dataset contains about 32,000 rows with 4 numerical columns. (The columns and their ranges are: age[17 - 90], fnlwgt[10000 - 1500000], hrsweek[1 - 100] and edunum[1 - 16].)</p>
    <p>For synthetic data, authors used uncorrelated columns of data having Zipfian distribution with zipf parameter 0.5.</p>
  </div>
  <div class="page">
    <p>Randomization and Reconstruction Experiments</p>
    <p>The l1 norm of the difference between the estimated (x) and actual (x' ) p.d.f. vectors is used as the metric of error.</p>
    <p>The results of the reconstruction algorithm are said to be quite accurate when the reconstruction error is much smaller than 1.</p>
    <p>The iterative procedure gives smaller errors than the inversion procedure, especially when a larger number of columns are reconstructed together, and the probability of retention, p, is small.</p>
  </div>
  <div class="page">
    <p>Scalibility Experiments</p>
    <p>Now lets see, how the reconstruction error varies as the number of columns reconstructed, retention probability AND number of rows vary.</p>
    <p>Both iterative and inversion algorithms show an exponential increase in the error as the number of columns increases and as the probability of retention decreases. For smaller number of columns and higher retention probabilities both algorithms give comparable reconstruction errors.</p>
  </div>
  <div class="page">
    <p>Reconstruction error decreases as the number of perturbed rows available for reconstruction increase, for the the iterative reconstruction algorithm.</p>
    <p>Number of rows in the table</p>
  </div>
  <div class="page">
    <p>Privacy Breaches</p>
    <p>The two figures plot the maximum retention probability, p, that would avoid a (s, 1, 2) breach. See y axis.</p>
    <p>The values of s used are the maximum value of s, the median value s = 1, and s = 0.1 for a rare set (because of a theorem given in the paper).</p>
    <p>Both figures show that if it suffices to just hide rare properties (i.e., with s &lt; 0.1),even the the retention probability p is as high as 0.8.</p>
  </div>
  <div class="page">
    <p>CONTRIBUTIONS and FUTURE WORK OF THE PAPER</p>
    <p>perturbed table, and develop algorithms to reconstruct multiple columns together.</p>
    <p>Future work includes extending this work to other aggregates over subcubes (Not only count aggregates).</p>
  </div>
  <div class="page">
    <p>CONCLUSION</p>
    <p>A good paper with experimental results, the approach and results are interesting</p>
    <p>Compared to previous related works, it takes into account more features for consideration</p>
    <p>BUT, In some sections more explanation is needed.</p>
  </div>
  <div class="page">
    <p>QUESTIONS ? QUESTIONS ?</p>
  </div>
  <div class="page">
    <p>As a concrete example, for uniform perturbation, with p=0.2, there are no</p>
    <p>(68, 0.1, 0.95) breaches. This means for any set S ,</p>
    <p>if 2 &gt; 0.95 with uniform perturbation,</p>
    <p>1 will be large (&gt; 0.1) when ps/ms &lt; 68. In fact, for a rare set, with s &lt; 1, there will be no (0.937, 0.95) privacy breaches in the original (1, 2) model for this perturbation.</p>
  </div>
  <div class="page">
    <p>Back-up slides</p>
  </div>
  <div class="page">
    <p>Selectivity of predicates</p>
    <p>(e-a) -&gt; absolute error (e-a)/a -&gt; relative error</p>
    <p>The absolute error in Figure 11 does not vary much with the interval width. However the relative error in Figure 12 increases as the interval width decreases.</p>
    <p>Both the absolute and relative errors decrease as the number of rows available for reconstruction increases.</p>
  </div>
</Presentation>
