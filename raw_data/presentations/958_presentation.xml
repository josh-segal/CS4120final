<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improving Memory System Performance for Soft Vector Processors Peter Yiannacouras J. Gregory Steffan Jonathan Rose</p>
    <p>WoSPS  Oct 26, 2008</p>
  </div>
  <div class="page">
    <p>Soft Processors in FPGA Systems</p>
    <p>Soft Processor</p>
    <p>Custom Logic</p>
    <p>HDL +</p>
    <p>CAD</p>
    <p>C +</p>
    <p>Compiler</p>
    <p>Easier  Faster  Smaller  Less Power</p>
    <p>Data-level parallelism  soft vector processors</p>
    <p>Configurable  how can we make use of this?</p>
  </div>
  <div class="page">
    <p>Vector Processing Primer</p>
    <p>// C code for(i=0;i&lt;16; i++) b[i]+=a[i]</p>
    <p>// Vectorized code set vl,16 vload vr0,b vload vr1,a vadd vr0,vr0,vr1 vstore vr0,b</p>
    <p>Each vector instruction holds many units of independent operations</p>
    <p>b[0]+=a[0] b[1]+=a[1] b[2]+=a[2]</p>
    <p>b[4]+=a[4] b[3]+=a[3]</p>
    <p>b[5]+=a[5] b[6]+=a[6] b[7]+=a[7] b[8]+=a[8] b[9]+=a[9]</p>
    <p>b[10]+=a[10] b[11]+=a[11] b[12]+=a[12] b[13]+=a[13] b[14]+=a[14] b[15]+=a[15]</p>
    <p>vadd</p>
  </div>
  <div class="page">
    <p>Vector Processing Primer</p>
    <p>// C code for(i=0;i&lt;16; i++) b[i]+=a[i]</p>
    <p>// Vectorized code set vl,16 vload vr0,b vload vr1,a vadd vr0,vr0,vr1 vstore vr0,b</p>
    <p>Each vector instruction holds many units of independent operations</p>
    <p>vadd</p>
    <p>b[0]+=a[0] b[1]+=a[1] b[2]+=a[2]</p>
    <p>b[4]+=a[4] b[3]+=a[3]</p>
    <p>b[5]+=a[5] b[6]+=a[6] b[7]+=a[7] b[8]+=a[8] b[9]+=a[9]</p>
    <p>b[10]+=a[10] b[11]+=a[11] b[12]+=a[12] b[13]+=a[13] b[14]+=a[14] b[15]+=a[15]</p>
  </div>
  <div class="page">
    <p>Sub-Linear Scalability</p>
    <p>Vector lanes not being fully utilized</p>
  </div>
  <div class="page">
    <p>Where Are The Cycles Spent?</p>
  </div>
  <div class="page">
    <p>Our Goals</p>
  </div>
  <div class="page">
    <p>Current Infrastructure</p>
    <p>Vectorized assembly</p>
    <p>subroutines</p>
    <p>GNU as +</p>
    <p>Vector support</p>
    <p>ELF Binary</p>
    <p>MINT Instruction</p>
    <p>Set Simulator</p>
    <p>scalar P</p>
    <p>+ vpu</p>
    <p>VC RF</p>
    <p>VS RF</p>
    <p>VC W B</p>
    <p>VS W B</p>
    <p>Logic</p>
    <p>Decode Replicate</p>
    <p>Hazard check</p>
    <p>VR RF</p>
    <p>A L U</p>
    <p>Mem Unit</p>
    <p>x &amp; satur.</p>
    <p>VR W B</p>
    <p>M U X</p>
    <p>Saturate</p>
    <p>Rshift</p>
    <p>VR RF</p>
    <p>A L U</p>
    <p>x &amp; satur.</p>
    <p>VR W B</p>
    <p>M U X</p>
    <p>Saturate</p>
    <p>Rshift</p>
    <p>EEMBC C Benchmarks</p>
    <p>Modelsim (RTL</p>
    <p>Simulator)</p>
    <p>SOFTWARE HARDWARE Verilog</p>
    <p>Altera Quartus II</p>
    <p>v 8.0</p>
    <p>cycles area, frequency</p>
    <p>GCC</p>
    <p>ld</p>
    <p>verification verification</p>
  </div>
  <div class="page">
    <p>VESPA Architecture Design</p>
    <p>Scalar Pipeline 3-stage</p>
    <p>Vector Control Pipeline 3-stage</p>
    <p>Vector Pipeline 6-stage</p>
    <p>Icache Dcache</p>
    <p>Decode RF A L U</p>
    <p>M U X</p>
    <p>WB</p>
    <p>VC RF</p>
    <p>VS RF</p>
    <p>VC WB</p>
    <p>VS WB</p>
    <p>Logic</p>
    <p>Decode Replicate</p>
    <p>Hazard check</p>
    <p>VR RF A</p>
    <p>L U</p>
    <p>x &amp; satur.</p>
    <p>VR WB</p>
    <p>M U X</p>
    <p>Saturate</p>
    <p>Rshift</p>
    <p>VR RF A</p>
    <p>L U</p>
    <p>x &amp; satur.</p>
    <p>VR WB</p>
    <p>M U X</p>
    <p>Saturate</p>
    <p>Rshift</p>
    <p>Mem Unit</p>
    <p>Decode</p>
    <p>Supports integer and fixed-point operations, and predication</p>
    <p>Shared Dcache</p>
  </div>
  <div class="page">
    <p>Vector Memory Crossbar</p>
    <p>Memory System Design</p>
    <p>DDR</p>
    <p>Scalar Vector Coproc</p>
    <p>Lane 0 Lane</p>
    <p>Dcache 4KB,</p>
    <p>Lane 0 Lane</p>
    <p>Lane 4 Lane</p>
    <p>VESPA 16 lanes</p>
    <p>DDR 9 cycle access</p>
    <p>vld.w (load 16 contiguous 32-bit words)</p>
  </div>
  <div class="page">
    <p>Vector Memory Crossbar</p>
    <p>Memory System Design</p>
    <p>DDR</p>
    <p>Scalar Vector Coproc</p>
    <p>Lane 0 Lane</p>
    <p>Dcache 16KB,</p>
    <p>Lane 0 Lane</p>
    <p>Lane 4 Lane</p>
    <p>VESPA 16 lanes</p>
    <p>DDR 9 cycle access</p>
    <p>vld.w (load 16 contiguous 32-bit words)</p>
    <p>Reduced cache accesses + some prefetching</p>
  </div>
  <div class="page">
    <p>Improving Cache Design</p>
    <p>Vary the cache depth &amp; cache line size  Using parameterized design  Cache line size: 16, 32, 64, 128 bytes  Cache depth: 4, 8, 16, 32, 64 KB</p>
    <p>Measure performance on 9 benchmarks  6 from EEMBC, all executed in hardware</p>
    <p>Measure area cost  Equate silicon area of all resources used</p>
    <p>Report in units of Equivalent LEs</p>
  </div>
  <div class="page">
    <p>Cache Design Space  Performance (Wall Clock Time)</p>
    <p>Best cache design almost doubles performance of original VESPA</p>
    <p>More pipelining/retiming could reduce clock frequency penalty</p>
    <p>Cache line more important than cache depth (lots of streaming)</p>
  </div>
  <div class="page">
    <p>Cache Design Space  Area</p>
    <p>M4K</p>
    <p>MRAM</p>
    <p>16bits</p>
    <p>System area almost doubled in worst case</p>
  </div>
  <div class="page">
    <p>Cache Design Space  Area</p>
    <p>M4K</p>
    <p>MRAM</p>
    <p>b) Dont use MRAMs: big, few, and overkill</p>
    <p>a) Choose depth to fill block RAMs needed for line size</p>
  </div>
  <div class="page">
    <p>Hardware Prefetching Example</p>
    <p>DDR</p>
    <p>Dcache</p>
    <p>vld.w</p>
    <p>No Prefetching Prefetching 3 blocks</p>
    <p>DDR</p>
    <p>Dcache</p>
    <p>vld.w</p>
    <p>MISS MISS</p>
    <p>vld.w vld.w</p>
    <p>HITMISS</p>
  </div>
  <div class="page">
    <p>Hardware Data Prefetching</p>
    <p>Advantages  Little area overhead  Parallelize memory fetching with computation  Use full memory bandwidth</p>
    <p>Disadvantages  Cache pollution</p>
    <p>We use Sequential Prefetching triggered on:  a) any miss, or  b) sequential vector instruction miss</p>
    <p>We measure performance/area using a 64B, 16KB dcache</p>
  </div>
  <div class="page">
    <p>Prefetching K Blocks  Any Miss</p>
    <p>Peak average speedup 28%</p>
    <p>Not receptive</p>
    <p>Only half the benchmarks significantly sped-up, max of 2.2x, avg 28%</p>
  </div>
  <div class="page">
    <p>dirty lines</p>
    <p>Prefetching Area Cost: Writeback Buffer</p>
    <p>Two options:  Deny prefetch  Buffer all dirty lines</p>
    <p>Area cost is small  1.6% of system area  Mostly block RAMs  Little logic</p>
    <p>No clock frequency impact</p>
    <p>Prefetching 3 blocks</p>
    <p>DDR</p>
    <p>Dcache</p>
    <p>vld.w</p>
    <p>MISS</p>
    <p>WB Buffer</p>
  </div>
  <div class="page">
    <p>Any Miss vs Sequential Vector Miss</p>
    <p>Collinear  nearly all misses in our benchmarks are sequential vector</p>
  </div>
  <div class="page">
    <p>Vector Length Prefetching</p>
    <p>Previously: constant # cache lines prefetched  Now: Use multiple of vector length</p>
    <p>Only for sequential vector memory instructions  Eg. Vector load of 32 elements</p>
    <p>Guarantees &lt;= 1 miss per vector memory instr</p>
    <p>vld.w 0 31</p>
    <p>fetch + prefetch 28*k</p>
  </div>
  <div class="page">
    <p>Vector Length Prefetching Performance</p>
    <p>Peak 29%</p>
    <p>Not receptive</p>
    <p>no cache pollution</p>
  </div>
  <div class="page">
    <p>Overall Memory System Performance</p>
    <p>(4KB) (16KB)</p>
    <p>Wider line + prefetching reduces memory unit stall cycles significantly</p>
    <p>Wider line + prefetching eliminates all but 4% of miss cycles</p>
  </div>
  <div class="page">
    <p>Improved Scalability</p>
    <p>Previous: 3-8x range, average of 5x for 16 lanes Now: 6-13x range, average of 10x for 16 lanes</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Explored cache design  ~2x performance for ~2x system area</p>
    <p>Area growth due largely to memory crossbar</p>
    <p>Widened cache line size to 64B and depth to 16KB</p>
    <p>Enhanced VESPA w/ hardware data prefetching  Up to 2.2x performance, average of 28% for K=15  Vector length prefetcher gains 21% on average for 1*VL</p>
    <p>Good for mixed workloads, no tuning, no cache pollution  Peak at 8*VL, average of 29% speedup</p>
    <p>Overall improved VESPA memory system &amp; scalability  Decreased miss cycles to 4%,  Decreased memory unit stall cycles to 31%</p>
  </div>
  <div class="page">
    <p>Vector Memory Unit</p>
    <p>Dcache</p>
    <p>base</p>
    <p>stride*0</p>
    <p>index0</p>
    <p>+ M U X</p>
    <p>...</p>
    <p>stride*1</p>
    <p>index1</p>
    <p>+ M U X</p>
    <p>stride*L</p>
    <p>indexL</p>
    <p>+ M U X</p>
    <p>Memory Request Queue</p>
    <p>Read Crossbar</p>
    <p>Memory Lanes=4</p>
    <p>rddata0 rddata1</p>
    <p>rddataL</p>
    <p>wrdata0 wrdata1</p>
    <p>wrdataL ...</p>
    <p>Write Crossbar</p>
    <p>Memory Write</p>
    <p>Queue</p>
    <p>L = # Lanes - 1</p>
  </div>
</Presentation>
