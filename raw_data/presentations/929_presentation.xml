<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Latency-Tolerant Software Distributed Shared Memory</p>
    <p>Jacob Nelson, Brandon Holt, Brandon Myers, Preston Briggs, Luis Ceze, Simon Kahan, Mark Oskin University of Washington USENIX ATC 2015 July 9, 2015</p>
  </div>
  <div class="page">
    <p>Memory Coherence in Shared Virtual Memory Systems KAI Ll Princeton University and PAUL HUDAK Yale University</p>
    <p>The memory coherence problem in designing and implementing a shared virtual memory on loosely coupled multiprocessors is studied in depth. Two classes of algorithms, centralized and distributed, for solving the problem are presented. A prototype shared virtual memory on an Apollo ring based on these algorithms has been implemented. Both theoretical and practical results show that the memory coherence problem can indeed be solved efficiently on a loosely coupled multiprocessor.</p>
    <p>Categories and Subject Descriptors: C.2.1 [Computer-Communication Networks]: Network Architecture and Design--network communications; C.2.4 [Computer-Communication Networks]: Distributed Systems-network operating systems; D.4.2 [Operating Systems]: Storage Management-distributed memories; uirtuol memory; D.4.7 [Operating Systems]: Organization and Design-distributed systems</p>
    <p>General Terms: Algorithms, Design, Experimentation, Measurement, Performance</p>
    <p>Additional Key Words and Phrases: Loosely coupled multiprocessors, memory coherence, parallel programming, shared virtual memory</p>
    <p>This research was supported in part by National Science Foundation grants MCS-8302018, DCR8106181, and CCR-8814265. A preliminary version of this paper appeared in the Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing [36]. Authors addresses: K. Li, Department of Computer Science, Princeton University, Princeton, NJ 08544; P. Hudak, Department of Computer Science, Yale University, New Haven, CT 06520. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 0 1989 ACM 0734-2071/89/1100-0321$01.50</p>
    <p>ACM Transactions on Computer Systems, Vol. 7, No. 4, November 1989, Pages 321-359.</p>
  </div>
  <div class="page">
    <p>DRAM DRAM DRAM DRAM</p>
    <p>CPU CPU CPU CPU</p>
    <p>Distributed shared memory, then</p>
    <p>Cache coherence protocol over network</p>
    <p>Use DRAM as cache</p>
    <p>Make transparent/fast with paging hardware</p>
    <p>(cache block = memory page)</p>
    <p>Best apps were: compute-focused</p>
    <p>dense coarse-grained</p>
  </div>
  <div class="page">
    <p>Distributed shared memory, now?</p>
    <p>New data-intensive applications: Social network analysis Machine learning Bioinformatics</p>
    <p>Data access, not compute, is the hard part</p>
    <p>Locality can be hard to find</p>
    <p>A bad fit for DSM of 25 years ago, so community has explored other abstractions: Spark, GraphLab, Naiad, etc. S.cerevisiae</p>
    <p>[von Mering et al.]</p>
  </div>
  <div class="page">
    <p>Commodity networkLinux x86 node Linux x86 node</p>
    <p>Grappa  Distributed shared memory</p>
    <p>MapReduce GraphLab Relational</p>
    <p>Query Engine</p>
    <p>Irregular apps, native code, etc...</p>
    <p>....</p>
    <p>Your next data-intensive application or framework!</p>
    <p>What is Grappa? Grappa: Software distributed shared memory for data-intensive apps</p>
  </div>
  <div class="page">
    <p>What makes this hard?</p>
    <p>Lack of locality</p>
    <p>Small messages</p>
    <p>Small tasks</p>
    <p>Parallelism is abundant in data-intensive</p>
    <p>applications!</p>
  </div>
  <div class="page">
    <p>What makes this hard?</p>
    <p>Lack of locality</p>
    <p>Small messages</p>
    <p>Small tasks</p>
    <p>Use parallelism to hide latency!</p>
  </div>
  <div class="page">
    <p>A remote read with latency tolerance</p>
    <p>Task 1</p>
    <p>read()</p>
    <p>DRAM DRAM DRAM DRAM</p>
  </div>
  <div class="page">
    <p>A remote read with latency tolerance</p>
    <p>Task 1</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>Task 2</p>
    <p>Task ~100</p>
    <p>read()</p>
  </div>
  <div class="page">
    <p>A remote read with latency tolerance</p>
    <p>Task 1</p>
    <p>Task 2</p>
    <p>Task ~100</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>read()</p>
  </div>
  <div class="page">
    <p>What makes this hard?</p>
    <p>Small messages</p>
    <p>Small tasks</p>
    <p>Lack of locality Use parallelism to hide latency!</p>
  </div>
  <div class="page">
    <p>The small message problem</p>
    <p>B an</p>
    <p>dw id</p>
    <p>th (G</p>
    <p>B )</p>
    <p>mpi RDMA (verbs)</p>
  </div>
  <div class="page">
    <p>The small message problem</p>
    <p>B an</p>
    <p>dw id</p>
    <p>th (G</p>
    <p>B )</p>
    <p>mpi RDMA (verbs)</p>
  </div>
  <div class="page">
    <p>The small message problem</p>
    <p>B an</p>
    <p>dw id</p>
    <p>th (G</p>
    <p>B )</p>
    <p>mpi RDMA (verbs)</p>
  </div>
  <div class="page">
    <p>Our goal is throughput! We can trade additional latency</p>
    <p>for increased throughput.</p>
  </div>
  <div class="page">
    <p>Aggregating remote operations</p>
    <p>Task 1</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>Task 2</p>
    <p>read()</p>
    <p>op()</p>
    <p>op()</p>
    <p>op()</p>
  </div>
  <div class="page">
    <p>Aggregating remote operations</p>
    <p>Task 1</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>Task 2</p>
    <p>Task ~1000</p>
    <p>read()</p>
    <p>op()</p>
    <p>op()</p>
    <p>op()</p>
  </div>
  <div class="page">
    <p>Aggregating remote operations</p>
    <p>Task 1</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>Task 2</p>
    <p>Task ~1000</p>
    <p>read()</p>
    <p>op()</p>
    <p>op()</p>
    <p>op()</p>
  </div>
  <div class="page">
    <p>What makes this hard?</p>
    <p>Small tasks</p>
    <p>Lack of locality</p>
    <p>Small messages</p>
    <p>Use parallelism to hide latency!</p>
    <p>Trade latency for more throughput!</p>
  </div>
  <div class="page">
    <p>What makes this hard?</p>
    <p>Lack of locality</p>
    <p>Small messages</p>
    <p>Small tasks</p>
    <p>Use parallelism to hide latency!</p>
    <p>Trade latency for more throughput!</p>
    <p>Make context switching fast.</p>
  </div>
  <div class="page">
    <p>User-level cooperative multithreading</p>
    <p>With ~1000 threads per core, contexts often dont fit in L1 Our scheduler prefetches contexts into cache Limited by DRAM bandwidth, not miss latency</p>
    <p>Context switch moves 1 cacheline of thread state, 3 cachelines of working set</p>
    <p>~50 ns</p>
  </div>
  <div class="page">
    <p>DSM implementation</p>
  </div>
  <div class="page">
    <p>Grappa Code: Expose DSM abstraction at language level using C++11 library</p>
    <p>void search(Vertex * vertex_addr) { Vertex v = *vertex_addr;</p>
    <p>Vertex * child0 = v.children; for( int i = 0; i &lt; v.num_children; ++i ) { search(child0+i); } }</p>
    <p>void search(GlobalAddress&lt;Vertex&gt; vertex_addr) { Vertex v = delegate::read(vertex_addr);</p>
    <p>GlobalAddress&lt;Vertex&gt; child0 = v.children; forall( 0, v.num_children, [child0](int64_t i) { search(child0+i); } }</p>
    <p>Grappa multi-node version</p>
    <p>Standard single-core versionSearching a large, unbalanced tree</p>
    <p>M Ve</p>
    <p>rt s/</p>
    <p>s</p>
    <p>System: Grappa Cray XMT1</p>
  </div>
  <div class="page">
    <p>Accessing data in the global address space</p>
    <p>Memory is partitioned by core All sharing is done using communication, so synchronization == scheduling</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>Global address space</p>
  </div>
  <div class="page">
    <p>var</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>var+1var+1var+1</p>
    <p>Accessing memory through delegates</p>
  </div>
  <div class="page">
    <p>var</p>
    <p>DRAM DRAM DRAM DRAM</p>
    <p>var+1</p>
    <p>var+1</p>
    <p>var+1</p>
    <p>Accessing memory through delegates</p>
    <p>var+1var+1var+1</p>
    <p>Move computation to data: All accesses to a word run on its home core</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>Delegation + aggregation makes random access fast</p>
    <p>GUPS pseudocode:</p>
    <p>int a[BIG]; int b[n] = {rand()}; for (i=0; i&lt;n; i++) a[ b[i] ]++;</p>
    <p>0e+00</p>
    <p>A to</p>
    <p>m ic</p>
    <p>in cr</p>
    <p>em en</p>
    <p>ts p</p>
    <p>er s</p>
    <p>ec on</p>
    <p>d</p>
    <p>Grappa delegate</p>
    <p>RDMA atomic increment</p>
  </div>
  <div class="page">
    <p>Building application frameworks on Grappa</p>
    <p>In-memory MapReduce GraphLab API</p>
    <p>Relational query execution engine</p>
  </div>
  <div class="page">
    <p>In-memory MapReduce</p>
    <p>Simple implementation of MapReduce model for iterative applications (no fault-tolerance)</p>
    <p>Compared with Spark, with faulttolerance disabled</p>
    <p>Benchmark: K-Means on SeaFlow ocean cytometry dataset (8.9GB)</p>
    <p>64 AMD Interlagos nodes, Mellanox 40Gb ConnectX-2 InfiniBand</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>ru nt</p>
    <p>im e</p>
    <p>Grappa Spark</p>
  </div>
  <div class="page">
    <p>Relational query execution</p>
    <p>Built a backend for the Raco relational algebra compiler/optimizer: github.com/uwescience/raco</p>
    <p>Queries are compiled into Grappa for() loops</p>
    <p>Compare with Shark, a Hive/SQL-like query system built on Spark using SP2Bench benchmark</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>ru nt</p>
    <p>im e</p>
    <p>Grappa Shark</p>
    <p>(16 nodes)</p>
  </div>
  <div class="page">
    <p>GraphLab on Grappa</p>
    <p>Subset of the GraphLab API described in PowerGraph paper</p>
    <p>GraphLab: replicated graph representation, complex partitioning strategy; Grappa: simple adjacency list, random partitioning</p>
    <p>Four benchmarks from GraphBench.org: PageRank, conn. components, SSSP, BFS</p>
    <p>Graphs: Friendster (65M vertices, 1.8B edges), Twitter (41M vertices, 1B edges)</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>R un</p>
    <p>tim e</p>
    <p>G rappa</p>
    <p>G raphLab (P</p>
    <p>D S</p>
    <p>)</p>
    <p>G raphLab (random</p>
    <p>)(31 nodes)</p>
  </div>
  <div class="page">
    <p>Why is Grappa fast?</p>
    <p>Much higher message rates</p>
    <p>Built to enable use of RDMA (but is still fast over TCP)</p>
    <p>Faster serialization</p>
    <p>Efficient fine-grained synchronization and scheduling</p>
  </div>
  <div class="page">
    <p>Not in the talk</p>
    <p>Also in paper: Deeper dive on performance Results from programming against Grappa directly</p>
    <p>Related projects:</p>
    <p>Alembic: Automatic Locality Extraction via Migration. B. Holt, P. Briggs, L. Ceze, M. Oskin OOPSLA 2014</p>
    <p>Radish: Compiling Efficient Query Plans for Distributed Shared Memory. B. Myers, D. Halperin, J. Nelson, M. Oskin, L. Ceze, B. Howe Tech report, October 2014</p>
    <p>Flat Combining Synchronized Global Data Structures. B. Holt, J. Nelson, B. Myers, P. Briggs, L. Ceze, S. Kahan, and M. Oskin International Conference on PGAS Programming Models (PGAS), October 2013</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Grappa is a platform for building new data-intensive analytics frameworks</p>
    <p>Latency tolerance enables fast distributed shared memory for analytics</p>
    <p>BSD-licensed source, more info:</p>
    <p>http://grappa.io</p>
  </div>
</Presentation>
