<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Distributed Computing</p>
    <p>Group</p>
    <p>A Self-Repairing Peer-to-Peer System Resilient to Dynamic</p>
    <p>Adversarial Churn</p>
    <p>Fabian Kuhn Stefan Schmid</p>
    <p>Roger Wattenhofer</p>
    <p>IPTPS, February 2005 Cornell University, Ithaca, New York, USA</p>
    <p>also held by Roger Wattenhofer at Dynamo COST 295 Paris, France; May 2005</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 2</p>
    <p>Dynamic Peer-to-Peer Systems</p>
    <p>Properties compared to centralized client/server approach</p>
    <p>Availability</p>
    <p>Reliability</p>
    <p>Efficiency</p>
    <p>=&gt; Peers may join and leave the network at any time!</p>
    <p>However, P2P systems are</p>
    <p>composed of unreliable desktop machines</p>
    <p>under control of individual users</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 3</p>
    <p>Churn</p>
    <p>How to maintain desirable properties such as</p>
    <p>Connectivity,</p>
    <p>Network diameter,</p>
    <p>Peer degree?</p>
    <p>Churn: Permanent joins and leaves</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 4</p>
    <p>Road-map</p>
    <p>Motivation for adversarial (worst-case) churn</p>
    <p>Components of our system</p>
    <p>Assembling the components</p>
    <p>Results</p>
    <p>Outlook and open problems</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 5</p>
    <p>Motivation</p>
    <p>Why permanent churn?</p>
    <p>Saroiu et al.: A Measurement Study of P2P File Sharing Systems</p>
    <p>Peers join system for one hour on average</p>
    <p>Hundreds of changes per second with millions of peers in the system!</p>
    <p>Why adversarial (worst-case) churn?</p>
    <p>E.g., a crawler takes down neighboring machines rather than randomly chosen peers!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 6</p>
    <p>The Adversary</p>
    <p>Model worst-case faults with an adversary ADV(J,L,)</p>
    <p>ADV(J,L,) has complete visibility of the entire state of the system</p>
    <p>May add at most J and remove at most L peers in any time period of length</p>
    <p>Note: Adversary is not Byzantine!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 7</p>
    <p>Synchronous Model</p>
    <p>Our system is synchronous, i.e., our algorithms run in rounds</p>
    <p>One round: receive messages, local computation, send messages</p>
    <p>However: Real distributed systems are asynchronous!</p>
    <p>But: Notion of time necessary to bound the adversary</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 8</p>
    <p>A First Approach</p>
    <p>What if number of peers is not 2i?</p>
    <p>How to prevent degeneration?</p>
    <p>Where to store data?</p>
    <p>Idea: Simulate the hypercube!</p>
    <p>Fault-tolerant hypercube?</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 9</p>
    <p>Simulated Hypercube System</p>
    <p>Basic components:</p>
    <p>Simulation: Node consists of several peers!</p>
    <p>Route peers to sparse areas</p>
    <p>Adapt dimension</p>
    <p>Token distribution algorithm!</p>
    <p>Information aggregation</p>
    <p>algorithm!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 10</p>
    <p>Components: Peer Distribution and Information Aggregation</p>
    <p>Peer Distribution</p>
    <p>Goal: Distribute peers evenly among all hypercube nodes in order to balance biased adversarial churn</p>
    <p>Basically a token distribution problem</p>
    <p>Counting the total number of peers (information aggregation)  Goal: Estimate the total number of peers in the system and adapt</p>
    <p>the dimension accordingly</p>
    <p>Ta ckl</p>
    <p>ed ne</p>
    <p>xt!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 11</p>
    <p>Peer Distribution (1)</p>
    <p>Algorithm: Cycle over dimensions and balance!</p>
    <p>Per fec</p>
    <p>tly bal</p>
    <p>anc ed</p>
    <p>aft er</p>
    <p>d s tep</p>
    <p>s!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 12</p>
    <p>Peer Distribution (2)</p>
    <p>But peers are not fractional!</p>
    <p>And an adversary inserts at most J and removes at most L peers per step!</p>
    <p>Theorem 1: Given adversary ADV(J,L,1), discrepancy never exceeds 2J+2L+d!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 13</p>
    <p>Components: Peer Distribution and Information Aggregation</p>
    <p>Peer Distribution</p>
    <p>Goal: Distribute peers evenly among all hypercube nodes in order to balance biased adversarial churn</p>
    <p>Basically a token distribution problem</p>
    <p>Counting the total number of peers (information aggregation)  Goal: Estimate the total number of peers in the system and adapt</p>
    <p>the dimension accordingly Ta ckl</p>
    <p>ed ne</p>
    <p>xt!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 14</p>
    <p>Information Aggregation (1)</p>
    <p>Goal: Provide the same (and good!) estimation of the total number of peers presently in the system to all nodes</p>
    <p>Thresholds for expansion and reduction</p>
    <p>Means: Exploit again the recursive structure of the hypercube!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 15</p>
    <p>Information Aggregation (2)</p>
    <p>Algorithm: Count peers in every sub-cube by exchange with corresponding neighbor!</p>
    <p>Co rre</p>
    <p>ct n um</p>
    <p>ber aft</p>
    <p>er d s</p>
    <p>tep s!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 16</p>
    <p>Information Aggregation (3)</p>
    <p>But again, we have a concurrent adversary!</p>
    <p>Solution: Pipelined execution!</p>
    <p>Theorem 2: The information aggregation algorithm yields the same estimation to all nodes. Moreover, this</p>
    <p>number represents the correct state of the system d steps ago!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 17</p>
    <p>Composing the Components</p>
    <p>Our system permanently runs</p>
    <p>Peer distribution algorithm to balance biased churn</p>
    <p>Information aggregation algorithm to estimate total number of peers and change dimension accordingly</p>
    <p>But: How are peers connected inside a node, and how are the edges of the hypercube represented?</p>
    <p>And: Where is the data of the DHT stored?</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 18</p>
    <p>Distributed Hash Table</p>
    <p>Hash function determines node where data item is replicated</p>
    <p>Problem: Peer which has to move to another node must replace all data items.</p>
    <p>Idea: Divide peers of a node into core and periphery</p>
    <p>Core peers store data,</p>
    <p>Peripheral peers are used for peer distribution</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 19</p>
    <p>Intra- and Interconnections</p>
    <p>Peers inside a node are completely connected.</p>
    <p>Peers are connected to all core peers of all neighboring nodes.</p>
    <p>May be improved: Lower peer degree by using a matching.</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 20</p>
    <p>Maintenance Algorithm</p>
    <p>Maintenance algorithm runs in phases</p>
    <p>Phase = 6 rounds</p>
    <p>In phase i:</p>
    <p>Snapshot of the state of the system in round 1</p>
    <p>One exchange to estimate number of peers in sub-cubes (information aggregation)</p>
    <p>Balances tokens in dimension i mod d</p>
    <p>Dimension change if necessary</p>
    <p>All based on the snapshot made in round 1, ignoring the changes that have happened in-between!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 21</p>
    <p>Results</p>
    <p>Given an adversary ADV(d+1,d+1,)... =&gt; Peer discrepancy at most 5d+4 (Theorem 1)</p>
    <p>=&gt; Total number of peers with delay d (Theorem 2)</p>
    <p>... we have, in spite of ADV(O(log n), O(log n), 1):</p>
    <p>always at least one core peer per node (no data lost!),</p>
    <p>at most O(log n) peers per node,</p>
    <p>network diameter O(log n).</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 22</p>
    <p>Summary</p>
    <p>Dynamics is a crucial part of every P2P system, but research is only emerging.</p>
    <p>Simulated topology: A simple blueprint for dynamic P2P systems!</p>
    <p>Requires algorithms for token distribution and information aggregation on the topology.</p>
    <p>Straight-forward for skip graphs</p>
    <p>Also possible for pancake graphs!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 23</p>
    <p>Open Problems</p>
    <p>Asynchrony: Message complexity!</p>
    <p>Data: Copying and load balancing?</p>
    <p>Byzantine peers?</p>
    <p>Thank you for your attention!</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 24</p>
    <p>Questions and Feedback?</p>
  </div>
  <div class="page">
    <p>Stefan Schmid, ETH Zurich @ IPTPS 2005 25</p>
    <p>Related Work</p>
    <p>Fiat, Saia, Gribble, Karlin, Saroiu  Censorship Resistant Peer-to-Peer Content Addressable Networks (SODA</p>
    <p>Abraham, Awerbuch, Azar, Bartal, Malkhi, Pavlov  A Generic Scheme for Building Overlay Networks in Adversarial Scenarios</p>
    <p>(IPDPS 2003)  Maintaining balanced network in the presence of concurrent faults  Times of quiescence</p>
    <p>Li, Misra, Plaxton  Active and Concurrent Topology Maintenance (DISC 2004)  Concurrent worst-case joins and leaves  Asynchronous  Weaker failure model: no crashes</p>
  </div>
</Presentation>
