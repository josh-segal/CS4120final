<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Awkward Disparity between BLEU / RIBES and</p>
    <p>Human Judgment in MT</p>
    <p>Liling Tan, Jon Dehdari and Josef van Genebith</p>
    <p>Saarland University, Germany</p>
    <p>@alvations</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Theres always a bone to pick on MT evaluation metrics (Babych and Hartley, 2004; CallisonBurch et al. 2006; Smith et al. 2014; Graham et al. 2015)</p>
    <p>Hypothesis 1: Appeared calm when he was taken to the American plane , which will to Miami , Florida . Hypothesis 2: which will he was , when taken Appeared calm to the American plane to Miami , Florida . Reference: Orejuela appeared calm as he was led to the American plane which will take him to Miami , Florida .</p>
    <p>Almost Same BLEU?!</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Conventional wisdom:</p>
    <p>lower BLEU not necessarily worse translation (Callison-Burch et al. 2006)</p>
    <p>higher BLEU = better translation</p>
    <p>(Callison-Burch et al. 2006; Nakazawa et al., 2014; Cettolo et al., 2014; Bojar et al., 2015)</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Callison-Burch et al. (2006) meta-evaluation on 2005 NIST MT Eval</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Conventional wisdom:</p>
    <p>lower BLEU not necessarily worse translation (Callison-Burch et al. 2006)</p>
    <p>higher BLEU = better translation</p>
    <p>(Callison-Burch et al. 2006; Nakazawa et al. 2014; Cettolo et al. 2014; Bojar et al. 2015)</p>
    <p>But is higher BLEU = better translation true?</p>
  </div>
  <div class="page">
    <p>BLEU</p>
    <p>Count the proportion of n-grams that appears in hypothesis and reference</p>
    <p>Penalize if the length of the hypothesis is too long</p>
  </div>
  <div class="page">
    <p>BLEU (in practice)</p>
    <p>Count the proportion of n-grams that appears in hypothesis and reference</p>
    <p>Penalize if the length of the hypothesis is too long</p>
  </div>
  <div class="page">
    <p>BLEU</p>
    <p>Hypothesis</p>
    <p>P1 : 90.0</p>
    <p>P2 : 78.9</p>
    <p>P3 : 66.7</p>
    <p>P4 : 52.9</p>
    <p>BP: 0.905</p>
    <p>BLEU: 64.03</p>
    <p>HUMAN: -5</p>
    <p>Baseline</p>
    <p>P1 : 84.2</p>
    <p>P2 : 66.7</p>
    <p>P3 : 47.1</p>
    <p>P4 : 25.0</p>
    <p>BP: 0.854</p>
    <p>BLEU: 43.29</p>
    <p>HUMAN: 0</p>
  </div>
  <div class="page">
    <p>RIBES</p>
    <p>Hypothesis</p>
    <p>RIBES: 94.04</p>
    <p>BLEU: 53.3</p>
    <p>HUMAN: -5</p>
    <p>Baseline</p>
    <p>RIBES: 86.33</p>
    <p>BLEU: 58.8</p>
    <p>HUMAN: 0</p>
  </div>
  <div class="page">
    <p>System Level HUMAN</p>
    <p>Hyp &lt; Base = 0 &lt; 5 = -1 HUMAN</p>
  </div>
  <div class="page">
    <p>System Level HUMAN</p>
    <p>Hyp &gt; Base = 3 &gt; 2 = +1 HUMAN</p>
  </div>
  <div class="page">
    <p>System Level HUMAN</p>
    <p>Hyp == Base = +0 HUMAN</p>
  </div>
  <div class="page">
    <p>Segment Level HUMAN</p>
    <p>#Hyp - #Base = 3 -2 = +1 HUMAN</p>
  </div>
  <div class="page">
    <p>Segment Level HUMAN</p>
    <p>#Hyp - #Base = 2 -2 = 0</p>
  </div>
  <div class="page">
    <p>#Hyp - #Base = 0 - 5 = -5 HUMAN</p>
    <p>Segment Level HUMAN</p>
  </div>
  <div class="page">
    <p>Experiment Setup (Our WAT Submission)</p>
  </div>
  <div class="page">
    <p>Results (Our WAT Submission)</p>
    <p>+15 BLEU -&gt; -17.75 HUMAN !!!</p>
  </div>
  <div class="page">
    <p>Results (Our WAT Submission)</p>
    <p>higher BLEU = better translation is not always true.</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>(Hint: click on the bubbles here on the interactive graph</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>Higher BLEU = Better translation (with 1-5 HUMAN)</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>Mostly, very good translation (4-5 HUMAN) dont go beyond +30 BLEU from baseline</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>Occasionally, lower BLEU is better translation but still in the range of 1-3 HUMAN score</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>There are some cases where &gt;+30 BLEU is as good as the baseline</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (+ve HUMAN)</p>
    <p>Sometimes, there are translations with &gt;+50 BLEU with low HUMAN scores.</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (-ve HUMAN)</p>
    <p>An interactive graph can be found here: https://plot.ly/173/~alvations/ (Hint: click on the bubbles here on the interactive graph</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (-ve HUMAN)</p>
    <p>Generally, -BLEU or RIBES from baseline means worse translations</p>
    <p>An interactive graph can be found here: https://plot.ly/171/~alvations/ (Hint: click on the bubbles here on the interactive graph</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (-ve HUMAN)</p>
    <p>Note that the grey bubbles are the same as the previous graph Its more prominent here since there are many more instances of +BLEU with 0 HUMAN score than negative HUMAN score</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (-ve HUMAN)</p>
    <p>There are +0 BLEU but around +10 RIBES that achieved -5 HUMAN score</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation (-ve HUMAN)</p>
    <p>Then, theres a whole lot of +BLEU that achieves  HUMAN scores, i.e. worse than baseline</p>
  </div>
  <div class="page">
    <p>Segment level Meta-Evaluation</p>
    <p>With regards to positive HUMAN scores, it fits the conventional wisdom that</p>
    <p>lower BLEU/RIBES = worse translation</p>
    <p>Higher BLEU/RIBES = better translation</p>
    <p>When it comes to negative HUMAN scores, it is inconsistent with the conventional wisdom</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Higher BLEU and RIBES doesnt necessary mean better translations  At segment level, &gt;+30 BLEU might not be reliable</p>
    <p>Possible reasons for BLEU/RIBES to not correlate with human judgments includes:  Minor lexical differences -&gt; huge difference in n-gram precision</p>
    <p>Minor MT evaluation metric differences not reflecting major translation inadequacy</p>
  </div>
  <div class="page">
    <p>References  Bogdan Babych and Anthony Hartley. 2004. Ex- tending the BLEU MT evaluation method with frequency weightings. In ACL.</p>
    <p>Onderej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the 2015 workshop on statistical machine translation. In WMT.</p>
    <p>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of Bleu in machine translation research. In EACL.</p>
    <p>Mauro Cettolo, Jan Niehues, Sebastian Stijker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT evaluation campaign, IWSLT 2014. In IWSLT.</p>
    <p>Yvette Graham, Timothy Baldwin, and Nitika Mathur. 2015. Accurate evaluation of segment-level machine translation metrics. In ACL.</p>
    <p>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In EMNLP.</p>
    <p>Toshiaki Nakazawa, Hideya Mino, Isao Goto, Graham Neubig, Sadao Kurohashi, and Eiichiro Sumita. 2015. Overview of the 2nd workshop on Asian translation. In WAT.</p>
    <p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</p>
    <p>Liling Tan and Francis Bond. 2014. Manipulating in- put data in machine translation. In WAT.</p>
    <p>Liling Tan, Josef van Genabith, and Francis Bond. 2015. Passive and pervasive use of bilingual dictionary in statistical machine translation. In HyTra. 33</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Experiment Setup (Our WAT Submission)</p>
  </div>
  <div class="page">
    <p>Results (Our WAT Submission)</p>
    <p>+15 BLEU -&gt; -17.75 HUMAN !!!</p>
  </div>
  <div class="page">
    <p>Models Log-Linear Weights (Our Baseline Replica)</p>
    <p># core weights [weight] LexicalReordering0= 0.0316949 0.0566969 0.0546839 0.0814468 0.0359473 0.0426681 Distortion0= 0.0445616 LM0= 0.274422 WordPenalty0= -0.132106 PhrasePenalty0= 0.0733761 TranslationModel0= 0.110846 0.030776 -0.013284 0.0174904 UnknownWordPenalty0= 1</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>Models Log-Linear Weights (Our MERT Run 2)</p>
    <p># core weights [weight] LexicalReordering0= 0.0156288 -0.0580331 0.0126421 0.0664739 0.137966 0.0303402 Distortion0= 0.048086 LM0= 0.301798 WordPenalty0= -0.029068 PhrasePenalty0= 0.0512106 TranslationModel0= 0.173756 0.0386685 -0.0237588 0.0125696 UnknownWordPenalty0= 1</p>
    <p>Despite the model differences, the results</p>
    <p>shows that higher BLEU = better translation is not always true.</p>
  </div>
</Presentation>
