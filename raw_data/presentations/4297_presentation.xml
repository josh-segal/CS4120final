<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Confidence Modeling for Neural Semantic Parsing</p>
    <p>July 16, 2018</p>
    <p>Li Dong*, Chris Quirk^, Mirella Lapata* *: University of Edinburgh</p>
    <p>^: Microsoft Research, Redmond</p>
  </div>
  <div class="page">
    <p>Model used in this work</p>
    <p>(Dong and Lapata, 2016; Jia and Liang, 2016)</p>
    <p>Neural Semantic Parsing (NSP)</p>
    <p>Sequence</p>
    <p>Encoder</p>
    <p>Sequence</p>
    <p>Decoder</p>
    <p>LS TM</p>
    <p>Android_phone_call,</p>
    <p>Any_phone_call_missed</p>
    <p>&lt;then&gt; Google_drive,</p>
    <p>Add_row_to_spreadsheet,</p>
    <p>((Spreadsheet_name</p>
    <p>missed) (Formatted_row</p>
    <p>( )) (Drivefolder_path</p>
    <p>IFTTT/Android))</p>
    <p>Attention Layer</p>
    <p>LS T</p>
    <p>M</p>
    <p>Archive your missed calls from Android to Google Drive</p>
    <p>Input</p>
    <p>Utterance</p>
    <p>Logical</p>
    <p>Form</p>
  </div>
  <div class="page">
    <p>Most models always tend to guess some outputs</p>
    <p>We also want to know how confident they are</p>
    <p>Confidence Modeling is Important</p>
    <p>Alexa, buy me something from</p>
    <p>Whole Foods</p>
    <p>Buying Whole Foods</p>
    <p>http://time.com/4821460/amazon-buying-whole-foods-twitter/</p>
  </div>
  <div class="page">
    <p>From the perspective of applications</p>
    <p>More reliable decisions</p>
    <p>Generate clarification questions to verify the results</p>
    <p>Nonlinearity of neural networks</p>
    <p>For linear models, (|)   Unclear for neural models (Johansen and Socher, 2017)</p>
    <p>Lack of explicit lexicons or templates</p>
    <p>Difficult to trace errors and inconsistencies</p>
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>Estimate confidence scores for NSP</p>
    <p>Higher score -&gt; the prediction is more likely correct</p>
    <p>Provide uncertainty interpretations</p>
    <p>Which parts of input contribute to uncertain predictions</p>
    <p>Research Goal</p>
  </div>
  <div class="page">
    <p>Confidence Estimation - Overview</p>
    <p>Encoder</p>
    <p>Sequence</p>
    <p>Decoder</p>
    <p>LS T M</p>
    <p>Android_phone_call,</p>
    <p>Any_phone_call_missed</p>
    <p>&lt;then&gt; Google_drive,</p>
    <p>Add_row_to_spreadsheet,</p>
    <p>((Spreadsheet_name</p>
    <p>missed) (Formatted_row</p>
    <p>( )) (Drivefolder_path</p>
    <p>IFTTT/Android))</p>
    <p>Attention Layer</p>
    <p>LS T M</p>
    <p>Archive your missed calls from Android to Google Drive</p>
    <p>Input</p>
    <p>Utterance</p>
    <p>Logical</p>
    <p>Form</p>
    <p>Logistic Model</p>
    <p>Confidence Score s ,  (0,1)</p>
    <p>Utterance  Prediction</p>
    <p>Confidence Metrics Characterize causes of uncertainty</p>
    <p>Indicate whether the prediction  is likely to be correct</p>
  </div>
  <div class="page">
    <p>Model is unconfident about (|)</p>
    <p>Model uncertainty</p>
    <p>Unsure about model parameters or structure</p>
    <p>Data uncertainty</p>
    <p>Out-of-distribution/-domain examples</p>
    <p>Estimate (|) reliably, but the entropy is large</p>
    <p>Input uncertainty</p>
    <p>Input itself is unspecific/ambiguous, which would lead to several different correct outputs</p>
    <p>Confidence Metrics</p>
  </div>
  <div class="page">
    <p>Posterior probability</p>
    <p>Sequence-level: log</p>
    <p>Token-level: avg log  ,&lt; , min{  ,&lt; }</p>
    <p>Dropout as a Bayesian approximation (Yarin Gal, Zoubin Ghahramani, 2016)</p>
    <p>Model Uncertainty</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>attention</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>1 2  1  &lt;s&gt; 1 2</p>
    <p>1 2  1 &lt;e&gt;1. Inject noise to the model multiple times</p>
  </div>
  <div class="page">
    <p>Out-of-distribution/-domain examples</p>
    <p>(|): probability of input</p>
    <p>KenLM (Heafield et al., 2013) estimated on the training set</p>
    <p>Number of unknown words of input</p>
    <p>Data Uncertainty</p>
  </div>
  <div class="page">
    <p>Variance of top candidates var</p>
    <p>Entropy of decoding  | =     log</p>
    <p>Approximated by Monte Carlo sampling</p>
    <p>Input Uncertainty</p>
    <p>1 1</p>
    <p>1</p>
    <p>2 1</p>
    <p>2</p>
    <p>1</p>
    <p>Beam Size=</p>
    <p>Predictive variance</p>
    <p>1 1</p>
    <p>1 2</p>
    <p>1</p>
    <p>2 1</p>
    <p>2 2</p>
    <p>2</p>
    <p>1</p>
    <p>2</p>
    <p>1</p>
    <p>2</p>
    <p>Sampling ~(|)</p>
  </div>
  <div class="page">
    <p>Use logistic regression to fit F1 scores of outputs</p>
    <p>Logistic loss:  = [ ln 1 +   + (1  )ln(1 +  )]</p>
    <p>Confidence Scoring</p>
    <p>Model Uncertainty Data Uncertainty Input Uncertainty</p>
    <p>Token-level  Dropout perturbation</p>
    <p>Gaussian noise  Posterior probability</p>
    <p>Probability of input  Number of</p>
    <p>unknown tokens</p>
    <p>Variance of top candidates</p>
    <p>Entropy of decoding Sequence-level</p>
    <p>Confidence Metrics</p>
    <p>Tree Boosting Model</p>
    <p>Confidence Score  (0,1)</p>
  </div>
  <div class="page">
    <p>Trace prediction uncertainty back to input words Users can verify or refine the input quickly</p>
    <p>Benefit the development cycle IF</p>
    <p>Tigger</p>
    <p>Weather</p>
    <p>Drops_below</p>
    <p>Tempera ture</p>
    <p>Degrees_in</p>
    <p>c</p>
    <p>Action</p>
    <p>Uncertainty Interpretation</p>
    <p>text me when its freezing</p>
    <p>?</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Uncertainty Backpropagation</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M attention</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>LS T</p>
    <p>M</p>
    <p>1 2  1  &lt;s&gt; 1 2</p>
    <p>1 2  1 &lt;e&gt;1 2 1</p>
    <p>1 2 1</p>
    <p>(Bach et al., 2015; Zhang et al., 2016)</p>
  </div>
  <div class="page">
    <p>Uncertainty Backpropagation</p>
    <p>: uncertainty</p>
    <p>score</p>
    <p>Parent  = {1,2}</p>
    <p>Child  = {1,2}</p>
  </div>
  <div class="page">
    <p>Uncertainty Backpropagation</p>
    <p>=</p>
    <p>Child()</p>
    <p>1: contribution</p>
    <p>ratio (how much</p>
    <p>we backprop</p>
    <p>from 1 to )</p>
    <p>Scores are backpropagated</p>
    <p>from child neurons</p>
  </div>
  <div class="page">
    <p>Uncertainty Backpropagation</p>
    <p>Parent()</p>
    <p>= 1 Contribution ratios from  to its</p>
    <p>parent neurons are normalized to 1</p>
  </div>
  <div class="page">
    <p>Fully-connected layers</p>
    <p>If 1 contributes more to s value, ratio 1  should</p>
    <p>be larger (i.e., backprop more from  to 1)</p>
    <p>Backpropagation Rules</p>
    <p>1  =</p>
    <p>|11|</p>
    <p>|11| + |11|  = (11 + 22)</p>
  </div>
  <div class="page">
    <p>IFTTT-style semantic parsing (Quirk et al., 2015) Archive your missed calls from Android to Google Drive</p>
    <p>Python code generation (Yin et al., 2017)</p>
    <p>Experiments</p>
  </div>
  <div class="page">
    <p>Confidence Estimation</p>
    <p>Posterior ConfS p</p>
    <p>e a</p>
    <p>rm a</p>
    <p>n</p>
    <p>c o</p>
    <p>rr e</p>
    <p>la ti</p>
    <p>o n</p>
    <p>Python</p>
    <p>Posterior ConfS p</p>
    <p>e a</p>
    <p>rm a</p>
    <p>n</p>
    <p>c o</p>
    <p>rr e</p>
    <p>la ti</p>
    <p>o n</p>
    <p>IFTTT</p>
    <p>Spearman  correlation ( 1,1 ) between confidence score and F1 score</p>
  </div>
  <div class="page">
    <p>Confidence scores are used as threshold to filter out uncertain examples</p>
    <p>Confidence Estimation</p>
    <p>IFTTT</p>
  </div>
  <div class="page">
    <p>Importance of Confidence Metrics</p>
    <p>Fe a</p>
    <p>tu re</p>
    <p>I m</p>
    <p>p o</p>
    <p>rt a</p>
    <p>n ce</p>
    <p>IFTTT Python</p>
    <p>Model Uncertainty Data Uncertainty Input Uncertainty</p>
  </div>
  <div class="page">
    <p>Agreement of top-4 uncertain input words</p>
    <p>Between model prediction and gold standard</p>
    <p>Uncertainty Interpretation</p>
    <p>o v</p>
    <p>e rl</p>
    <p>a p</p>
    <p>@ 4</p>
    <p>Attention BackProp</p>
    <p>IFTTT Python</p>
  </div>
  <div class="page">
    <p>Examples - IFTTT</p>
    <p>ATT: attention; BP: uncertainty backpropagation</p>
  </div>
  <div class="page">
    <p>Thanks! Q&amp;A</p>
    <p>Code Available:</p>
    <p>http://homepages.inf.ed.ac.uk/s1478528</p>
  </div>
</Presentation>
