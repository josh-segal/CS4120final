<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Improving Service Availability of Cloud Systems by Predicting Disk Error</p>
    <p>Yong Xu*, Kaixin Sui, Qingwei Lin, Keceng Jiang,</p>
    <p>Wenchi Zhang , Jian-Guang Lou, Dongmei Zhang Randolph Yao, Yingnong Dang, Murali Chintalapati</p>
    <p>Hongyu Zhang</p>
    <p>Microsoft Research Asia, Beijing, China Microsoft Azure, Redmond, USA</p>
    <p>Peng Li</p>
    <p>The University of Newcastle, Australia Nankai University, China</p>
    <p>USENIX ATC, July 12, 2018</p>
  </div>
  <div class="page">
    <p>&lt; 26 Sec</p>
    <p>&gt;99.999%</p>
    <p>High availability remains one of the top priorities of cloud systems.</p>
    <p>Motivation  Towards High Cloud Service Availability</p>
  </div>
  <div class="page">
    <p>Motivation  Impact of Disk Error on Cloud Service Availability</p>
    <p>Hardware issue is one of the top reasons of VM downtime</p>
    <p>Disk error contributes most to Hardware issue</p>
    <p>Disk error may result in irreversible data loss disaster</p>
    <p>Unplanned VM downtime is highly painful to customers.</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>HEALTHY UNHEALTHYRISKY</p>
    <p>Allocate new VM to</p>
    <p>predicted healthier disks Live Migration</p>
    <p>Improve VM availability by early prediction of disk errors and guide Live Migration ( moving VMs to healthy node without disconnection to the client or application.</p>
    <p>&gt; 99.999 %</p>
  </div>
  <div class="page">
    <p>State-of-the-art</p>
    <p>Complete failureDisk SMART data prediction</p>
    <p>model</p>
    <p>Methodology model venue</p>
    <p>Statistical Threshold setting FAST</p>
    <p>KDD</p>
    <p>USENIX ATC</p>
    <p>Unsupervised Clustering Markov chain</p>
    <p>Supervised</p>
    <p>classification</p>
    <p>SVM</p>
    <p>Neural Network Decision Tree</p>
    <p>Random Forest</p>
    <p>Self-Monitoring, Analysis and Reporting Technology</p>
    <p>Predicting disk errors in industrial settings is difficult.</p>
  </div>
  <div class="page">
    <p>State-of-the-art</p>
    <p>Complete failureDisk SMART data prediction</p>
    <p>model</p>
    <p>Methodology model venue</p>
    <p>Statistical Threshold setting FAST</p>
    <p>KDD</p>
    <p>USENIX ATC</p>
    <p>Unsupervised Clustering Markov chain</p>
    <p>Supervised</p>
    <p>classification</p>
    <p>SVM</p>
    <p>Neural Network Decision Tree</p>
    <p>Random Forest</p>
    <p>Self-Monitoring, Analysis and Reporting Technology</p>
    <p>Predicting disk errors in industrial settings is difficult.</p>
    <p>No real-production adoption reported in existing work.</p>
  </div>
  <div class="page">
    <p>VM downtime occurs far before disk complete failure</p>
    <p>Existing prediction flow(cross-validation guided) goes wrong</p>
    <p>Training with extremely imbalanced health labels of disks is difficult</p>
    <p>Why predicting disk errors in real production is difficult?</p>
    <p>The proof of the pudding is in the eating.</p>
    <p>Insights beyond laboratory work.</p>
  </div>
  <div class="page">
    <p>VM downtime occurs far before disk complete failure</p>
    <p>Existing prediction flow(cross-validation guided) goes wrong</p>
    <p>Training with extremely imbalanced health labels of disks is difficult</p>
    <p>Why predicting disk errors in real production is difficult?</p>
    <p>The proof of the pudding is in the eating.</p>
  </div>
  <div class="page">
    <p>VM VM</p>
    <p>Complete failure</p>
    <p>VM down by disk errors (I/O latency, VM not responding, etc)</p>
    <p>Problem 1  Predicting complete failure is not helpful to prevent VM downtime</p>
    <p>VM downtime occurs far before complete failure of disks.</p>
  </div>
  <div class="page">
    <p>SMART data</p>
    <p>System-level Signals (earlier signals of disk errors)</p>
    <p>Disk Errors (latency, timeout, sector error, etc)</p>
    <p>Complete failureDisk SAMRT data</p>
    <p>prediction model</p>
    <p>prediction model</p>
    <p>time</p>
    <p>Solution - Incorporate system-level features</p>
    <p>System-level signals manifest earlier symptoms of disk errors.</p>
  </div>
  <div class="page">
    <p>VM downtime occurs far before disk complete failure</p>
    <p>Existing prediction flow(cross-validation guided) goes wrong</p>
    <p>Training with extremely imbalanced health labels of disks is difficult</p>
    <p>Why predicting disk errors in real production is difficult?</p>
    <p>The proof of the pudding is in the eating.</p>
  </div>
  <div class="page">
    <p>Problem 2- Cross-Validation Guided prediction goes wrong</p>
    <p>Model</p>
    <p>Training data Prediction data</p>
    <p>trainingvalidation</p>
    <p>First iteration</p>
    <p>Second iteration</p>
    <p>Third iteration</p>
    <p>Cross Validation</p>
    <p>State-of-the-art do prediction in cross-validation guided way,</p>
    <p>not applicable in real production scenario.</p>
  </div>
  <div class="page">
    <p>CV training Prediction</p>
    <p>T P</p>
    <p>R (</p>
    <p>FP R</p>
    <p>= 0</p>
    <p>)</p>
    <p>CV-guided model lead to Low result in real online prediction</p>
    <p>Good result of CV-guided evaluation.</p>
    <p>Problem 2- Cross-Validation guided prediction goes wrong</p>
    <p>Experiment result shows good result in CV evaluation, but poor result in real online prediction.</p>
  </div>
  <div class="page">
    <p>Problem 2- Cross-Validation guided prediction goes wrong</p>
    <p>Environment change at t</p>
    <p>time</p>
    <p>validationTraining</p>
    <p>Prone to highlight the features(i.e. one-off outage) that are essentially not that predictive</p>
    <p>no/different changes in the future</p>
    <p>Eg. Rack 3 encounter outage at time t.</p>
    <p>Fundamentally, training phase of Cross-Validation is not applicable for disk error prediction.</p>
  </div>
  <div class="page">
    <p>Problem 2- Cross-Validation guided prediction goes wrong</p>
    <p>Environment change at t</p>
    <p>time</p>
    <p>validationTraining</p>
    <p>Prone to highlight the features(i.e. one-off outage) that are essentially not that predictive</p>
    <p>no/different changes in the future</p>
    <p>Eg. Rack 3 encounter outage at time t.</p>
    <p>Fundamentally, training phase of Cross-Validation is not applicable for disk error prediction.</p>
    <p>Errors of different disks dont happen independently</p>
    <p>in complex cloud systems.</p>
  </div>
  <div class="page">
    <p>Solution  Online prediction guided way</p>
    <p>Prediction dataTraining data</p>
    <p>Feature selection</p>
    <p>Model validation</p>
    <p>Model</p>
    <p>Online prediction guided</p>
  </div>
  <div class="page">
    <p>Solution  Online prediction guided way</p>
    <p>Training data</p>
    <p>Model</p>
    <p>Will prune the features that related to the change</p>
    <p>Prediction data</p>
    <p>Validation Training</p>
    <p>time Online prediction guided</p>
    <p>Strictly separate training and validation set by time.</p>
  </div>
  <div class="page">
    <p>Cross-Validation guided vs. Online prediction guided</p>
    <p>Online-prediction guided outperforms.</p>
  </div>
  <div class="page">
    <p>VM downtime occur before disk complete failure</p>
    <p>Existing prediction flow(cross-validation guided) go wrong</p>
    <p>Training with extremely imbalanced health labels of disks is difficult</p>
    <p>Why predicting disk errors in real production is difficult?</p>
    <p>The proof of the pudding is in the eating.</p>
  </div>
  <div class="page">
    <p>Fault : good ~3 : 10,000</p>
    <p>prone to predict all to be good low recall</p>
    <p>Problem 3  Extremely imbalanced dataset</p>
    <p>Extremely small portion of fault samples leads to low recall using common classification model.</p>
  </div>
  <div class="page">
    <p>Rethinking the problem</p>
    <p>Ranking instead of Classification</p>
  </div>
  <div class="page">
    <p>Solution - Cost-sensitive ranking model</p>
    <p>Live Migration</p>
    <p>New VMs allocated to healthier disks</p>
    <p>Predicted worst</p>
    <p>Predicted healthier</p>
    <p>Predicted risky</p>
    <p>Ranking Model</p>
    <p>Best cutting point r = argmin(Cost= Cost1*FP + Cost2* FN)</p>
    <p>False predictions, both false positive(FP) and false negative(FN), bring cost to real cloud system.</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Dataset  Real dataset from Azure</p>
    <p>Training: October 2017</p>
    <p>Testing: 3 parts divided from November 2017</p>
    <p>Healthy disks: faulty disks is ~10,000 : 3</p>
    <p>Setup  Data store and process: Microsoft COSMOS</p>
    <p>Ranking algorithm: FastTree implemented by Microsoft AzureML</p>
    <p>Windows Server 2012 with Intel CPU E5-4657L v2 @2.40GHz 2.40 with 1.0 TB Memory</p>
    <p>Evaluation metrics  True Positive Rate(TPR) = TP/(TP + FN), under 0.1% False Positive Rate(FPR) = FP/(FP + TN)</p>
  </div>
  <div class="page">
    <p>Result</p>
    <p>RQ1: How effective is the proposed approach in predicting disk errors?</p>
  </div>
  <div class="page">
    <p>Result</p>
    <p>RQ2: How effective is the proposed OnlinePrediction-guided way?</p>
  </div>
  <div class="page">
    <p>Result</p>
    <p>RQ3: How effective is the proposed ranking model?</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Point out the CrossValidation-guided prediction does not work for real online prediction in industry settings, and develop an OnlinePrediction-guided approach</p>
    <p>Leverage system-level signals in additional to SMART data in disk fault prediction</p>
    <p>Propose a ranking model to conquer the issue of extremely data imbalance</p>
    <p>Deployed to large scale industrial cloud system, Microsoft Azure, and significantly improved Azure service availability</p>
  </div>
</Presentation>
