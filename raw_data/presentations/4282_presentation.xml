<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Goran Glava Data &amp; Web Science Group</p>
    <p>University of Mannheim</p>
    <p>ACL, Melbourne</p>
    <p>July 16, 2018</p>
    <p>University of Cambridge</p>
  </div>
  <div class="page">
    <p>You shall know the meaning of the word</p>
    <p>by the company it keeps</p>
    <p>Words that occur in similar contexts tend to have similar meanings</p>
    <p>Harris, 1954</p>
  </div>
  <div class="page">
    <p>Words co-occur in text due to  Paradigmatic relations (e.g., synonymy, hypernymy), but also due to</p>
    <p>Syntagmatic relations (e.g., selectional preferences)</p>
    <p>Distributional vectors conflate all types of association  driver and car are not paradigmatically related</p>
    <p>Not synonyms, not antonyms, not hypernyms, not co-hyponyms, etc.</p>
    <p>But both words will co-occur frequently with</p>
    <p>driving, accident, wheel, vehicle, road, trip, race, etc.</p>
  </div>
  <div class="page">
    <p>Key idea: refine vectors using external resources</p>
    <p>Specializing vectors for semantic similarity</p>
    <p>Integrate external constraints into the learning objective</p>
    <p>E.g., Yu &amp; Dredze, 14; Kiela et al., 15; Osborne et al., 16; Nguyen et al., 17</p>
    <p>Modify the pre-trained word embeddings using lexical constraints</p>
    <p>E.g., Faruqui et al., 15; Wieting et al., 15; Mrki et al., 16; Mrki et al., 17</p>
  </div>
  <div class="page">
    <p>Joint specialization models  (+) Specialize the entire vocabulary (of the corpus)</p>
    <p>() Tailored for a specific embedding model</p>
    <p>Retrofitting models  () Specialize only the vectors of words found in external constraints</p>
    <p>(+) Applicable to any pre-trained embedding space</p>
    <p>(+) Much better performance than joint models (Mrki et al., 2016)</p>
  </div>
  <div class="page">
    <p>Best of both worlds  Performance and flexibility of retrofitting models, while</p>
    <p>Specializing entire embedding spaces (vectors of all words)</p>
    <p>Simple idea  Learn an explicit retrofitting/specialization function</p>
    <p>Using external lexical constraints as training examples</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Constraints (synonyms and antonyms) used as training examples for learning the explicit specialization function  Non-linear: Deep Feed-Forward Network (DFFN)</p>
  </div>
  <div class="page">
    <p>Specialization function: x = f(x)</p>
    <p>Distance function: g(x1, x2)</p>
    <p>Assumptions 1. (wi, wj, syn)  embeddings as close as possible after specialization</p>
    <p>g(xi, xj) = gmin</p>
    <p>g(xi, xj) = gmax</p>
    <p>g(xi, xj) = g(xi, xj)</p>
  </div>
  <div class="page">
    <p>Micro-batches  each constraint (wi, wj, r) paired with  K pairs {(wi, wm</p>
    <p>k)}k  wm k most similar to wi in distributional space</p>
    <p>K pairs {(wj, wn k)}k  wn</p>
    <p>k most similar to wj in distributional space</p>
    <p>Total: 2K+1 word pairs</p>
  </div>
  <div class="page">
    <p>Contrastive Objective (CNT)</p>
    <p>Regularization</p>
    <p>Gold diff. Predicted diff.</p>
    <p>= 0</p>
    <p>= 2</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Distance function g: cosine distance</p>
    <p>DFFN activation function: hyperbolic tangent</p>
    <p>Constraints from previous work (Zhang et al, 14; Ono et al., 15)  1M synonymy constraints</p>
    <p>380K antonymy constraints</p>
    <p>But only 57K unique words in these constraints!</p>
    <p>10% of micro-batches used for model validation  H (hidden layers) = 5, dh (layer size) = 1000,  = 0.3  K = 4 (micro-batch size = 9), batches of 100 micro-batches</p>
    <p>ADAM optimization (Kingma &amp; Ba, 2015) 15</p>
  </div>
  <div class="page">
    <p>SimLex-999 (Hill et al., 2014), SimVerb-3500 (Gerz et al., 2016)</p>
    <p>Important aspect: percentage of test words covered by constraints</p>
    <p>Comparison with Attract-Repel (Mrki et al., 2017)</p>
    <p>GloVe-CC fastText SGNS-W2</p>
    <p>SimLex, lexically disjoint (0%)</p>
    <p>Distributional Attract-Repel Explicit retrofitting</p>
    <p>GloVe-CC fastText SGNS-W2</p>
    <p>SimLex, lexical overlap (99%)</p>
    <p>Distributional Attract-Repel Explicit retrofitting</p>
  </div>
  <div class="page">
    <p>Intrinsic evaluation depicts two extreme settings</p>
    <p>Lexical overlap setting  Synonymy and antonymy constraints contain 99% of SL and SV words</p>
    <p>Performance is an optimistic estimate or true performance</p>
    <p>Lexically disjoint setting  Constraints contain 0% of SL and SV words</p>
    <p>Performance is a pessimistic estimate of true performance</p>
    <p>Realistic setting: downstream tasks  Coverage of test set words by constraints between 0% and 100%</p>
  </div>
  <div class="page">
    <p>Dialog state tracking (DST)  first component of a dialog system  Neural Belief Tracker (NBT) (Mrki et al., 17)  Makes inferences purely based on an embedding space</p>
    <p>57% of words in NBT test set (Wen et al., 17) covered by specialization constraints</p>
    <p>Lexical simplification (LS)  complex words to simpler synonyms  Light-LS (Glava &amp; tajner, 15)  decisions purely based on an embedding space</p>
    <p>59% of LS dataset words (Horn et al., 14) found in specialization constraints</p>
    <p>Crucial to distinguish similarity from relatedness  DST: cheap pub in the east vs. expensive restaurant in the west</p>
    <p>LS: Ferraris pilot Sebastian Vettel won the race., driver vs. airplane</p>
  </div>
  <div class="page">
    <p>Lexical simplification (LS) and Dialog state tracking (DST)</p>
    <p>GloVe-CC</p>
    <p>DST</p>
    <p>Distributional Attract-Repel Explirefit</p>
    <p>GloVe-CC fastText SGNS-W2</p>
    <p>LS</p>
    <p>Distributional Attract-Repel Explirefit</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Lexico-semantic resources such as WordNet needed to collect synonymy and antonymy constraints</p>
    <p>Idea: use shared bilingual embedding spaces to transfer the specialization to another language</p>
    <p>Most models learn a (simple) linear mapping  Using word alignments (Mikolov et al., 2013; Smith et al., 2017)</p>
    <p>Without word alignments (Lample et al., 2018; Artetxe et al., 2018) 21</p>
    <p>*Image taken from</p>
    <p>Lample et al., ICLR 2018</p>
  </div>
  <div class="page">
    <p>Transfer to three languages: DE, IT, and HR  Different levels of proximity to English</p>
    <p>Variants of SimLex-999 exist for each of these three languages</p>
    <p>German (DE) Italian (IT) Croatian (HR)</p>
    <p>Cross-lingual specialization transfer</p>
    <p>Distributional ExpliRefit (language transfer)</p>
  </div>
  <div class="page">
    <p>Retrofitting models specialize (i.e., fine-tune) distributional vectors for semantic similarity  Shortcoming: specialize only vectors of words seen in external constraints</p>
    <p>Explicit retrofitting  Learning the specialization function using constrains as training examples</p>
    <p>Able to specialize distributional vectors of all words</p>
    <p>Good intrinsic (SL, SV) and downstream (DST, LS) performance</p>
    <p>Cross-lingual specialization transfer possible for languages without lexico-semantic resources</p>
  </div>
  <div class="page">
    <p>Code &amp; data  https://github.com/codogogo/explirefit</p>
    <p>Contact  goran@informatik.uni-mannheim.de</p>
    <p>iv250@hermes.cam.ac.uk</p>
  </div>
</Presentation>
