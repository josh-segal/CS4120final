<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>IX: A Protected Dataplane Opera3ng System for High Throughput and</p>
    <p>Low Latency</p>
    <p>Adam Belay, George Prekas, Samuel Grossman, Ana Klimovic,</p>
    <p>Christos Kozyrakis, Edouard Bugnion</p>
  </div>
  <div class="page">
    <p>HW is fast, but SW is a BoLleneck</p>
    <p>Microseconds</p>
    <p>HW Limit</p>
    <p>Linux</p>
    <p>IX</p>
    <p>Requests per Second</p>
    <p>M ill io ns</p>
  </div>
  <div class="page">
    <p>HW is fast, but SW is a BoLleneck</p>
    <p>Microseconds</p>
    <p>HW Limit</p>
    <p>Linux</p>
    <p>IX</p>
    <p>Requests per Second</p>
    <p>M ill io ns</p>
  </div>
  <div class="page">
    <p>IX Closes the SW Performance Gap</p>
    <p>Microseconds</p>
    <p>HW Limit</p>
    <p>Linux</p>
    <p>IX</p>
    <p>Requests per Second</p>
    <p>M ill io ns</p>
  </div>
  <div class="page">
    <p>Two Contribu3ons</p>
    <p>#1: Protec3on and direct HW access through virtualiza3on</p>
    <p>#2: Execu3on model for low latency and high throughput</p>
    <p>Microseconds</p>
    <p>HW Limit</p>
    <p>Linux</p>
    <p>IX</p>
    <p>Requests per Second</p>
    <p>M ill io ns</p>
  </div>
  <div class="page">
    <p>Why is SW Slow?</p>
    <p>Created by: Arnout Vandecappelle hLp://www.linuxfounda3on.org/collaborate/workgroups/networking/kernel_flow 6</p>
    <p>Complex Interface</p>
    <p>Code Paths Convoluted by Interrupts and Scheduling</p>
  </div>
  <div class="page">
    <p>Problem: 1980s Sobware Architecture</p>
    <p>Berkeley sockets, designed for CPU 3me sharing  Todays large-scale datacenter workloads:</p>
    <p>Hardware: Dense Mul;core + 10 GbE (soon 40) - API scalability cri3cal! - Gap between compute and RAM -&gt; Cache behavior maLers - Packet inter-arrival 3mes of 50 ns</p>
    <p>Scale out access paFerns - Fan-in -&gt; Large connec3on counts, high request rates - Fan-out -&gt; Tail latency maLers!</p>
  </div>
  <div class="page">
    <p>Conven3onal Wisdom  Bypass the kernel</p>
    <p>Move TCP to user-space (Onload, mTCP, Sandstorm)  Move TCP to hardware (TOE)</p>
    <p>Avoid the connec3on scalability boLleneck  Use datagrams instead of connec3ons (DIY conges3on management)  Use proxies at the expense of latency</p>
    <p>Replace classic Ethernet  Use a lossless fabric (Infiniband)  Offload memory access (rDMA)</p>
    <p>Common thread: Give up on systems soJware</p>
  </div>
  <div class="page">
    <p>Our Approach  Bypass the kernel</p>
    <p>Move TCP to user-space (Onload, mTCP, Sandstorm)  Move TCP to hardware (TOE)</p>
    <p>Avoid the connec3on scalability boLleneck  Use datagrams instead of connec3ons (DIY conges3on management)  Use proxies at the expense of latency</p>
    <p>Replace classic Ethernet  Use a lossless fabric (Infiniband)  Offload memory access (rDMA)</p>
    <p>Tackle the problem head on</p>
    <p>Robust Protec;on Between App and Netstack</p>
    <p>Connec;on Scalability</p>
    <p>Commodity 10Gb Ethernet</p>
  </div>
  <div class="page">
    <p>Separa3on of Control and Data Plane</p>
    <p>DP</p>
    <p>DP</p>
    <p>Host Kernel</p>
    <p>CP</p>
    <p>Userspace</p>
    <p>Kernelspace</p>
    <p>C C C C C</p>
  </div>
  <div class="page">
    <p>Separa3on of Control and Data Plane</p>
    <p>DP</p>
    <p>DP</p>
    <p>Host Kernel</p>
    <p>CP</p>
    <p>Userspace</p>
    <p>Kernelspace</p>
    <p>C C C C C</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
  </div>
  <div class="page">
    <p>Separa3on of Control and Data Plane</p>
    <p>IX DP</p>
    <p>IX DP</p>
    <p>Host Kernel</p>
    <p>IX CP</p>
    <p>C C C C C</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
    <p>Host Ring 0</p>
  </div>
  <div class="page">
    <p>Separa3on of Control and Data Plane</p>
    <p>IX DP</p>
    <p>IX DP</p>
    <p>Linux kernel</p>
    <p>IX CP</p>
    <p>C C C C C</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
    <p>Host Ring 0 Dune</p>
  </div>
  <div class="page">
    <p>Separa3on of Control and Data Plane</p>
    <p>Linux kernel</p>
    <p>IX CP</p>
    <p>C C C C C</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>RX TX</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
    <p>Host Ring 0 Dune</p>
    <p>IX</p>
    <p>libIX</p>
    <p>Memcached</p>
    <p>IX</p>
    <p>libIX</p>
    <p>HTTPd</p>
  </div>
  <div class="page">
    <p>The IX Execu3on Pipeline</p>
    <p>event-driven app</p>
    <p>libIX</p>
    <p>RX TX</p>
    <p>TCP/IP</p>
    <p>RX FIFO</p>
    <p>Event Condi3ons</p>
    <p>Batched Syscalls</p>
    <p>TCP/IP</p>
    <p>Timer</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
  </div>
  <div class="page">
    <p>Design (1): Run to Comple3on</p>
    <p>event-driven app</p>
    <p>libIX</p>
    <p>RX TX</p>
    <p>TCP/IP</p>
    <p>RX FIFO</p>
    <p>Event Condi3ons</p>
    <p>Batched Syscalls</p>
    <p>TCP/IP</p>
    <p>Timer</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
    <p>Improves Data-Cache Locality Removes Scheduling Unpredictably</p>
  </div>
  <div class="page">
    <p>Design (2): Adap3ve Batching</p>
    <p>event-driven app</p>
    <p>libIX</p>
    <p>RX TX</p>
    <p>TCP/IP</p>
    <p>RX FIFO</p>
    <p>Event Condi3ons</p>
    <p>Batched Syscalls</p>
    <p>TCP/IP</p>
    <p>Timer</p>
    <p>Ring 3</p>
    <p>Guest Ring 0</p>
    <p>Improves Instruc;on-Cache Locality and Prefetching</p>
    <p>Adap3ve Batch Calcula3on</p>
  </div>
  <div class="page">
    <p>See the Paper for more Details</p>
    <p>Design (3): Flow consistent hashing  Synchroniza3on &amp; coherence free opera3on</p>
    <p>Design (4): Na3ve zero-copy API  Flow control exposed to applica3on</p>
    <p>Libix: Libevent-like event-based programming  IX prototype implementa3on  Dune, DPDK, LWIP, ~40K SLOC of kernel code</p>
  </div>
  <div class="page">
    <p>Evalua3on</p>
    <p>Comparison IX to Linux and mTCP [NSDI 14]  TCP microbenchmarks and Memcached</p>
    <p>~ 25 Linux Hosts</p>
    <p>IX IX 1x10GbE 4x10GbE</p>
    <p>w/ L3+L4 bond</p>
  </div>
  <div class="page">
    <p>TCP Netpipe</p>
    <p>G o o d p u t</p>
    <p>(G b p s)</p>
    <p>Message Size (KB)</p>
    <p>mTCP-mTCP Linux-Linux</p>
    <p>IX-IX</p>
    <p>Bandwidth @ 20 KB</p>
    <p>Bandwidth @ 135 KB</p>
  </div>
  <div class="page">
    <p>TCP Echo: Mul3core Scalability for Short Connec3ons</p>
    <p>M e s s a g e s / s e c ( x 1 0 6 )</p>
    <p>Number of CPU cores</p>
    <p>IX 10GbE IX 4x10GbE Linux 10GbE Linux 4x10GbE mTCP 10GbE</p>
    <p>Saturates 1x10GbE</p>
  </div>
  <div class="page">
    <p>Connec3on Scalability</p>
    <p>M e ss</p>
    <p>a g</p>
    <p>e s/</p>
    <p>se c (</p>
    <p>x 1</p>
    <p>Connection Count (log scale)</p>
    <p>Linux-10Gbps Linux-40Gbps</p>
    <p>IX-10Gbps IX-40Gbps</p>
    <p>~10,000 Connec;ons Limited by L3</p>
  </div>
  <div class="page">
    <p>Memcached over TCP</p>
    <p>L a te</p>
    <p>n c y (</p>
    <p>s)</p>
    <p>USR: Throughput (RPS x 10 3 )</p>
    <p>SLA</p>
    <p>IX (p99) IX (avg) Linux (p99) Linux (avg)</p>
    <p>Latency</p>
    <p>With IX clients</p>
  </div>
  <div class="page">
    <p>IX Conclusion</p>
    <p>A protected dataplane OS for datacenter applica3ons with an event-driven model and demanding connec3on scalability requirements</p>
    <p>Efficient access to HW, without sacrificing security, through virtualiza3on</p>
    <p>High throughput and low latency enabled by a dataplane execu3on model</p>
  </div>
</Presentation>
