<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning the Structure of Markov Logic</p>
    <p>Networks</p>
    <p>Stanley Kok &amp; Pedro Domingos</p>
    <p>Dept. of Computer Science and Eng.</p>
    <p>University of Washington</p>
  </div>
  <div class="page">
    <p>Overview  Motivation  Background  Structure Learning Algorithm  Experiments  Future Work &amp; Conclusion</p>
  </div>
  <div class="page">
    <p>Motivation  Statistical Relational Learning (SRL)</p>
    <p>combines the benefits of:  Statistical Learning: uses probability to handle</p>
    <p>uncertainty in a robust and principled way  Relational Learning: models domains with</p>
    <p>multiple relations</p>
  </div>
  <div class="page">
    <p>Motivation  Many SRL approaches combine a logical</p>
    <p>language and Bayesian networks  e.g. Probabilistic Relational Models</p>
    <p>[Friedman et al., 1999]</p>
    <p>The need to avoid cycles in Bayesian networks causes many difficulties [Taskar et al., 2002]</p>
    <p>Started using Markov networks instead</p>
  </div>
  <div class="page">
    <p>Motivation  Relational Markov Networks [Taskar et al., 2002]</p>
    <p>conjunctive database queries + Markov networks  Require space exponential in the size of the cliques</p>
    <p>Markov Logic Networks [Richardson &amp; Domingos, 2004]  First-order logic + Markov networks  Compactly represent large cliques  Did not learn structure (used external ILP system)</p>
  </div>
  <div class="page">
    <p>Motivation  Relational Markov Networks [Taskar et al., 2002]</p>
    <p>conjunctive database queries + Markov networks  Require space exponential in the size of the cliques</p>
    <p>Markov Logic Networks [Richardson &amp; Domingos, 2004]  First-order logic + Markov networks  Compactly represent large cliques  Did not learn structure (used external ILP system)</p>
    <p>This paper develops a fast algorithm that learns MLN structure  Most powerful SRL learner to date</p>
  </div>
  <div class="page">
    <p>Overview  Motivation  Background  Structure Learning Algorithm  Experiments  Future Work &amp; Conclusion</p>
  </div>
  <div class="page">
    <p>Markov Logic Networks First-order KB: set of hard constraints</p>
    <p>Violate one formula, a world has zero probability</p>
    <p>MLNs soften constraints  OK to violate formulas  The fewer formulas a world violates,</p>
    <p>the more probable it is  Gives each formula a weight,</p>
    <p>reflects how strong a constraint it is</p>
  </div>
  <div class="page">
    <p>MLN Definition  A Markov Logic Network (MLN) is a set of</p>
    <p>pairs (F, w) where  F is a formula in first-order logic  w is a real number</p>
    <p>Together with a finite set of constants, it defines a Markov network with  One node for each grounding of each predicate</p>
    <p>in the MLN  One feature for each grounding of each formula F</p>
    <p>in the MLN, with the corresponding weight w</p>
  </div>
  <div class="page">
    <p>Ground Markov Network</p>
    <p>Student(STAN)</p>
    <p>Professor(PEDRO)</p>
    <p>AdvisedBy(STAN,PEDRO)</p>
    <p>Professor(STAN)</p>
    <p>Student(PEDRO)</p>
    <p>AdvisedBy(PEDRO,STAN)</p>
    <p>AdvisedBy(STAN,STAN)</p>
    <p>AdvisedBy(PEDRO,PEDRO)</p>
    <p>AdvisedBy(S,P) ) Student(S) ^ Professor(P)2.7</p>
    <p>constants: STAN, PEDRO</p>
  </div>
  <div class="page">
    <p>MLN Model</p>
  </div>
  <div class="page">
    <p>MLN Model</p>
    <p>Vector of value assignments to ground predicates</p>
  </div>
  <div class="page">
    <p>MLN Model</p>
    <p>Vector of value assignments to ground predicates</p>
    <p>Partition function. Sums over all possible value assignments to ground predicates</p>
  </div>
  <div class="page">
    <p>MLN Model</p>
    <p>Vector of value assignments to ground predicates</p>
    <p>Partition function. Sums over all possible value assignments to ground predicates</p>
    <p>Weight of ith formula</p>
  </div>
  <div class="page">
    <p>MLN Model</p>
    <p>Vector of value assignments to ground predicates</p>
    <p>Partition function. Sums over all possible value assignments to ground predicates</p>
    <p>Weight of ith formula</p>
    <p># of true groundings of ith formula</p>
  </div>
  <div class="page">
    <p>MLN Weight Learning Likelihood is concave function of weights</p>
    <p>Quasi-Newton methods to find optimal weights  e.g. L-BFGS [Liu &amp; Nocedal, 1989]</p>
  </div>
  <div class="page">
    <p>MLN Weight Learning Likelihood is concave function of weights</p>
    <p>Quasi-Newton methods to find optimal weights  e.g. L-BFGS [Liu &amp; Nocedal, 1989]</p>
    <p>SLOW #P-complete</p>
  </div>
  <div class="page">
    <p>MLN Weight Learning Likelihood is concave function of weights</p>
    <p>Quasi-Newton methods to find optimal weights  e.g. L-BFGS [Liu &amp; Nocedal, 1989]</p>
    <p>SLOW #P-completeSLOW</p>
    <p>#P-complete</p>
  </div>
  <div class="page">
    <p>MLN Weight Learning R&amp;D used pseudo-likelihood [Besag, 1975]</p>
  </div>
  <div class="page">
    <p>MLN Weight Learning R&amp;D used pseudo-likelihood [Besag, 1975]</p>
  </div>
  <div class="page">
    <p>MLN Structure Learning R&amp;D learned MLN structure in two disjoint steps:</p>
    <p>Learn first-order clauses with an off-the-shelf</p>
    <p>ILP system (CLAUDIEN [De Raedt &amp; Dehaspe, 1997])  Learn clause weights by optimizing</p>
    <p>pseudo-likelihood  Unlikely to give best results because CLAUDIEN</p>
    <p>find clauses that hold with some accuracy/frequency</p>
    <p>in the data  dont find clauses that maximize datas</p>
    <p>(pseudo-)likelihood</p>
  </div>
  <div class="page">
    <p>Overview  Motivation  Background  Structure Learning Algorithm  Experiments  Future Work &amp; Conclusion</p>
  </div>
  <div class="page">
    <p>This paper develops an algorithm that:  Learns first-order clauses by directly optimizing</p>
    <p>pseudo-likelihood  Is fast enough  Performs better than R&amp;D, pure ILP,</p>
    <p>purely KB and purely probabilistic approaches</p>
    <p>MLN Structure Learning</p>
  </div>
  <div class="page">
    <p>Structure Learning Algorithm High-level algorithm</p>
    <p>REPEAT</p>
    <p>MLN  MLN [ FindBestClauses(MLN)</p>
    <p>UNTIL FindBestClauses(MLN) returns NULL</p>
    <p>FindBestClauses(MLN)</p>
    <p>Create candidate clauses</p>
    <p>FOR EACH candidate clause c</p>
    <p>Compute increase in evaluation measure of adding c to MLN</p>
    <p>RETURN k clauses with greatest increase</p>
  </div>
  <div class="page">
    <p>Structure Learning Evaluation measure  Clause construction operators  Search strategies  Speedup techniques</p>
  </div>
  <div class="page">
    <p>Evaluation Measure R&amp;D used pseudo-log-likelihood</p>
    <p>This gives undue weight to predicates with large # of groundings</p>
  </div>
  <div class="page">
    <p>Weighted pseudo-log-likelihood (WPLL)</p>
    <p>Gaussian weight prior  Structure prior</p>
    <p>Evaluation Measure</p>
  </div>
  <div class="page">
    <p>Weighted pseudo-log-likelihood (WPLL)</p>
    <p>Gaussian weight prior  Structure prior</p>
    <p>Evaluation Measure</p>
    <p>weight given to predicate r</p>
  </div>
  <div class="page">
    <p>Weighted pseudo-log-likelihood (WPLL)</p>
    <p>Gaussian weight prior  Structure prior</p>
    <p>Evaluation Measure</p>
    <p>sums over groundings of predicate r</p>
    <p>weight given to predicate r</p>
  </div>
  <div class="page">
    <p>Weighted pseudo-log-likelihood (WPLL)</p>
    <p>Gaussian weight prior  Structure prior</p>
    <p>Evaluation Measure</p>
    <p>sums over groundings of predicate r</p>
    <p>weight given to predicate r CLL: conditional log-likelihood</p>
  </div>
  <div class="page">
    <p>Clause Construction Operators Add a literal (negative/positive)  Remove a literal  Flip signs of literals  Limit # of distinct variables to restrict</p>
    <p>search space</p>
  </div>
  <div class="page">
    <p>Beam Search</p>
    <p>Same as that used in ILP &amp; rule induction  Repeatedly find the single best clause</p>
  </div>
  <div class="page">
    <p>Shortest-First Search (SFS)</p>
    <p>each clause to create clauses of length L 4. Repeatedly add K best clauses of length L</p>
    <p>to the MLN until no clause of length L improves WPLL</p>
    <p>Similar to Della Pietra et al. (1997), McCallum (2003)</p>
  </div>
  <div class="page">
    <p>Speedup Techniques FindBestClauses(MLN)</p>
    <p>Creates candidate clauses</p>
    <p>FOR EACH candidate clause c</p>
    <p>Compute increase in WPLL (using L-BFGS)</p>
    <p>of adding c to MLN RETURN k clauses with greatest increase</p>
  </div>
  <div class="page">
    <p>Speedup Techniques FindBestClauses(MLN)</p>
    <p>Creates candidate clauses</p>
    <p>FOR EACH candidate clause c</p>
    <p>Compute increase in WPLL (using L-BFGS) of adding c to MLN</p>
    <p>RETURN k clauses with greatest increase</p>
    <p>SLOW Many candidates</p>
  </div>
  <div class="page">
    <p>Speedup Techniques FindBestClauses(MLN)</p>
    <p>Creates candidate clauses</p>
    <p>FOR EACH candidate clause c</p>
    <p>Compute increase in WPLL (using L-BFGS) of adding c to MLN</p>
    <p>RETURN k clauses with greatest increase</p>
    <p>SLOW Many candidates</p>
    <p>SLOW Many CLLs</p>
    <p>SLOW Each CLL involves a #P-complete problem</p>
  </div>
  <div class="page">
    <p>Speedup Techniques FindBestClauses(MLN)</p>
    <p>Creates candidate clauses</p>
    <p>FOR EACH candidate clause c</p>
    <p>Compute increase in WPLL (using L-BFGS) of adding c to MLN</p>
    <p>RETURN k clauses with greatest increase</p>
    <p>SLOW Many candidates</p>
    <p>NOT THAT FAST</p>
    <p>SLOW Many CLLs</p>
    <p>SLOW Each CLL involves a #P-complete problem</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Speedup Techniques Clause Sampling  Predicate Sampling  Avoid Redundancy  Loose Convergence Thresholds  Ignore Unrelated Clauses  Weight Thresholding</p>
  </div>
  <div class="page">
    <p>Overview  Motivation  Background  Structure Learning Algorithm  Experiments  Future Work &amp; Conclusion</p>
  </div>
  <div class="page">
    <p>Experiments  UW-CSE domain</p>
    <p>22 predicates, e.g., AdvisedBy(X,Y), Student(X), etc.  10 types, e.g., Person, Course, Quarter, etc.  # ground predicates  4 million  # true ground predicates  3000  Handcrafted KB with 94 formulas</p>
    <p>Each student has at most one advisor  If a student is an author of a paper, so is her advisor</p>
    <p>Cora domain  Computer science research papers  Collective deduplication of author, venue, title</p>
  </div>
  <div class="page">
    <p>Systems</p>
    <p>MLN(SLB): structure learning with beam search</p>
    <p>MLN(SLS): structure learning with SFS</p>
  </div>
  <div class="page">
    <p>Systems</p>
    <p>MLN(SLB)</p>
    <p>MLN(SLS)</p>
    <p>KB: hand-coded KB</p>
    <p>CL: CLAUDIEN</p>
    <p>FO: FOIL</p>
    <p>AL: Aleph</p>
  </div>
  <div class="page">
    <p>Systems</p>
    <p>MLN(SLB)</p>
    <p>MLN(SLS)</p>
    <p>KB</p>
    <p>CL</p>
    <p>FO</p>
    <p>AL</p>
    <p>MLN(KB)</p>
    <p>MLN(CL)</p>
    <p>MLN(FO)</p>
    <p>MLN(AL)</p>
  </div>
  <div class="page">
    <p>Systems</p>
    <p>MLN(SLB)</p>
    <p>MLN(SLS)</p>
    <p>NB: Nave Bayes BN: Bayesian</p>
    <p>networks</p>
    <p>KB</p>
    <p>CL</p>
    <p>FO</p>
    <p>AL</p>
    <p>MLN(KB)</p>
    <p>MLN(CL)</p>
    <p>MLN(FO)</p>
    <p>MLN(AL)</p>
  </div>
  <div class="page">
    <p>Methodology  UW-CSE domain</p>
    <p>DB divided into 5 areas:</p>
    <p>AI, Graphics, Languages, Systems, Theory  Leave-one-out testing by area</p>
    <p>Measured  average CLL of the ground predicates  average area under the precision-recall curve</p>
    <p>of the ground predicates (AUC)</p>
  </div>
  <div class="page">
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O</p>
    <p>)</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>C L L</p>
    <p>A U</p>
    <p>C</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O )</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>UW-CSE</p>
  </div>
  <div class="page">
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O</p>
    <p>)</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>C L L</p>
    <p>A U</p>
    <p>C</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O )</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>UW-CSE</p>
  </div>
  <div class="page">
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O</p>
    <p>)</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>C L L</p>
    <p>A U</p>
    <p>C</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O )</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>UW-CSE</p>
  </div>
  <div class="page">
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O</p>
    <p>)</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>C L L</p>
    <p>A U</p>
    <p>C</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>) M</p>
    <p>LN (S</p>
    <p>LB )</p>
    <p>M LN</p>
    <p>(C L)</p>
    <p>M LN</p>
    <p>(F O )</p>
    <p>M LN</p>
    <p>(A L)</p>
    <p>M LN</p>
    <p>(K B )</p>
    <p>C L</p>
    <p>FO A L</p>
    <p>K B</p>
    <p>UW-CSE</p>
  </div>
  <div class="page">
    <p>C L L</p>
    <p>A U</p>
    <p>C</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>)</p>
    <p>M LN</p>
    <p>(S LB</p>
    <p>)</p>
    <p>N B</p>
    <p>B N</p>
    <p>M LN</p>
    <p>(S LS</p>
    <p>)</p>
    <p>M LN</p>
    <p>(S LB</p>
    <p>)</p>
    <p>N B</p>
    <p>B N</p>
    <p>UW-CSE</p>
  </div>
  <div class="page">
    <p>Timing  MLN(SLS) on UW-CSE</p>
    <p>Cluster of 15 dual-CPUs 2.8 GHz Pentium 4 machines</p>
    <p>Without speedups: did not finish in 24 hrs  With speedups: 5.3 hrs</p>
  </div>
  <div class="page">
    <p>Lesion Study  Disable one speedup technique at a time; SFS</p>
    <p>UW-CSE (one-fold)</p>
    <p>H o u r</p>
    <p>all speedups</p>
    <p>no clause sampling</p>
    <p>no predicate sampling</p>
    <p>dont avoid redundancy</p>
    <p>no loose converg.</p>
    <p>threshold</p>
    <p>no weight thresholding</p>
  </div>
  <div class="page">
    <p>Overview  Motivation  Background  Structure Learning Algorithm  Experiments  Future Work &amp; Conclusion</p>
  </div>
  <div class="page">
    <p>Future Work  Speed up counting of # true</p>
    <p>groundings of clause  Probabilistically bound the loss in</p>
    <p>accuracy due to subsampling  Probabilistic predicate discovery</p>
  </div>
  <div class="page">
    <p>Conclusion  Markov logic networks: a powerful combination</p>
    <p>of first-order logic and probability  Richardson &amp; Domingos (2004) did not learn</p>
    <p>MLN structure  We develop an algorithm that automatically learns</p>
    <p>both first-order clauses and their weights  We develop speedup techniques to make our</p>
    <p>algorithm fast enough to be practical  We show experimentally that our algorithm</p>
    <p>outperforms  Richardson &amp; Domingos  Pure ILP  Purely KB approaches  Purely probabilistic approaches</p>
    <p>(For software, email: koks@cs.washington.edu)</p>
  </div>
</Presentation>
