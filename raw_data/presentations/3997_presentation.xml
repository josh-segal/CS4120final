<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Pretraining Sentiment Classifiers with Unlabeled</p>
    <p>Dialog Data</p>
    <p>Jul. 18, 2018</p>
    <p>Toru Shimizu*1, Hayato Kobayashi*1,*2, Nobuyuki Shimizu*1 *1 Yahoo Japan Corporation, *2 RIKEN AIP</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>The amount of labeled training data  You will need at least 100k training records to surpass classical</p>
    <p>approaches (Hu+ 2014, Wu+ 2014)  Large-scale labeled datasets of document classification</p>
    <p>0 2 6 4 05 8 5 1 55 2 9 -2 75 B85 5B85 2 2 5</p>
  </div>
  <div class="page">
    <p>Semi-supervised approaches  Language model</p>
    <p>LSTM-RNN</p>
    <p>! /</p>
    <p>LSTM-RNN</p>
    <p>/</p>
    <p>transfer</p>
    <p>positive</p>
    <p>/ !</p>
  </div>
  <div class="page">
    <p>Semi-supervised approaches  Sequence autoencoder (Dai and Le 2015)</p>
    <p>LSTM-RNN</p>
    <p>!</p>
    <p>transfer</p>
    <p>positive</p>
    <p>LSTM-RNN LSTM-RNN</p>
  </div>
  <div class="page">
    <p>Pretraining strategy with unlabeled dialog data  Pretrain an encoder-decoder model for sentiment classifiers</p>
    <p>Outperform other semi-supervised methods  Language model  Sequence autoencoder  Distant supervision with emoji and emoticons</p>
    <p>Case study based on...  Costly labeled sentiment dataset of 99.5K items  Large-scale unlabeled dialog dataset of 22.3M utterance</p>
    <p>response pairs</p>
  </div>
  <div class="page">
    <p>Emotional conversations in a dialog dataset</p>
    <p>Implicitly learn sentiment-handling capabilities through learning a dialog model</p>
    <p>(</p>
    <p>, , , !</p>
    <p>! ' ! ,</p>
    <p>, ,( , ) (</p>
  </div>
  <div class="page">
    <p>Datasets  Large-scale dialog corpus: a set of a large number of</p>
    <p>unlabeled utterance-response tweet pairs  Labeled dataset: a set of a moderate number of tweets with</p>
    <p>a sentiment label</p>
    <p>Pretraining</p>
    <p>Fine-tuning</p>
    <p>-</p>
    <p>LSTM-RNN LSTM-RNN</p>
    <p>!</p>
    <p>LSTM-RNN</p>
    <p>'</p>
    <p>transfer -</p>
    <p>positive</p>
  </div>
  <div class="page">
    <p>Dialog data  Extract 22.3M pairs of an utterance tweet and its</p>
    <p>response tweet from Twitter Firehose data</p>
    <p>Sentiment data  Positive: 15.0%, Negative: 18.6%, Neutral 66.4%</p>
    <p>training validation test total</p>
    <p>Dialog data 22,300,000 10,000 50,000 22,360,000</p>
    <p>training validation test total</p>
    <p>Sentiment data 80,591 4,000 15,000 99,591</p>
  </div>
  <div class="page">
    <p>Dialog model</p>
    <p>One-layer LSTM-RNN encoder-decoder  Embedding layer: 4000 tokens, 256 elements  LSTM: 1024 elements  Representation which encoder gives: 1024 elements  Decoder's readout layer: 256 elements  Decoder's output layer: 4000 tokens  LSTMs of the encoder and decoder share the parameter</p>
    <p>LSTM-RNN LSTM-RNN</p>
    <p>' '</p>
    <p>!</p>
    <p>dist. repr.</p>
  </div>
  <div class="page">
    <p>enc</p>
    <p>dec</p>
    <p>dec</p>
    <p>embedding layer</p>
    <p>recurrent layer htenc</p>
    <p>embedding layer</p>
    <p>recurrent layer htdec</p>
    <p>readout layer</p>
    <p>output layer ot</p>
    <p>token ID ut</p>
    <p>token ID xt</p>
    <p>token ID yt</p>
    <p>encoder RNN</p>
    <p>decoder RNN</p>
    <p>dec</p>
  </div>
  <div class="page">
    <p>Classification model</p>
    <p>The architecture of the encoder RNN part is identical to that of the dialog model</p>
    <p>Produce a probability distribution over sentiment classes by a fully-connected layer and softmax function</p>
    <p>encoder RNN</p>
    <p>output layer</p>
  </div>
  <div class="page">
    <p>Model pretraining with the dialog data</p>
    <p>MLE training objective  1 GPU (7 TFLOPS)  5 epochs = 15.9 days  Batch size: 64  Optimizer: ADADELTA  Apply gradient clipping  Evaluate validation costs 10 times per epoch and pick up</p>
    <p>the best model  Theano-based implementation</p>
  </div>
  <div class="page">
    <p>Classifier model training with the sentiment data</p>
    <p>Apply 5 different data sizes for each method  5k10k20k40k80k (all)</p>
    <p>5 runs for each method/data size with varying random seeds</p>
    <p>Evaluate the results by the average of f-measure scores  Adjust the duration so that the cost surely converges</p>
    <p>Pretrained models converge very quickly but those trained from scratch converge slowly</p>
    <p>The other aspects are the same with pretraining</p>
  </div>
  <div class="page">
    <p>The proposed method: Dial</p>
    <p>LSTM-RNN LSTM-RNN</p>
    <p>!</p>
    <p>LSTM-RNN</p>
    <p>'</p>
    <p>transfer -</p>
    <p>positive</p>
  </div>
  <div class="page">
    <p>Default  No pretraining  Directly trained by the sentiment data</p>
    <p>LSTM-RNN</p>
    <p>!</p>
    <p>positive</p>
  </div>
  <div class="page">
    <p>Lang  Pretrain an LSTM-RNNs as a language model</p>
    <p>LSTM-RNN</p>
    <p>/ '</p>
    <p>LSTM-RNN</p>
    <p>/</p>
    <p>transfer</p>
    <p>positive</p>
    <p>' !/</p>
  </div>
  <div class="page">
    <p>SeqAE  Pretrain an LSTM-RNNs as a sequence autoencoder</p>
    <p>(Dai and Le 2015)</p>
    <p>LSTM-RNN LSTM-RNN</p>
    <p>!</p>
    <p>!</p>
    <p>LSTM-RNN</p>
    <p>'</p>
    <p>transfer</p>
    <p>positive</p>
  </div>
  <div class="page">
    <p>- -</p>
    <p>Emoji and emoticon-based distant supervision  Prepare large-scale datasets utilizing emoticons or emoji as</p>
    <p>pseudo labels (Go+ 2009)  Positive emoticon examples</p>
    <p>!&quot;#$%&amp;()*+  )) o(^-^)o  Negative emoticon examples</p>
    <p>,-./01234 (TT) ( (* orz</p>
  </div>
  <div class="page">
    <p>Emo2M and Emo6M  Pretrain models as classifier models using pseudo-labeled</p>
    <p>data</p>
    <p>LSTM-RNN</p>
    <p>! ' !</p>
    <p>negative</p>
    <p>LSTM-RNN</p>
    <p>'</p>
    <p>transfer</p>
    <p>positive</p>
  </div>
  <div class="page">
    <p>Data  Use only the sentiment data</p>
    <p>Preprocessing  Segment text with a defact-standard morphological</p>
    <p>analyzer, MeCab  50,000 unigrams and 50,000 bigrams  +233 emoji and emoticons</p>
    <p>LogReg  Logistic regression (LIBLINEAR)</p>
    <p>LinSVM  Linear SVM (LIBLINEAR)</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>-</p>
  </div>
  <div class="page">
    <p>Effectiveness of the pretraining strategy using paired dialog data for sentiment analysis  Even more effective in extremely low-resource situations  Character-based processing</p>
    <p>Future work  Explore combinations of a large-scale unlabeled dataset</p>
    <p>and a supervised task  Exploit other kinds of structures</p>
  </div>
</Presentation>
