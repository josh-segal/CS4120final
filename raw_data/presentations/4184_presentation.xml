<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Final Projects Embeddings Words and Senses Together via Joint Knowledge-Enhanced Training</p>
    <p>Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci and Roberto Navigli</p>
    <p>lcl.uniroma1.it/sw2v</p>
  </div>
  <div class="page">
    <p>Motivation: Model senses instead of only words</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Motivation: Model senses instead of only words</p>
    <p>bank#1</p>
    <p>bank#2</p>
    <p>...</p>
    <p>...</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Motivation: Model senses instead of only words</p>
    <p>bank#1</p>
    <p>bank#2</p>
    <p>...</p>
    <p>...</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Unsupervised sense embeddings</p>
    <p>Knowledge-based sense embeddings</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Unsupervised sense embeddings</p>
    <p>Learn sense embeddings exploiting text corpora only (Huang et al. ACL 2012; Neelakantan et al. EMNLP 2014; Tian et al. COLING 2014; Li and Jurafsky, EMNLP 2015...). Easily adaptable to new domains.</p>
    <p>Drawbacks:</p>
    <p>Senses not interpretable (+change from model to model)  Knowledge from resources cannot be easily exploited  Senses (esp. not frequent ones) not easy to discriminate</p>
    <p>Knowledge-based sense embeddings</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Unsupervised sense embeddings</p>
    <p>Knowledge-based sense embeddings</p>
    <p>Model senses as defined on a sense inventory.</p>
    <p>Usually obtained as a postprocessing of word embeddings (Chen et al. EMNLP 2014; Rothe and Schtze, ACL 2015...):</p>
    <p>Several training phases  Infrequent senses not accurately captured</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Unsupervised sense embeddings</p>
    <p>Knowledge-based sense embeddings (Our approach)</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Unsupervised sense embeddings</p>
    <p>Knowledge-based sense embeddings (Our approach)</p>
  </div>
  <div class="page">
    <p>Idea</p>
    <p>A word is the surface form of a sense: we can exploit this intrinsic relationship for jointly training word and sense embeddings.</p>
  </div>
  <div class="page">
    <p>Idea</p>
    <p>A word is the surface form of a sense: we can exploit this intrinsic relationship for jointly training word and sense embeddings.</p>
    <p>How?</p>
    <p>Updating the representation of the word and its associated senses interchangeably.</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Methodology: Linking words and senses in context</p>
    <p>He withdrew money from the bank</p>
    <p>retire geographycash</p>
    <p>financial institution</p>
    <p>building</p>
    <p>take out</p>
  </div>
  <div class="page">
    <p>Methodology: Linking words and senses in context</p>
    <p>He withdrew money from the bank</p>
    <p>retire geographycash</p>
    <p>financial institution</p>
    <p>building</p>
    <p>take out</p>
    <p>Graph-based representation of the sentence using semantic networks (e.g. WordNet, BabelNet)</p>
  </div>
  <div class="page">
    <p>Methodology: Linking words and senses in context</p>
    <p>He withdrew money from the bank</p>
    <p>retire geographycash</p>
    <p>financial institution</p>
    <p>building</p>
    <p>take out</p>
    <p>Graph-based representation of the sentence using semantic networks (e.g. WordNet, BabelNet)</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He bank</p>
    <p>money</p>
    <p>withdrew thefrom</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He bank</p>
    <p>money</p>
    <p>withdrew thefrom</p>
    <p>error</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He bank</p>
    <p>money</p>
    <p>withdrew thefrom</p>
    <p>error</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He bank</p>
    <p>money</p>
    <p>withdrew thefrom</p>
    <p>error</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He bank</p>
    <p>money</p>
    <p>withdrew thefrom</p>
    <p>error</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>In this way it is possible to learn word and sense/synset embeddings jointly on a single training.</p>
  </div>
  <div class="page">
    <p>Once each word is connected to its set of senses in context, it is possible to modify standard word embedding architectures to take into account this information.</p>
    <p>In this work we explore the CBOW architecture of Word2Vec (Mikolov et al. 2013) -&gt; SW2V (Senses and Words to Vectors).</p>
    <p>Other neural network architectures could be explored as well (Skip-gram also included in the code).</p>
    <p>Methodology: Joint training of words and sense embeddings</p>
  </div>
  <div class="page">
    <p>Full architecture of W2V (Mikolov et al. 2013)</p>
    <p>E=-log(p(wt|W t))</p>
    <p>Words and associated senses used both as input and output.</p>
  </div>
  <div class="page">
    <p>Full architecture of SW2V (this work)</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
    <p>Words and associated senses used both as input and output.</p>
  </div>
  <div class="page">
    <p>The architecture does not try to predict senses. No loss contribution from them.</p>
    <p>Output layer alternatives: only words</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
  </div>
  <div class="page">
    <p>The architecture does not try to predict words. No loss contribution from them.</p>
    <p>Output layer alternatives: only senses</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
  </div>
  <div class="page">
    <p>Senses are not included in the input layer. Only words contribute to the hidden state. This way, during backpropagation sense embeddings do not receive any gradient.</p>
    <p>Input layer alternatives: only words</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
  </div>
  <div class="page">
    <p>During backpropagation, sense embeddings will receive the same gradient of the word they are associated with.</p>
    <p>Input layer alternatives: only words</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
  </div>
  <div class="page">
    <p>Input layer alternatives: only senses</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
    <p>Words are not included in the input layer. Only senses contribute to the hidden state. This way, during backpropagation word embeddings do not receive any gradient.</p>
  </div>
  <div class="page">
    <p>Input layer alternatives: only senses</p>
    <p>E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
    <p>During backpropagation, their embeddings will receive the same gradient of their associated senses.</p>
  </div>
  <div class="page">
    <p>Analysis: Model configurations</p>
    <p>We used word similarity for analyzing the performance of sense embeddings on each of the nine configurations.</p>
    <p>- Best configuration</p>
    <p>Input layer: Only senses  Output layer: Both words and senses</p>
    <p>Why? (Intuition) Co-occurrence information gets duplicated if both words and senses are included in the input layer.</p>
  </div>
  <div class="page">
    <p>Evaluation: Experimental setting</p>
    <p>Best configuration used in all experiments</p>
    <p>Standard hyperparameters</p>
    <p>Semantic networks used: WordNet and BabelNet</p>
    <p>Corpora used: UMBC and Wikipedia</p>
    <p>Experiments on:</p>
    <p>- Word and sense interconnectivity (qualitative)</p>
    <p>- Word similarity</p>
    <p>- Sense clustering</p>
  </div>
  <div class="page">
    <p>Evaluation: Comparison systems</p>
    <p>Sense embeddings:</p>
    <p>Chen et al. (2014)</p>
    <p>AutoExtend (Rothe and Schtze, 2015)</p>
    <p>SensEmbed (Iacobacci et al. 2015)</p>
    <p>NASARI (Camacho-Collados et al. 2016)</p>
  </div>
  <div class="page">
    <p>Evaluation: Comparison systems</p>
    <p>Sense embeddings:</p>
    <p>Chen et al. (2014)</p>
    <p>AutoExtend (Rothe and Schtze, 2015)</p>
    <p>SensEmbed (Iacobacci et al. 2015)</p>
    <p>NASARI (Camacho-Collados et al. 2016)</p>
    <p>WordNet</p>
  </div>
  <div class="page">
    <p>Evaluation: Comparison systems</p>
    <p>Sense embeddings:</p>
    <p>Chen et al. (2014)</p>
    <p>AutoExtend (Rothe and Schtze, 2015)</p>
    <p>SensEmbed (Iacobacci et al. 2015)</p>
    <p>NASARI (Camacho-Collados et al. 2016)</p>
    <p>Word embeddings:</p>
    <p>Word2Vec (Mikolov et al. 2013)</p>
    <p>Retrofitting (Faruqui et al. 2015)</p>
    <p>WordNet</p>
  </div>
  <div class="page">
    <p>Evaluation: Comparison systems</p>
    <p>Sense embeddings:</p>
    <p>Chen et al. (2014)</p>
    <p>AutoExtend (Rothe and Schtze, 2015)</p>
    <p>SensEmbed (Iacobacci et al. 2015)</p>
    <p>NASARI (Camacho-Collados et al. 2016)</p>
    <p>Word embeddings:</p>
    <p>Word2Vec (Mikolov et al. 2013)</p>
    <p>Retrofitting (Faruqui et al. 2015)</p>
    <p>WordNet</p>
  </div>
  <div class="page">
    <p>Evaluation: Word and sense interconnectivity</p>
    <p>How coherent is the shared vector space of word and sense embeddings?</p>
    <p>Intuition: the Most Frequent Sense (MFS) should be close to the word embedding -&gt; Reasonably strong MFS baseline for WSD</p>
    <p>Evaluation on two WSD datasets using the embeddings as a MFS baseline (closest sense embedding to its associated word embedding is selected).</p>
  </div>
  <div class="page">
    <p>Evaluation: Word and sense interconnectivity</p>
    <p>SemEval-07 SemEval-13</p>
    <p>SW2V 39.9 54.0</p>
    <p>AutoExtend 17.6 31.0</p>
    <p>Baseline 24.8 34.9</p>
  </div>
  <div class="page">
    <p>Word and sense interconnectivity: Example I</p>
    <p>Ten closest word and sense embeddings to the sense company (military unit)</p>
  </div>
  <div class="page">
    <p>Word and sense interconnectivity: Example II</p>
    <p>Ten closest word and sense embeddings to the sense school (group of fish)</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
    <p>All models using Wikipedia corpus (Pearson correlation)</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
    <p>All models using Wikipedia corpus (Pearson correlation)</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
    <p>All models using UMBC corpus (Pearson correlation)</p>
  </div>
  <div class="page">
    <p>Evaluation: Sense clustering</p>
    <p>Some sense inventories make a fine-grained distinction between senses, which can be harmful on downstream applications (Hovy et al. 2013, Pilehvar et al. 2017).</p>
    <p>Example: Bank</p>
    <p>Evaluation datasets (Dandala et al. 2013): Highly ambiguous words from past SemEval competitions.</p>
    <p>Institution</p>
    <p>Physical building</p>
  </div>
  <div class="page">
    <p>Evaluation: Sense clustering</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We presented SW2V: a neural architecture for jointly learning word and sense embeddings in the same vector space using text corpora and knowledge obtained from semantic networks.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We presented SW2V: a neural architecture for jointly learning word and sense embeddings in the same vector space using text corpora and knowledge obtained from semantic networks. Future work:</p>
    <p>- Exploiting our model for other linked representations such as multilingual or Image-to-Text embeddings.</p>
    <p>- Word Sense Disambiguation and Entity Linking. - Integrating our embeddings into downstream NLP applications,</p>
    <p>following the lines of Pilehvar et al. (ACL 2017).</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We presented SW2V: a neural architecture for jointly learning word and sense embeddings in the same vector space using text corpora and knowledge obtained from semantic networks. Future work:</p>
    <p>- Exploiting our model for other linked representations such as multilingual or Image-to-Text embeddings.</p>
    <p>- Word Sense Disambiguation and Entity Linking. - Integrating our embeddings into downstream NLP applications,</p>
    <p>following the lines of Pilehvar et al. (ACL 2017).</p>
    <p>http://lcl.uniroma1.it/sw2v</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Code and pre-trained models available at</p>
    <p>http://lcl.uniroma1.it/sw2v</p>
    <p>Embedding Words and Senses Together via Joint Knowledge-Enhanced training Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci and Roberto Navigli Embedding Words and Senses Together via Joint Knowledge-Enhanced training Massimiliano Mancini, Jose Camacho-Collados, Ignacio Iacobacci and Roberto Navigli</p>
  </div>
  <div class="page">
    <p>SECRET SLIDES</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Related work</p>
    <p>Our approach: SW2V (Senses and Words to Vectors)</p>
    <p>Linking words and senses in context</p>
    <p>Joint training of words and sense embeddings</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Methodology</p>
    <p>Given as input a corpus and a semantic network:</p>
    <p>He withdrew money from the bank.</p>
  </div>
  <div class="page">
    <p>Joint training of word and sense embeddings</p>
    <p>Once each word is connected to its set of senses in context, it is possible to modify standard word embedding models to take into account this information.</p>
    <p>Formally, given a target word at position t we have a set of words:</p>
    <p>W={wt-n,  , wt, , wt+n} with W t=W \ wt</p>
    <p>and a set of associated senses:</p>
    <p>S = {St-n,  , St, , St+n} and S t=S \ St</p>
    <p>with Si={si 1,  , si</p>
    <p>k,i} the senses associated with the ith word.</p>
    <p>We aim at minimizing: E=-log(p(wt|W t,St)) - sSt log(p(s|W</p>
    <p>t,St))</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity Sense Embeddings SimLex-999 MEN</p>
    <p>System Corpus r p r p</p>
    <p>SW2VBN UMBC 0.49 0.47 0.75 0.75</p>
    <p>SW2VWN UMBC 0.46 0.45 0.76 0.76</p>
    <p>AutoExtend UMBC 0.47 0.45 0.74 0.75</p>
    <p>AutoExtend Google-News 0.46 0.46 0.68 0.70</p>
    <p>SW2VBN Wikipedia 0.47 0.43 0.71 0.73</p>
    <p>SW2VWN Wikipedia 0.47 0.43 0.71 0.72</p>
    <p>SensEmbed Wikipedia 0.43 0.39 0.65 0.70</p>
    <p>Chen et al. (2014)</p>
    <p>Wikipedia 0.46 0.43 0.62 0.62</p>
    <p>Word Embeddings SimLex-999 MEN</p>
    <p>System Corpus r p r p</p>
    <p>Word2Vec UMBC 0.39 0.39 0.75 0.75</p>
    <p>RetrofittingBN UMBC 0.47 0.46 0.75 0.76</p>
    <p>RetrofittingWN UMBC 0.47 0.46 0.76 0.76</p>
    <p>Word2Vec Wikipedia 0.39 0.38 0.71 0.72</p>
    <p>RetrofittingBN Wikipedia 0.35 0.32 0.66 0.66</p>
    <p>RetrofittingWN Wikipedia 0.47 0.44 0.73 0.73</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
  </div>
  <div class="page">
    <p>Evaluation: Word similarity</p>
  </div>
  <div class="page">
    <p>Evaluation: Sense clustering</p>
    <p>Accuracy F-Measure</p>
    <p>SW2V 87.8 63.9</p>
    <p>SensEmbed 82.7 40.3</p>
    <p>NASARI 87.0 62.5</p>
    <p>Multi-SVM 85.5</p>
    <p>Mono-SVM 83.5</p>
    <p>Baseline 17.5 29.8</p>
  </div>
  <div class="page">
    <p>Evaluation: Sense clustering</p>
    <p>Accuracy F-Measure</p>
    <p>SW2V 87.8 63.9</p>
    <p>SensEmbed 82.7 40.3</p>
    <p>NASARI 87.0 62.5</p>
    <p>Multi-SVM 85.5</p>
    <p>Mono-SVM 83.5</p>
    <p>Baseline 17.5 29.8</p>
  </div>
  <div class="page">
    <p>Word and sense interconnectivity</p>
    <p>SemEval-07 SemEval-13</p>
    <p>SW2V 39.9 54.0</p>
    <p>AutoExtend 17.6 31.0</p>
    <p>Baseline 24.8 34.9</p>
  </div>
</Presentation>
