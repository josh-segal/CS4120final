<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads</p>
    <p>Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee,</p>
    <p>Junjie Qian, Wencong Xiao, Fan Yang</p>
  </div>
  <div class="page">
    <p>Deep Learning at a Large Enterprise</p>
    <p>Speech, Image, Ads, NLP, Web Search</p>
    <p>DL training jobs require large GPU clusters</p>
    <p>Philly: Cluster manager for DL workloads on large shared GPU clusters</p>
    <p>Cortana</p>
    <p>Recent Cluster Managers</p>
    <p>Optimus [EuroSys 18]</p>
    <p>Gandiva [OSDI 18]</p>
    <p>Tiresias [NSDI 19]</p>
    <p>Objective Average JCT Consolidation Average JCT</p>
    <p>Scheduler SRTF Time-sharing Gittins Index</p>
    <p>Motivated by observations in Philly</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>1. First characterization study of large-scale</p>
    <p>GPU clusters for DNN training</p>
    <p>2. Uncover inefficiencies in cluster utilization</p>
    <p>3. Present lessons for better cluster manager designs</p>
  </div>
  <div class="page">
    <p>Low Locality in Distributed Training</p>
    <p>Scheduler &amp;</p>
    <p>Job Placement</p>
    <p>Free GPU</p>
    <p>Occupied GPU</p>
    <p>N N-GPU DL job</p>
    <p>Queue</p>
    <p>High intra-server locality  High communication efficiency</p>
    <p>Long waiting time</p>
    <p>Low intra-server locality  Low waiting time</p>
    <p>Contention in the use of network</p>
    <p>Risk of intra-server interference (across jobs)</p>
    <p>Low GPU locality is detrimental to</p>
    <p>cluster efficiency  Prioritize locality</p>
  </div>
  <div class="page">
    <p>Failures across Stack Reduce Cluster Utilization</p>
    <p>Infrastructure AI Engine User</p>
    <p>Resource Scheduler</p>
    <p>Frequency User errors in code</p>
    <p>Temporal Infrastructure failures</p>
    <p>GPU hours wastes Semantics errors</p>
    <p>Improve failure handling</p>
    <p>(e.g., pre-run jobs)</p>
    <p>F a il u re</p>
    <p>t y p e s</p>
  </div>
  <div class="page">
    <p>Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads</p>
    <p>on July 12 at USENIX ATC 2019</p>
  </div>
</Presentation>
