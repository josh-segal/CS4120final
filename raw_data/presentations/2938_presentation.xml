<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Thank you, and welcome to OSDI! My name is Jeremy Elson and Im going to be talking about a project called flat datacenter storage, or FDS. I want to start with a little thought experiment. This session is called big data, but imagine for a minute were in an alternative universe where whats really important is little data.</p>
  </div>
  <div class="page">
    <p>In that case, I think we all would have stopped making headlines a long time ago. Because, from a SYSTEMS perspective, I think little-data really is a solved problem. We already HAVE the perfect little-data computer, and its right there on Page 1, above the fold: A single machine, that has a bunch of processors, and a bunch of disks, connected by something like a RAID controller. For I/O intensive workloads, that computer really is ideal.</p>
  </div>
  <div class="page">
    <p>When applications WRITE, the RAID controller splits those writes up pretty finely and stripes them over all the disks. You might have a small number of writers writing a lot, or a large number of writers writing a little bit, or even both kinds writing at the same time. But the lulls in one writer are filled in by the bursts in another, meaning we get good statistical multiplexing. All the disks stay busy, and high utilization means were extracting all the performance we can out of our hardware.</p>
  </div>
  <div class="page">
    <p>Reads can also exploit the striped writes. (***) Again, we can get full performance out of the disks. Even if some processors consume data slowly, and others consume it quickly, all the disks stay busy, which is what we want. (***) Its easy to write software for this computer, too. It doesnt matter how many physical disks there are; programmers can pretend theres just one big one. And files written by any process can be read by any other without caring about locality. (***) Also, if were trying to attack a large problem in parallel  for example, trying to parse a giant log file  the input doesnt need to be partitioned in advance. All the workers drain a global pool of work, and when its exhausted, they all finish at about the same time. This prevents stragglers and means the job finishes sooner. We call this dynamic work allocation.</p>
  </div>
  <div class="page">
    <p>Another benefit of this computer is that its (***) easy to adjust the ratio of processors to disks just by adding more of whichever one you need. You can fit the machines resources to match its expected workload. OK, so, whats the problem? Back in the real world of big data, the problem is that this machine doesnt scale. We can add a few dozen processors and disks, but not thousands. Why not?</p>
  </div>
  <div class="page">
    <p>Lets take a look at this thing in the middle here. Thats really where the magic happens. Roughly, its doing two things. (***) The first is metadata management  when a process writes, that thing decides how the write should be striped, and keeps enough state around so that reads can find the data later. (***) Second, its responsible for physically routing data from disks to processors  actually transporting the bits. In FDS, weve built a blob storage system that fully distributes both of these tasks. This means we can build a cluster that has the essential properties of the ideal machine I described, but potentially can scale to the size of a datacenter. And in the next twenty minutes, Ill describe how.</p>
  </div>
  <div class="page">
    <p>Heres the 90 second overview. (***) FDS is a simple, scalable blob store. Compute and storage are logically separate, and theres no affinity, meaning any processor can access all data in the system uniformly. Thats why we call it flat. Weve combined that conceptual simplicity with the very high I/O performance that youve come to expect only from systems that couple storage and computation together, like MapReduce, Dryad and Hadoop. (***) FDS has a novel way of distributing metadata. In fact, the common case read and write paths go through no centralized components at all. (***) We get the bandwidth we need from full-bisection-bandwidth CLOS networks, using novel techniques to schedule traffic. (***) With FDS, weve demonstrated very high read and write performance  in a singlereplicated cluster, a single process in tight read or write loop can achieve more than 2 gigabytes per second all the way to the remote disk platters. To give you a sense of scale, that means were writing to REMOTE disks faster than many systems can write LOCALLY, to a RAID array. (***) Disks can also talk TO EACH OTHER at high speed, meaning FDS can recover from failed disks very quickly. For example, in one test with a 1,000-disk cluster, we killed a machine with 7 disks holding a total of about two-thirds of a terabyte; FDS brought the lost</p>
  </div>
  <div class="page">
    <p>data back to full replication in 34 seconds. (***) Finally, weve shown that FDS can make APPLICATIONS very fast. We describe several applications from diverse domains in the paper, but in this talk Ill just be talking about one: we wrote a straightforward sort application on top of FDS which beat the 2012 world record for disk-to-disk sorting. Our general purpose remote blob store beat previous implementations that exploited local disks. (***) This summary is also going to serve as an outline of my talk, which I will now give again, but . Slower.</p>
  </div>
  <div class="page">
    <p>So lets get started, and talk about the architecture in general.</p>
  </div>
  <div class="page">
    <p>First, lets talk about how the system looks to applications. In FDS, all blobs are identified with a simple GUID. Each blob contains 0 or more allocation units we call TRACTS, which are numbered sequentially, starting from 0. All tracts in a system are the same size. In most of our clusters, a tract is 8 megabytes; Ill tell you a little later why we picked that number. A tract is the basic unit of reading and writing in FDS. (***) The interface is pretty simple. It has only about a dozen calls, such as CreateBlob, ReadTract and WriteTract. Its designed to be asynchronous, meaning that the functions dont block, but instead call a callback when theyre done. A typical high-throughput FDS application will issue a few dozen reads or writes at a time, and issue more as the earlier ones complete. We call applications using the FDS API FDS clients.</p>
  </div>
  <div class="page">
    <p>In addition to clients, there are two other types of actors in FDS. The first is what we call a tractserver. A tractserver is a simple piece of software that sits between a raw disk and the network, accepting commands from the network such as read tract and write tract. Theres also a special node called the metadata server which coordinates the cluster and helps clients rendezvous with tractservers. As we saw on the previous slide, the existence of tractservers and the metadata server is invisible to programmers. The API just talks about blobs and tract numbers. Underneath, our library contacts the metadata server as necessary and sends read and write messages over the network to tractservers. An important question is, how does the client know which tractserver should be used to read or write a tract?</p>
  </div>
  <div class="page">
    <p>This brings us to the next part of my talk  metadata management.</p>
  </div>
  <div class="page">
    <p>To understand how FDS handles metadata, its useful to consider the spectrum of solutions in other systems. On one extreme, we have systems like GFS and Hadoop that manage metadata centrally. On basically every read or write, clients consult a metadata server that has canonical information about the placement of all data in the system. This gives you really good visibility and control. But its also a centralized bottleneck thats exerted pressure on these systems to increase the size of writes  for example, GFS uses 64 megabyte extents, almost an order of magnitude larger than FDS. This makes it harder to do fine-grained load balancing like our ideal little-data computer does. On the other end, we have distributed hash tables. Theyre fully decentralized, meaning theres no bottleneck, but all reads and writes typically require multiple trips over the network before they find data. In addition, failure recovery is relatively slow because recovery is a localized operation among nearby neighbors in the ring. In FDS we tried to find a spot in between that gives us some of the best properties of both extremes: One-hop access to data and fast failure recovery without any centralized bottlenecks in common-case paths.</p>
  </div>
  <div class="page">
    <p>FDS does have a centralized metadata server, but its role is very limited. When a client first starts, the metadata server sends some state to the client  for now, think of it as an oracle. When a client wants to read or write a tract, the underlying FDS library has two pieces of information: The blobs GUID and the tract number. The client library feeds those into the oracle and gets out the address of the tractservers responsible for replicas of that tract. In a system with more than one replica, reads go to ONE replica at random, and writes go to all of them. The oracles mapping of tracts to tractservers needs two important properties. First, it needs to be consistent: a client READING a tract needs to get the same answer as the writer got when it wrote that tract. Second, it has to be pseudo-random. As I mentioned earlier, clients have lots of tract reads and writes outstanding simultaneously. The oracle needs to ensure that all of those operations are being serviced by different tractservers. We dont want all the requests going to just 1 disk if we have 10 of them. Once a client has this oracle, reads and writes ALL happen without contacting the metadata</p>
  </div>
  <div class="page">
    <p>server again. Since reads and writes dont generate metadata server traffic, we can afford to do a LARGE number of SMALL reads and writes that all go to different spindles, even in very large-scale systems, giving us really good statistical multiplexing of the disks  just like the little-data computer. And we really have the flexibility to make writes as small as we need to. For THROUGHPUTsensitive applications, we use 8 megabyte tracts because 8 megs is large enough to amortize seeks, meaning reading and writing randomly goes almost as fast as sequentially. But, weve also done experiments with seek-bound workloads, where we pushed the tract size all the way down to 64K. Thats very hard with a centralized metadata server but no problem with this oracle. By now youre probably wondering what this oracle is. (***) What it is is a table of all the disks in the system collected centrally by the metadata server. We call this table the tract locator table. The table has as many columns as there are replicas; this example shows a triple-replicated system. In SINGLE-replicated systems, the number of rows in this table grows linearly with the number of disks in the system. (***) In MULTIPLY-replicated systems, it grows as n-squared; well see why a little later. Okay, so, how does a client use this table? It takes the blob GUID and tract number and runs them through a deterministic function that yields a row index. As long as readers and writers are using consistent versions of the table, the mappings they get will also be consistent. In the paper, we describe the details of how we do consistent table versioning. (((We hash the blobs GUID so that independent clients start at random places in the table, even if the GUIDs //themselves are not randomly distributed.))) Something very important about this table is that it only contains disks, not tracts. In other words, reads and writes dont change the table. So, to a first approximation, clients can retrieve it from the metadata server once, then never contact the metadata server again. The only time the table changes is when a disk fails or is added.</p>
  </div>
  <div class="page">
    <p>Theres another clever thing we can do with the tract locator table, and thats distribute per-blob metadata, such as each blobs length and permission bits. We store this in tract 1. Clients find the metadata tract the same way that they find regular data, just by plugging -1 into the tract locator formula we saw previously. This means that the metadata is spread pseudo-randomly across all tractservers in the system, just like the regular data. Tractservers have support for consistent metadata updates. For example, lets say several writers are trying to append to the same blob. In FDS, each executes an FDS function called extend blob. This is a request for a range of tract numbers that can be written without conflict. The tractserver serializes the requests and returns a unique range to each client. This is how FDS supports atomic append. In systems with more than one replica, the requests go to the tractserver in the FIRST column of the table. THAT server does a two-phase commit to the others before returning a result to the client. Because were using the tract locator table to determine which tractserver owns each blobs metadata, different blobs will most likely have their metadata operations served by DIFFERENT tractservers. The metadata traffic is spread across every server in the system. But requests that need to be serialized because they refer to the same blob will always end up at the same tractserver, which maintains correctness.</p>
  </div>
  <div class="page">
    <p>Okay, now that weve talked about how FDS handles metadata, lets talk about networking. Up until now, Ive assumed that there was an uncongested path from tractservers to clients. Let me now describe how we build such a network.</p>
  </div>
  <div class="page">
    <p>First, a little background. Until recently, the standard way to build a datacenter was with significant oversubscription: a top-of-rack switch might have 40 gigabits of bandwidth down to servers in the rack, but only 2 or 4 gigabits going up to the network core. In other words, the link to the core was oversubscribed a factor of 10 or 20. This, of course, was done to save money.</p>
  </div>
  <div class="page">
    <p>But, in the last few years, theres been an explosion of research in the networking community in what are called CLOS networks. CLOS networks more-or-less do for networks what RAID did for disks: by connecting up a large number of low-cost, commodity switches and doing some clever routing, its now economical, for the first time, to build fullbisection-bandwidth networks at the scale of a datacenter. In FDS, we take the idea a step further.</p>
  </div>
  <div class="page">
    <p>Even with CLOS networks, many computers in todays datacenters still have a bottleneck between disks and the network. A typical disk can read or write at about a gigabit per second, but there are four, or 12, or even 25 disks in a machine, all stuck behind a single 1 gigabit link. For applications that have to move data, such as sort or a distributed join, this is a big problem.</p>
  </div>
  <div class="page">
    <p>In FDS, we make sure all machines with disks have as much network bandwidth as they have disk bandwidth. For example, a machine with 10 disks needs a 10 gigabit NIC, and a machine with 20 disks needs two of them. Adding all this bandwidth has a cost; depending on the size of the network, maybe about 30% more per machine. But as well see a little later, we get a lot more than a 30% increase in performance for that investment.</p>
  </div>
  <div class="page">
    <p>So, we really built this. Weve gone through several generations of testbeds, but our largest has 250 machines and about 1,500 disks. Theyre all connected using 14 top-of-rack switches and 8 spine switches, giving it 4.5 terabits of bisection bandwidth. We made a company that sells 10G network cables very happy.</p>
  </div>
  <div class="page">
    <p>Unfortunately, just adding all this bandwidth doesnt automatically give you good performance, like we thought it would when we were young and innocent at the beginning of the project. Part of the problem is that in realistic conditions, datacenter CLOS networks dont guarantee full bisection bandwidth. They only make it stochastically likely. This is an artifact of routing algorithms that select a single, persistent path for each TCP flow to prevent packet reordering. As a result, CLOS networks have a well-known problem handling long, fat flows. We designed our data layout how we did partly because of the network load it generates. FDS generates a large number of very short-lived flows to a wide set of random destinations, which is the ideal case for a CLOS network. A second problem is that even a perfect CLOS network doesnt actually eliminate congestion. It just pushes the congestion out to the edges. So good traffic shaping is still necessary. But whats really nasty is that these two constraints are in tension. CLOS networks need really SHORT flows for load balancing, but TCP needs nice LONG flows for its bandwidth allocation algorithm to find an equilibrium. We ended up doing our own application-layer bandwidth allocation using an RTS/CTS scheme which is described in the paper, along with a bunch of other tricks.</p>
  </div>
  <div class="page">
    <p>Now, let me show you some microbenchmarks from one of our test clusters. In these experiments, we had test clients READING FROM or WRITING TO a fixed number of tractservers. We varied the number of clients and measured their aggregate bandwidth. The clients each had a single 10 gig Ethernet connection, and the servers had either one or two, depending on how many disks were in the server.</p>
  </div>
  <div class="page">
    <p>The first result Ill show you is from a single-replicated cluster. Note the X-axis here is logarithmic. And I have the pleasure of showing you a graph that is delightfully uninteresting. The aggregate read and write bandwidth go up close to linearly with the number of clients, from 1 to 170. Read bandwidth goes up at about 950 megabytes per second per client and write bandwidth goes up by 1,150 megabytes per second per client. Writers saturated about 90% of their theoretical network bandwidth, and readers saturated about 74%. This graph shows two different cluster configurations: one used 1,033 disks, and the other used about half that. In the 1,033 disk test, there was just as much disk bandwidth as there was client bandwidth, so performance kept going up as we added more clients. In the 516 disk test, there was much MORE CLIENT bandwidth available than DISK bandwidth. Since disks were the bottleneck, aggregate bandwidth kept going up until wed saturated the disks, then got flat. Its not shown on this graph, but we also tested clients that had 20 gigabits of network bandwidth instead of ten. There, clients were able to read and write at over 2 gigabytes per second. Thats writing remotely, over the network, all the way to disk platters, faster than a</p>
  </div>
  <div class="page">
    <p>lot of systems write to a local RAID. Decoupling storage and computation does not have to mean giving up performance!</p>
  </div>
  <div class="page">
    <p>This test is similar to the previous one but were now writing to a triple-replicated cluster instead of a single-replicated cluster. Read bandwidth is just about the same, but as youd expect, writes saturate the disks much sooner because were writing everything three times. The aggregate write bandwidth is about one-third of the read bandwidth in all cases.</p>
  </div>
  <div class="page">
    <p>Id like to switch gears now, a little bit, and talk about failure recovery. Ill first describe how it works in FDS, and then show you some more performance numbers.</p>
  </div>
  <div class="page">
    <p>The way that data is organized in a blob store has a pretty dramatic effect on recovery performance. The simplest method of replication is unfortunately also the slowest. You could organize disks into pairs or triples that are always kept identical. When a disk fails, you bring in a hot spare, and use a replica thats still alive to make an exact copy of the disk that died. This is really slow because its constrained by the speed of a single disk. Filling a 1 terabyte disk takes at least several hours, and such slow recovery actually DECREASES DURABILITY because it lengthens the window of vulnerability to additional failures. But we can do better. In FDS, when a disk dies, our goal isnt to reconstruct an exact duplicate of the disk that died. We just want to make sure that somewhere in the system, extra copies of the lost data get made. It doesnt matter where. We just want to get back to a state where there are 3 copies of everything SOMEWHERE.</p>
  </div>
  <div class="page">
    <p>In FDS, failure recovery exploits the fine-grained blob striping I was talking about earlier. We lay out data so that when a disk dies (***), there isnt just a single disk that contains backup copies of that disks data. Instead, all n of the disks that are still alive (***) will have about 1/nth of the data that was lost. Every disk sends a copy of its small part of the lost data to some other disk that has some free space. (***) Since we have a full bisection bandwidth network, all the disks can do this in parallel, and failure recovery goes really fast. In fact, since EVERY disk is participating in the recovery, FDS has a really nice property: as a cluster gets larger, recovery actually goes faster. (***) THIS is sort of amazing, because its just the opposite of something like a RAID, where larger volumes require LONGER recovery times.</p>
  </div>
  <div class="page">
    <p>Its pretty easy to implement this using the tract locator table. What we do is construct a table such that that every possible PAIR (***) of disks appears in a row of the table. This is why, in replicated clusters, the number of rows in the table grows as n-squared. We can add more COLUMNS for more durability if we WANT to, but to get the fastest recovery speed, we never need more than n-squared rows.</p>
  </div>
  <div class="page">
    <p>When a disk dies, such as Disk A in this example, the metadata server (***) first selects a random disk to replace the failed disk in every row of the table.</p>
  </div>
  <div class="page">
    <p>Then, it selects one of the remaining GOOD disks in each row to transfer the lost data to the replacement disk. It sends a message to each disk to start the transfer.(***) And, since were on a full bisection bandwidth network, all the transfers can happen in parallel. Im leaving out a lot of details, but theyre described in the paper. Instead, let me show you some results.</p>
  </div>
  <div class="page">
    <p>We tested failure recovery in a number of configurations, in clusters with both 100 and 1,000 disks, and killing both INDIVIDUAL disks and all the disks in a single machine at the same time. Let me just highlight a couple of the results.</p>
  </div>
  <div class="page">
    <p>In our largest test, we used a 1,000-disk cluster and killed a machine with 7 disks holding a total of about two-thirds of a terabyte of data. All the lost data was re-replicated in 34 seconds. But, more interesting is that every time we made the cluster larger, we got about another 40 megabytes per second per disk of aggregate recovery speed. Thats less than half the speed of a disk, but remember thats because every disk is simultaneously READING the data its sending, and WRITING to its free space that some other disk is filling. Extrapolating these numbers out, we estimate that if we lost a 1-terabyte disk out of a 3,000-disk cluster, wed recover all the data in less than 20 seconds. And remember, thats not to memory, thats writing recovered data all the way out to disk. Whats really magical about these numbers is they are strikingly linear. Recovery time is a combination of some FIXED time to detect the failure plus some VARIABLE time for data transfer. In our experiments, the variable part INCREASES LINEARLY with the amount of data recovered, and DECREASES LINEARLY with the number of disks involved in the recovery. I think that getting such linear results in a system of this scale is really cool.</p>
  </div>
  <div class="page">
    <p>Finally Id like to discuss one application of FDS. We describe several in the paper from a few different domains, but Im just going to highlight one: we set the 2012 world record for disk-to-disk sorting using a small FDS application.</p>
  </div>
  <div class="page">
    <p>MinuteSort is a test devised by a group led by the late Jim Gray about 25 years ago. The question is, given 60 seconds, how much randomly distributed data can you shuffle into sorted order? This was meant as an **I/O** test, so the rules specify the data must START and END in stable storage. We competed in two divisions. One was for general-purpose systems, and one was for purpose-built systems that were allowed to exploit the specifics of the benchmark. In the general purpose division, the previous record, which stood for three years, was set by Yahoo, using a large Hadoop cluster. By large, I mean about 1,400 machines, and about 5,600 disks. With FDS, with less than ONE-FIFTH of the computers and disks, we nearly TRIPLED the amount of data sorted  which multiplies out to a pretty remarkable 15x improvement in efficiency. This all came from the fact that Yahoos cluster, like MOST Hadoop-style clusters, had serious oversubscription both from disk to network, and from rack to network core. We ATTACKED that bottleneck, by investing, on average, 30 percent more money per machine for more bandwidth, and HARNESSED that bandwidth using everything you heard in this talk. The result is that instead of a cluster having mostly IDLE disks, we built a cluster with disks working CONTINUOUSLY. In the specially-optimized class, the record was set last year by UCSDs fantastic TritonSort.</p>
  </div>
  <div class="page">
    <p>They wrote a tightly integrated and optimized sort application that did a beautiful job of really squeezing everything they could out of their hardware. They used local storage, so they did beat us on CPU efficiency, but not on disk efficiency. In absolute terms, we set that record by about 8%. But I think what really distinguishes our sort is that it REALLY WAS just a small app sitting on top of FDS. FDS is general-purpose, and had absolutely no sort-specific optimizations in it at all. Sort used no special calls into FDS, just plain old read and write. In fact, that meant we had to transport the data three times: the clients read data over the network from tractservers, then exchanged, then wrote back to the tractservers, but we beat both records anyway.</p>
  </div>
  <div class="page">
    <p>I also want to take a minute to show you the effect of dynamic work allocation. At the beginning of the talk I mentioned that one advantage of ignoring locality constraints, like in the little-data computer, is that all workers can draw work from a global pool, which prevents stragglers. Early versions of our sort didnt use dynamic work allocation. We just divided the input file evenly among all the nodes. As you can see with this time diagram, stragglers were a big problem. Each line represents one stage of the sort. A horizontal line would mean all nodes finished that stage at the same time, which would be ideal. But as you can see, the red stage was pretty far from ideal. About half the nodes would finish the stage within 25 seconds and a few would straggle along for another 30. This was bad because there was a global barrier between the red stage and the green stage. Now, we knew it wasnt a problem with the hardware because DIFFERENT nodes were coming in last each time. It was just a big distributed system with a lot of random things happening, and a few nodes would always get unlucky. Finally we switched to using dynamic work allocation. These two tests were actually done on the same day with no other change. In the version on the right, each node would process a tiny part of the input, and when it</p>
  </div>
  <div class="page">
    <p>was almost done, it would ask the head sort node for more. This was all done at the application layer  FDS didnt know anything about it. As you can see, it dramatically reduced stragglers, which made the whole job faster. It was a really nice self-clocking scheme. A worker that finished early would get lots more work assigned, and unlucky nodes wouldnt. And this was entirely enabled by the fact that FDS uses a global store. Clients can read any part of they input they want, so shuffling the assignments around at the last second really has no cost. And, yes, I admit it: this is digital socialism.</p>
  </div>
  <div class="page">
    <p>In conclusion, FDS gives you the agility and conceptual simplicity of a global store, but without the usual performance penalty. We can write to remote storage just as fast as other systems can write to local storage, but were able to throw away the locality constraints. This also means we can build clusters with very high utilization: you buy as many disks as you need I/O bandwidth, and as many CPUs as you need processing power. Individual applications can use resources in whatever ratio they need. We do have to invest more money in the network, but in exchange we really unlock the potential of all the other hardware weve paid for, both because weve opened the network bottleneck and because a global store gives us global statistical multiplexing. Today, a lot of people have the mindset that certain kinds of high-bandwidth applications just HAVE to fit into a rack if theyre going to be fast. And racks really arent that big! Id argue weve shown a path around that constraint. The bottom line with FDS is really not JUST that it can make todays applications go faster. I think it lets us imagine NEW KINDS of applications, too.</p>
  </div>
  <div class="page"/>
</Presentation>
