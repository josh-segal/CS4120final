<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Jeremy Stribling, Yair Sovran, Irene Zhang, Xavid Pretzer, Yair Sovran, Irene Zhang, Xavid Pretzer,</p>
    <p>Jinyang Li, M. Frans Kaashoek, and Robert Morris</p>
    <p>MIT CSAIL &amp; New York University</p>
  </div>
  <div class="page">
    <p>PlanetLabPlanetLab</p>
    <p>Apps store data on widely-spread resources  Testbeds, Grids, data centers, etc.  Yet theres no universal storage layer</p>
    <p>Whats so hard about the wide-area?  Failures and latency and bandwidth, oh my!</p>
  </div>
  <div class="page">
    <p>CoralCDN prefers low delay to strong consistency</p>
    <p>Google stores email near consumer (Coral Sloppy DHT)</p>
    <p>(Gmails storage layer)  Facebook forces writes to one data center</p>
    <p>Each app builds its own storage layer</p>
    <p>(Gmails storage layer)</p>
    <p>(Customized MySQL/Memcached)</p>
  </div>
  <div class="page">
    <p>Apps need control of wide-area tradeoffs  Fast timeouts vs. consistency  Fast writes vs. durability  Proximity vs. availability Proximity vs. availability</p>
    <p>Need a common, familiar API: File system  Easy to program, reuse existing apps</p>
    <p>No existing DFS allows such control</p>
  </div>
  <div class="page">
    <p>Small set of app-specified controls  Correspond to wide-area challenges:</p>
    <p>EventualConsistency: relax consistency  RepLevel=N: control number of replicas RepLevel=N: control number of replicas  Site=site: control data placement</p>
    <p>Allow apps to specify on per-file basis  /fs/.EventualConsistency/file</p>
  </div>
  <div class="page">
    <p>Wide-area file system  Apps embed cues directly in pathnames  Many apps can reuse existing software  Multi-platform prototype w/ several apps</p>
    <p>Wide-area file system  Apps embed cues directly in pathnames  Many apps can reuse existing software  Multi-platform prototype w/ several apps Multi-platform prototype w/ several apps Multi-platform prototype w/ several apps</p>
  </div>
  <div class="page">
    <p>Data stored in WheelFS</p>
    <p>Distributed Application</p>
    <p>WheelFS FUSE</p>
    <p>WheelFS client nodes</p>
    <p>WheelFS configuration</p>
    <p>Service (Paxos + RSM)</p>
    <p>WheelFS client</p>
    <p>software</p>
    <p>WheelFS client nodes WheelFS storage nodes</p>
    <p>Files and directories are spread across storage nodes</p>
  </div>
  <div class="page">
    <p>Files have a primary and two replicas  A files primary is its creator</p>
    <p>Clients can cache files  Lease-based invalidation protocol Lease-based invalidation protocol</p>
    <p>Strict close-to-open consistency  All operations serialized through the primary</p>
  </div>
  <div class="page">
    <p>v2</p>
    <p>By default, failing to reach the primary blocks the operation to</p>
    <p>offer close-to-open consistency in the face of partitions</p>
    <p>v2</p>
    <p>v2</p>
    <p>Read 562</p>
    <p>Eventually, the configuration service decides to promote a</p>
    <p>backup to be primary</p>
    <p>Write file</p>
    <p>(backup)</p>
    <p>(backup)</p>
  </div>
  <div class="page">
    <p>Transient failures are common  Fast timeouts vs. consistency</p>
    <p>High latency  Fast writes vs. durability</p>
    <p>Low wide-area bandwidth  Proximity vs. availability</p>
    <p>Only applications can make these tradeoffs</p>
  </div>
  <div class="page">
    <p>Apps want to control consistency, data placement ...</p>
    <p>How? Embed cues in path names</p>
    <p>Flexible and minimal interface change</p>
    <p>/wfs/cache/a/b/.cue/foo/wfs/cache/a/b/.EventualConsistency/foo/wfs/cache/a/b/foo</p>
  </div>
  <div class="page">
    <p>Cues can apply to directory subtrees</p>
    <p>Cues apply recursively over an entire subtree of files</p>
    <p>/wfs/cache/.EventualConsistency/a/b/foo</p>
    <p>Multiple cues can be in effect at once</p>
    <p>Assume developer applies cues sensibly</p>
    <p>/wfs/cache/.EventualConsistency/.RepLevel=2/a/b/foo</p>
    <p>Both cues apply to the entire subtree</p>
  </div>
  <div class="page">
    <p>Name Purpose</p>
    <p>RepLevel= (permanent)</p>
    <p>How many replicas of this file should be maintained</p>
    <p>HotSpot (transient)</p>
    <p>This file will be read simultaneously by many nodes, so use p2p caching</p>
    <p>Large reads</p>
    <p>Durability</p>
    <p>Site= (permanent)</p>
    <p>Hint which group of nodes a file should be stored</p>
    <p>Hint about data placement</p>
    <p>Cues designed to match wide-area challenges</p>
    <p>(transient) many nodes, so use p2p caching</p>
    <p>EventualConsistency (trans/perm)</p>
    <p>Control whether reads must see fresh data, and whether writes</p>
    <p>must be serialized Consistency</p>
  </div>
  <div class="page">
    <p>Read latest version of the file you can find quickly  In a given time limit (.MaxTime=)</p>
    <p>v2</p>
    <p>v2</p>
    <p>v2</p>
    <p>Read file</p>
    <p>(cached) (backup)</p>
    <p>(backup)</p>
  </div>
  <div class="page">
    <p>Write file</p>
    <p>Write to any replica of the file</p>
    <p>v2v3</p>
    <p>Reconciling divergent replicas:</p>
    <p>Directories Files  Merge replicas into single  Choose one of the replicas to</p>
    <p>(backup) v2</p>
    <p>Write file</p>
    <p>v3v3</p>
    <p>Create new version at backup</p>
    <p>Background process will merge divergent replicas</p>
    <p>(No application involvement)</p>
    <p>Merge replicas into single directory by taking union of entries  Tradeoff: May lose some unlinks</p>
    <p>Choose one of the replicas to win</p>
    <p>Tradeoff: May lose some writes</p>
  </div>
  <div class="page">
    <p>Apache Caching</p>
    <p>Proxy</p>
    <p>Apache Caching</p>
    <p>Proxy</p>
    <p>Apache Caching</p>
    <p>Proxy</p>
    <p>Apache Caching</p>
    <p>Proxy If $url exists in cache dir read $url from WheelFS</p>
    <p>Blocks under failure with default strong consistency</p>
    <p>read $url from WheelFS else</p>
    <p>get page from web server store page in WheelFS</p>
    <p>One line change in Apache config file: /wfs/cache/$URL</p>
  </div>
  <div class="page">
    <p>.EventualConsistency</p>
    <p>Apache proxy handles potentially stale files well</p>
    <p>The freshness of cached web pages can be determined from saved HTTP headers</p>
    <p>Cache dir: /wfs/cache/ /.HotSpot/.MaxTime=200</p>
    <p>Read a cached file even when the corresponding</p>
    <p>primary cannot be contacted</p>
    <p>Write the file data anywhere even</p>
    <p>when the corresponding</p>
    <p>primary cannot be contacted</p>
    <p>Tells WheelFS to read data from</p>
    <p>the nearest client cache it can find</p>
    <p>Reads only block for 200 ms; after that,</p>
    <p>fall back to origin server</p>
  </div>
  <div class="page">
    <p>Runs on Linux, MacOS, and FreeBSD  User-level file system using FUSE  20K+ lines of C++  Unix ACL support, network coordinates Unix ACL support, network coordinates  Deployed on PlanetLab and Emulab</p>
  </div>
  <div class="page">
    <p>App Cues used</p>
    <p>Lines of code/configuration written or changed</p>
    <p>Cooperative Web Cache</p>
    <p>.EventualConsistency, .MaxTime, .HotSpot 1</p>
    <p>All-Pairs-Pings .EventualConsistency, .MaxTime, .HotSpot, .WholeFile 13.HotSpot, .WholeFile</p>
    <p>Distributed Mail .EventualConsistency, .Site,</p>
    <p>.RepLevel, .RepSites, .KeepTogether</p>
    <p>File distribution .WholeFile, .HotSpot N/A</p>
    <p>Distributed make</p>
    <p>.EventualConsistency (for objects), .Strict (for source), .MaxTime</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Working set of files exceeds NFS servers</p>
    <p>buffer cache</p>
    <p>PlanetLab vs.</p>
    <p>dedicated MIT server</p>
  </div>
  <div class="page">
    <p>40 PlanetLab nodes as Web proxies  40 PlanetLab nodes as clients  Web server</p>
    <p>400 Kbps link 400 Kbps link  100 unique 41 KB pages</p>
    <p>Each client downloads random pages  (Same workload as in CoralCDN paper)</p>
    <p>CoralCDN vs. WheelFS + Apache</p>
  </div>
  <div class="page">
    <p>CoralCDN ramps up more quickly due to special optimizations</p>
    <p>. . . but WheelFS soon achieves similar</p>
    <p>performance</p>
    <p>Total reqs/unique page: &gt; 32,000 Origin reqs/unique page: 1.5 (CoralCDN) 2.6 (WheelFS)</p>
  </div>
  <div class="page">
    <p>15 proxies at 5 wide-area sites on Emulab  1 client per site  Each minute, one site offline for 30 secs</p>
    <p>Data primaries at site unavailable Data primaries at site unavailable  Eventual vs. strict consistency</p>
  </div>
  <div class="page">
    <p>EventualConsistency allows nodes to use</p>
    <p>cached version when primary is unavailable</p>
  </div>
  <div class="page">
    <p>Single-server FS: NFS, AFS, SFS  Cluster FS: Farsite, GFS, xFS, Ceph  Wide-area FS: Shark, CFS, JetFile  Grid: LegionFS, GridFTP, IBP Grid: LegionFS, GridFTP, IBP</p>
    <p>WheelFS gives applications control over wide-area tradeoffs</p>
  </div>
  <div class="page">
    <p>PNUTS !&quot;#$%&amp;'()* + ,-.//012&amp;34256478593:&amp;;439&lt;-69-&amp;3-5-7-29</p>
    <p>PADS !=99&amp;&gt;9?5&amp;5-@A* PADS !=99&amp;&gt;9?5&amp;5-@A*  Flexible toolkit for creating new storage layers</p>
    <p>WheelFS offers broad range of controls in the context of a single file system</p>
  </div>
  <div class="page">
    <p>Storage must let apps control data behavior  Small set of semantic cues to allow control</p>
    <p>Placement, Durability, Large reads and ConsistencyConsistency</p>
    <p>WheelFS:  Wide-area file system with semantic cues  Allows quick prototyping of distributed apps</p>
    <p>ht tp: / /pdos.csai l .mi t .edu/wheelfs</p>
  </div>
</Presentation>
