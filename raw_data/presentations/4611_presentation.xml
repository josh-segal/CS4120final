<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Phoenix Rebirth: Scalable MapReduce on a Large-Scale Shared-Memory System</p>
    <p>Richard Yoo, Anthony Romano, Christos Kozyrakis</p>
    <p>Stanford University</p>
    <p>http://mapreduce.stanford.edu</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Talk in a Nutshell</p>
    <p>Scaling a shared-memory MapReduce system on a 256-thread</p>
    <p>machine with NUMA characteristics</p>
    <p>Major challenges &amp; solutions</p>
    <p>Memory mgmt and locality =&gt; locality-aware task distribution</p>
    <p>Data structure design =&gt; mechanisms to tolerate NUMA latencies</p>
    <p>Interactions with the OS =&gt; thread pool and concurrent allocators</p>
    <p>Results &amp; lessons learnt</p>
    <p>Improved speedup by up to 19x (average 2.5x)</p>
    <p>Scalability of the OS still the major bottleneck</p>
  </div>
  <div class="page">
    <p>Background</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>MapReduce and Phoenix</p>
    <p>MapReduce</p>
    <p>A functional parallel programming framework for large clusters</p>
    <p>Users only provide map / reduce functions Map: processes input data to generate intermediate key / value pairs</p>
    <p>Reduce: merges intermediate pairs with the same key</p>
    <p>Runtime for MapReduce Automatically parallelizes computation</p>
    <p>Manages data distribution / result collection</p>
    <p>Phoenix: shared-memory implementation of MapReduce</p>
    <p>An efficient programming model for both CMPs and SMPs [HPCA07]</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Phoenix on a 256-Thread System</p>
    <p>chip 0 chip 1</p>
    <p>chip 2 chip 3</p>
    <p>hub</p>
    <p>mem 0</p>
    <p>mem 3 mem 2</p>
    <p>mem 1</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>The Problem: Application Scalability</p>
    <p>Baseline Phoenix scales well on a single socket machine</p>
    <p>Performance plummets with multiple sockets &amp; large thread counts</p>
    <p>Speedup on a Single Socket UltraSPARC T2 Speedup on a 4-Socket UltraSPARC T2+</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>The Problem: OS Scalability</p>
    <p>OS / libraries exhibit NUMA effects as well</p>
    <p>Latency increases rapidly when crossing chip boundary</p>
    <p>Similar behavior on a 32-core Opteron running Linux</p>
    <p>Synchronization Primitive Performance on the 4-Socket Machine</p>
  </div>
  <div class="page">
    <p>Optimizing the Phoenix Runtime on a Large-Scale NUMA System</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Optimization Approach</p>
    <p>Focus on the unique position of runtimes in a software stack</p>
    <p>Runtimes exhibit complex interactions with user code &amp; OS</p>
    <p>Optimization approach should be multi-layered as well</p>
    <p>Algorithm should be NUMA aware  Implementation should be optimized around NUMA challenges  OS interaction should be minimized as much as possible</p>
    <p>App</p>
    <p>Phoenix Runtime</p>
    <p>OS</p>
    <p>HW</p>
    <p>Algorithmic Level</p>
    <p>Implementation Level</p>
    <p>OS Interaction Level</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Algorithmic Optimizations</p>
    <p>App</p>
    <p>Phoenix Runtime</p>
    <p>OS</p>
    <p>HW</p>
    <p>Algorithmic Level</p>
    <p>Implementation Level</p>
    <p>OS Interaction Level</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Algorithmic Optimizations (contd.)</p>
    <p>Runtime algorithm itself should be NUMA-aware</p>
    <p>Problem: original Phoenix did not distinguish local vs. remote threads</p>
    <p>On Solaris, the physical frames for mmap()ed data spread out across multiple locality groups (a chip + a dedicated memory channel)</p>
    <p>Blind task assignment can have local threads work on remote data</p>
    <p>chip 0 chip 1</p>
    <p>chip 2 chip 3</p>
    <p>hub</p>
    <p>mem 0</p>
    <p>mem 3 mem 2</p>
    <p>mem 1 remote access</p>
    <p>remote</p>
    <p>access</p>
    <p>remote</p>
    <p>access</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Algorithmic Optimizations (contd.)</p>
    <p>Solution: locality-aware task distribution</p>
    <p>Utilize per-locality group task queues  Distribute tasks according to their locality group</p>
    <p>Threads work on their local task queue first, then perform task stealing</p>
    <p>chip 0 chip 1</p>
    <p>chip 2 chip 3</p>
    <p>hub</p>
    <p>mem 0</p>
    <p>mem 3 mem 2</p>
    <p>mem 1</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Implementation Optimizations</p>
    <p>App</p>
    <p>Phoenix Runtime</p>
    <p>OS</p>
    <p>HW</p>
    <p>Algorithmic Level</p>
    <p>Implementation Level</p>
    <p>OS Interaction Level</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Implementation Optimizations (contd.)</p>
    <p>Runtime implementation should handle large data sets efficiently</p>
    <p>Problem: Phoenix core data structure not efficient at handling large-scale data</p>
    <p>Map Phase</p>
    <p>Each column of pointers amounts to a fixed-size hash table  keys_array and vals_array all thread-local</p>
    <p>apple</p>
    <p>keys_array</p>
    <p>vals_array</p>
    <p>banana</p>
    <p>orange</p>
    <p>pear</p>
    <p>map thread id</p>
    <p>hash(orange)</p>
    <p>too many</p>
    <p>keys buffer</p>
    <p>reallocations</p>
    <p>num_map_threads</p>
    <p>n u</p>
    <p>m _ re</p>
    <p>d u</p>
    <p>c e _ ta</p>
    <p>s k s</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Implementation Optimizations (contd.)</p>
    <p>Reduce Phase</p>
    <p>Each row amounts to one reduce task</p>
    <p>Mismatch in access pattern results in remote accesses</p>
    <p>orange</p>
    <p>orange</p>
    <p>reduce task index</p>
    <p>Copy and pass to user reduce function</p>
    <p>keys_array</p>
    <p>vals_array</p>
    <p>vals_array</p>
    <p>keys_array remote</p>
    <p>access</p>
    <p>contiguous</p>
    <p>memory</p>
    <p>remote</p>
    <p>access</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Implementation Optimizations (contd.)</p>
    <p>apple</p>
    <p>keys_array</p>
    <p>vals_array</p>
    <p>banana</p>
    <p>orange</p>
    <p>pear</p>
    <p>Solution 1: make the hash bucket count user-tunable</p>
    <p>Adjust the bucket count to get few keys per bucket</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Implementation Optimizations (contd.)</p>
    <p>Solution 2: implement iterator interface to vals_array</p>
    <p>Removed copying / allocating the large value array</p>
    <p>Buffer implemented as distributed chunks of memory</p>
    <p>Implemented prefetch mechanism behind the interface</p>
    <p>orange</p>
    <p>orange</p>
    <p>reduce task index</p>
    <p>keys_array</p>
    <p>vals_array</p>
    <p>vals_array</p>
    <p>keys_array</p>
    <p>&amp;vals_array</p>
    <p>Expose iterator to user reduce function</p>
    <p>&amp;vals_array 2 4 1 1</p>
    <p>Copy and pass to user reduce function</p>
    <p>prefetch!</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Other Optimizations Tried</p>
    <p>Replace hash table with more sophisticated data structures</p>
    <p>Large amount of access traffic</p>
    <p>Simple changes negated the performance improvement E.g., excessive pointer indirection</p>
    <p>Combiners</p>
    <p>Only works for commutative and associative reduce functions</p>
    <p>Perform local reduction at the end of the map phase</p>
    <p>Little difference once the prefetcher was in place Could be good for energy</p>
    <p>See paper for details</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>OS Interaction Optimizations</p>
    <p>App</p>
    <p>Phoenix Runtime</p>
    <p>OS</p>
    <p>HW</p>
    <p>Algorithmic Level</p>
    <p>Implementation Level</p>
    <p>OS Interaction Level</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>OS Interaction Optimizations (contd.)</p>
    <p>Runtimes should deliberately manage OS interactions</p>
    <p>Problem: large, unpredictable amount of intermediate / final data</p>
    <p>Solution Sensitivity study on various memory allocators</p>
    <p>At high thread count, allocator performance limited by sbrk()</p>
    <p>Problem: stack deallocation (munmap()) in thread join</p>
    <p>Solution Implement thread pool</p>
    <p>Reuse threads over various MapReduce phases and instances</p>
  </div>
  <div class="page">
    <p>Results</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Experiment Settings</p>
    <p>Workloads released in the original Phoenix</p>
    <p>Input set significantly increased to stress the large-scale machine</p>
    <p>Solaris 5.10, GCC 4.2.1 O3</p>
    <p>Similar performance improvements and challenges on a 32</p>
    <p>thread Opteron system (8-sockets, quad-core chips) running</p>
    <p>Linux</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Scalability Summary</p>
    <p>Significant scalability improvement</p>
    <p>Scalability of the Original Phoenix Scalability of the Optimized Version</p>
    <p>workloads scale up to 256 threads</p>
    <p>limited by OS scalability issues</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Execution Time Improvement</p>
    <p>Optimizations more effective for NUMA</p>
    <p>Relative Speedup over the Original Phoenix</p>
    <p>little variation average: 1.5x, max: 2.8x</p>
    <p>significant improvement average: 2.53x, max: 19x</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Analysis: Thread Pool</p>
    <p>kmeans performs a sequence of MapReduces</p>
    <p>160 iterations, 163,840 threads Thread pool effectively reduces the number of calls to munmap()</p>
    <p>threads before after</p>
    <p>Number of Calls to munmap() on kmeans</p>
    <p>kmeans Performance Improvement due to Thread Pool</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Analysis: Locality-Aware Task Distribution</p>
    <p>Locality group hit rate (% of tasks supplied from local memory)</p>
    <p>Significant locality group hit rate improvement under NUMA environment</p>
    <p>Locality Group Hit Rate on string_match</p>
    <p>forced misses result</p>
    <p>in similar hit rate</p>
    <p>improved hit rates = improved performance</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Analysis: Hash Table Size</p>
    <p>No single hash table size worked for all the workloads</p>
    <p>Some workloads generated only a small / fixed number of unique keys  For those that did benefit, the improvement was not consistent</p>
    <p>Recommended values provided for each application</p>
    <p>word_count Sensitivity to Hash Table Size</p>
    <p>trend reverses with more threads</p>
    <p>kmeans Sensitivity to Hash Table Size</p>
    <p>no thread count leads</p>
    <p>to speedup</p>
  </div>
  <div class="page">
    <p>Why Are Some Applications Not Scaling?</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Non-Scalable Workloads</p>
    <p>Non-scalable workloads shared two common trends</p>
    <p>Execution Time Breakdown on histogram</p>
    <p>idle time increase</p>
    <p>kernel time increases</p>
    <p>significantly</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Profiler Analysis</p>
    <p>histogram</p>
    <p>64 % execution time spent idling for data page fault</p>
    <p>linear_regression</p>
    <p>63 % execution time spent idling for data page fault</p>
    <p>word_count</p>
    <p>28 % of its execution time in sbrk() called inside the memory allocator</p>
    <p>27 % of execution time idling for data pages</p>
    <p>Memory allocator and mmap()turned out to be the bottleneck</p>
    <p>Not the physical I/O problem</p>
    <p>OS buffer cache warmed up by repeating the same experiment with the same input</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Memory Allocator Scalability</p>
    <p>Memory Allocator Scalability Comparison on word_count</p>
    <p>sbrk() scalability a major issue</p>
    <p>A single user-level lock serialized accesses  Per-address space locks protected in-kernel virtual memory objects</p>
    <p>mmap() even worse</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>mmap() Scalability</p>
    <p>Microbenchmark: mmap()user file and calculate the sum by</p>
    <p>streaming through data chunks</p>
    <p>mmap() Microbenchmark Scalability</p>
    <p>mmap() alone does not scale</p>
    <p>Kernel lock serialization on per process page table</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Conclusion</p>
    <p>Multi-layered optimization approach proved to be effective</p>
    <p>Average 2.5x speedup, maximum 19x</p>
    <p>OS scalability issues need to be addressed for further scalability</p>
    <p>Memory management and I/O</p>
    <p>Opens up a new research opportunity</p>
  </div>
  <div class="page">
    <p>Yoo, Phoenix2 October 6, 2009</p>
    <p>Questions?</p>
    <p>The Phoenix System for MapReduce Programming, v2.0</p>
    <p>Publicly available at http://mapreduce.stanford.edu</p>
  </div>
</Presentation>
