<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Jiri Schindler with Sandip Shete &amp; Keith A. Smith</p>
    <p>Advanced Technology Group</p>
    <p>Improving throughput for small disk requests with proximal I/O</p>
    <p>USENIX FAST 2011</p>
  </div>
  <div class="page">
    <p>Important Workload in Datacenters</p>
    <p>Serial reads after random writes  Writes are (small) logical updates  Serial reads of continuously changing data</p>
    <p>Database systems example  Data acquired through transactions  Data scans for business intelligence</p>
    <p>Datacenter/enterprise scale  Flash memory prohibitively expensive  Use disk drives for serial reads</p>
  </div>
  <div class="page">
    <p>Inherent Tradeoff in Current FS Designs</p>
    <p>Optimized for one prevalent access pattern  either serial reads or random updates</p>
    <p>Update-in-place writes (e.g., FFS, ext2 etc.)  Preserve physical locality for serial reads  Updates inefficient, especially with parity RAID</p>
    <p>LFS-style log-append writes  Fragment on-media layout for serial reads</p>
    <p>Want a file system where both random writes and serial reads are efficient</p>
  </div>
  <div class="page">
    <p>Our Approach  Briefly</p>
    <p>Stage random updates in NV memory buffer  Destage (bulk write-allocate) to disk</p>
    <p>Use proximal disk I/O for efficient writes  Can retire multiple I/Os per revolution</p>
    <p>Write allocation with no overwrites  Old versions used for snapshots etc.  Maintains desired serial-read efficiency</p>
    <p>Write to free locations near related data</p>
    <p>RAID 1-like write performance on parity RAID Minimal serial read degradation on aged FS</p>
  </div>
  <div class="page">
    <p>Remainder of the Talk Outline</p>
    <p>System concepts</p>
    <p>Experimental results</p>
    <p>Concluding remarks</p>
  </div>
  <div class="page">
    <p>SYSTEM CONCEPTS</p>
  </div>
  <div class="page">
    <p>Efficient Proximal Disk I/O  Concept</p>
    <p>Disk technology trends  Minimal seek/head switch time &lt;1ms</p>
    <p>Same repositioning cost within ~100 of tracks  Increasing aerial bit density is in our favor</p>
    <p>Can service multiple small I/Os per rev.  Up to 10 locations within a span ~100K blocks  Large degree of freedom for write allocation</p>
    <p>Near-line (SATA) 7200 RPM disk  8.3 ms per revolution  0.8 ms head switch time</p>
  </div>
  <div class="page">
    <p>Proximal Disk I/O Details</p>
    <p>X X</p>
    <p>X</p>
    <p>X</p>
    <p>X X X</p>
    <p>~100 tracks</p>
    <p>Track switch time dictates bin width</p>
    <p>Min. seek time governs</p>
    <p>range of tracks</p>
    <p>Two revolutions needed for the schedule</p>
    <p>I/Os in the same bin serviced in different revolutions (head switch time is too big)</p>
    <p>Assume 8 I/Os per revolution</p>
    <p>X</p>
    <p>+</p>
    <p>+</p>
    <p>+</p>
    <p>+</p>
    <p>+</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Achieving Enough I/O Density</p>
    <p>Assume random updates  Single 1 TB disk holds  billion blocks  Proximal I/O needs 6 to 8 blocks/ ~100K blocks</p>
    <p>Stage random updates in NV memory  Destage to disk when required density achieved</p>
    <p>Trends in out favors  Not all I/Os are not user I/Os  Degree of randomness workload-dependent</p>
    <p>Stage area sized at 1% of the working set sufficient</p>
  </div>
  <div class="page">
    <p>Putting it All Together</p>
    <p>Disk reorders</p>
    <p>requests in the queue</p>
    <p>RAM-based buffer cache NV memory-based staging area</p>
    <p>Traditional approach Our approach with proximal I/O</p>
  </div>
  <div class="page">
    <p>Features &amp; Some Details</p>
    <p>Small (over)writes absorbed by Flash memory  Metadata updated immediately</p>
    <p>Updates create holes in on-disk layout  New versions of data accessed from Flash in</p>
    <p>parallel while streaming other data from disk</p>
    <p>Destaging  Batch blocks belonging to the same extent  Metadata reads amortized during destage</p>
    <p>read once access many times</p>
  </div>
  <div class="page">
    <p>EXPERIMENTAL RESULTS</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>File system data layout engine (DLE) prototype  Extent Create/Read/Write/Unlink ops  Basic implementation of allocation algorithms</p>
    <p>Data Storage  NetApp DS4243 Shelf with 1TB SATA disks  Staging area on FLASH-based SSDs  NetApp RAID user-level emulator</p>
    <p>Each FS 4KB block has checksum/context info  One block for every n user-blocks</p>
  </div>
  <div class="page">
    <p>Basic Workload Description</p>
    <p>Write sequentially large (16MB) extents  Many random small updates</p>
    <p>age initial on-disk layout  destage I/Os from Flash to disk</p>
    <p>Adverse scenario: 90% full FS  Limits allocation choices</p>
    <p>Vary staging area size  n% of disks capacity</p>
  </div>
  <div class="page">
    <p>New File System Layout</p>
    <p>Flash memory  holds metadata</p>
    <p>RAID  sequentially laid extents</p>
  </div>
  <div class="page">
    <p>Aged File System Layout</p>
    <p>Flash memory  holds metadata &amp; user updates in the stage area</p>
    <p>RAID</p>
  </div>
  <div class="page">
    <p>Small Random Updates</p>
    <p>6x smaller per user write service time  5.3 I/Os serviced per revolution</p>
    <p>With minimal reduction in write amplification</p>
    <p>User writes per batch</p>
    <p>Batch resp. time</p>
    <p>Service time per user write</p>
    <p>I/Os per revolution</p>
    <p>I/O amplification</p>
    <p>Baseline 1 16.1 ms 16.1 ms 2.0 8x</p>
    <p>Batch: one destage operation (blocks of one file/extent) I/O amplification: # of disk I/Os for each user write</p>
    <p>BASE: update-in-place writes w/ checksum block % Stage: Flash size relative to RAID capacity</p>
  </div>
  <div class="page">
    <p>Serial Reads after Random Updates</p>
    <p>Per-disk bandwidth</p>
    <p>I/Os to RAID per request</p>
    <p>Data Disks</p>
    <p>Avg. I/Os per disk</p>
    <p>Utilization (Busy time)</p>
    <p>New FS 89.0 MB/s 1.2 11.7 85%</p>
    <p>Aged FS 86.2 MB/s -3% 26.3 20.7 82%</p>
    <p>Aged LFS-style* 2.6 MB/s -97% 509.9 210.2 85%</p>
    <p>Mean RAID group I/O request size: 819 blocks Max I/O size per disk: 256 blocks (1024 KB)</p>
    <p>Only 3% BW degradation on aged layout</p>
    <p>* no segment cleaning/background reallocation</p>
  </div>
  <div class="page">
    <p>Flash Cost vs. Performance Trade off</p>
    <p>Diminishing return after stage area of 3% 19 USENIX FAST 2011</p>
    <p>Increasing stage area size  Expressed as % of the RAID group size</p>
    <p>N um</p>
    <p>be r</p>
    <p>of I/</p>
    <p>O s</p>
    <p>Staging Area Size (% relative to RAID size)</p>
    <p>I/Os per revolution</p>
    <p>Disk I/Os per user I/O</p>
  </div>
  <div class="page">
    <p>CONCLUDING REMARKS</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Flash memory (~1% of HDD capacity)  Absorbs small random writes  Destage when enough data for efficient disk write  3x more IOPS vs. equivalent disk-only system</p>
    <p>Making destage disk I/O efficient  Proximal I/O maximizes blocks accessed per seek</p>
    <p>5.3 I/Os per rev in the vicinity ~100,000 LBNs  RAID1-like I/O performance with RAID 4</p>
    <p>Minimize/eliminate background disk grooming  In-band reallocation during destage operation</p>
  </div>
  <div class="page">
    <p>Contact us</p>
    <p>At the poster session</p>
    <p>Email jiri.schindler@netapp.com</p>
    <p>Advanced Technology Group www.netapp.com/us/company/leadership/advanced-technology/</p>
    <p>Yes, we are hiring! http://www.netapp.com/careers</p>
  </div>
</Presentation>
