<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>From JVM to FPGA: Bridging Abstraction Hierarchy via Optimized Deep Pipelining</p>
    <p>Jason Cong, Peng Wei and Cody Hao Yu</p>
    <p>University of California, Los Angeles</p>
  </div>
  <div class="page">
    <p>Motivation:</p>
    <p>Harnessing FPGAs in Datacenter</p>
  </div>
  <div class="page">
    <p>Harnessing FPGAs in Datacenters: Why?</p>
    <p>u Heterogeneous architecture: an agreement from the hardware community</p>
    <p>Generalization Customization TPU GPGPU FPGA</p>
    <p>Sources: Google, NVIDIA, Intel, Bob Broderson, Berkeley Wireless group</p>
  </div>
  <div class="page">
    <p>u Heterogeneous architecture: an agreement from the hardware community</p>
    <p>u The FPGA-based cluster is a promising paradigm  Standalone FPGA accelerators demonstrate orders-of-magnitude performance/watt</p>
    <p>improvement</p>
    <p>Harnessing FPGAs in Datacenters: Why?</p>
  </div>
  <div class="page">
    <p>Harnessing FPGAs in Datacenters: Why?</p>
    <p>u Heterogeneous architecture: an agreement from the hardware community</p>
    <p>u The FPGA-based cluster is a promising paradigm  Standalone FPGA accelerators demonstrate orders-of-magnitude performance/watt</p>
    <p>improvement</p>
    <p>FPGAs are reconfigurable  A relatively general specialized device  It is now in the cloud</p>
  </div>
  <div class="page">
    <p>Challenge:</p>
    <p>system integration - from kernel speedup to system acceleration</p>
  </div>
  <div class="page">
    <p>Accelerator (FPGA)-as-a-Service</p>
    <p>Client RM AM</p>
    <p>NM</p>
    <p>NM</p>
    <p>Container Container</p>
    <p>Accelerator status</p>
    <p>GAM NAM</p>
    <p>NAM</p>
    <p>FPGA</p>
    <p>GPU</p>
    <p>Global Accelerator Manager: accelerator-centric scheduling</p>
    <p>Node Accelerator Manager: local accelerator service management, JVM-to-ACC communication optimization</p>
    <p>GAM</p>
    <p>NAM</p>
  </div>
  <div class="page">
    <p>In terms of performance, there is much to say...</p>
    <p>u Time breakdown of AES</p>
    <p>Pack: app.-dep.; ~4GB/s</p>
    <p>Send (via socket): ~3GB/s</p>
    <p>Usr-&gt;Kernel: ~6GB/s</p>
    <p>DMA: ~5GB/s</p>
    <p>Load: ~6GB/s (shd w/ Store)</p>
    <p>Compute: 12.8GB/s  &gt;100x over CPU</p>
    <p>1/(1/4+1/3+) = 0.47 GB/s</p>
    <p>27x performance loss!!!</p>
    <p>Host</p>
    <p>Kernel Memory Space</p>
    <p>User Memory Space</p>
    <p>JVM</p>
    <p>JVM Buf</p>
    <p>Pinned Buf</p>
    <p>Device</p>
    <p>Device Memory</p>
    <p>FPGA Fabric</p>
    <p>FPGA Accelerator</p>
    <p>BRAM and Flip-Flops</p>
    <p>Pinned Buf</p>
  </div>
  <div class="page">
    <p>Verilog HDLC/C++ &amp; Vendor APIsJava</p>
    <p>JVM-FPGA Communication Pipelining</p>
    <p>Pack</p>
    <p>JVM Input Buffer Queue</p>
    <p>Send</p>
    <p>Pageable Input Buffer Queue</p>
    <p>Pin</p>
    <p>Kernel Input Buffer Queue</p>
    <p>DMA Forward</p>
    <p>Device Input Buffer Queue</p>
    <p>Unpack</p>
    <p>JVM Output Buffer Queue</p>
    <p>Recv</p>
    <p>Pageable Output Buffer Queue</p>
    <p>Unpin</p>
    <p>Kernel Output Buffer Queue</p>
    <p>DMA Backward</p>
    <p>Device Output Buffer Queue</p>
    <p>Compute</p>
    <p>Input Java Objects</p>
    <p>Output Java Objects</p>
    <p>Load</p>
    <p>BRAM Input Buffer Queue</p>
    <p>Store</p>
    <p>BRAM Output Buffer Queue</p>
  </div>
  <div class="page">
    <p>JVM-FPGA Communication Pipelining</p>
    <p>u Send + Pin =&gt; Send  Limitation of vendor APIs</p>
    <p>u Load/Compute/Store =&gt; Compute  Overlapping comm. &amp; comp.</p>
    <p>u Programmers responsibility  Pack and unpack</p>
    <p>Implementing an iterator interface to supply input data</p>
    <p>Header + payload</p>
  </div>
  <div class="page">
    <p>u The pipeline efficiency is bounded by the slowest stage u In general, latency = time_setup (one-time) + payload_size * time_unit (linear) u Adjust the payload sizes of different pipeline stages to balance their throughputs u  but how to? u Linear with constraints =&gt; linear programming</p>
    <p>OpenCL Data Load Socket Data Send</p>
    <p>In terms of performance, there is still much to say...</p>
  </div>
  <div class="page">
    <p>Linear Programming Formulation Problem Formulation:</p>
    <p>maximize the pipeline throughput, i.e.,</p>
    <p>Modeling of Data Transfer Stages: for each individual data transfer stage,</p>
    <p>impose the payload size constraint, and model the relation between the payload size and the latency via linear equations:</p>
    <p>Modeling of Compute Stage: profile a set of payload sizes (power of two), and</p>
    <p>model the latency of the compute stage into a selection equation with a set of binary variables:</p>
    <p>Modeling of Memory Constraints: constrain the memory usage of the pipeline in both</p>
    <p>the CPU and the FPGA sides for separate-memory platforms, and in only the CPU side for shared-memory platfroms:</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>u A set of computation kernels as benchmarks</p>
    <p>u Each with a Java program as the host</p>
    <p>u Currently single-threaded, and will showcase the realapplication results in the near future</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>u A set of computation kernels as benchmarks</p>
    <p>u Each with a Java program as the host</p>
    <p>u Currently single-threaded, and will showcase the realapplication results in the near future</p>
  </div>
  <div class="page">
    <p>Lessons Learned and Future Work</p>
    <p>u Single thread -&gt; multiple threads -&gt; Mainstream frameworks  Modeling in the multithreaded scenario</p>
    <p>Integration with frameworks like Apache Hadoop and Spark</p>
    <p>u Adapt to various platforms  Latest platforms support FPGAs direct access of user-space data, like IBM CAPI and</p>
    <p>Intel Xeon+FPGA</p>
    <p>Amazon EC2 F1 instance brings virtualization into consideration</p>
    <p>u JVM related improvement  Fast and safe allocation and management of native-space memory</p>
  </div>
  <div class="page">
    <p>THANKS FOR YOUR ATTENTION.</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Host</p>
    <p>Kernel Memory Space</p>
    <p>User Memory Space</p>
    <p>JVM</p>
    <p>JVM Buf</p>
    <p>Pinned Buf</p>
    <p>Device</p>
    <p>Device Memory</p>
    <p>FPGA Fabric</p>
    <p>FPGA Accelerator</p>
    <p>BRAM and Flip-Flops</p>
    <p>Pinned Buf</p>
    <p>Verilog HDLC/C++ &amp; Vendor APIs</p>
    <p>Java</p>
    <p>Pack</p>
    <p>JVM Input Buffer Queue</p>
    <p>Send</p>
    <p>Pageable Input Buffer Queue</p>
    <p>Pin</p>
    <p>Kernel Input Buffer Queue</p>
    <p>DMA Forward</p>
    <p>Device Input Buffer Queue</p>
    <p>Unpack</p>
    <p>JVM Output Buffer Queue</p>
    <p>Recv</p>
    <p>Pageable Output Buffer Queue</p>
    <p>Unpin</p>
    <p>Kernel Output Buffer Queue</p>
    <p>DMA Backward</p>
    <p>Device Output Buffer Queue</p>
    <p>Compute</p>
    <p>Input Java Objects</p>
    <p>Output Java Objects</p>
    <p>Load</p>
    <p>BRAM Input Buffer Queue</p>
    <p>Store</p>
    <p>BRAM Output Buffer Queue</p>
    <p>Problem Formulation: maximize the pipeline throughput, i.e.,</p>
    <p>Modeling of Data Transfer Stages: for each individual data transfer stage,</p>
    <p>impose the payload size constraint, and model the relation between the payload size and the latency via linear equations:</p>
    <p>Modeling of Compute Stage: profile a set of payload sizes (power of two),</p>
    <p>and model the latency of the compute stage into a selection equation with a set of binary variables:</p>
    <p>Modeling of Memory Constraints: constrain the memory usage of the pipeline</p>
    <p>in both the CPU and the FPGA sides for separate-memory platforms, and in only the CPU side for shared-memory platfroms:</p>
    <p>Linear Programming</p>
  </div>
</Presentation>
