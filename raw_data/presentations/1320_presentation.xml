<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Open CirrusTM Cloud Computing Testbed Federated Data Centers for Open Source Systems and Services ResearchFederated Data Centers for Open Source Systems and Services Research</p>
    <p>Roy Campbell,5 Indranil Gupta,5 Michael Heath,5 Steven Y. Ko,5</p>
    <p>Michael Kozuch,3 Marcel Kunze,4 Thomas Kwan,6 Kevin Lai,1 Hing Yan Lee,2</p>
    <p>Martha Lyons,1 Dejan Milojicic,1 David OHallaron,3 and Yeng Chai Soh2</p>
  </div>
  <div class="page">
    <p>Open Cirrus Cloud Computing Testbed</p>
    <p>Shared: research, applications, infrastructure (11K cores), data sets</p>
    <p>Global services: sign on, monitoring, store. Open src stack (prs, tashi, hadoop)</p>
    <p>Sponsored by HP, Intel, and Yahoo! (with additional support from NSF)</p>
    <p>9 sites currently, target of around 20 in the next two years.</p>
  </div>
  <div class="page">
    <p>Open Cirrus</p>
    <p>Objectives  Foster systems research around cloud computing</p>
    <p>Vendor-neutral open-source stacks and APIs for the cloud</p>
    <p>Expose research community to enterprise level requirements</p>
    <p>Provide realistic traces of cloud workloads</p>
    <p>How are we unique  Support for systems research and applications research</p>
    <p>Federation of heterogeneous datacenters</p>
    <p>Interesting data sets</p>
  </div>
  <div class="page">
    <p>Process</p>
    <p>Central Management Office, oversees Open Cirrus</p>
    <p>Governance model  Research team</p>
    <p>Technical Team</p>
    <p>New site additions  New site additions</p>
    <p>Support (legal (export, privacy), IT, etc.)</p>
    <p>Each site  Runs its own research and technical teams,</p>
    <p>Contributes individual technologies</p>
    <p>Operates some of the global services</p>
    <p>E.g. HP Site supports: Portal and PRS</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>Open Cirrus Sites Current Status (April 2009)</p>
    <p>Site</p>
    <p>Characteristics</p>
    <p>#Cores #Servers Public</p>
    <p>partition Memory</p>
    <p>Size</p>
    <p>Storage</p>
    <p>Size Spindles Network Focus</p>
    <p>HP 1,024 256 178 3.3TB 632TB 1152 10G internal</p>
    <p>Hadoop,</p>
    <p>Cells, PRS,</p>
    <p>scheduling</p>
    <p>IDA 2,400 300 100 4.8TB 43TB+</p>
    <p>Apps based on</p>
    <p>Hadoop, Pig16TB SAN Hadoop, Pig</p>
    <p>Intel 1060 155 145 1.16TB</p>
    <p>local 60TB</p>
    <p>attach</p>
    <p>MPI, Hadoop</p>
    <p>KIT 2048 256 128 10TB 1PB 192 1Gb/s</p>
    <p>Apps with</p>
    <p>high</p>
    <p>throughput</p>
    <p>UIUC 1024 128 64 2TB ~500TB 288 1Gb/s</p>
    <p>Datasets,</p>
    <p>cloud</p>
    <p>infrastructure</p>
    <p>Yahoo 3200 480 400 2.4TB 1.2PB 1600 1Gb/s Hadoop on</p>
    <p>demand</p>
  </div>
  <div class="page">
    <p>Open Cirrus Software Stack</p>
    <p>Cloud application services</p>
    <p>Virtual Resource Sets</p>
    <p>| Ma rcel Kun ze | Op enC irru s, Ne SC Edi nbu rgh | Ma rch 20 09 8</p>
    <p>IT infrastructure layer</p>
    <p>(Physical Resource Sets)</p>
    <p>Cloud infrastructure services</p>
    <p>Eucalyptus Tashi</p>
  </div>
  <div class="page">
    <p>Physical Resource Sets (PRS)</p>
    <p>PRS service goals  Provide mini-datacenters to researchers</p>
    <p>Isolate experiments from each other</p>
    <p>Stable base for other research</p>
    <p>PRS service approach</p>
    <p>PRS service approach  Allocate sets of physical co-located nodes, isolated inside VLANs</p>
    <p>Start simple, add features as we go</p>
    <p>Base to implement virtual resource sets</p>
    <p>Hardware as a Service (HaaS)</p>
  </div>
  <div class="page">
    <p>Client</p>
    <p>Resource DB</p>
    <p>Node Manager DB</p>
    <p>VM instance DB</p>
    <p>Data</p>
    <p>Location</p>
    <p>Service</p>
    <p>Client API</p>
    <p>Site Specific</p>
    <p>Plugin(s)</p>
    <p>Centralized cluster administration Resource</p>
    <p>Telemetry</p>
    <p>Service</p>
    <p>Scheduling</p>
    <p>Agent</p>
    <p>Tashi Software Architecture</p>
    <p>Cluster Manager (CM)</p>
    <p>VM VM</p>
    <p>VM</p>
    <p>Node Manager</p>
    <p>(NM)</p>
    <p>system software</p>
    <p>Resource Controller Plugins</p>
    <p>(VMM, DFS, power, etc.) VMM</p>
    <p>VM</p>
    <p>DFS</p>
    <p>nmd iptables</p>
    <p>/vlan</p>
    <p>Service</p>
    <p>DFS</p>
    <p>Metadata</p>
    <p>Server</p>
    <p>CM-NM API</p>
    <p>Compute node</p>
    <p>Tashi</p>
    <p>component</p>
    <p>non-Tashi</p>
    <p>component</p>
    <p>Legend Sensor</p>
    <p>Plugins</p>
    <p>Pub/Sub</p>
    <p>System</p>
  </div>
  <div class="page">
    <p>Programming the Cloud: Hadoop  An open-source Apache software foundation project sponsored by Yahoo!  http://wiki.apache.org/hadoop/ProjectDescription</p>
    <p>reproduce the proprietary software infrastructure developed by Google</p>
    <p>Provides a parallel programming model (MapReduce), a distributed file system, and a parallel database</p>
    <p>distributed file system, and a parallel database  http://en.wikipedia.org/wiki/Hadoop</p>
    <p>http://code.google.com/edu/parallel/mapreduce-tutorial.html</p>
  </div>
  <div class="page">
    <p>How do users get access to Open Cirrus sites?</p>
    <p>Project PIs apply to each site separately</p>
    <p>Contact email addresses on the Open Cirrus portal  http://opencirrus.org</p>
    <p>Each Open Cirrus site decides which users and projects get access to its site</p>
    <p>A global sign on for all sites  Users are able to login to each OpenCirrus site for which they are authorized using the same login and password.</p>
  </div>
  <div class="page">
    <p>What kinds of research projects are Open Cirrus sites looking for?</p>
    <p>Open CirrusTM is seeking research in the following areas (different centers will weight these differently) Datacenter federation</p>
    <p>Datacenter management</p>
    <p>Web services</p>
    <p>Data-intensive applications and systems</p>
    <p>Hadoop map-reduce applications</p>
    <p>The following kinds of projects are of less interest Traditional HPC application development</p>
    <p>Production applications that just need lots of cycles</p>
    <p>Closed source system development</p>
  </div>
  <div class="page">
    <p>Metrics of Success</p>
    <p>Community  Technology used</p>
    <p># Sites, Projects, (Vibrant) Users</p>
    <p>Research Productivity (Shared Cost of Research), # papers published</p>
    <p>Cross-collaboration (Portal traffic)</p>
    <p># New open source components</p>
    <p># sites</p>
    <p># developers</p>
    <p># users</p>
    <p># New open source components</p>
    <p>Technical  Utilization of Open Cirrus, TCO</p>
    <p>Ease of use (e.g. provision 50% of OC nodes in &lt; 30sec)</p>
    <p>Federation transparency/adoption</p>
    <p>Reliability</p>
  </div>
  <div class="page">
    <p>Single site Cloud: to Outsource or Own?</p>
    <p>Medium-sized organization: wishes to run a service for M months  Service requires 128 servers (1024 cores) and 524 TB  Same as UIUC cloud site</p>
    <p>Outsource (e.g., via AWS): monthly cost  Storage ~ $62 K  Total ~ $136 K (using 0.45:0.0.4:0.15 split for hardware:power:network) Total ~ $136 K (using 0.45:0.0.4:0.15 split for hardware:power:network)</p>
    <p>Own: monthly cost  Storage ~ $349 K / M  $ 1555 K / M + 7.5 K (includes 1 sysadmin / 100 nodes)</p>
    <p>Breakeven analysis: more preferable to own if:  M &gt; 5.55 months (storage)</p>
    <p>Not surprising: Cloud providers benefit monetarily most from storage</p>
    <p>M &gt; 12 months (overall)</p>
    <p>With underutilization of x%, still more preferable to own if:  x &gt; 33.3%  Even with CPU util of 20%, storage &gt; 47% makes owning preferable</p>
  </div>
  <div class="page">
    <p>Federation Economics</p>
    <p>M o</p>
    <p>n th</p>
    <p>ly C</p>
    <p>o st</p>
    <p>i n</p>
    <p>$ K</p>
    <p>Existing DC</p>
    <p>Open Cirrus 6</p>
    <p>Open Cirrus 50</p>
    <p>Federation can help contain demand overflow within itself</p>
    <p>M o</p>
    <p>n th</p>
    <p>ly C</p>
    <p>o st</p>
    <p>i n</p>
    <p>$ K</p>
    <p>Utilization</p>
    <p>Open Cirrus 50</p>
    <p>Cost of outsourcing overflow to AWS is higher than to federation of 6 sites</p>
    <p>Cost reduces with size of federation increasing to 50</p>
  </div>
  <div class="page">
    <p>Testbeds</p>
    <p>Open</p>
    <p>Cirrus IBM/Google TeraGrid PlanetLab EmuLab</p>
    <p>Open Cloud</p>
    <p>Consortium</p>
    <p>Amazon</p>
    <p>EC2</p>
    <p>LANL/NSF</p>
    <p>cluster</p>
    <p>Type of</p>
    <p>research</p>
    <p>Systems &amp;</p>
    <p>applications</p>
    <p>Data-intensive</p>
    <p>applications</p>
    <p>research</p>
    <p>Scientific</p>
    <p>applications</p>
    <p>Systems and</p>
    <p>services Systems</p>
    <p>interoperab.</p>
    <p>across clouds</p>
    <p>using open</p>
    <p>Commer.</p>
    <p>use Systems</p>
    <p>Open Cirrus v. Other Testbeds</p>
    <p>research applications research</p>
    <p>applications services using open</p>
    <p>APIs</p>
    <p>use</p>
    <p>Approach</p>
    <p>Federation</p>
    <p>of heterog.</p>
    <p>data centers</p>
    <p>A cluster</p>
    <p>supported by</p>
    <p>Google</p>
    <p>and IBM</p>
    <p>Multi-site</p>
    <p>hetero</p>
    <p>clusters</p>
    <p>super comp.</p>
    <p>A few 100</p>
    <p>nodes hosted</p>
    <p>by research</p>
    <p>instit.</p>
    <p>A single-site</p>
    <p>cluster with</p>
    <p>flexible</p>
    <p>control</p>
    <p>Multi-site</p>
    <p>heterogeneous</p>
    <p>clusters</p>
    <p>Raw</p>
    <p>access to</p>
    <p>virtual</p>
    <p>machines</p>
    <p>Re-use of</p>
    <p>LANLs</p>
    <p>retiring</p>
    <p>clusters</p>
    <p>Participants</p>
    <p>HP, Intel,</p>
    <p>IDA, KIT,</p>
    <p>UIUC,</p>
    <p>Yahoo!</p>
    <p>IBM, Google,</p>
    <p>Stanford,</p>
    <p>U.Washington,</p>
    <p>MIT</p>
    <p>Many univ.</p>
    <p>&amp; orgs</p>
    <p>Many univ &amp;</p>
    <p>organizations</p>
    <p>University</p>
    <p>of Utah 4 centers  Amazon</p>
    <p>CMU, LANL,</p>
    <p>NSF</p>
    <p>Distribution 6 sites 1 site 11 partners</p>
    <p>in US</p>
    <p>&gt; 700 nodes</p>
    <p>world-wide</p>
    <p>&gt;300 nodes</p>
    <p>univ@Utah</p>
    <p>distributed in</p>
    <p>four locations</p>
    <p>older, still</p>
    <p>useful nodes</p>
    <p>at 1 site</p>
  </div>
  <div class="page">
    <p>Open Cirrus Research Summary</p>
    <p>Cloud application frameworks and services</p>
    <p>Mercado</p>
    <p>Policy Aware Data Mgmt</p>
    <p>Wikipedia Mining &amp; tagging</p>
    <p>SPARQL Query over Hadoop (UTD)</p>
    <p>N-tier App Benchmark (GaTech)</p>
    <p>Everyday Sensing and Perception</p>
    <p>SLIPstream/Sprout</p>
    <p>Parallel Machine Learning</p>
    <p>NeuroSys</p>
    <p>Computational Health</p>
    <p>FastBeat ( w/France Telecom)</p>
    <p>HP Intel</p>
    <p>IT infrastructure layer</p>
    <p>Cloud infrastructure services</p>
    <p>Economic Cloud Stack</p>
    <p>Parallel Data Series</p>
    <p>OpenNet</p>
    <p>Exascale Data Center</p>
    <p>Tashi (with CMU, Yahoo)</p>
    <p>PRS (with HP)</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Cloud is creating a new paradigm in computing  Flexible and elastic resource provisioning</p>
    <p>Economy of scale makes it attractive</p>
    <p>Move from manufacture towards industrialization of IT (Everything as a Service)</p>
    <p>OpenCirrus offers interesting R&amp;D opportunities</p>
    <p>OpenCirrus offers interesting R&amp;D opportunities  Cloud systems and applications research and development</p>
    <p>Interesting data sets and federation of heterogeneous data centers</p>
    <p>OpenCirrus workshop at HP Palo Alto on June 8/9 has links to a lot of materials</p>
  </div>
  <div class="page"/>
</Presentation>
