<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Power-Aware Computer Systems (PACS) Lab</p>
    <p>Coordinating Liquid and Free Air Cooling with Workload Allocation for</p>
    <p>Data Center Power Minimization</p>
    <p>Dept. of Electrical and Computer Engineering The Ohio State University</p>
    <p>Li Li, Wenli Zheng, Xiaodong Wang, and Xiaorui Wang</p>
  </div>
  <div class="page">
    <p>Introduction  Data centers consume a huge amount of energy.</p>
    <p>Servers and cooling are the two biggest energy consumers.  New cooling techniques are increasingly adopted.</p>
    <p>Such as liquid cooling and free cooling.  Concern of large IT companies</p>
    <p>How to minimize both server and cooling power of geo-distributed data centers equipped with heterogeneous cooling techniques.</p>
    <p>Servers</p>
    <p>Cooling</p>
    <p>Networks Ligh3ng</p>
    <p>UPS loss</p>
  </div>
  <div class="page">
    <p>Background: Cooling System</p>
    <p>Liquid Cooling:  Uses coolant to directly absorb heat from servers.  Pros: High cooling efficiency  Cons: High maintenance cost &amp; implementation cost</p>
    <p>Air Cooling:  Uses CRAC (Computer Room Air Conditioning)</p>
    <p>system to supply cold air  Pros: Low maintenance cost  Cons: Low cooling efficiency &amp; High heat recirculation</p>
    <p>Free Cooling:  Directly uses outside cold air.  Pros: Highest efficiency &amp; low maintenance cost  Cons: Highly depends on outside temperature</p>
  </div>
  <div class="page">
    <p>Closely Related Work  Power minimization in a single data center.</p>
    <p>Server power minimization. [ASPLOS09] [USENIX08] [SIGMETRICS09]  Cooling power minimization.</p>
    <p>Free Cooling system. [CLOUDCOM11] [SEMI-THERM12]  Air Cooling system. [CASE10][ITHERM06]  Liquid Cooling system. [SEMI-THERM12][ITHERM12]</p>
    <p>Joint minimization of server power &amp; air cooling power. [ASPLOS10] [USENIX05]</p>
    <p>Our paper minimizes total power of a data center with air, liquid and free cooling systems.</p>
    <p>Geographically distributed data centers.  Temperature aware workload management. [SIGMETRICS12] [ICAC13]  Our paper dispatches workload to data centers with different</p>
    <p>cooling configurations for total power minimization.</p>
  </div>
  <div class="page">
    <p>Challenges in Joint Power Minimization</p>
    <p>Datacenter Air Cooling</p>
    <p>Workload Workload</p>
    <p>Free Cooling</p>
    <p>Liquid Cooling</p>
    <p>For a single hybrid-cooled data center:  How to efficiently coordinate air, liquid, and free cooling systems?  How to distribute workloads for joint power minimization?</p>
    <p>For geographically distributed data centers:  How to make online management decisions in real time?  Computing complexity is too high if all factors (workload distribution,</p>
    <p>cooling mode selection) are considered simultaneously.</p>
  </div>
  <div class="page">
    <p>What is This Paper About?  SmartCool: jointly minimizes server and cooling power.</p>
    <p>For reduced complexity, we decompose the problem to two levels.</p>
    <p>Level 1: Single hybrid-cooled data center:  Intelligent coordination of cooling systems to jointly optimize server</p>
    <p>and cooling power.  We formulate the cooling management in a hybrid-cooled data</p>
    <p>center as a constrained optimization problem.</p>
    <p>Level 2: Geographically distributed data centers:  A light-weight solution to dynamically dispatch requests among a</p>
    <p>network of data centers with heterogeneous cooling systems.</p>
  </div>
  <div class="page">
    <p>SmartCool: Single Data Center Level</p>
    <p>Air Cooling System</p>
    <p>Free Cooling System</p>
    <p>Liquid Cooling System</p>
    <p>O ut</p>
    <p>si de</p>
    <p>Te</p>
    <p>m pe</p>
    <p>ra tu</p>
    <p>re</p>
    <p>Time</p>
    <p>C P</p>
    <p>U u</p>
    <p>til iz</p>
    <p>at io</p>
    <p>n</p>
    <p>Time</p>
    <p>3 example scenarios of SmartCool for cooling system coordination. 1. Low Temperature + High Workload 2. High Temperature + Low Workload 3. Medium Temperature + Medium Workload</p>
    <p>Server Inlet temp threshold</p>
    <p>Air-Cooled Server</p>
    <p>Liquid-Cooled Server</p>
  </div>
  <div class="page">
    <p>Optimization Formulation for Single Data Center</p>
    <p>We formulate the power minimization problem of the hybridcooled data center.  N servers are deployed in the data center.  M servers are liquid-cooled.</p>
    <p>Total workload is  .  Problem Formulation  Models</p>
    <p>Minimize total power consumption</p>
    <p>: Workload handled by server i.</p>
    <p>: Binary variable to determine cooling mode.</p>
  </div>
  <div class="page">
    <p>SmartCool: Distributed Data Centers Level  A light-weight dynamic request dispatching algorithm.</p>
    <p>Among different data center sites with heterogeneous cooling systems.  For real-time online management decisions.</p>
    <p>Derive the cPUE model:</p>
    <p>=( , ) using the single data center optimization process.</p>
    <p>Optimize the total power by manipulating the workload assigned to each data center.</p>
    <p>G en</p>
    <p>ev a</p>
    <p>C hi</p>
    <p>ca go</p>
    <p>H</p>
    <p>am in</p>
    <p>a</p>
  </div>
  <div class="page">
    <p>Formulation for Distributed Data Centers  We formulate a global optimization problem for minimizing the</p>
    <p>total power consumption of geo-distributed data centers.  Each data center has N servers.  The data center system has K data centers to handle workload.  Problem Formulation  Models</p>
    <p>Data center outside temperature.</p>
    <p>Workload handled by certain data center.</p>
  </div>
  <div class="page">
    <p>Simulations for one data center  Standard data center configuration (alternating hot and cold aisle).  Four rows of servers, the first is liquid-cooled.  CFD is used to simulate heat recirculation.</p>
    <p>Simulations for multiple data centers.  Three geo-distributed data centers are evaluated.  One-week temperature traces of Geneva, Hamina and Chicago.</p>
    <p>Hardware experiments.  One liquid-cooled server and three air-cooled servers.  Heater is used to change the ambient temperature.</p>
    <p>Experiment Setup</p>
  </div>
  <div class="page">
    <p>Baselines for Comparison  Baselines for a single data center</p>
    <p>Load-Unaware: (state of the practice)  Use a fixed temperature threshold to determine cooling mode.  Prefers to distribute workload to liquid-cooled servers.</p>
    <p>Liquid-First:  First distribute workload in the same way as Load-Unaware.  Dynamically adjusts the temperature threshold based on real time</p>
    <p>workload.</p>
    <p>Baselines for geo-distributed data centers  Low-Temp-First:</p>
    <p>First distribute workload to datacenter with lowest outside temperature.  Liquid-First:</p>
    <p>Dispatch workload to data centers according to their cooling efficiency.</p>
  </div>
  <div class="page">
    <p>Results for A Single Data Center  SmartCool vs. Load-Unaware and Liquid-First</p>
    <p>a. Load-Unaware b. Liquid-First c. Smart-Cool Total Power at 50% Workload Power breakdown at 50% Workload</p>
    <p>Improves utilization of free cooling.  Distribute workload to air-cooled servers at low temperature.  Consumes less cooling power when traditional air cooling is used because</p>
    <p>of the consideration of heat recirculation.</p>
    <p>SmartCool:</p>
  </div>
  <div class="page">
    <p>Results for Geo-Distributed Data Centers</p>
    <p>IBM trace file Total Power Consumption</p>
    <p>We evaluate different power management schemes on three geodistributed data centers with real workload and temperature traces.</p>
    <p>Traces:  One week temperature traces from Geneva, Hamina and Chicago [online].  Average CPU utilization from IBM data center [PACT09].</p>
    <p>SmartCool jointly considers the impact of outside temperature and cooling configuration.</p>
    <p>More results (including hardware testbed results) are in the paper.</p>
  </div>
  <div class="page">
    <p>Conclusion  Joint minimization of server and cooling power</p>
    <p>For large IT companies with geo-distributed data centers.  Each data center uses heterogeneous cooling techniques.</p>
    <p>SmartCool features a two-level solution  Level 1: a single data center:</p>
    <p>Effectively coordinates different cooling techniques.  Dynamically allocates workload for jointly optimized total power.</p>
    <p>Level 2: geo-distributed data centers:  Light-weight dynamic request dispatching for real-time online</p>
    <p>decisions.</p>
  </div>
  <div class="page">
    <p>Thank you ! Questions?</p>
  </div>
  <div class="page">
    <p>Backup: Comparison of Cooling Schemes</p>
    <p>(a) Total Power at 30% Workload (b) Power break at 70% Workload</p>
    <p>We compare SmartCool with Load-Unaware and Liquid-First.</p>
    <p>Results have the same trend with that at 50% workload.</p>
  </div>
  <div class="page">
    <p>Cooling Power Break</p>
    <p>(a) Power break at 30% workload (b) Power break at 70% workload</p>
    <p>We break the cooling power consumption of three schemes: a. Load-Unaware b. Liquid-First c. Smart-Cool  We evaluate:</p>
    <p>Air Free (free cooling power consumption)  Air Chiller (traditional air cooling power consumption)  Liquid Cooling ( liquid cooling power consumption)</p>
  </div>
  <div class="page">
    <p>SmartCool vs. Global Optimal Solution  We evaluate our two-layer power optimization algorithm.  The two-layer solution is compared with global optimal scheme</p>
    <p>including:  Optimized total power consumption  Time overhead.</p>
    <p>Three smaller scale data centers are adopted.  Two rows of racks.  Each row has 4 racks and each rack has 4 blocks.</p>
    <p>(a) Power Comparison (b) Time Comparison</p>
  </div>
  <div class="page">
    <p>Backup: Hardware Experiment  We conduct experiments on our hardware testbed.</p>
    <p>1 liquid-cooled server and 3 air-cooled servers.  A heater is used to set the ambient temperatures.  Power meter is used to measure the power consumed by servers</p>
    <p>and cold plates.</p>
    <p>(a) Total Power at 30% Loading (b) Total Power at 50% Loading (c) Total Power at 50% Loading</p>
  </div>
</Presentation>
