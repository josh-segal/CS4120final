<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Distilling Knowledge for Search-based Structured Prediction</p>
    <p>Yijia Liu*, Wanxiang Che, Huaipeng Zhao, Bing Qin, Ting Liu Research Center for Social Computing and Information Retrieval</p>
    <p>Harbin Institute of Technology</p>
  </div>
  <div class="page">
    <p>Complex Model Wins</p>
    <p>[ResNet, 2015]</p>
    <p>[He+, 2017]</p>
  </div>
  <div class="page">
    <p>Dependency Parsing</p>
    <p>Baseline search SOTA Distillation Ensemble</p>
    <p>NMT</p>
    <p>Baseline search SOTA Distillation Ensemble</p>
  </div>
  <div class="page">
    <p>Dependency Parsing</p>
    <p>Baseline search SOTA Distillation Ensemble</p>
    <p>NMT</p>
    <p>Baseline search SOTA Distillation Ensemble</p>
  </div>
  <div class="page">
    <p>Classification vs. Structured Prediction</p>
    <p>Classifier! &quot;</p>
    <p>Structured Predictor! &quot;#,&quot;%,,y(</p>
  </div>
  <div class="page">
    <p>Classification vs. Structured Prediction</p>
    <p>ClassifierI like this book</p>
    <p>Structured Predictor I like this book</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Search-based Structured Prediction</p>
    <p>I thislike book</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>! &quot; # that Controls Search Process</p>
    <p>I thislike book</p>
    <p>book i like love the this</p>
    <p>p(y | I, like)</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Generic ! &quot; # LearningAlgorithm</p>
    <p>I thislike book</p>
    <p>book I like love the this</p>
    <p>argmax p(y | I, like)$(y=this)</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Problems of the Generic LearningAlgorithm</p>
    <p>I thislike book</p>
    <p>the</p>
    <p>Ambiguities in training data both this and the seems reasonable</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Problems of the Generic LearningAlgorithm</p>
    <p>I thislike book</p>
    <p>the</p>
    <p>Ambiguities in training data both this and the seems reasonable</p>
    <p>Training and test discrepancy What if I made wrong decision?</p>
    <p>?love</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Solutions in Previous Works</p>
    <p>I thislike book</p>
    <p>the</p>
    <p>Ambiguities in training data Ensemble (Dietterich, 2000)</p>
    <p>Training and test discrepancy Explore (Ross and Bagnell, 2010)</p>
    <p>love</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>Where WeAre</p>
    <p>I thislike book</p>
    <p>thelove</p>
    <p>Knowledge Distillation Ambiguities in training data Training and test discrepancy</p>
  </div>
  <div class="page">
    <p>Knowledge Distillation</p>
    <p>Learning from negative log-likelihood Learning from knowledge distillation</p>
    <p>book I like love the this</p>
    <p>argmax p(y | I, like)!(y=this)</p>
    <p>book I like love the this</p>
    <p>argmax sumy q(y) p(y |I, like)</p>
    <p>&quot; # $,&amp;'()) is the output distribution of a teacher model (e.g. ensemble)</p>
    <p>On supervised data argmax0</p>
    <p>book I like love the this</p>
    <p>p(y | I, like)!(y=this)</p>
    <p>book I like love the this</p>
    <p>sumy q(y) p(y | I, like)</p>
  </div>
  <div class="page">
    <p>Knowledge Distillation: from Where</p>
    <p>Learning from knowledge distillation</p>
    <p>book I like love the this</p>
    <p>argmax sumy q(y) p(y |I, like)</p>
    <p>Ambiguities in training data Ensemble (Dietterich, 2000) We use ensemble of M structure predictor as the teacher q</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>KD on Supervised (reference) Data</p>
    <p>I thislike book</p>
    <p>the</p>
    <p>!(y=this)</p>
    <p>book I like love the this</p>
    <p>p(y | I, like)</p>
    <p>book I like love the this</p>
    <p>sumy q(y) p(y | I, like)</p>
  </div>
  <div class="page">
    <p>Search Space</p>
    <p>KD on Explored Data</p>
    <p>I</p>
    <p>this like book</p>
    <p>the</p>
    <p>book I like love the this</p>
    <p>sumy q(y) p(y | I, like, the)</p>
    <p>Training and test discrepancy Explore (Ross and Bagnell, 2010) We use teacher q to explore the search space &amp; learn from KD on the explored data</p>
  </div>
  <div class="page">
    <p>We combine KD on reference and explored data</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Transition-based Dependency Parsing Penn Treebank (Stanford dependencies)</p>
    <p>LAS</p>
    <p>Baseline 90.83 Ensemble (20) 92.73 Distill (reference, ! = 1.0) 91.99 Distill (exploration) 92.00 Distill (both) 92.14 Ballesteros et al. (2016) (dyn. oracle) 91.42 Andor et al. (2016) (local, B=1) 91.02</p>
    <p>Neural Machine Translation IWSLT 2014 de-en</p>
    <p>BLEU</p>
    <p>Baseline 22.79 Ensemble (10) 26.26 Distill (reference, ! = 0.8) 24.76 Distill (exploration) 24.64 Distill (both) 25.44 MIXER (Ranzato et al. 2015) 20.73 Wiseman and Rush (2016) (local B=1) 22.53 Wiseman and Rush (2016) (global B=1) 23.83</p>
  </div>
  <div class="page">
    <p>Analysis: Why the Ensemble Works Better?</p>
    <p>Examining the ensemble on the problematic states.</p>
    <p>I thislike book</p>
    <p>the</p>
    <p>Optimal-yet-ambiguous Non-optimal</p>
    <p>love</p>
  </div>
  <div class="page">
    <p>Analysis: Why the Ensemble Works Better?</p>
    <p>Examining the ensemble on the problematic states.  Testbed: Transition-based dependency parsing.  Tools: dynamic oracle, which returns a set of reference actions</p>
    <p>for one state.  Evaluate the output distributions against the reference actions.</p>
    <p>optimal-yet-ambiguous non-optimal Baseline 68.59 89.59 Ensemble 74.19 90.90</p>
  </div>
  <div class="page">
    <p>Analysis: Is it Feasible to Fully Learn from KD w/o NLL?</p>
    <p>Fully learning from KD is feasible</p>
    <p>Transition-based Parsing Neural Machine Translation</p>
  </div>
  <div class="page">
    <p>Analysis: Is Learning from KD Stable?</p>
    <p>Transition-based Parsing Neural Machine Translation</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We propose to distill an ensemble into a single model both from reference and exploration states.</p>
    <p>Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single models performance.</p>
    <p>Analysis gives empirically guarantee for our distillation method.</p>
  </div>
  <div class="page">
    <p>Thanks and Q/A</p>
  </div>
</Presentation>
