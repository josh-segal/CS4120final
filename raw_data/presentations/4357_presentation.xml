<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Language Generation via DAG Transduction</p>
    <p>Yajie Ye, Weiwei Sun and Xiaojun Wan {yeyajie,ws,wanxiaojun}@pku.edu.cn</p>
    <p>Institute of Computer Science and Technology Peking University</p>
    <p>July 17, 2018</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Overview</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>A NLG system Architecture</p>
    <p>Communicative Goal</p>
    <p>Document Planning</p>
    <p>Document Plans</p>
    <p>Microplanning</p>
    <p>Sentence Plans</p>
    <p>Linguistic Realisation</p>
    <p>Surface Text</p>
    <p>Reference Ehud Reiter and Robert Dale, Building Natural Language Generation Systems, Cambridge University Press, 2000.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>A NLG system Architecture</p>
    <p>Communicative Goal</p>
    <p>Document Planning</p>
    <p>Document Plans</p>
    <p>Microplanning</p>
    <p>Sentence Plans</p>
    <p>Linguistic Realisation</p>
    <p>Surface Text</p>
    <p>In this paper, we study surface realization, i.e. mapping meaning representations to natural language sentences.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Meaning Representation</p>
    <p>Logic form, e.g. lambda calculus</p>
    <p>Feature structures  This paper: Graphs!</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Meaning Representation</p>
    <p>Logic form, e.g. lambda calculus  Feature structures</p>
    <p>This paper: Graphs!</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Meaning Representation</p>
    <p>Logic form, e.g. lambda calculus  Feature structures  This paper: Graphs!</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Graph-Structured Meaning Representation</p>
    <p>Different kinds of graph-structured semantic representations:  Semantic Dependency Graphs (SDP)  Abstract Meaning Representations (AMR)  Dependency-based Minimal Recursion Semantics (DMRS)  Elementary Dependency Structures (EDS)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Graph-Structured Meaning Representation</p>
    <p>Different kinds of graph-structured semantic representations:  Semantic Dependency Graphs (SDP)  Abstract Meaning Representations (AMR)  Dependency-based Minimal Recursion Semantics (DMRS)  Elementary Dependency Structures (EDS)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Graph-Structured Meaning Representation</p>
    <p>Different kinds of graph-structured semantic representations:  Semantic Dependency Graphs (SDP)  Abstract Meaning Representations (AMR)  Dependency-based Minimal Recursion Semantics (DMRS)  Elementary Dependency Structures (EDS)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Graph-Structured Meaning Representation</p>
    <p>Different kinds of graph-structured semantic representations:  Semantic Dependency Graphs (SDP)  Abstract Meaning Representations (AMR)  Dependency-based Minimal Recursion Semantics (DMRS)  Elementary Dependency Structures (EDS)</p>
    <p>_want_v_1_the_q</p>
    <p>_boy_n_1_the_q _believe_v_1 pronoun_q</p>
    <p>_girl_n_1 pron</p>
    <p>BV</p>
    <p>BV</p>
    <p>ARG1 ARG2</p>
    <p>ARG1 ARG2 BV</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Type-Logical Semantic Graph</p>
    <p>EDS graphs are grounded under type-logical semantics. They are usually very flat and multi-rooted graphs.</p>
    <p>_want_v_1_the_q</p>
    <p>_boy_n_1_the_q _believe_v_1 pronoun_q</p>
    <p>_girl_n_1 pron</p>
    <p>BV</p>
    <p>BV</p>
    <p>ARG1 ARG2</p>
    <p>ARG1 ARG2 BV</p>
    <p>The boy wants the girl to believe him.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Previous Work</p>
    <p>Reference Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Previous Work</p>
    <p>Reference Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text generation with synchronous node replacement grammar.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Previous Work</p>
    <p>Reference Carroll, John and Oepen, Stephan 2005. High efficiency realization for a wide-coverage unification grammar</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Formalisms for Strings, Trees and Graphs</p>
    <p>Chomsky hierarchy Grammar Abstract machines Type-0 - Turing machine Type-1 Context-sensitive Linear-bounded</p>
    <p>- Tree-adjoining Embedded pushdown Type-2 Context-free Nondeterministic pushdown Type-3 Regular Finite</p>
    <p>Manipulating Graphs: Graph Grammar and DAG Automata.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Existing System</p>
    <p>David Chiang, Frank Drewes, Daniel Gildea, Adam Lopez and Giorgio Satta. Weighted DAG Automata for Semantic Graphs.</p>
    <p>the longest NLP paper that Ive ever read</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata</p>
    <p>A weighted DAG automaton is a tuple</p>
    <p>M = , Q, , K</p>
    <p>q1</p>
    <p>q2</p>
    <p>q3</p>
    <p>qm</p>
    <p>r1</p>
    <p>r2</p>
    <p>rn</p>
    <p>{q1,    , qm} /  {r1,    , rn}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata</p>
    <p>q1</p>
    <p>q2</p>
    <p>q3</p>
    <p>qm</p>
    <p>r1</p>
    <p>r2</p>
    <p>rn</p>
    <p>A run of M on DAG D = V, E,  is an edge labeling function  : E  Q.</p>
    <p>The weight of  is the product of all weight of local transitions:</p>
    <p>() =  vV</p>
    <p>[ (in(v))</p>
    <p>(v)  (out(v))</p>
    <p>]</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Failed !</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Automata: Toy Example</p>
    <p>States:</p>
    <p>John wants to go.</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>Accept !</p>
    <p>Recognition Rules:</p>
    <p>{} _want_v_1 { , }</p>
    <p>{} proper_q  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ } _go_v_1  { }</p>
    <p>{ , , } named(John)  {}</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Existing System</p>
    <p>Daniel Quernheim and Kevin Knight. 2012. Towards probabilistic acceptors and transducers for feature structures</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG-to-Tree Transducer q</p>
    <p>WANT</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG-to-Tree Transducer q</p>
    <p>S</p>
    <p>qnomb wants qinfbWANT</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG-to-Tree Transducer q</p>
    <p>S</p>
    <p>qnomb wa n t s qinfb</p>
    <p>S</p>
    <p>qnomb wa n t s IN F</p>
    <p>qaccg to believe qaccb</p>
    <p>WANT</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
    <p>BOY GIRL</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG-to-Tree Transducer q</p>
    <p>S</p>
    <p>qnomb wa n t s qinfb</p>
    <p>S</p>
    <p>qnomb wa n t s IN F</p>
    <p>qaccg to believe qaccb</p>
    <p>S</p>
    <p>IN F</p>
    <p>N P N P N P</p>
    <p>t h e b oy wa n t s t h e gir l t o b elieve h im</p>
    <p>WANT</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
    <p>BELIEVE</p>
    <p>BOY GIRL</p>
    <p>BOY GIRL</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG-to-Tree Transducer</p>
    <p>q</p>
    <p>WA N T</p>
    <p>B E LIE V E</p>
    <p>B O Y G IR L</p>
    <p>S</p>
    <p>qnomb wa n t s qinfb</p>
    <p>B E LIE V E</p>
    <p>B O Y G IR L</p>
    <p>S</p>
    <p>qnomb wa n t s IN F</p>
    <p>qaccg to believe qaccb</p>
    <p>B O Y G IR L</p>
    <p>S</p>
    <p>IN F</p>
    <p>N P N P N P</p>
    <p>t h e b oy wa n t s t h e gir l t o b elieve h im</p>
    <p>Challenges for DAG-to-tree transduction on EDS graphs:  Cannot easily reverse the directions of edges  Cannot easily handle multiple roots</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
    <p>S = x21 + want + x11</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
    <p>S = x21 + want + x11 x11 = to + go</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
    <p>S = x21 + want + x11 x11 = to + go x41 =</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
    <p>S = x21 + want + x11 x11 = to + go x41 =  x21 = x41 + John</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Our DAG-to-program transducer The basic idea:</p>
    <p>Rewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.</p>
    <p>Obtaining target structures based on side effects of the DAG recognition.</p>
    <p>States:</p>
    <p>_want_v_1</p>
    <p>_go_v_1 proper_q</p>
    <p>named(John)</p>
    <p>John wants to go.</p>
    <p>The output of our transducer is a program:</p>
    <p>S = x21 + want + x11 x11 = to + go x41 =  x21 = x41 + John</p>
    <p>= S = John want to go</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Transducation Rules Recognition Part Generation Part</p>
    <p>A valid DAG Automata transition Statement template(s)</p>
    <p>{} _want_v_1 { , } S = v + L + v</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Transducation Rules Recognition Part Generation Part</p>
    <p>A valid DAG Automata transition Statement template(s)</p>
    <p>{} _want_v_1 { , } S = v + L + v</p>
    <p>We use parameterized states:</p>
    <p>label(number,direction)</p>
    <p>The range of direction: unchanged, empty, reversed.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Transducation Rules Recognition Part Generation Part</p>
    <p>A valid DAG Automata transition Statement template(s)</p>
    <p>{} _want_v_1 { , } S = v + L + v</p>
    <p>We use parameterized states:</p>
    <p>label(number,direction)</p>
    <p>The range of direction: unchanged, empty, reversed.</p>
    <p>{} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>_want_v_1</p>
    <p>_go_v_1</p>
    <p>proper_q</p>
    <p>named(John)</p>
    <p>e3</p>
    <p>e1</p>
    <p>e2</p>
    <p>e4</p>
    <p>Recognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>_want_v_1</p>
    <p>_go_v_1</p>
    <p>proper_q</p>
    <p>named(John)</p>
    <p>e3</p>
    <p>VP(1,u) e1</p>
    <p>e2 NP(1,u)</p>
    <p>e4</p>
    <p>Recognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>_want_v_1</p>
    <p>_go_v_1</p>
    <p>proper_q</p>
    <p>named(John)</p>
    <p>Empty(0,e) e3</p>
    <p>VP(1,u) e1</p>
    <p>e2 NP(1,u)</p>
    <p>e4</p>
    <p>Recognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>_want_v_1</p>
    <p>_go_v_1</p>
    <p>proper_q</p>
    <p>named(John)</p>
    <p>Empty(0,e) e3</p>
    <p>VP(1,u) e1</p>
    <p>e2 NP(1,u)</p>
    <p>e4 DET(1,r)</p>
    <p>Recognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>_want_v_1</p>
    <p>_go_v_1</p>
    <p>proper_q</p>
    <p>named(John)</p>
    <p>Empty(0,e) e3</p>
    <p>VP(1,u) e1</p>
    <p>e2 NP(1,u)</p>
    <p>e4 DET(1,r)</p>
    <p>Recognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().</p>
    <p>Accept !</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Toy Example Q = {DET(1,r), Empty(0,e), VP(1,u), NP(1,u)} Rule For Recognition For Generation 1 {} proper_q {DET(1,r)} vDET(1,r) =  2 {} _want_v_1 {VP(1,u), NP(1,u)} S = vNP(1,u) + L + vVP(1,u) 3 {VP(1,u)} _go_v_1 {Empty(0,e)} vVP(1,u) = to + L 4 {NP(1,u), DET(1,r)} named {} vNP(1,u) = vDET(1,r) + L</p>
    <p>S = vNP(1,u) + L + vVP(1,u)</p>
    <p>S = x21 + want + x11</p>
    <p>Instantiation: replace vl(j,d) of edge ei with variable xij and L with the output string in the statement templates.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>DAG Transduction based-NLG</p>
    <p>A general framework for DAG transduction based-NLG:</p>
    <p>Semantic Graph</p>
    <p>DAG Transducer</p>
    <p>Sequential Lemmas</p>
    <p>Seq2seq Model</p>
    <p>Surface string</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4</p>
    <p>e3 e2</p>
    <p>e5 e12</p>
    <p>e1</p>
    <p>e10e9</p>
    <p>e7</p>
    <p>e9</p>
    <p>e6</p>
    <p>e11</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>Finding intermediate</p>
    <p>tree Assigning spans Assigning labels</p>
    <p>Generating statement templates</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4</p>
    <p>e3 e2</p>
    <p>e5 e12</p>
    <p>e1</p>
    <p>e10e9</p>
    <p>e7</p>
    <p>e9</p>
    <p>e6</p>
    <p>e11</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>Finding intermediate</p>
    <p>tree Assigning spans Assigning labels</p>
    <p>Generating statement templates</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4 {&lt;34:48&gt;}</p>
    <p>e3 {&lt;0:48&gt;}</p>
    <p>e2{&lt;0:48&gt;} e5</p>
    <p>{&lt;16:20&gt;,&lt;29:48&gt;} e12</p>
    <p>e1{&lt;16:20&gt;}</p>
    <p>e10{&lt;0:4&gt;}e9 {}</p>
    <p>e7 {}</p>
    <p>e9 {&lt;37:48&gt;}</p>
    <p>e6 {&lt;49:51&gt;}</p>
    <p>e11 {&lt;0:12&gt;}</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>Finding intermediate</p>
    <p>tree Assigning spans Assigning labels</p>
    <p>Generating statement templates</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4 {PP&lt;34:48&gt;}</p>
    <p>e3 {S&lt;0:48&gt;}</p>
    <p>e2{S&lt;0:48&gt;} e5</p>
    <p>{ADV&lt;16:20&gt;,PP&lt;29:48&gt;} e12</p>
    <p>e1{ADV&lt;16:20&gt;}</p>
    <p>e10{DET&lt;0:4&gt;}e9 {}</p>
    <p>e7 {}</p>
    <p>e9 {NP&lt;37:48&gt;}</p>
    <p>e6 {NP&lt;49:51&gt;}</p>
    <p>e11 {NP&lt;0:12&gt;}</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>Finding intermediate</p>
    <p>tree Assigning spans Assigning labels</p>
    <p>Generating statement templates</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4 {PP&lt;34:48&gt;}</p>
    <p>e3 {S&lt;0:48&gt;}</p>
    <p>e2{S&lt;0:48&gt;} e5</p>
    <p>{ADV&lt;16:20&gt;,PP&lt;29:48&gt;} e12</p>
    <p>e1{ADV&lt;16:20&gt;}</p>
    <p>e10{DET&lt;0:4&gt;}e9 {}</p>
    <p>e7 {}</p>
    <p>e9 {NP&lt;37:48&gt;}</p>
    <p>e6 {NP&lt;49:51&gt;}</p>
    <p>e11 {NP&lt;0:12&gt;}</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>Finding intermediate</p>
    <p>tree Assigning spans Assigning labels</p>
    <p>Generating statement templates</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4 {PP&lt;34:48&gt;}</p>
    <p>e3 {S&lt;0:48&gt;}</p>
    <p>e2{S&lt;0:48&gt;} e5</p>
    <p>{ADV&lt;16:20&gt;,PP&lt;29:48&gt;} e12</p>
    <p>e1{ADV&lt;16:20&gt;}</p>
    <p>e10{DET&lt;0:4&gt;}e9 {}</p>
    <p>e7 {}</p>
    <p>e9 {NP&lt;37:48&gt;}</p>
    <p>e6 {NP&lt;49:51&gt;}</p>
    <p>e11 {NP&lt;0:12&gt;}</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>{ADV(1,r)} comp  {PP(1,u), ADV_PP(2,r)}</p>
    <p>vADV_PP(1,r) = vADV(1,r) vADV_PP(2,r) = than + vPP(1,u)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Inducing Transduction Rules</p>
    <p>_steep_a_1&lt;21:28&gt;</p>
    <p>_decline_n_1&lt;5:12&gt;</p>
    <p>focus_d</p>
    <p>mofy&lt;37:48&gt;</p>
    <p>comp pronoun_q</p>
    <p>pron&lt;49:51&gt;</p>
    <p>_say_v_to&lt;52:57&gt;</p>
    <p>proper_q _the_q&lt;0:4&gt;</p>
    <p>_even_x_deg&lt;16:20&gt;</p>
    <p>_in_p_temp&lt;34:36&gt;</p>
    <p>e4 {PP&lt;34:48&gt;}</p>
    <p>e3 {S&lt;0:48&gt;}</p>
    <p>e2{S&lt;0:48&gt;} e5</p>
    <p>{ADV&lt;16:20&gt;,PP&lt;29:48&gt;} e12</p>
    <p>e1{ADV&lt;16:20&gt;}</p>
    <p>e10{DET&lt;0:4&gt;}e9 {}</p>
    <p>e7 {}</p>
    <p>e9 {NP&lt;37:48&gt;}</p>
    <p>e6 {NP&lt;49:51&gt;}</p>
    <p>e11 {NP&lt;0:12&gt;}</p>
    <p>the decline is even steeper than in September, he said.</p>
    <p>{PP(1,u)} _in_p_temp  {NP(1,u)}</p>
    <p>vPP(1,u) = in + vNP(1,u)</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>NLG via DAG transduction</p>
    <p>Experimental set-up  Data: DeepBank + Wikiwoods  Decoder: Beam search (beam size = 128)  About 37,000 induced rules are directly obtained from</p>
    <p>DeepBank training dataset by a group of heuristic rules.  Disambiguation: global linear model</p>
    <p>Transducer Lemmas Sentences Coverage induced rules 89.44 74.94 67%</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Fine-to-coarse Transduction</p>
    <p>To deal with data sparseness problem, we use some heuristic rules to generate extened rules by slightly changing an induced rule. Given a induced rule:</p>
    <p>{NP, ADJ} X {} vNP = vADJ + L</p>
    <p>New rule generated by deleting:</p>
    <p>{NP} X {} vNP = L</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Fine-to-coarse Transduction</p>
    <p>To deal with data sparseness problem, we use some heuristic rules to generate extened rules by slightly changing an induced rule. Given a induced rule:</p>
    <p>{NP, ADJ} X {} vNP = vADJ + L</p>
    <p>New rule generated by copying:</p>
    <p>{NP, ADJ1, ADJ2} X {} vNP = vADJ1 + vADJ2 + L</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>NLG via DAG transduction</p>
    <p>Experimental set-up  Data: DeepBank + Wikiwoods  Decoder: Beam search (beam size = 128)  About 37,000 induced rules and 440,000 exteneded rules  Disambiguation: global linear model</p>
    <p>Transducer Lemmas Sentences Coverage induced rules 89.44 74.94 67% induced and exteneded rules 88.41 74.03 77%</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Fine-to-coarse transduction</p>
    <p>q1</p>
    <p>q2</p>
    <p>q3</p>
    <p>qm</p>
    <p>r1</p>
    <p>r2</p>
    <p>rn</p>
    <p>During decoding, when neither induced nor extended rule is applicable, we use markov model to create a dynamic rule on-the-fly:</p>
    <p>P({r1,    , rn}|C) = P(r1|C) n</p>
    <p>i=2 P(ri|C)P(ri|ri1, C)</p>
    <p>C = {q1,    , qm}, D represents the context.  r1,    , rn denotes the outgoing states.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>NLG via DAG transduction</p>
    <p>Experimental set-up  Data: DeepBank + Wikiwoods  Decoder: Beam search (beam size = 128)  Other tool: OpenNMT</p>
    <p>Transducer Lemmas Sentences Coverage induced rules 89.44 74.94 67% induced and exteneded rules 88.41 74.03 77% induced, exteneded and dynamic rules 82.04 68.07 100% DFS-NN 50.45 100% AMR-NN 33.8 100% AMR-NRG 25.62 100%</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Conclusion and Future Work</p>
    <p>English Resouce Semantics is fantastic!</p>
    <p>Conclusion  Formalism works for graph-to-string mapping, not surprisingly</p>
    <p>or surprisingly Future work</p>
    <p>Is the decoder perfect? No, not even close  Is the disambiguation model a neural one? No, graph</p>
    <p>embedding is non-trivial.</p>
  </div>
  <div class="page">
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>QUESTIONS? COMMENTS?</p>
  </div>
</Presentation>
