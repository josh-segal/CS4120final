<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Do Neural Network Cross-Modal Mappings Really Bridge Modalities?</p>
    <p>Language Intelligence and Information Retrieval group (LIIR) Department of Computer Science</p>
  </div>
  <div class="page">
    <p>Story</p>
    <p>Collell, G., Zhang, T., Moens, M.F. (2017) Imagined Visual Representations as Multimodal Embeddings. AAAI</p>
    <p>Learn mapping f : text  vision.</p>
    <p>Finding 1: Imagined vectors, f (text), outperform original visual vectors in 7/7 word similarity tasks. So, why are mapped vectors multimodal? We conjecture:</p>
    <p>Continuity. Output vector is nothing but the input vector transformed by a continuous map: f (x ) = x .</p>
    <p>Finding 2 (not in AAAI paper): Vectors imagined with an untrained network do even better.</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Applications (e.g., zero-shot image tagging, zero-shot translation or cross-modal retrieval ):</p>
    <p>Use linear or NN maps to bridge modalities / spaces.</p>
    <p>Then, they tag / translate based on neighborhood structure of mapped vectors f (X ).</p>
    <p>Research question: Is the neighborhood structure of f (X ) similar to that of Y ? Or rather to X ?</p>
    <p>How to measure similarity of 2 sets of vectors from different spaces? Idea: mean nearest neighbor overlap (mNNO)</p>
  </div>
  <div class="page">
    <p>General Setting</p>
    <p>Mappings f : X Y to bridge modalities X and Y:</p>
    <p>Linear (lin): f (x) = W0x + b0</p>
    <p>Feed-forward neural net (nn): f (x) = W1(W0x + b0) + b1</p>
    <p>f(M )</p>
    <p>M</p>
    <p>f(M )</p>
  </div>
  <div class="page">
    <p>Experiment 1</p>
    <p>Definition</p>
    <p>Nearest Neighbor Overlap (NNOK (vi, zi )) = number of K nearest neighbors that two paired data points vi , zi share in their respective spaces.</p>
    <p>The mean NNO is:</p>
    <p>mNNOK (V, Z ) = 1</p>
    <p>KN</p>
    <p>N i=1</p>
    <p>NNOK (vi, zi )</p>
    <p>{ NN3(vcat ) = {vdog, vtiger, vlion} NN3(zcat ) = {zmouse, ztiger, zlion}</p>
    <p>NNO3(vcat, zcat ) = 2</p>
  </div>
  <div class="page">
    <p>Experiment 1</p>
    <p>Goal: Learn map f : X  Y and calculate mNNO(Y , f (X )). Compare it with mNNO(X, f (X ))</p>
    <p>Experimental Setup Datasets: (i) ImageNet ; (ii) IAPR TC-12; (iii) Wikipedia</p>
    <p>Visual features: VGG-128 and ResNet.</p>
    <p>Text features: ImageNet (GloVe and word2vec); IAPR TC-12 &amp; Wikipedia (biGRU).</p>
    <p>Loss: MSE = 12f (x) y 2. We also tried max-margin and</p>
    <p>cosine.</p>
  </div>
  <div class="page">
    <p>Experiment 1: Results</p>
    <p>ResNet VGG-128</p>
    <p>X, f (X ) Y , f (X ) X, f (X ) Y , f (X )</p>
    <p>Im ag</p>
    <p>eN et I  T</p>
    <p>lin 0.681 0.262 0.723 0.236 nn 0.622 0.273 0.682 0.246</p>
    <p>T  I lin 0.379 0.241 0.339 0.229 nn 0.354 0.27 0.326 0.256</p>
    <p>IA P</p>
    <p>R T</p>
    <p>C -1</p>
    <p>I  T lin 0.358 0.214 0.382 0.163 nn 0.336 0.219 0.331 0.18</p>
    <p>T  I lin 0.48 0.2 0.419 0.167 nn 0.413 0.225 0.372 0.182</p>
    <p>W ik</p>
    <p>ip ed</p>
    <p>ia I  T lin 0.235 0.156 0.235 0.143 nn 0.269 0.161 0.282 0.148</p>
    <p>T  I lin 0.574 0.156 0.6 0.148 nn 0.521 0.156 0.511 0.151</p>
    <p>Table: X, f (X ) and Y , f (X ) denote mNNO10(X, f (X )) and mNNO10(Y , f (X )), respectively.</p>
  </div>
  <div class="page">
    <p>Experiment 2</p>
    <p>Goal: Map X with an untrained net f and compare performance of X with that of f (X ).</p>
    <p>We ablate from Experiment 1 the learning part and the choices of loss and output vectors.</p>
    <p>Experimental Setup Evaluate vectors in:</p>
    <p>(i) Semantic similarity: SemSim, Simlex-999 and SimVerb-3500. (ii) Relatedness: MEN and WordSim-353. (iii) Visual similarity: VisSim.</p>
  </div>
  <div class="page">
    <p>Experiment 2: Results</p>
    <p>WS-353 Men SemSim</p>
    <p>Cos Eucl Cos Eucl Cos Eucl</p>
    <p>fnn(GloVe) 0.632 0.634 0.795 0.791 0.75 0.744</p>
    <p>flin(GloVe) 0.63 0.606 0.798 0.781 0.763 0.712 GloVe 0.632 0.601 0.801 0.782 0.768 0.716</p>
    <p>fnn(ResNet) 0.402 0.408 0.556 0.554 0.512 0.513 flin(ResNet) 0.425 0.449 0.566 0.534 0.533 0.514 ResNet 0.423 0.457 0.567 0.535 0.534 0.516</p>
    <p>VisSim SimLex SimVerb</p>
    <p>Cos Eucl Cos Eucl Cos Eucl</p>
    <p>fnn(GloVe) 0.594 0.59 0.369 0.363 0.313 0.301</p>
    <p>flin(GloVe) 0.602 0.576 0.369 0.341 0.326 0.23 GloVe 0.606 0.58 0.371 0.34 0.32 0.235</p>
    <p>fnn(ResNet) 0.527 0.526 0.405 0.406 0.178 0.169 flin(ResNet) 0.541 0.498 0.409 0.404 0.198 0.182 ResNet 0.543 0.501 0.409 0.403 0.211 0.199</p>
    <p>Table: Spearman correlations between human ratings and similarities (cosine or Euclidean) predicted from embeddings.</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Conclusions:</p>
    <p>Neighborhood structure of f (X ) more similar to X than Y . Neighborhood structure of embeddings not significantly disrupted by mapping them with an untrained net.</p>
    <p>Future Work: How to mitigate the problem?</p>
    <p>Discriminator (adversarial) trying to guess whether the sample is from Y or f (X ). Incorporate pairwise similarities into loss function.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Questions?</p>
  </div>
</Presentation>
