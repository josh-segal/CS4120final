<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Garaph: Efficient GPU-accelerated Graph Processing on a Single Machine with Balanced Replication</p>
    <p>Lingxiao Ma , Zhi Yang , Han Chen , Jilong Xue , Yafei Dai *</p>
    <p>Computer Science Department, Peking University</p>
    <p>Microsoft Research</p>
    <p>* SPCCTA, Peking University</p>
    <p>USENIX ATC17, Santa Clara, CA, USA</p>
  </div>
  <div class="page">
    <p>Large-Scale Graph Processing</p>
  </div>
  <div class="page">
    <p>Powerful Storage &amp; Computation Technologies</p>
    <p>* Figure from Internet</p>
  </div>
  <div class="page">
    <p>Our Goal</p>
    <p>- Large Memory + Fast Secondary Storages</p>
    <p>- CPU+GPUs</p>
    <p>Host (CPU) Device (GPUs)</p>
    <p>Main Memory Global MemoryPCIe Bus</p>
    <p>Input Secondary Storages</p>
  </div>
  <div class="page">
    <p>Architecture</p>
    <p>Secondary Storages</p>
    <p>Edges</p>
    <p>Vertices</p>
    <p>M e</p>
    <p>m o</p>
    <p>ry</p>
    <p>CPU Kernel</p>
    <p>GPU Kernel</p>
    <p>Dispatcher</p>
    <p>Edges</p>
  </div>
  <div class="page">
    <p>Graph Representation for Hybrid CPU and GPU</p>
    <p>- CSC &amp; CSR representation</p>
    <p>- Shard: vertex interval</p>
    <p>- Page: batched shards</p>
    <p>Nbr</p>
    <p>Idx 6 9 9 CSC (incomming edges)</p>
    <p>IdxOff 0 0 1 1 2 3 4 4 4 Shard 0 Shard 1</p>
    <p>Idx 5 7 9 CSR (outgoing edges)</p>
  </div>
  <div class="page">
    <p>Programming APIs</p>
    <p>- GAS Decomposition</p>
    <p>- One program for both CPU and GPU</p>
    <p>*GAS figure from PowerGraph slides</p>
    <p>Activate</p>
  </div>
  <div class="page">
    <p>GPU Computation Kernel</p>
    <p>Edges</p>
    <p>Vertices</p>
    <p>H o</p>
    <p>st M e</p>
    <p>m o</p>
    <p>ry</p>
    <p>Device Memory</p>
    <p>Vertices</p>
    <p>Edges Streaming Multiprocessor</p>
    <p>L1 Cache/ Shared</p>
    <p>Memory</p>
    <p>Vertices Init</p>
    <p>Apply</p>
    <p>GPU</p>
    <p>Sync</p>
    <p>PCIe A p</p>
    <p>p ly</p>
  </div>
  <div class="page">
    <p>Gather in GPU Computation Kernel</p>
    <p>Shared Memory</p>
    <p>Global Memory</p>
  </div>
  <div class="page">
    <p>Problems in Gather</p>
    <p>Shared Memory</p>
    <p>Global Memory</p>
    <p>*Gomez-Luna, Juan, et al. &quot;Performance modeling of atomic additions on GPU scratchpad memory.&quot; TPDS 24.11 (2013): 2273-2282.</p>
    <p>- Conflicts - Linear penalty</p>
    <p>- Intra-warp &gt;&gt; Inter-warp</p>
  </div>
  <div class="page">
    <p>Replication-Based Gather</p>
    <p>- Customized replication - O(N) -&gt; O(logN), N32</p>
    <p>- Modeling: balance profits and costs</p>
    <p>Global Memory</p>
    <p>Shared Memory</p>
    <p>Mapping</p>
    <p>Aggregation</p>
  </div>
  <div class="page">
    <p>CPU Computation Kernel</p>
    <p>Edges</p>
    <p>Rep Rep Rep</p>
    <p>GlobalVertices</p>
    <p>Aggregation</p>
    <p>LocalVertices</p>
    <p>- Sequential memory access &amp; lock-free &amp; load balance</p>
  </div>
  <div class="page">
    <p>Dual Modes Processing Engine</p>
    <p>- Pull &amp; Notify-pull</p>
  </div>
  <div class="page">
    <p>Hybrid CPU-GPU Scheduling</p>
    <p>- CPU - Pros: thread sequential processing</p>
    <p>- Suit: pull/notify-pull dual-mode processing</p>
    <p>- GPU - Pros: SIMD parallel processing</p>
    <p>- Suit: replication-based gather processing (only pull)</p>
    <p>Page Page</p>
    <p>Page PageGPU0</p>
    <p>CPU</p>
    <p>Page PageGPU1 Page Page</p>
    <p>Page Page</p>
    <p>Page Page Page PageSchedule</p>
    <p>Time/Page Time/Page</p>
    <p>Time TimeGPU0</p>
    <p>CPU</p>
    <p>Time TimeGPU1 Time Time</p>
    <p>Time Time</p>
  </div>
  <div class="page">
    <p>Experiment Setup - Machine information</p>
    <p>- CPU: Intel Xeon E5-2650 v3 - 10 cores, 20 threads, 2.3-3.0GHz</p>
    <p>- Memory: 64GB dual-channel DDR4 2133MHz</p>
    <p>- GPU: NVidia GeForce GTX 1070 - 1920 cores, 15 SMs, 8GB memory</p>
    <p>- Typical graph applications - PR, CC, SSSP, NN, HS, CS</p>
    <p>- Datasets - 7 real world datasets</p>
    <p>- Compare - CuSha(HPDC14), Ligra(PPoPP13), Gemini(OSDI16)</p>
    <p>Graph |V| |E| Max in-deg Avg deg Size</p>
    <p>uk-2007@1M 1M 41M 0.4M 41 0.6GB</p>
    <p>uk-2014-host 4.8M 51M 0.7M 11 0.8GB</p>
    <p>enwiki-2013 4.2M 0.1B 0.4M 24 1.7GB</p>
    <p>gsh-2015-tpd 31M 0.6B 2.2M 20 10GB</p>
    <p>twitter-2010 42M 1.5B 0.8M 35 27GB</p>
    <p>sk-2005 51M 1.9B 8.6M 39 35GB</p>
    <p>renren-2010 58M 2.8B 0.3M 48 44GB</p>
  </div>
  <div class="page">
    <p>Evaluation: Overall Performance</p>
    <p>- Run 10 iterations of PR</p>
    <p>- Up to 4.05x faster than the fastest one</p>
    <p>gsh-2015-tpd twitter-2010 sk-2005 renren-2010</p>
    <p>R u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>CuSha Ligra Gemini Garaph-C Garaph-G Garaph-H</p>
    <p>uk-2007-05@1M uk-2014-host enwiki-2013</p>
    <p>R u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>CuSha Ligra Gemini Garaph-C Garaph-G Garaph-H</p>
  </div>
  <div class="page">
    <p>Evaluation: Overall Performance</p>
    <p>- Run CC to convergence</p>
    <p>- GPU is much slower than CPU without activation scheme</p>
    <p>- Up to 5.36x faster than the fastest one</p>
    <p>uk-2007-05@1M uk-2014-host enwiki-2013</p>
    <p>R u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>CuSha Ligra Gemini Garaph-C Garaph-G Garaph-H</p>
    <p>gsh-2015-tpd twitter-2010 sk-2005 renren-2010</p>
    <p>R u</p>
    <p>n ti</p>
    <p>m e</p>
    <p>(s )</p>
    <p>CuSha Ligra Gemini Garaph-C Garaph-G Garaph-H</p>
  </div>
  <div class="page">
    <p>Evaluation: Customized Replication</p>
    <p>- SK-2005 dataset - Slowest is 45.17x slower than the fastest one</p>
    <p>- Correlation: 0.9853 - =&gt; vertices of high degree</p>
    <p>- Customized replication - Up to 32.15x speedup</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Evaluation: Hybrid CPU-GPU Scheduling</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Garaph: efficient GPU-accelerated graph processing on a single machine - Replication-based GPU computation kernel.</p>
    <p>- Dual modes replication-based CPU computation kernel.</p>
    <p>- Scheduler for hybrid CPU and GPU.</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
  </div>
</Presentation>
