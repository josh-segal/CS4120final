<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Evaluating Neural Machine Translation in English-Japanese Task</p>
    <p>(TEAM ID: WEBLIO MT)</p>
    <p>Zhongyuan Zhu @raphaelshu</p>
  </div>
  <div class="page">
    <p>Empirically evaluate various models in EJ task  Two network architectures</p>
    <p>Three recurrent units  LSTM, GRU, IRNN</p>
    <p>multi-layer encoder-decoder model soft-attention model</p>
    <p>Two kinds of training data  naturally-ordered, pre-reordered</p>
  </div>
  <div class="page">
    <p>Results: perplexities</p>
  </div>
  <div class="page">
    <p>Results: evaluation scores</p>
    <p>BLEU RIBES HUMAN JPO</p>
    <p>Baseline phrase-based SMT 29.80 0.691</p>
    <p>Baseline hierarchical phrase-based SMT 32.56 0.746</p>
    <p>Baseline Tree-to-string SMT 33.44 0.758 30.00</p>
    <p>Submitted system 1 (NMT) 34.19 0.802 43.50</p>
    <p>Submitted system 2 (NMT + System combination) 36.21 0.809 53.75 3.81</p>
    <p>Best competitor 1: NAIST (Travatar System with NeuralMT Reranking) 38.17 0.813 62.25 4.04</p>
    <p>Best competitor 2: naver (SMT t2s + Spell correction + NMT reranking) 36.14 0.803 53.25 4.00</p>
  </div>
  <div class="page">
    <p>Finding &amp; Insights  Soft-attention models outperforms multi-layer</p>
    <p>encoder-decoder models  Training models on pre-reordered data hurts</p>
    <p>the performance  NMT models tend to make grammatically</p>
    <p>valid but incomplete translations</p>
  </div>
  <div class="page">
    <p>Thanks.</p>
  </div>
</Presentation>
