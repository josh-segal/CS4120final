<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention</p>
    <p>Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, Xueqi Cheng</p>
    <p>University of Chinese Academy of Sciences CAS Key Lab of Network Data Science and Technology, Institute of Computing</p>
    <p>Technology, Chinese Academy of Sciences</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation</p>
    <p>Model</p>
    <p>Experiments</p>
    <p>Summary</p>
    <p>Content</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Event Detection</p>
    <p>subtask of event extraction</p>
    <p>given a document, extract event triggers from individual sentences and further</p>
    <p>identifies the (pre-defined) type of events</p>
    <p>Event Trigger</p>
    <p>words in sentences that most clearly expresses occurrence of events</p>
    <p>They have been married for three years.</p>
    <p>Event Trigger ismarried, which represents a marry event</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>... I knew it was time to leave.</p>
    <p>... I knew it was time to leave. Is not that a great argument for term limits?</p>
    <p>End-Position event</p>
    <p>Transport event? End-Position event?</p>
    <p>The contextual information of a individual sentence offers</p>
    <p>more confident for classifying</p>
    <p>A single sentence may cause ambiguous</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Some shortcomings of existing works</p>
    <p>Manually designed document-level feature</p>
    <p>Ji and Grishman, ACL, 2008</p>
    <p>Liao and Grishman, ACL, 2010</p>
    <p>Huang and Riloff, AAAI, 2012</p>
    <p>Learning document embedding without supervision, cannot specifically</p>
    <p>capture event-related information</p>
    <p>Duan et al., IJCNLP, 2017</p>
  </div>
  <div class="page">
    <p>DEEB-RNN : The Proposed Model</p>
    <p>ED Oriented Document</p>
    <p>Embedding Learning</p>
    <p>Document-level Enhanced Event Detector 5</p>
  </div>
  <div class="page">
    <p>Word-level embeddings</p>
    <p>Word encoder</p>
    <p>Word attention</p>
    <p>Sentence representation</p>
    <p>Bi- GRU ([ , ]) it w it it</p>
    <p>h w e</p>
    <p>tanh( ) it w it</p>
    <p>T</p>
    <p>it it w</p>
    <p>u W h</p>
    <p>u c</p>
    <p>T</p>
    <p>i it it</p>
    <p>t</p>
    <p>s h</p>
    <p>6</p>
    <p>Model - ED Oriented Document Embedding Learning</p>
  </div>
  <div class="page">
    <p>Gold word-level attention signal:</p>
    <p>Loss function:</p>
    <p>( , ) ( ) L T</p>
    <p>w it it</p>
    <p>i t</p>
    <p>E</p>
    <p>Indicatedis a event trigger and is setted as 1, other words are setted as 0.</p>
    <p>The square error as the general loss of the attention at word level to supervise the learning process.</p>
    <p>Model - ED Oriented Document Embedding Learning</p>
  </div>
  <div class="page">
    <p>Sentence-level embeddings</p>
    <p>Sentence encoder</p>
    <p>Sentence attention</p>
    <p>Document representation</p>
    <p>Bi- GRU ( ) i s i</p>
    <p>q s</p>
    <p>tanh( ) i s i</p>
    <p>T</p>
    <p>i i s</p>
    <p>t W q</p>
    <p>t c</p>
    <p>L</p>
    <p>i i</p>
    <p>i</p>
    <p>d s</p>
    <p>8</p>
    <p>Model - ED Oriented Document Embedding Learning</p>
  </div>
  <div class="page">
    <p>Loss function:</p>
    <p>S1, S3 and SL are sentences with event triggers and is setted as 1, other sentences are setted as 0.</p>
    <p>The square error as the general loss of the attention at sentence level to supervise the learning process.</p>
    <p>( , ) ( ) L</p>
    <p>s i i</p>
    <p>i</p>
    <p>E</p>
    <p>Model - ED Oriented Document Embedding Learning</p>
    <p>Gold sentence-level attention signal:</p>
  </div>
  <div class="page">
    <p>Bi-GRU ([ , , ]) jt e jt jt</p>
    <p>f d w e</p>
    <p>( )</p>
    <p>( , ) I( ) log L T K</p>
    <p>k</p>
    <p>jt jt</p>
    <p>j t k</p>
    <p>J y o y k o</p>
    <p>Loss function:</p>
    <p>Event Detector:</p>
    <p>softmax output layer to get the predicted probability for each word</p>
    <p>cross-entropy error</p>
    <p>Model - Document-level Enhanced Event Detector</p>
  </div>
  <div class="page">
    <p>Model - Joint Training</p>
    <p>( ) ( ( , ) ( , ) ( , )) w s</p>
    <p>d</p>
    <p>J J y o E E</p>
    <p>denotes all parameters used in DEEB-RNN</p>
    <p>is the training document set</p>
    <p>and are hyper-parameters for striking a balance</p>
    <p>Joint Loss Function:</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>ACE 2005 Corpus</p>
    <p>33 categories</p>
    <p>6 sources</p>
    <p>599 documents</p>
    <p>5349 labeled events</p>
  </div>
  <div class="page">
    <p>Experiments - Configuration</p>
    <p>Parameters Setting</p>
    <p>entity type embeddings 50 (randomly initialized)</p>
    <p>word embeddings 300 (Google pre-trained)</p>
    <p>dropout rate 0.5</p>
    <p>training SGD</p>
    <p>Partitions #Documents</p>
    <p>Training set 529</p>
    <p>Validation set 30</p>
    <p>Test set 40</p>
    <p>GRU , GRU , GRU w s e</p>
    <p>, w s</p>
    <p>W W</p>
  </div>
  <div class="page">
    <p>Experiments  Model analysis</p>
    <p>Model Variants  DEEB-RNN computes attentions without supervision</p>
    <p>DEEB-RNN1 uses only the gold word-level attention signal</p>
    <p>DEEB-RNN2 uses only the gold sentence-level attention signal</p>
    <p>DEEB-RNN3 employs the gold attention signals at both word and sentence levels</p>
    <p>The model with both gold attention signals at word and sentence levels performs best.</p>
    <p>Models with document embeddings outperform the pure Bi-GRU method.</p>
  </div>
  <div class="page">
    <p>Experiments - Baselines</p>
    <p>Feature-based methods without document-level information :</p>
    <p>Sentence-level(2011), Joint Local(2013)</p>
    <p>Representation-based methods without document-level information :</p>
    <p>JRNN(2016), Skip-CNN(2016), ANN-S2(2017)</p>
    <p>Feature-based methods using document level information :</p>
    <p>Cross-event(2010), PSL(2016)</p>
    <p>Representation-based methods using document-level information :</p>
    <p>DLRNN(2017)</p>
  </div>
  <div class="page">
    <p>Experiments  Main Results</p>
    <p>Traditional Event Detection</p>
    <p>Models</p>
    <p>DEEB Models</p>
    <p>Feature-based without Document-level</p>
    <p>Representation-based without Document-level</p>
    <p>Using Document-level</p>
    <p>Our models consistently out-perform the existing state-of-the-art methods in terms of both recall and F1-measure.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Conclusions</p>
    <p>We proposed a hierarchical and supervised attention based and document</p>
    <p>embedding enhanced Bi-RNN method.</p>
    <p>We explored different strategies to construct gold word- and sentence-level attentions</p>
    <p>to focus on event information.</p>
    <p>We also showed this method achieves best performance in terms of both recall and</p>
    <p>F1-measure.</p>
    <p>Future work</p>
    <p>Automatically determine the weights of sentence and document embeddings.</p>
    <p>Use the architecture for another text task.</p>
  </div>
  <div class="page">
    <p>Thank you for your attention</p>
    <p>Q&amp;A</p>
    <p>Name Yue Zhao</p>
    <p>Email zhaoyue@software.ict.ac.cn</p>
  </div>
</Presentation>
