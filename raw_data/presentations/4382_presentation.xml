<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-extraction</p>
    <p>Wenya Wang and Sinno Jialin Pan Nanyang Technological University, Singapore</p>
    <p>SAP Innovation Center Singapore {wa0001ya, sinnopan}@ntu.edu.sg</p>
    <p>July 18, 2018</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Background: What is Aspect/Opinion Extraction</p>
    <p>Fine-grained Opinion Mining</p>
    <p>Figure 1: An example of review outputs.</p>
    <p>I Our focus: Aspect and Opinion Terms Co-extraction I Challenge: Limited resources for fine-grained annotations</p>
    <p>Cross-domain extraction</p>
  </div>
  <div class="page">
    <p>Background: What is Aspect/Opinion Extraction</p>
    <p>Fine-grained Opinion Mining</p>
    <p>Figure 1: An example of review outputs.</p>
    <p>I Our focus: Aspect and Opinion Terms Co-extraction I Challenge: Limited resources for fine-grained annotations</p>
    <p>Cross-domain extraction</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Problem Definition</p>
    <p>phone has a good screen</p>
    <p>N N N N BO B</p>
    <p>Input x</p>
    <p>Labels</p>
    <p>The</p>
    <p>2 3 4 5 6Features</p>
    <p>B { Beginning of aspect</p>
    <p>size</p>
    <p>7</p>
    <p>I</p>
    <p>I { Inside of aspect</p>
    <p>BO { Beginning of opinion</p>
    <p>IO { Inside of opinion</p>
    <p>N { None</p>
    <p>Figure 2: A deep learning model for sequence labeling.</p>
    <p>nS i=1, unlabeled</p>
    <p>data in target domain DT = {xTj } nT j=1</p>
    <p>I Idea: Build bridges across domains, learn shared space</p>
  </div>
  <div class="page">
    <p>Motivation: Domain Adaptation</p>
    <p>Figure 3: Domain shift for different domains. Figure 4: Syntactic patterns.</p>
  </div>
  <div class="page">
    <p>Motivation: Domain Adaptation</p>
    <p>Figure 3: Domain shift for different domains. Figure 4: Syntactic patterns.</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Overview &amp; Contribution</p>
    <p>Recursive Neural Structural Correspondence Network (RNSCN) I Structural correspondences are built based on common syntactic</p>
    <p>structures I Use relation vectors with auxiliary labels to learn a shared space across</p>
    <p>domains</p>
    <p>Label denoising auto-encoder I Deal with auxiliary label noise I Group relation vectors into their intrinsic clusters in an unsupervised</p>
    <p>manner</p>
    <p>A joint deep model</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Model Architecture: Recursive Neural Network</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>h4</p>
    <p>h3h</p>
    <p>h2</p>
    <p>r2 r43</p>
    <p>r24</p>
    <p>x x2 x3 x4</p>
    <p>Figure 5: A recursive neural network.</p>
    <p>Domain Adaptation</p>
    <p>Relation vectors: Relations as embeddings in the feature space</p>
    <p>r43 = tanh(Whh3 + Wx x4)</p>
    <p>h4 = tanh(Wamodr43 + Wx x4 + b)</p>
    <p>Auxiliary task: Dependency relation prediction</p>
    <p>y R 43 = softmax(WR r43 + bR )</p>
  </div>
  <div class="page">
    <p>Model Architecture: Recursive Neural Network</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>h4</p>
    <p>h3h</p>
    <p>h2</p>
    <p>r2 r43</p>
    <p>r24</p>
    <p>x x2 x3 x4</p>
    <p>y43 y2</p>
    <p>y24</p>
    <p>Figure 5: A recursive neural network.</p>
    <p>Domain Adaptation</p>
    <p>Relation vectors: Relations as embeddings in the feature space</p>
    <p>r43 = tanh(Whh3 + Wx x4)</p>
    <p>h4 = tanh(Wamodr43 + Wx x4 + b)</p>
    <p>Auxiliary task: Dependency relation prediction</p>
    <p>y R 43 = softmax(WR r43 + bR )</p>
  </div>
  <div class="page">
    <p>Model Architecture: Learn Shared Representations</p>
    <p>Recursive Neural Structural Correspondence Network (RNSCN)</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>x x2 x3 x4</p>
    <p>niceahaslaptop</p>
    <p>det</p>
    <p>amodnsubj</p>
    <p>x2 x3 x4 x5 x6x</p>
    <p>The screen</p>
    <p>dobj</p>
    <p>det</p>
    <p>RNSCN</p>
    <p>ource Target</p>
    <p>Figure 6: An example of how RNSCN learns the correspondences.</p>
  </div>
  <div class="page">
    <p>Model Architecture: Learn Shared Representations</p>
    <p>Recursive Neural Structural Correspondence Network (RNSCN)</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>h4</p>
    <p>h3</p>
    <p>r43</p>
    <p>x x2 x3 x4</p>
    <p>y43</p>
    <p>niceahaslaptop</p>
    <p>det</p>
    <p>amodnsubj</p>
    <p>h6</p>
    <p>h5 r65</p>
    <p>r36</p>
    <p>x2 x3 x4 x5</p>
    <p>y65</p>
    <p>x6x</p>
    <p>The screen</p>
    <p>dobj</p>
    <p>det</p>
    <p>h4</p>
    <p>r64</p>
    <p>y64</p>
    <p>RNSCN</p>
    <p>ource Target</p>
    <p>Figure 6: An example of how RNSCN learns the correspondences.</p>
  </div>
  <div class="page">
    <p>Model Architecture: Learn Shared Representations</p>
    <p>Recursive Neural Structural Correspondence Network (RNSCN)</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>h4</p>
    <p>h3h</p>
    <p>h2</p>
    <p>r2 r43</p>
    <p>r24</p>
    <p>x x2 x3 x4</p>
    <p>y43 y2</p>
    <p>y24</p>
    <p>niceahaslaptop</p>
    <p>det</p>
    <p>amodnsubj</p>
    <p>h6</p>
    <p>h5 h</p>
    <p>h3</p>
    <p>r2 r65</p>
    <p>r36</p>
    <p>x2 x3 x4 x5</p>
    <p>y2 y65</p>
    <p>y36</p>
    <p>x6x</p>
    <p>The screen</p>
    <p>dobj</p>
    <p>h2</p>
    <p>r32</p>
    <p>y32</p>
    <p>det</p>
    <p>h4</p>
    <p>r64</p>
    <p>y64</p>
    <p>RNSCN</p>
    <p>ource Target</p>
    <p>Figure 6: An example of how RNSCN learns the correspondences.</p>
  </div>
  <div class="page">
    <p>Model Architecture: Learn Shared Representations</p>
    <p>Recursive Neural Structural Correspondence Network (RNSCN)</p>
    <p>appetizersgoodoerthey</p>
    <p>root dobj</p>
    <p>amodnsubj</p>
    <p>h4</p>
    <p>h3h</p>
    <p>h2</p>
    <p>r2 r43</p>
    <p>r24</p>
    <p>x x2 x3 x4</p>
    <p>y43 y2</p>
    <p>y24</p>
    <p>niceahaslaptop</p>
    <p>det</p>
    <p>amodnsubj</p>
    <p>h6</p>
    <p>h5 h</p>
    <p>h3</p>
    <p>r2 r65</p>
    <p>r36</p>
    <p>x2 x3 x4 x5</p>
    <p>y2 y65</p>
    <p>y36</p>
    <p>x6x</p>
    <p>The screen</p>
    <p>dobj</p>
    <p>h2</p>
    <p>r32</p>
    <p>y32</p>
    <p>det</p>
    <p>h4</p>
    <p>r64</p>
    <p>y64</p>
    <p>h h</p>
    <p>h</p>
    <p>RNSCN</p>
    <p>GRU</p>
    <p>ource Target</p>
    <p>Figure 6: An example of how RNSCN learns the correspondences. 10 / 19</p>
  </div>
  <div class="page">
    <p>Model Architecture: Auxiliary Label Denoising</p>
    <p>ppetizersgood</p>
    <p>amod</p>
    <p>h4</p>
    <p>h</p>
    <p>r4</p>
    <p>x x4</p>
    <p>y4</p>
    <p>nice</p>
    <p>dobj</p>
    <p>h6</p>
    <p>h5 r65</p>
    <p>x5 x6</p>
    <p>screen</p>
    <p>ource Target</p>
    <p>y65</p>
    <p>correct lbel noisy lbel</p>
    <p>Figure 7: An autoencoder for label denoising.</p>
    <p>Reduce label noise: auto-encoders</p>
    <p>Encoding:</p>
    <p>gnm = fenc (Wenc, rnm)</p>
    <p>Decoding:</p>
    <p>rnm = fdec (Wdec, gnm)</p>
    <p>Auxiliary task:</p>
    <p>yRnm = softmax(WR gnm)</p>
  </div>
  <div class="page">
    <p>Model Architecture: Auxiliary Label Denoising</p>
    <p>ppetizersgood</p>
    <p>amod</p>
    <p>h4</p>
    <p>h</p>
    <p>r4</p>
    <p>x x4</p>
    <p>y4</p>
    <p>nice</p>
    <p>dobj</p>
    <p>h6</p>
    <p>h5 r65</p>
    <p>x5 x6</p>
    <p>screen</p>
    <p>Source Target</p>
    <p>intrinsic group</p>
    <p>g4</p>
    <p>Autoencoder</p>
    <p>y65</p>
    <p>intrinsic group</p>
    <p>g65</p>
    <p>Autoencoder</p>
    <p>Figure 7: An autoencoder for label denoising.</p>
    <p>Reduce label noise: auto-encoders</p>
    <p>Encoding:</p>
    <p>gnm = fenc (Wenc, rnm)</p>
    <p>Decoding:</p>
    <p>rnm = fdec (Wdec, gnm)</p>
    <p>Auxiliary task:</p>
    <p>yRnm = softmax(WR gnm)</p>
  </div>
  <div class="page">
    <p>Model Architecture: Auxiliary Label Denoising</p>
    <p>ynm</p>
    <p>autoencoder</p>
    <p>rnm</p>
    <p>g g2 gjj</p>
    <p>Wenc gnm</p>
    <p>encode</p>
    <p>rnm</p>
    <p>decode</p>
    <p>Wdec</p>
    <p>autoencoder</p>
    <p>group emedding</p>
    <p>rnm hm xn</p>
    <p>hn</p>
    <p>ynm</p>
    <p>Figure 8: An autoencoder for relation grouping.</p>
    <p>p(Gnm = i|rnm) = exp(r&gt;nmWenc gi )</p>
    <p>jG exp(r&gt;nmWenc gj )</p>
    <p>(1)</p>
    <p>gnm =</p>
    <p>|G| i=1</p>
    <p>p(Gnm = i|rnm)gi (2)</p>
    <p>`R = `R1 + `R2 + `R3 (3)</p>
    <p>`R1 = rnm  Wdec gnm 2 2</p>
    <p>`R2 = K</p>
    <p>k=1</p>
    <p>yRnm[k] log y R nm[k]</p>
    <p>`R3 = I  G&gt;G2</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Dataset Description # Sentences Training Testing R Restaurant 5,841 4,381 1,460 L Laptop 3,845 2,884 961 D Device 3,836 2,877 959</p>
    <p>Table 1: Data statistics with number of sentences.</p>
    <p>Table 2: Comparisons with different baselines.</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Injecting noise into syntactic relations</p>
    <p>Models RL RD LR LD DR DL</p>
    <p>AS OP AS OP AS OP AS OP AS OP AS OP</p>
    <p>RNSCN-GRU 37.77 62.35 33.02 57.54 53.18 71.44 35.65 60.02 49.62 69.42 45.92 63.85 RNSCN-GRU (r) 32.97 50.18 26.21 53.58 35.88 65.73 32.87 57.57 40.03 67.34 40.06 59.18 RNSCN+-GRU 40.43 65.85 35.10 60.17 52.91 72.51 40.42 61.15 48.36 73.75 51.14 71.18</p>
    <p>RNSCN+-GRU (r) 39.27 59.41 33.42 57.24 45.79 69.96 38.21 59.12 45.36 72.84 50.45 68.05</p>
    <p>Table 3: Effect of auto-encoders for auxiliary label denoising.</p>
    <p>Words grouping learned from auto-encoders</p>
    <p>Group 1 this, the, their, my, here, it, I, our, not</p>
    <p>Group 2 quality, jukebox, maitre-d, sauces, portions, volume, friend, noodles, calamari</p>
    <p>Group 3 in, slightly, often, overall, regularly, since, back, much, ago</p>
    <p>Group 4 handy, tastier, white, salty, right, vibrant, first, ok</p>
    <p>Group 5 get, went, impressed, had, try, said, recommended, call, love</p>
    <p>Group 6 is, are, feels, believes, seems, like, will, would</p>
    <p>Table 4: Case studies on word clustering</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>f1 -o p in io n</p>
    <p>trade-off parameter ()</p>
    <p>f1 -a sp</p>
    <p>e c t</p>
    <p>(a) trade-off</p>
    <p>f1 -o p in io n</p>
    <p>number of groups (|G|) 0.36 0.37 0.38 0.39 0.40 0.41 0.42</p>
    <p>f1 -a sp</p>
    <p>e c t</p>
    <p>(b) Groups</p>
    <p>Figure 9: Sensitivity studies for LD.</p>
  </div>
  <div class="page">
    <p>Domain Adaptation: Experiments</p>
    <p>proportion of unlabeled target data</p>
    <p>f1 ( H ie rJo in t)</p>
    <p>f1 ( o u rs )</p>
    <p>(a) RL</p>
    <p>proportion of unlabeled target data</p>
    <p>f1 ( H ie rJo in t)</p>
    <p>f1 ( o u rs )</p>
    <p>(b) DL</p>
    <p>Figure 10: F1 vs proportion of unlabeled target data.</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>A novel deep learning framework for Cross-domain aspect and opinion terms extraction.</p>
    <p>Embed syntactic structure into a deep model to bridge the gap between different domains.</p>
    <p>Apply auxiliary task to assist knowledge transfer.</p>
    <p>Address the problem of negative effect brought by label noise.</p>
    <p>Achieve promising results.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Ding, Y., Yu, J., and Jiang, J. (2017).</p>
    <p>Recurrent neural networks with auxiliary labels for cross-domain opinion target extraction. In AAAI.</p>
    <p>Li, F., Pan, S. J., Jin, O., Yang, Q., and Zhu, X. (2012).</p>
    <p>Cross-domain co-extraction of sentiment and topic lexicons. In ACL.</p>
  </div>
  <div class="page">
    <p>Appendix: Domain Adaptation</p>
    <p>Models RL RD LR LD DR DL</p>
    <p>AS OP AS OP AS OP AS OP AS OP AS OP</p>
    <p>CrossCRF 19.72 59.20 21.07 52.05 28.19 65.52 29.96 56.17 6.59 39.38 24.22 46.67 (1.82) (1.34) (0.44) (1.67) (0.58) (0.89) (1.69) (1.49) (0.49) (3.06) (2.54) (2.43)</p>
    <p>RAP 25.92 62.72 22.63 54.44 46.90 67.98 34.54 54.25 45.44 60.67 28.22 59.79 (2.75) (0.49) (0.52) (2.20) (1.64) (1.05) (0.64) (1.65) (1.61) (2.15) (2.42) (4.18)</p>
    <p>Hier-Joint 33.66 - 33.20 - 48.10 - 31.25 - 47.97 - 34.74 (1.47) - (0.52) - (1.45) - (0.49) - (0.46) - (2.27)</p>
    <p>RNCRF 24.26 60.86 24.31 51.28 40.88 66.50 31.52 55.85 34.59 63.89 40.59 60.17 (3.97) (3.35) (2.57) (1.78) (2.09) (1.48) (1.40) (1.09) (1.34) (1.59) (0.80) (1.20)</p>
    <p>RNGRU 24.23 60.65 20.49 52.28 39.78 62.99 32.51 52.24 38.15 64.21 39.44 60.85 (2.41) (1.04) (2.68) (2.69) (0.61) (0.95) (1.12) (2.37) (2.82) (1.11) (2.79) (1.25)</p>
    <p>RNSCN-CRF 35.26 61.67 32.00 52.81 53.38 67.60 34.63 56.22 48.13 65.06 46.71 61.88 (1.31) (1.35) (1.48) (1.29) (1.49) (0.99) (1.38) (1.10) (0.71) (0.66) (1.16) (1.52)</p>
    <p>RNSCN-GRU 37.77 62.35 33.02 57.54 53.18 71.44 35.65 60.02 49.62 69.42 45.92 63.85 (0.45) (1.85) (0.58) (1.27) (0.75) (0.97) (0.77) (0.80) (0.34) (2.27) (1.14) (1.97)</p>
    <p>RNSCNh-GRU 39.13 63.65 33.97 59.24 55.74 75.20 40.30 60.57 51.23 71.93 48.35 68.20 (1.23) (1.36) (1.49) (1.59) (2.27) (1.03) (0.50) (0.93) (0.42) (1.55) (1.00) (1.11)</p>
    <p>RNSCN+-GRU 40.43 65.85 35.10 60.17 52.91 72.51 40.42 61.15 48.36 73.75 51.14 71.18 (0.96) (1.50) (0.62) (0.75) (1.82) (1.03) (0.70) (0.60) (1.14) (1.76) (1.68) (1.58)</p>
    <p>Table 5: Comparisons with different baselines.</p>
  </div>
</Presentation>
