<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Tackling the Biases in the Story Cloze Test Endings</p>
    <p>Rishi Sharma, James F. Allen, Omid Bakhshandeh, Nasrin Mostafazadeh</p>
    <p>July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 18, 2018</p>
    <p>Story Understanding and Story Generation</p>
    <p>An extremely challenging and long-running goal in AI (Charniak 1972; Turner, 1994; Schubert and Hwang, 2000)  The biggest challenge: having commonsense knowledge for</p>
    <p>the interpretation of narrative events. Requires commonsense reasoning, going beyond pattern recognition and explicit information extraction.</p>
    <p>akhshandeh,Rishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>ROC Stories (Mostafazadeh et al., 2016)</p>
    <p>A collection of high quality short five sentence stories. Each story:  Is realistic  Has a specific beginning and ending, where something happens in</p>
    <p>between  Has nothing irrelevant or redundant to the core story</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>Story Title Story</p>
    <p>The Test Jennifer has a big exam tomorrow. She got so stressed, she pulled an all-nighter. She went into class the next day, weary as can be. Her teacher stated that the test is postponed for next week. Jennifer felt bittersweet about it</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Story Cloze Test (Mostafazadeh et al., 2016)</p>
    <p>Context Right Ending Wrong Ending</p>
    <p>Jim got his first credit card in college. He didnt have a job so he bought everything on his card. After he graduated he amounted a $10,000 debt. Jim realized that he was foolish to spend so much money.</p>
    <p>Jim decided to devised a plan for repayment.</p>
    <p>Jim decided to open another credit card.</p>
    <p>The current benchmark for evaluating story understanding and narrative structure learning.</p>
    <p>Story Cloze Task: Given a context of four sentences, predict the ending of the story, i.e. Select from the right and wrong ending choices.</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>From now on we will refer to SCT as SCT-v1.0</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Results On SCT-1.0</p>
    <p>Baseline Results Constant Choose First 0.513</p>
    <p>Frequency 0.520 N-gram-overlap 0.494</p>
    <p>GenSim 0.539 Sentiment-Full 0.492 Sentiment-Last 0.522 Skip-thoughts 0.552</p>
    <p>Narrative-Chains-AP 0.478 Narrative-Chains-Stories 0.494</p>
    <p>DSSM 0.585 Human 1.0</p>
    <p>LSDSem17 and Other Models cogcomp Logistic 0.776</p>
    <p>msap Logistic 0.752 tbmihaylov LSTM 0.728</p>
    <p>ukp BiLSTM 0.717 acoli SVM 0.700</p>
    <p>roemmele RNN 0.672 mflor Rule Based 0.621</p>
    <p>Pranav Goel Logistic 0.604</p>
    <p>cogcomp(UIUC) - Linear classiftcation system that measures a storys coherence based on the sequence of events, emotional trajectory, and plot consistency (includes endings).</p>
    <p>msap(UW) - Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences.</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Story Ending Biases</p>
    <p>Mostafazadeh et al. (2016) were very careful with the task design, the data collection process, and establishing various baselines  sampled from ROC Stories  created Wrong Ending stories through Amazon MTurk  had an AMT to verify quality</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>Despite that, Schwartz et al. found stylistic differences between right and wrong endings:  number of words  n-gram distribution  character n-gram distribution</p>
    <p>Their classifier without feeding context achieves 72.4% accuracy on SCT-v1.0! **similar results confirmed by other models, (Cai et al., 2017)</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Biases in Various AI Datasets</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>From NLI, to VQA, and now Story Cloze Test, our narrow benchmarks inevitably have data creation artifacts and hence yield biased models.</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Our Main Contributions</p>
    <p>The summary of this talk 1. Analyzed SCT-v1.0 ending features 2. Developed a strong classifier on SCT-v1.0 using only ending</p>
    <p>features 3. Developed a new crowd-sourcing scheme to tackle the ending</p>
    <p>biases 4. Collected a new dataset, SCT-v1.5</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Statistical Analysis of Endings</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>We did an extensive analysis comparing the Right Endings and Wrong Ending features:  Token count  Sentiment  Complexity  Token n-grams  Character n-grams  Part of Speech n-grams  Combined Token + POS n-grams</p>
    <p>Analysis was done by performing  A two sample t-test between token count, sentiment, an</p>
    <p>complexity  Count measurements for the n-grams between Right and Wrong</p>
    <p>Endings</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Analysis: Token Count</p>
    <p>right endings wrong endings</p>
    <p>token count 8.705 8.466</p>
    <p>p-value = 6.63x105</p>
    <p>Conclusion: Right Endings tend to be longer than Wrong Endings.</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Analysis: Sentiment Analysis</p>
    <p>Used the Stanford Sentiment Analyzer [0-4] and Vader Sentiment Tagger [-1,1].</p>
    <p>right endings wrong endings p-value</p>
    <p>Stanford VADER</p>
    <p>Conclusion:  VADER Sentiment score is significant, right endings tend to be more positive</p>
    <p>than wrong endings.  The of most stories would probably yield neutral to positive higher and more</p>
    <p>concentrated peak around Right Endings wider distribution of Right Endings</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Analysis: Syntactic Complexity Measurement</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>Image from Roark et al. 2014</p>
    <p>right endings wrong endings p-value Frazie r Yngv e</p>
    <p>Conclusion: Yngve score was generally more stable and Wrong Endings are more complex than Right Endings.</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Analysis: N-gram Counts</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>Token n-grams 1-5 length stemmed token n-grams, with START token</p>
    <p>Character n-grams 4 character size n-grams</p>
    <p>Part of Speech n-grams POS tag and bucketed</p>
    <p>Combined Token + POS n-grams</p>
    <p>Analysis:  got or learn often in Right decid often in Wrong  Wrong frequently have tokens like nt  or snt  Right Endings are more likely to feature pronouns (PRP)</p>
    <p>whereas Wrong Endings are likely to use the proper noun (NNP).</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>EndingReg Model</p>
    <p>A Logistic Regression Model to perform the Story Cloze Test using only the following features extracted from the endings:  Number of tokens  VADER sentiment score  Yngve complexity score  Token-POS n-grams  POS n-grams  Four length character-grams</p>
    <p>*also added an L2 regularization penalty and used a grid search was conducted for parameter tuning</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>tokencount, VADER, yngve ngram pos chargrams All accuracy 50.3% 69.7% 68.7% 63.4% 71.5%</p>
    <p>**the above results indicate the minimum classification accuracy after 10 runs of the EndingReg Model build and classification</p>
    <p>Results on SCT-v1.0</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>The Criteria for the New Dataset</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>The Right and Wrong Endings should:</p>
    <p>Contain a similar number of tokens  Have similar distributions of token n-grams and char-grams  Occur as standalone events with the same likelihood to occur,</p>
    <p>with topical, sentimental, or emotional consistencies when applicable.</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Collecting The New Dataset</p>
    <p>After various rounds of pilot studies, we found the following paradigm to work the best: New Data Collection Steps:</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>token + POS n-gram char-gram POS n-gram</p>
    <p>SCT-v1.0 13.9 12.4 16.4</p>
    <p>SCT-v1.5 7.0 6.3 7.5</p>
    <p>Standard deviation of the word and character n-gram counts, as well as the part of speech (POS) counts, between the right and wrong endings.</p>
    <p>prompt instructs the workers to make sure: a. Wrong Ending makes sense standalone b. the Right and Wrong ending do not differ in # of words by &gt;3 c. changes cannot be as simple as negating the verb</p>
    <p>This entire process resulted in creating the Story Cloze Test v1.5 (SCT-v1.5) dataset, consisting of 1,571 stories for each validation and test sets.</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>EndingReg Results</p>
    <p>SCT-v1.0 Val SCT-v1.0 Test</p>
    <p>SCT-v1.5 Test</p>
    <p>cogcomp 0.751 0.776 0.608</p>
    <p>EndingReg N/A 0.715 0.558</p>
    <p>msap N/A 0.724 0.556 Human 1.0 1.0 1.0</p>
    <p>Classification accuracy for various models on the SCT-v1.0 and SCT-v1.5 datasets.</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>The SOTA Models</p>
    <p>In Improving Language Understanding by Generative Pre-Training model achieves accuracy of 86.5 on SCT-v1.0!  Pretrained language model made with Transformer network  Task specific supervised learning approach to classify</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018 !18</p>
    <p>Initial results on SCT-1.5 show an accuracy of 81.06% for this model, which suggests a deeper story understanding model that goes beyond leveraging the intricacies of the particular test sets.</p>
    <p>in Radford, Alec, et al. &quot;Improving Language Understanding by Generative Pre-Training.&quot;</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Conclusion</p>
    <p>In summary  We presented a comprehensive analysis of the stylistic features isolated in the</p>
    <p>endings of the original Story Cloze Test (SCT-v1.0).  Developed a strong classifier using only the story endings  Developed a new data collection schemes for tackling the stylistic ending</p>
    <p>features  Created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
    <p>Takeaways:  The success of our modified data collection method shows how</p>
    <p>extreme care must be given for sourcing new datasets.  However, as shown in multiple AI tasks, no collected dataset is</p>
    <p>entirely without its inherent biases and often the biases in datasets go undiscovered.</p>
    <p>Remember:  There is still a wide gap between system and human</p>
    <p>performance, on either SCT 1.0 or SCT 1.5 ;)</p>
  </div>
  <div class="page">
    <p>July 15, 2018July 18, 2018</p>
    <p>Next Steps</p>
    <p>We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.  Stay tuned for updates on the dataset and SOTA models via http://</p>
    <p>cs.rochester.edu/nlp/rocstories/  We expect to release the final dataset, along with reporting the performance</p>
    <p>of the most recent SCT 1.0 SOTA models on the new dataset, shortly after ACL</p>
    <p>akhshandeh, TRishi Sharma, James F. Allen, Omid B Tackling the Biases in the Story Cloze July 15, 2018</p>
  </div>
  <div class="page">
    <p>Thanks for your attention! Any Questions?</p>
  </div>
</Presentation>
