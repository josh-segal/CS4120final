<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Quantification of Integrity</p>
    <p>Michael Clarkson and Fred B. Schneider Cornell University</p>
    <p>IEEE Computer Security Foundations Symposium July 17, 2010</p>
  </div>
  <div class="page">
    <p>Goal</p>
    <p>Information-theoretic Quantification of</p>
    <p>programs impact on Integrity</p>
    <p>of Information</p>
    <p>Clarkson: Quantification of Integrity 2</p>
    <p>[Denning 1982]</p>
  </div>
  <div class="page">
    <p>What is Integrity?</p>
    <p>Common Criteria: Protection of assets from unauthorized modification</p>
    <p>Biba (1977): Guarantee that a subsystem will perform as it was</p>
    <p>intended Isolation necessary for protection from subversion Dual to confidentiality</p>
    <p>Databases: Constraints that relations must satisfy Provenance of data Utility of anonymized data</p>
    <p>no universal definition Clarkson: Quantification of Integrity 3</p>
  </div>
  <div class="page">
    <p>Our Notions of Integrity</p>
    <p>Clarkson: Quantification of Integrity 4</p>
    <p>Starting Point Corruption Measure</p>
    <p>Taint analysis Contamination Program correctness</p>
    <p>Suppression</p>
    <p>Corruption: damage to integrity</p>
  </div>
  <div class="page">
    <p>Our Notions of Integrity</p>
    <p>Clarkson: Quantification of Integrity 5</p>
    <p>Starting Point Corruption Measure</p>
    <p>Taint analysis Contamination Program correctness</p>
    <p>Suppression</p>
    <p>Corruption: damage to integrity</p>
    <p>Contamination: bad information present in output Suppression: good information lost from output</p>
    <p>distinct, but interact</p>
  </div>
  <div class="page">
    <p>Contamination</p>
    <p>Goal: model taint analysis</p>
    <p>Clarkson: Quantification of Integrity 6</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
  </div>
  <div class="page">
    <p>Contamination</p>
    <p>Goal: model taint analysis</p>
    <p>Untrusted input contaminates trusted output</p>
    <p>Clarkson: Quantification of Integrity 7</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
  </div>
  <div class="page">
    <p>Contamination</p>
    <p>u contaminates o</p>
    <p>Clarkson: Quantification of Integrity 8</p>
    <p>o:=(t,u)o:=(t,u)</p>
  </div>
  <div class="page">
    <p>Contamination</p>
    <p>u contaminates o</p>
    <p>(Cant u be filtered from o?)</p>
    <p>Clarkson: Quantification of Integrity 9</p>
    <p>o:=(t,u)o:=(t,u)</p>
  </div>
  <div class="page">
    <p>Quantification of Contamination</p>
    <p>Use information theory: information is surprise</p>
    <p>X, Y, Z: distributions</p>
    <p>I(X,Y): mutual information between X and Y (in bits)</p>
    <p>I(X,Y | Z): conditional mutual information</p>
    <p>Clarkson: Quantification of Integrity 10</p>
  </div>
  <div class="page">
    <p>Quantification of Contamination</p>
    <p>Clarkson: Quantification of Integrity 11</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
  </div>
  <div class="page">
    <p>Quantification of Contamination</p>
    <p>Clarkson: Quantification of Integrity 12</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>UinUin</p>
    <p>TinTin ToutTout</p>
  </div>
  <div class="page">
    <p>Quantification of Contamination</p>
    <p>Contamination = I(Uin,Tout | Tin)</p>
    <p>Clarkson: Quantification of Integrity 13</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>UinUin</p>
    <p>TinTin ToutTout</p>
    <p>[Newsome et al. 2009]</p>
    <p>Dual of [Clark et al. 2005, 2007]</p>
  </div>
  <div class="page">
    <p>Example of Contamination</p>
    <p>Clarkson: Quantification of Integrity 14</p>
    <p>o:=(t,u)o:=(t,u)</p>
    <p>Contamination = I(U, O | T) = k bits</p>
    <p>if U is uniform on [0,2k-1]</p>
  </div>
  <div class="page">
    <p>Our Notions of Integrity</p>
    <p>Clarkson: Quantification of Integrity 15</p>
    <p>Starting Point Corruption Measure</p>
    <p>Taint analysis Contamination Program correctness</p>
    <p>Suppression</p>
    <p>Corruption: damage to integrity</p>
    <p>Contamination: bad information present in output Suppression: good information lost from output</p>
  </div>
  <div class="page">
    <p>Program Suppression</p>
    <p>Goal: model program (in)correctness</p>
    <p>Clarkson: Quantification of Integrity 16</p>
    <p>SenderSender ReceiverReceiver correct</p>
    <p>Specification</p>
    <p>(Specification must be deterministic)</p>
  </div>
  <div class="page">
    <p>Program Suppression</p>
    <p>Goal: model program (in)correctness</p>
    <p>Clarkson: Quantification of Integrity 17</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SpecificationSpecificationSenderSender ReceiverReceiver correct</p>
    <p>real</p>
  </div>
  <div class="page">
    <p>Program Suppression</p>
    <p>Goal: model program (in)correctness</p>
    <p>Clarkson: Quantification of Integrity 18</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SenderSender ReceiverReceiver</p>
    <p>Implementation might suppress information about correct output from real output</p>
    <p>real</p>
    <p>correct SpecificationSpecification</p>
  </div>
  <div class="page">
    <p>Example of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 19</p>
    <p>for (i=0; i&lt;m; i++) { s := s + a[i]; } for (i=0; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>Spec.</p>
    <p>a[0..m-1]: trusted</p>
  </div>
  <div class="page">
    <p>Example of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 20</p>
    <p>for (i=0; i&lt;m; i++) { s := s + a[i]; } for (i=0; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>for (i=1; i&lt;m; i++) { s := s + a[i]; } for (i=1; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>Spec.</p>
    <p>Impl. 1</p>
    <p>Suppressiona[0] missing</p>
    <p>No contamination</p>
    <p>a[0..m-1]: trusted</p>
  </div>
  <div class="page">
    <p>Example of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 21</p>
    <p>for (i=0; i&lt;m; i++) { s := s + a[i]; } for (i=0; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>for (i=1; i&lt;m; i++) { s := s + a[i]; } for (i=1; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>for (i=0; i&lt;=m; i++) { s := s + a[i]; } for (i=0; i&lt;=m; i++) { s := s + a[i]; }</p>
    <p>Spec.</p>
    <p>Impl. 1 Impl. 2</p>
    <p>Suppressiona[0] missing</p>
    <p>No contamination</p>
    <p>Suppressiona[m] added</p>
    <p>Contamination</p>
    <p>a[0..m-1]: trusted</p>
    <p>a[m]: untrusted</p>
  </div>
  <div class="page">
    <p>Suppression vs. Contamination</p>
    <p>Clarkson: Quantification of Integrity 22</p>
    <p>**</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>**</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>Contaminati on</p>
    <p>Contaminati on</p>
    <p>SuppressionSuppression</p>
    <p>output := input</p>
    <p>output := input</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 23</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SpecificationSpecificationSenderSender ReceiverReceiver</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 24</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SpecificationSpecification</p>
    <p>InIn SpecSpec</p>
    <p>SenderSender ReceiverReceiver</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 25</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SpecificationSpecification</p>
    <p>UinUin</p>
    <p>TinTin ImplImpl</p>
    <p>InIn SpecSpec</p>
    <p>SenderSender ReceiverReceiver</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 26</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>SpecificationSpecification</p>
    <p>UinUin</p>
    <p>TinTin</p>
    <p>Program transmission = I(Spec , Impl)</p>
    <p>InIn SpecSpec</p>
    <p>ImplImpl</p>
    <p>SenderSender ReceiverReceiver</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>H(X): entropy (uncertainty) of X H(X|Y): conditional entropy of X given Y</p>
    <p>Program Transmission = I(Spec, Impl) = H(Spec)  H(Spec | Impl)</p>
    <p>Clarkson: Quantification of Integrity 27</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>H(X): entropy (uncertainty) of X H(X|Y): conditional entropy of X given Y</p>
    <p>Program Transmission = I(Spec, Impl) = H(Spec)  H(Spec | Impl)</p>
    <p>Clarkson: Quantification of Integrity 28</p>
    <p>Total info to learn about Spec</p>
    <p>Total info to learn about Spec</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>H(X): entropy (uncertainty) of X H(X|Y): conditional entropy of X given Y</p>
    <p>Program Transmission = I(Spec, Impl) = H(Spec)  H(Spec | Impl)</p>
    <p>Clarkson: Quantification of Integrity 29</p>
    <p>Info actually learned about</p>
    <p>Spec by observing Impl</p>
    <p>Info actually learned about</p>
    <p>Spec by observing Impl</p>
    <p>Total info to learn about Spec</p>
    <p>Total info to learn about Spec</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>H(X): entropy (uncertainty) of X H(X|Y): conditional entropy of X given Y</p>
    <p>Program Transmission = I(Spec, Impl) = H(Spec)  H(Spec | Impl)</p>
    <p>Clarkson: Quantification of Integrity 30</p>
    <p>Info actually learned about</p>
    <p>Spec by observing Impl</p>
    <p>Info actually learned about</p>
    <p>Spec by observing Impl</p>
    <p>Total info to learn about Spec</p>
    <p>Total info to learn about Spec Info NOT learned</p>
    <p>about Spec by observing Impl</p>
    <p>Info NOT learned about Spec by observing Impl</p>
  </div>
  <div class="page">
    <p>Quantification of Program Suppression</p>
    <p>H(X): entropy (uncertainty) of X H(X|Y): conditional entropy of X given Y</p>
    <p>Program Transmission = I(Spec, Impl) = H(Spec)  H(Spec | Impl)</p>
    <p>Program Suppression = H(Spec | Impl)</p>
    <p>Clarkson: Quantification of Integrity 31</p>
  </div>
  <div class="page">
    <p>Example of Program Suppression</p>
    <p>Clarkson: Quantification of Integrity 32</p>
    <p>for (i=0; i&lt;m; i++) { s := s + a[i]; } for (i=0; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>for (i=1; i&lt;m; i++) { s := s + a[i]; } for (i=1; i&lt;m; i++) { s := s + a[i]; }</p>
    <p>for (i=0; i&lt;=m; i++) { s := s + a[i]; } for (i=0; i&lt;=m; i++) { s := s + a[i]; }</p>
    <p>Spec.</p>
    <p>Impl. 1 Impl. 2</p>
    <p>Suppression = H(A) Suppression  H(A)</p>
    <p>A = distribution of individual array elements</p>
  </div>
  <div class="page">
    <p>Belief-based Metrics</p>
    <p>What if users/receivers distribution on unobservable inputs is wrong?</p>
    <p>Belief-based information flow [Clarkson et al. 2005]</p>
    <p>Belief-based generalizes informationtheoretic:</p>
    <p>On single executions, the same  In expectation, the same if users/receivers</p>
    <p>distribution is correct</p>
    <p>Clarkson: Quantification of Integrity 33</p>
  </div>
  <div class="page">
    <p>Suppression and Confidentiality</p>
    <p>Declassifier: program that reveals (leaks) some information; suppresses rest</p>
    <p>Leakage: [Denning 1982, Millen 1987, Gray 1991, Lowe 2002, Clark et al. 2005, 2007, Clarkson et al. 2005, McCamant &amp; Ernst 2008, Backes et al. 2009]</p>
    <p>Thm. Leakage + Suppression is a constant  What isnt leaked is suppressed</p>
    <p>Clarkson: Quantification of Integrity 34</p>
  </div>
  <div class="page">
    <p>Database Privacy</p>
    <p>Statistical database anonymizes query results:</p>
    <p>sacrifices utility for privacys sake</p>
    <p>Clarkson: Quantification of Integrity 35</p>
    <p>AnonymizerAnonymizer</p>
    <p>UserUser</p>
    <p>Databa se</p>
    <p>Databa se</p>
    <p>UserUser</p>
    <p>query</p>
    <p>response</p>
    <p>anonymized response</p>
  </div>
  <div class="page">
    <p>Database Privacy</p>
    <p>Statistical database anonymizes query results:</p>
    <p>sacrifices utility for privacys sake suppresses to avoid leakage</p>
    <p>Clarkson: Quantification of Integrity 36</p>
    <p>AnonymizerAnonymizer</p>
    <p>UserUser</p>
    <p>Databa se</p>
    <p>Databa se</p>
    <p>UserUser</p>
    <p>query</p>
    <p>response</p>
    <p>anonymized response</p>
    <p>anon. resp. := resp.</p>
    <p>anon. resp. := resp.</p>
  </div>
  <div class="page">
    <p>Database Privacy</p>
    <p>Statistical database anonymizes query results:</p>
    <p>sacrifices utility for privacys sake suppresses to avoid leakage sacrifices integrity for confidentialitys</p>
    <p>sakeClarkson: Quantification of Integrity 37</p>
    <p>AnonymizerAnonymizer</p>
    <p>UserUser</p>
    <p>Databa se</p>
    <p>Databa se</p>
    <p>UserUser</p>
    <p>query</p>
    <p>response</p>
    <p>anonymized response</p>
  </div>
  <div class="page">
    <p>Database Privacy Security Conditions</p>
    <p>k-anonymity: [Sweeney 2002]  Every individual must be anonymous within set of size k.  Every output corresponds to k inputs.</p>
    <p>no bound on leakage or suppression</p>
    <p>L-diversity: [hrn and Ohno-Machado 1999, Machanavajjhala et al. 2007]</p>
    <p>Every individuals sensitive information should appear to have L roughly equally likely values.</p>
    <p>Every output corresponds to L (roughly) equally likely inputs implies suppression  log L</p>
    <p>Differential privacy: [Dwork et al. 2006, Dwork 2006]  No individual loses privacy by including data in database  Output reveals almost no information about individual input</p>
    <p>beyond what other inputs already reveal implies almost all information about individual suppressed quite similar to noninterference</p>
    <p>Clarkson: Quantification of Integrity 38</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Measures of information corruption:  Contamination (generalizes taint analysis, dual</p>
    <p>to leakage)</p>
    <p>Suppression (generalizes program correctness, no dual)</p>
    <p>Application: database privacy (model anonymizers; relate utility and privacy; security conditions)</p>
    <p>Clarkson: Quantification of Integrity 39</p>
  </div>
  <div class="page">
    <p>More Integrity Measures</p>
    <p>Channel suppression same as channel model from information theory, but with attacker</p>
    <p>Attacker- and program-controlled suppression</p>
    <p>Granularity:  Average over all executions  Single executions  Sequences of executions</p>
    <p>interaction of attacker with program</p>
    <p>Application: Error-correcting codes</p>
    <p>Clarkson: Quantification of Integrity 40</p>
  </div>
  <div class="page">
    <p>Quantification of Integrity</p>
    <p>Michael Clarkson and Fred B. Schneider Cornell University</p>
    <p>IEEE Computer Security Foundations Symposium July 17, 2010</p>
  </div>
  <div class="page">
    <p>Beyond Contamination and Suppression?</p>
    <p>ClarkWilson integrity policy  Relational integrity constraints  Software testing metrics</p>
    <p>Clarkson: Quantification of Integrity 42</p>
  </div>
  <div class="page">
    <p>Beyond Contamination &amp; Suppression?</p>
    <p>Clarkson: Quantification of Integrity 43</p>
    <p>**</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>**</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
  </div>
  <div class="page">
    <p>Confidentiality Dual to Suppression?</p>
    <p>Clarkson: Quantification of Integrity 44</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>Public inputs lost from public outputs</p>
    <p>Public inputs lost from public outputs</p>
    <p>output := input</p>
    <p>output := input</p>
    <p>Classic duality of confidentiality and integrity is incomplete</p>
    <p>public</p>
    <p>secret</p>
  </div>
  <div class="page">
    <p>Value of Information</p>
    <p>What if some bits are worth more than others?</p>
    <p>Discrete worth: security levels Top secret, secret, confidential, unclassified</p>
    <p>Continuous worth: ?</p>
    <p>Clarkson: Quantification of Integrity 45</p>
  </div>
  <div class="page">
    <p>Bounded Reasoning</p>
    <p>Our agents are logically omniscient.</p>
    <p>Bounds? Algorithmic knowledge, computational entropy,</p>
    <p>Clarkson: Quantification of Integrity 46</p>
  </div>
  <div class="page">
    <p>Information</p>
    <p>Information is surprise.</p>
    <p>X: random variable on set of events {e, }</p>
    <p>Clarkson: Quantification of Integrity 47</p>
    <p>I(e): self-information conveyed by event e I(e) =  log2 Pr[X=e] (unit is bits)</p>
    <p>I(e): self-information conveyed by event e I(e) =  log2 Pr[X=e] (unit is bits)</p>
  </div>
  <div class="page">
    <p>Suppression vs. Contamination</p>
    <p>Clarkson: Quantification of Integrity 48</p>
    <p>n:=rnd(); o:=t xor n n:=rnd(); o:=t xor n</p>
    <p>o:=t xor uo:=t xor u</p>
    <p>o:=(t,u)o:=(t,u)</p>
    <p>t suppressed by noise no contamination</p>
    <p>t suppressed by u o contaminated by u</p>
    <p>o contaminated by u no suppression</p>
  </div>
  <div class="page">
    <p>Echo Specification</p>
    <p>Clarkson: Quantification of Integrity 49</p>
    <p>output := input</p>
    <p>output := inputSender</p>
    <p>Sender ReceiverReceivertrusted</p>
    <p>TinTin TinTin</p>
  </div>
  <div class="page">
    <p>Echo Specification</p>
    <p>Clarkson: Quantification of Integrity 50</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>output := inputSender</p>
    <p>Sender ReceiverReceivertrusted</p>
    <p>TinTin TinTin</p>
    <p>TinTin TimplTimpl</p>
    <p>UinUin</p>
  </div>
  <div class="page">
    <p>Echo Specification</p>
    <p>Clarkson: Quantification of Integrity 51</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>output := input</p>
    <p>output := inputSender</p>
    <p>Sender ReceiverReceivertrusted</p>
    <p>TinTin TinTin</p>
    <p>TinTin ToutTout</p>
    <p>UinUin</p>
  </div>
  <div class="page">
    <p>Echo Specification</p>
    <p>Clarkson: Quantification of Integrity 52</p>
    <p>Implementati on</p>
    <p>Implementati on</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>output := input</p>
    <p>output := inputSender</p>
    <p>Sender ReceiverReceivertrusted</p>
    <p>TinTin TinTin</p>
    <p>TinTin ToutTout</p>
    <p>UinUin</p>
    <p>Simplifies to information-theoretic model of channels, with attacker</p>
  </div>
  <div class="page">
    <p>Channel Suppression</p>
    <p>Clarkson: Quantification of Integrity 53</p>
    <p>Channel transmission = I(Tin,Tout)Channel suppression = H(Tin | Tout)</p>
    <p>(Tout depends on Uin)</p>
    <p>ChannelChannel</p>
    <p>SenderSender</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>ReceiverReceiver</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>TinTin ToutTout</p>
    <p>UinUin</p>
  </div>
  <div class="page">
    <p>Probabilistic Specifications</p>
    <p>Correct output: distributions on outputs</p>
    <p>Correct output distribution: distribution on distribution on outputs</p>
    <p>Continuous distributions, differential entropy?</p>
    <p>Clarkson: Quantification of Integrity 54</p>
    <p>o := rnd(1)o := rnd(1)</p>
  </div>
  <div class="page">
    <p>Duality</p>
    <p>Clarkson: Quantification of Integrity 55</p>
  </div>
  <div class="page">
    <p>Contamination vs. Leakage</p>
    <p>Clarkson: Quantification of Integrity 56</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>secret</p>
    <p>public</p>
    <p>ProgramProgram</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>UserUser</p>
    <p>Attacke r</p>
    <p>Attacke r</p>
    <p>trusted</p>
    <p>untrusted</p>
    <p>Contamination = I(Uin,Tout | Tin)</p>
    <p>Leakage= I(Sin,Pout | Pin)</p>
  </div>
  <div class="page">
    <p>Contamination vs. Leakage</p>
    <p>Clarkson: Quantification of Integrity 57</p>
    <p>Contamination = I(Uin,Tout | Tin)</p>
    <p>Leakage= I(Sin,Pout | Pin) [Denning 1982, Millen 1987, Gray 1991, Lowe 2002, Clark et al. 2005,</p>
  </div>
  <div class="page">
    <p>Contamination vs. Leakage</p>
    <p>Clarkson: Quantification of Integrity 58</p>
    <p>Contamination = I(Uin,Tout | Tin)</p>
    <p>Leakage= I(Sin,Pout | Pin) [Denning 1982, Millen 1987, Gray 1991, Lowe 2002, Clark et al. 2005,</p>
    <p>Contamination is dual to leakage</p>
  </div>
  <div class="page">
    <p>Confidentiality Dual to Suppression?</p>
    <p>Clarkson: Quantification of Integrity 59</p>
    <p>No.</p>
  </div>
  <div class="page">
    <p>L-diversity</p>
    <p>Every individuals sensitive information should appear to have L (roughly) equally likely values. [Machanavajjhala et al. 2007]</p>
    <p>Entropy L-diversity: H(anon. block)  log L [hrn and Ohno-Machado 1999, Machanavajjhala et al. 2007]</p>
    <p>H(Tin | tout)  log L (if Tin uniform)</p>
    <p>implies suppression  log L Clarkson: Quantification of Integrity 60</p>
  </div>
  <div class="page">
    <p>To Measure is to Know</p>
    <p>When you can measure what you are speaking aboutyou know something about it;</p>
    <p>but when you cannot measure ityour knowledge ismeager and unsatisfactory You have scarcely advanced to the state of Science.</p>
    <p>Lord Kelvin</p>
    <p>Clarkson: Quantification of Integrity 61</p>
  </div>
</Presentation>
