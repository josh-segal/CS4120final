<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Yu Gan, Meghna Pancholi, Dailun Cheng, Siyuan Hu, Yuan He and Christina Delimitrou</p>
    <p>Cornell University</p>
    <p>HotCloud July 9th 2018</p>
    <p>Seer: Leveraging Big Data to Navigate The Increasing Complexity of Cloud Debugging</p>
  </div>
  <div class="page">
    <p>Microservices puts more pressure on performance predictability  Microservices dependencies  propagate &amp; amplify QoS violations  Finding the culprit of a QoS violation is difficult  Post-QoS violation, returning to nominal operation is hard</p>
    <p>Anticipating QoS violations &amp; identifying culprits</p>
    <p>Seer: Data-driven Performance Debugging for Microservices  Combines lightweight RPC-level distributed tracing with hardware</p>
    <p>monitoring  Leverages scalable deep learning to signal QoS violations with</p>
    <p>enough slack to apply corrective action</p>
    <p>Executive Summary</p>
  </div>
  <div class="page">
    <p>From Monoliths to Microservices</p>
  </div>
  <div class="page">
    <p>Advantages of microservices:  Ease &amp; speed of code development &amp; deployment  Security, error isolation  PL/framework heterogeneity</p>
    <p>Challenges of microservices:  Change server design assumptions  Complicate resource management  dependencies  Amplify tail-at-scale effects  More sensitive to performance unpredictability  No representative end-to-end apps with microservices</p>
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>4 end-to-end applications using popular open-source microservices  ~30-40 microservices per app  Social Network  Movie Reviewing/Renting/Streaming  E-commerce  Drone control service</p>
    <p>Programming languages and frameworks:  node.js, Python, C/C++, Java/Javascript, Scala, PHP, and Go  Nginx, memcached, MongoDB, CockroachDB, Mahout, Xapian  Apache Thrift RPC, RESTful APIs  Docker containers  Lightweight RPC-level distributed tracing</p>
    <p>An End-to-End Suite for Cloud &amp; IoT Microservices</p>
  </div>
  <div class="page">
    <p>Resource Management Implications</p>
    <p>Challenges of microservices:  Dependencies complicate resource management  Dependencies change over time  difficult for users to express  Amplify tail@scale effects</p>
    <p>Netflix Twitter Amazon Movie Streaming</p>
  </div>
  <div class="page">
    <p>Detecting QoS violations after they occur:  Unpredictable performance propagates through system  Long time until return to nominal operation  Does not scale</p>
    <p>The Need for Proactive Performance Debugging</p>
  </div>
  <div class="page">
    <p>Performance Implications CPU Mem Net DiskQueue</p>
  </div>
  <div class="page">
    <p>Performance Implications CPU Mem Net DiskQueue</p>
  </div>
  <div class="page">
    <p>Leverage the massive amount of traces collected over time</p>
    <p>Need to predict 100s of msec  a few sec in the future</p>
    <p>Seer: Data-Driven Performance Debugging</p>
  </div>
  <div class="page">
    <p>RPC level tracing  Based on Apache Thrift</p>
    <p>Timestamp start-end for each microservice</p>
    <p>Store in centralized DB (Cassandra)</p>
    <p>Record all requests  No sampling</p>
    <p>Overhead: &lt;0.1% in throughput and &lt;0.2% in tail latency</p>
    <p>Tracing Collector</p>
    <p>WebUI</p>
    <p>Client</p>
    <p>http</p>
    <p>Cassandra</p>
    <p>QueryEngine</p>
    <p>[]</p>
    <p>m ic</p>
    <p>ro se</p>
    <p>rv ic</p>
    <p>es</p>
    <p>latency</p>
    <p>Gantt charts</p>
    <p>zTracer</p>
    <p>TCP</p>
    <p>TCP</p>
    <p>Proc</p>
    <p>uService K RPC timeTX</p>
    <p>zTracer</p>
    <p>TCP</p>
    <p>TCP</p>
    <p>Proc</p>
    <p>uService K+1</p>
    <p>RPC timeRX</p>
    <p>TCP procTX</p>
    <p>TCP procRX App proc</p>
    <p>[]</p>
    <p>Tracing Framework</p>
  </div>
  <div class="page">
    <p>Why?  Architecture-agnostic  Adjusts to changes in</p>
    <p>dependencies over time</p>
    <p>High accuracy, good scalability</p>
    <p>Inference within the required window</p>
    <p>Deep Learning to the Rescue</p>
  </div>
  <div class="page">
    <p>Container utilization</p>
    <p>Latency</p>
    <p>Queue depth</p>
    <p>DNN Configuration</p>
    <p>Output signal</p>
    <p>Which microservice will cause a</p>
    <p>QoS violation in the near</p>
    <p>future?</p>
    <p>Input signal</p>
  </div>
  <div class="page">
    <p>Container utilization</p>
    <p>Latency</p>
    <p>Queue depth</p>
    <p>DNN Configuration</p>
    <p>Output signal</p>
    <p>Which microservice will cause a</p>
    <p>QoS violation in the near</p>
    <p>future?</p>
    <p>Input signal</p>
  </div>
  <div class="page">
    <p>Training once: slow (hours - days)  Across load levels, load distributions, request types  Distributed queue traces, annotated with QoS violations  Weight/bias inference with SGD  Retraining in the background</p>
    <p>Inference continuously: streaming trace data</p>
    <p>DNN Configuration</p>
  </div>
  <div class="page">
    <p>Challenges:  In large clusters inference too slow to prevent QoS violations  Offload on TPUs, 10-100x improvement; 10ms for 90th %ile</p>
    <p>inference  Fast enough for most corrective actions to take effect (net bw</p>
    <p>partitioning, RAPL, cache partitioning, scale-up/out, etc.)</p>
    <p>DNN Configuration</p>
    <p>Accuracy stable or increasing with cluster size</p>
  </div>
  <div class="page">
    <p>40 dedicated servers  ~1000 single-concerned</p>
    <p>containers  Machine utilization 80-85%</p>
    <p>Inject interference to cause QoS violation  Using microbenchmarks</p>
    <p>(CPU, cache, memory, network, disk I/O)</p>
    <p>Experimental Setup</p>
  </div>
  <div class="page">
    <p>Identify cause of QoS violation  Private cluster: performance counters &amp; utilization monitors  Public cluster: contentious microbenchmarks</p>
    <p>Adjust resource allocation  RAPL (fine-grain DVFS) &amp; scale-up for CPU contention  Cache partitioning (CAT) for cache contention  Memory capacity partitioning for memory contention  Network bandwidth partitioning (HTB) for net contention  Storage bandwidth partitioning for I/O contention</p>
    <p>Restoring QoS</p>
  </div>
  <div class="page">
    <p>Post-detection, baseline system  dropped requests</p>
    <p>Post-detection, Seer  maintain nominal performance</p>
    <p>Restoring QoS</p>
  </div>
  <div class="page">
    <p>Demo CPU Mem Net DiskQueue</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Security implications of data-driven approaches</p>
    <p>Fall-back mechanisms when ML goes wrong</p>
    <p>Not a single-layer solution  Predictability needs vertical approaches</p>
    <p>Challenges Ahead</p>
    <p>Thank you!</p>
    <p>Serverless microservices IoT swarms</p>
  </div>
</Presentation>
