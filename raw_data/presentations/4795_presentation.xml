<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Active Feedback: UIUC TREC 2003 HARD Track</p>
    <p>Experiments</p>
    <p>Xuehua Shen, ChengXiang Zhai</p>
    <p>Department of Computer Science</p>
    <p>University of Illinois, UrbanaChampaign</p>
  </div>
  <div class="page">
    <p>Goal of Participation</p>
    <p>Our general goal is to test and extend language modeling</p>
    <p>approaches for a variety of different tasks</p>
    <p>Language Modeling Retrieval Methods</p>
    <p>HARD</p>
    <p>Active feedback</p>
    <p>Robust Genomics Web</p>
    <p>Robust feedback</p>
    <p>Semistructured</p>
    <p>query model</p>
    <p>Relevance propagation</p>
    <p>model</p>
    <p>this talk notebook papers</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Active Feedback Three Methods HARD Track Experiment Design Results Conclusions &amp; Future Work</p>
  </div>
  <div class="page">
    <p>What is Active Feedback?</p>
    <p>An IR system actively selects documents for obtaining relevance judgments</p>
    <p>If a user is willing to judge k documents,</p>
    <p>which k documents should we present</p>
    <p>in order to maximize learning effectiveness?</p>
    <p>Aim at minimizing a users effort</p>
  </div>
  <div class="page">
    <p>Normal Relevance Feedback</p>
    <p>Feedback</p>
    <p>Judgments: d1 + d2  dk</p>
    <p>Query Retrieval Engine</p>
    <p>Top K Results</p>
    <p>d1 3.5 d2 2.4  dk 0.5</p>
    <p>User</p>
    <p>Document collection</p>
  </div>
  <div class="page">
    <p>Active Feedback</p>
    <p>Feedback</p>
    <p>Judgments: d1 + d2  dk</p>
    <p>Query Retrieval Engine</p>
    <p>Which k docs</p>
    <p>to present ?</p>
    <p>User</p>
    <p>Document collection</p>
    <p>Can we do better than just presenting top-K? (Consider redundancy)</p>
  </div>
  <div class="page">
    <p>Active Feedback Methods</p>
    <p>Top-K (normal feedback)</p>
    <p>Gapped Top-K</p>
    <p>K-cluster centroid</p>
    <p>Aiming at high diversity</p>
  </div>
  <div class="page">
    <p>Evaluating Active Feedback in HARD Track</p>
    <p>Query Select 6 passages</p>
    <p>Clarification form</p>
    <p>User</p>
    <p>+</p>
    <p>Completed form</p>
    <p>+ +</p>
    <p>+</p>
    <p>Initial ResultsNo feedback</p>
    <p>(Top-k, gapped, clustering)</p>
    <p>Feedback Feedback Results</p>
    <p>(doc-based, passage-based)</p>
  </div>
  <div class="page">
    <p>Retrieval Methods (Lemur toolkit)</p>
    <p>Query Q</p>
    <p>DDocument D</p>
    <p>Q</p>
    <p>)||( DQD  Results</p>
    <p>Kullback-Leibler Divergence Scoring</p>
    <p>Feedback Docs F={d1, , dn}</p>
    <p>Active Feedback</p>
    <p>Default parameter settings</p>
    <p>FQQ   )1(' F</p>
    <p>Mixture Model Feedback</p>
    <p>Only learn from relevant docs</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Top-k is always worse than gapped top-k and the clustering method</p>
    <p>Clustering generates fewer, but higher quality examples</p>
    <p>Passage-based query model updating performs better than document-based updating</p>
  </div>
  <div class="page">
    <p>Comparison of Three Active Feedback Methods</p>
    <p>Collection Active FB Method</p>
    <p>#Rel</p>
    <p>Include judged docs Exclude judged docs</p>
    <p>MAP Pr@20doc MAP Pr@20doc</p>
    <p>TREC2003</p>
    <p>(Official)</p>
    <p>Top-K 146 0.325 0.498 0.302 0.470</p>
    <p>Gapped 150 0.328 0.504 0.311 0.477</p>
    <p>Clustering 105 0.330* 0.514* 0.326* 0.503*</p>
    <p>AP88-89</p>
    <p>Top-K 198 0.230 0.327 0.193 0.300</p>
    <p>Gapped 180 0.234* 0.342* 0.214 0.322</p>
    <p>Clustering 118 0.232 0.341 0.221* 0.328*</p>
    <p>Top-K is the worst!Clustering uses fewest relevant docs</p>
    <p>bold font = worst * = best</p>
  </div>
  <div class="page">
    <p>Appropriate Evaluation of Active Feedback</p>
    <p>New DB Original DB</p>
    <p>with judged docs</p>
    <p>+ +</p>
    <p>Original DB without judged docs</p>
    <p>+ +</p>
    <p>Cant tell if the ranking of un-judged documents</p>
    <p>is improved</p>
    <p>Different methods have different test documents</p>
    <p>See the learning effect more explicitly</p>
    <p>But the docs must be similar to original docs</p>
  </div>
  <div class="page">
    <p>Comparison of Different Test Data</p>
    <p>(Learning on AP88-89)</p>
    <p>Test Data Active FB Method</p>
    <p>#Rel MAP Pr@20doc</p>
    <p>AP88-89</p>
    <p>Including</p>
    <p>judged docs</p>
    <p>Top-K 198 0.230 0.327</p>
    <p>Gapped 180 0.234* 0.342*</p>
    <p>Clustering 118 0.232 0.341</p>
    <p>AP88-89</p>
    <p>Excluding</p>
    <p>judged docs</p>
    <p>Top-K 198 0.193 0.300</p>
    <p>Gapped 180 0.214 0.322</p>
    <p>Clustering 118 0.221* 0.328*</p>
    <p>AP90 Top-K 198 0.220 0.278</p>
    <p>Gapped 180 0.222 0.283*</p>
    <p>Clustering 118 0.223* 0.282</p>
    <p>Top-K is consistently the worst!</p>
    <p>Clustering generates fewer, but higher quality examples</p>
  </div>
  <div class="page">
    <p>Effectiveness of Query Model Updating:</p>
    <p>Doc-based vs. Passage-based</p>
    <p>Judgments Updating Method</p>
    <p>MAP Pr@20doc</p>
    <p>None Baseline (no updating)</p>
    <p>Gapped Doc-based 0.332 0.503</p>
    <p>Passage-based 0.351 0.517</p>
    <p>Improvement + 5.7% +2.7%</p>
    <p>Clustering Doc-based 0.329 0.502</p>
    <p>Passage-based 0.347 0.522</p>
    <p>Improvement +5.4% +4.0%</p>
    <p>Mixture model query updating methods are effective</p>
    <p>Passage-based is consistently better than doc-based</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Introduced the active feedback problem Proposed and tested three methods for</p>
    <p>active feedback (top-k, gapped top-k, clustering)</p>
    <p>Studied the issue of evaluating active feedback methods</p>
    <p>Results show that  Presenting the top-k is not the best strategy</p>
    <p>Clustering can generate fewer, higher quality feedback examples</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Explore other methods for active feedback (e.g,. negative feedback, MMR method)</p>
    <p>Develop a general framework that  Combines all the utility factors (e.g., being</p>
    <p>informative and best for learning)</p>
    <p>Can model different questions (e.g., model both term selection and relevance judgments)</p>
    <p>Further study how to evaluate active feedback methods</p>
  </div>
</Presentation>
