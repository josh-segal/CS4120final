<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Efficient Weight Learning for Markov Logic Networks</p>
    <p>Daniel Lowd University of Washington</p>
    <p>(Joint work with Pedro Domingos)</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background  Algorithms</p>
    <p>Gradient descent  Newtons method  Conjugate gradient</p>
    <p>Experiments  Cora  entity resolution  WebKB  collective classification</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Markov Logic Networks</p>
    <p>Statistical Relational Learning: combining probability with first-order logic</p>
    <p>Markov Logic Network (MLN) = weighted set of first-order formulas</p>
    <p>Applications: link prediction [Richardson &amp; Domingos, 2006], entity resolution [Singla &amp; Domingos, 2006], information extraction [Poon &amp; Domingos, 2007], and more</p>
    <p>i iinwxXP Z exp)( 1</p>
  </div>
  <div class="page">
    <p>Example: WebKB</p>
    <p>Collective classification of university web pages: Has(page, homework)  Class(page,Course)</p>
    <p>Has(page, sabbatical)  Class(page,Student)</p>
    <p>Class(page1,Student)  LinksTo(page1,page2)  Class(page2,Professor)</p>
  </div>
  <div class="page">
    <p>Example: WebKB</p>
    <p>Collective classification of university web pages: Has(page,+word)  Class(page,+class)</p>
    <p>Has(page,+word)  Class(page,+class)</p>
    <p>Class(page1,+class1)  LinksTo(page1,page2)  Class(page2,+class2)</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Discriminative weight learning in MLNs is a convex optimization problem.</p>
    <p>Problem: It can be prohibitively slow.</p>
    <p>Solution: Second-order optimization methods</p>
    <p>Problem: Line search and function evaluations are intractable.</p>
    <p>Solution: This talk!</p>
  </div>
  <div class="page">
    <p>Sneak preview</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background  Algorithms</p>
    <p>Gradient descent  Newtons method  Conjugate gradient</p>
    <p>Experiments  Cora  entity resolution  WebKB  collective classification</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Gradient descent</p>
    <p>Move in direction of steepest descent, scaled by learning rate:</p>
    <p>wt+1 = wt +  gt</p>
  </div>
  <div class="page">
    <p>Gradient descent in MLNs  Gradient of conditional log likelihood is:</p>
    <p>P(Y=y|X=x)/ wi = ni - E[ni]  Problem: Computing expected counts is hard  Solution: Voted perceptron [Collins, 2002; Singla &amp; Domingos, 2005]</p>
    <p>Approximate counts use MAP state  MAP state approximated using MaxWalkSAT  The only algorithm ever used for MLN discriminative learning</p>
    <p>Solution: Contrastive divergence [Hinton, 2002]  Approximate counts from a few MCMC samples  MC-SAT gives less correlated samples [Poon &amp; Domingos, 2006]  Never before applied to Markov logic</p>
  </div>
  <div class="page">
    <p>Per-weight learning rates</p>
    <p>Some clauses have vastly more groundings than others  Smokes(X)  Cancer(X)  Friends(A,B)  Friends(B,C)  Friends(A,C)</p>
    <p>Need different learning rate in each dimension  Impractical to tune rate to each weight by hand  Learning rate in each dimension is:</p>
    <p>/(# of true clause groundings)</p>
  </div>
  <div class="page">
    <p>Ill-Conditioning</p>
    <p>Skewed surface  slow convergence  Condition number: (max/min) of Hessian</p>
  </div>
  <div class="page">
    <p>The Hessian matrix</p>
    <p>Hessian matrix: all second-derivatives  In an MLN, the Hessian is the negative</p>
    <p>covariance matrix of clause counts  Diagonal entries are clause variances  Off-diagonal entries show correlations</p>
    <p>Shows local curvature of the error function</p>
  </div>
  <div class="page">
    <p>Newtons method</p>
    <p>Weight update: w = w + H-1 g  We can converge in one step if error surface is</p>
    <p>quadratic  Requires inverting the Hessian matrix</p>
  </div>
  <div class="page">
    <p>Diagonalized Newtons method</p>
    <p>Weight update: w = w + D-1 g  We can converge in one step if error surface is</p>
    <p>quadratic AND the features are uncorrelated  (May need to determine step length)</p>
  </div>
  <div class="page">
    <p>Conjugate gradient</p>
    <p>Include previous direction in new search direction</p>
    <p>Avoid undoing any work  If quadratic, finds n optimal weights in n steps  Depends heavily on line searches</p>
    <p>Finds optimum along search direction by function evals.</p>
  </div>
  <div class="page">
    <p>Scaled conjugate gradient</p>
    <p>Include previous direction in new search direction</p>
    <p>Avoid undoing any work  If quadratic, finds n optimal weights in n steps  Uses Hessian matrix in place of line search  Still cannot store entire Hessian matrix in memory</p>
    <p>[Mller, 1993]</p>
  </div>
  <div class="page">
    <p>Step sizes and trust regions</p>
    <p>Choose the step length  Compute optimal quadratic step length: gTd/dTHd  Limit step size to trust region  Key idea: within trust region, quadratic approximation is good</p>
    <p>Updating trust region  Check quality of approximation</p>
    <p>(predicted and actual change in function value)  If good, grow trust region; if bad, shrink trust region</p>
    <p>Modifications for MLNs  Fast computation of quadratic forms:</p>
    <p>Use a lower bound on the function change:</p>
    <p>])[( E- ])[(E Hdd 22T iiiwiiiw ndnd</p>
    <p>[Mller, 1993; Nocedal &amp; Wright, 2007]</p>
    <p>)()()( 11   tt T ttt wwgwfwf</p>
  </div>
  <div class="page">
    <p>Preconditioning</p>
    <p>Initial direction of SCG is the gradient  Very bad for ill-conditioned problems</p>
    <p>Well-known fix: preconditioning  Multiply by matrix to lower condition number  Ideally, approximate inverse Hessian</p>
    <p>Standard preconditioner: D-1</p>
    <p>[Sha &amp; Pereira, 2003]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background  Algorithms</p>
    <p>Gradient descent  Newtons method  Conjugate gradient</p>
    <p>Experiments  Cora  entity resolution  WebKB  collective classification</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Experiments: Algorithms</p>
    <p>Voted perceptron (VP, VP-PW)  Contrastive divergence (CD, CD-PW)  Diagonal Newton (DN)  Scaled conjugate gradient (SCG, PSCG)</p>
    <p>Baseline: VP New algorithms: VP-PW, CD, CD-PW, DN, SCG, PSCG</p>
  </div>
  <div class="page">
    <p>Experiments: Datasets</p>
    <p>Cora  Task: Deduplicate 1295 citations to 132 papers  Weights: 6141 [Singla &amp; Domingos, 2006]  Ground clauses: &gt; 3 million  Condition number: &gt; 600,000</p>
    <p>WebKB [Craven &amp; Slattery, 2001]  Task: Predict categories of 4165 web pages  Weights: 10,891  Ground clauses: &gt; 300,000  Condition number: ~7000</p>
  </div>
  <div class="page">
    <p>Experiments: Method</p>
    <p>Gaussian prior on each weight  Tuned learning rates on held-out data  Trained for 10 hours  Evaluated on test data</p>
    <p>AUC: Area under precision-recall curve  CLL: Average conditional log-likelihood of all</p>
    <p>query predicates</p>
  </div>
  <div class="page">
    <p>Results: Cora AUC</p>
  </div>
  <div class="page">
    <p>Results: Cora AUC</p>
  </div>
  <div class="page">
    <p>Results: Cora AUC</p>
  </div>
  <div class="page">
    <p>Results: Cora AUC</p>
  </div>
  <div class="page">
    <p>Results: Cora CLL</p>
  </div>
  <div class="page">
    <p>Results: Cora CLL</p>
  </div>
  <div class="page">
    <p>Results: Cora CLL</p>
  </div>
  <div class="page">
    <p>Results: Cora CLL</p>
  </div>
  <div class="page">
    <p>Results: WebKB AUC</p>
  </div>
  <div class="page">
    <p>Results: WebKB AUC</p>
  </div>
  <div class="page">
    <p>Results: WebKB AUC</p>
  </div>
  <div class="page">
    <p>Results: WebKB CLL</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Ill-conditioning is a real problem in statistical relational learning</p>
    <p>PSCG and DN are an effective solution  Efficiently converge to good models  No learning rate to tune  Orders of magnitude faster than VP</p>
    <p>Details remaining  Detecting convergence  Preventing overfitting  Approximate inference</p>
    <p>Try it out in Alchemy: http://alchemy.cs.washington.edu/</p>
  </div>
</Presentation>
