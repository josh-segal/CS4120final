<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Now Do Voters Notice Review Screen Anomalies?</p>
    <p>A Look at Voting System Usability</p>
    <p>Bryan A. Campbell Michael D. Byrne Department of Psychology Rice University Houston, TX bryan.campbell@rice.edu byrne@acm.org http://chil.rice.edu/</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>Background</p>
    <p>Usability and security  Previous research on review screen anomaly detection</p>
    <p>Methods</p>
    <p>New experiment on anomaly detection Results</p>
    <p>Improved detection  Replication of some previous findings  New findings</p>
    <p>Discussion</p>
  </div>
  <div class="page">
    <p>Usability and Security</p>
    <p>Consider the amount of time and energy spent on voting system security, for example:</p>
    <p>Californias Top-to-Bottom review  Ohios EVEREST review  Many other papers past and present EVT/WOTE</p>
    <p>This despite a lack of conclusive evidence that any major U.S. election has been stolen due to security flaws in DREs</p>
    <p>Though of course this could have happened But we know major U.S. elections have turned on voting system usability</p>
  </div>
  <div class="page">
    <p>http://www2.indystar.com/library/factfiles/gov/politics/election2000/img/prezrace/butterfly_large.jpg</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Usability and Security</p>
    <p>There are numerous other examples of this</p>
    <p>See the 2008 Brennan Center report This is not to suggest that usability is more important than security</p>
    <p>Though wed argue that it does deserve equal time, which has not been the case</p>
    <p>Furthermore, usability and security are intertwined</p>
    <p>The voter is the first line of defense against malfunctioning and/or malicious systems</p>
    <p>Voters may be able to detect when things are not as they should be</p>
    <p>The oft-given check the review screen advice</p>
  </div>
  <div class="page">
    <p>Usability and Review Screens</p>
    <p>Other usability findings from our previous work regarding DREs vs. older technologies</p>
    <p>Voters are not more accurate voting with a DRE  Voters are not faster voting with a DRE  However, DREs are vastly preferred to older voting</p>
    <p>technologies</p>
    <p>But do voters actually check the review screen?</p>
    <p>Or rather, how closely do they check?  Assumption has certainly been that voters do</p>
    <p>Everett (2007) research</p>
    <p>Two experiments on review screen anomaly detection using the VoteBox DRE</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Everett (2007)</p>
    <p>First study</p>
    <p>Two or eight entire contests were added or subtracted from the review screen</p>
    <p>Second study</p>
    <p>One, two, or eight changes were made to the review screen  Changes were to an opposing candidate or an undervote</p>
    <p>and appeared on the top or bottom of the ballot</p>
    <p>Results</p>
    <p>First study: 32% noticed the anomalies  Second study: 37% noticed the anomalies</p>
  </div>
  <div class="page">
    <p>Everett (2007)</p>
    <p>Also examined what other variables did and did not influence detection performance</p>
    <p>Affected detection performance:</p>
    <p>Time spent on review screen  Causal direction not clear here</p>
    <p>Whether or not voters were given a list of candidates to vote for</p>
    <p>Those with a list noticed more often</p>
    <p>Did not affect detection performance:</p>
    <p>Number of anomalies  Location on the ballot of anomalies</p>
  </div>
  <div class="page">
    <p>Everett (2007) Limitations</p>
    <p>Participants were never explicitly told to check the review screen.</p>
    <p>Would simple instructions increase noticing rates? The interface did little to aid voters in performing accuracy checks</p>
    <p>Was there too little information on the screen?</p>
  </div>
  <div class="page">
    <p>Current Study: VoteBox Modifications</p>
    <p>Explicit instructions</p>
    <p>Voting instructions, both prior to and on the review screen, explicitly warned voters to check the accuracy of the review screen</p>
    <p>Review screen interface alterations</p>
    <p>Undervotes were highlighted in a bright red-orange color  Party affiliation markers were added to candidate names on</p>
    <p>the review screen.</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Methods: Participants</p>
    <p>Recruited from the greater Houston area via newspaper ads, paid $25 for participation</p>
    <p>Native English speakers 18 years of age or older  Mean age = 43.1 years (SD = 17.9); 60 female, 48 male  Previous voting experience: mean number of national</p>
    <p>elections was 5.8, mean non-national elections was 6.3</p>
    <p>Self-rated computer expertise mean of 6.2 on a 10-point Likert scale</p>
  </div>
  <div class="page">
    <p>Design: Independent Variables</p>
    <p>Number of anomalies</p>
    <p>Either 1, 2, or 8 anomalies were present on the review screen Anomaly type</p>
    <p>Contests were changed to an opposing candidate or to an undervote</p>
    <p>Anomaly location</p>
    <p>Anomalies were present on either the top or bottom half of the ballot</p>
  </div>
  <div class="page">
    <p>Design: Independent Variables</p>
    <p>Information condition</p>
    <p>Undirected: Voter guide, voters told to vote as they wished  Directed: Given list of candidates to vote for, cast a vote in</p>
    <p>every race</p>
    <p>Directed with roll-off: Given a list of candidates to vote for, but instructed to abstain in some races</p>
    <p>Voting system</p>
    <p>Voters voted on the DRE and one other non-DRE system Other system</p>
    <p>Voters voted on either a bubble-style paper, lever machine, or punch card voting system</p>
  </div>
  <div class="page">
    <p>Design: Dependent Variables</p>
    <p>Anomaly detection</p>
    <p>Voters, by self-report, either noticed the anomalies or they did not</p>
    <p>Also, self-report on how carefully the review screen was checked</p>
    <p>Efficiency</p>
    <p>Time taken to complete a ballot Effectiveness</p>
    <p>Error rate Satisfaction</p>
    <p>Subjective SUS scores</p>
  </div>
  <div class="page">
    <p>Design: Error Types</p>
    <p>Wrong choice errors</p>
    <p>Voter selected a different candidate Undervote errors</p>
    <p>Voter failed to make a selection Extra vote errors</p>
    <p>Voter made a selection when s/he should have abstained Overvote errors</p>
    <p>Made multiple selections (DRE and lever prevent this error) Also, voters in the undirected condition could intentionally undervote, though this is not an error</p>
    <p>Raises issue of true error rate vs. residual error rate</p>
  </div>
  <div class="page">
    <p>Results: Anomaly Detection</p>
    <p>95% confidence interval: 40.1% to 59.9%  Clear improvement beyond Everett (2007), but still less than</p>
    <p>ideal</p>
    <p>So, what drove anomaly detection?</p>
    <p>Time spent on review screen (p = .003)  Noticers spent an average of 130 seconds on review screen,</p>
    <p>mean was 40 seconds for non-noticers</p>
    <p>Anomaly type (p = .02)  Undervotes more likely to be noticed than flipped votes (61% vs.</p>
  </div>
  <div class="page">
    <p>Results: Anomaly Detection</p>
    <p>Self-reported care in checking review screen (p = .04)</p>
    <p>Information condition (marginal, p = .10)</p>
    <p>Undirected Directed</p>
    <p>with roll-off Fully</p>
    <p>Directed</p>
    <p>Detection Rate</p>
    <p>Not at all Somewhat Carefully</p>
    <p>Very Carefully</p>
    <p>Detected 0% 4% 47%</p>
    <p>Did Not 6% 24% 19%</p>
    <p>Total 6% 28% 66%</p>
  </div>
  <div class="page">
    <p>Results: Anomaly Detection</p>
    <p>Suggestive, but not statistically significant</p>
    <p>The number of anomalies (p = .10)  Some evidence that 1 anomaly is harder than 2 or 8</p>
    <p>The location of anomalies (p = .10)  Some tendency for up-ballot anomalies to be noticed more</p>
    <p>Non-significant factors</p>
    <p>Age, education, computer experience, news following, personality variables</p>
  </div>
  <div class="page">
    <p>No system was significantly more effective then the others</p>
    <p>Results: Errors (Effectiveness)</p>
    <p>Bubble Lever Punch Card</p>
    <p>M e</p>
    <p>a n</p>
    <p>E rr</p>
    <p>o r</p>
    <p>R a</p>
    <p>te (</p>
    <p>% )</p>
    <p>1</p>
    <p>S E</p>
    <p>M</p>
    <p>Non-DRE Voting Technology</p>
    <p>DRE</p>
    <p>Other</p>
  </div>
  <div class="page">
    <p>Results: Error Types</p>
    <p>Overvote Errors</p>
    <p>Undervote Errors</p>
    <p>Wrong Chioice Errors</p>
    <p>Extra Vote Errors</p>
    <p>M e</p>
    <p>a n</p>
    <p>E rr</p>
    <p>o r</p>
    <p>R a</p>
    <p>te (</p>
    <p>% )</p>
    <p>1</p>
    <p>S E</p>
    <p>M</p>
    <p>Error Type</p>
  </div>
  <div class="page">
    <p>Results: True Errors vs. Residual Vote</p>
    <p>At the aggregate level agreement was moderate</p>
    <p>However, agreement was poor at the level of individuals</p>
    <p>For DREs: r(32) = .30, p = .10</p>
    <p>For others: r(32) = .02, p = .89</p>
    <p>DRE Non-DRE</p>
    <p>M e a n</p>
    <p>R a te</p>
    <p>( %</p>
    <p>)  1</p>
    <p>S E</p>
    <p>M</p>
    <p>Voting Technology</p>
    <p>True Rate</p>
    <p>Residual Rate</p>
  </div>
  <div class="page">
    <p>Results: Efficiency</p>
    <p>The DRE was consistently slower then the non-DRE voting technologies</p>
    <p>Noticing of the anomalies was not a significant factor in overall DRE completion times</p>
    <p>Bubble Lever Punch</p>
    <p>M e</p>
    <p>a n</p>
    <p>b a</p>
    <p>ll o</p>
    <p>t c</p>
    <p>o m</p>
    <p>p le</p>
    <p>ti o</p>
    <p>n t</p>
    <p>im e</p>
    <p>( s</p>
    <p>e c</p>
    <p>)  1</p>
    <p>S E</p>
    <p>M</p>
    <p>Non-DRE Voting Technology</p>
    <p>DRE</p>
    <p>Other</p>
  </div>
  <div class="page">
    <p>Those who did not notice an anomaly preferred the DRE</p>
    <p>Despite no clear performance advantages</p>
    <p>Replicates previous findings</p>
    <p>Results: Satisfaction, Non-noticers</p>
    <p>Bubble Lever Punch Card</p>
    <p>M e</p>
    <p>a n</p>
    <p>S U</p>
    <p>S R</p>
    <p>a ti</p>
    <p>n g</p>
    <p>1</p>
    <p>S E</p>
    <p>M</p>
    <p>Non-DRE Voting Technology</p>
    <p>DRE</p>
    <p>Other</p>
  </div>
  <div class="page">
    <p>Results: Satisfaction, Noticers</p>
    <p>However, if an anomaly was noticed, voter preference was mixed</p>
    <p>Bubble Lever Punch Card</p>
    <p>M e a n</p>
    <p>S U</p>
    <p>S R</p>
    <p>a ti</p>
    <p>n g</p>
    <p>1</p>
    <p>S E</p>
    <p>M</p>
    <p>Non-DRE Voting Technology</p>
    <p>DRE</p>
    <p>Other</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Despite our GUI improvements, only 50% of voters noticed up to 8 anomalies on their DRE review screen</p>
    <p>While this is an improvement over Everett (2007), half of the voters are still not noticing anomalies</p>
    <p>Data suggest that the improvement is mostly in detecting anomalous undervotes (orange highlighting helps!)</p>
    <p>But vote flipping is still largely invisible</p>
    <p>This suggests that simple GUI improvement may not be enough to drastically improve anomaly detection</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>VVPATs</p>
    <p>If voters are not checking review screens, how likely are they to check an external paper record?</p>
    <p>Residual vote rate</p>
    <p>The relationship between the residual vote rate and the true error rate may not be straightforward</p>
    <p>May be dangerous to simply assume correspondence Subjective vs. objective performance</p>
    <p>In general, no strong association between preference and performance</p>
    <p>However, voters who noticed the anomalies were less satisfied with the DRE</p>
  </div>
</Presentation>
