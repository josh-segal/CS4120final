<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BDUOL: Double Updating Online Learning on a Fixed Budget</p>
    <p>Peilin Zhao Steven C.H. Hoi</p>
    <p>Nanyang Technological University, Singapore</p>
    <p>Sep 01, 2012</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Online Learning</p>
    <p>Online Classification The learner firstly receives an instance, then predicts the label, and then receives the true label, finally updates the prediction function.</p>
    <p>Why Online Learning</p>
    <p>In many cases, data arrives sequentially while predictions are required immediately.</p>
    <p>Examples: spam filtering, stock market prediction</p>
    <p>For some large scale problem, it is time consuming to use off-line learning.</p>
    <p>Examples: large scale support vector machine training</p>
  </div>
  <div class="page">
    <p>Kernel-based online learning</p>
    <p>Online Prediction Function Sequentially update a nonlinear kernel-based classfier :</p>
    <p>ft () =</p>
    <p>iSV</p>
    <p>i yi (xi , ) (1)</p>
    <p>Can increase the modeling capacity over linear online learning and usually improve the prediction performance.</p>
    <p>Limitation Potentially unbounded number of support vectors</p>
    <p>Large amount of memory for storing SVs</p>
    <p>high computational cost per iteration</p>
    <p>- not suitable for large scale applications</p>
  </div>
  <div class="page">
    <p>Budget Online Learning</p>
    <p>SV removal Random Budget Perceptron(RBP, Cavallanti et al, Machine Learning 2007): randomly remove support vector</p>
    <p>Forgetron(Dekel et al, NIPS 2005): remove the support vector with least weight</p>
    <p>SV projection(Orabona et al, ICML 2008, JMLR 2009)</p>
    <p>cannot guarantee the exact number of support vectors</p>
    <p>high computational cost of the projection step</p>
  </div>
  <div class="page">
    <p>Problem Setting</p>
    <p>Kernel-based Online Classification</p>
    <p>Receive Example xt  Rd</p>
    <p>Make Prediction yt = sgn(ft1(xt )), the predicted label. Where ft1 H, H is an RKHS endowed with a kernel function  and (, ) : Rd Rd  R, (x, x)  1 for any x  Rd</p>
    <p>Reveal true label yt {1, +1}</p>
    <p>Suffer Loss (ft1(xt), yt ) = max(0, 1  yt ft1(xt )) : RY  R.</p>
  </div>
  <div class="page">
    <p>Double Updating Online Learning (DUOL)</p>
    <p>Prediction</p>
    <p>the t -th step, yt = sgn(ft1(xt )) using:</p>
    <p>ft1() =</p>
    <p>iSt1</p>
    <p>i yi (xi , ),</p>
    <p>where St1 is the index set of the SVs for the (t  1)-th step, and i is the weight of the i -th existing support vector.</p>
  </div>
  <div class="page">
    <p>Double Updating Online Learning (DUOL)</p>
    <p>Motivation</p>
    <p>yt ft1(xt )  , then  (xb, yb), b  St1 s.t. yt yb k(xt, xb)  /|St1|.</p>
    <p>ft (x) = ft1(x) + yt (xt , x) and yb ft1(xb)  /n, then yb ft (xb)  0.</p>
    <p>In order to alleviate this problem, we propose to update the weight for the existing support vector whose classification confidence is significantly affected by the new misclassified example.</p>
    <p>Conflict</p>
    <p>(xt , yt ) conflicts with (xb, yb), b  St1, when the following conditions are satisfied:</p>
    <p>t = 1  yt ft1(xt ) &gt; 0;</p>
    <p>b = 1  yb ft1(xb) &gt; 0;</p>
    <p>yt yb(xt , xb)  min(, yt ya(xt , xa)), a  St1, and a 6= b, where   [0, 1) is a threshold,</p>
  </div>
  <div class="page">
    <p>Double Updating Online Learning (DUOL)</p>
    <p>Double Updating: if conflict exists</p>
    <p>ft () = ft1() + t yt (xt , ) + db yb(xb, ),</p>
    <p>where t and db are computed in the following equations:</p>
    <p>(t , db )=</p>
    <p>(C, C  b) if (kt C + wab(C  b)  t ) &lt; 0 and (kb(C  b) + wab C  b) &lt; 0</p>
    <p>(C, b wab C kb</p>
    <p>) if w 2ab Cwab bkt kb C+kbt</p>
    <p>kb &gt; 0 and</p>
    <p>bwab C kb</p>
    <p>[b, C  b]</p>
    <p>( t wab(Cb )</p>
    <p>kt , C  b) if</p>
    <p>t wab (Cb) kt</p>
    <p>[0, C] and</p>
    <p>b  kb(C  b)  wab t wab(Cb )</p>
    <p>kt &gt; 0</p>
    <p>( kbt wab b kt kbw</p>
    <p>, kt b wabt kt kb w</p>
    <p>) if kbt wab b kt kbw</p>
    <p>[0, C] and kt bwabt kt kbw</p>
    <p>[b, C  b]</p>
    <p>, (2)</p>
    <p>where kt = (xt , xt ), kb = (xb, xb), wab = yt yb(xt , xb) and C &gt; 0;</p>
  </div>
  <div class="page">
    <p>Double Updating Online Learning (DUOL)</p>
    <p>Single Updating: if no conflict exists</p>
    <p>ft () = ft1() + t yt (xt , ), (3)</p>
    <p>where t = min(C, t /k 2 t ).</p>
    <p>Remark: It is not difficult to see that the single update strategy is reduced to the Passive-Aggressive updating strategy (Crammer et al, JMLR 2006).</p>
  </div>
  <div class="page">
    <p>Budget Double Updating Online Learning</p>
    <p>Key Issue</p>
    <p>to ensure |SVt| less than a pre-fixed budget B</p>
    <p>Solution when |SVt1| = B, BDUOL performs:</p>
    <p>ft1  ft1 ft1</p>
    <p>such that the support vector size of the updated ft1 is smaller than B.</p>
  </div>
  <div class="page">
    <p>Algorithm</p>
    <p>Initialize S0 = , f0 = 0; for t = 1, 2, . . . , T do</p>
    <p>Receive a new instance xt ; Predict yt = sign(ft1(xt )); Receive its label yt ; t = max{0, 1  yt ft1(xt )}; if t &gt; 0 then</p>
    <p>if (|St| == B) then ft1 = ft1 ft1; (Budget Maintenance)</p>
    <p>end if t = max{0, 1  yt ft1(xt )}; if t &gt; 0 then</p>
    <p>Regular DUOL update end if</p>
    <p>end if end for Return fT , ST</p>
  </div>
  <div class="page">
    <p>Theoretical Analysis</p>
    <p>Theorem Let (x1, y1), . . . , (xT , yT ) be a sequence of examples, where xt  Rd , yt {1, +1} and (xt , xt )  1 for all t , and assume C  1. Then for any function f in H, the number of prediction mistakes M made by BDUOL on this sequence of examples is bounded by:</p>
    <p>{ 1 2 f 2H + C</p>
    <p>T</p>
    <p>i=1</p>
    <p>(f (xi ), yi ) }  2</p>
    <p>T</p>
    <p>i=1</p>
    <p>DAi  2</p>
    <p>M sd (),</p>
    <p>where   [0, 1).</p>
    <p>To minimize the bound, maximize the following value</p>
    <p>max 1,...,t</p>
    <p>DAt =  t</p>
    <p>i=1</p>
    <p>i +</p>
    <p>t</p>
    <p>i=1</p>
    <p>i yi ft (xi ) 1 2 ft</p>
    <p>. (4)</p>
  </div>
  <div class="page">
    <p>Budget Maintenance Strategies</p>
    <p>BDUOL Algorithm by Removal Strategy</p>
    <p>assume the j -th SV is removed</p>
    <p>ft = j yj (xj , ). (5)</p>
    <p>the optimal solution is to discard the SV which can maximize</p>
    <p>DAt,j = j(1  yj ft (xj )) 1 2 (j )</p>
  </div>
  <div class="page">
    <p>Budget Maintenance Strategies</p>
    <p>BDUOL Algorithm by Projection Strategy (exact)</p>
    <p>the j -th SV for removal will be projected to the space spanned by the rest SVs, formally:</p>
    <p>min i [i ,Ci ],i 6=j</p>
    <p>j yj (xj , )</p>
    <p>i 6=j</p>
    <p>i yi (xi , ) 2 H</p>
    <p>. (7)</p>
    <p>which is essentially a Quadratic Programming (QP) problem.</p>
  </div>
  <div class="page">
    <p>Budget Maintenance Strategies</p>
    <p>BDUOL Algorithm by Projection Strategy (approximate)</p>
    <p>firstly solve the unconstrained optimization problem and then project the solution into the feasible region of the constraints</p>
    <p>setting the gradient with respect to  = [i ] , i 6= j as zero:</p>
    <p>= j yj K 1kj./y, (8)</p>
    <p>where K is the kernel matrix for xi, i 6= j , kj = [(xi , xj)] ,</p>
    <p>i 6= j , ./ is element-wise division and y = [yi ] , i 6= j .</p>
    <p>project each i as follows:</p>
    <p>= [,C](j yj K 1kj./y), (9)</p>
    <p>where [a,b](x) = max(a, min(b, x)) and  = [i ] , i 6= j .</p>
  </div>
  <div class="page">
    <p>Budget Maintenance Strategies</p>
    <p>BDUOL Algorithm by Nearest Neighbor Strategy</p>
    <p>project the removed SV to its nearest neighbor SV, based on the distance in the mapped feature space</p>
    <p>the corresponding solution can be expressed as:</p>
    <p>Nj = [Nj ,CNj ] (j yj (xNj , xNj )</p>
    <p>1(xNj , xj)/yNj ), (10)</p>
    <p>where (xNj , ) is the nearest neighbor of (xj , ).</p>
    <p>As a result, the corresponding ft = j yj (xj, ) Nj yNj (xNj , ).</p>
  </div>
  <div class="page">
    <p>Experimental Datasets</p>
    <p>Table: Details of the datasets in our experiments. Dataset # instances # features</p>
    <p>german 1000 24 MITface 6977 361 mushrooms 8124 112 spambase 4601 57 splice 3175 60 w7a 24692 300</p>
    <p>All the datasets are downloaded from LIBSVM website 1 and UCI machine learning repository 2.</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Compared Algorithms</p>
    <p>RBP&quot;: the Random Budget Perceptron algorithm,</p>
    <p>Forgetron&quot;: the Forgetron algorithm,</p>
    <p>Projectron&quot;: the Projectron algorithm, and</p>
    <p>Projectron++&quot;: the aggressive version of Projectron algorithm.</p>
    <p>The Proposed Algorithms</p>
    <p>BDUOLremo&quot;: the BDUOL algorithm by the removal strategy,</p>
    <p>BDUOLproj &quot;: the BDUOL algorithm by the exact projection strategy,</p>
    <p>BDUOLappr &quot;: the BDUOL algorithm by the approximate projection strategy ,</p>
    <p>BDUOLnear &quot;: the BDUOL algorithm by the nearest neighbor strategy.</p>
  </div>
  <div class="page">
    <p>Parameter Setting</p>
    <p>A Gaussian kernel with kernel width 8</p>
    <p>C is selected with cross validation from 2[10:10]</p>
    <p>is set as 0</p>
    <p>B for different datasets are set as proper fractions of the support vector size of Perceptron</p>
    <p>All the experiments were conducted over 20 random permutations for each dataset.</p>
    <p>The results were reported by averaging over these 20 runs.</p>
  </div>
  <div class="page">
    <p>Budget Size B=500 B=1000 B=2000</p>
    <p>Algorithm Mistake (%) Time (s) Mistakes (%) Time (s) Mistakes (%) Time (s)</p>
    <p>RBP 31.826 % 0.924 0.065 29.220 % 0.550 0.069 27.417 % 0.598 0.059</p>
    <p>Fogetron 32.461 % 0.971 0.077 29.641 % 0.742 0.084 27.424 % 0.644 0.082</p>
    <p>Projectron 29.237 % 0.750 0.238 27.480 % 0.484 0.929 27.750 % 2.134 2.776</p>
    <p>Projectron++ 28.822 % 0.725 0.819 28.693 % 6.781 3.874 27.559 % 1.572 7.277</p>
    <p>BDUOLremo 34.559 % 1.308 0.522 28.989 % 0.927 2.013 25.661 % 0.709 4.541</p>
    <p>BDUOLnear 28.180 % 1.084 0.756 25.607 % 0.748 3.874 24.187 % 0.584 8.570</p>
    <p>BDUOLappr 28.950 % 2.001 5.555 26.843 % 0.948 16.396 24.846 % 0.695 30.186</p>
    <p>BDUOLproj 27.448 % 0.961 8.837 25.307 % 0.793 53.987 23.780 % 0.640 197.199</p>
    <p>Table: Evaluation of several budgeted algorithms on spambase.</p>
    <p>RBP and Forgetron achieve similar performance</p>
    <p>Projectron++ achieves a lower mistake rate than Projectron</p>
    <p>BDUOLremo algorithm achieves comparable or better mistake rate when the budget size is large, while fails to improve otherwise</p>
    <p>BDUOLproj always achieves the lowest mistake rates, while suffers the highest time consumption</p>
    <p>BDUOLnear achieves better trade off between mistake rates and time complexity than BDUOLappr</p>
  </div>
  <div class="page">
    <p>Budget Size B=200 B=400 B=600</p>
    <p>Algorithm Mistake (%) Time (s) Mistakes (%) Time (s) Mistakes (%) Time (s)</p>
    <p>RBP 6.551 % 1.056 0.054 17.841 % 1.539 0.073 3.488 % 0.713 0.049</p>
    <p>Fogetron 11.273 % 1.750 0.085 14.154 % 2.748 0.094 3.895 % 1.141 0.079</p>
    <p>Projectron 4.264 % 0.613 0.095 3.703 % 0.731 0.099 3.207 % 0.473 0.107</p>
    <p>Projectron++ 3.986 % 0.297 0.161 3.557 % 0.174 0.178 3.484 % 0.117 0.197</p>
    <p>BDUOLremo 8.754 % 2.438 0.210 2.547 % 0.865 0.181 0.891 % 0.152 0.162</p>
    <p>BDUOLnear 1.329 % 0.198 0.141 0.710 % 0.095 0.132 0.618 % 0.081 0.131</p>
    <p>BDUOLappr 1.065 % 0.218 0.193 0.667 % 0.092 0.194 0.623 % 0.080 0.248</p>
    <p>BDUOLproj 0.729 % 0.060 0.230 0.604 % 0.080 0.266 0.577 % 0.071 0.290</p>
    <p>Budget Size B=500 B=1000 B=1500</p>
    <p>Algorithm Mistake (%) Time (s) Mistakes (%) Time (s) Mistakes (%) Time (s)</p>
    <p>RBP 4.681 % 0.257 0.291 4.597 % 0.227 0.316 4.434 % 0.108 0.332</p>
    <p>Fogetron 4.697 % 0.102 0.367 4.584 % 0.180 0.390 4.466 % 0.137 0.398</p>
    <p>Projectron 4.680 % 0.290 0.727 4.625 % 0.456 1.174 4.406 % 0.130 1.929</p>
    <p>Projectron++ 3.880 % 0.526 4.366 3.672 % 0.214 7.763 3.666 % 0.152 8.796</p>
    <p>BDUOLremo 3.832 % 0.170 0.995 3.361 % 0.145 1.824 3.175 % 0.133 2.401</p>
    <p>BDUOLnear 3.572 % 0.114 1.241 3.269 % 0.133 3.324 3.183 % 0.087 4.859</p>
    <p>BDUOLappr 4.126 % 0.502 4.553 4.001 % 0.153 6.794 3.775 % 0.121 9.646</p>
    <p>BDUOLproj 3.367 % 0.096 11.306 3.227 % 0.126 18.965 3.254 % 0.332 29.807</p>
    <p>Table: Evaluation on mushrooms and w7a. 22 / 24</p>
  </div>
  <div class="page">
    <p>Conclusion We presented a new framework of budget double updating online learning for kernel-based online learning on a fixed budget</p>
    <p>We theoretically analyzed the bounds of the proposed algorithms.</p>
    <p>We proposed three budget maintenance strategies: removal, projection, and nearest neighbor</p>
    <p>Encouraging results show the proposed algorithms are effective.</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Any question?</p>
  </div>
</Presentation>
