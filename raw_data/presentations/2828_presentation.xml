<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Stardust Divide and Conquer in the Data Center Network</p>
    <p>Golan Schzukin &amp; Gabi Bracha Broadcom</p>
    <p>Noa Zilberman University of Cambridge</p>
    <p>February 2019</p>
  </div>
  <div class="page">
    <p>Network switches</p>
    <p>Switch silicon</p>
    <p>Switch box</p>
    <p>Switch chassis</p>
    <p>Scale: 12.8Tbps, 32400GE 2</p>
  </div>
  <div class="page">
    <p>Network switch systems</p>
    <p>Scale: Petabit / second</p>
  </div>
  <div class="page">
    <p>Data center networks Connecting 10Ks to 100Ks of servers</p>
  </div>
  <div class="page">
    <p>Do data center networks scale?</p>
    <p>Network Fabric Link Bundle</p>
  </div>
  <div class="page">
    <p>Example: Building DC with 100K servers (2500 ToR switches)  Option 1  Link bundle of 1 (L=1):</p>
    <p>6.4Tbps Fabric Switch, 25625G  Requires 2 Tiers #fabric-switches = 1172</p>
    <p>Option 2  Link bundle of 4 (L=4):  6.4Tbps Fabric Switch, 64100G  Requires 3 Tiers #fabric-switches = 1954 (1.66 more)</p>
    <p>Do data center networks scale?</p>
    <p>In a network of  tiers scale is</p>
  </div>
  <div class="page">
    <p>Observation: A link bundle of one enables an optimum build of the network (i.e., less tiers, less switches, )</p>
    <p>Do data center networks scale?</p>
  </div>
  <div class="page">
    <p>Designing new network devices</p>
    <p>A decade ago: Can we implement this feature?</p>
    <p>Today: Is this feature worth implementing, given the design constraints?</p>
  </div>
  <div class="page">
    <p>The resource wall  Network silicon die &gt; 7 Billion transistors (Tomahawk, 2014)</p>
    <p>Limited by:  Power density  Die size  Manufacturing feasibility</p>
  </div>
  <div class="page">
    <p>PKT</p>
    <p>Data center network</p>
  </div>
  <div class="page">
    <p>Switch system</p>
    <p>Line card Fabric card Fabric card Fabric card Line card</p>
    <p>PKT</p>
  </div>
  <div class="page">
    <p>Why waste resources? in n tier network</p>
    <p>O(n(Switching+2I/O+2NIF)+n(Ingress Processing + Egress Processing + Queueing))</p>
    <p>O(n(Switching+2I/O+2NIF)+1(Ingress Processing + Egress Processing + Queueing))</p>
  </div>
  <div class="page">
    <p>Observation: Significant resources can be saved by simplifying the data center network</p>
    <p>Why waste resources?</p>
  </div>
  <div class="page">
    <p>Lets convert to packet rate requirements: 5800 Mpps @ 256B (100GE38.7Mpps)</p>
    <p>But clock rate is only ~1GHz.</p>
    <p>The single-pipeline switch</p>
  </div>
  <div class="page">
    <p>Observation: To support full line rate for all packet sizes, network devices need to process multiple packets each and every clock cycle.</p>
    <p>The age of multi core has reached switching</p>
    <p>The single-pipeline switch</p>
  </div>
  <div class="page">
    <p>The switch pipeline The common depiction:</p>
    <p>PKT</p>
  </div>
  <div class="page">
    <p>PACKET 512B</p>
    <p>Actual Implementation: Throughput = clock frequency x bus width</p>
    <p>Data path Width e.g. 256B</p>
    <p>CLOCK CLOCK CYCLE2 CYCLE1</p>
    <p>The switch pipeline</p>
  </div>
  <div class="page">
    <p>PACKET 257B</p>
    <p>Actual Implementation: Throughput  clock frequency x bus width</p>
    <p>Data path Width e.g. 256B</p>
    <p>The switch pipeline</p>
  </div>
  <div class="page">
    <p>Re qu</p>
    <p>ire d</p>
    <p>Pa ra</p>
    <p>lle lis</p>
    <p>m</p>
    <p>Packet Size [B]</p>
    <p>Lets convert to packet rate requirements: 5800 Mpps @ 256B (100GE38.7Mpps) 19200 Mpps @ 64B (100GE150Mpps) But clock rate is only ~1GHz.</p>
    <p>The single-pipeline switch</p>
    <p>Re qu</p>
    <p>ire d</p>
    <p>Pa ra</p>
    <p>lle lis</p>
    <p>m</p>
    <p>Packet Size [B]</p>
    <p>But if we pack data optimally</p>
  </div>
  <div class="page">
    <p>Observation: To support full line rate for all packet sizes, network devices need to process multiple packets each and every clock cycle.</p>
    <p>Observation: For best switch utilization, use fixed-size data units (cells)</p>
    <p>The age of multi core has reached networking</p>
    <p>The single-pipeline switch</p>
  </div>
  <div class="page">
    <p>A link bundle of one enables an optimum build of the network (i.e. less tiers, less switches, )</p>
    <p>Significant resources can be saved by simplifying the network fabric</p>
    <p>To support full line rate for all packet sizes, network devices need to process multiple packets each and every clock cycle.</p>
    <p>For best switch utilization, use fixed-size data units (cells)</p>
    <p>Observations</p>
  </div>
  <div class="page">
    <p>Introducing Stardust From switch-system to data-center scale</p>
  </div>
  <div class="page">
    <p>Introducing Stardust  Complex edge, simple network fabric</p>
    <p>Fabric Element - Fabric device</p>
    <p>A simple cell switch</p>
    <p>Fabric Adapter  Edge device</p>
    <p>A packet switch</p>
    <p>Quite similar to a ToR</p>
    <p>Chops packets to cells 7th generation</p>
    <p>Widely used in switch-systems</p>
  </div>
  <div class="page">
    <p>A Stardust based network</p>
    <p>No Link Bundles</p>
  </div>
  <div class="page">
    <p>Dynamic cell routing</p>
    <p>Input 1  Output 1</p>
    <p>Input 7  Output 1</p>
    <p>Non-Blocking</p>
    <p>Input 9  Output 7 Input 8  Output 2</p>
  </div>
  <div class="page">
    <p>Reachability table  Need to know only the destination Fabric Adapter</p>
    <p>1M virtual machines  100K end hosts  2500 Fabric Adapters</p>
    <p>Entries indicate reachable through these links</p>
    <p>You can get to Fabric Adapter 1 using links 1,5,8,14,36</p>
    <p>Bitmap of size switch radix</p>
    <p>Automatically constructed and updated</p>
    <p>Using reachability messages 2</p>
  </div>
  <div class="page">
    <p>Buffering and scheduling  Packet buffering at the edge</p>
    <p>Using virtual output queues (VOQ) at the ingress Fabric Adapter</p>
    <p>A distributed scheduled fabric  A Fabric Adapter generates credits (e.g. 4KB) to all non-empty</p>
    <p>associated VOQ</p>
    <p>KB F</p>
    <p>lo w</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>Packet packing</p>
  </div>
  <div class="page">
    <p>Packet packing</p>
    <p>+</p>
    <p>NetFPGA SUME</p>
  </div>
  <div class="page">
    <p>Properties  Protocol and traffic pattern agnosticism</p>
    <p>Improved resilience and self healing</p>
    <p>Less network tiers, better scalability</p>
    <p>Optimal load balancing</p>
    <p>Lossless transmission</p>
    <p>Incast absorption</p>
    <p>Pull fabric and port fairness 30</p>
    <p>Cell switching &amp; packing, dynamic routing, fabric scheduling</p>
    <p>Reachability messages, link bundling, dynamic routing</p>
    <p>Link bundling, reachability messages, dynamic routing</p>
    <p>Dynamic routing, cell switching &amp; packing, fabric scheduling</p>
    <p>Fabric scheduling, dynamic routing, cell switching, reachability messages</p>
    <p>Fabric scheduling, dynamic routing, cell switching, reachability messages</p>
    <p>Fabric scheduling, dynamic routing, cell switching, link bundling</p>
  </div>
  <div class="page">
    <p>Power and cost  entire network  Less network tiers  less devices  Less power &amp; area (cost) per device</p>
    <p>Fabric Element saves 35% of power  Fabric Element saves 33.3% of silicon area</p>
    <p>Save 87% of header processing area  Save 70% of network interface area</p>
  </div>
  <div class="page">
    <p>What about the future?  Scalability of ToR / Fabric Adapter is the bottleneck</p>
    <p>Let us replace the ToR with a Fabric Element</p>
    <p>Let us turn the NIC into a Fabric Adapter  Lighter MAC</p>
    <p>Smaller tables</p>
    <p>Limited VOQs</p>
    <p>Fabric adapters already support DMA</p>
    <p>Po rt</p>
    <p>s</p>
    <p>PCIe</p>
    <p>SoC</p>
    <p>Light MAC DMA</p>
    <p>Engine</p>
    <p>VoQ Reachability Table</p>
  </div>
  <div class="page">
    <p>Stardust - summary From switch-system to data center scale:  Simple network fabric  Push complexity to the edge</p>
    <p>Combines:  Cell switching and Packet packing  Load balancing  Scheduled fabric  Reduced network tiers</p>
    <p>Better performance  Lower power, lower cost</p>
  </div>
  <div class="page">
    <p>Acknowledgements</p>
  </div>
</Presentation>
