<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</p>
    <p>Sameer Bansal Herman Kamper Karen Livescu Adam Lopez Sharon Goldwater</p>
  </div>
  <div class="page">
    <p>Current systems</p>
    <p>?</p>
    <p>Spanish Audio:</p>
    <p>English text:</p>
  </div>
  <div class="page">
    <p>Current systems</p>
    <p>?</p>
    <p>ola mi nombre es hodorSpanish text: Automatic Speech</p>
    <p>Recognition</p>
    <p>Spanish Audio:</p>
    <p>English text:</p>
  </div>
  <div class="page">
    <p>Current systems</p>
    <p>hi my name is hodor</p>
    <p>ola mi nombre es hodor Automatic Speech</p>
    <p>Recognition</p>
    <p>Machine Translation</p>
    <p>Spanish text:</p>
    <p>Spanish Audio:</p>
    <p>English text:</p>
  </div>
  <div class="page">
    <p>~100 languages supported by Google Translate ...</p>
  </div>
  <div class="page">
    <p>Unwritten languages</p>
    <p>not availableMboshi text: Automatic Speech</p>
    <p>Recognition</p>
    <p>~3000 languages with no writing system</p>
    <p>Mboshi:</p>
    <p>Bantu language, Republic of Congo, ~160K speakers</p>
  </div>
  <div class="page">
    <p>Unwritten languages</p>
    <p>Efforts to collect speech and translations using mobile apps</p>
    <p>Aikuma: Bird et al. 2014, LIG-Aikuma: Blachon et al. 2016</p>
    <p>Mboshi:</p>
    <p>paired with French translations (Godard et al. 2018)</p>
    <p>~3000 languages with no writing system</p>
  </div>
  <div class="page">
    <p>Haiti Earthquake, 2010</p>
    <p>Moun kwense nan Sakre K nan Ptoprens</p>
    <p>Survivors sent text messages to helpline</p>
    <p>International rescue teams face language barrier  No automated tools available  Volunteers from global Haitian diaspora help create</p>
    <p>parallel text corpora in short time [Munro 2010]</p>
    <p>People trapped in Sacred Heart Church, PauP</p>
  </div>
  <div class="page">
    <p>Are we better prepared in 2019?</p>
    <p>Moun kwense nan Sakre K nan Ptoprens</p>
    <p>People trapped in Sacred Heart Church, PauP</p>
    <p>Voice messages</p>
  </div>
  <div class="page">
    <p>paired with translations</p>
    <p>(source audio)</p>
    <p>Tens of hours of speech paired with text translations</p>
    <p>No source text available</p>
    <p>Can we build a speech-to-text translation (ST) system?</p>
    <p>given as training data:</p>
  </div>
  <div class="page">
    <p>Neural models ...</p>
    <p>Sequence-to-Sequence Weiss et al. (2017)</p>
    <p>Directly translate speech</p>
    <p>hi my name is hodor</p>
    <p>Spanish Audio:</p>
    <p>English text:</p>
  </div>
  <div class="page">
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish Audio</p>
    <p>telephone speech (unscripted)  realistic noise conditions  multiple speakers and dialects  crowdsourced English text translations</p>
    <p>Spanish speech to English text</p>
    <p>Closer to real-world conditions</p>
  </div>
  <div class="page">
    <p>Good performance if</p>
    <p>trained on 100+ hours</p>
    <p>Spanish speech to English text</p>
    <p>Weiss et al.</p>
    <p>*for comparison text-to-text = 58</p>
  </div>
  <div class="page">
    <p>Poor performance in</p>
    <p>low-resource settings</p>
    <p>But ...</p>
    <p>*for comparison text-to-text = 58</p>
    <p>Weiss et al.</p>
  </div>
  <div class="page">
    <p>Goal: to improve translation performance</p>
  </div>
  <div class="page">
    <p>Goal: to improve translation performance</p>
    <p>without labeling more low-resource speech</p>
  </div>
  <div class="page">
    <p>typically used to train ASR systems</p>
    <p>English text</p>
    <p>(English Audio)</p>
    <p>French text</p>
    <p>(French Audio)</p>
    <p>Key idea: leverage monolingual data from a different high-resource language</p>
  </div>
  <div class="page">
    <p>typically used to train ASR systems</p>
    <p>English text</p>
    <p>(English Audio)</p>
    <p>French text</p>
    <p>(French Audio)</p>
    <p>Sequence-to-Sequence English text</p>
    <p>?</p>
    <p>Spanish Audio</p>
    <p>~20 hours of Spanish-English</p>
  </div>
  <div class="page">
    <p>typically used to train ASR systems</p>
    <p>Spanish text</p>
    <p>(Spanish Audio)</p>
    <p>Weiss et al. 2017 Anastasopoulos and Chiang 2018</p>
    <p>Brard et al. 2018 Sperber et al. 2019</p>
    <p>Sequence-to-Sequence English textSpanish Audio</p>
    <p>~20 hours of Spanish-English</p>
  </div>
  <div class="page">
    <p>typically used to train ASR systems</p>
    <p>English text</p>
    <p>(English Audio)</p>
    <p>French text</p>
    <p>(French Audio)</p>
    <p>Sequence-to-Sequence English text</p>
    <p>?</p>
    <p>Spanish Audio</p>
    <p>~20 hours of Spanish-English</p>
  </div>
  <div class="page">
    <p>Why Spanish-English?</p>
  </div>
  <div class="page">
    <p>Why Spanish-English?</p>
    <p>simulate low-resource settings and test our method</p>
  </div>
  <div class="page">
    <p>Why Spanish-English?</p>
    <p>simulate low-resource settings and test our method</p>
    <p>Later: results on truly low-resource language --Mboshi to French</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Audio</p>
    <p>Same model architecture for ASR and ST</p>
    <p>*randomly initialized parameters</p>
    <p>text</p>
  </div>
  <div class="page">
    <p>Pretrain on high-resource</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>English audio</p>
    <p>English text</p>
    <p>*train until convergence</p>
  </div>
  <div class="page">
    <p>Fine-tune on low-resource</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder transfer from English ASR</p>
    <p>English audio</p>
    <p>English text</p>
    <p>Spanish audio</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Fine-tune on low-resource</p>
    <p>*train until convergence</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish audio</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Will this work?</p>
  </div>
  <div class="page">
    <p>Spanish-English BLEU scores</p>
    <p>baseline</p>
    <p>*for comparison Weiss et al. = 47.3</p>
  </div>
  <div class="page">
    <p>Spanish-English BLEU scores</p>
    <p>baseline</p>
    <p>*for comparison Weiss et al. = 47.3</p>
    <p>pretraining</p>
  </div>
  <div class="page">
    <p>Spanish-English BLEU scores</p>
    <p>baseline</p>
    <p>*for comparison Weiss et al. = 47.3</p>
    <p>pretraining</p>
    <p>+9 BLEU</p>
  </div>
  <div class="page">
    <p>Spanish-English BLEU scores</p>
    <p>baseline</p>
    <p>*for comparison Weiss et al. = 47.3</p>
    <p>pretraining</p>
    <p>better performance with half the data</p>
  </div>
  <div class="page">
    <p>Further analysis</p>
    <p>baseline</p>
    <p>*for comparison Weiss et al. = 47.3</p>
    <p>pretraining</p>
  </div>
  <div class="page">
    <p>Faster training time</p>
    <p>baseline</p>
    <p>pretraining</p>
  </div>
  <div class="page">
    <p>baseline</p>
    <p>pretraining</p>
    <p>Faster training time</p>
    <p>~20 hours</p>
    <p>potentially useful in time critical scenarios</p>
  </div>
  <div class="page">
    <p>Ablation: model parameters</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>English</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Ablation: model parameters</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: decoder 10.5</p>
    <p>English</p>
    <p>English text</p>
    <p>random Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Ablation: model parameters</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: decoder 10.5</p>
    <p>+English ASR: encoder 16.6</p>
    <p>English</p>
    <p>English text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Ablation: model parameters</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: decoder 10.5</p>
    <p>+English ASR: encoder 16.6</p>
    <p>transferring encoder only parameters works well!</p>
    <p>English</p>
    <p>English text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Ablation: model parameters</p>
    <p>EnglishSpanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: decoder 10.5</p>
    <p>+English ASR: encoder 16.6</p>
    <p>can pretrain on a language different from both source and target in ST pair</p>
    <p>English text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on French</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: encoder 16.6</p>
    <p>+French ASR: encoder ?</p>
    <p>*only 20 hours of French ASR</p>
    <p>French</p>
    <p>French text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on French</p>
    <p>French</p>
    <p>French text</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+English ASR 19.9</p>
    <p>+English ASR: encoder 16.6</p>
    <p>+French ASR: encoder 12.5</p>
    <p>random</p>
    <p>French ASR helps Spanish-English ST</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Spanish</p>
    <p>English text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Pretraining on a different language helps  transfer all model parameters for best gains  encoder parameters account for most of these</p>
    <p>useful when target vocabulary is different</p>
  </div>
  <div class="page">
    <p>Mboshi-French ST</p>
  </div>
  <div class="page">
    <p>Mboshi-French ST</p>
    <p>ST data by Godard et al. 2018  ~4 hours of speech, paired with French translations</p>
    <p>Mboshi  Bantu language, Republic of Congo  Unwritten  ~160K speakers</p>
  </div>
  <div class="page">
    <p>Mboshi-French: Results</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline ?</p>
    <p>Mboshi</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French text</p>
  </div>
  <div class="page">
    <p>Mboshi-French: Results</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>*outperformed by a naive baseline</p>
    <p>Mboshi</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French text</p>
  </div>
  <div class="page">
    <p>Pretraining on French ASR</p>
    <p>French MboshiMboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all ?</p>
    <p>transfer all parameters</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on French ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>French ASR helps Mboshi-French ST</p>
    <p>French Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on French ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>French ASR helps Mboshi-French ST</p>
    <p>French Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on English ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>+English ASR: encoder ?</p>
    <p>using encoder trained on a lot more data</p>
    <p>English</p>
    <p>English text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on English ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>+English ASR: encoder 5.3</p>
    <p>English ASR helps Mboshi-French ST</p>
    <p>English</p>
    <p>English text</p>
    <p>random</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
  </div>
  <div class="page">
    <p>Pretraining on French ASR: can transfer all parameters</p>
    <p>but only 20 hours of data</p>
    <p>Pretraining on English ASR: trained on a lot more data (300 hours)</p>
    <p>but can only transfer encoder parameters</p>
  </div>
  <div class="page">
    <p>but only 20 hours of data</p>
    <p>but can only transfer encoder parameters</p>
    <p>combine both? 54</p>
    <p>Pretraining on French ASR: can transfer all parameters</p>
    <p>Pretraining on English ASR: trained on a lot more data (300 hours)</p>
  </div>
  <div class="page">
    <p>Pretraining on French and English ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French</p>
    <p>French text</p>
    <p>English</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Pretraining on French and English ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French</p>
    <p>French text</p>
    <p>English</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Pretraining on French and English ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French</p>
    <p>French text</p>
    <p>English</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Pretraining on English ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>+English ASR: encoder 5.3</p>
    <p>+English ASR: encoder +French ASR: remaining</p>
    <p>?</p>
    <p>From English ASR</p>
    <p>From French ASR</p>
  </div>
  <div class="page">
    <p>Pretraining on English ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>+English ASR: encoder 5.3</p>
    <p>+English ASR: encoder +French ASR: remaining</p>
    <p>combining gives the best gains</p>
    <p>From English ASR</p>
    <p>From French ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
  </div>
  <div class="page">
    <p>Pretraining on English ASR</p>
    <p>Mboshi to French, N = 4 hours</p>
    <p>BLEU</p>
    <p>baseline 3.5</p>
    <p>+French ASR: all 5.9</p>
    <p>+English ASR: encoder 5.3</p>
    <p>+English ASR: encoder +French ASR: remaining</p>
    <p>BLEU score is still low  but above naive baseline</p>
    <p>From English ASR</p>
    <p>From French ASR</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>Mboshi</p>
    <p>French text</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Pretraining on high-resource ASR improves low-resource ST  Potentially useful for endangered and/or unwritten languages  Bootstrap ST in time-critical scenarios  Future work: experiments on more languages, multilingual</p>
    <p>training with joint vocabulary</p>
  </div>
  <div class="page">
    <p>Anonymous reviewers, Edinburgh NLP members  Source code available at: https://github.com/0xSameer/ast</p>
    <p>I am looking for full-time positions starting November 2019!</p>
    <p>Thanks</p>
    <p>4th June, 3:30-5 pm - Fluent Translations from Disfluent Speech in End-to-End Speech Translation, Salesky et al.</p>
    <p>5th June, 10:30-10:48 am - Neural Machine Translation of Text from Non-Native Speakers, Anastasopoulos et al.</p>
  </div>
  <div class="page">
    <p>Backup</p>
  </div>
  <div class="page">
    <p>Mboshi-French naive baseline</p>
  </div>
  <div class="page">
    <p>Speaker invariance  ASR data contains audio from 100s of speakers</p>
    <p>Learning to factor out background noise (?)</p>
    <p>Why does pretraining help?</p>
    <p>BLEU Baseline +English ASR</p>
  </div>
  <div class="page">
    <p>Spanish-English ST</p>
    <p>N hrs 2.5h 5h 10h 20h 50h 160h Weiss</p>
    <p>baseli ne</p>
    <p>+ASR 5.7 9.1 14.5 20.2 28.3 --</p>
    <p>*results on Fisher test set ...</p>
  </div>
  <div class="page">
    <p>Spanish-English ST</p>
    <p>BLEU</p>
    <p>baseline 10.8</p>
    <p>+En ASR: 300h 16.6</p>
    <p>+Fr ASR:20h 12.5</p>
    <p>+En ASR: 20h 13.2</p>
    <p>Spanish to English, N = 20 hours</p>
    <p>Encoder</p>
    <p>Attention</p>
    <p>Decoder</p>
    <p>French ASR helps improve Spanish-English ST 67</p>
    <p>Spanish</p>
    <p>English text</p>
  </div>
  <div class="page">
    <p>Spanish-English ST</p>
  </div>
  <div class="page">
    <p>Neural model</p>
    <p>CNN 1</p>
    <p>MFCCs 150 x 13</p>
    <p>CNN 2</p>
    <p>bi-LSTM 1</p>
    <p>bi-LSTM 2</p>
    <p>bi-LSTM 3</p>
    <p>yo vive en bronx</p>
    <p>Embedding</p>
    <p>FF-Softmax</p>
    <p>LSTM 1</p>
    <p>LSTM 2</p>
    <p>LSTM 3</p>
    <p>i live in br_ _ on_ _ x EOS</p>
    <p>GO i live in br_ _ on_ _ x</p>
    <p>Attention</p>
  </div>
  <div class="page">
    <p>Neural model</p>
    <p>CNN</p>
    <p>RNN Embedding</p>
    <p>FF-Softmax</p>
    <p>RNN</p>
    <p>predicted text</p>
    <p>Attention</p>
    <p>Encoder Decoder</p>
    <p>MFCCs</p>
    <p>prediction history</p>
  </div>
  <div class="page">
    <p>typically used to train ASR systems</p>
    <p>English text</p>
    <p>Glehre et al., 2015 Toshniwal et al., 2018</p>
    <p>Sequence-to-Sequence English textSpanish Audio</p>
    <p>~20 hours of Spanish-English</p>
  </div>
</Presentation>
