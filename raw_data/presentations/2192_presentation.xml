<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BARNS: Backup and Recovery for NoSQL Databases Atish Kathpal, Priya Sehgal Advanced Technology Group, NetApp</p>
    <p>2017 NetApp, Inc. All rights reserved.1</p>
  </div>
  <div class="page">
    <p>Why Backup/Restore NoSQL DBs?</p>
    <p>Customers are directly ingesting into NoSQL</p>
    <p>Security breach are on the rise e.g. ransomware attacks on MongoDB [1] and recent WannaCrypt exploits</p>
    <p>Fat-finger errors eventually propagate to replicas</p>
    <p>Sandbox deployments for test/dev</p>
    <p>Bring up shadow clusters of different cardinality (from production cluster snapshots)</p>
    <p>Compliance and regulatory requirements</p>
    <p>IDC, 2016 report [2] lists data-protection and retention as one of the top infrastructural requirements for NoSQL</p>
    <p>2017 NetApp, Inc. All rights reserved.2 [1] http://thehackernews.com/2017/01/secure-mongodb-database.html [2] Nadkarni A., Polyglot Persistence: Insights on NoSQL Adoption and the Resulting Impact on Infrastructure. IDC. 2016 Feb.</p>
    <p>Ransomware</p>
  </div>
  <div class="page">
    <p>From Backup/Restore Service Perspective NoSQL Database Classes</p>
    <p>Master-slave  Authoritative copy of each partition is contained in the master node</p>
    <p>that we can backup.  Loss of primary node leads to shard/partition-unavailability until new</p>
    <p>leader is elected.  Example: MongoDB, Redis, Oracle NoSQL, MarkLogic</p>
    <p>Master-less  Data is scattered across nodes using consistent hashing techniques,</p>
    <p>no single node has all data for a given partition  Eventual consistency: Unavailability of a destination node does not</p>
    <p>lead to write-failure, data is eventually replicated  Example: Cassandra, Couchbase</p>
    <p>2017 NetApp, Inc. All rights reserved.3</p>
    <p>https://www.slideshare.net/mongodb/sharding-v-final, https://blog.imaginea.com/consistent-hashing-in-cassandra/</p>
  </div>
  <div class="page">
    <p>NoSQL DBs Hosted on Shared Storage High-level, conceptual deployment architecture</p>
    <p>2017 NetApp, Inc. All rights reserved.4</p>
    <p>NoSQL DB software (Node1)</p>
    <p>Filesystem (DB Journal, Logs,</p>
    <p>Data Files)</p>
    <p>NoSQL DB software (Node2)</p>
    <p>Filesystem (DB Journal, Logs,</p>
    <p>Data Files)</p>
    <p>NoSQL DB software (Node3)</p>
    <p>Filesystem (DB Journal, Logs,</p>
    <p>Data Files)</p>
    <p>Shared Storage Array (Snapshots, Cloning, Compression, Deduplication, Encryption, Cloud Integrations)</p>
    <p>LUN-1 LUN-2 LUN-3</p>
    <p>Backup: Leverage storage snapshots Restore: Leverage</p>
    <p>cloning</p>
  </div>
  <div class="page">
    <p>Backup/Restore Challenges  Cluster-consistency at scale</p>
    <p>Cluster/App quiesce  significantly hampers application performance. Cross node consistency not guaranteed  Take crash consistent snapshots  Post process crash consistent snapshots (in a sand-box) using NoSQL DB stack</p>
    <p>to reach an cluster-consistent state</p>
    <p>Space Efficiency  Replica set data copies do not de-duplicate  small row sizes, scattered</p>
    <p>across nodes (Cassandra) and unique ids added by storage engines (MongoDB)</p>
    <p>DB performs compression and encryption  Remove replicas logically (application aware backup)</p>
    <p>Topology Changes  Commodity nodes, at scale of 10-100s of nodes. E.g., Primary node</p>
    <p>might be unreachable while taking backup in case of MongoDB  Storage snapshots do not have context about cluster topology  Use cases may require restore to a test/dev cluster of different cardinality  Save Cluster topology and storage mapping as part of backup</p>
    <p>2017 NetApp, Inc. All rights reserved.5</p>
    <p>Cassandra N1</p>
    <p>Cassandra N2</p>
    <p>Cassandra N3</p>
    <p>Write: (K1, V1), (K3, V3)</p>
    <p>Write: (K1, V1), (K2, V2)</p>
    <p>Write: (K2, V2), (K3, V3)</p>
    <p>Shared Storage cluster LUN1: K1, K3 LUN2: K1, K2 LUN3: K2, K3</p>
    <p>Insignificant block based</p>
    <p>deduplication</p>
    <p>Replication factor = 2</p>
    <p>Existing open source utilities like Mongodump and Cassandra snapshots suffer from above challenges.</p>
  </div>
  <div class="page">
    <p>NoSQL Data Protection  Challenges</p>
    <p>Challenges:</p>
    <p>Fault tolerance  Backup may capture stale data due</p>
    <p>to eventual consistency  Higher restore times, since</p>
    <p>Cassandra will perform repairs during restore</p>
    <p>Master-Less Databases</p>
    <p>2017 NetApp, Inc. All rights reserved.6</p>
    <p>Cassandra N1</p>
    <p>Cassandra N2</p>
    <p>Cassandra N3</p>
    <p>Update: (K1, V11, Tnew)</p>
    <p>Update: (K1, V11, Tnew)</p>
    <p>Shared Storage cluster LUN1: (K1, V1, Told),  LUN2: (K1, V1, Told),  LUN3:</p>
    <p>Client, Update K1 (CL.ONE)</p>
    <p>(K1, V11, Tnew)</p>
    <p>Ack</p>
    <p>Snapshot of LUN2 will point to stale</p>
    <p>data</p>
    <p>http://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsRepairNodesReadRepair.html, https://wiki.apache.org/cassandra/HintedHandoff</p>
  </div>
  <div class="page">
    <p>BARNS Architecture</p>
    <p>2017 NetApp, Inc. All rights reserved.7</p>
    <p>Light Weight Backup Process (LWB)</p>
    <p>PostProcess</p>
    <p>Restore Process</p>
    <p>Lib_DB</p>
    <p>Lib_Storage</p>
    <p>Lib_MongoDB Lib_Cassandra</p>
    <p>Lib_Store1 Lib_Store2</p>
    <p>Store1 Store2</p>
    <p>APIs APIs</p>
    <p>MongoDB Cluster</p>
    <p>Cassandra Cluster</p>
    <p>Mongo APIs Cassandra APIs</p>
    <p>(PP)</p>
    <p>Stop_balancer() Get_topology() Check_cluster_health() Prepare_Backup_Topo() Etc.</p>
    <p>Get_lun_mappings() Take_snapshots() Lun_clone() Vault_backup() Etc.</p>
    <p>Addresses challenges of: 1. Taking cluster-consistent</p>
    <p>backup at scale 2. Taking storage efficient</p>
    <p>backups (through replica removal)</p>
  </div>
  <div class="page">
    <p>Master-less Distributed Database BARNS Solution: Cassandra</p>
    <p>2017 NetApp, Inc. All rights reserved.8</p>
  </div>
  <div class="page">
    <p>Phase 1: Light-weight Backup Phase</p>
    <p>2017 NetApp, Inc. All rights reserved.9</p>
    <p>C1</p>
    <p>CL1 D1</p>
    <p>C4</p>
    <p>CL4 D4</p>
    <p>C3</p>
    <p>CL3 D3</p>
    <p>C2</p>
    <p>CL2 D2</p>
    <p>L1 L2 L3 L4</p>
    <p>BARNS: LWB</p>
    <p>Production Cluster</p>
    <p>CL1</p>
    <p>D1</p>
    <p>sn1</p>
    <p>CL2</p>
    <p>D2</p>
    <p>sn2</p>
    <p>CL3</p>
    <p>D3</p>
    <p>sn3</p>
    <p>CL4</p>
    <p>D4</p>
    <p>sn4</p>
    <p>Token1 Token4Token3Token2</p>
    <p>Backup Metadata example</p>
  </div>
  <div class="page">
    <p>Phase2: Post Process Phase Part1: Flush Commitlogs</p>
    <p>2017 NetApp, Inc. All rights reserved.10</p>
    <p>P1</p>
    <p>CL1 D1</p>
    <p>P4</p>
    <p>CL4 D4</p>
    <p>P3</p>
    <p>CL3 D3</p>
    <p>P2</p>
    <p>CL2 D2</p>
    <p>CL_sn1 CL_sn2 CL_sn3 CL_sn4</p>
    <p>BARNS: PP-Phase</p>
    <p>PP Node(s)</p>
    <p>E.g. Data</p>
    <p>K1, V1, T1</p>
    <p>K2, V2, T1</p>
    <p>K1,V11, T2</p>
  </div>
  <div class="page">
    <p>Phase2: Post Process Phase Part2: Compaction</p>
    <p>2017 NetApp, Inc. All rights reserved.11</p>
    <p>CL_sn1 CL_sn2 CL_sn3 CL_sn4</p>
    <p>Fullbackup LUN</p>
    <p>Data 1</p>
    <p>K1, V1, T1</p>
    <p>K2, V2, T1</p>
    <p>K1,V11, T2</p>
    <p>/cassandra</p>
    <p>Data 2</p>
    <p>K1, V1, T1</p>
    <p>K1, V11, T2</p>
    <p>K3,V3, T3</p>
    <p>Data 3</p>
    <p>K1, V1, T1</p>
    <p>K2, V2, T1</p>
    <p>Data 4</p>
    <p>K3,V3, T3</p>
    <p>K2, V22, T4</p>
    <p>Data Compact</p>
    <p>K1, V11, T2</p>
    <p>K2, V22, T1</p>
    <p>K3,V3, T3</p>
    <p>Full_bk_lun</p>
    <p>unionFS</p>
    <p>PP Node</p>
    <p>Keeping single copy of data in the backup =&gt; ~66% reduction in backup storage requirements</p>
    <p>BARNS: PP-Phase</p>
  </div>
  <div class="page">
    <p>Cassandra Restore The post-process step enables cloning to different restore/clone topologies</p>
    <p>2017 NetApp, Inc. All rights reserved.12</p>
    <p>Full Backup</p>
    <p>K1, V11, T2</p>
    <p>K2, V22, T1</p>
    <p>K3,V3, T3</p>
    <p>Token1 Token2</p>
    <p>N1 N2</p>
    <p>Full_bk_lun</p>
    <p>Clones</p>
    <p>Token3 Token4</p>
  </div>
  <div class="page">
    <p>Evaluation - Cassandra Backup and Restore  Production cluster  4 nodes  4 iSCSI LUNs  Commitlog and SSTables for a</p>
    <p>node on same LUN  Cassandra 4.0</p>
    <p>Post Process Node  2 CPUs  8GB RAM</p>
    <p>YCSB to ingest data</p>
    <p>Full Backup</p>
    <p>2017 NetApp, Inc. All rights reserved.13</p>
    <p>LWB  &lt;10 secs  pp-flush - ~40 secs  pp-compact  time increases by 35-40%  incremental backup  Restore time  less than ~2 mins (irrespective of cluster size and data set size)</p>
  </div>
  <div class="page">
    <p>Master-Slave Distributed Database</p>
    <p>Check the paper or just attend the poster session J</p>
    <p>BARNS Solution: MongoDB</p>
    <p>2017 NetApp, Inc. All rights reserved.14</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Tracking replicas and cluster topologies is important for taking backups and performing flexible topology restores</p>
    <p>Existing open-source solutions have several inefficiencies like need for repairs after restore, lack of storage efficiency in backup and poor integrations with shared storage</p>
    <p>Opportunity to provide efficient backup and restore through light-weight snapshots and clones</p>
    <p>2017 NetApp, Inc. All rights reserved.15</p>
  </div>
  <div class="page">
    <p>2017 NetApp, Inc. All rights reserved.16</p>
    <p>Thank You.</p>
  </div>
</Presentation>
