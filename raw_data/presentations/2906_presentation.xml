<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Experimentation and Analytics Framework for Large-Scale AI Operations Platforms</p>
    <p>IBM Research AI</p>
    <p>Thomas Rausch</p>
    <p>Waldemar Hummer</p>
    <p>Vinod Muthusamy</p>
    <p>OpML20, July 27  August 3, 2020</p>
  </div>
  <div class="page">
    <p>Operationalizing AI</p>
    <p>Process Train Evaluate Deploy</p>
    <p>Perf.</p>
    <p>Model</p>
    <p>Runtime Monitoring</p>
    <p>Data</p>
    <p>new data</p>
    <p>e</p>
    <p>Watson AI OpenScale</p>
    <p>TensorFlow Extended</p>
    <p>Azure ML Pipelines ...</p>
  </div>
  <div class="page">
    <p>Abstract Pipeline View</p>
    <p>Platform View</p>
    <p>Process Train Validate Harden Deploy</p>
    <p>COS</p>
    <p>Compute Cluster</p>
    <p>Learning Cluster</p>
    <p>Read Execute Write ......</p>
    <p>Operationalizing AI: Three Views</p>
  </div>
  <div class="page">
    <p>Operating Automated AI Pipelines</p>
    <p>e Retraining rule/trigger</p>
    <p>if performance &lt; 0.8 and cnt(new_data) &gt; n</p>
    <p>Process Train Evaluate Deploye</p>
    <p>Perf. Model</p>
    <p>Runtime Monitoring</p>
    <p>Data</p>
    <p>every 4 weeks</p>
    <p>Models become stale, automatic retraining will become common  Operators will be challenged to maintain many</p>
    <p>automated continuous training loops  Cost-benefit trade-off between retraining and</p>
    <p>model performance improvement</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Operating Automated AI Pipelines</p>
    <p>e Retraining rule/trigger</p>
    <p>if performance &lt; 0.8 and cnt(new_data) &gt; n</p>
    <p>Process Train Evaluate Deploye</p>
    <p>Perf. Model</p>
    <p>Runtime Monitoring</p>
    <p>Data</p>
    <p>every 4 weeks</p>
    <p>Probabilistic parameters  Expected improvement (EI)  Risk (e.g., human intervention)  Priority (how important is</p>
    <p>this model/tenant)?  Resource availability</p>
    <p>Scheduler / Optimizer</p>
    <p>Infrastructure / Budget</p>
    <p>Fitness?</p>
  </div>
  <div class="page">
    <p>DRAFT</p>
    <p>System model? How to validate? System model?</p>
    <p>How to validate?</p>
  </div>
  <div class="page">
    <p>AI Ops Experimentation and Analytics Framework</p>
  </div>
  <div class="page">
    <p>AI Ops Experimentation &amp; Analytics Framew.  Develop and test operational mechanisms  Use current understanding of platforms to build an</p>
    <p>AI Ops experimentation environment  Model a pipeline execution and AI operations platform  Generate synthetic pipelines and data  Simulate execution of pipelines  Study and analyze system behavior (answer what if  questions)</p>
    <p>Requires good fidelity to be useful, i.e., exceed simple theoretical models, grounded using empirical data</p>
  </div>
  <div class="page">
    <p>Modeled System</p>
    <p>Simulator</p>
    <p>Statistical Analysis</p>
    <p>Experiment Parameters</p>
    <p>System Model</p>
    <p>Exploratory Analysis</p>
    <p>Operational Strategies</p>
    <p>Simulation Parameters</p>
    <p>Real System</p>
    <p>Operational Mechanisms</p>
    <p>Empirical Data (traces)</p>
    <p>OpenScale ModelOps Infrastructure</p>
    <p>emulates</p>
    <p>emulates</p>
    <p>Predictive Models</p>
    <p>revise</p>
    <p>revise</p>
    <p>controls</p>
    <p>creates</p>
  </div>
  <div class="page">
    <p>Developing the System Model</p>
    <p>Modeled System</p>
    <p>System Model</p>
    <p>Operational Strategies</p>
    <p>Simulation Parameters</p>
    <p>Empirical Data (traces)</p>
  </div>
  <div class="page">
    <p>Key Ingredients</p>
    <p>System model for AI ops platforms</p>
    <p>Synthetic data and pipelines</p>
    <p>Process model</p>
  </div>
  <div class="page">
    <p>System model: build time</p>
    <p>Process Train Validate Harden</p>
    <p>Data Store</p>
    <p>Compute Cluster</p>
    <p>Learning Cluster</p>
    <p>Read Data</p>
    <p>Train Model</p>
    <p>Write Model</p>
    <p>Data Asset</p>
    <p>Trained Model</p>
    <p>Pipeline</p>
    <p>Task Task Executor</p>
    <p>Simulation Events</p>
    <p>Compute Resource</p>
    <p>Data Store</p>
    <p>Artifact Execution Trace</p>
    <p>requests</p>
    <p>...</p>
  </div>
  <div class="page">
    <p>System model: run time</p>
    <p>Model Train &amp; Deploy</p>
    <p>... ... ...</p>
    <p>Trained Classifier v1</p>
    <p>Time</p>
    <p>Confidence 0.78 0.82 0.80 0.85</p>
    <p>Drift 0.09 0.10 0.21 0.08</p>
    <p>Staleness 0.00 0.01 0.02 0.00</p>
    <p>Trained Classifier v2</p>
    <p>Drift &amp; Staleness Detector</p>
    <p>Pipeline Instances</p>
    <p>Models &amp; Metrics</p>
    <p>Performance Monitoring</p>
    <p>Model Train &amp; Deploy</p>
    <p>... ... ...</p>
    <p>Trigger Rules</p>
    <p>Scoring Requests</p>
    <p>triggers</p>
    <p>monitors t1 t2 t3</p>
    <p>t4</p>
    <p>Drift</p>
  </div>
  <div class="page">
    <p>Synthetic data</p>
    <p>Data Asset</p>
    <p>Data Processing</p>
    <p>Data Asset</p>
    <p>t</p>
    <p>Training</p>
    <p>read write read</p>
    <p>Trained Model</p>
    <p>write</p>
    <p>...</p>
    <p>dimensions?</p>
    <p>size?</p>
    <p>pipeline steps?</p>
    <p>Pipeline</p>
    <p>framework? estimator type?</p>
    <p>size?</p>
  </div>
  <div class="page">
    <p>Synthetic data: pipelines Process Train Evaluate Deploy</p>
    <p>Process Train Harden Compress</p>
    <p>Bias Detection</p>
    <p>Evaluate Deploy</p>
    <p>Process Train Evaluate</p>
    <p>Base Model</p>
    <p>User-specific models</p>
    <p>Some pipelines we know about 1. Simple (typical AI web platforms)</p>
    <p>Frameworks used</p>
    <p>tensorflow</p>
    <p>pytorch</p>
    <p>caffe</p>
    <p>spark</p>
  </div>
  <div class="page">
    <p>loge transformation</p>
    <p>Sample</p>
    <p>Cluster with Gaussian Mixture</p>
    <p>ex transformation</p>
    <p>Data Asset</p>
    <p>Synthetic data: assets</p>
    <p>Example</p>
    <p>Data Asset</p>
    <p>?</p>
    <p>?</p>
    <p>cells = columns * rows</p>
    <p>Data refinery</p>
  </div>
  <div class="page">
    <p>Process model</p>
    <p>How long do pipeline task execute?</p>
    <p>How frequently are pipelines executed?</p>
    <p>How do models metrics change over time?</p>
    <p>Process Train Harden</p>
    <p>t</p>
    <p>Performance Model</p>
  </div>
  <div class="page">
    <p>Process model: task computation time Data Processing</p>
    <p>(size of data asset vs processing time)</p>
    <p>Spark</p>
    <p>Tensorflow</p>
    <p>Pytorch</p>
    <p>Caffe</p>
    <p>Training (stratified per framework)</p>
    <p>p.95 = 8000</p>
    <p>p.95 = 20000</p>
    <p>p.95 = 350</p>
    <p>p.95 = 50 f(x) = abx+c</p>
  </div>
  <div class="page">
    <p>Process model: generating load</p>
    <p>t</p>
    <p>Pipeline arrivals (sampled from training jobs)</p>
    <p>t0 t1 t2 ...</p>
    <p>a0,1 a1,2 ...</p>
    <p>We model the interarrivals A as a random variable</p>
    <p>Pareto Weibull Minimum Exponential Weibull</p>
    <p>...</p>
    <p>Long tail</p>
  </div>
  <div class="page">
    <p>Process model: arrival profiles</p>
    <p>Just another manic Monday  (of training jobs)</p>
    <p>wish it was Sunday.</p>
  </div>
  <div class="page">
    <p>Process model: arrival profiles</p>
  </div>
  <div class="page">
    <p>Process model: run time</p>
    <p>Process ModelData</p>
    <p>e</p>
    <p>...</p>
    <p>data sources</p>
    <p>?</p>
    <p>?</p>
    <p>scoring</p>
    <p>Lots of open questions how run time behaves!</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>More open problems  Conditional modeling between pipeline tasks  Modeling of system dynamics and growth</p>
  </div>
  <div class="page">
    <p>Demo</p>
    <p>Simulator Exploratory Analysis</p>
  </div>
  <div class="page">
    <p>Simulation accuracy &amp; performance</p>
    <p>Details in an extended tech report</p>
    <p>https://arxiv.org/abs/2006.12587</p>
  </div>
  <div class="page">
    <p>t.rausch@dsg.tuwien.ac.at @thrauat</p>
  </div>
</Presentation>
