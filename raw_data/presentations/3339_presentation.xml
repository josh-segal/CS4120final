<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Stealing Machine Learning Models via Prediction APIs</p>
    <p>Florian Tramr, Fan Zhang, Ari Juels, Michael K. Reiter, Thomas Ristenpart</p>
    <p>Usenix Security Symposium Austin, Texas, USA August, 11th 2016</p>
  </div>
  <div class="page">
    <p>Machine Learning (ML) Systems</p>
    <p>(1) Gather labeled data</p>
    <p>x(1), y(1) x(2), y(2)</p>
    <p>Dependent variable yn-dimensional feature vector x</p>
    <p>Data Bob Tim Jake</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>(3) Use f in some application or publish it for others to use</p>
    <p>Training</p>
    <p>y =</p>
    <p>Model f</p>
    <p>x = Bob Ti mJake</p>
    <p>(2) Train ML model f from data f ( x ) = y</p>
    <p>Prediction</p>
    <p>Confidence</p>
    <p>Application</p>
  </div>
  <div class="page">
    <p>Machine Learning as a Service (MLaaS)</p>
    <p>$$$ per query</p>
    <p>Model f</p>
    <p>input</p>
    <p>Black Box classification</p>
    <p>Prediction API</p>
    <p>Data</p>
    <p>Training API</p>
    <p>Goal 1: Rich Prediction APIs  Highly Available  High-Precision Results</p>
    <p>Goal 2: Model Confidentiality  Model/Data Monetization  Sensitive Data</p>
  </div>
  <div class="page">
    <p>Machine Learning as a Service (MLaaS)</p>
    <p>Service Model types Amazon Logistic regressions</p>
    <p>Google ??? (announced: logistic regressions, decision trees, neural networks, SVMs)</p>
    <p>Microsoft Logistic regressions, decision trees,neural networks, SVMs PredictionIO Logistic regressions, decision trees, SVMs (white-box) BigML Logistic regressions, decision trees</p>
    <p>Sell Datasets  Models  Prediction Queries to other users$$$ $$$</p>
  </div>
  <div class="page">
    <p>Goal: Adversarial client learns close approximation of f using as few queries as possible</p>
    <p>Applications:</p>
    <p>Model Extraction Attacks</p>
    <p>Attack Model f Datax</p>
    <p>f(x) f</p>
    <p>Target: f(x) = f(x) on  99.9% of inputs</p>
  </div>
  <div class="page">
    <p>Goal: Adversarial client learns close approximationof f using as few queries as possible</p>
    <p>Model Extraction Attacks (Prior Work)</p>
    <p>If f(x) is just a class label: learning with membership queries - Boolean decision trees [Kushilevitz, Mansour 1993] - Linear models (e.g., binary regression) [Lowd, Meek  2005]</p>
    <p>Attack Model f Datax</p>
    <p>f(x) f</p>
    <p>Isnt this just Machine Learning?No! Prediction APIs return more information than assumed in prior work and traditional ML</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>DataAttack Model fx</p>
    <p>f(x) f</p>
    <p>Logistic Regressions, Neural Networks, Decision Trees, SVMs</p>
    <p>Reverse-engineer model type &amp; features</p>
    <p>f(x) = f(x) on 100% of inputs 100s-1000s of online queries</p>
    <p>Inversion Attack</p>
    <p>x f(x)</p>
    <p>Improved Model-Inversion Attacks [Fredrikson et al. 2015]</p>
  </div>
  <div class="page">
    <p>Model Extraction Example: Logistic Regression Task: Facial Recognition of two people (binary classification)</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>Model f</p>
    <p>Bob</p>
    <p>Alice</p>
    <p>Feature vectors are pixel data e.g., n = 92 * 112 = 10,304</p>
    <p>Data</p>
    <p>f (x) = 1 / (1+e -(w*x + b))</p>
    <p>f maps features to predicted probability of being Alice  0.5 classify as Bob &gt; 0.5 classify as Alice</p>
    <p>n+1 parameters w,b chosen using training set to minimize expected error</p>
    <p>Generalize to c &gt; 2 classes with multinomial logistic regression f(x) = [p1, p2, , pc] predict label as argmaxi pi</p>
  </div>
  <div class="page">
    <p>Model Extraction Example: Logistic Regression</p>
    <p>Goal: Adversarial client learns close approximation of f using as few queries as possible</p>
    <p>Attack</p>
    <p>Linear equation in n+1 unknowns w,b</p>
    <p>ln = w*x + b f (x) 1 - f(x)</p>
    <p>( )</p>
    <p>f (x) = 1 / (1+e -(w*x + b))</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>The image on the left is a face that was altered by computer processing. It may or may not correspond to one of the faces displayed to the</p>
    <p>right of it.</p>
    <p>If you believe that it does correspond to one of the other faces, please select the corresponding image. If you do not believe that it corresponds</p>
    <p>to one of the other faces, select Not Present.</p>
    <p>Altered Image</p>
    <p>Fig. 10. Task shown to Mechanical Turk workers for reconstruction attack evaluation. The actual tasks shown to workers rendered the altered image above the other images, while here we show them configured horizontally to save space.</p>
    <p>Softmax MLP DAE</p>
    <p>overall identified excluded</p>
    <p>% c o</p>
    <p>r r e c t</p>
    <p>(a) Average over all responses.</p>
    <p>overall identified excluded</p>
    <p>(b) Correct by majority vote of responses.</p>
    <p>overall identified excluded</p>
    <p>(c) Accuracy with high-performing workers.</p>
    <p>Fig. 11. Reconstruction attack results.</p>
    <p>In 80% of the experiments, one of the five images contained the individual corresponding to the label used in the attack. As a control, 10% of the instances used a plain image from the data set rather than one produced by MI-FACE. This allowed us to gauge the baseline ability of the workers at matching faces from the training set. In all cases, the images not corresponding to the attack label were selected at random from the training set. Workers were paid $0.08 for each task that they completed, and given a $0.05 bonus if they answered the question correctly, and workers were generally able to provide a response in less than 40 seconds. They were allowed to complete at most three tasks for a given experiment. As a safeguard against random or careless responses, we only allowed workers who have completed at least 1,000 jobs on Mechanical Turk and achieved at least a 95% approval rating, to complete the task.</p>
    <p>algorithm time (s) epochs</p>
    <p>Softmax 1.4 5.6 MLP 1298.7 3096.3 DAE 692.5 4728.5</p>
    <p>Fig. 12. Attack performance.</p>
    <p>Target Softmax MLP DAE</p>
    <p>Fig. 13. Reconstruction of the individual on the left by Softmax, MLP, and DAE.</p>
    <p>is due to the fact that the search takes place in the latent feature space of the first autoencoder layer. Because this has fewer units than the visible layer of our MLP architecture, each epoch takes less time to complete.</p>
    <p>Figure 11a gives results averaged over all responses, whereas 11b only counts an instance as correct when a majority (at least two out of three) users responded correctly. In both cases, Softmax produced the best reconstructions, yielding 75% overall accuracy and up to an 87% identifi</p>
    <p>Model f</p>
    <p>Bob</p>
    <p>Alice Data</p>
    <p>f(x) = f(x) on 100% of inputs</p>
    <p>Query n+1 random points  solve a linear system of n+1 equations</p>
    <p>x</p>
    <p>f(x) f</p>
  </div>
  <div class="page">
    <p>f</p>
    <p>f</p>
    <p>Generic Equation-Solving Attacks</p>
    <p>MLaaS Service</p>
    <p>Solve non-linear equation systemin the weights W - Optimization problem + gradient descent - Noiseless Machine Learning</p>
    <p>Multinomial Regressions &amp; Deep Neural Networks: - &gt;99.9% agreement between f and f -  1 query per model parameter of f - 100s - 1,000s of queries / seconds to minutes</p>
    <p>random inputsX outputs Y</p>
    <p>confidence values</p>
    <p>[f1(x), f2(x), . . . , fc(x)] 2 [0, 1]c</p>
    <p>Model f has k parameters W</p>
  </div>
  <div class="page">
    <p>MLaaS: A Closer Look</p>
    <p>x</p>
    <p>Model f</p>
    <p>f(x)</p>
    <p>Prediction API Training API</p>
    <p>Data</p>
    <p>- Class labels and confidence scores - Support for partial inputs</p>
    <p>ML Model Type Selection: logisticor linear regression</p>
    <p>Feature Extraction: (automated and partially documented)</p>
  </div>
  <div class="page">
    <p>Online Attack: AWS Machine Learning</p>
    <p>input</p>
    <p>Model Online Queries Time (s) Price ($) Handwritten Digits 650 70 0.07 Adult Census 1,485 149 0.15</p>
    <p>Extracted model f agrees with f on 100% of tested inputs</p>
    <p>Feature Extraction: QuantileBinning + One</p>
    <p>Hot-Encoding</p>
    <p>Reverse-engineered with partial queries and confidence scores</p>
    <p>prediction</p>
    <p>Extract-and-test</p>
    <p>Model Choice: Logistic Regression</p>
  </div>
  <div class="page">
    <p>Training samples of 40 individuals</p>
    <p>DataMultinomial LR Model f</p>
    <p>Application: Model-Inversion Attacks Infer training data from trained models [Fredrikson et al.  2015]</p>
    <p>Strategy Attack against 1 individual Attack against all 40 individuals</p>
    <p>Online Queries Attack Time Online Queries Attack Time</p>
    <p>Black-Box Inversion [Fredrikson et al.] 20,600 24 min 800,000 16 hours</p>
    <p>Extract-and-Invert (our work) 41,000 10 hours 41,000 10 hours</p>
    <p>Attack recovers image of one individual</p>
    <p>Inversion Attack</p>
    <p>x</p>
    <p>f(x)</p>
    <p>White-Box Attack</p>
    <p>f(x) = f(x) for &gt;99.9% of inputs</p>
    <p>f f(x)</p>
    <p>Extraction Attack</p>
    <p>x</p>
    <p>40</p>
    <p>1</p>
  </div>
  <div class="page">
    <p>Extracting a Decision Tree</p>
    <p>Kushilevitz-Mansour (1992)</p>
    <p>Poly-time algorithm with membership queries only  Only for Boolean trees, impractical complexity</p>
    <p>(Ab)using Confidence Values</p>
    <p>Assumption:all tree leaves have unique confidence values  Reconstruct tree decisions with differential testing  Online attacks on BigML</p>
    <p>x Confidence value derived from class distribution in the training set</p>
    <p>Inputs x and x differ in a single feature x x</p>
    <p>v v</p>
    <p>Different leaves are reached</p>
    <p>Tree splits on this feature</p>
  </div>
  <div class="page">
    <p>Attack on Linear Classifiers [Lowd,Meek 2005]</p>
    <p>How to prevent extraction?</p>
    <p>API Minimization</p>
    <p>Countermeasures</p>
    <p>decision boundary</p>
    <p>f ( x ) = y Prediction</p>
    <p>Confidence</p>
    <p>Prediction = class label only  Learning with Membership</p>
    <p>Queries</p>
    <p>n+1 parameters w,b</p>
    <p>f(x) = sign(w*x + b) classify as + if w*x + b &gt; 0 and - otherwise</p>
  </div>
  <div class="page">
    <p>Generic Model Retraining Attacks</p>
    <p>Extend the Lowd-Meek approach to non-linear models</p>
    <p>Active Learning: - Query points close to decision boundary - Update f to fit these points</p>
    <p>Multinomial Regressions, Neural Networks, SVMs: - &gt;99% agreement between f and f -  100 queries per model parameter of f</p>
    <p>100 less efficient than equation-solving</p>
    <p>query more points here</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Rich prediction APIs Model &amp; data confidentiality</p>
    <p>Efficient Model-Extraction Attacks  Logistic Regressions, Neural Networks, Decision Trees, SVMs  Reverse-engineering of model type, feature extractors  Active learning attacks in membership-query setting</p>
    <p>Applications  Sidestep model monetization  Boost other attacks: privacy breaches, model evasion</p>
    <p>Thanks! Find out more: https://github.com/ftramer/Steal-ML</p>
  </div>
</Presentation>
