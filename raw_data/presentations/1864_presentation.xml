<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Interactive Debugging for Big Data Analytics</p>
    <p>Muhammad Ali Gulzar, Xueyuan Han, Matteo Interlandi, Shaghayegh Mardani, Sai Deep Tetali, Tyson Condie, Todd Millstein, Miryung Kim University of California, Los Angeles</p>
  </div>
  <div class="page">
    <p>Debugging Big Data Analytics</p>
    <p>Todays platforms lack debugging support  Programs (i.e., queries, jobs) are batch executed / black boxes  Errors reflect low-level details (e.g., task id?!) not relevant to the logical bug  Long program execution time =&gt; long development cycles</p>
    <p>What do programmers do?  Trial and error debugging on sample data  Post-mortem analysis of error logs  Analyze physical view of the execution (a job id, failed node, etc).</p>
  </div>
  <div class="page">
    <p>I would like to understand the flow of control through the Spark source code on the worker nodes when I submit my application  I am assuming I</p>
    <p>should setup Spark on Eclipse  to enable stepping through Spark source code on the worker nodes.</p>
    <p>Trying to debug a Spark Application on a cluster</p>
  </div>
  <div class="page">
    <p>After a year, still no good answers!</p>
  </div>
  <div class="page">
    <p>BigDebug Project Overview BigDebug: Debugging Primitives</p>
    <p>for Interactive Big Data Processing in Spark</p>
    <p>[ICSE 2016]</p>
    <p>Simulated Breakpoint On-Demand Watchpoint Crash Culprit Remediation Forward Backward Tracing</p>
    <p>Titian: Data Provenance for FineGrained Tracing [PVLDB 2016]</p>
    <p>Vega: Incremental Computation for Interactive Debugging</p>
    <p>[Under Review]</p>
  </div>
  <div class="page">
    <p>Example Query Development Session</p>
    <p>Dataset: NYC Open Data Project  Calls to non-emergency service centers  Dataset contains call records for 2010-2015  Record contents: call time, agency, caller location, etc.</p>
    <p>Query: Identify the agencies that received the most calls during busy hours  E.g., busy hour if number of calls &gt; 10,000</p>
  </div>
  <div class="page">
    <p>Spark Program</p>
    <p>case class Calls(id:String, hour:Int, agency:String,...) format = new SimpleDateFormat(&quot;M/d/y h:m:s a&quot;) input = sc.textFile(&quot;hdfs://...&quot;) calls = input.map(_.split(&quot;,&quot;))</p>
    <p>.map(r =&gt; Calls(r(0),format.parse(r(1)).getHours,r(2),...) calls.registerTempTable(&quot;calls&quot;) hist = sqlContext.sql(&quot;</p>
    <p>SELECT agency, count(*) FROM calls JOIN (</p>
    <p>SELECT hour FROM calls GROUP BY hour HAVING count(*) &gt; 100000 ) counts</p>
    <p>ON calls.hour= counts.hour GROUP BY agency&quot;)</p>
    <p>hist.show()</p>
  </div>
  <div class="page">
    <p>Extract Dataset from HDFS Transform it into a DataFrame (i.e., table) Load it into Spark SQL</p>
    <p>case class Calls(id:String, hour:Int, agency:String,...) format = new SimpleDateFormat(&quot;M/d/y h:m:s a&quot;) input = sc.textFile(&quot;hdfs://...&quot;) calls = input.map(_.split(&quot;,&quot;))</p>
    <p>.map(r =&gt; Calls(r(0),format.parse(r(1)).getHours,r(2),...) calls.registerTempTable(&quot;calls&quot;) hist = sqlContext.sql(&quot;</p>
    <p>SELECT agency, count(*) FROM calls JOIN (</p>
    <p>SELECT hour FROM calls GROUP BY hour HAVING count(*) &gt; 100000 ) counts</p>
    <p>ON calls.hour= counts.hour GROUP BY agency&quot;)</p>
    <p>hist.show()</p>
  </div>
  <div class="page">
    <p>Express Query in Spark SQL</p>
    <p>case class Calls(id:String, hour:Int, agency:String,...) format = new SimpleDateFormat(&quot;M/d/y h:m:s a&quot;) input = sc.textFile(&quot;hdfs://...&quot;) calls = input.map(_.split(&quot;,&quot;))</p>
    <p>.map(r =&gt; Calls(r(0),format.parse(r(1)).getHours,r(2),...) calls.registerTempTable(&quot;calls&quot;) hist = sqlContext.sql(&quot;</p>
    <p>SELECT agency, count(*) FROM calls JOIN (</p>
    <p>SELECT hour FROM calls GROUP BY hour HAVING count(*) &gt; 100000 ) counts</p>
    <p>ON calls.hour= counts.hour GROUP BY agency&quot;)</p>
    <p>hist.show()</p>
    <p>Identify the busy hours i.e., #calls &gt; 10,000</p>
    <p>Join busy hours with calls then group by agency and count the number of calls received by each agency</p>
  </div>
  <div class="page">
    <p>Debugging Query Results  Analyst observes some unexpected results  Agencies that should not appear  e.g., Brooklyn Public Library</p>
    <p>Expected agencies that should appear  e.g, NYPD, NYFD</p>
    <p>Titian support for query triage  Analyst can trace back from outlier results to contributing data at some intermediate stage</p>
    <p>Analyst can execute queries against intermediate data leading to outlier results</p>
  </div>
  <div class="page">
    <p>Query Triage with Titian  Intermediate results for subquery  Trace back to subquery and show distribution of calls per hour  On intermediate data leading to outlier results</p>
    <p>Significant skew in the midnight hour=0!</p>
    <p>SELECT hour, count(*) FROM calls GROUP BY hour</p>
  </div>
  <div class="page">
    <p>Identify Bug and Revise the Query  The Bug</p>
    <p>System assigns default value hour=0 for  Calls that did not log a time</p>
    <p>Possible course of action  Filter out calls assigned to hour=0</p>
    <p>SELECT agency, count(*) FROM calls JOIN (</p>
    <p>SELECT hour FROM calls WHERE hour != 0 GROUP BY hour HAVING count(*) &gt; 100000 ) counts</p>
    <p>ON calls.hour= counts.hour GROUP BY agency</p>
    <p>Introduce predicate that filters out midnight hour</p>
  </div>
  <div class="page">
    <p>Vega: Re-execute revised Query  Vega materializes intermediate stage results  i.e., The previous subquery result is saved</p>
    <p>Vega Query Rewriter leverages this to rewrite the query into</p>
    <p>SELECT agency, count(*) FROM calls JOIN counts WHERE counts.hour != 0 ON calls.hour = counts.hour GROUP BY agency</p>
    <p>Materialized result from previous execution Rewrite filter to remove hour 0 from joining records</p>
  </div>
  <div class="page">
    <p>Vega: Modified Query Evaluation  Execute an incremental join  Diff records specify changes in the (join) result  For this example, we incrementally remove all records for hour 0 from join and final aggregation results</p>
    <p>Vega Optimizer Results Consequence: over an order-ofmagnitude runtime improvement</p>
  </div>
  <div class="page">
    <p>When a program fails, a user may want to investigate a subset of the original input inducing a crash, a failure, or a wrong outcome.</p>
    <p>Delta Debugging [Zeller 1999]  Well known debugging algorithm for minimizing failure-inducing inputs</p>
    <p>Requires multiple runs to isolate failure-inducing inputs</p>
    <p>Automated Isolation of FailureInducing Inputs for Big Data Analytics</p>
  </div>
  <div class="page">
    <p>First we run the test to find the failure inducing input dataset</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Test Fails</p>
    <p>First, we run the test to find the failure inducing input dataset</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Second, we split the failing input data</p>
    <p>Test Fails Split</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Test Fails Split</p>
    <p>Test Passes</p>
    <p>Test Fails</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Test Fails Split</p>
    <p>Test Passes</p>
    <p>Test Fails Split</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Test Fails Split</p>
    <p>Test Passes</p>
    <p>Test Fails Split</p>
    <p>...</p>
    <p>Background: Delta Debugging [Zeller, FSE 1999]</p>
  </div>
  <div class="page">
    <p>Scalable Automated Isolation of Failure-Inducing Inputs</p>
    <p>Leverage data provenance to reduce search space  Avoid costly executions on data not relevant to the bug</p>
    <p>Leverage Vega optimize subsequent runs.</p>
    <p>Delta DebuggingTitian</p>
  </div>
  <div class="page">
    <p>Conclusion  BigDebug Project  Debugging Primitives for Interactive Big Data Processing in Apache Spark  https://sites.google.com/site/sparkbigdebug/</p>
    <p>Titian: Interactive Data Provenance  Supports trace back queries from a set of results  Execution replay from an intermediate point</p>
    <p>Vega: Optimizing modified query execution  Novel query rewrite mechanism that pushes changes backwards to save work  Incremental evaluation that operates on data changes induced by query modifications</p>
  </div>
</Presentation>
