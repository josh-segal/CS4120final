<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Bayesian Optimization via Simulation with Correlated Sampling and Correlated Prior Beliefs</p>
    <p>Peter I. Frazier Jing Xie</p>
    <p>Stephen E. Chick</p>
    <p>Operations Research &amp; Information Engineering, Cornell University Technology &amp; Operations Management Area, INSEAD</p>
    <p>Dec 12, 2011 2011 Winter Simulation Conference</p>
    <p>Phoenix, Arizona</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution</p>
    <p>WE CONSIDER Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative</p>
    <p>R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution</p>
    <p>WE CONSIDER Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution WE CONSIDER</p>
    <p>Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution</p>
    <p>WE CONSIDER Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means</p>
    <p>Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution</p>
    <p>WE CONSIDER Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Motivation &amp; Contribution</p>
    <p>WE CONSIDER Optimization via simulation over a finite set of alternatives Efficient allocation of simulation effort to find the best alternative R&amp;S  discrete optimization via simulation (DOvS)!</p>
    <p>WE EMPLOY a Bayesian value-of-information approach</p>
    <p>WE ALLOW a general combination of both types of correlation Correlated prior belief on the sampling means Correlated sampling (through common random numbers)</p>
    <p>WE DERIVE An exact value of sampling the difference between a pair of alts New knowledge-gradient (KG) methods based on this valuation</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Literature</p>
    <p>Jones, Schonlau and Welch 1998</p>
    <p>Brochu, Cora and Freitas 2009</p>
    <p>Negoescu, Frazier and Powell 2011</p>
    <p>Chick and Inoue 2001</p>
    <p>Frazier, Powell and Dayanik 2009</p>
    <p>Scott, Frazier, and Powell 2011</p>
    <p>Fu, Hu, Chen, and Xiong 2007</p>
    <p>Clark and Yang 1986</p>
    <p>Nelson and Matejcik 1995</p>
    <p>Yang and Nelson 1991</p>
    <p>Nakayama 2000, Kim 2005</p>
    <p>Goldsman,Marshall,Kim and Nelson 2000</p>
    <p>Novelty of This Work:</p>
    <p>Sampling plan: PAIRS Correlation types: BOTH Potential: LARGE-SCALE</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Literature</p>
    <p>Jones, Schonlau and Welch 1998</p>
    <p>Brochu, Cora and Freitas 2009</p>
    <p>Negoescu, Frazier and Powell 2011</p>
    <p>Chick and Inoue 2001</p>
    <p>Frazier, Powell and Dayanik 2009</p>
    <p>Scott, Frazier, and Powell 2011</p>
    <p>Fu, Hu, Chen, and Xiong 2007</p>
    <p>Clark and Yang 1986</p>
    <p>Nelson and Matejcik 1995</p>
    <p>Yang and Nelson 1991</p>
    <p>Nakayama 2000, Kim 2005</p>
    <p>Goldsman,Marshall,Kim and Nelson 2000</p>
    <p>Novelty of This Work:</p>
    <p>Sampling plan: PAIRS</p>
    <p>Correlation types: BOTH Potential: LARGE-SCALE</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Literature</p>
    <p>Jones, Schonlau and Welch 1998</p>
    <p>Brochu, Cora and Freitas 2009</p>
    <p>Negoescu, Frazier and Powell 2011</p>
    <p>Chick and Inoue 2001</p>
    <p>Frazier, Powell and Dayanik 2009</p>
    <p>Scott, Frazier, and Powell 2011</p>
    <p>Fu, Hu, Chen, and Xiong 2007</p>
    <p>Clark and Yang 1986</p>
    <p>Nelson and Matejcik 1995</p>
    <p>Yang and Nelson 1991</p>
    <p>Nakayama 2000, Kim 2005</p>
    <p>Goldsman,Marshall,Kim and Nelson 2000</p>
    <p>Novelty of This Work:</p>
    <p>Sampling plan: PAIRS Correlation types: BOTH</p>
    <p>Potential: LARGE-SCALE</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Literature</p>
    <p>Jones, Schonlau and Welch 1998</p>
    <p>Brochu, Cora and Freitas 2009</p>
    <p>Negoescu, Frazier and Powell 2011</p>
    <p>Chick and Inoue 2001</p>
    <p>Frazier, Powell and Dayanik 2009</p>
    <p>Scott, Frazier, and Powell 2011</p>
    <p>Fu, Hu, Chen, and Xiong 2007</p>
    <p>Clark and Yang 1986</p>
    <p>Nelson and Matejcik 1995</p>
    <p>Yang and Nelson 1991</p>
    <p>Nakayama 2000, Kim 2005</p>
    <p>Goldsman,Marshall,Kim and Nelson 2000</p>
    <p>Novelty of This Work:</p>
    <p>Sampling plan: PAIRS Correlation types: BOTH Potential: LARGE-SCALE</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Modeling Sampling &amp; Prior Correlations</p>
    <p>Suppose that we have a collection of k alternatives.</p>
    <p>If we sample from them together using CRN, then we observe a normal random vector with</p>
    <p>mean vector  = (1, . . . ,k ) and covariance matrix .</p>
    <p>We assume that  is unknown and that  is known fully (can be relaxed). Our interest here is in determining x = argmaxx{x}.</p>
    <p>We begin with a multivariate normal prior on ,  N(0, 0).</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Modeling Sampling &amp; Prior Correlations</p>
    <p>Suppose that we have a collection of k alternatives.</p>
    <p>If we sample from them together using CRN, then we observe a normal random vector with</p>
    <p>mean vector  = (1, . . . ,k ) and covariance matrix .</p>
    <p>We assume that  is unknown and that  is known fully (can be relaxed). Our interest here is in determining x = argmaxx{x}.</p>
    <p>We begin with a multivariate normal prior on ,  N(0, 0).</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Modeling Sampling &amp; Prior Correlations</p>
    <p>Suppose that we have a collection of k alternatives.</p>
    <p>If we sample from them together using CRN, then we observe a normal random vector with</p>
    <p>mean vector  = (1, . . . ,k ) and covariance matrix .</p>
    <p>We assume that  is unknown and that  is known fully (can be relaxed). Our interest here is in determining x = argmaxx{x}.</p>
    <p>We begin with a multivariate normal prior on ,  N(0, 0).</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Sampling Process &amp; Posterior Inference</p>
    <p>At each point in time n = 1,2, . . .</p>
    <p>Choose a subset of the alternatives to sample, described by Xn. Sample them using CRN, observing from them a vector Yn.</p>
    <p>When computing the value (VOI) of sampling Xn: Allow sampling Xn for n times and observe Yn as the average. Allow observing only the difference if measuring a pair of alts.</p>
    <p>Update the posterior distribution on :  | Dn  N(n, n). Require Xn+1 to be a function of Dn = (X1,Y1, . . . ,Xn,Yn).</p>
    <p>Stop after taking  stages of samples and select x = argmaxx{,x}.</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Sampling Process &amp; Posterior Inference</p>
    <p>At each point in time n = 1,2, . . .</p>
    <p>Choose a subset of the alternatives to sample, described by Xn. Sample them using CRN, observing from them a vector Yn.</p>
    <p>When computing the value (VOI) of sampling Xn: Allow sampling Xn for n times and observe Yn as the average.</p>
    <p>Allow observing only the difference if measuring a pair of alts.</p>
    <p>Update the posterior distribution on :  | Dn  N(n, n). Require Xn+1 to be a function of Dn = (X1,Y1, . . . ,Xn,Yn).</p>
    <p>Stop after taking  stages of samples and select x = argmaxx{,x}.</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Sampling Process &amp; Posterior Inference</p>
    <p>At each point in time n = 1,2, . . .</p>
    <p>Choose a subset of the alternatives to sample, described by Xn. Sample them using CRN, observing from them a vector Yn.</p>
    <p>When computing the value (VOI) of sampling Xn: Allow sampling Xn for n times and observe Yn as the average. Allow observing only the difference if measuring a pair of alts.</p>
    <p>Update the posterior distribution on :  | Dn  N(n, n). Require Xn+1 to be a function of Dn = (X1,Y1, . . . ,Xn,Yn).</p>
    <p>Stop after taking  stages of samples and select x = argmaxx{,x}.</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Sampling Process &amp; Posterior Inference</p>
    <p>At each point in time n = 1,2, . . .</p>
    <p>Choose a subset of the alternatives to sample, described by Xn. Sample them using CRN, observing from them a vector Yn.</p>
    <p>When computing the value (VOI) of sampling Xn: Allow sampling Xn for n times and observe Yn as the average. Allow observing only the difference if measuring a pair of alts.</p>
    <p>Update the posterior distribution on :  | Dn  N(n, n). Require Xn+1 to be a function of Dn = (X1,Y1, . . . ,Xn,Yn).</p>
    <p>Stop after taking  stages of samples and select x = argmaxx{,x}.</p>
  </div>
  <div class="page">
    <p>Sampling Model and Posterior Inference</p>
    <p>Sampling Process &amp; Posterior Inference</p>
    <p>At each point in time n = 1,2, . . .</p>
    <p>Choose a subset of the alternatives to sample, described by Xn. Sample them using CRN, observing from them a vector Yn.</p>
    <p>When computing the value (VOI) of sampling Xn: Allow sampling Xn for n times and observe Yn as the average. Allow observing only the difference if measuring a pair of alts.</p>
    <p>Update the posterior distribution on :  | Dn  N(n, n). Require Xn+1 to be a function of Dn = (X1,Y1, . . . ,Xn,Yn).</p>
    <p>Stop after taking  stages of samples and select x = argmaxx{,x}.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>Information is valued according to the improvement it produces in some decision to be made later. (Howard 1966)</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
    <p>Vn(X,) is the expected improvement (conditioned on Dn) that  samples from X can produce in the implementation decision to be made at time n + 1.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>What is Value of Information (VOI)?</p>
    <p>The VOI of observing  samples from set X, given Dn, is,</p>
    <p>Vn(X,) = En [ max</p>
    <p>x n+1,x | Xn+1 = X,n+1 =</p>
    <p>] max</p>
    <p>x n,x.</p>
    <p>n,x is the expected value of x given Dn.</p>
    <p>maxx n,x is the best we can do given Dn.</p>
    <p>maxx n+1,x is the best we will be able to do given Dn and what we learn from n+1 measurements of Xn+1 .</p>
    <p>Vn(X,) is the expected improvement (conditioned on Dn) that  samples from X can produce in the implementation decision to be made at time n + 1.</p>
    <p>Can be computed analytically (using algorithm 1 in Frazier et al. 2009) when</p>
    <p>X = x (a singleton) OR X = x(1) x(2) (difference between a pair)</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1.</p>
    <p>Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>c(X ) is the computational cost of measuring X. Vn(X,)/c(X ) is the average rate of earning VOI per unit effort. Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1. Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>c(X ) is the computational cost of measuring X. Vn(X,)/c(X ) is the average rate of earning VOI per unit effort. Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1. Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>Consider instead an approximation to the true non-myopic VOI</p>
    <p>Vn(X,) = max 1</p>
    <p>Vn(X,) c(X )</p>
    <p>c(X ) is the computational cost of measuring X. Vn(X,)/c(X ) is the average rate of earning VOI per unit effort. Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1. Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>Consider instead an approximation to the true non-myopic VOI</p>
    <p>Vn(X,) = max 1</p>
    <p>Vn(X,) c(X )</p>
    <p>c(X ) is the computational cost of measuring X.</p>
    <p>Vn(X,)/c(X ) is the average rate of earning VOI per unit effort. Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1. Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>Consider instead an approximation to the true non-myopic VOI</p>
    <p>Vn(X,) = max 1</p>
    <p>Vn(X,) c(X )</p>
    <p>c(X ) is the computational cost of measuring X. Vn(X,)/c(X ) is the average rate of earning VOI per unit effort.</p>
    <p>Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Value of Information Myopic Value of Information</p>
    <p>Non-Myopic Value of Information</p>
    <p>Vn(X,) is myopic &amp; is exact only if we stop at time n + 1. Vn(X,) is non-concave in  (Frazier and Powell 2010).</p>
    <p>Consider instead an approximation to the true non-myopic VOI</p>
    <p>Vn(X,) = max 1</p>
    <p>Vn(X,) c(X )</p>
    <p>c(X ) is the computational cost of measuring X. Vn(X,)/c(X ) is the average rate of earning VOI per unit effort. Vn(X,) is the best rate of earning VOI by taking a fixed number of repeated samples from X.</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>KG Allocation and Stopping Rules (Often  = 1)</p>
    <p>The single-alternative KG factor for each x, at time n, is</p>
    <p>KGn (x) = Vn (x,) c(x)</p>
    <p>.</p>
    <p>KG allocation rule: X KGn+1 = x KG n+1 = argmaxx{</p>
    <p>KG n (x)}.</p>
    <p>KG stopping rule: KG = min{n : KGn (x) &lt; 1,x}.</p>
    <p>Introduced in Gupta and Miescke 1996, Frazier and Powell 2008, and discussed further in Frazier et al. 2008, Chick and Frazier 2009.</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>k = 100. (i, i) = 50, (i, j) = 25, for i, j = 1, . . . ,100. 0 = ~0,</p>
    <p>0(i, j) = 100 exp [</p>
    <p>(i  j)2 50</p>
    <p>] .</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>k = 100. (i, i) = 50, (i, j) = 25, for i, j = 1, . . . ,100. 0 = ~0,</p>
    <p>0(i, j) = 100 exp [</p>
    <p>(i  j)2 50</p>
    <p>] .</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG 2</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 2</p>
    <p>1  1.96</p>
    <p>(x 1 , y</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG 2</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 2</p>
    <p>1  1.96</p>
    <p>(x 2 , y</p>
    <p>(x 1 , y</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 3</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 4</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 5</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 6</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 7</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 8</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 9</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 10</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 12</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 14</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 16</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 18</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 20</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 22</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 25</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 30</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 35</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 40</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 45</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 50</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 60</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 70</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 80</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 90</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 100</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 110</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 120</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 130</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 140</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 150</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 160</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 170</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 180</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 190</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 200</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 220</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 240</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 260</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 280</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm</p>
    <p>Illustration of the KG1 Algorithm</p>
    <p>50</p>
    <p>40</p>
    <p>30</p>
    <p>20</p>
    <p>10</p>
    <p>x</p>
    <p>log [ vKG n</p>
    <p>( x ) / c ( x ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 300</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (x i , y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>KG2 Allocation and Stopping Rules (Often  = 1)</p>
    <p>The pairwise-difference KG factor of a pair of alternatives( x(1),x(2)</p>
    <p>) , at time n, is</p>
    <p>KGn</p>
    <p>( x(1),x(2)</p>
    <p>) =</p>
    <p>Vn ( x(1) x(2),</p>
    <p>)  [ c ( x(1)</p>
    <p>) + c</p>
    <p>( x(2)</p>
    <p>)].</p>
    <p>KG2 Allocation Rule: Find the largest KG factor among all KGn () and KGn (, ).</p>
    <p>If it corresponds to a single alternative x, let X KG</p>
    <p>n+1 = x. a pair</p>
    <p>( x(1),x(2)</p>
    <p>) , let X KG</p>
    <p>n+1 = ( x(1),x(2)</p>
    <p>) .</p>
    <p>KG2 Stopping Rule: Stop when all KGn () and KGn (, ) drop below 1.</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>KG2 Allocation and Stopping Rules (Often  = 1)</p>
    <p>The pairwise-difference KG factor of a pair of alternatives( x(1),x(2)</p>
    <p>) , at time n, is</p>
    <p>KGn</p>
    <p>( x(1),x(2)</p>
    <p>) =</p>
    <p>Vn ( x(1) x(2),</p>
    <p>)  [ c ( x(1)</p>
    <p>) + c</p>
    <p>( x(2)</p>
    <p>)]. KG2 Allocation Rule:</p>
    <p>Find the largest KG factor among all KGn () and KGn (, ). If it corresponds to</p>
    <p>a single alternative x, let X KG 2</p>
    <p>n+1 = x. a pair</p>
    <p>( x(1),x(2)</p>
    <p>) , let X KG</p>
    <p>n+1 = ( x(1),x(2)</p>
    <p>) .</p>
    <p>KG2 Stopping Rule: Stop when all KGn () and KGn (, ) drop below 1.</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 1</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1 0</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 2</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 3</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 4</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 5</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 6</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 7</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 8</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 9</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>7 6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 10</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>7 6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 11</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>7 6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 12</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>7 6 5 4 3 2 1 0</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 13</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>7 6 5 4 3 2 1 0</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 14</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>10 9 8 7 6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 15</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>10 9 8 7 6 5 4 3 2 1</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 16</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>11 10 9 8 7 6 5 4 3 2</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 17</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>16 14 12 10 8 6 4 2</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 18</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>18 16 14 12 10 8 6 4</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 19</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>18 16 14 12 10 8 6 4</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 20</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>16 14 12 10 8 6 4</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 22</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>14 12 10 8 6 4 2</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 24</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>14 12 10 8 6 4</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 26</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>22 20 18 16 14 12 10 8 6 4</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 28</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>25 20 15 10 5</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 30</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 10 5</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 32</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>25 20 15 10 5</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 34</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 10 5</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 36</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 10</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 38</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>26 24 22 20 18 16 14 12 10 8 60 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 40</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 100 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 45</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>25 20 15 10 50 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 50</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>25 20 15 10 50 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 55</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(x n , y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 10 50 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 60</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 10 50 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 65</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>30 25 20 15 100 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 70</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1) x(</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>35 30 25 20 15 100 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 75</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1)</p>
    <p>x( 2 )</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>45 40 35 30 25 20 15 100 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 80</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1)</p>
    <p>x( 2 )</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>50 45 40 35 30 25 20 150 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 85</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1)</p>
    <p>x( 2 )</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>60 55 50 45 40 35 30 25 20 150 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 90</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1)</p>
    <p>x( 2 )</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>65 60 55 50 45 40 35 30 25 20 150 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 95</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2</p>
    <p>Allocation and Stopping Rules</p>
    <p>Illustration of the KG21 Algorithm</p>
    <p>x(1)</p>
    <p>x( 2 )</p>
    <p>log [ vKG n</p>
    <p>( x(1), x(2) ) / c ( x(1) , x(2) ) ]</p>
    <p>70 60 50 40 30 200 10 20 30 40 50 60 70 80 90 100 25</p>
    <p>20</p>
    <p>15</p>
    <p>10</p>
    <p>5</p>
    <p>x</p>
    <p>n = 100</p>
    <p>n1</p>
    <p>n1</p>
    <p>1.96 n1</p>
    <p>(X n , Y</p>
    <p>n )</p>
    <p>{ (X i , Y</p>
    <p>i ) }</p>
    <p>i&lt;n</p>
  </div>
  <div class="page">
    <p>Sampling Algorithm KG2 Allocation and Stopping Rules</p>
    <p>KG2 Allocation and Stopping Rules</p>
    <p>Perform like the KG2 allocation and stopping rules, except</p>
    <p>Assess non-myopic VOI in the calculation of KG factors, i.e.,</p>
    <p>KG,n (x) = max 1</p>
    <p>Vn (x,) c(x)</p>
    <p>, and</p>
    <p>KG,n</p>
    <p>( x(1),x(2)</p>
    <p>) = max</p>
    <p>1</p>
    <p>Vn ( x(1) x(2),</p>
    <p>)  [ c ( x(1)</p>
    <p>) + c</p>
    <p>( x(2)</p>
    <p>)]. Chick and Frazier 2009, Frazier and Powell 2010 looked at the KG rules, showing considerable improvement in selection efficiency compared with KG rules.</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation &amp; Stopping Rules</p>
    <p>Figure: 2 alts, a priori independent, with fixed sampling variance &amp; cost.</p>
    <p>Expected total penalty of not knowing the true means is = E [OC +</p>
    <p>n=1 c(Xn)] , where</p>
    <p>OC= maxx x maxx ,x is the opportunity cost when sampling stops after  stages.</p>
    <p>We see the following tendencies: 1 KG2  as sampling correlation  2 KG2 stopping  KG</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation &amp; Stopping Rules</p>
    <p>Figure: 2 alts, a priori independent, with fixed sampling variance &amp; cost.</p>
    <p>Expected total penalty of not knowing the true means is = E [OC +</p>
    <p>n=1 c(Xn)] , where</p>
    <p>OC= maxx x maxx ,x is the opportunity cost when sampling stops after  stages.</p>
    <p>We see the following tendencies: 1 KG2  as sampling correlation</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation &amp; Stopping Rules</p>
    <p>Figure: 2 alts, a priori independent, with fixed sampling variance &amp; cost.</p>
    <p>Expected total penalty of not knowing the true means is = E [OC +</p>
    <p>n=1 c(Xn)] , where</p>
    <p>OC= maxx x maxx ,x is the opportunity cost when sampling stops after  stages.</p>
    <p>We see the following tendencies: 1 KG2  as sampling correlation  2 KG2 stopping  KG</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation &amp; Stopping Rules</p>
    <p>Figure: 2 alts, a priori independent, with fixed sampling variance &amp; cost.</p>
    <p>Expected total penalty of not knowing the true means is = E [OC +</p>
    <p>n=1 c(Xn)] , where</p>
    <p>OC= maxx x maxx ,x is the opportunity cost when sampling stops after  stages.</p>
    <p>We see the following tendencies: 1 KG2  as sampling correlation  2 KG2 stopping  KG</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation Rules Figure: Alts on a 2-d grid with means correlated via a Branin function.</p>
    <p>Independent prior: - -</p>
    <p>Discrete Gaussian process prior:  with homogeneous mean , covariance between x and x as 0(x,x) = 20 exp</p>
    <p>[</p>
    <p>i=1,2 i|zi (x)  zi (x )|2 ] .</p>
    <p>We see the following tendencies: 1 GP prior  independent prior 2 KG2 allocation  KG allocation 3 KG21 allocation  KG</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation Rules Figure: Alts on a 2-d grid with means correlated via a Branin function.</p>
    <p>Independent prior: - -</p>
    <p>Discrete Gaussian process prior:  with homogeneous mean , covariance between x and x as 0(x,x) = 20 exp</p>
    <p>[</p>
    <p>i=1,2 i|zi (x)  zi (x )|2 ] .</p>
    <p>We see the following tendencies: 1 GP prior  independent prior</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation Rules Figure: Alts on a 2-d grid with means correlated via a Branin function.</p>
    <p>Independent prior: - -</p>
    <p>Discrete Gaussian process prior:  with homogeneous mean , covariance between x and x as 0(x,x) = 20 exp</p>
    <p>[</p>
    <p>i=1,2 i|zi (x)  zi (x )|2 ] .</p>
    <p>We see the following tendencies: 1 GP prior  independent prior 2 KG2 allocation  KG allocation</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation Rules Figure: Alts on a 2-d grid with means correlated via a Branin function.</p>
    <p>Independent prior: - -</p>
    <p>Discrete Gaussian process prior:  with homogeneous mean , covariance between x and x as 0(x,x) = 20 exp</p>
    <p>[</p>
    <p>i=1,2 i|zi (x)  zi (x )|2 ] .</p>
    <p>We see the following tendencies: 1 GP prior  independent prior 2 KG2 allocation  KG allocation 3 KG21 allocation  KG</p>
  </div>
  <div class="page">
    <p>Numerical Results</p>
    <p>Comparison of The Allocation Rules Figure: A k = 105 grid problem with correlated means &amp; sampling noises.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Conclusions</p>
    <p>WE DEVELOP new KG2 sampling algorithms.</p>
    <p>WE USE both correlated prior beliefs and correlated sampling noises.</p>
    <p>WE REDUCE the number of samples required to find a good alternative by existing KG methods that do not use correlations.</p>
    <p>WE AIM to do efficient VOI-based optimization via simulation for large-scale problems that takes advantage of correlated sampling!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Conclusions</p>
    <p>WE DEVELOP new KG2 sampling algorithms.</p>
    <p>WE USE both correlated prior beliefs and correlated sampling noises.</p>
    <p>WE REDUCE the number of samples required to find a good alternative by existing KG methods that do not use correlations.</p>
    <p>WE AIM to do efficient VOI-based optimization via simulation for large-scale problems that takes advantage of correlated sampling!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Conclusions</p>
    <p>WE DEVELOP new KG2 sampling algorithms.</p>
    <p>WE USE both correlated prior beliefs and correlated sampling noises.</p>
    <p>WE REDUCE the number of samples required to find a good alternative by existing KG methods that do not use correlations.</p>
    <p>WE AIM to do efficient VOI-based optimization via simulation for large-scale problems that takes advantage of correlated sampling!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Conclusions</p>
    <p>WE DEVELOP new KG2 sampling algorithms.</p>
    <p>WE USE both correlated prior beliefs and correlated sampling noises.</p>
    <p>WE REDUCE the number of samples required to find a good alternative by existing KG methods that do not use correlations.</p>
    <p>WE AIM to do efficient VOI-based optimization via simulation for large-scale problems that takes advantage of correlated sampling!</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>THANK YOU!</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Brochu, E., V. Cora, and N. de Freitas. 2009, November. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. Technical Report TR-2009-23, Department of Computer Science, University of British Columbia.</p>
    <p>Chick, S., and P. Frazier. 2009a, December. The Conjunction of the Knowledge Gradient and Economic Approach to Simulation Selection. In Proceedings of the 2009 Winter Simulation Conference, edited by M. D. Rossetti, R. R. Hill, B. Johansson, A. Dunkin, and R. G. Ingalls, 528-539. Piscataway, New Jersey: Institute of Electrical and Electronics Engineerings, Inc.</p>
    <p>Chick, S., and P. Frazier. 2009b. Sequential Sampling for Selection with ESP. in review.</p>
    <p>Chick, S., and K. Inoue. 2001. New Procedures to Select the Best Simulated System Using Common Random Numbers. Management Science, 47 (8): 1133-1149.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Clark, G., and W. Yang. 1986, December. A Bonferroni selection procedure when using common random numbers with unknown variances. In Proceedings of the 1986 Winter Simulation Conference, edited by J. Wilson, J. Henrikson, and S. Robert, 313 - 315. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.</p>
    <p>Frazier, P., and W. Powell. 2008, December. The Knowledge-Gradient Stopping Rule for Ranking and Selection. In Proceedings of the 2008 Winter Simulation Conference, edited by S. J. Mason, R. R. Hill, L. Moench, O. Rose, T. Jefferson, and J. W. Fowler, 305-312. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, Inc.</p>
    <p>Frazier, P., and W. Powell. 2010. Paradoxes in Learning and the Marginal Value of Information. Decision Analysis 7 (4): 378-403.</p>
    <p>Frazier, P., W. Powell, and S. Dayanik. 2008. A Knowledge Gradient Policy for Sequential Information Collection. SIAM Journal on Control and Optimization 47 (5): 2410-2439.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Frazier, P., W. Powell, and S. Dayanik. 2009. The Knowledge Gradient Policy for Correlated Normal Beliefs. INFORMS Journal on Computing 21 (4): 599-613.</p>
    <p>Fu, M. C., J.-Q. Hu, C.-H. Chen, and X. Xiong. 2007. Simulation Allocation for Determining the Best Design in the Presence of Correlated Sampling. INFORMS J. on Computing, 19 (1): 101-111.</p>
    <p>Gelman, A., J. Carlin, H. Stern, and D. Rubin. 2004. Bayesian data analysis. second ed. Boca Raton, FL: CRC Press.</p>
    <p>Gupta, S., and K. Miescke. 1996. Bayesian look ahead one-stage sampling allocations for selection of the best population. Journal of statistical planning and inference 54 (2): 229-244.</p>
    <p>Howard, R. 1966. Information Value Theory. IEEE Trans. Systems Science and Cybernetics 2 (1): 22-26.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Kim, S. 2005. Comparison with a standard via fully sequential procedures. ACM Transactions on Modeling and Computer Simulation (TOMACS), 15 (2): 155-174.</p>
    <p>Nakayama, M. 2000. Multiple comparisons with the best using common random numbers for steady-state simulations. Journal of Statistical Planning and Inference, 85 (1-2): 37-48.</p>
    <p>Negoescu, D., P. Frazier, and W. Powell. 2011. The Knowledge Gradient Algorithm for Sequencing Experiments in Drug Discovery. INFORMS Journal on Computing, 23 (3): 346-363.</p>
    <p>Nelson, B., and F. Matejcik. 1995. Using Common Random Numbers for Indifference-Zone Selection and Multiple Comparisons in Simulation. Manage- ment Science, 41 (12): 1935-1945.</p>
    <p>Rasmussen, C., and C. Williams. 2006. Gaussian Processes for Machine Learning. Cambridge, MA: MIT Press.</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>References</p>
    <p>Ryzhov, I., P. Frazier, and W. Powell. 2010. On the robustness of a one-period look-ahead policy in multi-armed bandit problems. Procedia Computer Science 1 (1): 1635-1644.</p>
    <p>Scott, W., P. Frazier, and W. Powell. 2011. The Correlated Knowledge Gradient for Simulation Optimization of Continuous Parameters Using Gaussian Process Regression. SIAM Journal on Optimization 21: 996-1026.</p>
    <p>Yang, W., and B. Nelson. 1991. Using Common Random Numbers and Control Variates in Multiple-Comparison Procedures. Operations Research, 39 (4): 583-591.</p>
  </div>
</Presentation>
