<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>INFLOW 2016</p>
    <p>ENABLING NVM FOR DATA-INTENSIVE SCIENTIFIC SERVICES</p>
    <p>erhtjhtyhy</p>
    <p>PHILIP CARNS John Jenkins Sangmin Seo Shane Snyder Robert Ross (Argonne National Laboratory)</p>
    <p>Torsten Hoefler (ETH Zurich)</p>
    <p>November 1, 2016 Savannah GA</p>
    <p>Chuck Cranor (Carnegie Mellon University)</p>
    <p>Scott Atchley (Oak Ridge National Laboratory)</p>
  </div>
  <div class="page">
    <p>DATA-INTENSIVE SCIENTIFIC SERVICES</p>
    <p>Transient data services allocated</p>
    <p>on demand to provide domain</p>
    <p>specific functionality and</p>
    <p>semantics</p>
    <p>Use in-system storage devices</p>
    <p>Overlap with application resources in</p>
    <p>some cases</p>
    <p>Provision appropriate resources for</p>
    <p>the task at hand</p>
    <p>Supplement, not replace,</p>
    <p>conventional global file system</p>
    <p>Blue: applications</p>
    <p>Yellow: transient data services</p>
    <p>Red: persistent global file system</p>
    <p>H P</p>
    <p>C e</p>
    <p>x a m</p>
    <p>p le</p>
  </div>
  <div class="page">
    <p>P ro</p>
    <p>v is</p>
    <p>io n</p>
    <p>in g</p>
    <p>C o</p>
    <p>m m</p>
    <p>.</p>
    <p>L o</p>
    <p>c a</p>
    <p>l S</p>
    <p>to ra</p>
    <p>g e</p>
    <p>F a</p>
    <p>u lt</p>
    <p>M g</p>
    <p>m t.</p>
    <p>a n</p>
    <p>d G</p>
    <p>ro u</p>
    <p>p</p>
    <p>M e</p>
    <p>m b</p>
    <p>e rs</p>
    <p>h ip</p>
    <p>S e</p>
    <p>c u</p>
    <p>ri ty</p>
    <p>DataSpaces</p>
    <p>Data store and pub/sub. Indep. job Dart</p>
    <p>RAM</p>
    <p>(SSD)</p>
    <p>Under</p>
    <p>devel. N/A</p>
    <p>DataWarp</p>
    <p>Burst buffer mgmt. Admin./</p>
    <p>sched.</p>
    <p>DVS/</p>
    <p>lnet SSD</p>
    <p>Ext.</p>
    <p>monitor</p>
    <p>Kernel,</p>
    <p>lnet</p>
    <p>FTI</p>
    <p>Checkpoint/restart mgmt. MPI ranks MPI RAM, SSD N/A N/A</p>
    <p>Kelpie</p>
    <p>Dist. in-mem. key/val store MPI ranks Nessie</p>
    <p>RAM</p>
    <p>(Object) N/A</p>
    <p>Obfusc.</p>
    <p>IDs</p>
    <p>HDFS *</p>
    <p>Dist. file system for MapReduce EMR or</p>
    <p>similar TCP/IP Disk, SSD Zookeeper</p>
    <p>Key</p>
    <p>encryption</p>
    <p>Spark *</p>
    <p>Data processing EMR or</p>
    <p>similar TCP/IP RAM Zookeeper</p>
    <p>Key</p>
    <p>encryption</p>
    <p>Specialized data services</p>
    <p>are already here!</p>
    <p>Examples from HPC and</p>
    <p>cloud environments</p>
    <p>* Persistent services such as HDFS, Spark, and others can be provisioned on-demand in cloud environments</p>
  </div>
  <div class="page">
    <p>WHATS NEXT? The evolution of data-intensive scientific services</p>
    <p>Support emerging hardware</p>
    <p>NVM (byte-addressible non-volatile memory) to accelerate data access</p>
    <p>See Aurora (ANL), Summit (ORNL), and others</p>
    <p>Lower the barrier to entry</p>
    <p>Avoid building new services from the ground up</p>
    <p>Reuse existing toolkit/framework/library for critical functionality</p>
    <p>Microservice model: lightweight composable building block services</p>
    <p>Performance portability</p>
    <p>Example microservices: key/value storage, object storage, group membership,</p>
    <p>replication, namespace management</p>
    <p>Example composed scientific service: Genomics Query Service</p>
  </div>
  <div class="page">
    <p>CASE STUDY:</p>
    <p>AN OBJECT STORAGE MICROSERVICE</p>
  </div>
  <div class="page">
    <p>GOAL</p>
    <p>Why not just use local NVM devices through local APIs?</p>
    <p>Ability to share data across tasks or ensembles</p>
    <p>Support imbalanced workloads</p>
    <p>NVM devices may not be present on every node</p>
    <p>Challenges for remotely accessible object storage service:</p>
    <p>High concurrency is the norm (thousands of application processes)</p>
    <p>Network portability for big data and HPC environments</p>
    <p>Latency (software overheads wont be masked by slow disks)</p>
    <p>Being a friendly neighbor to co-located applications and services</p>
    <p>Low-latency, high-throughput access to distributed NVM, using an object storage API, that embodies the microservice concept for dataintensive science</p>
  </div>
  <div class="page">
    <p>COMPONENTS USED IN PROTOTYPE*</p>
    <p>Local NVM access: libpmemobj (Intel)</p>
    <p>Object abstraction for local NVM access</p>
    <p>Network abstraction: Common Communication Interface (ORNL)</p>
    <p>High-performance network abstraction with concise primitives</p>
    <p>RPCs: Mercury (HDF Group and ANL)</p>
    <p>HPC-oriented RPC framework with RDMA path for data</p>
    <p>Concurrency: Argobots (ANL)</p>
    <p>User-level threading and tasking</p>
    <p>Bindings: Margo and abt-snoozer (ANL)</p>
    <p>Maps communication to threads with custom scheduling</p>
    <p>*See URLs for source code at end of presentation</p>
  </div>
  <div class="page">
    <p>Put services in charge of coordinating the following at shared resource</p>
    <p>(contention) points:</p>
    <p>Flow control and memory consumption</p>
    <p>Coherency</p>
    <p>Staging to/from slower storage tiers</p>
    <p>Access rights</p>
    <p>To accomplish this:</p>
    <p>Clients initiate access using explicit RPCs</p>
    <p>Servers drive payload transfers with RDMA</p>
    <p>Can we still get reasonably low latency with this model?</p>
    <p>Trade off latency for ability to optimize concurrent access</p>
    <p>CHALLENGE: HIGH CONCURRENCY How do we avoid overwhelming services under large-scale, highly-concurrent workloads?</p>
    <p>RPC req</p>
    <p>RPC ack</p>
    <p>RDMA</p>
    <p>payload</p>
    <p>clients server</p>
    <p>NVM</p>
  </div>
  <div class="page">
    <p>CHALLENGE: NETWORK PORTABILITY</p>
    <p>Anecdote: Argonne (ALCF) machine room has production systems with the</p>
    <p>following transports and fabric:</p>
    <p>TCP/IP (Ethernet)</p>
    <p>Verbs (multistage InfiniBand)</p>
    <p>uGNI (dragonfly)</p>
    <p>PAMI (torus)</p>
    <p>Cant afford to tune all the way to metal while supporting wide range of big data</p>
    <p>and HPC deployments</p>
    <p>Mercury RPC API is network agnostic, provides RDMA path for bulk data transfer</p>
    <p>RDMA is emulated on networks that dont support it</p>
    <p>CCI communication library (used within Mercury as an optional transport)</p>
    <p>supports TCP, Verbs, uGNI, shared memory, and others</p>
    <p>Can we support a variety of networks effectively?</p>
  </div>
  <div class="page">
    <p>CHALLENGE: LATENCY</p>
    <p>All threads are user-level threads implemented by Argobots</p>
    <p>Spawn with low overhead (for incoming requests)</p>
    <p>Schedule and context switch cooperatively (when blocked on I/O resources)</p>
    <p>No OS-level context switching</p>
    <p>Make path from network to storage as direct as possible</p>
    <p>Server drives RDMA directly to libpmemobj regions when appropriate</p>
    <p>Memory copy is more efficient than memory registration in some cases,</p>
    <p>though. More on this later.</p>
    <p>Avoid extraneous serialization by eschewing POSIX semantics and name-space</p>
    <p>rules when they arent needed</p>
    <p>How do we prevent software from becoming the bottleneck for highperformance storage devices?</p>
  </div>
  <div class="page">
    <p>CHALLENGE: FRIENDLY CO-LOCATION</p>
    <p>No busy-spinning (at least by default) for network events; requires considerations</p>
    <p>within multiple components:</p>
    <p>CCI exposes file descriptor to block callers until events are ready</p>
    <p>Mercury decouples event management from network progress</p>
    <p>Abt-snoozer scheduler for Argobots gracefully idles (but resumes promptly)</p>
    <p>when all threads are blocked on I/O</p>
    <p>Constrain CPU core usage</p>
    <p>Argobots allows fine grained control of which threads and taskets execute on</p>
    <p>which cores (execution streams)</p>
    <p>How do we minimize interference when microservices run on the same node as an application?</p>
  </div>
  <div class="page">
    <p>EVALUATION:</p>
    <p>WHAT KIND OF PERFORMANCE CAN WE</p>
    <p>ACHIEVE UNDER THESE CONSTRAINTS?</p>
  </div>
  <div class="page">
    <p>PERFORMANCE MEASUREMENT</p>
    <p>Prototype API model is similar to libpmemobj, but tailored for remote access:</p>
    <p>Explicit reads and writes rather than load/store access</p>
    <p>Allow concurrent access to objects from many clients simultaneously</p>
    <p>No transaction grouping</p>
    <p>Benchmarks:</p>
    <p>Create new plugin for IOR (HPC concurrent I/O benchmark) to read and</p>
    <p>write to distributed objects rather than files</p>
    <p>Create custom microbenchmark for access latency</p>
    <p>Measure latency of a sequential set of write or read operations</p>
    <p>Does not persist on write</p>
  </div>
  <div class="page">
    <p>THE TESTBED</p>
    <p>126 Linux nodes, each with 12 Haswell CPU cores</p>
    <p>FDR InfiniBand fabric with multistage switch</p>
    <p>384 GiB RAM per node (used to emulate NVM in these experiments)</p>
    <p>Cooley cluster at ALCF</p>
  </div>
  <div class="page">
    <p>PUTTING THE COMPONENTS TOGETHER A prototype object storage service architecture</p>
  </div>
  <div class="page">
    <p>ACCESS LATENCY</p>
    <p>Components are thin: no context</p>
    <p>switches or extra memory copies</p>
    <p>Recall that there is no busy polling</p>
    <p>Each access is at least one network round</p>
    <p>trip, one libpmem access, and one new</p>
    <p>thread Protocol modes:</p>
    <p>Eager mode, data is packed into</p>
    <p>RPC msg</p>
    <p>Data is copied to/from pre-registered</p>
    <p>RDMA buffers</p>
    <p>RDMA in place by registering</p>
    <p>memory on demand</p>
    <p>Crossover points would be different</p>
    <p>depending on transport</p>
    <p>How much latency do those software layers add?</p>
  </div>
  <div class="page">
    <p>ACCESS LATENCY</p>
    <p>Observations and questions</p>
    <p>Single digit microsecond access latencies, but could it be tuned further?</p>
    <p>Consider adaptive polling</p>
    <p>Optimize memory allocation</p>
    <p>What about the long tail?</p>
    <p>Previous slide shows confidence interval for 10,000 samples at each point,</p>
    <p>and the intervals are quite narrow</p>
    <p>But outliers are present: one of the noop samples was &gt; 70 microseconds</p>
    <p>The cost of memory copy vs. registration is a key factor in choosing protocol</p>
    <p>crossover points</p>
  </div>
  <div class="page">
    <p>AGGREGATE BANDWIDTH</p>
    <p>8 server nodes, with one service daemon</p>
    <p>per node</p>
    <p>2 to 16 client nodes, with 12 application</p>
    <p>processes per node (one per core)</p>
    <p>Grey line is projected maximum</p>
    <p>Blue line is normal, random allocation</p>
    <p>Whiskers (min and max) have</p>
    <p>significant variance</p>
    <p>Green line is special allocation with all</p>
    <p>nodes on one leaf switch</p>
    <p>Whiskers (min and max) have very</p>
    <p>little variance</p>
  </div>
  <div class="page">
    <p>AGGREGATE BANDWIDTH</p>
    <p>Observations and questions</p>
    <p>InfiniBand switches are not true crossbars, no matter how much us computer</p>
    <p>scientists would like them to be</p>
    <p>Consider switch routing and congestion-avoidance algorithms?</p>
    <p>Would better internal service instrumentation help?</p>
    <p>Are we going to observe similar phenomena on other networks?</p>
    <p>The service can saturate bandwidth aggregate bandwidth relatively easily</p>
    <p>No drop-off up to 192 application processes accessing 8 daemons</p>
    <p>What if we scale higher?</p>
    <p>What about a real scientific application?</p>
  </div>
  <div class="page">
    <p>CONCLUSIONS AND FUTURE WORK</p>
  </div>
  <div class="page">
    <p>WHATS NEXT?</p>
    <p>Initial results are encouraging</p>
    <p>Continue tuning and developing best practices</p>
    <p>Investigate performance outliers</p>
    <p>Isolate latency costs</p>
    <p>What can be done to further evaluate concept?</p>
    <p>Real applications</p>
    <p>Larger scale</p>
    <p>More architectures</p>
    <p>Real NVM hardware</p>
    <p>Stage in and out to long term storage</p>
    <p>Update microservice model to allow remote or local access under consistent API</p>
    <p>Continue to leverage new techniques from the community</p>
  </div>
  <div class="page">
    <p>AVAILABLE CODE</p>
    <p>Key components are already open-sourced under permissive licenses</p>
    <p>CCI: https://github.com/CCI/cci</p>
    <p>Network abstraction</p>
    <p>Mercury: https://mercury-hpc.github.io</p>
    <p>RPC framework</p>
    <p>Argobots: https://github.com/pmodels/argobots</p>
    <p>User-level threading</p>
    <p>Margo: https://xgitlab.cels.anl.gov/sds/margo</p>
    <p>Mercury/Argobots bindings</p>
    <p>Abt-snoozer: https://xgitlab.cels.anl.gov/sds/abt-snoozer/</p>
    <p>Argobots I/O-aware scheduler</p>
  </div>
  <div class="page">
    <p>www.anl.gov</p>
    <p>THANK YOU!</p>
    <p>THIS WORK WAS SUPPORTED BY THE U.S. DEPARTMENT OF</p>
    <p>ENERGY, OFFICE OF SCIENCE, ADVANCED SCIENTIFIC COMPUTING</p>
    <p>RESEARCH, UNDER CONTRACT DE-AC02-06CH11357.</p>
    <p>THIS RESEARCH USED RESOURCES OF THE ARGONNE LEADERSHIP</p>
    <p>COMPUTING FACILITY, WHICH IS A DOE OFFICE OF SCIENCE USER</p>
    <p>FACILITY SUPPORTED UNDER CONTRACT DE-AC02-06CH11357.</p>
  </div>
  <div class="page">
    <p>BASELINE PERFORMANCE How did we come up with projected bandwidth and access latency values?</p>
    <p>We used mpptest benchmark</p>
    <p>atop the MVAPICH MPI</p>
    <p>implementation to gather</p>
    <p>baseline numbers</p>
    <p>5.9 GiB/s max bandwidth and</p>
    <p>latency with two-sided</p>
    <p>asynchronous messaging</p>
  </div>
</Presentation>
