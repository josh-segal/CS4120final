<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DistCache: Provable Load Balancing for Large-Scale Storage Systems with Distributed Caching</p>
    <p>Zaoxing (Alan) Liu</p>
    <p>Zhihao Bai, Zhenming Liu, Xiaozhou Li, Changhoon Kim, Vladimir Braverman, Xin Jin, Ion Stoica</p>
  </div>
  <div class="page">
    <p>Large-scale cloud services need large storage clusters</p>
    <p>Major cloud services serve billions of users.</p>
    <p>Large datacenter clusters</p>
  </div>
  <div class="page">
    <p>Data Item</p>
    <p>Q ue</p>
    <p>ry P</p>
    <p>op ul</p>
    <p>ar ity</p>
    <p>Storage servers have load imbalance issue</p>
    <p>Server load</p>
    <p>...</p>
    <p>The skewness of the workload brings imbalance.</p>
    <p>Typical workloads [Sigmetrics12]:  Highly skewed.  Dynamic.</p>
  </div>
  <div class="page">
    <p>Solutions to mitigate the load imbalance</p>
    <p>Consistent hashing and related. o Do not handle dynamic and skewed workloads.</p>
    <p>Front-end cache as a load balancer.  Low update overhead.  Work for arbitrary workloads.</p>
    <p>Data migration or replication. o Large system and storage overhead. o High cache coherence cost.</p>
  </div>
  <div class="page">
    <p>Solutions to mitigate the load imbalance</p>
    <p>Consistent hashing and related. o Do not handle dynamic and skewed workloads.</p>
    <p>Data migration or replication. o Large system and storage overhead. o High cache coherence cost.</p>
    <p>Front-end cache as a load balancer.  Low update overhead.  Work for arbitrary workloads.</p>
  </div>
  <div class="page">
    <p>Prior work: Fast, small cache alleviates load imbalance</p>
    <p>Cache hottest O(n log n) items [SoCC11]</p>
    <p>Server load is balanced</p>
    <p>A cache node brings load balancing in a cluster.</p>
    <p>n servers</p>
    <p>Application to cluster-scale: [NSDI16, SOSP17]</p>
  </div>
  <div class="page">
    <p>Strawman: Big, fast cache for inter-cluster load balancing</p>
    <p>m Clusters</p>
    <p>of n</p>
    <p>Servers</p>
  </div>
  <div class="page">
    <p>One big cache is not scalable.</p>
    <p>of 32</p>
    <p>Servers</p>
    <p>One Big cache is infeasible</p>
  </div>
  <div class="page">
    <p>First, balance the load within each cluster</p>
    <p>m clusters</p>
    <p>Cluster load</p>
  </div>
  <div class="page">
    <p>Second, balance the load between clusters</p>
    <p>m clusters</p>
    <p>Cache hottest O(m log m) items.</p>
    <p>BIG Server</p>
    <p>BIG Server</p>
    <p>BIG Server</p>
    <p>We need to avoid using big node anywhere.</p>
  </div>
  <div class="page">
    <p>Upper-layer cache nodes</p>
    <p>Lower-layer cache nodes</p>
    <p>DistCache: Distributed caching as load balancer</p>
    <p>m clusters</p>
    <p>Cache hottest O(m log m) items.</p>
    <p>Provable, Practical, General mechanism.</p>
  </div>
  <div class="page">
    <p>Natural goals on a distributed caching mechanism</p>
    <p>Ideally, DistCache should be as good as one big cache to absorb O(m log m) hottest items.</p>
    <p>Lower-layer</p>
    <p>Upper-layer</p>
    <p>m clusters</p>
  </div>
  <div class="page">
    <p>Natural goals on a distributed caching mechanism</p>
    <p>Ideally, DistCache should be as good as one big cache to absorb O(m log m) hottest items.</p>
    <p>To achieve one big cache: o Support ANY query workload to hottest O(m log m) items. o Each cache node is NOT overloaded. o Keep cache coherence with MINIMAL cost.</p>
    <p>Lower-layer</p>
    <p>Upper-layer</p>
  </div>
  <div class="page">
    <p>Design Challenges of DistCache</p>
    <p>Challenge #1: How to allocate cached items?  Do not overload any cache node.  Do not incur high cache coherence cost.</p>
    <p>Challenge #2: How to query the cached items?  Provide best and stable cache query distribution.</p>
    <p>Challenge #3: How to update the cached items?  Two-phase update to ensure cache coherence.</p>
  </div>
  <div class="page">
    <p>Design Challenges of DistCache</p>
    <p>Challenge #1: How to allocate cached items?  Do not overload any cache node.  Do not incur high cache coherence cost.</p>
    <p>Challenge #2: How to query the cached items?  Provide best and stable cache query distribution.</p>
    <p>Challenge #3: How to update the cached items?  Two-phase update to ensure cache coherence.</p>
  </div>
  <div class="page">
    <p>Challenge #1: How to allocate the cached items?</p>
    <p>Strawman Sol #1: Cache-Replication</p>
    <p>{A,B,C,D,E}</p>
    <p>{A, B, C} {D, E} {F}</p>
    <p>Cache-Replication incurs high cache coherence cost.</p>
    <p>{A,B,C,D,E} {A,B,C,D,E}</p>
    <p>Update cache Update cache Update cache</p>
    <p>Update cache Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
  </div>
  <div class="page">
    <p>Challenge #1: How to allocate the cached items?</p>
    <p>Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
    <p>Strawman Sol #2: Cache-Partition</p>
    <p>{A, B, C} {D} {E}</p>
    <p>{A, B, C} {E} {D}</p>
    <p>Cache-Partition could put too many hottest items into the same cache node.</p>
    <p>Overload</p>
    <p>Overload</p>
  </div>
  <div class="page">
    <p>Independent hashes to allocate the cached items</p>
    <p>{B, E}</p>
    <p>{A, B, C} {D, E} {F}</p>
    <p>{A} {C, D, F}</p>
    <p>Update cache</p>
    <p>Update cache</p>
    <p>Two independent hashes H1 and H2 to allocate hot items</p>
    <p>Stable and best cache allocation.  Small cache coherence cost.</p>
    <p>Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
  </div>
  <div class="page">
    <p>Challenge #2: How to query the cached items?</p>
    <p>{B, E}</p>
    <p>{A, B, C} {D, E} {F}</p>
    <p>{A} {C, D, F}</p>
    <p>Querying item with upper layer first does not guarantee best throughput.</p>
    <p>Get(C) with upper layer first</p>
    <p>Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
  </div>
  <div class="page">
    <p>{B, E}</p>
    <p>{A, B, C} {D, E} {F}</p>
    <p>{A} {C, D, F}</p>
    <p>Power-of-two-choices to route the queries guarantee stable throughput.</p>
    <p>Power-of-two-choices to query the cached items</p>
    <p>Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
    <p>Get(C) with upper layer first</p>
  </div>
  <div class="page">
    <p>Putting together: DistCache</p>
    <p>.</p>
    <p>.</p>
    <p>Independent hashes to allocate cache items.  Power-of-two-choices of current cache loads to route queries.</p>
    <p>{B, E}</p>
    <p>{A, C} {B, E} {D, F}</p>
    <p>{A} {C, D, F}</p>
    <p>Get(B)</p>
    <p>Lower layer cache nodes</p>
    <p>Upper layer cache nodes</p>
  </div>
  <div class="page">
    <p>Theoretical Guarantee behind DistCache</p>
    <p>For m storage clusters: o DistCache absorbs any query workload to the</p>
    <p>hottest O(m log m) items.</p>
    <p>with the following condition: o Query rate for a single item is no larger than  of one</p>
    <p>cache nodes throughput. (No more half of a cluster!)</p>
  </div>
  <div class="page">
    <p>Proof Sketch: Convert to a perfect matching problem</p>
    <p>B</p>
    <p>A</p>
    <p>C</p>
    <p>D</p>
    <p>E</p>
    <p>F</p>
    <p>Hottest items</p>
    <p>Upper layer cache nodes</p>
    <p>Lower layer cache nodes</p>
    <p>Our PoT query can find a perfect match for any query</p>
    <p>workload distribution.</p>
    <p>Proofs leverage tools from expander graph, network flow, and querying theory</p>
  </div>
  <div class="page">
    <p>Remarks of the DistCache Analysis</p>
    <p>The numbers of cache nodes in two layers can be different as long as m isnt too small.</p>
    <p>The throughput of cache nodes can be different.</p>
    <p>Aggregated throughput is almost same as big cache.</p>
  </div>
  <div class="page">
    <p>Example Deployment Scenarios of DistCache</p>
    <p>DRAM</p>
    <p>O(100) KQPS each O(10) MQPS each</p>
    <p>O(10) MQPS each O(1) BQPS each</p>
    <p>Flash / Disk</p>
    <p>+ + DRAM/SSD Array Programmable Switch</p>
    <p>Servers</p>
    <p>Cache</p>
  </div>
  <div class="page">
    <p>Case Study: Switch-based distributed caching</p>
    <p>.</p>
    <p>Redis Storage Clusters</p>
    <p>Client Cluster</p>
    <p>When cache hit, cache switch will reply the query immediately.</p>
    <p>Programmable switches</p>
    <p>Programmable switches</p>
  </div>
  <div class="page">
    <p>Case Study: Switch-based distributed caching</p>
    <p>.</p>
    <p>Redis Storage Clusters</p>
    <p>Client Clusters</p>
    <p>When cache miss, query is handled by the server.</p>
  </div>
  <div class="page">
    <p>Implementation Overview</p>
    <p>Controller</p>
    <p>Query Routing</p>
    <p>Key-Value Cache</p>
    <p>Heavy Hitter Detector</p>
    <p>Cache Management</p>
    <p>Network Management</p>
    <p>Controller</p>
    <p>Query Load Statistics</p>
    <p>Cache Switch</p>
    <p>Client-side Switch Clients</p>
    <p>Servers</p>
    <p>Query Routing</p>
  </div>
  <div class="page">
    <p>P4: Programmable Protocol-Independent Packet Processing</p>
    <p>User-defined Packet Format: ETH IP TCP SEQ OP KEY VALUE</p>
    <p>Existing Packet Header Packet Header for Caching</p>
    <p>Pa rs</p>
    <p>er</p>
    <p>D ep</p>
    <p>ar se</p>
    <p>r</p>
    <p>Header/Metadata in Shared Memory</p>
    <p>Match-Action Table</p>
    <p>Match-Action Table</p>
    <p>Match-Action Table</p>
  </div>
  <div class="page">
    <p>P4: Programmable Protocol-Independent Packet Processing</p>
    <p>User-defined Packet Format: ETH IP TCP SEQ OP KEY VALUE</p>
    <p>Existing Packet Header Packet Header for Caching</p>
    <p>Pa rs</p>
    <p>er</p>
    <p>D ep</p>
    <p>ar se</p>
    <p>r</p>
    <p>Header/Metadata in Shared Memory</p>
    <p>Match: OP == GET</p>
    <p>Action: Get_Load ++</p>
    <p>Match: Val of A is fetched</p>
    <p>Action: Update to header</p>
    <p>ETH IP TCP 1 GET A NULL</p>
    <p>Match: KEY == A &amp; Vaild</p>
    <p>Action: Get value of A</p>
    <p>ETH IP TCP 1 GET A V(A)</p>
  </div>
  <div class="page">
    <p>Evaluation Setup</p>
    <p>Baselines: NoCache, Cache-Partition, Cache-Replication.</p>
    <p>Emulated Storage Servers</p>
    <p>Emulated Lower-layer nodes Emulated Client-side switches</p>
    <p>Emulated Upper-layer nodes</p>
    <p>Two Physical Servers Emulated Client Servers</p>
  </div>
  <div class="page">
    <p>Evaluation Takeaways</p>
    <p>For read queries, DistCache works as good as Cache-Replication.</p>
    <p>For write queries, DistCache has performed significantly better:  When write ratio (&lt;0.3), better throughput.  When write ratio (&gt;0.3), as good as Cache-Partition.</p>
  </div>
  <div class="page">
    <p>DistCache balances the loads of different clusters</p>
    <p>DistCache offers nearly perfect throughput for skewed workloads</p>
  </div>
  <div class="page">
    <p>DistCache scales linearly with the number of nodes</p>
    <p>P Dl</p>
    <p>iz ed</p>
    <p>Ru gh</p>
    <p>Su t</p>
    <p>DistCDche CDcheReSlicDtiRn CDche3DrtitiRn 1RCDche</p>
  </div>
  <div class="page">
    <p>DistCache incurs small cache coherence cost</p>
    <p>P Dl</p>
    <p>iz ed</p>
    <p>T hr</p>
    <p>Ru gh</p>
    <p>pu W DisWCDche</p>
    <p>CDche5eplicDWiRn CDchePDrWiWiRn 1RCDche</p>
    <p>Under Zipf-0.99 workload, DistCache offers best write throughput.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>DistCache is a general distributed caching mechanism to ensure load balancing crossing many storage clusters.</p>
    <p>DistCache requires simple primitives (independent hashing, power-of-two-choices routing).</p>
    <p>DistCache provides near-perfect throughput with rigorous theoretical guarantees.</p>
  </div>
</Presentation>
