<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>USING PSEUDO-SENSES FOR IMPROVING THE EXTRACTION OF SYNONYMS FROM WORD EMBEDDINGS</p>
    <p>Olivier Ferret</p>
  </div>
  <div class="page">
    <p>| 2</p>
    <p>Context  semantic specialization of word embeddings  most approaches following Retrofitting [Faruqui et al., 2015]</p>
    <p>a priori set of lexical semantic relations  bring word vectors closer if they are part of similarity relations (synonymy, lexical</p>
    <p>association ...)</p>
    <p>move them away from each other if they are part of dissimilarity relations (antonymy )</p>
    <p>Objectives of Pseudofit  improving word embeddings for semantic similarity without a priori lexical</p>
    <p>relations</p>
    <p>CONTEXT AND OBJECTIVES</p>
  </div>
  <div class="page">
    <p>| 3</p>
    <p>Theoritical hypothesis  homogeneous corpus C  equal split of C in 2 parts: C1 and C2  distributional representation of a word w from a corpus C = distrepC(w) =</p>
    <p>set of contexts</p>
    <p>distrepC1(w) = distrepC2(w)</p>
    <p>In practice  distrepC1(w)  distrepC2(w)</p>
    <p>Hypothesis  differences between distrepC1(w) and distrepC2(w) are contingent  bringing distrepC1(w) and distrepC2(w) closer  more general (and better)</p>
    <p>distributional representation of w</p>
    <p>PRINCIPLES: GENERAL PERSPECTIVE</p>
  </div>
  <div class="page">
    <p>| 4</p>
    <p>Distributional representations  dense representations: Skip-Gram [Mikolov et al., 2013]</p>
    <p>Notion of pseudo-sense  2 sub-corpora  2 representation spaces</p>
    <p>require projection in a shared space  source of disturbances</p>
    <p>instead, 1 corpus but 2 pseudo-senses for each word  pseudo-sense</p>
    <p>arbitrarily split the occurrences of a word into two or more subsets</p>
    <p>Overall process  generation of distributional contexts for pseudo-senses  turning pseudo-sense contexts into dense representations  convergence of pseudo-word representations  more general word</p>
    <p>representation</p>
    <p>PRINCIPLES: IMPLEMENTATION</p>
  </div>
  <div class="page">
    <p>| 5</p>
    <p>REPRESENTATIONS OF PSEUDO-WORDS</p>
    <p>Generation of contexts  2 successive occurrences of a word  2 different pseudo-senses  3 representations / word</p>
    <p>2 pseudo-senses + word itself  for each occurrence, generation of contexts for the current pseudo-sense + word</p>
    <p>frequency trick : adding the representation of the word  avoiding the impact of having half the occurrences for each pseudo-sense</p>
    <p>Building of dense representations  word2vecf [Levy &amp; Goldberg, 2014]</p>
    <p>A policeman1 was arrested by another policeman2.</p>
    <p>TARGET CONTEXT TARGET CONTEXT TARGET CONTEXT</p>
    <p>policeman a policeman1 a policeman2 another</p>
    <p>policeman be policeman1 be policeman2 by</p>
    <p>policeman arrest (x2) policeman1 arrest policeman2 arrest</p>
    <p>policeman by (x2) policeman1 by</p>
    <p>policeman another</p>
  </div>
  <div class="page">
    <p>| 6</p>
    <p>Principles  3 representations / word w: v (word); v1, v2 (pseudo-senses)</p>
    <p>v, v1 and v2: supposed to be semantically equivalent</p>
    <p>3 similarity relations: (v, v1), (v, v2) and (v1, v2)</p>
    <p>application of a semantic specialization method for word embeddings to v, v1 and v2 with the similarity relations between them</p>
    <p>final representation for w: v after its  specialization</p>
    <p>Implementation  specialization method: PARAGRAM [Wieting et al., 2015]</p>
    <p>comparable to Retrofitting but includes an automatically generated repelling component</p>
    <p>for each target word to specialize, selection of a repelling word, either randomly or according to their dissimilarity</p>
    <p>CONVERGENCE OF PSEUDO-WORD REPRESENTATIONS</p>
  </div>
  <div class="page">
    <p>| 7</p>
    <p>Experimental setup  1 billion lemmatized words randomly selected from the Annotated English</p>
    <p>Gigaword corpus [Napoles et al., 2012] at the level of sentences</p>
    <p>word embeddings built with the best parameters from [Baroni et al., 2014]  focus on nouns</p>
    <p>Word similarity evaluation  Spearmans rank correlation between human judgments and similarity</p>
    <p>between vectors for 3 representative datasets of word pairs</p>
    <p>INTRINSIC EVALUATION</p>
    <p>SimLex-999 MEN Mturk 771</p>
    <p>INITIAL 49.5 78.3 65.6</p>
    <p>Pseudofit 51.2 79.9 68.0</p>
    <p>Retrofitting 49.6 77.4 65.0</p>
    <p>Counter-fitting 49.5 77.2 64.9</p>
    <p>100</p>
  </div>
  <div class="page">
    <p>| 8</p>
    <p>Evaluation framework  Gold Standard: WordNets synonyms</p>
    <p>2.9 / word</p>
    <p>evaluated words = 11,481 nouns  frequency &gt; 20</p>
    <p>for each evaluated noun, retrieval of its 100 nearest neighbors  neighbors ranked from most similar (Cosine) to less similar</p>
    <p>Information Retrieval (IR) paradigm  evaluated word  query; neighbors  docs  IR measures: MAP, R-precision, precision@{1,2,5}</p>
    <p>SYNONYM EXTRACTION</p>
    <p>R-prec. MAP P@1 P@2 P@5</p>
    <p>INITIAL 13.0 15.2 18.3 13.1 7.7</p>
    <p>Pseudofit +2.5 +3.3 +3.0 +2.5 +1.8</p>
    <p>100</p>
  </div>
  <div class="page">
    <p>| 9</p>
    <p>Evaluation task  Semantic Textual Similarity: STS Benchmark dataset [Cer et al., 2017]  Pearson rank correlation between human judgments and similarity between</p>
    <p>sentences for a set of reference sentence pairs</p>
    <p>Computation of sentence similarity  strong baseline approach based on word embeddings  sentence representation: elementwise addition of the embeddings of the</p>
    <p>plain words of the sentence</p>
    <p>use of Pseudofit[max,fus-max-pooling] embeddings, defined for nouns, verbs and adjectives</p>
    <p>sentence similarity: Cosine between sentence representations</p>
    <p>SENTENCE SIMILARITY</p>
    <p>100</p>
    <p>INITIAL 63.2</p>
    <p>Pseudofit[max,fus-max-pooling] 66.0</p>
    <p>Best baseline (Cer et al., 2017) 56.5</p>
  </div>
  <div class="page">
    <p>| 10</p>
    <p>To sum up  Pseudofit: method for improving word embeddings towards semantic</p>
    <p>similarity without external semantic relations</p>
    <p>method based on the convergence of several representations built from the same corpus  more general representation</p>
    <p>successful intrinsic and extrinsic evaluations for word similarity, synonym extraction and sentence similarity</p>
    <p>Research directions  transposition of Pseudofit with several corpora  link with researches</p>
    <p>about meta-embeddings and ensembles of word embeddings</p>
    <p>CONCLUSIONS AND PERSPECTIVES</p>
  </div>
  <div class="page">
    <p>Commissariat  lnergie atomique et aux nergies alternatives</p>
    <p>Institut List | CEA SACLAY NANO-INNOV | BAT. 861  PC142</p>
    <p>www-list.cea.fr</p>
    <p>tablissement public  caractre industriel et commercial | RCS Paris B 775 685 019</p>
  </div>
</Presentation>
