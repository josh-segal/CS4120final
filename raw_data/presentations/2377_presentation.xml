<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Bimodal Chunking</p>
    <p>Erik Kruus</p>
    <p>Cezary Dubnicki</p>
    <p>Cristian Ungureanu</p>
    <p>Feb 29, 2010</p>
    <p>Work done at NEC laboratories 1</p>
  </div>
  <div class="page">
    <p>Content defined chunking  Motivation, approach  Introduce bimodal algorithms, transition regions  Example algorithms  Results  Conclusions, Questions</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Cut points selected based on values of a function evaluated on local data window</p>
    <p>Produces variably sized chunks</p>
    <p>Effect of small edit operations (replace,insert,delete) likely restricted to single chunks  Often used to store backup data (multiple versions)</p>
    <p>Only store one copy of duplicate chunks.  Duplicate Elimination Ratio = (input bytes) / (stored bytes)  Want high DER</p>
    <p>Content Defined Chunking</p>
  </div>
  <div class="page">
    <p>To get reproducible chunks, fix various parameters</p>
    <p>Function evaluated on local window  Choice not so important (typically a fast, rolling hash function)</p>
    <p>Average chunk size  Depends on predicate used to select cut point  Ex. function of local data window has 10 LSBs zero</p>
    <p>Expect 1 match out of every 1024</p>
    <p>Minimum chunk size, Maximum chunk size  Random chunk boundary selection  geometric distribution of</p>
    <p>chunk sizes. Too many small chunks!</p>
    <p>Perhaps mechanism for reducing # of occurences of non</p>
    <p>content-defined cut points as a result of max chunk size</p>
    <p>Baseline Chunking Parameters</p>
  </div>
  <div class="page">
    <p>?</p>
    <p>Larger blocks help I/O performance  Larger blocks reduce metadata storage overhead</p>
    <p>Large storage systems may have many bytes of metadata associated with each chunk.</p>
    <p>Motivation</p>
    <p>Small block size: High DER</p>
    <p>Large Block size: Low DER</p>
    <p>Desire Large Blocks and High DER</p>
  </div>
  <div class="page">
    <p>So what can we do improve the chunking algorithm?  Use other easily-available information</p>
    <p>In this work we investigate what can be done if a fast chunk existence query is available.</p>
    <p>NECLA archive data set: 14 backups of the main filesystem used by labs researchers every day. Full backups done every other week totaled 1.1 TB.  Analyses done using smaller chunking summary of the full</p>
    <p>dataset.</p>
    <p>Approach</p>
  </div>
  <div class="page">
    <p>Bimodal Algorithms</p>
    <p>unimodal chunking</p>
    <p>Input data block boundaries</p>
    <p>block size</p>
    <p>Uni-modal distribution</p>
    <p>bimodal chunking</p>
    <p>Input data block boundaries</p>
    <p>block size</p>
    <p>Bimodal distribution</p>
    <p>block repository</p>
    <p>block existence query yes/no</p>
  </div>
  <div class="page">
    <p>Historical intuitions</p>
    <p>Intuitive model of file system backups 1. Long stretches of unseen data should be assumed to be</p>
    <p>good candidates for appearing later on (i.e. at the next backup run).  Original data should have reasonable DER to begin with  Long stretches of unseen data should be chunked with large</p>
    <p>average chunk size.</p>
    <p>Inefficiency: short blocks can delineate the beginnings and ends of duplication regions more finely.</p>
    <p>Change regions: existence queries give us a way to detect these transition regions</p>
  </div>
  <div class="page">
    <p>Duplicate/nonduplicate byte regions in input stream</p>
    <p>Fine-grained and coarse-grained cut points:</p>
    <p>Expect transition point ~ uniformly distributed within the encompassing large chunk</p>
    <p>Why transition regions?</p>
    <p>Have been seen before! Should be duplicate eliminated.</p>
    <p>Perhaps a frequent change region? Reduced chance to see again later</p>
    <p>Small chunks in transition region could be beneficial</p>
    <p>Small chunks in duplication region are bad</p>
  </div>
  <div class="page">
    <p>Assign Duplicate/Nonduplicate byte regions  Begin with infrequent cutpoints</p>
    <p>Example: breaking-apart</p>
    <p>D N N N N D D D</p>
    <p>Final Chunking decision</p>
    <p>Existence queries required: 1 per large chunk</p>
  </div>
  <div class="page">
    <p>Assign Duplicate/Nonduplicate byte regions  Begin with frequent cutpoints</p>
    <p>Form large chunks by concatenating k small chunks (ex. k=4) Check duplication status to find all previous large chunks</p>
    <p>Example: amalgamation</p>
    <p>Transition regions  small chunks</p>
    <p>Extended nonduplicate regions remain big</p>
    <p>Big duplicate regions always good!</p>
    <p>Final Chunking decision</p>
    <p>D D D D</p>
    <p>Fixed / variable concatenation?</p>
    <p>Existence query bound: k per large chunk  Or k(k-1) if 2 to k smalls can generate a big chunk.</p>
  </div>
  <div class="page">
    <p>Transition region subcases</p>
    <p>Statistics of small chunks for some frequent subcases of fixed-size (8) amalgamation: Baseline chunkers with average</p>
    <p>chunk size from 4k to 24k.</p>
    <p>Extend to 32 chunks, see bulk 8k small chunk recurrence prob. tailing off to ~65%</p>
    <p>Will I ever see you again?</p>
    <p>Ask an oracle  Using transition regions to guide small chunk output</p>
    <p>decisions gave future hit rates that were higher than bulk expectation</p>
  </div>
  <div class="page">
    <p>Based on full NECLA data set, how good could it get?</p>
    <p>A simple, empirical limit</p>
    <p>Concatenate all chunks that always occur together</p>
    <p>x x</p>
    <p>x x</p>
    <p>Whenever a stored item has unique successor, merge!</p>
    <p>For uncompressed storage, DER is unaffected</p>
    <p>Began with 512-byte and 8k baseline chunkings of the full dataset (2 expts)</p>
    <p>Result: almost 10x larger average block size</p>
    <p>Algorithm not practical  Uses post-processing  Computationally very</p>
    <p>expensive</p>
  </div>
  <div class="page">
    <p>Comparison to empirical limit</p>
    <p>Using 56-64 existence queries per big chunk, can get ~ halfway to theoretical limit</p>
  </div>
  <div class="page">
    <p>Results summary</p>
    <p>x3</p>
    <p>x1.5</p>
    <p>Simplified storage model assumptions  Same data redundancy, No metadata, No compression</p>
    <p>Ran several algorithms, covering a range of parameter settings</p>
    <p>Algorithms 1 &amp; 2  Up to 1 or 8 queries per</p>
    <p>large chunk</p>
    <p>Chunk size  x1.5</p>
    <p>Algorithm 3  Up to 56 or 64 queries per</p>
    <p>large chunk</p>
    <p>Chunk size  x3</p>
    <p>Chunking transition regions small seems beneficial</p>
  </div>
  <div class="page">
    <p>Effect of compression</p>
    <p>A small subset of these runs used the raw dataset to obtain accurate values including compression.</p>
    <p>Amalgamation compression</p>
    <p>DER up</p>
    <p>Larger blocks compress better.</p>
    <p>Avg blocks size down 64 KB  45 KB, but little compression at 8 KB</p>
    <p>Increasing chunk size by 50% has enhanced effect at smaller chunk sizes</p>
  </div>
  <div class="page">
    <p>Effect of Metadata</p>
    <p>Consider baseline measurements</p>
    <p>Transform for effect of 100, 400, 800 bytes of metadata per chunk</p>
    <p>Simple transform to new DER = DER / (1+f), where f=metadata/&lt;chunk size&gt;</p>
    <p>Metadata impact can be severe at low chunk sizes</p>
  </div>
  <div class="page">
    <p>Detailed results: breaking apart</p>
    <p>Typical settings:  Min:avg:max = 1:2:3  3 backup levels  Small chunker settings</p>
    <p>divided by 1:2:4:8</p>
    <p>1 existence query per big chunk</p>
    <p>Small chunker 4-8x smaller (on average) was a reasonable choice.</p>
    <p>Variations on min:avg:max had little effect</p>
  </div>
  <div class="page">
    <p>Detailed results: amalgamation</p>
    <p>Typical settings:  Min:avg:max = 1:2:3  3 backup levels  Big chunk = 8 smalls  fixed size big chunks (8</p>
    <p>existence queries per big chunk)</p>
    <p>(or variable, big = 1-8 smalls, 64 existence queries per big chunk)</p>
    <p>Settings robust to minor variations</p>
    <p>Ex. 8-12 smalls all lying along same curve.</p>
  </div>
  <div class="page">
    <p>Intuitive model of file system backups 1. Long stretches of unseen data should be assumed to be</p>
    <p>good candidates for appearing later on (i.e. at the next backup run).</p>
    <p>Confirmed by oracle experiments</p>
    <p>Historical intuitions: beware!</p>
    <p>Experiment:  Run baseline chunker  Count (# dup, # following nondup)  Weight for # of bytes of input data</p>
    <p>Over these 14 backups, long stretches of unseen data were rather rare.</p>
    <p># dup</p>
    <p># following dup</p>
  </div>
  <div class="page">
    <p>Non-backup archives</p>
    <p>Source code archives, ~ 10 or so versions  Ran amalgamation with fixed-size big chunks of k smalls  Varied k</p>
    <p>Gcc sources showed some small benefit, while emacs source showed no benefit.  Not a universal solution</p>
    <p>DER/chunk size gains definitely depend on nature of archive  Expect problems if unimodal DER is low:</p>
    <p>Ex: emacs uncompressed DER was only ~1.73 for &lt;8k&gt; chunks  One of our assumptions is failing --- duplication probability is</p>
    <p>never very high.</p>
    <p>When blocks frequently fail assumption of high probability to be seen later, bimodal chunking may not be worthwhile.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>For archival data with DER &gt;3-4, chunking transition regions small is a useful mechanism to achieve competitive DER with larger than usual chunk sizes.</p>
    <p>Transition regions can be determined by adding an existence query capability to existing block stores.</p>
    <p>Small chunks in transition regions can show enhanced probability to be seen later.</p>
    <p>Questions?</p>
  </div>
</Presentation>
