<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning to Control the Specificity in Neural Response Generation</p>
    <p>Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, Xueqi Cheng 1. CAS Key Lab of Network Data Science and Technology</p>
    <p>Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China</p>
  </div>
  <div class="page">
    <p>Background - Dialog</p>
    <p>p Personal assistant, helps people complete specific tasks</p>
    <p>p Combination of rules and statistical components</p>
    <p>Task-Oriented Dialog Chit-Chat Dialog p No specific goal, attempts to produce natural responses</p>
    <p>p Using variants of seq2seq model</p>
  </div>
  <div class="page">
    <p>Background  Neural Model</p>
    <p>Must support! Cheer!</p>
    <p>Support! Its good.</p>
    <p>My friends and I are shocked!</p>
    <p>utterance-response: n-to-1 relationship</p>
    <p>e.g., the response Must support! Cheer! is used for 1216 different input utterances</p>
    <p>treat all the utterance-response pairs uniformly  employ a single model to learn the mapping</p>
    <p>between utterance and response</p>
    <p>introduce latent responding factors to model multiple responding mechanisms</p>
    <p>lack of interpretation</p>
    <p>Rank-frequency distribution</p>
    <p>Seq2Seq framework</p>
    <p>pre-defined a set of topics from an external corpus</p>
    <p>rely on external corpus</p>
    <p>favor such general responses with high frequency</p>
    <p>Performance</p>
    <p>TA-Seq2Seq</p>
    <p>MARM</p>
  </div>
  <div class="page">
    <p>How to capture different utterance-response relationships ? Conversation context Topic information Keyword Coherence Scenarios heuristics</p>
    <p>Our motivation comes from Human Conversation Process</p>
  </div>
  <div class="page">
    <p>Human Conversation Process</p>
    <p>Do you know a good eating place for Australian special food?</p>
    <p>current mood</p>
    <p>I dont know</p>
    <p>Good Australian eating places include steak, seafood, cake,</p>
    <p>etc. What do you want to choose?</p>
    <p>general response</p>
    <p>specific response knowledge state dialogue partner</p>
  </div>
  <div class="page">
    <p>Key Idea</p>
    <p>introduce an explicit specificity control variable s to represent the response purpose</p>
    <p>- s summarizes many latent factors into one variable - s has explicit meaning on specificity -  actively controls the generation of the response</p>
    <p>current mood</p>
    <p>knowledge state dialogue partner</p>
  </div>
  <div class="page">
    <p>Model Architecture</p>
    <p>the specificity control variable  is introduced into the Seq2Seq model  single model -&gt; multiple model  different &lt;utterance, response&gt;, different , different models</p>
    <p>word representation  semantic representation: relates to the semantic meaning  usage representation: relates to the usage preference</p>
    <p>Attentive Read</p>
    <p>!!&quot;#$ !!&quot;#% !!&quot;#&amp; !!&quot;#'</p>
    <p>&lt;eos&gt;</p>
    <p>my</p>
    <p>my</p>
    <p>name</p>
    <p>name</p>
    <p>is</p>
    <p>is</p>
    <p>John</p>
    <p>P(John) = ()(John) + (*(John)</p>
    <p>Semantic Representation Usage Representation</p>
    <p>!!&quot;#'</p>
    <p>!!&quot;+!!&quot;, !!&quot;- !!&quot;.!!&quot;/</p>
    <p>what is your name ?</p>
    <p>Utterance Encoder</p>
    <p>Response Decoder</p>
    <p>Gaussian Kernel Layer</p>
    <p>Specificity Control Variable</p>
    <p>Semantic-based &amp; Specificity-based Generation</p>
  </div>
  <div class="page">
    <p>Model - Encoder</p>
    <p>p Bi-RNN: modeling the utterance from both forward and backward directions n {&amp;</p>
    <p>,,* }</p>
    <p>,,&amp;</p>
    <p>n / = [/ ,*2/3&amp;</p>
    <p>]</p>
  </div>
  <div class="page">
    <p>Model - Decoder</p>
    <p>predict target word based on a mixture of two probabilities: the semantic-based and specificity-based generation probability</p>
    <p>/ = 8 / + ;(/)</p>
    <p>semantic-based probability - decides what to say next given the input</p>
    <p>8 / =  = * 8 A B CD +</p>
    <p>B/2&amp; + 8</p>
    <p>hidden state semantic representation</p>
  </div>
  <div class="page">
    <p>Model - Decoder</p>
    <p>specificity-based probability - decides how specific we should reply</p>
    <p>l Gaussian Kernel layer  the specificity control variable interacts with the usage</p>
    <p>representation of words through the layer  let the word usage representation regress to the variable  through certain mapping function (sigmoid)</p>
    <p>l specificity control variable   [0,1]  0 denotes the most general response  1 denotes the most specific response</p>
    <p>; / =  = 1 2</p>
    <p>exp( (; ,  )U</p>
    <p>; , = (*( B V + V))</p>
    <p>usage representation variance</p>
  </div>
  <div class="page">
    <p>Model Training</p>
    <p>Objective function  log likelihood  = X log(|,;)</p>
    <p>(,)b</p>
    <p>Training data: triples (,,)  s is not directly available in the raw conversation corpus</p>
    <p>How to obtain s to learn our model?</p>
    <p>We propose to acquire distant labels for</p>
  </div>
  <div class="page">
    <p>Distant Supervision</p>
    <p>l Normalized Inverse Response Frequency (NIRF)  a response is more general if it corresponds to more input utterances  the Inverse Response Frequency (IRF) in a conversation corpus</p>
    <p>l Normalized Inverse Word Frequency (NIWF)  a response is more specific if it contains more specific words  the maximum of the Inverse Word Frequency (IWF) of all the words in a response</p>
  </div>
  <div class="page">
    <p>Specificity Controlled Response Generation</p>
    <p>Given a new input utterance, we can generate responses at different specificity levels by varying the control variable s  Different s, different models, different responses</p>
    <p>n  = 1: the most informative response n   0,1 : more dynamic , enrich the styles in the response n  = 0: the most general response</p>
    <p>s</p>
  </div>
  <div class="page">
    <p>Experiments - Dataset</p>
    <p>l Short Text Conversation (STC) dataset  released in NTCIR-13  a large repository of post-comment pairs from the Sina Weibo  3.8 million post-comment pairs  Jieba Chinese word segmenter</p>
  </div>
  <div class="page">
    <p>Experiments  Model Analysis</p>
  </div>
  <div class="page">
    <p>Experiments  Model Analysis</p>
    <p>general</p>
    <p>specific</p>
  </div>
  <div class="page">
    <p>Experiments  Comparisons</p>
    <p>When  = 1, our SC-Seq2Seqopqr model can achieve the best specificity performance</p>
  </div>
  <div class="page">
    <p>Experiments  Comparisons</p>
  </div>
  <div class="page">
    <p>Experiments  Comparisons</p>
  </div>
  <div class="page">
    <p>Experiments  Case study</p>
    <p>The responses generated by the four baselines are often quite general and short</p>
  </div>
  <div class="page">
    <p>Experiments  Case study</p>
    <p>With s from 1 to 0, SC-Seq2Seqopqr can generate very long and specific responses, to more general and shorter responses.</p>
  </div>
  <div class="page">
    <p>Experiments  Analysis</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>l We argue n employing a single model to learn the mapping between the utterance and</p>
    <p>response will inevitably favor general responses</p>
    <p>l We propose n an explicit specificity control variable is introduced into the Seq2Seq model</p>
    <p>handle different utterance-response relationships in terms of specificity</p>
    <p>l Future work  employ some reinforcement learning technique to learn to adjust the control</p>
    <p>variable depending on users feedbacks  apply to other tasks, like summarization, QA, etc</p>
  </div>
  <div class="page">
    <p>Thanks Q &amp; A  Name: Ruqing Zhang | Email: zhangruqing@software.ict.ac.cn</p>
  </div>
</Presentation>
