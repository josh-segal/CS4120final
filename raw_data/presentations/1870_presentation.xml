<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Accelerating Complex Data Transfer for Cluster Computing</p>
    <p>Alexey Khrabrov, Eyal de Lara University of Toronto</p>
    <p>HotCloud 2016</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Data processing is now CPU-bound</p>
    <p>Software layers cant leverage fast datacenter networks</p>
    <p>network responsible for as low as 2% of overall performance [Ousterhout, K. et al., Making sense of performance in data analytics frameworks, NSDI15]</p>
    <p>Data [de]serialization is one of the bottlenecks</p>
    <p>up to 26% of total CPU time [Trivedi, A. et al., On the [ir]relevance of network performance for data processing, HotCloud16]</p>
    <p>prevents from fully leveraging RDMA</p>
  </div>
  <div class="page">
    <p>Serialized data transfer</p>
    <p>object2</p>
    <p>object3</p>
    <p>header</p>
    <p>field1</p>
    <p>field2</p>
    <p>pointer1</p>
    <p>pointer2</p>
    <p>o b</p>
    <p>je ct</p>
    <p>Serialization</p>
    <p>object2 data</p>
    <p>object3 data</p>
    <p>auxiliary info</p>
    <p>o b</p>
    <p>je ct</p>
    <p>d a</p>
    <p>ta</p>
    <p>field1</p>
    <p>field2</p>
    <p>object2</p>
    <p>object3</p>
    <p>header</p>
    <p>field1</p>
    <p>field2</p>
    <p>pointer1</p>
    <p>pointer2</p>
    <p>o b</p>
    <p>je ct</p>
    <p>Deserialization</p>
    <p>object2 data</p>
    <p>object3 data</p>
    <p>auxiliary info</p>
    <p>o b</p>
    <p>je ct</p>
    <p>d a</p>
    <p>ta</p>
    <p>field1</p>
    <p>field2 Transfer</p>
    <p>Source Node Destination Node</p>
  </div>
  <div class="page">
    <p>Transfer time breakdown: complex data</p>
    <p>TreeMap; size: 64 MB raw, 24 MB serialized; 10 Gbit/s</p>
  </div>
  <div class="page">
    <p>Transfer time breakdown: simple data</p>
    <p>double[]; size: 80 MB; 10 Gbit/s</p>
  </div>
  <div class="page">
    <p>Eliminating data [de]serialization</p>
    <p>Reason: pointer-based data structures become invalid when copied directly to another address space</p>
    <p>other reasons (e.g. different endianness) are irrelevant: assume that all nodes have the same architecture</p>
    <p>General idea: shared cluster-wide virtual address space</p>
    <p>Compact allocation of objects to be copied together</p>
    <p>continuous regions copied in a single operation  RDMA-friendly</p>
  </div>
  <div class="page">
    <p>Compact object format and Direct transfer</p>
    <p>object2</p>
    <p>object3</p>
    <p>header</p>
    <p>field1</p>
    <p>field2</p>
    <p>pointer1</p>
    <p>pointer2</p>
    <p>o b</p>
    <p>je ct</p>
    <p>G lo</p>
    <p>b a</p>
    <p>l H</p>
    <p>e a</p>
    <p>p O</p>
    <p>b je</p>
    <p>ct</p>
    <p>object2</p>
    <p>object3</p>
    <p>header</p>
    <p>field1</p>
    <p>field2</p>
    <p>pointer1</p>
    <p>pointer2</p>
    <p>o b</p>
    <p>je ct</p>
    <p>Transfer</p>
    <p>Source Node Destination Node</p>
  </div>
  <div class="page">
    <p>Cluster-wide shared address space</p>
    <p>Virtual address space is huge -&gt; can be shared  128 TB (247), potentially 263 bytes</p>
    <p>Limited version of DSM (distributed shared memory)</p>
    <p>DSM original goal: trade off performance for transparency / ease of programming</p>
    <p>We use DSM to improve performance (but increase programming complexity)</p>
  </div>
  <div class="page">
    <p>Assumptions</p>
    <p>Immutable shared objects</p>
    <p>modifications of the original are not propagated</p>
    <p>not very restrictive: e.g. immutable RDDs in Spark</p>
    <p>No need to be completely transparent to programmer</p>
    <p>explicit management of global objects</p>
    <p>possible to hide most of the details inside the framework</p>
  </div>
  <div class="page">
    <p>Global heap</p>
    <p>Node 1</p>
    <p>Local heap</p>
    <p>obj orig obj orig</p>
    <p>exclusive region</p>
    <p>Node 2</p>
    <p>Local heap</p>
    <p>obj</p>
    <p>copy</p>
    <p>exclusive region</p>
    <p>Coordinator</p>
    <p>GObject obj = new GObject(...); obj.data = new MyFancyClass(...); //... obj.commit(&quot;key&quot;); //... obj.release();</p>
    <p>GObject obj = GHeap.get(&quot;key&quot;); MyFancyClass data = obj.data; //... obj.release();</p>
    <p>direct copy</p>
    <p>obj orig</p>
    <p>Directory</p>
    <p>(rare) phys mem</p>
    <p>phys mem</p>
    <p>Architecture</p>
  </div>
  <div class="page">
    <p>Global heap architecture</p>
    <p>Huge virtual address space region; the same on all nodes</p>
    <p>Partitioning: nodes allocate objects in own exclusive regions  minimal amount of coordination required</p>
    <p>Mapping to physical memory on demand</p>
    <p>Objects identified by keys mapped to &lt;node, vaddr&gt;</p>
    <p>3-stage object creation: (1) reserve space; (2) populate with data; (3) commit (make available to other nodes)</p>
    <p>Explicit release of objects</p>
  </div>
  <div class="page">
    <p>JVM-based implementation</p>
    <p>Prototype based on JamVM  HotSpot (standard JVM)  in progress</p>
    <p>Most of functionality implemented in native methods</p>
    <p>Still need some JVM modifications  memory allocator / garbage collector</p>
    <p>object header format</p>
    <p>bytecode interpreter / JIT compiler</p>
    <p>Details: in the paper</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Microbenchmark (performance of the mechanism alone)</p>
    <p>Transfer objects between 2 identical nodes</p>
    <p>Direct copy vs. serialized  both standard Java serialization and Kryo</p>
    <p>HotSpot for serialized measurements, JamVM for direct copy</p>
    <p>TCP transport, 10 Gbit/s; expect better results with RDMA</p>
    <p>Overhead of JVM modifications: within 1%</p>
  </div>
  <div class="page">
    <p>Evaluation: complex data (TreeMap)</p>
  </div>
  <div class="page">
    <p>Evaluation: simple data (double[])</p>
  </div>
  <div class="page">
    <p>Evaluation: small simple objects</p>
  </div>
  <div class="page">
    <p>Proposed applications</p>
    <p>Data processing frameworks: Spark, Hadoop, etc.</p>
    <p>optimize shuffle stages (data exchange between all nodes)</p>
    <p>possible scheduling improvements; data migration is now cheaper</p>
    <p>Distributed in-memory storage</p>
    <p>store complex data efficiently</p>
    <p>reduce latency of set/get operations</p>
    <p>Fast IPC and RPC</p>
    <p>zero-copy within one machine (using shared memory)</p>
  </div>
  <div class="page">
    <p>Current and future work directions</p>
    <p>Applications and macrobenchmarks</p>
    <p>RDMA</p>
    <p>Reliability / fault tolerance</p>
    <p>Storage considerations (spills to disk)</p>
    <p>Multiple address spaces for extremely large datasets</p>
    <p>Global heap space management, other implementation details</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Data [de]serialization is a bottleneck; doesnt let us fully leverage fast network</p>
    <p>Designed a data transfer mechanism to avoid serialization</p>
    <p>main idea: shared cluster-wide virtual address space</p>
    <p>Use DSM to improve performance, trading off increased programming complexity</p>
    <p>Evaluation shows significant (up to 10x) speedup of data transfer</p>
    <p>Will explore applications that can benefit from this mechanism</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
