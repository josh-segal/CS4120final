<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards Interactive Training with an Avatar</p>
    <p>based Human-Computer Interface</p>
    <p>University of Central Florida University of Illinois at Chicago</p>
    <p>UCF</p>
  </div>
  <div class="page">
    <p>Topics</p>
    <p>What we are doing</p>
    <p>Why are we doing it</p>
    <p>How we are doing it</p>
    <p>How this relates to simulation and training</p>
    <p>Results</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>What are we doing?  briefly</p>
    <p>Building a guru system with an avatar interface capable of communicating with humans via spoken language  Our current emphasis is on avatar interface</p>
    <p>Lifelike in its appearance and dialog</p>
    <p>Project funded by NSF to a collaborative research team at the University of Central Florida (UCF) and the University of Illinois at Chicago (UIC)</p>
  </div>
  <div class="page">
    <p>The What  (continued)</p>
    <p>The initial objective: replace a specific human being  Dr. Alex Schwarzkopf at the NSF  Founding and long-time director of the</p>
    <p>Industry/University Collaborative Research Centers (IUCRC) program</p>
    <p>Recently retired</p>
    <p>Follows a separate three-year research project at UCF to capture Dr. Schwarzkopfs knowledge about the I/UCRC and code it in an easily retrievable system (called AskAlex)</p>
  </div>
  <div class="page">
    <p>Dr. Alex Schwarzkopf</p>
  </div>
  <div class="page">
    <p>The AlexAvatar</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>Facing several challenges in this project: 1. Make an avatar that is visually recognizable as Dr.</p>
    <p>Schwarzkopf, both in facial features and in his mannerisms and tendencies.</p>
  </div>
  <div class="page">
    <p>Why are we doing this?</p>
    <p>We seek to show the feasibility of effective interfaces that employ natural language and a recognizable human-like avatar.  Is it practical?  Do humans respond better to lifelike avatars than</p>
    <p>text-based interfaces?</p>
    <p>We are trying to preserve the legacy of Dr. Schwarzkopf by not only preserving his knowledge but in many ways himself, too.</p>
  </div>
  <div class="page">
    <p>How are we doing this?</p>
    <p>Two parts to the work:</p>
  </div>
  <div class="page">
    <p>Creating a Virtual Alex</p>
    <p>Vicon motion capture hardware</p>
  </div>
  <div class="page">
    <p>Creating a Virtual Alex (cont)</p>
    <p>FaceGen software for expressions</p>
  </div>
  <div class="page">
    <p>Alex at Work</p>
    <p>Alex sits at his desk at NSF, a familiar environment (sitting at desk in office wearing a suit).</p>
    <p>Alex provides information by speaking and showing textual or graphical information on the TV screen in his office.</p>
  </div>
  <div class="page">
    <p>Interacting with the User</p>
    <p>Alex displayed on a 50 display  Shows upper body with room for him to gesture</p>
    <p>Multiple microphones used so Alex can turn towards speaker</p>
    <p>Blinking and other involuntary motions based on Alexs mannerisms</p>
    <p>Responsive Avatar Framework  Skeleton Animation Synthesizer  Facial Expression Synthesizer  Lip Synchronizer</p>
  </div>
  <div class="page">
    <p>Speech Recognizer Architecture</p>
    <p>MS SAPI 5.1 LifeLike</p>
    <p>Recognizer</p>
    <p>Grammars XML Dictation Mode Text Repository</p>
    <p>Dictation Mode</p>
    <p>Grammar Recognition</p>
    <p>LifeLike Dialog System</p>
    <p>ChantSR</p>
    <p>SAPI4 Recog SAPI5</p>
    <p>Recog</p>
    <p>IBM Via Voice</p>
    <p>Dragon Nat. Speak</p>
    <p>Nuance VoCon 3200</p>
    <p>SAPI4 SAPI5</p>
    <p>SMAPI Dragon VoCon</p>
    <p>ChantSR</p>
    <p>LifeLike Recognizer</p>
    <p>Operational View:</p>
    <p>Layered Recognition Design:</p>
  </div>
  <div class="page">
    <p>LifeLike Dialog System</p>
    <p>Knowledge Manager</p>
    <p>Speech Disambiguator</p>
    <p>Context-based Dialogue Manager</p>
    <p>Dialog System Architecture</p>
    <p>LifeLike Recognizer NSF User</p>
    <p>Data</p>
    <p>General Knowledge</p>
    <p>AskAlex Ontology</p>
    <p>Spell Check</p>
    <p>Semantics Check</p>
    <p>Context Specific Knowledge</p>
    <p>LifeLike Speech Output</p>
    <p>Dictation String</p>
    <p>Phrase String</p>
    <p>Context</p>
    <p>C o n t e x t</p>
    <p>D i s a m b i g u a t e d</p>
    <p>S t r i n g</p>
    <p>Context</p>
    <p>Dataset</p>
    <p>Context</p>
    <p>Dataset</p>
    <p>Response String</p>
    <p>Response String</p>
    <p>Updated Data</p>
  </div>
  <div class="page">
    <p>Dialog Management</p>
    <p>Semantic Disambiguator  Spelling and semantic check of SR input  Uses contextual matching processes</p>
    <p>Knowledge Manager  3 sources of knowledge: General, Domain-specific, User Profile  Contextualized Knowledge Base</p>
    <p>CxBR-based Dialog Manager  Goal recognition</p>
    <p>Inference Engine determines state of conversation for contextual relevance</p>
    <p>Goal management  Goal Stack keeps track of conversational goals</p>
    <p>Agent actions  Context Topology dictates conversational output using Inference</p>
    <p>Engine and Goal Stack</p>
  </div>
  <div class="page">
    <p>Relation to S&amp;T</p>
    <p>Perform functions done by humans  Mixed Reality Training, AAR, Concierges, Interrogator</p>
    <p>Training, Intelligent Tutoring Systems</p>
    <p>Multimedia Integration  hybrid speech and pointing interface with user  Text, graphics, links to clarifying information or documents</p>
    <p>Customizable environments support changing mission requirements  Characteristics (age, gender, appearance) can be varied</p>
    <p>independent of domain knowledge to match each trainee  Avatar can represent specific individual  CO, DI  Surroundings in which avatar occupies can be updated</p>
  </div>
  <div class="page">
    <p>Video</p>
    <p>&lt;insert new video here&gt;</p>
  </div>
  <div class="page">
    <p>Initial Evaluation</p>
    <p>Controlled experiment  30 diverse students from UIC  study displayed ten 30-second life-size videos of AlexAvatar  Videos paired to compare/contrast:</p>
    <p>Maintain eye contact, Head motion, Body motion, Pre-recorded vs computer generated Voice</p>
    <p>Baseline comparison to our previous avatar implementation from a year ago.</p>
    <p>Test subjects preferred by a significant margin:  Body movement compared to a still body  An avatar with purposeful motions incorporating motion-capture data</p>
    <p>from Alex  More detailed and realistic textures</p>
    <p>No clear preference in pre-recorded vs. computer-generated voice</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Currently on year two of three-year project  Current results are encouraging, several challenges</p>
    <p>still ahead  Focusing on:</p>
    <p>avatar handling conversation with more than one human,  Increasing capacity for understanding spoken language</p>
    <p>Expanding capabilities of dialog system,</p>
    <p>Evaluating effect of whiteboard feature.</p>
    <p>Expect to finish second edition of avatar soon for demo in early January at NSF conference</p>
    <p>Looking for partnerships with military applications</p>
  </div>
</Presentation>
