<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>MultiMulti--Level Memory Level Memory PrefetchingPrefetching forfor Media and Stream ProcessingMedia and Stream Processing</p>
    <p>Jason Fritts</p>
    <p>Assistant Professor Department of Computer Science,</p>
    <p>Washington University</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>IntroductionIntroduction  Multimedia is a dominant computer workload  Traditional cache-based memory systems not well-suited to</p>
    <p>multimedia data  cache memory designed for data with high temporal &amp; spatial locality  Multimedia data has high spatial locality by low temporal locality</p>
    <p>Growing processor-memory gap requires more efficient memory system for multimedia</p>
    <p>P ro c e s s o rP ro c e s s o r</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Prefetching FundamentalsPrefetching Fundamentals  Four aspects of prefetching</p>
    <p>Detection - how a memory access pattern is detected</p>
    <p>Pre-Pattern Detection</p>
    <p>Post-Pattern Detection  Synchronization - how &amp; when prefetch requests are issued  Storage - where prefetched data is stored</p>
    <p>Software Prefetching  Compiler statically determines predictable memory access patterns  Prefetch operation inserted into code for each individual prefetch request  Static Detection &amp; Static Synchronization</p>
    <p>Hardware Prefetching  Hardware unit dynamically determines predictable memory access patterns  Hardware unit dynamically issues prefetch requests  Dynamic Detection &amp; Dynamic Synchronization</p>
    <p>Hybrid Hardware/Software Prefetching  Compiler statically determines predictable memory access patterns  Prefetch operation inserted into code for each memory access pattern  Hardware table performs individual prefetch requests  Static Detection &amp; Dynamic Synchronization</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Prefetching TradeoffsPrefetching Tradeoffs  Software Prefetching</p>
    <p>Advantage: no hardware required  Disadvantage: increased code size</p>
    <p>static pattern detection</p>
    <p>Hardware Prefetching  Advantage: dynamic pattern detection  Disadvantage: cost of hardware (area, power, etc.)</p>
    <p>Hybrid Hardware/Software Prefetching  Advantage: little increase in code size  Disadvantage: some hardware needed</p>
    <p>static pattern detection</p>
    <p>z Pimentel et al. (IPCCC 2000) found:  Hybrid prefetching performed best on statically detectable patterns  Hardware prefetching performed best overall</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Streaming Prefetch EnginesStreaming Prefetch Engines</p>
    <p>L 2 C a c h e o r M a in M e m o ry</p>
    <p>L 1 D a ta C a c h e</p>
    <p>P ro c e s s o r</p>
    <p>L 2 C a c h e o r M a in M e m o r y</p>
    <p>S tr e a m C a c h e</p>
    <p>P ro c e s s o r</p>
    <p>L 1 D a ta C a c h e</p>
    <p>S P T</p>
    <p>In s tr A d d r L a s t M e m A d d r S trid e S ta te Va lid</p>
    <p>P C o r L A -P C</p>
    <p>+</p>
    <p>E ffe c tiv e A d d re s s</p>
    <p>P re fe tc h A d d re s s</p>
    <p>Stride Prediction Table (SPT)</p>
    <p>Stream Buffers Stream Cache</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Issues in Hardware PrefetchingIssues in Hardware Prefetching</p>
    <p>Hardware prefetching usually occurs on-chip  In parallel or series w/ L1 cache  Prefetching performance usually best closest to L1 cache</p>
    <p>Prefetching increases bus bandwidth requirements by 30-100%</p>
    <p>Prefetch time varies in multi-level memory hierarchies</p>
    <p>Growing processor-memory gap requires aggressive prefetching</p>
    <p>Result: Need more aggressive &amp; more efficient prefetching that:  Minimizes extra bus bandwidth  Supports longer memory latencies  Supports variable prefetch distance</p>
    <p>Multi-Level Memory Prefetching</p>
    <p>Adaptive Prefetching</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>More Aggressive PrefetchingMore Aggressive Prefetching</p>
    <p>t</p>
    <p>p ( t)</p>
    <p>Prefetch accuracy decreases with increasing prefetch distance</p>
    <p>More aggressive prefetching = prefetching further ahead</p>
    <p>To achieve same amount of useful data, prefetching further ahead in time requires prefetching more data</p>
    <p>Prefetch Accuracy vs. Prefetch Distance</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>MultiMulti--Level Memory Prefetch Level Memory Prefetch HierarchyHierarchy</p>
    <p>D a ta p a th</p>
    <p>L 2 C a c h e</p>
    <p>L 1 In s tr.</p>
    <p>C a c h e</p>
    <p>L 1 D a ta</p>
    <p>C a c h e</p>
    <p>D R A M In te r fa c e</p>
    <p>B u s In te r fa c e</p>
    <p>B u s In te r fa c e</p>
    <p>M e m o r y</p>
    <p>P ro c e s s o r</p>
    <p>M e m o r y C o n tro lle r</p>
    <p>M e m o r y B u s</p>
    <p>D R A M P re fe tc h</p>
    <p>U n it</p>
    <p>L 1 P re fe tc h</p>
    <p>U n it</p>
    <p>L 2 P re fe tc h</p>
    <p>U n it</p>
    <p>Conservative On-Chip Prefetching  Prefetch short distance ahead</p>
    <p>Aggressive Off-Chip Prefetching  Prefetch long distance ahead</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Base Architecture ModelBase Architecture Model  Architecture model</p>
    <p>4-issue media processor  1 GHz processor frequency</p>
    <p>L1 Cache  32 KB direct-mapped L1 data</p>
    <p>cache w/ 64 byte lines  16 KB direct-mapped L1 instruction</p>
    <p>cache w/ 256 byte lines</p>
    <p>On-Chip L2 Cache  256 KB 4-way set associate</p>
    <p>w/ 64 byte lines</p>
    <p>External Memory  64-bit processor-memory bus  8:1 processor-to-memory frequency ratio</p>
    <p>Memory Prefetching  8-way stream buffers</p>
    <p>5 entries per stream at L1 level</p>
    <p>3 entries per stream at L2 and MC levels</p>
    <p>L1 Data</p>
    <p>Cache</p>
    <p>Datapath</p>
    <p>L2 Cache</p>
    <p>L1 Instr</p>
    <p>Cache</p>
    <p>External bus</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Initial ResultsInitial Results</p>
    <p>jp g2</p>
    <p>K de</p>
    <p>c</p>
    <p>m pe</p>
    <p>g4 de</p>
    <p>c</p>
    <p>pe gw</p>
    <p>itd ec</p>
    <p>un ep</p>
    <p>ic</p>
    <p>vo rb</p>
    <p>is en</p>
    <p>c</p>
    <p>Ap p lic a tio n</p>
    <p>In s</p>
    <p>tr u</p>
    <p>c ti</p>
    <p>o n</p>
    <p>s P</p>
    <p>e r</p>
    <p>C y</p>
    <p>c le</p>
    <p>( IP</p>
    <p>) b a s e L 1 L 2 M C L 1 + L 2 L 1 + M C L 1 + L 2 + M C</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Summary of ResultsSummary of Results</p>
    <p>Compared single-level prefetching vs. multi-level prefetching  8-way stream buffer w/ 5 entries at L1 level  8-way stream buffer w/ 3 entries at L2 and MC levels</p>
    <p>Average speedups for single-level prefetching  45% for L1 prefetching  34% for L2 prefetching  24% for MC prefetching</p>
    <p>Average speedups for multi-level prefetching  66% for L1+L2 prefetching  84% for L1+MC prefetching  89% for L1+L2+MC prefetching</p>
    <p>L1+L2+MC has 89% speedup, but excess bandwidth of ~100%  L1+MC has 84% speedup, but excess bandwidth of ~50%</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Summary and ConclusionsSummary and Conclusions</p>
    <p>Proposed multi-level memory prefetching for bandwidth-efficient aggressive prefetching of streaming data</p>
    <p>Perform conservative prefetching on-chip and aggressive prefetching off-chip  Designed to:</p>
    <p>provide aggressive prefetching (i.e. enable prefetching further ahead in time)</p>
    <p>reduce extra bandwidth consumption from prefetching</p>
    <p>Multi-level memory prefetching provides valuable benefits  Nearly twice the speedup</p>
    <p>off-chip prefetching aggressively tackles growing processor-memory gap  Half the extra bandwidth consumption</p>
    <p>conservatively prefetching on-chip minimizes extra bandwidth consumption on system bus  Prefetching at L1 and MC levels identified as best overall prefetching method</p>
    <p>Definitive conclusions require more thorough evaluation  Test different prefetching methods  Explore variety of parameters (including varying latencies)  Examine implicit vs. explicit control/communication between prefetch levels  Study impact of multi-level memory prefetching on general-purpose workloads</p>
  </div>
  <div class="page">
    <p>ICME 2002 August 28, 2002</p>
    <p>Future WorkFuture Work  Adaptive Prefetching</p>
    <p>Three adaptive methods:</p>
    <p>Adaptive separation of predictable vs. demand-based data  allows for separate, smaller and faster memory structures for different data types</p>
    <p>Adapting post-pattern prefetch distance</p>
    <p>Adapting pre-pattern prefetch distance  Examine effectiveness of three adaptive prefetching methods  Will prefetch distances achieve steady-state or constantly fluctuate?  How well does separation of predictable vs. demand-based data work?</p>
    <p>Too much crossover will decrease performance</p>
    <p>How much smaller can each memory structure be made?</p>
    <p>Examine variety of performance metrics:  Speedup  Bandwidth  Power  Miss Ratio &amp; Miss CPI  Tradeoffs in Size/Area of Hardware Prefetch Units</p>
  </div>
</Presentation>
