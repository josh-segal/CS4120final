<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Flexplane: An Experimenta0on Pla3orm for Resource Management in Datacenters</p>
    <p>Amy Ousterhout, Jonathan Perry, Hari Balakrishnan, Petr Lapukhov</p>
  </div>
  <div class="page">
    <p>Datacenter Networks  Applica0ons have diverse requirements  Dozens of new resource management schemes  Low latency: DCTCP  Min FCT: PDQ, RCP, pFabric, PERC  Deadlines: D3, D2TCP</p>
    <p>Difficult to experiment with schemes in real networks  Requires changes to hardware routers</p>
  </div>
  <div class="page">
    <p>Experimenta0on with Resource Management</p>
    <p>Experimenta0on in real networks  SoSware routers - limited throughput  Programmable hardware - limited flexibility</p>
    <p>By Altera Corpora0on - Altera Corpora0on, CC BY 3.0</p>
  </div>
  <div class="page">
    <p>Experimenta0on with Resource Management</p>
    <p>Experimenta0on in simula0on (e.g., ns, OMNeT++)  Does not accurately model real network stacks, NICs, and distributed applica0ons</p>
    <p>Does not run in real 0me</p>
    <p>No exis0ng approach to experimenta0on provides accuracy, flexibility, and high throughput</p>
  </div>
  <div class="page">
    <p>Our Contribu0ons  Key idea: whole-network emula0on  Flexplane: a pla3orm for faithful experimenta0on with resource management schemes  Accurate  predicts behavior of hardware  Flexible  express schemes in C++  High throughput  761 Gbits/s</p>
  </div>
  <div class="page">
    <p>Approach: Whole-Network Emula0on Real Network</p>
    <p>class MyScheduler {}</p>
    <p>Emulated Network</p>
    <p>Emulator class MyAQM {}</p>
  </div>
  <div class="page">
    <p>Abstract Packets  Resource management schemes are data- independent</p>
    <p>Concise representa0on of one MTU  Source, des0na0on, flow, ID  Custom per-scheme fields</p>
  </div>
  <div class="page">
    <p>Emulator  Real-0me network simulator  Faster than standard network simulators  Time divided into abstract-packet-sized 0meslots  Omits endpoint soSware</p>
    <p>Real Network</p>
    <p>Emulated Network</p>
    <p>Emulator</p>
  </div>
  <div class="page">
    <p>Accuracy  Goal: predict behavior of a hardware network  Hardware latency: unloaded delay + queuing delay  Added latency of Flexplane:  RTT to emulator  Unloaded delay  Queuing delay in real network</p>
    <p>Real Network</p>
    <p>Emulated Network</p>
    <p>Emulator</p>
  </div>
  <div class="page">
    <p>Flexplane API  Decouples schemes from framework</p>
    <p>incoming packets</p>
    <p>outgoing packets</p>
    <p>route classify enqueue schedule</p>
    <p>Emulator</p>
    <p>int route(AbstractPkt *pkt) int classify(AbstractPkt *pkt, int port) enqueue(AbstractPkt *pkt, int port, int queue) AbstractPkt *schedule(int output_port)</p>
    <p>Emulator</p>
    <p>int route(AbstractPkt *pkt) int classify(AbstractPkt *pkt, int port) enqueue(AbstractPkt *pkt, int port, int queue) AbstractPkt *schedule(int output_port)</p>
    <p>Endpoints prepare_request(sk_buff *skb, char *request_data) prepare_to_send(sk_buff *skb, char *alloca0on_data)</p>
  </div>
  <div class="page">
    <p>Mul0core Emulator Architecture  Pin network components (routers, endpoints) to cores</p>
    <p>Communica0on via FIFO queues  Router state not shared across cores</p>
    <p>aggrega0on</p>
    <p>endpoints</p>
    <p>ToR CPU core</p>
  </div>
  <div class="page">
    <p>Implementa0on  Emulator uses Intel DPDK for low-latency NIC access  Endpoints run a Linux qdisc</p>
  </div>
  <div class="page">
    <p>Evalua0on  Accuracy  U0lity  Emulator throughput</p>
  </div>
  <div class="page">
    <p>Flexplane is Accurate  Bulk TCP: 5 senders, 1 receiver  Throughput 9.2-9.3 Gbits/s vs. 9.4 Gbits/s in hardware</p>
    <p>Similar queue occupancies by two. We run DropTail both in Flexplane and on the hardware switch.</p>
    <p>The results in Figure 3 demonstrate that the per-packet latency overhead of Flexplane is modest. Under the lightest offered load we measure (10,000 packets/s), the median latency in Flexplane is 33.8 s, compared to 14.9 s on hardware. As the load increases, the latency in Flexplane increases slightly due to the additional load on the kernel module in the sending endpoint. Flexplane is unable to meet the highest offered load (6 Gbits/s), because of the CPU overhead of the kernel module. Note that stateof-the-art software routers add latencies of the same order of magnitude for each hop, even without the added roundtrip time to an off-path emulator: 47.6-66.4 s [21] for a CPU-based software router; 30 s [31] or 140-260 s [27] for GPU-based software routers.</p>
    <p>Throughput. Next we evaluate accuracy for bulk-transfer TCP, using network-level metrics: throughput and innetwork queueing. In each experiment, five machines send TCP traffic at maximum throughput to one receiver.</p>
    <p>We compare Flexplane to hardware for three schemes that our router supports: TCP-cubic/DropTail, TCPcubic/RED, and DCTCP. We configure the hardware router and the emulator using the same parameters for each scheme. For DropTail we use a static per-port buffer size of 1024 MTUs. For RED, we use min th=150, max th=300, max p=0.1, and weight=5. For DCTCP, we use an ECN-marking threshold of 65 MTUs, as recommended by its designers [10].</p>
    <p>Flexplane achieves similar aggregate throughput as the hardware. All three schemes consistently saturate the bottleneck link, achieving an aggregate throughput of 9.4 Gbits/s in hardware, compared to 9.2-9.3 Gbits/s in Flexplane. This 1-2% difference in throughput is due to bandwidth allocated for abstract packets in Flexplane.</p>
    <p>Queueing. During the experiment described above, we sample the total buffer occupancy in the hardware router every millisecond, and the emulator logs the occupancy of each emulated port at the same frequency.</p>
    <p>Table 2 shows that Flexplane maintains similar queue occupancies as the hardware schemes. For DropTail it maintains high occupancies (close to the max of 1024) with large variations in occupancy, while for the other two schemes the occupancies are lower and more consistent. Flexplane does differ from hardware in that its occupancies tend to be slightly lower and to display more variation. We believe this is due to the effectively longer RTT in Flexplane. When the congestion window is reduced, the pause before sending again is longer in Flexplane, allowing the queues to drain more.</p>
    <p>During the Flexplane experiments, the hardware queue sizes remain small: the mean is 7-10 MTUs and the 95th percentile is 14-22 MTUs. These numbers are small com</p>
    <p>Median Queue Occupancies (MTUs)</p>
    <p>Hardware Flexplane DropTail 931 837</p>
    <p>RED 138 104 DCTCP 61 51</p>
    <p>Table 2: Flexplane achieves similar queue occupancies and standard deviations in occupancies (s ) as hardware.</p>
    <p>pared to the queue sizes in the emulator or in the hardware queues during the hardware experiments, and indicate that queueing in the hardware network does not significantly impact the accuracy of Flexplane (3.4). Flow completion time. Next we evaluate Flexplanes accuracy at the application level in terms of flow completion time (FCT). We run an RPC-based application in which four clients repeatedly request data from 32 servers. The size of the requested data is determined by an empirical workload derived from live traffic in a production datacenter that supports web search (first presented in [10]). It includes a mixture of flows of different sizes. 53% of the flows are small flows of less than 100KB, but 37% of the bytes come from large flows of 10MB or larger. Request times are chosen by a Poisson process such that the clients receive a specified load between 10% and 80%. We normalize the FCT for each flow to the average FCT achieved by a flow of the same size, in an unloaded network, when flows are requested continuously.</p>
    <p>We run this application for DropTail and DCTCP, in Flexplane and in the hardware network. Figure 4 shows the average normalized FCTs. For both small flows and large flows, results with Flexplane closely match results obtained with a hardware network. For loads up to 60% with both schemes, Flexplane estimates average normalized FCTs of hardware to within 2-8% for small flows and 3-14% for large flows. Accuracy decreases slightly for higher loads of 70% and 80%, but remains within 18% for small flows and 24% for large flows.</p>
  </div>
  <div class="page">
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>Flexplane Hardware</p>
    <p>(a) DropTail, (0, 100KB]</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(b) DCTCP, (0, 100KB]</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(c) DropTail, (10MB, )</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(d) DCTCP, (10MB, )</p>
    <p>Figure 4: Flexplane closely matches the average normalized flow completion times of hardware for DropTail and DCTCP. The left two graphs show results for small flow sizes; the right two graphs show results for large flow sizes.</p>
    <p>class PriorityScheduler : public Scheduler {</p>
    <p>public:</p>
    <p>AbstractPkt</p>
    <p>*</p>
    <p>PriorityScheduler::schedule(uint32_t</p>
    <p>port) {</p>
    <p>/</p>
    <p>*</p>
    <p>get the mask of non-empty queues</p>
    <p>*</p>
    <p>/</p>
    <p>uint64_t mask = m_bank-&gt;non_empty_qmask(port);</p>
    <p>uint64_t q_index;</p>
    <p>/</p>
    <p>*</p>
    <p>bsfq: find the first set bit in mask</p>
    <p>*</p>
    <p>/</p>
    <p>asm(&quot;bsfq %1,%0&quot;:&quot;=r&quot;(q_index):&quot;r&quot;(mask));</p>
    <p>return m_bank-&gt;dequeue(port, q_index);</p>
    <p>}</p>
    <p>private:</p>
    <p>PacketQueueBank</p>
    <p>*</p>
    <p>m_bank;</p>
    <p>}</p>
    <p>Figure 5: Source code for a priority scheduler in Flexplane over  64 queues. A PacketQueueBank stores packets between the calls to enqueue and schedule.</p>
    <p>scheme LOC drop tail queue manager 39</p>
    <p>RED queue manager 125 DCTCP queue manager 43</p>
    <p>priority queueing scheduler 29 round robin scheduler 40</p>
    <p>HULL scheduler 60 pFabric QM, queues, scheduler 251</p>
    <p>Table 3: Lines of code (LOC) in the emulator for each resource management scheme.</p>
    <p>works. For example, the authors of HULL [11] conducted evaluations using a testbed with 1 Gbits/s links; we use Flexplane to tune HULLs parameters to fit our 10 Gbits/s network. We use the recommended phantom queue drain rate of 95% of the link speed (9.5 Gbits/s). The HULL authors use a 1 KB marking threshold in a 1 Gbits/s network, and suggest a marking threshold of 3-5 KB for 10 Gbits/s links. We found, however, that throughput degraded significantly with a 3 KB marking threshold, achieving only 5 Gbits/s total with four concurrent flows. We therefore increased the marking threshold until our achieved throughput was 92% of the drain rate (this is what [11] achieves with their parameters); the resulting threshold is 15 KB. Flexplane helped us conduct this parameter search quickly and effectively. Evaluating trade-offs. In this example, we demonstrate how one might use Flexplane to evaluate the performance</p>
    <p>better</p>
    <p>HULL</p>
    <p>DropTail</p>
    <p>Priority Queueing250</p>
    <p>H ig</p>
    <p>h pr</p>
    <p>io rit</p>
    <p>y FC</p>
    <p>T (u</p>
    <p>s)</p>
    <p>DCTCP DropTail HULL pFabric</p>
    <p>Priority Queueing RED Round Robin</p>
    <p>Figure 6: Flexplane enables users to explore trade-offs between different schemes. Large points show averages over the entire experiment, faded points show averages over 1s, and ellipses show one standard deviation. Note the flipped axes.</p>
    <p>of a specific application with different resource management schemes. We do not argue that any scheme is better than any other, but instead demonstrate that there are tradeoffs between different schemes (as described in [47]), and that Flexplane can help users explore these trade-offs.</p>
    <p>We use an RPC-based workload and consider the tradeoff that schemes make between performance for short flows and performance for long flows. In the experiment, four clients repeatedly request data from 32 servers. 80% of the requests are short 1.5 KB high priority requests, while the remaining 20% are 10 Mbyte low priority requests. Request times are chosen by a Poisson process such that the client NICs are receiving at about 60% of their maximum throughput. We evaluate the schemes discussed in 5.1, as well as TCP-cubic/per-flow-DRR, TCP-cubic/priority-queueing, HULL, and pFabric.</p>
    <p>Figure 6 shows the trade-off each scheme makes on this workload. With DropTail, large queues build up in the network, leading to high flow completion times for the high-priority requests. However, DropTail senders rarely cut back their sending rates and therefore achieve good FCTs for the long requests. At the other end of the spectrum, HULLs phantom queues cause senders to decrease their sending rates early, leading to unutilized bandwidth and worse performance for the low priority flows; the high priority flows achieve relatively good performance because they encounter little queueing in the network.</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>Flexplane Hardware</p>
    <p>(a) DropTail, (0, 100KB]</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(b) DCTCP, (0, 100KB]</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(c) DropTail, (10MB, )</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(d) DCTCP, (10MB, )</p>
    <p>Figure 4: Flexplane closely matches the average normalized flow completion times of hardware for DropTail and DCTCP. The left two graphs show results for small flow sizes; the right two graphs show results for large flow sizes.</p>
    <p>class PriorityScheduler : public Scheduler {</p>
    <p>public:</p>
    <p>AbstractPkt</p>
    <p>*</p>
    <p>PriorityScheduler::schedule(uint32_t</p>
    <p>port) {</p>
    <p>/</p>
    <p>*</p>
    <p>get the mask of non-empty queues</p>
    <p>*</p>
    <p>/</p>
    <p>uint64_t mask = m_bank-&gt;non_empty_qmask(port);</p>
    <p>uint64_t q_index;</p>
    <p>/</p>
    <p>*</p>
    <p>bsfq: find the first set bit in mask</p>
    <p>*</p>
    <p>/</p>
    <p>asm(&quot;bsfq %1,%0&quot;:&quot;=r&quot;(q_index):&quot;r&quot;(mask));</p>
    <p>return m_bank-&gt;dequeue(port, q_index);</p>
    <p>}</p>
    <p>private:</p>
    <p>PacketQueueBank</p>
    <p>*</p>
    <p>m_bank;</p>
    <p>}</p>
    <p>Figure 5: Source code for a priority scheduler in Flexplane over  64 queues. A PacketQueueBank stores packets between the calls to enqueue and schedule.</p>
    <p>scheme LOC drop tail queue manager 39</p>
    <p>RED queue manager 125 DCTCP queue manager 43</p>
    <p>priority queueing scheduler 29 round robin scheduler 40</p>
    <p>HULL scheduler 60 pFabric QM, queues, scheduler 251</p>
    <p>Table 3: Lines of code (LOC) in the emulator for each resource management scheme.</p>
    <p>works. For example, the authors of HULL [11] conducted evaluations using a testbed with 1 Gbits/s links; we use Flexplane to tune HULLs parameters to fit our 10 Gbits/s network. We use the recommended phantom queue drain rate of 95% of the link speed (9.5 Gbits/s). The HULL authors use a 1 KB marking threshold in a 1 Gbits/s network, and suggest a marking threshold of 3-5 KB for 10 Gbits/s links. We found, however, that throughput degraded significantly with a 3 KB marking threshold, achieving only 5 Gbits/s total with four concurrent flows. We therefore increased the marking threshold until our achieved throughput was 92% of the drain rate (this is what [11] achieves with their parameters); the resulting threshold is 15 KB. Flexplane helped us conduct this parameter search quickly and effectively. Evaluating trade-offs. In this example, we demonstrate how one might use Flexplane to evaluate the performance</p>
    <p>better</p>
    <p>HULL</p>
    <p>DropTail</p>
    <p>Priority Queueing250</p>
    <p>H ig</p>
    <p>h pr</p>
    <p>io rit</p>
    <p>y FC</p>
    <p>T (u</p>
    <p>s)</p>
    <p>DCTCP DropTail HULL pFabric</p>
    <p>Priority Queueing RED Round Robin</p>
    <p>Figure 6: Flexplane enables users to explore trade-offs between different schemes. Large points show averages over the entire experiment, faded points show averages over 1s, and ellipses show one standard deviation. Note the flipped axes.</p>
    <p>of a specific application with different resource management schemes. We do not argue that any scheme is better than any other, but instead demonstrate that there are tradeoffs between different schemes (as described in [47]), and that Flexplane can help users explore these trade-offs.</p>
    <p>We use an RPC-based workload and consider the tradeoff that schemes make between performance for short flows and performance for long flows. In the experiment, four clients repeatedly request data from 32 servers. 80% of the requests are short 1.5 KB high priority requests, while the remaining 20% are 10 Mbyte low priority requests. Request times are chosen by a Poisson process such that the client NICs are receiving at about 60% of their maximum throughput. We evaluate the schemes discussed in 5.1, as well as TCP-cubic/per-flow-DRR, TCP-cubic/priority-queueing, HULL, and pFabric.</p>
    <p>Figure 6 shows the trade-off each scheme makes on this workload. With DropTail, large queues build up in the network, leading to high flow completion times for the high-priority requests. However, DropTail senders rarely cut back their sending rates and therefore achieve good FCTs for the long requests. At the other end of the spectrum, HULLs phantom queues cause senders to decrease their sending rates early, leading to unutilized bandwidth and worse performance for the low priority flows; the high priority flows achieve relatively good performance because they encounter little queueing in the network.</p>
    <p>DCTCP (0, 100KB]</p>
    <p>DCTCP (10MB, )</p>
    <p>Av er</p>
    <p>ag e</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>Flexplane Hardware</p>
    <p>RPC web search workload  Accurate to within 2-14% for loads up to 60%  Observe behavior not visible in simula0ons</p>
    <p>Flexplane is Accurate</p>
  </div>
  <div class="page">
    <p>Queue Occupancies (MTUs) Hardware Flexplane</p>
    <p>median s median s DropTail 931 73.7 837 98.6</p>
    <p>RED 138 12.9 104 32.5 DCTCP 61 4.9 51 13.0</p>
    <p>Table 2: Flexplane achieves similar queue occupancies and standard deviations in occupancies (s ) as hardware.</p>
    <p>use an ECN-marking threshold of 65 MTUs, as recommended by its designers [8].</p>
    <p>Flexplane achieves similar aggregate throughput as the hardware. All three schemes consistently saturate the bottleneck link, achieving an aggregate throughput of 9.4 Gbits/s in hardware, compared to 9.2-9.3 Gbits/s in Flexplane. This 1-2% difference in throughput is due to bandwidth allocated for abstract packets in Flexplane. Queueing. During the experiment described above, we sample the total buffer occupancy in the hardware router every millisecond, and the emulator logs the occupancy of each emulated port at the same frequency.</p>
    <p>Table 2 shows that Flexplane maintains similar queue occupancies as the hardware schemes. For DropTail it maintains high occupancies (close to the max of 1024) with large variations in occupancy, while for the other two schemes the occupancies are lower and more consistent. Flexplane does differ from hardware in that its occupancies tend to be slightly lower and to display more variation. We believe this is due to the effectively longer RTT in Flexplane. When the congestion window is reduced, the pause before sending again is longer in Flexplane, allowing the queues to drain more.</p>
    <p>During the Flexplane experiments, the hardware queue sizes remain small: the mean is 7-10 MTUs and the 95th percentile is 14-22 MTUs. These numbers are small compared to the queue sizes in the emulator or in the hardware queues during the hardware experiments, and indicate that queueing in the hardware network does not significantly impact the accuracy of Flexplane (3.4). Flow completion time. Next we evaluate Flexplanes accuracy at the application level in terms of flow completion time (FCT). We run an RPC-based application in which four clients repeatedly request data from 32 servers. The size of the requested data is determined by an empirical workload derived from live traffic in a production datacenter that supports web search (first presented in [8]). It includes a mixture of flows of different sizes. 53% of the flows are small flows of less than 100KB, but 37% of the bytes come from large flows of 10MB or larger. Request times are chosen by a Poisson process such that the clients receive a specified load between 10% and 80%. We normalize the FCT for each flow to the average FCT achieved by a flow of the same size, in an unloaded network, when flows are requested continuously.</p>
    <p>class PriorityScheduler : public Scheduler {</p>
    <p>public:</p>
    <p>AbstractPkt</p>
    <p>*</p>
    <p>PriorityScheduler::schedule(uint32_t</p>
    <p>port) {</p>
    <p>/</p>
    <p>*</p>
    <p>get the mask of non-empty queues</p>
    <p>*</p>
    <p>/</p>
    <p>uint64_t mask = m_bank-&gt;non_empty_qmask(port);</p>
    <p>uint64_t q_index;</p>
    <p>/</p>
    <p>*</p>
    <p>bsfq: find the first set bit in mask</p>
    <p>*</p>
    <p>/</p>
    <p>asm(&quot;bsfq %1,%0&quot;:&quot;=r&quot;(q_index):&quot;r&quot;(mask));</p>
    <p>return m_bank-&gt;dequeue(port, q_index);</p>
    <p>}</p>
    <p>private:</p>
    <p>PacketQueueBank</p>
    <p>*</p>
    <p>m_bank;</p>
    <p>}</p>
    <p>Figure 5: Source code for a priority scheduler in Flexplane over  64 queues. A PacketQueueBank stores packets between the calls to enqueue and schedule.</p>
    <p>scheme LOC drop tail queue manager 39</p>
    <p>RED queue manager 125 DCTCP queue manager 43</p>
    <p>priority queueing scheduler 29 round robin scheduler 40</p>
    <p>HULL scheduler 60 pFabric QM, queues, scheduler 251</p>
    <p>Table 3: Lines of code (LOC) in the emulator for each resource management scheme.</p>
    <p>We run this application for DropTail and DCTCP, in Flexplane and in the hardware network. Figure 4 shows the average normalized FCTs. For both small flows and large flows, results with Flexplane closely match results obtained with a hardware network. For loads up to 60% with both schemes, Flexplane estimates average normalized FCTs of hardware to within 2-8% for small flows and 3-14% for large flows. Accuracy decreases slightly for higher loads of 70% and 80%, but remains within 18% for small flows and 24% for large flows.</p>
    <p>Flexplane is Easy to Use  Implemented several schemes in dozens of lines of code</p>
  </div>
  <div class="page">
    <p>Flexplane Enables Experimenta0on  Evalua0ng trade-offs between resource management schemes</p>
    <p>better</p>
    <p>HULL</p>
    <p>DropTail</p>
    <p>Priority Queueing250</p>
    <p>H ig</p>
    <p>h pr</p>
    <p>io rit</p>
    <p>y FC</p>
    <p>T (u</p>
    <p>s)</p>
    <p>DCTCP DropTail HULL pFabric</p>
    <p>Priority Queueing RED Round Robin</p>
  </div>
  <div class="page">
    <p>Flexplane Enables Experimenta0on  Experiment with real distributed applica0ons such as Spark</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>DCTCP HULL pFabric TCPDropTail</p>
    <p>(a) (0, 100KB]: Average</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(b) (0, 100KB]: 99th percentile</p>
    <p>N or</p>
    <p>m al</p>
    <p>iz ed</p>
    <p>F C</p>
    <p>T</p>
    <p>(c) (10MB, ): Average</p>
    <p>Figure 7: Normalized flow completion times for the web search workload, for four different schemes run in Flexplane. Note the different y axes.</p>
    <p>Priority queueing performs well on this simple workload, achieving good performance for both flow types. A network operator could use these results to determine what scheme to run in their network, depending on how they value performance of high priority flows relative to low priority flows.</p>
    <p>Real applications. In addition to enabling experimentation with network-bound workloads like the one above, Flexplane enables users to evaluate the performance impact of different resource management schemes on real applications whose performance depends on both network and computational resources. We consider two applications that perform distributed computations using Spark [1]. The first uses block coordinate descent [2] to compute the optimal solution to a least squares problem; this is a staple of many machine learning tasks. The second performs an in-memory sort [4]. For this experiment, we use a small cluster of 9 machines (1 master and 8 workers), each with 8 cores, connected via a single switch with 1 Gbit/s links. We use Flexplane to run each application with DropTail, DCTCP, and HULL.</p>
    <p>Table 4 shows that different Spark applications are affected in different ways by a change in resource management scheme. The sort application, which includes multiple waves of small tasks and small data transfers, shows small improvements in completion time, relative to DropTail, when run with DCTCP or HULL. In contrast, coordinate descent takes 4.4% longer to complete when run with DCTCP, and 29.4% longer when run with HULL. This is because this application sends data in a small number of bulk transfers whose throughput is degraded by HULLs, and to a lesser extent DCTCPs, more aggressive responses to congestion. Flexplane enabled us to quickly evaluate the impact of a change in resource management scheme on these real-world applications. Because these applications spend much of their time performing computation (&gt;75%), it is not possible to accurately conduct this experiment in a network simulator today.</p>
    <p>Reproducible research. Here we demonstrate how ex</p>
    <p>% Change in Completion Time Relative to DropTail</p>
    <p>Coordinate descent Sort DCTCP +4.4% -4.8% HULL +29.4% -2.6%</p>
    <p>Table 4: Percent change in completion time of two Spark applications when run with DCTCP or HULL, relative to when run with DropTail.</p>
    <p>periments that researchers conducted in simulation in the past can be conducted on a real network with Flexplane, and how results in a real network might differ from those in simulation. To do so, we recreate an experiment that has been conducted in several other research papers [12, 14, 26]. We use the same network configuration and workload as in the flow completion time experiment in 5.1; this is the same workload used in prior work.</p>
    <p>Figure 7 shows the results of running this workload for DropTail, DCTCP, HULL, and pFabric, in Flexplane, at loads ranging from 10% to 80%. We present the average and 99th percentile normalized flow completion time for small flows, and the average normalized flow completion time for large flows, as in prior work.</p>
    <p>We observe the same general trends as in prior work. For the small flows, DropTail performs the worst, with performance degrading significantly at the highest loads and at the 99th percentile. In contrast, pFabric maintains good performance for small flows, even at high load and at the tail. For large flows, DCTCP and DropTail maintain the best performance, while HULL and pFabric degrade significantly at loads of 70%-80%. For HULL, this is because the required bandwidth headroom begins to significantly limit large flows. For pFabric, performance degrades at high load because short queues cause many packets to be dropped. This may be exacerbated by the fact that we do not use all TCP modifications at the endpoints, including the probe mode (which is particularly important at high load).</p>
    <p>Our results demonstrate an unexpected phenomenon. One would expect that under low load (e.g., 10%),</p>
    <p>Performance depends on network and CPU</p>
  </div>
  <div class="page">
    <p>R ou</p>
    <p>te r t</p>
    <p>hr ou</p>
    <p>gh pu</p>
    <p>t ( G</p>
    <p>bi ts</p>
    <p>/s )</p>
    <p>racks connected with Agg</p>
    <p>Emulator Throughput  Emulator provides 761 Gbits/s of aggregate throughput with 10 total cores</p>
    <p>R ou</p>
    <p>te r t</p>
    <p>hr ou</p>
    <p>gh pu</p>
    <p>t ( G</p>
    <p>bi ts</p>
    <p>/s )</p>
    <p>racks connected with Agg</p>
    <p>isolated racks</p>
    <p>R ou</p>
    <p>te r t</p>
    <p>hr ou</p>
    <p>gh pu</p>
    <p>t ( G</p>
    <p>bi ts</p>
    <p>/s )</p>
    <p>racks connected with Agg</p>
    <p>isolated racks</p>
    <p>81x as much throughput per clock cycle as RouteBricks</p>
  </div>
  <div class="page">
    <p>Flexplane: an Experimenta0on Pla3orm</p>
    <p>Whole-network emula0on  Flexplane: a pla3orm for faithful experimenta0on with resource management schemes  Accuracy, flexibility, and high throughput</p>
    <p>hqps://github.com/aousterh/flexplane</p>
  </div>
</Presentation>
