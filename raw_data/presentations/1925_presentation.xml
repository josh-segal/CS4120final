<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>PMI: A Scalable Process-Management Interface for ExtremeScale Systems Pavan Balaji, Darius Buntinas, David Goodell, William Gropp, Jayesh Krishna, Ewing Lusk and Rajeev Thakur</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Process management is integral part of HPC  Scalability and performance are critical  Close interaction between process management</p>
    <p>and parallel library (e.g., MPI) is important  Need not be integrated</p>
    <p>Separation allows  Independent development and improvement  Parallel libraries portable to different</p>
    <p>environments</p>
  </div>
  <div class="page">
    <p>PMI</p>
    <p>Generic process management interface for parallel applications</p>
    <p>PMI-1 is widely used  MVAPICH, Intel MPI, Microsoft MPI  SLURM, OSC mpiexec, OSU mpirun</p>
    <p>Introducing PMI-2  Improved scalability  Better interaction with hybrid MPI+threads</p>
  </div>
  <div class="page">
    <p>PMI Functionality</p>
    <p>Process management  Launch and monitoring</p>
    <p>Initial job  Dynamic processes</p>
    <p>Process control  Information exchange</p>
    <p>Contact information  Environmental attributes</p>
  </div>
  <div class="page">
    <p>System Model</p>
  </div>
  <div class="page">
    <p>System Model</p>
    <p>MPICH2MPICH2 MVAPICH2MVAPICH2 Intel-MPIIntel-MPI SCX-MPISCX-MPI Microsoft MPI</p>
    <p>Microsoft MPI</p>
    <p>Simple PMI</p>
    <p>Simple PMI</p>
    <p>SMPD PMI</p>
    <p>SMPD PMI</p>
    <p>SLURM PMI</p>
    <p>SLURM PMI</p>
    <p>BG/L PMIBG/L PMI</p>
    <p>HydraHydra MPDMPD SLURMSLURM SMPDSMPD OSC mpiexec</p>
    <p>OSC mpiexec</p>
    <p>OSU mpirun</p>
    <p>OSU mpirun</p>
    <p>Communication Subsystem</p>
    <p>Communication Subsystem</p>
    <p>MPI Library</p>
    <p>PMI API</p>
    <p>PMI Library</p>
    <p>Process Manager</p>
  </div>
  <div class="page">
    <p>Process Manager</p>
    <p>Handles  Process launch</p>
    <p>Start and stop processes  Forwarding I/O and signals</p>
    <p>Information exchange  Contact information to set up communication</p>
    <p>Implementation  May be separate components  May be distributed</p>
    <p>E.g., PBS, Sun Grid Engine, SSH 7</p>
  </div>
  <div class="page">
    <p>PMI Library</p>
    <p>Provides interface between parallel library and process manager</p>
    <p>Can be system specific  E.g, BG/L uses system specific features</p>
    <p>Wire protocol between PMI library and PM  PMI-1 and PMI-2 have specified wire protocols  Allows PMI lib to be used with different PM  Note: wire protocol and PMI API are separate</p>
    <p>entities  PMI implementation need not have wire protocol</p>
  </div>
  <div class="page">
    <p>PMI API</p>
    <p>PMI-1 and PMI-2  Functions associated with</p>
    <p>Initialization and finalization  Init, Finalize, Abort</p>
    <p>Information exchange  Put, Get, Fence</p>
    <p>Process creation  Spawn</p>
  </div>
  <div class="page">
    <p>Information Exchange</p>
    <p>Processes need to exchange connection info  PMI uses a Key-Value database (KVS)  At init, processes Put contact information</p>
    <p>E.g., IP address and port  Processes Get contact info when establishing</p>
    <p>connections  Collective Fence operation to allow</p>
    <p>optimizations</p>
  </div>
  <div class="page">
    <p>Connection Data Exchange Example  At init</p>
    <p>Proc 0 Puts (key=bc-p0, value=192.168.10.20;3893)  Proc 1 Puts (key=bc-p1, value=192.168.10.32;2897)  Proc 0 and 1 call Fence</p>
    <p>PM can collectively distribute database  Later Proc 0 wants to send a message to Proc 1</p>
    <p>Proc 0 does a Get of key bc-p1  Receives value 192.168.10.32;2897</p>
    <p>Proc 0 can now connect to Proc 1</p>
  </div>
  <div class="page">
    <p>Implementation Considerations  Allow the use of native process manager with low</p>
    <p>overhead  Systems often have existing PM</p>
    <p>E.g., integrated with resource manager  Minimize async processing and interrupts</p>
    <p>Scalable data exchange  Distributed process manager  Collective Fence provides opportunity for scalable</p>
    <p>collective exchange</p>
  </div>
  <div class="page">
    <p>Second Generation PMI</p>
  </div>
  <div class="page">
    <p>New PMI-2 Features</p>
    <p>Attribute query functionality  Database scope  Thread safety  Dynamic processes  Fault tolerance</p>
  </div>
  <div class="page">
    <p>PMI-2 Attribute Query Functionality  Process and resource managers have system</p>
    <p>specific information  Node topology, network topology, etc.</p>
    <p>Without this, processes need to determine this themselves  Each process gets each other's contact-info</p>
    <p>to discover local processes  O(p2) queries</p>
  </div>
  <div class="page">
    <p>PMI-2 Database Scope</p>
    <p>Previously KVS had only global scope  PMI-2 adds node-level scoping</p>
    <p>E.g., keys for shared memory segments  Allows for optimized storage and retrieval of</p>
    <p>values</p>
  </div>
  <div class="page">
    <p>PMI-2 Thread Safety</p>
    <p>PMI-1 is not thread safe  All PMI calls must be serialized</p>
    <p>Wait for request and response  Can affect multithreaded programs</p>
    <p>PMI-2 adds thread safety  Multiple threads can call PMI functions  One call cannot block the completion of</p>
    <p>another</p>
  </div>
  <div class="page">
    <p>PMI-2 Dynamic Processes  In PMI-1 a separate database is maintained for</p>
    <p>each MPI_COMM_WORLD (process group)  Queries are not allowed across databases  Requires out-of-band exchange of databases</p>
    <p>PMI-2 allows cross-database queries  Spawned or connected process groups can</p>
    <p>now query each others databases  Only process group ids need to be exchanged</p>
  </div>
  <div class="page">
    <p>PMI-2 Fault Tolerance</p>
    <p>PMI-1 provides no mechanism for respawning a failed process  New processes can be spawned, but they</p>
    <p>have a unique rank and process group  Respawn is critical for supporting fault</p>
    <p>tolerance  Not just for MPI but other programming</p>
    <p>models</p>
  </div>
  <div class="page">
    <p>Evaluation and Analysis</p>
  </div>
  <div class="page">
    <p>Evaluation and Analysis</p>
    <p>PMI-2 implemented in Hydra process manager  Evaluation</p>
    <p>System information query performance  Impact of added PMI functionality over native</p>
    <p>PM  Multithreaded performance</p>
  </div>
  <div class="page">
    <p>System Information Query Performance  PMI-1 provides no attribute query</p>
    <p>functionality  Processes must discover local processes  O(p2) queries</p>
    <p>PMI-2 has node topology attribute  Benchmark (5760 cores on SiCortex)</p>
    <p>MPI_Init();MPI_Finalize();</p>
  </div>
  <div class="page">
    <p>Process Launch (5760-core SiCortex)</p>
    <p>Launch Time</p>
    <p>PMI1</p>
    <p>System Size (cores)</p>
    <p>Ti m</p>
    <p>e (s</p>
    <p>ec on</p>
    <p>ds )</p>
    <p>PMI Request Count</p>
    <p>System Size (cores)</p>
    <p>Re qu</p>
    <p>es ts</p>
    <p>(t ho</p>
    <p>us an</p>
    <p>ds )</p>
  </div>
  <div class="page">
    <p>Impact of PMI Functionality  Systems often have integrated process managers</p>
    <p>Not all provide PMI functionality  Efficient PMI implementation must make effective use of</p>
    <p>native process managers  Minimizing overhead</p>
    <p>Benchmark (1600 cores on SiCortex)  Class C BT, EP and SP  Using SLURM (which provides PMI-1)  Using Hydra over SLURM (for launch and management)</p>
    <p>plus PMI daemon</p>
  </div>
  <div class="page">
    <p>Runtime Impact of Separate PMI Daemons (1600 cores SiCortex)Absolute Performance Percentage Variation</p>
    <p>BT EP SP 0%</p>
    <p>Va ri</p>
    <p>ati on</p>
    <p>in E</p>
    <p>xe cu</p>
    <p>ti on</p>
    <p>T im</p>
    <p>e</p>
    <p>BT EP SP 0</p>
    <p>SLURM Hydra</p>
    <p>Ex ec</p>
    <p>uti on</p>
    <p>T im</p>
    <p>e (s</p>
    <p>ec on</p>
    <p>ds )</p>
  </div>
  <div class="page">
    <p>Multithreaded Performance  PMI-1 is not thread safe</p>
    <p>External locking is needed  PMI-2 is thread safe</p>
    <p>Multiple threads can communicate with PM  Hydra: lock only for internal communication</p>
    <p>Benchmark (8-core x86_64 node)  Multiple threads calling MPI_Publish_name();</p>
    <p>MPI_Unpublish _name()  Work is fixed, number of threads increases</p>
  </div>
  <div class="page">
    <p>Multithreaded Performance</p>
    <p>PMI-1</p>
    <p>PMI-2</p>
    <p>Thread Count</p>
    <p>La te</p>
    <p>nc y</p>
    <p>( s)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We presented a generic process management interface PMI</p>
    <p>PMI-2: second generation eliminates PMI-1 shortcomings  Scalability issues for multicore systems  Issues for hybrid MPI-with-threads  Fault tolerance</p>
    <p>Performance evaluation  PMI-2 allows better implementations over PMI-1  Low overhead implementation over existing PM</p>
  </div>
</Presentation>
