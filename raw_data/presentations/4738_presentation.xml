<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Lattice Boltzmann Simulation Optimization on Leading</p>
    <p>Multicore Platforms</p>
    <p>Samuel Williams1,2, Jonathan Carter2, Leonid Oliker1,2, John Shalf2, Katherine Yelick1,2</p>
    <p>samw@eecs.berkeley.edu</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Motivation</p>
    <p>Multicore is the de facto solution for improving peak performance for the next decade</p>
    <p>How do we ensure this applies to sustained performance as well ?</p>
    <p>Processor architectures are extremely diverse and compilers can rarely fully exploit them</p>
    <p>Require a HW/SW solution that guarantees performance without completely sacrificing productivity</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Overview</p>
    <p>Examined the Lattice-Boltzmann Magneto-hydrodynamic (LBMHD) application</p>
    <p>Present and analyze two threaded &amp; auto-tuned implementations</p>
    <p>Benchmarked performance across 5 diverse multicore microarchitectures  Intel Xeon (Clovertown)  AMD Opteron (rev.F)  Sun Niagara2 (Huron)  IBM QS20 Cell Blade (PPEs)  IBM QS20 Cell Blade (SPEs)</p>
    <p>We show  Auto-tuning can significantly improve application performance  Cell consistently delivers good performance and efficiency  Niagara2 delivers good performance and productivity</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Multicore SMPs used</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(memory hierarchy)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>Co nve</p>
    <p>ntio nal</p>
    <p>Ca che</p>
    <p>-ba sed</p>
    <p>Me mo</p>
    <p>ry H iera</p>
    <p>rch y</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(memory hierarchy)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>Co nve</p>
    <p>ntio nal</p>
    <p>Ca che</p>
    <p>-ba sed</p>
    <p>Me mo</p>
    <p>ry H iera</p>
    <p>rch y</p>
    <p>Dis join</p>
    <p>t Lo cal</p>
    <p>St ore</p>
    <p>Me mo</p>
    <p>ry H iera</p>
    <p>rch y</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(memory hierarchy)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>Ca che</p>
    <p>+ P thre</p>
    <p>ads</p>
    <p>imp lem</p>
    <p>ent atio</p>
    <p>nsb</p>
    <p>Loc al S</p>
    <p>tore + l</p>
    <p>ibs pe</p>
    <p>imp lem</p>
    <p>ent atio</p>
    <p>ns</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(peak flops)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>PPEs: 13 Gflop/s</p>
    <p>SPEs: 29 Gflop/s 11 Gflop/s</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(peak DRAM bandwidth)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Auto-tuning</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuning</p>
    <p>Hand optimizing each architecture/dataset combination is not feasible</p>
    <p>Our auto-tuning approach finds a good performance solution by a combination of heuristics and exhaustive search  Perl script generates many possible kernels  (Generate SIMD optimized kernels)  Auto-tuning benchmark examines kernels and reports back with</p>
    <p>the best one for the current architecture/dataset/compiler/  Performance depends on the optimizations generated  Heuristics are often desirable when the search space isnt</p>
    <p>tractable</p>
    <p>Proven value in Dense Linear Algebra(ATLAS), Spectral(FFTW,SPIRAL), and Sparse Methods(OSKI)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Introduction to LBMHD</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Introduction to Lattice Methods</p>
    <p>Structured grid code, with a series of time steps  Popular in CFD (allows for complex boundary conditions)  Overlay a higher dimensional phase space</p>
    <p>Simplified kinetic model that maintains the macroscopic quantities  Distribution functions (e.g. 5-27 velocities per point in space)</p>
    <p>are used to reconstruct macroscopic quantities  Significant Memory capacity requirements</p>
    <p>+Z</p>
    <p>+Y</p>
    <p>+X</p>
  </div>
  <div class="page">
    <p>BIPSBIPS LBMHD</p>
    <p>(general characteristics)</p>
    <p>Plasma turbulence simulation  Couples CFD with Maxwells equations  Two distributions:</p>
    <p>momentum distribution (27 scalar velocities)  magnetic distribution (15 vector velocities)</p>
    <p>Three macroscopic quantities:  Density  Momentum (vector)  Magnetic Field (vector)</p>
    <p>momentum distribution</p>
    <p>+Y</p>
    <p>+Z</p>
    <p>+X</p>
    <p>magnetic distribution</p>
    <p>+Y</p>
    <p>+Z</p>
    <p>+X</p>
    <p>macroscopic variables</p>
    <p>+Y</p>
    <p>+Z</p>
    <p>+X</p>
  </div>
  <div class="page">
    <p>BIPSBIPS LBMHD</p>
    <p>(flops and bytes)</p>
    <p>Must read 73 doubles, and update 79 doubles per point in space (minimum 1200 bytes)</p>
    <p>Requires about 1300 floating point operations per point in space</p>
    <p>Flop:Byte ratio  0.71 (write allocate architectures)  1.07 (ideal)</p>
    <p>Rule of thumb for LBMHD:  Architectures with more flops than bandwidth are likely</p>
    <p>memory bound (e.g. Clovertown)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS LBMHD</p>
    <p>(implementation details)</p>
    <p>Data Structure choices:  Array of Structures: no spatial locality, strided access  Structure of Arrays: huge number of memory streams per</p>
    <p>thread, but guarantees spatial locality, unit-stride, and vectorizes well</p>
    <p>Parallelization  Fortran version used MPI to communicate between tasks.</p>
    <p>= bad match for multicore  The version in this work uses pthreads for multicore, and MPI for</p>
    <p>inter-node  MPI is not used when auto-tuning</p>
    <p>Two problem sizes:  643 (~330MB)  1283 (~2.5GB)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Stencil for Lattice Methods</p>
    <p>Very different the canonical heat equation stencil  There are multiple read and write arrays  There is no reuse</p>
    <p>read_lattice[ ][ ]</p>
    <p>write_lattice[ ][ ]</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Side Note on Performance</p>
    <p>Graphs  Threads are mapped first to cores, then sockets.</p>
    <p>i.e. multithreading, then multicore, then multisocket  Niagara2 always used 8 threads/core.  Show two problem sizes</p>
    <p>Well step through performance as optimizations/features are enabled within the auto-tuner</p>
    <p>More colors implies more optimizations were necessary  This allows us to compare architecture performance while</p>
    <p>keeping programmer effort(productivity) constant</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Performance and Analysis of Pthreads Implementation</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Pthread Implementation</p>
    <p>Not nave  fully unrolled loops  NUMA-aware  1D parallelization</p>
    <p>Always used 8 threads per core on Niagara2</p>
    <p>1P Niagara2 is faster than 2P x86 machines</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>Not nave  fully unrolled loops  NUMA-aware  1D parallelization</p>
    <p>Always used 8 threads per core on Niagara2</p>
    <p>1P Niagara2 is faster than 2P x86 machines</p>
    <p>Pthread Implementation</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>Not nave  fully unrolled loops  NUMA-aware  1D parallelization</p>
    <p>Always used 8 threads per core on Niagara2</p>
    <p>1P Niagara2 is faster than 2P x86 machines</p>
    <p>Initial Pthread Implementation</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>Performance degradation despite improved surface</p>
    <p>to volume ratio</p>
    <p>Performance degradation despite improved surface</p>
    <p>to volume ratio</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Cache effects</p>
    <p>Want to maintain a working set of velocities in the L1 cache  150 arrays each trying to keep at least 1 cache line.</p>
    <p>Impossible with Niagaras 1KB/thread L1 working set = capacity misses</p>
    <p>On other architectures, the combination of:  Low associativity L1 caches (2 way on opteron)  Large numbers or arrays  Near power of 2 problem sizes</p>
    <p>can result in large numbers of conflict misses</p>
    <p>Solution: apply a lattice (offset) aware padding heuristic to the velocity arrays to avoid/minimize conflict misses</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+Stencil-aware Padding)</p>
    <p>This lattice method is essentially 79 simultaneous 72-point stencils</p>
    <p>Can cause conflict misses even with highly associative L1 caches (not to mention opterons 2 way)</p>
    <p>Solution: pad each component so that when accessed with the corresponding stencil(spatial) offset, the components are uniformly distributed in the cacheIBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Blocking for the TLB</p>
    <p>Touching 150 different arrays will thrash TLBs with less than 128 entries.</p>
    <p>Try to maximize TLB page locality  Solution: borrow a technique from compilers for vector machines:</p>
    <p>Fuse spatial loops  Strip mine into vectors of size vector length (VL)  Interchange spatial and velocity loops</p>
    <p>Can be generalized by varying:  The number of velocities simultaneously accessed  The number of macroscopics / velocities simultaneously updated</p>
    <p>Has the side benefit expressing more ILP and DLP (SIMDization) and cleaner loop structure at the cost of increased L1 cache traffic</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Multicore SMP Systems</p>
    <p>(TLB organization)</p>
    <p>Chipset (4x64b controllers)</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>Core2</p>
    <p>FSB</p>
    <p>Core2 Core2 Core2</p>
    <p>EIB (Ring Network)</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>XDR BIF</p>
    <p>PPE</p>
    <p>EIB (Ring Network)</p>
    <p>BIF XDR</p>
    <p>PPE</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>SPE</p>
    <p>MFC</p>
    <p>MFC MFC MFC MFC</p>
    <p>SPE SPE SPE SPE</p>
    <p>&lt;20GB/s each</p>
    <p>direction</p>
    <p>IBM QS20 Cell BladeSun Niagara2 (Huron)</p>
    <p>AMD Opteron (rev.F)Intel Xeon (Clovertown)</p>
    <p>Opteron Opteron</p>
    <p>H T1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>Opteron Opteron</p>
    <p>H T 1MB</p>
    <p>victim 1MB victim</p>
    <p>SRI / crossbar</p>
    <p>/s (e</p>
    <p>ac h</p>
    <p>di re</p>
    <p>ct io</p>
    <p>n)</p>
    <p>Crossbar Switch</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>MT Sparc</p>
    <p>PPEs: 1024 entries SPEs: 256 entries</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Cache / TLB Tug-of-War</p>
    <p>For cache locality we want small VL  For TLB page locality we want large VL  Each architecture has a different balance between these two</p>
    <p>forces</p>
    <p>Solution: auto-tune to find the optimal VL</p>
    <p>L1 miss penalty TLB miss penalty</p>
    <p>L1 / 1200 Page size / 8VL</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+Vectorization)</p>
    <p>Each update requires touching ~150 components, each likely to be on a different page</p>
    <p>TLB misses can significantly impact performance</p>
    <p>Solution: vectorization  Fuse spatial loops,</p>
    <p>strip mine into vectors of size VL, and interchange with phase dimensional loops</p>
    <p>Auto-tune: search for the optimal vector length</p>
    <p>Significant benefit on some architectures</p>
    <p>Becomes irrelevant when bandwidth dominates performance</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+Explicit Unrolling/Reordering)</p>
    <p>Give the compilers a helping hand for the complex loops</p>
    <p>Code Generator: Perl script to generate all power of 2 possibilities</p>
    <p>Auto-tune: search for the best unrolling and expression of data level parallelism</p>
    <p>Is essential when using SIMD intrinsics</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+Software prefetching)</p>
    <p>Expanded the code generator to insert software prefetches in case the compiler doesnt.</p>
    <p>Auto-tune:  no prefetch  prefetch 1 line ahead  prefetch 1 vector</p>
    <p>ahead.  Relatively little benefit for</p>
    <p>relatively little workIBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+Software prefetching)</p>
    <p>Expanded the code generator to insert software prefetches in case the compiler doesnt.</p>
    <p>Auto-tune:  no prefetch  prefetch 1 line ahead  prefetch 1 vector ahead.</p>
    <p>Relatively little benefit for relatively little work</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+SIMDization, including non-temporal stores)</p>
    <p>Compilers(gcc &amp; icc) failed at exploiting SIMD.</p>
    <p>Expanded the code generator to use SIMD intrinsics.</p>
    <p>Explicit unrolling/reordering was extremely valuable here.</p>
    <p>Exploited movntpd to minimize memory traffic (only hope if memory bound)</p>
    <p>Significant benefit for significant work</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+SIMDization, including non-temporal stores)</p>
    <p>Compilers(gcc &amp; icc) failed at exploiting SIMD.</p>
    <p>Expanded the code generator to use SIMD intrinsics.</p>
    <p>Explicit unrolling/reordering was extremely valuable here.</p>
    <p>Exploited movntpd to minimize memory traffic (only hope if memory bound)</p>
    <p>Significant benefit for significant work</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F) 7.5% of peak flops 18% of bandwidth</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(+SIMDization, including non-temporal stores)</p>
    <p>Compilers(gcc &amp; icc) failed at exploiting SIMD.</p>
    <p>Expanded the code generator to use SIMD intrinsics.</p>
    <p>Explicit unrolling/reordering was extremely valuable here.</p>
    <p>Exploited movntpd to minimize memory traffic (only hope if memory bound)</p>
    <p>Significant benefit for significant work</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>IBM Cell Blade (PPEs)Sun Niagara2 (Huron)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Performance and Analysis of Cell Implementation</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Cell Implementation</p>
    <p>Double precision implementation  DP will severely hamper performance</p>
    <p>Vectorized, double buffered, but not auto-tuned  No NUMA optimizations  No Unrolling  VL is fixed  Straight to SIMD intrinsics  Prefetching replaced by DMA list commands</p>
    <p>Only collision() was implemented.</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Auto-tuned Performance</p>
    <p>(Local Store Implementation)</p>
    <p>First attempt at cell implementation.</p>
    <p>VL, unrolling, reordering fixed  No NUMA  Exploits DMA and double</p>
    <p>buffering to load vectors  Straight to SIMD intrinsics.  Despite the relative</p>
    <p>performance, Cells DP implementation severely impairs performance</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>Sun Niagara2 (Huron) IBM Cell Blade (SPEs)*</p>
    <p>*collision() only</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>First attempt at cell implementation.</p>
    <p>VL, unrolling, reordering fixed  No NUMA  Exploits DMA and double</p>
    <p>buffering to load vectors  Straight to SIMD intrinsics.  Despite the relative</p>
    <p>performance, Cells DP implementation severely impairs performance</p>
    <p>Auto-tuned Performance (Local Store Implementation)</p>
    <p>Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>Sun Niagara2 (Huron) IBM Cell Blade (SPEs)*</p>
    <p>*collision() only</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>First attempt at cell implementation.</p>
    <p>VL, unrolling, reordering fixed  Exploits DMA and double</p>
    <p>buffering to load vectors  Straight to SIMD intrinsics.  Despite the relative</p>
    <p>performance, Cells DP implementation severely impairs performance</p>
    <p>Speedup from Heterogeneity Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>Sun Niagara2 (Huron) IBM Cell Blade (SPEs)*</p>
    <p>*collision() only</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>First attempt at cell implementation.</p>
    <p>VL, unrolling, reordering fixed  Exploits DMA and double</p>
    <p>buffering to load vectors  Straight to SIMD intrinsics.  Despite the relative</p>
    <p>performance, Cells DP implementation severely impairs performance</p>
    <p>Speedup over naive Intel Xeon (Clovertown) AMD Opteron (rev.F)</p>
    <p>Sun Niagara2 (Huron) IBM Cell Blade (SPEs)*</p>
    <p>*collision() only</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Summary</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Aggregate Performance</p>
    <p>(Fully optimized)</p>
    <p>Cell SPEs deliver the best full system performance  Although, Niagara2 delivers near comparable per socket performance</p>
    <p>Dual core Opteron delivers far better performance (bandwidth) than Clovertown  Clovertown has far too little effective FSB bandwidth</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Parallel Efficiency</p>
    <p>(average performance per thread, Fully optimized)</p>
    <p>Aggregate Mflop/s / #cores  Niagara2 &amp; Cell showed very good multicore scaling  Clovertown showed very poor multicore scaling (FSB limited)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Power Efficiency</p>
    <p>(Fully Optimized)</p>
    <p>Used a digital power meter to measure sustained power  Calculate power efficiency as:</p>
    <p>sustained performance / sustained power  All cache-based machines delivered similar power efficiency  FBDIMMs (~12W each) sustained power</p>
    <p>8 DIMMs on Clovertown (total of ~330W)  16 DIMMs on N2 machine (total of ~450W)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Productivity</p>
    <p>Niagara2 required significantly less work to deliver good performance (just vectorization for large problems).</p>
    <p>Clovertown, Opteron, and Cell all required SIMD (hampers productivity) for best performance.</p>
    <p>Cache based machines required search for some optimizations, while Cell relied solely on heuristics (less time to tune)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Summary</p>
    <p>Niagara2 delivered both very good performance and productivity  Cell delivered very good performance and efficiency (processor</p>
    <p>and power)  On the memory bound Clovertown parallelism wins out over</p>
    <p>optimization and auto-tuning</p>
    <p>Our multicore auto-tuned LBMHD implementation significantly outperformed the already optimized serial implementation</p>
    <p>Sustainable memory bandwidth is essential even on kernels with moderate computational intensity (flop:byte ratio)</p>
    <p>Architectural transparency is invaluable in optimizing code</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Multi-core arms race</p>
  </div>
  <div class="page">
    <p>BIPSBIPS New Multicores 2.2GHz Opteron (rev.F)</p>
  </div>
  <div class="page">
    <p>BIPSBIPS New Multicores</p>
    <p>Barcelona is a quad core Opteron</p>
    <p>Victoria Falls is a dual socket (128 threads) Niagara2</p>
    <p>Both have the same total bandwidth</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>+Smaller pages</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Speedup from multicore/socket</p>
    <p>Barcelona is a quad core Opteron</p>
    <p>Victoria Falls is a dual socket (128 threads) Niagara2</p>
    <p>Both have the same total bandwidth</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>+Smaller pages</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Speedup from Auto-tuning</p>
    <p>Barcelona is a quad core Opteron</p>
    <p>Victoria Falls is a dual socket (128 threads) Niagara2</p>
    <p>Both have the same total bandwidth</p>
    <p>+SIMDization</p>
    <p>+SW Prefetching</p>
    <p>+Unrolling</p>
    <p>+Vectorization</p>
    <p>+Padding</p>
    <p>Nave+NUMA</p>
    <p>+Smaller pages</p>
  </div>
  <div class="page">
    <p>BIPSBIPS</p>
    <p>C O M P U T A T I O N A L R E S E A R C H D I V I S I O N</p>
    <p>Questions?</p>
  </div>
  <div class="page">
    <p>BIPSBIPS Acknowledgements</p>
    <p>UC Berkeley  RADLab Cluster (Opterons)  PSI cluster(Clovertowns)</p>
    <p>Sun Microsystems  Niagara2 donations</p>
    <p>Forschungszentrum Jlich  Cell blade cluster access</p>
    <p>George Vahala, et. al  Original version of LBMHD</p>
    <p>ASCR Office in the DOE office of Science  contract DE-AC02-05CH11231</p>
  </div>
</Presentation>
