<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Massively Parallel Genomic Sequence Search on Blue Gene/P</p>
    <p>Heshan Lin (NCSU) Pavan Balaji (ANL)</p>
    <p>Ruth Poole, Carlos Sosa (IBM) Xiaosong Ma (NCSU &amp; ORNL)</p>
    <p>Wu Feng (VT)</p>
  </div>
  <div class="page">
    <p>Sequence Search  Fundamental tool in computational biology</p>
    <p>Search similarities between a set of query sequences an d sequence databases (DBs)</p>
    <p>Analogous to web search engines</p>
    <p>Example application: predict functions of newly discovere d sequences</p>
    <p>Web Search Engine Sequence Search</p>
    <p>Input Keyword(s) Query sequence(s)</p>
    <p>Search space Internet Known sequence database</p>
    <p>Output Related web pages DB sequences similar to the query</p>
    <p>Sorted by Closeness &amp; rank Similarity score</p>
  </div>
  <div class="page">
    <p>Exponentia lly Growing</p>
    <p>Demands of Large-scale Sequence Search</p>
    <p>Genomic databases growing faster than compute capability of single CPU</p>
    <p>CPU clock scaling hits the power wall  Many biological problems requiring larg</p>
    <p>e-scale sequence search  Example: Finding Missing Genes  Compute: 12,000+ cores across 7 superco</p>
    <p>mputer centers worldwide  WAN: Shared Gigabit Ethernet  Feat: Completed in 10 days what</p>
    <p>would normally take years to finish</p>
    <p>Storage Challenge Award</p>
  </div>
  <div class="page">
    <p>Challenge: Scaling of Sequence Search  Performance of a massively parallel sequence se</p>
    <p>arch tool  mpiBLAST BG/L prototype  Search 28,014 Arabidopsis thaliana against the NCBI</p>
    <p>NT DB</p>
  </div>
  <div class="page">
    <p>Our Major Contributions</p>
    <p>Identification of key design issues of scalable s equence search on next-generation massively p arallel supercomputers</p>
    <p>I/O and scheduling optimizations that allow effi cient sequence searching across 10,000s of pr ocessors  An extended prototype of mpiBLAST on BG/P with up</p>
    <p>to 32,768 compute cores (93% efficiency).</p>
  </div>
  <div class="page">
    <p>Road Map</p>
    <p>Introduction  Background  Optimizations  Performance Results  Conclusion</p>
  </div>
  <div class="page">
    <p>BLAST &amp; mpiBLAST</p>
    <p>BLAST (Basic Local Alignment Sequence Tool)  De facto standard tool for sequence search  Computationally intensive: O(n2) worst-case  Algorithm: Heuristics-based alignment</p>
    <p>Execution time of a BLAST job hard to predic</p>
    <p>mpiBLAST  Originally master-worker with database segmentation  Enhanced for large-scale deployments</p>
    <p>Parallel I/O [Lin05, Ching06]  Hierarchical Scheduling [Gardner06]  Integrated I/O &amp; Scheduling [Thorsen07]</p>
  </div>
  <div class="page">
    <p>Blue Gene/P System Overview  1024 nodes, 4096 cores per rack</p>
    <p>Compute node: Quad-core PowerPC 450, 2GB memory</p>
    <p>Scalable I/O system  Compute and I/O nodes</p>
    <p>organized into PSET  Five networks</p>
    <p>3D torus, global collective, global interrupt, 10-Gigabit Ethernet, control</p>
    <p>Execution modes  SMP, DUAL, VN</p>
    <p>Node Card</p>
    <p>Cards</p>
  </div>
  <div class="page">
    <p>Road Map</p>
    <p>Introduction  Background  Optimizations  Performance Results  Conclusion</p>
  </div>
  <div class="page">
    <p>Current mpiBLAST Scheduling</p>
    <p>Scheduling hierarchy  Limitations</p>
    <p>Fixed worker-to-master mapping  High overhead with fine-grained load balancing</p>
    <p>SuperMaster</p>
    <p>f1 P1,1</p>
    <p>Partition 1</p>
    <p>f2 P1,2</p>
    <p>qi qi</p>
    <p>Master1</p>
    <p>f1 P2,1</p>
    <p>f2 P2,2</p>
    <p>qj qj</p>
    <p>Master2</p>
    <p>f1 Pn-1,1</p>
    <p>Partition n</p>
    <p>f2 Pn-1,2</p>
    <p>qk qk</p>
    <p>Mastern-1</p>
    <p>f1 Pn,1</p>
    <p>f2 Pn,2</p>
    <p>ql ql</p>
    <p>Mastern</p>
  </div>
  <div class="page">
    <p>Scheduling Optimization Overview</p>
    <p>Optimizations  Allow mapping arbitrary workers to a master  Hide balancing overhead with query prefetching</p>
    <p>SuperMaster</p>
    <p>f1 P1,1</p>
    <p>Partition 1</p>
    <p>f2 P1,2</p>
    <p>f1 P1,3</p>
    <p>qi qi qj</p>
    <p>f2 P1,4</p>
    <p>Master1</p>
    <p>qj</p>
    <p>f1 Pn,1</p>
    <p>Partition n</p>
    <p>f2 Pn,2</p>
    <p>f1 Pn,3</p>
    <p>qk qk ql</p>
    <p>f2 Pn,4</p>
    <p>Mastern</p>
    <p>ql</p>
    <p>pre fetc</p>
    <p>h</p>
  </div>
  <div class="page">
    <p>Mapping on BG/P</p>
    <p>Partition 1</p>
    <p>Disk Disk Disk DiskDisk</p>
    <p>DB frags cached in workers, queri es streamed across</p>
    <p>One output file per partition  Results merged and written to GP</p>
    <p>FS through I/O nodes</p>
    <p>Compute Nodes</p>
    <p>IO Node IO Node</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>Config example  PSize 128  4 DBs / partition  32768/128 =</p>
    <p>IO Node</p>
    <p>Partition i</p>
    <p>Compute Nodes</p>
    <p>Compute Nodes</p>
    <p>Partition 2 Partition i</p>
    <p>File iqi qi+1File 1 File 2</p>
    <p>G P</p>
    <p>F S</p>
  </div>
  <div class="page">
    <p>I/O Characteristics</p>
    <p>Irregularity  Output data from a process is unstructur</p>
    <p>ed and non-contiguous  I/O data distribution varies from</p>
    <p>query to query  Query search time are imbalanced acros</p>
    <p>s workers</p>
    <p>Straightforward non-contiguous I/O bad for BG/P I/O systems  Too many requests create contention at I/O node  GPFS less optimized for small-chunk, irregular I/O</p>
    <p>[Yu06]</p>
  </div>
  <div class="page">
    <p>Existing Option 1: Collective I/O</p>
    <p>Optimizes accesses from N processes  Two-phase I/O: Merge small, non-contiguous I/O reques</p>
    <p>ts into large, contiguous ones  Advantage</p>
    <p>Excellent I/O throughput  Highly optimized on Blue Gene [Yu06]</p>
    <p>Disadvantage  Synchronization between processes</p>
    <p>Suitable applications  Synchronizations in the compute kernel  Balanced computation time between I/O phases</p>
  </div>
  <div class="page">
    <p>Option1 Implementation: WorkerCollective</p>
    <p>qi</p>
    <p>Merge blocks info</p>
    <p>qi+1 Calculate offsets of qi</p>
    <p>Output of qi</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>Search Merge</p>
    <p>Write data4 Exchange data</p>
    <p>Wait</p>
    <p>qi+2</p>
    <p>Worker 1 Worker 2 Worker 3 Master</p>
    <p>Output File</p>
  </div>
  <div class="page">
    <p>Existing Option2: Optimized Independent I/O</p>
    <p>Optimized with data sieving  Read-modify-write: read in large chunks and only modify t</p>
    <p>arget regions  Advantages</p>
    <p>Does not incur synchronizations  Performs well for dense non-contiguous requests</p>
    <p>Disadvantages  Introduces redundant data accesses  Causes lock contentions with false sharing</p>
  </div>
  <div class="page">
    <p>Option2 Implementation: WorkerIndividual</p>
    <p>qi</p>
    <p>Merge blocks info</p>
    <p>qi+1 Calculate offsets of qi</p>
    <p>Worker 1 Worker 2 Worker 3 Master</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2 Search Merge</p>
    <p>Output of qiOutput File</p>
  </div>
  <div class="page">
    <p>Our Design: Asynchronous Two-Phase I/O</p>
    <p>Achieving high I/O throughput without forcing synch ronization  Asynchronously aggregate small, non-contiguous I/O req</p>
    <p>uests into large chunks</p>
    <p>Current implementation  Master selects a write leader for each query  Write leader aggregates requests from other workers</p>
  </div>
  <div class="page">
    <p>Our Design Implementation: WorkerMerge</p>
    <p>qi</p>
    <p>Merge blocks info</p>
    <p>qi+1 Calculate offsets of qi</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>qi</p>
    <p>qi+1</p>
    <p>qi+2</p>
    <p>Output of qi</p>
    <p>Search Merge</p>
    <p>Worker 1 Worker 2 Worker 3 Master</p>
    <p>Output File</p>
    <p>Write data4 Exchange data</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>WorkerMerge implementation concerns  Non-blocking MPI communications vs. threads  Incremental write to avoid memory overflow</p>
    <p>Only a limited amount of data collected and written at a time</p>
    <p>Will split-collective I/O help?  MPI_File_write_all_begin, MPI_File_write_all_end  MPI_File_set_view is synchronized by definition</p>
  </div>
  <div class="page">
    <p>Road Map</p>
    <p>Introduction  Background  Optimizations  Performance Results  Conclusion</p>
  </div>
  <div class="page">
    <p>Compare I/O Strategies  Single Partition  Experimental setup</p>
    <p>Database: NT (over 6 million seqs, 23 GB raw size)  Query: 512 sequences randomly sampled from the database  Metric: Overall execution time</p>
    <p>WM outperforms WC and WI by a factor</p>
    <p>of 2.7 and 4.9</p>
  </div>
  <div class="page">
    <p>Compare I/O Strategies  Multi-Partitions  Experimental setup</p>
    <p>Database: NT  Query: 2048 sequences randomly sampled from the database  Fixed partition size at 128, scale number of partitions</p>
  </div>
  <div class="page">
    <p>Scalability on A Real Problem</p>
    <p>Discovering missing genes  Database: 16M microbial sequences  Query: 0.25M randomly sampled sequences  93% parallel efficiency</p>
    <p>All to all search entire DB within 12 hours</p>
  </div>
  <div class="page">
    <p>Road Map</p>
    <p>Introduction  Background  Optimizations  Performance Results  Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Blue Gene/P: Well-suited for massively parallel se quence search when designed properly</p>
    <p>We proposed I/O and scheduling optimizations for scalable sequence searching across 10,000s proc essors  mpiBLAST prototype scales efficiently (93% efficiency) on</p>
    <p>For non-contiguous I/O with imbalanced compute k ernels, collective I/O without synchronization is des irable</p>
  </div>
  <div class="page">
    <p>References  O. Thorsen, K. Jiang, A. Peters, B. Smith, H. Lin, W. Feng and C. Sosa, &quot;Parallel</p>
    <p>Genomic Sequence-Search on a Massively Parallel System,&quot; ACM Intl Conferen ce on Computing Frontiers, 2007.</p>
    <p>H. Yu, R. Sahoo, C. Howson, G. Almasi, J. Castanos, M. Gupta J. Moreira, J. Par ker, T. Engelsiepen, R. Ross, R. Thakur, R. Latham, and W. Gropp, &quot;High Perfor mance File I/O for the BlueGene/L Supercomputer,&quot; 12th IEEE Intl Symposium on High-Performance Computer Architecture (HPCA-12), 2006.</p>
    <p>M. Gardner, W. Feng, J. Archuleta, H. Lin and X. Ma, &quot;Parallel Genomic Sequenc e-Searching on an Ad-Hoc Grid: Experiences, Lessons Learned, and Implication s,&quot; IEEE/ACM Intl Conference for High-Performance Computing, Networking, Sto rage and Analysis (SC), Best Paper Finalist, 2006.</p>
    <p>H. Lin, X. Ma, P. Chandramohan, A. Geist and N. Samatova, &quot;Efficient Data Acce ss for Parallel BLAST,&quot; IEEE Intl Parallel and Distributed Processing Symposium (IPDPS), 2005.</p>
    <p>A. Ching, W. Feng, H. Lin, X. Ma and A. Choudhary, &quot;Exploring I/O Strategies for Parallel Sequence Database Search Tools with S3aSim,&quot; ACM Intl Symposium on High Performance Distributed Computing (HPDC), 2006.</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Questions?</p>
  </div>
</Presentation>
