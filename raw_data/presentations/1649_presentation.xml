<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hybrid Data Reliability for Emerging Key-Value Storage Devices</p>
    <p>Rekha Pitchumani</p>
    <p>Yang-suk Kee Memory Solutions Lab</p>
    <p>Samsung Semiconductor Inc</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Emerging Key-Value Storage Devices Enable Providing Better Data Reliability (in many cases) at</p>
    <p>Competitive/Lower Cost on Throughput than Traditional RAID for Block Devices!!!</p>
  </div>
  <div class="page">
    <p>Key Value Storage Device</p>
    <p>Key-Value interface instead of traditional block interface</p>
    <p>Store, retrieve and delete KVs</p>
    <p>Check KV exist</p>
    <p>Iterator support</p>
    <p>Thin host software stack</p>
    <p>SNIA standard Key Value Storage API Specification is available</p>
  </div>
  <div class="page">
    <p>Prototype NVMe KV SSD from Samsung</p>
    <p>Same hardware as the enterprise-grade block SSD, but with KV firmware  4-255 byte keys and up-to 2 MB values</p>
    <p>For more on the ecosystem software, please check https://github.com/OpenMPDK</p>
  </div>
  <div class="page">
    <p>Details This Work Does NOT Go Into</p>
    <p>KV IO throughput vs block IO throughput</p>
    <p>Depends on value size, key size; Prototype firmware</p>
    <p>Not apples-to-apples - more internal tasks to do with same resources</p>
    <p>How about more hardware resources for KV SSDs?</p>
    <p>Interesting question; Power, cost, etc.,</p>
    <p>If KV SSD does not always beat block SSDs, why should I care?</p>
    <p>Towards Building a High-Performance, Scale-In Key-Value Storage System. Kang et.al. SYSTOR 19.</p>
    <p>Little teaser</p>
  </div>
  <div class="page">
    <p>Data Reliability Requirements</p>
    <p>Multiple options with different trade-offs</p>
    <p>Kind of like RAID for block storage devices</p>
    <p>Suitable for variable-length keys and variable-length values</p>
    <p>Should preserve the low host resource requirements of KV devices</p>
    <p>Flexibility and co-existence of multiple options</p>
  </div>
  <div class="page">
    <p>Key-Value Multi-Device (KVMD)</p>
    <p>Hybrid data reliability manager for KV SSDs  Stateless design</p>
    <p>Multiple pluggable reliability mechanisms suitable for variablelength keys and values</p>
    <p>Pluggable erasure code implementations</p>
    <p>Sits here</p>
  </div>
  <div class="page">
    <p>Reliability Mechanisms (RM)</p>
    <p>Serves as counterparts to the traditional RAID0, RAID1, and RAID6 architectures</p>
    <p>Hashing  No redundancy</p>
    <p>Replication  Replicas</p>
    <p>Splitting  Erasure Coding</p>
    <p>Packing  Erasure Coding</p>
  </div>
  <div class="page">
    <p>Modes of Operation</p>
    <p>Standalone mode</p>
    <p>Choose a single RM for all the KV pairs</p>
    <p>Hybrid mode</p>
    <p>Configuration file based  different RMs for KVs in different value size ranges that co-exist</p>
    <p>Custom mode</p>
    <p>Set either the standalone mode or the hybrid mode, and specify a RM per write</p>
    <p>To be used for known outliers</p>
  </div>
  <div class="page">
    <p>Hybrid Mode</p>
    <p>To co-exist in the hybrid mode, the RMs have to</p>
    <p>Place the first copy/chunk of the KV pair on the primary device, determined using the same hash function on the key, modulo the number of devices</p>
    <p>Store at-least the first copy/chunk/info using the same key as the user key</p>
  </div>
  <div class="page">
    <p>Hybrid Mode &amp; KVMD Metadata</p>
    <p>Store required metadata in the beginning of the value</p>
    <p>Hybrid Mode reads before any operation</p>
    <p>Optional caching layer can help</p>
    <p>Huge Object handling</p>
  </div>
  <div class="page">
    <p>Hashing &amp; Replication</p>
    <p>Hashing</p>
    <p>Similar to RAID0; Distributes KV objects.</p>
    <p>Replication</p>
    <p>Similar to RAID1; Replicates KV objects</p>
  </div>
  <div class="page">
    <p>Splitting</p>
    <p>Splits the value into k equal-sized objects and add r parity objects</p>
    <p>Configurable erasure coding methods</p>
  </div>
  <div class="page">
    <p>Packing</p>
    <p>Groups multiple KV objects</p>
    <p>Packs up-to k different objects into a single reliability set</p>
    <p>Configurable erasure coding methods &amp; virtual zero padding</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Evaluate software RAID (Linux Mdadm) for block devices and KVMD reliability mechanisms for KV SSDs</p>
    <p>Used the same 6 NVMe SSDs with different firm wares</p>
    <p>KVMD also has hash calculations and 32-bit checksum calculation and verification overhead for every operation</p>
    <p>crc32 IEEE checksum calculation function using ISA-L library</p>
    <p>Reed Solomon erasure coding implementation for any k and r using the ISA-L library</p>
  </div>
  <div class="page">
    <p>RAIDs Cost on Throughput</p>
  </div>
  <div class="page">
    <p>KVMDs Cost on Throughput</p>
  </div>
  <div class="page">
    <p>Updates and Deletes</p>
  </div>
  <div class="page">
    <p>Single Device Failure Rebuild</p>
  </div>
  <div class="page">
    <p>Limitations &amp; Future Directions</p>
    <p>Data/Metadata Caching</p>
    <p>Versioned Updates</p>
    <p>Packing performance</p>
    <p>Concurrency control</p>
    <p>Crash consistency</p>
    <p>Automatic RM Determination</p>
    <p>Even Capacity Utilization</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Better MTTDL than block SSDs in many cases due to</p>
    <p>Reduced write amplification (Not yet for Packing updates)</p>
    <p>Faster device rebuilds, proportional to number of user objects, rather than device capacity</p>
    <p>KVMD throughput degradation comparable to or lower than software RAID incurred degradation in most cases</p>
    <p>Offers high flexibility</p>
  </div>
  <div class="page">
    <p>Backup</p>
  </div>
  <div class="page">
    <p>Comparison</p>
  </div>
</Presentation>
