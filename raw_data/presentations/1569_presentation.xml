<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SHRD: Improving Spatial Locality in Flash Storage Accesses by Sequentializing in Host and Randomizing in Device</p>
    <p>Hyukjoong Kim1, Dongkun Shin1, Yun Ho Jeong2 and Kyung Ho Kim2</p>
    <p>Sungkyunkwan University1</p>
    <p>Samsung Electronics2</p>
    <p>February 27  March 2, 2017</p>
  </div>
  <div class="page">
    <p>Random write is still slow at SSD</p>
    <p>eMMC 5.0 (Odroid-XU3)</p>
    <p>UFS (Galaxy S6)</p>
    <p>SATA SSD (Intel 525)</p>
    <p>SATA SSD (Samsung 850 Pro)</p>
    <p>NVMe SSD (Intel 750)</p>
    <p>B a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>random write (4KB)</p>
    <p>sequential write (512KB)</p>
  </div>
  <div class="page">
    <p>Why is RW slower than SW?</p>
  </div>
  <div class="page">
    <p>Why is RW slower than SW?</p>
    <p>Sequential write  Large, few requests</p>
    <p>Random write  Small, many requests</p>
    <p>Packed command  e.g. eMMC</p>
    <p>Interrupt coalescing  e.g. NVMe, SATA NCQ</p>
    <p>Vectored I/O  e.g. OpenChannel SSD [FAST17]</p>
  </div>
  <div class="page">
    <p>Why is RW slower than SW?</p>
    <p>Hot/cold separation  Stores hot and cold data into different blocks</p>
    <p>Incremental GC / bgGC  can hide GC latency</p>
    <p>Blocks with RWBlocks with SW</p>
    <p>Invalid</p>
    <p>I</p>
    <p>I</p>
    <p>I</p>
    <p>Valid</p>
    <p>V</p>
    <p>V</p>
    <p>V</p>
    <p>I</p>
    <p>V</p>
    <p>I</p>
    <p>V</p>
    <p>V</p>
    <p>I</p>
    <p>V</p>
    <p>I</p>
    <p>RW generates hot/cold-mixed blocks  Dispersed invalid pages  high GC overhead</p>
  </div>
  <div class="page">
    <p>Why is RW slower than SW?</p>
    <p>Page-level mapping FTL shows good performance on RW  Requires a large DRAM to maintain fine-grained mapping table  4 byte per 4 KB  8 GB DRAM for 8 TB storage</p>
    <p>Demand loading FTL (DFTL [ASPLOS08])  Uses a small map cache with on-demand map loading  Random writes invoke frequent map loading/unloading</p>
  </div>
  <div class="page">
    <p>Demand-loading FTL (DFTL)</p>
    <p>Map caching scheme can show good performance by utilizing temporal &amp; spatial locality  Page level map load/unload</p>
    <p>One map page contains multiple contiguous mapping entries</p>
    <p>Vulnerable to random workload  low temporal &amp; spatial locality</p>
    <p>high map miss rate</p>
    <p>high map loading overhead</p>
    <p>NAND Flash Chips</p>
    <p>DRAM Map Cache</p>
    <p>map page</p>
    <p>map page</p>
    <p>map page</p>
    <p>map blocks data blocks</p>
    <p>map</p>
    <p>pages</p>
    <p>data</p>
    <p>O O</p>
    <p>B</p>
    <p>loads unloads</p>
    <p>LRU list</p>
    <p>Write LPN 768</p>
  </div>
  <div class="page">
    <p>Previous Solution: LFS</p>
    <p>Generate only sequential writes  out-of-place append-only write scheme</p>
    <p>Problems  reclaiming log space (cleaning overhead)</p>
    <p>Filesystem needs to copy valid page  host-to-device data transfer</p>
    <p>Large metadata, wandering tree problem</p>
    <p>Fragmented read operation</p>
    <p>Append logging</p>
    <p>Invalid Invalid</p>
    <p>FS</p>
    <p>Storage</p>
    <p>copy</p>
    <p>read</p>
    <p>write</p>
    <p>Cleaning requires host-to-device data transfer operations</p>
    <p>storage space</p>
    <p>LPN: 32 128</p>
  </div>
  <div class="page">
    <p>Can we remove copy overhead?</p>
    <p>SSD maintains a page-level mapping table</p>
    <p>Address remapping  Can change the logical address of a written data</p>
    <p>by modifying mapping table</p>
    <p>AnViL [FAST15], SHARE [SIGMOD16]</p>
    <p>Can reclaim log space with address remapping</p>
    <p>FS</p>
    <p>Storage</p>
    <p>copy</p>
    <p>read</p>
    <p>write</p>
    <p>&lt; Copying at filesystem &gt;</p>
    <p>FS</p>
    <p>Storage</p>
    <p>&lt; Remapping at storage &gt;</p>
    <p>remap LPN: 32 128</p>
    <p>remap request</p>
    <p>LPN: 32 128</p>
    <p>Logical Physical</p>
    <p>Mapping Table</p>
  </div>
  <div class="page">
    <p>Which layer? File System or Block Layer</p>
    <p>Our solution is Append logging on Block Layer  Append logging on log area temporarily</p>
    <p>Remap to the original location</p>
    <p>Can utilize legacy filesystems (e.g. EXT4)</p>
    <p>Simpler metadata management</p>
    <p>Faster sequential read performance</p>
    <p>Storage</p>
    <p>Append-logging D/D</p>
    <p>Legacy Filesystems</p>
    <p>Legacy Applications</p>
  </div>
  <div class="page">
    <p>SHRD (Sequentializing in Host, Randomizing in Device)</p>
    <p>Sequentializing in Host  Host OS writes random requests sequentially at log area</p>
    <p>Randomizing in Device  SSD modifies the mapping table to change the logical address</p>
    <p>Log area (reserved)Normal area</p>
    <p>(1) Sequentializing</p>
    <p>(2) Randomizing</p>
  </div>
  <div class="page">
    <p>SHRD Example: write</p>
    <p>Log area (FS invisible)</p>
    <p>Logical address</p>
    <p>NAND flash</p>
    <p>Normal area (FS visible)</p>
    <p>Host redirection table</p>
    <p>oLPN tLPN</p>
    <p>multiple small random writes</p>
    <p>single large sequential write</p>
    <p>Logging 1024</p>
    <p>LPN PPN Device mapping table</p>
    <p>physical address</p>
    <p>oLPN: original LPN tLPN: temporal LPN</p>
  </div>
  <div class="page">
    <p>SHRD Example: read redirection</p>
    <p>Logical address</p>
    <p>NAND flash</p>
    <p>Host redirection table</p>
    <p>oLPN tLPN</p>
    <p>LPN PPN Device mapping table</p>
    <p>Read 32</p>
    <p>redirect to 1024</p>
    <p>physical address</p>
    <p>oLPN: original LPN tLPN: temporal LPN</p>
  </div>
  <div class="page">
    <p>SHRD Example: remap</p>
    <p>Logical address</p>
    <p>NAND flash</p>
    <p>Host redirection table</p>
    <p>oLPN tLPN</p>
    <p>LPN PPN Device mapping table</p>
    <p>remap 1024-1027</p>
    <p>LPN PPN</p>
    <p>oLPN tLPN</p>
    <p>physical address</p>
    <p>oLPN: original LPN tLPN: temporal LPN</p>
  </div>
  <div class="page">
    <p>Can we really reduce map loading overhead?</p>
    <p>Remap modifies the mapping entries of sequentialized pages  A time-ordered access scheme</p>
    <p>inherits the original random pattern</p>
    <p>low spatial locality</p>
    <p>oLPN-ordered map access  The mapping table is oLPN-indexed</p>
    <p>Can increase spatial locality</p>
    <p>time-ordered access 8 map loads</p>
    <p>oLPN tLPN</p>
    <p>oLPN-ordered access 5 map loads</p>
    <p>re m</p>
    <p>a p</p>
    <p>p in</p>
    <p>g se</p>
    <p>q u</p>
    <p>e n</p>
    <p>ce</p>
    <p>oLPN tLPN</p>
  </div>
  <div class="page">
    <p>Special commands: twrite &amp; remap</p>
    <p>twrite (oLPN[n], tLPN_start, n, data)  Write command sends two addresses, (tLPN, oLPN)</p>
    <p>oLPN is stored at the OOB area of physical page</p>
    <p>used for power-off-recovery / GC</p>
    <p>Packed command with multiple RW requests</p>
    <p>remap (oLPN[m], tLPN[m], m)  m = # of remapping entries per remap command</p>
    <p>oLPN-sorted entries  Improving spatial locality</p>
    <p>Changes mapping table from tLPN to oLPN</p>
    <p>tLPN : PPN  oLPN : PPN</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>SHRD D/D is implemented in Linux kernel 3. 17.4  Additional kernel module at SCSI D/D layer</p>
    <p>Host redirection table: about 1 MB for 64 MB log area</p>
    <p>Prototype SSD device  Modified the firmware of a commercial</p>
    <p>SATA3 SSD device (Samsung 843)</p>
    <p>DFTL &amp; SHRD-FTL are implemented</p>
    <p>Map cache size is configurable</p>
  </div>
  <div class="page">
    <p>RW Performance According to cache</p>
    <p>Better performance than DFTL  By reducing map loading/unloading overhead</p>
    <p>SHRD shows steady performance regardless of cache size</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (</p>
    <p>M B</p>
    <p>/s )</p>
    <p>Map cache size</p>
    <p>DFTL SHRD</p>
    <p>m a</p>
    <p>p l</p>
    <p>o a</p>
    <p>d p</p>
    <p>e r</p>
    <p>p a g</p>
    <p>e I</p>
    <p>O</p>
    <p>Map cache size</p>
    <p>DFTL SHRD</p>
    <p>fio random write test (32GB space, 4KB write)</p>
  </div>
  <div class="page">
    <p>tpcc YCSB postmark fileserver varmail</p>
    <p>th ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>(n o</p>
    <p>rm a</p>
    <p>li z e</p>
    <p>d t</p>
    <p>o D</p>
    <p>F T</p>
    <p>L )</p>
    <p>Performance on Real Benchmarks</p>
    <p>Better performance at all workloads</p>
    <p>Small gains at sequential or read dominant workload  still better than DFTL</p>
    <p>small random write dominant workloads</p>
    <p>sequential write workload</p>
    <p>read/flush dominant workload</p>
    <p>CMT caches about 5% of entire workload space</p>
  </div>
  <div class="page">
    <p>b a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>time(s)</p>
    <p>F2FS (SSR) w/ DFTL</p>
    <p>EXT4 w/ DFTL</p>
    <p>SHRD gains at EXT4 vs. F2FS</p>
    <p>EXT4 shows bad performance on random write</p>
    <p>Performance of F2FS decreases due to SSR at high utilization</p>
    <p>low utilization high utilization</p>
  </div>
  <div class="page">
    <p>b a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>time(s)</p>
    <p>F2FS (SSR) w/ DFTL</p>
    <p>F2FS (SSR) w/ SHRD</p>
    <p>EXT4 w/ DFTL</p>
    <p>EXT4 w/ SHRD</p>
    <p>SHRD gains at EXT4 vs. F2FS</p>
    <p>SHRD improves both EXT4 and F2FS  SHRD improves the bandwidth of aged F2FS</p>
    <p>EXT4 shows similar performance as F2FS by using SHRD</p>
    <p>low utilization high utilization</p>
    <p>ext4 improvement</p>
    <p>F2FS improvement</p>
  </div>
  <div class="page">
    <p>b a n</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>time(s)</p>
    <p>F2FS (SSR) w/ DFTL</p>
    <p>F2FS (SSR) w/ SHRD</p>
    <p>EXT4 w/ DFTL</p>
    <p>EXT4 w/ SHRD</p>
    <p>SHRD gains at EXT4 vs. F2FS</p>
    <p>Sequential read performance of EXT4 is much better  The out-of-place scheme of F2FS scatters the data blocks of a file</p>
    <p>Sequential read Random read</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (</p>
    <p>M B</p>
    <p>/s )</p>
    <p>EXT4 w/ SHRD</p>
    <p>F2FS (SSR) w/ SHRD</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>SHRD is a address reshaping technique  transforms RW into SW at the block D/D</p>
    <p>restores the original addresses without copy operations</p>
    <p>Solves POR / GC issues of address remapping</p>
    <p>SHRD improves 30x better performance at a small map cache  reduce DRAM drastically</p>
  </div>
  <div class="page">
    <p>Thank you.</p>
  </div>
  <div class="page">
    <p>The effect of request reordering</p>
    <p>NONE 2MB 4MB 8MB 16MB 32MB 64MB</p>
    <p>Reordering window size</p>
    <p>u ti</p>
    <p>li z a ti</p>
    <p>o n</p>
    <p>o f</p>
    <p>p a r a ll</p>
    <p>e l</p>
    <p>u n</p>
    <p>it</p>
    <p>M a p</p>
    <p>m is</p>
    <p>s r a ti</p>
    <p>o stg_0 proj_0 stg_0 proj_0</p>
    <p>reduce map miss</p>
    <p>improve parallelism</p>
  </div>
  <div class="page">
    <p>SHRD Architecture</p>
  </div>
  <div class="page">
    <p>Reverse map in out-of-band (OOB) area  SSD stores corresponding LPN in OOB area</p>
    <p>Reverse map is used for GC &amp; recovery</p>
    <p>GC: change the mapping table of victim valid page</p>
    <p>Recovery: recover the mapping table of active blocks</p>
    <p>GC &amp; Power Off Recovery (POR)</p>
    <p>Data OOB</p>
    <p>ECC LPN</p>
    <p>Physical page layout</p>
  </div>
  <div class="page">
    <p>Store oLPN at the OOB area of RWLB  RWLB blocks must be excluded from choosing victim</p>
    <p>until entire data stored in the blocks are remapped</p>
    <p>Non-remapped data will be auto-remapped at POR</p>
    <p>by scanning the OOB area of RWLB blocks</p>
    <p>GC &amp; Power Off Recovery (POR)</p>
    <p>Data OOB</p>
    <p>RWLB Block</p>
    <p>oLPN  moved into data block  can be victim of GC</p>
    <p>POR will scan OOB  do auto-remap</p>
    <p>after remap</p>
    <p>before remap</p>
    <p>twrite</p>
  </div>
  <div class="page">
    <p>Latency Comparison</p>
    <p>fio mixed workload (32GB area, 4KB random read/write mixed)</p>
    <p>DFTL SHRD</p>
    <p>b a</p>
    <p>n d</p>
    <p>w id</p>
    <p>th (</p>
    <p>M B</p>
    <p>/s ) read write</p>
  </div>
  <div class="page">
    <p>RW size threshold and Log size</p>
    <p>DFTL 4 8 16 32 64 128 256 512</p>
    <p>th ro</p>
    <p>u g</p>
    <p>h p u</p>
    <p>t (n</p>
    <p>o rm</p>
    <p>a li</p>
    <p>z e d</p>
    <p>t o</p>
    <p>D F</p>
    <p>T L</p>
    <p>)</p>
    <p>size threshold (KB)</p>
    <p>fileserver</p>
    <p>postmark</p>
    <p>DFTL 8 16 24 32 40 48 56 64 72 80</p>
    <p>th ro</p>
    <p>u g</p>
    <p>h p u</p>
    <p>t (n</p>
    <p>o rm</p>
    <p>a li</p>
    <p>z e d</p>
    <p>t o</p>
    <p>D F</p>
    <p>T L</p>
    <p>)</p>
    <p>log size (MB)</p>
    <p>fileserver</p>
    <p>postmark</p>
  </div>
  <div class="page">
    <p>Postmark map access pattern</p>
    <p>&lt; withiout SHRD &gt; &lt; with SHRD &gt;</p>
    <p>remap command access seqeuntialized write</p>
  </div>
  <div class="page">
    <p>Sequentializing in Host</p>
    <p>Normal area Log area</p>
    <p>a b c d I/O scheduler</p>
    <p>SHRD driver</p>
    <p>SATA Interface</p>
    <p>SSD Device</p>
    <p>mapping</p>
    <p>twrite header</p>
    <p>oLPN tLPN</p>
    <p>twrite data</p>
    <p>SATA write command (OOB) Completion</p>
    <p>SATA write command (OOB)</p>
    <p>a</p>
    <p>a 32</p>
    <p>data spare</p>
    <p>NAND</p>
    <p>b c d</p>
    <p>Completion</p>
    <p>rand_ptr seq_ptr</p>
  </div>
  <div class="page">
    <p>Randomizing in Device</p>
    <p>Normal area Log area</p>
    <p>I/O scheduler</p>
    <p>SHRD driver</p>
    <p>SATA Interface</p>
    <p>SSD Device</p>
    <p>a b c d valid log area</p>
    <p>oLPN tLPN</p>
    <p>SATA write command (OOB)</p>
    <p>Change mapping table</p>
    <p>rand_ptr seq_ptr</p>
  </div>
</Presentation>
