<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks</p>
    <p>Aishwarya Jadhav Indian Institute of Science</p>
    <p>Bangalore, India</p>
    <p>Vaibhav Rajan School of Computing</p>
    <p>National University of Singapore</p>
  </div>
  <div class="page">
    <p>Select salient sentences from input document to create a summary</p>
    <p>Extractive Summarization</p>
    <p>S1</p>
    <p>S2</p>
    <p>Sn</p>
    <p>INPUT Document with</p>
    <p>sentences S1, S2,.., Sn</p>
    <p>Supervised extractive summarization for single document inputs</p>
    <p>Si1</p>
    <p>Sim</p>
    <p>OUTPUT Summary 1 ik  n</p>
  </div>
  <div class="page">
    <p>Our Contribution</p>
    <p>Unlike previous methods, SWAP-NET uses keywords for sentence selection</p>
    <p>Predicts both important words and sentences in document</p>
    <p>Two-level Encoder-Decoder Attention model  Outperform state of the art extractive</p>
    <p>summarisers.</p>
    <p>S1</p>
    <p>S2</p>
    <p>Sn</p>
    <p>INPUT Document with</p>
    <p>sentences S1, S2,.., Sn</p>
    <p>OUTPUT Summary 1 ik  n</p>
    <p>Si1</p>
    <p>Sim</p>
    <p>A Deep Learning Architecture for training an extractive summarizer: SWAP-NET</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Methods</p>
    <p>Recent extractive summarization methods</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Methods</p>
    <p>Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. 54th Annual Meeting of the Association for Computational Linguistics.</p>
    <p>Sentence encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>(with decoder)</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>Recent extractive summarization methods</p>
    <p>NN (Cheng and Lapata, 2016)</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Methods</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of docments. In Association for the Advancement of Artificial Intelligence, pages 30753081. Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. 54th Annual Meeting of the Association for Computational Linguistics.</p>
    <p>Sentence encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>(with decoder)</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>NN (Cheng and Lapata, 2016)</p>
    <p>Sentence Encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>Word Encodings wrt other words</p>
    <p>Document Encoding wrt its sentences</p>
    <p>SummaRuNNer (Nallapati et al., 2017)</p>
    <p>Recent extractive summarization methods</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Methods</p>
    <p>Sentence encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>(with decoder)</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>NN (Cheng and Lapata, 2016)</p>
    <p>Sentence Encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>Word Encodings wrt other words</p>
    <p>Document Encoding wrt its sentences</p>
    <p>SummaRuNNer (Nallapati et al., 2017)</p>
    <p>Both assume saliency of sentence s depends on salient sentences appearing before s</p>
    <p>Recent extractive summarization methods</p>
    <p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of docments. In Association for the Advancement of Artificial Intelligence, pages 30753081. Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. 54th Annual Meeting of the Association for Computational Linguistics.</p>
  </div>
  <div class="page">
    <p>Our hypothesis: saliency of a sentence depends on both salient sentences and words appearing before that sentence in the document</p>
    <p>Similar to graph based models by Wan et al. (2007)  Along with labelling sentences we also label words to determine their saliency  Moreover, saliency of a word depends on previous salient words and sentences</p>
    <p>Intuition Behind Approach</p>
    <p>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th annual meeting of the association of computational linguistics, pages 552559.</p>
    <p>Question: Which sentence should be considered salient (part of summary)?</p>
  </div>
  <div class="page">
    <p>Intuition Behind Approach</p>
    <p>Sentence-Sentence Interaction</p>
    <p>Word-Word Interaction</p>
    <p>Sentence-Word Interaction</p>
    <p>Three types of Interactions:</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence - Sentence</p>
    <p>A sentence should be salient if it is heavily linked with other salient sentences</p>
    <p>Intuition: Interaction Between Sentences</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Word-Word</p>
    <p>A word should be salient if it is heavily linked with other salient words</p>
    <p>Intuition: Interaction Between Words</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-Word</p>
    <p>A word should be salient if it appears in many salient sentences</p>
    <p>A sentence should be salient if it contains many salient words</p>
    <p>Intuition: Words and Sentences Interaction</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
    <p>Generate extractive summary using both important words and sentences</p>
    <p>Intuition: Words and Sentences Interaction</p>
    <p>Important Sentences: S3 Important Words: V2, V3</p>
  </div>
  <div class="page">
    <p>Sentence to Sentence Interaction as Sentence Extraction</p>
    <p>Word to Word Interaction as Word Extraction</p>
    <p>For discrete sequences, pointer networks have been successfully used to learn how to select positions from an input sequence</p>
    <p>We use two pointer networks one at word-level and another at sentence-level</p>
    <p>Keyword Extraction and Sentence Extraction</p>
  </div>
  <div class="page">
    <p>Pointer Network</p>
    <p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems, pages 26922700.</p>
    <p>e4e3e2e1</p>
    <p>x1 x2 x3 x4</p>
    <p>d2d1</p>
    <p>Input (X):</p>
    <p>Output Indices (R): 2,3</p>
    <p>Encoder Decoder</p>
    <p>Attention Vector</p>
    <p>Pointer network (Vinyals et al., 2015),</p>
    <p>Encoder-Decoder architecture with Attention  Attention mechanism is used to select one of the inputs at each decoding step  Thus, effectively pointing to an input</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-Level Pointer Network</p>
    <p>Word-Level Pointer Network</p>
    <p>?</p>
    <p>Three Interactions</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
  </div>
  <div class="page">
    <p>Sentence-Level Pointer Network</p>
    <p>Word-Level Pointer Network</p>
    <p>Three Interactions: SWAP-NET</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
    <p>A Mechanism to Combine Word Level Attentions and Sentence Level Attentions</p>
    <p>Generate Summary</p>
  </div>
  <div class="page">
    <p>A Mechanism to Combine Word Level Attentions and Sentence Level Attentions</p>
    <p>Q1 : How can the two attentions be combined?</p>
    <p>Q2 : How can the summaries be generated considering both the attentions?</p>
    <p>Sentence-Word ? ?</p>
    <p>Q1 Q2</p>
    <p>Generate Summary</p>
    <p>Questions</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-Level Pointer Network</p>
    <p>Word-Level Pointer Network</p>
    <p>?</p>
    <p>Three Interactions: SWAP-NET</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
  </div>
  <div class="page">
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>SWAP-NET Architecture: Word-Level Pointer Network</p>
    <p>Word Encoder</p>
    <p>Word Decoder</p>
    <p>Similar to Pointer Network,</p>
    <p>The word encoder is bi-directional LSTM  Word-level decoder learns to point to</p>
    <p>important words</p>
  </div>
  <div class="page">
    <p>Purple line: attention vector given as input to each decoding step  Sum of word encodings weighted by</p>
    <p>attention probabilities generated in previous step</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>Probability of word i, at decoding step j</p>
    <p>Word Attention</p>
    <p>SWAP-NET Architecture: Word-Level Pointer Network</p>
    <p>Word Attention Vector</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-Level Pointer Network</p>
    <p>Word-Level Pointer Network</p>
    <p>?</p>
    <p>Three Interactions: SWAP-NET</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>s1 s2</p>
    <p>D S 1</p>
    <p>D S 3</p>
    <p>D S 2</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>SWAP-NET Architecture: Sentence-Level Hierarchical Pointer Network</p>
    <p>Word Encoder</p>
    <p>Word Decoder</p>
    <p>Sentence Encoder</p>
    <p>Sentence Decoder</p>
    <p>Sentence is represented by encoding of last word of that sentence</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>s1 s2</p>
    <p>D S 1</p>
    <p>D S 3</p>
    <p>D S 2</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>Probability of sentence k, at decoding step j</p>
    <p>Sentence Attention</p>
    <p>Attention vectors are sum of sentence encodings weighted by attention probabilities by previous decoding step</p>
    <p>SWAP-NET Architecture: Sentence-Level Hierarchical Pointer Network</p>
    <p>Sentence Attention Vector</p>
  </div>
  <div class="page">
    <p>Combining Sentence Attention and Word Attention</p>
    <p>Q1 : How can the two attentions be combined?</p>
    <p>V1 V2</p>
    <p>S1</p>
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>A document with three sentences and corresponding words is shown</p>
    <p>Sentences</p>
    <p>Words</p>
  </div>
  <div class="page">
    <p>V1 V2</p>
    <p>S1</p>
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Possible Solution: Step 1: Hold sentence processing. Then group all words and determine their saliency sequentially</p>
  </div>
  <div class="page">
    <p>V1 V2</p>
    <p>S1</p>
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Possible Solution: Step 2: Using output of step 1, i.e., using keywords, process sentences to determine salient sentences</p>
    <p>INCOMPLETE SOLUTION : This methods processes sentence depending on words but does not use sentences for processing words.</p>
  </div>
  <div class="page">
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Solution: Group each sentence and its words separately and process them sequentially</p>
    <p>V1 V2</p>
    <p>S1</p>
  </div>
  <div class="page">
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Step1: Hold sentence processing. Determine saliency of words in S1</p>
    <p>V1 V2</p>
    <p>S1</p>
  </div>
  <div class="page">
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions Step2: Using information about saliency of words in S1  Hold word processing and resume sentence processing.  Determine saliency of S1</p>
    <p>V1 V2</p>
    <p>S1</p>
  </div>
  <div class="page">
    <p>V1 V2</p>
    <p>S1</p>
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Step3: Using information about saliency of both S1 and its words  Hold sentence processing and resume word processing.  Determine saliency of words in next sentence S2</p>
  </div>
  <div class="page">
    <p>V1 V2</p>
    <p>S1</p>
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>Step4: Using information about saliency of words in S2 and saliency of previous sentence S1  Hold word processing and resume sentence processing.  Determine saliency of sentence S2</p>
  </div>
  <div class="page">
    <p>V4 V6V5</p>
    <p>S2</p>
    <p>V2 V3</p>
    <p>S3</p>
    <p>V2V4</p>
    <p>Sentence and Word Interactions</p>
    <p>This methods ensures that saliency of word and sentence is determined from previously predicted both salient sentences and words</p>
    <p>V1 V2</p>
    <p>S1</p>
    <p>Solution: And so on.</p>
  </div>
  <div class="page">
    <p>Sentence and Word Interactions</p>
    <p>Sharing Attention Vectors: Determine salient words and sentences</p>
    <p>Synchronising Decoding Steps: Decide when to turn off and on word processing and sentence processing to synchronise word and sentence prediction</p>
    <p>Using previously predicted salient word and sentences</p>
  </div>
  <div class="page">
    <p>V1 V4 V6V2 V3 V5</p>
    <p>S1 S3S2</p>
    <p>Sentence-Level Pointer Network</p>
    <p>Word-Level Pointer Network</p>
    <p>Switch Mechanism</p>
    <p>Three Interaction : SWAP-NET</p>
    <p>Sentence-WordSentence - Sentence</p>
    <p>Word-Word</p>
  </div>
  <div class="page">
    <p>Synchronising decoding steps of the two decoders by allowing only one decoder output at a step</p>
    <p>Sharing both attention vectors (purple and orange lines) between the two decoder</p>
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>D S 1</p>
    <p>D S 3</p>
    <p>D S 2</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>q0 q1</p>
    <p>Switch ProbabilityFeedforward Network</p>
    <p>SWAP-NET : Switch Mechanism</p>
    <p>Word Decoder Hidden State</p>
    <p>Sentence Decoder Hidden State</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>D S 1</p>
    <p>D S 3</p>
    <p>D S 2</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>Word Attention</p>
    <p>w1 w2 w3 w4 w5 q0 q1</p>
    <p>w1 w2 w3 w4 w5 s1 s2</p>
    <p>SWAP-NET : Switch Mechanism Output is selected with maximum of final word and sentence probabilities</p>
    <p>s1 s2</p>
    <p>Sentence Attention</p>
    <p>Final Word Probabilities</p>
    <p>Final Sentence Probabilities</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>Word Encodings</p>
    <p>s1 s2</p>
    <p>Prediction with SWAP-NET: Encoding</p>
    <p>Input Document</p>
    <p>Word Encoder</p>
    <p>Sentence Encoder Sentence Encodings</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>Word Attention</p>
    <p>Sentence Attention</p>
    <p>D S 1</p>
    <p>D W 1</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>Q=0</p>
    <p>Prediction with SWAP-NET: Decoding Step 1</p>
    <p>Switch</p>
    <p>Switch has two states, Q = 0 : word selection and Q = 1 : sentence selection</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>s1 s2</p>
    <p>W2</p>
    <p>Output</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>D S 1</p>
    <p>D S 2</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>s1 s2</p>
    <p>Q=1 Switch</p>
    <p>Word Attention</p>
    <p>Sentence Attention</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>s1 s2</p>
    <p>W2</p>
    <p>Output</p>
    <p>S1</p>
    <p>Prediction with SWAP-NET: Decoding Step 2</p>
  </div>
  <div class="page">
    <p>E S 1</p>
    <p>E W 5</p>
    <p>E W 4</p>
    <p>E W 3</p>
    <p>E W 2</p>
    <p>E W 1</p>
    <p>E S 2</p>
    <p>D S 1</p>
    <p>D S 3</p>
    <p>D S 2</p>
    <p>D W 3</p>
    <p>D W 2</p>
    <p>D W 1</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>W2</p>
    <p>Output</p>
    <p>S1</p>
    <p>W5</p>
    <p>Q=0</p>
    <p>Switch</p>
    <p>w1 w2 w3 w4 w5</p>
    <p>s1 s2</p>
    <p>Word Attention</p>
    <p>Sentence Attention</p>
    <p>Prediction with SWAP-NET: Decoding Step 2</p>
  </div>
  <div class="page">
    <p>A Mechanism to Combine Word Level Attentions and Sentence Level Attentions</p>
    <p>Q1 : How can the two attentions be combined?</p>
    <p>Q2 : How can the summaries be generated considering both the attentions?</p>
    <p>Sentence-Word ? ?</p>
    <p>Switch Q2</p>
    <p>Generate Summary</p>
    <p>Questions</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>= Ps +  Pi</p>
    <p>Top 3 sentences with maximum scores are chosen as summary</p>
    <p>Score of Given Sentence = (Sentence Probability) + (Sum of its keyword Probabilities)</p>
    <p>Summary Generation</p>
    <p>House prices across the UK will rise at a fraction of last years frenetic pace, forecasts show</p>
    <p>Probability of Sentence Ps</p>
    <p>show</p>
    <p>P7</p>
    <p>forecasts</p>
    <p>P6</p>
    <p>pace</p>
    <p>P5</p>
    <p>frenetic</p>
    <p>P4</p>
    <p>fraction</p>
    <p>P3</p>
    <p>prices rise</p>
    <p>P1 P2KeyWord Probability</p>
    <p>i=1</p>
    <p>k</p>
    <p>where k is number of keywords in sentence S</p>
  </div>
  <div class="page">
    <p>Extractive Summarization Methods</p>
    <p>Sentence Encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>(with decoder)</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>Word Encodings wrt other words</p>
    <p>Word Label Prediction</p>
    <p>(with decoder) SWAP-NET</p>
    <p>Sentence encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>(with decoder)</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>NN (Cheng and Lapata, 2016)</p>
    <p>Sentence Encodings wrt other sentences</p>
    <p>Sentence Label Prediction</p>
    <p>Sentence Encoding wrt words in it</p>
    <p>Pre-trained word embeddings</p>
    <p>Word Encodings wrt other words</p>
    <p>Document Encoding wrt its sentences</p>
    <p>SummaRuNNer (Nallapati et al., 2017)</p>
  </div>
  <div class="page">
    <p>Dataset and Evaluation</p>
    <p>Dataset Training Validation Test</p>
    <p>CNN 83568 1220 1093 Dailymail 193986 12147 10346</p>
    <p>Number Labeled Documents</p>
    <p>Sentences: Anonymised version of dataset given by (Cheng and Lapata, 2016)</p>
    <p>Words: Extract keywords from each gold summary using RAKE</p>
    <p>GroundTruth Binary Labels For Training</p>
    <p>ROUGE-1 (R1): Unigrams</p>
    <p>ROUGE-2 (R2): Bigrams</p>
    <p>ROUGE-L (RL): Longest Common Subsequences</p>
    <p>Standard Evaluation Metric: Three Variates of Rouge Score Comparing generated summaries and gold summaries for matching:</p>
    <p>Large Benchmark Dataset CNN/DailyMail News Corpus News articles from CNN/DailyMail along with human generated summary (gold summary) for each article</p>
    <p>Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic key word extraction from individual documents. Text Mining: Applications and Theory.</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Performance on DailyMail Dataset using limited length recall of Rouge</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Performance on CNN and Daily-Mail test set using the full length Rouge F score</p>
  </div>
  <div class="page">
    <p>Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia , Bulgaria or Nigeria - and say they have their parents ' hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons</p>
    <p>Their parents came to the U.S. for opportunities and now these four teens have them in abundance . The high-achieving high schoolers have each been accepted to all eight Ivy League schools : Brown University , Columbia University , Cornell University , Dartmouth College , Harvard University , University of Pennsylvania , Princeton University and Yale University . And as well as the Ivy League colleges , each of them has also been accepted to other top schools . While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira Khalif from Minnesota , Stefan Stoykov from Indiana , Victor Agbafe from North Carolina , and Harold Ekeh from New York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon . The teens have one more thing in common : they do n't know which school they 're going to pick yet . The daughter of Somali immigrants who has already received a U.N. award and wants to improve education across the world Star pupil : Munira Khalif , from St. Paul , Minnesota , says she has always been driven by the thought that her parents , who left Somalia during the civil war , fled to the U.S. so she would have better opportunities Munira Khalif , who attends Mounds Park Academy in St. Paul , Minnesota , was shocked when she was accepted by eight Ivy Schools and three others - but her teachers were not . ` She is composed and she is just articulate all the time , ' Randy Comfort , an upper school director at the private school , told KMSP . ` She 's pretty remarkable . ' The 18-year-old student , who was born and raised in Minnesota after her parents fled Somalia during the civil war , she said she was inspired to work hard because of the opportunities her family and the U.S. had given her . ` The thing is , when you come here as an immigrant , you 're hoping to have opportunities not only for yourself , but for your kids , ' she told the channel . ` And that 's always been at the back of my mind . ' As well as achieving top grades , Khalif has immersed herself in other activities both in and out of school - particularly those aimed at doing good . She was one of nine youngsters in the world to receive the UN Special Envoy for Global Education 's Youth Courage Award for her education activism , which she started when she was just 13 .</p>
    <p>Meet the four immigrant students each accepted to ALL EIGHT Ivy League schools who want to pay back their parents who moved to the U.S. to give them a better PUBLISHED: 19:56 BST, 9</p>
    <p>Gold Summary</p>
    <p>Summary Generated by SWAP-NET</p>
    <p>Example</p>
  </div>
  <div class="page">
    <p>Summary Generated by SWAP-NET</p>
    <p>While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon</p>
    <p>SWAP-NET Predicted Keywords</p>
    <p>SWAP-NET predictions highlighted in green</p>
  </div>
  <div class="page">
    <p>Keywords: Ground truth vs. SWAP-NET predictions</p>
    <p>Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia , Bulgaria or Nigeria - and say they have their parents ' hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons</p>
    <p>Gold Summary</p>
    <p>While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon</p>
    <p>SWAP-NET key words (green) and Ground truth (blue)</p>
    <p>While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon</p>
  </div>
  <div class="page">
    <p>While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon</p>
    <p>Summary Generated by SWAP-NET:</p>
    <p>Gold Summary:</p>
    <p>While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . And all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon</p>
    <p>Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia , Bulgaria or Nigeria - and say they have their parents ' hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons</p>
    <p>Almost no keyword is repeated across different sentence in the summary</p>
    <p>Presence of key words in all the overlapping segments of text with the gold summary</p>
    <p>Most of the predicted keywords are actual keywords</p>
    <p>Most of the extracted summary sentences contain keywords</p>
    <p>Large proportion of key words from the gold summary present in the generated summary</p>
    <p>Observations</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Average pairwise cosine distance between paragraph vector representations of sentences in summaries to measure semantic redundancy in summaries</p>
    <p>Highlights the importance of key words in finding salient sentences for extractive summaries</p>
    <p>SWAP-NET summaries are similar in redundancy to the Gold summary</p>
    <p>Key word coverage measures the proportion of key words from those in the gold summary present in the generated summary</p>
    <p>Sentences with key words measures the proportion of sentences containing at least one key word</p>
  </div>
  <div class="page">
    <p>We develop SWAP-NET, a neural sequence-to- sequence model for extractive summarization</p>
    <p>By effective modelling of interactions between sentences and key words, SWAP- NET outperforms state-of-the-art extractive single-document summarizers</p>
    <p>SWAP-NET models these interactions using a new two-level pointer network based architecture with a switching mechanism</p>
    <p>Experiments suggest that modelling sentence-keyword interaction has the desirable property of less semantic redundancy in summaries generated by SWAP-NET</p>
    <p>Conclusion</p>
    <p>An implementation of SWAP-NET and generated summaries from the test sets are available online: https://github.com/aishj10/swap-net</p>
  </div>
</Presentation>
