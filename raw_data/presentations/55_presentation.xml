<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Analytical Cache Models with Applications to Cache Partitioning</p>
    <p>G. Edward Suh, Srinivas Devadas, and Larry Rudolph</p>
    <p>LCS, MIT</p>
  </div>
  <div class="page">
    <p>Motivation Memory system performance is critical Everyone thinks about their own application</p>
    <p>But modern computer systems execute multiple applications concurrently/simultaneously Context switches cause cold misses Simultaneous applications compete for cache space</p>
    <p>Caches should be managed more carefully, considering multiple processes</p>
    <p>Explicit management of cache space =&gt; partitioning Cache-aware job schedulers</p>
  </div>
  <div class="page">
    <p>Related Work Analytical Cache Models</p>
    <p>Thibaut and Stone (1987) Agarwal, Horowitz and Hennessy (1989) Both only focus on long time quanta Inputs are hard to obtain on-line</p>
    <p>Cache Partitioning Stone, Turek and Wolf (1992)</p>
    <p>Optimal cache partitioning for very short time quanta</p>
    <p>Our Model &amp; Partitioning Work for any time quantum Inputs are easier to obtain (possible to estimate on-line)</p>
  </div>
  <div class="page">
    <p>Input C: Cache Size Schedule: job sequences with time quantum (TA) MA(x): a miss rate as a function of cache size for Process A</p>
    <p>Output Overall miss-rate (OMR) for multi-tasking</p>
    <p>Cache Model</p>
    <p>Overall Miss Rate</p>
    <p>MA(x)C Schedule</p>
    <p>Cache Size</p>
    <p>M is</p>
    <p>sR</p>
    <p>a t e</p>
    <p>M is</p>
    <p>s R</p>
    <p>a t e</p>
    <p>Cache Size</p>
    <p>M is</p>
    <p>sR</p>
    <p>a t e</p>
    <p>Our Multi-tasking Cache Model</p>
  </div>
  <div class="page">
    <p>The miss-rate of a process is a function of cache size alone, not time One MR(size) per application</p>
    <p>Curve is averaged over application lifetime In cases of high variance</p>
    <p>Split the application into phases One MR(size) per phase</p>
    <p>Generated off-line (or on-line with HW support) No shared memory space among processes</p>
    <p>Assumptions</p>
  </div>
  <div class="page">
    <p>Assumptions: Cont.</p>
    <p>Fully-associative caches Extended to set-associative caches (memo 433) The fully-associative model works for setassociative cache partitioning</p>
    <p>LRU replacement policy Time in terms of the number of memory references</p>
    <p>The number of memory reference can be easily converted to real time in a steady-state</p>
  </div>
  <div class="page">
    <p>Independent Footprint xA(t) Independent footprint</p>
    <p>The amount of data for Process A at time t starting from an empty cache, xA(0) = 0 Assume only one process executes</p>
    <p>Changes If hit, xA(t+1) = xA(t) If miss, xA(t+1) = MIN[ xA(t) + 1, C ]</p>
    <p>If we approximate real value of xA(t) with its expectation:</p>
    <p>E[xA(t+1)] = MIN[ E[xA(t)] + PA(t), C ] = MIN[ E[xA(t)] + MA(E[xA(t)]), C ]</p>
  </div>
  <div class="page">
    <p>Dependent Footprint xA(t)</p>
    <p>Dependent footprint The amount of data for Process A when multiple processes concurrently execute Obtained from the given schedule and the independent footprint of all processes</p>
    <p>Example Four processes: A, B, C, D round-robin schedule: ABCDABCD</p>
  </div>
  <div class="page">
    <p>An infinite size cache when Process A is executed for time t</p>
    <p>M R</p>
    <p>U D</p>
    <p>a ta</p>
    <p>LR U</p>
    <p>D a</p>
    <p>ta</p>
    <p>D-1 C-1 B-1 D-3A-1 C-2D-2 B-2 A-2 C-3A0</p>
    <p>xA(t) xA(t+TA)- xA(t)</p>
    <p>t t+TA</p>
    <p>xA(t)</p>
    <p>Independent Footprint of A</p>
    <p>Time B</p>
    <p>lo ck</p>
    <p>s</p>
    <p>Dependent Footprint xA(t): Cont.</p>
    <p>Compute block sizes from left: A0,D-1,C-1,B-1,A-1,D-2,</p>
    <p>Use independent footprint</p>
    <p>Until cache is full</p>
  </div>
  <div class="page">
    <p>An infinite size cache when Process A is executed for time t</p>
    <p>M R</p>
    <p>U D</p>
    <p>a ta</p>
    <p>LR U</p>
    <p>D a</p>
    <p>ta</p>
    <p>D-1 C-1 B-1 D-3A-1 C-2D-2 B-2 A-2 C-3A0</p>
    <p>Dependent Footprint xA(t): Cont.</p>
    <p>Cache Size (C)</p>
    <p>Case 1: dormant process block is the LRU xA(t) = A0+ A-1 = xA(t+TA)</p>
  </div>
  <div class="page">
    <p>An infinite size cache when Process A is executed for time t</p>
    <p>M R</p>
    <p>U D</p>
    <p>a ta</p>
    <p>LR U</p>
    <p>D a</p>
    <p>ta</p>
    <p>D-1 C-1 B-1 D-3A-1 C-2D-2 B-2 A-2 C-3A0</p>
    <p>Dependent Footprint xA(t): Cont.</p>
    <p>Cache Size (C)</p>
    <p>Case 1: dormant process block is the LRU xA(t) = A0+ A-1 = xA(t+TA)</p>
    <p>Case 2: active process block is the LRU xA(t) = C-(D0+C0+B0+D-1+C-1+B-1)</p>
    <p>= C-xD(TD)-xC(TC)- xB(TB)</p>
  </div>
  <div class="page">
    <p>Computing the Miss Probability: PA(t)</p>
    <p>Effective cache size xA(t): The amount of data in a cache for process A at time t</p>
    <p>The probability to miss at time t</p>
    <p>PA(t) = MA(xA(t))</p>
    <p>Process As Data xA(t)</p>
    <p>Other Process Data</p>
    <p>Cache at time t</p>
    <p>Cache Size</p>
    <p>MA(x)</p>
    <p>PA(t) M</p>
    <p>is s</p>
    <p>R at</p>
    <p>e</p>
    <p>xA(t)</p>
  </div>
  <div class="page">
    <p>Miss-rate of Process A In a steady-state, all time quanta of Process A are identical Time starts (t=0) at the beginning of a time quantum</p>
    <p>= AT</p>
    <p>A A</p>
    <p>A (t)dtPT mr</p>
    <p>P ro</p>
    <p>ba bi</p>
    <p>lit y</p>
    <p>to M</p>
    <p>is s</p>
    <p>Integrate</p>
    <p>The number of misses</p>
    <p>PA(t)</p>
    <p>Time TA</p>
    <p>Estimating Miss-Rate</p>
    <p>Overall miss-rate (OMR) Weighted sum of each process miss-rate</p>
  </div>
  <div class="page">
    <p>Model Summary</p>
    <p>Miss-rate Curve MA(x)</p>
    <p>OMR</p>
    <p>IF xA(t) DF xA(t)</p>
    <p>Miss-rate Curve MB(x)</p>
    <p>IF xB(t) DF xB(t)</p>
    <p>Miss-rate mrA</p>
    <p>Miss-rate mrB(t)</p>
    <p>Schedule</p>
    <p>Schedule</p>
    <p>=</p>
    <p>N</p>
    <p>i ii</p>
    <p>sum</p>
    <p>Tmr T 1</p>
    <p>AT</p>
    <p>A A</p>
    <p>)dtt(xM T 0</p>
    <p>)( 1Cache</p>
    <p>snapshot))(()]([ )]1([</p>
    <p>txMtxE</p>
    <p>txE</p>
    <p>AAA</p>
    <p>A</p>
    <p>+=</p>
    <p>+</p>
  </div>
  <div class="page">
    <p>Model vs. Simulation: 2 Processes</p>
    <p>Time Quantum</p>
    <p>M is</p>
    <p>sra</p>
    <p>te</p>
    <p>Simulation</p>
    <p>Model</p>
    <p>Miss-rate (vpr+vortex, 32KB)</p>
  </div>
  <div class="page">
    <p>Model vs. Simulation: 4 Processes</p>
    <p>Miss-rate (vpr+vortex+gcc+bzip2, 32KB)</p>
    <p>Time Quantum</p>
    <p>M is</p>
    <p>sra</p>
    <p>te</p>
    <p>Simulation Model</p>
  </div>
  <div class="page">
    <p>Cache Partitioning</p>
    <p>Time-sharing degrades the cache performance significantly for some time quanta</p>
    <p>Due to dumb allocation by LRU policy Could be improved by explicit cache partitioning</p>
    <p>Specifying a partition Dedicated Area (DA)</p>
    <p>Cache blocks that only Process A can use</p>
    <p>Shared Area (S) Cache blocks that any process can use while it is active</p>
  </div>
  <div class="page">
    <p>Strategy</p>
    <p>Off-line profiling of MR(size) curves One for each phase Independent of other processes Can also be obtained on-line with HW support</p>
    <p>On-line partitioning Partitioning decision based on the model Modify the LRU policy to partition the cache</p>
  </div>
  <div class="page">
    <p>Optimal Cache Partition</p>
    <p>Dedicated areas (DA) specify the initial amount of data for each process</p>
    <p>xA(0) = DA Shared (S) and dedicated (DA) areas specify the maximum cache space for each process</p>
    <p>CA = DA + S The model can estimate the miss-rate for a given partition Use a gradient based search algorithm</p>
  </div>
  <div class="page">
    <p>Simulation Results: Fully-Associative Caches</p>
    <p>Time Quantum</p>
    <p>M is</p>
    <p>sra</p>
    <p>te</p>
    <p>LRU Partition</p>
  </div>
  <div class="page">
    <p>From Full to Partial Associative Use the fully-associative model and curves to determine DA, S Modify the LRU replacement policy to partition</p>
    <p>Count the number of cache blocks for each process (XA) Try to match XA to the allocated cache space Replacement (Process A active)</p>
    <p>Replace Process As LRU block if Replace Process Bs LRU block if Replace the standard LRU block if there is no over-allocated process</p>
    <p>Add a small victim cache (16 entries)</p>
    <p>SDX AA + BB DX</p>
  </div>
  <div class="page">
    <p>Simulation Results: Set-Associative Caches</p>
    <p>Time Quantum</p>
    <p>M is</p>
    <p>sra</p>
    <p>te</p>
    <p>LRU Partition</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Analytical cache model Very accurate, yet tractable Works for any cache size and time quanta Applicable to set-associative cache partitioning</p>
    <p>Applications Dynamic cache partitioning with on-line/off-line approximations of miss-rate curves Various scheduling problems</p>
  </div>
</Presentation>
