<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Stock Movement Prediction from Tweets and Historical Prices</p>
    <p>Yumo Xu and Shay B. Cohen Institute for Language, Cognition, and Computation</p>
    <p>School of Informatics, University of Edinburgh</p>
    <p>ACL, 2018. https://yumoxu.github.io/, yumo.xu@ed.ac.uk</p>
  </div>
  <div class="page">
    <p>Who cares about stock movements?</p>
    <p>No one would be unhappy if they could predict stock movements</p>
    <p>Investor Government Researcher</p>
  </div>
  <div class="page">
    <p>Who cares about stock movements?</p>
    <p>No one would be unhappy if they could predict stock movements</p>
    <p>Investor Government Researcher</p>
  </div>
  <div class="page">
    <p>Who cares about stock movements?</p>
    <p>No one would be unhappy if they could predict stock movements</p>
    <p>Investor Government Researcher</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I Two mainstreams in finance: technical and fundamental analysis</p>
    <p>I Two main content resources in NLP: public news and social media I History of NLP models</p>
    <p>Feature engineering (before 2010)</p>
    <p>Topic models (2013-2015)</p>
    <p>Event-driven neural nets (2014-2015)</p>
    <p>Hierarchical attention nets (2018)</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I Two mainstreams in finance: technical and fundamental analysis I Two main content resources in NLP: public news and social media</p>
    <p>I History of NLP models</p>
    <p>Feature engineering (before 2010)</p>
    <p>Topic models (2013-2015)</p>
    <p>Event-driven neural nets (2014-2015)</p>
    <p>Hierarchical attention nets (2018)</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I Two mainstreams in finance: technical and fundamental analysis I Two main content resources in NLP: public news and social media I History of NLP models</p>
    <p>Feature engineering (before 2010)</p>
    <p>Topic models (2013-2015)</p>
    <p>Event-driven neural nets (2014-2015)</p>
    <p>Hierarchical attention nets (2018)</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I Two mainstreams in finance: technical and fundamental analysis I Two main content resources in NLP: public news and social media I History of NLP models</p>
    <p>Feature engineering (before 2010)</p>
    <p>Topic models (2013-2015) Generative</p>
    <p>Event-driven neural nets (2014-2015)</p>
    <p>Hierarchical attention nets (2018)</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>I Two mainstreams in finance: technical and fundamental analysis</p>
    <p>I Two main content resources in NLP: public news and social media I History of NLP models</p>
    <p>Feature engineering (before 2010)</p>
    <p>Topic models (2013-2015) Generative</p>
    <p>Event-driven neural nets (2014-2015)</p>
    <p>Hierarchical attention nets (2018)</p>
  </div>
  <div class="page">
    <p>However, it has never been easy...</p>
    <p>Complexities The market is highly stochastic, and we make</p>
    <p>temporally-dependent predictions from chaotic data.</p>
  </div>
  <div class="page">
    <p>Divide and Treat</p>
  </div>
  <div class="page">
    <p>Divide and treat</p>
    <p>Random walk theory (Malkiel, 1999)</p>
    <p>When a company suffers from a major scandal on a trading day, its stock price will have a downtrend in the coming trading days Public information needs time to be absorbed into movements over time (Luss and dAspremont, 2015), and thus is largely shared across temporally-close predictions</p>
  </div>
  <div class="page">
    <p>Problem Formulation</p>
    <p>Stock Movement Prediction I We estimate the binary movement where 1 denotes rise and 0 denotes fall I Target trading day: d I We use the market information comprising relevant tweets, and historical prices, in</p>
    <p>the lag [d  d, d  1] where d is a fixed lag size</p>
  </div>
  <div class="page">
    <p>Generative Process</p>
    <p>X</p>
    <p>|D|</p>
    <p>Z</p>
    <p>y I T eligible trading days in the d lag I Encode observed market information as a</p>
    <p>random variable X = [x1; . . . ; xT ]</p>
    <p>I Generate the latent driven factor Z = [z1; . . . ; zT ]</p>
    <p>I Generate stock movements y = [y1, . . . , yT ] from X, Z</p>
  </div>
  <div class="page">
    <p>Generative Process</p>
    <p>X</p>
    <p>|D|</p>
    <p>Z</p>
    <p>y I T eligible trading days in the d lag I Encode observed market information as a</p>
    <p>random variable X = [x1; . . . ; xT ] I Generate the latent driven factor</p>
    <p>Z = [z1; . . . ; zT ]</p>
    <p>I Generate stock movements y = [y1, . . . , yT ] from X, Z</p>
  </div>
  <div class="page">
    <p>Generative Process</p>
    <p>X</p>
    <p>|D|</p>
    <p>Z</p>
    <p>y I T eligible trading days in the d lag I Encode observed market information as a</p>
    <p>random variable X = [x1; . . . ; xT ] I Generate the latent driven factor</p>
    <p>Z = [z1; . . . ; zT ] I Generate stock movements</p>
    <p>y = [y1, . . . , yT ] from X, Z</p>
  </div>
  <div class="page">
    <p>Factorization</p>
    <p>I For multi-task learning, we model p (y|X ) =</p>
    <p>Z p (y, Z|X ) instead of p(yT |X ) Main target: yT Temporal auxiliary target: y  = [y1, . . . , yT 1]</p>
    <p>I Factorization</p>
    <p>p (y, Z|X ) = p (yT |X, Z ) p(zT |z&lt;T , X ) T1</p>
    <p>t=1</p>
    <p>p (yt|xt, zt ) p (zt|z&lt;t, xt, yt )</p>
  </div>
  <div class="page">
    <p>Primary components</p>
    <p>X</p>
    <p>|D|</p>
    <p>Z</p>
    <p>y 1 Market Information Encoder (MIE)</p>
    <p>Encodes X 2 Variational Movement Decoder (VMD)</p>
    <p>Infers Z with X, y and decodes stock movements y from X, Z</p>
  </div>
  <div class="page">
    <p>StockNet architecture</p>
    <p>z1 z2 z3</p>
    <p>h2 h3</p>
    <p>Output</p>
    <p>hdec</p>
    <p>henc</p>
    <p>log 2</p>
    <p>z</p>
    <p>N (0, I)</p>
    <p>DKL  N (, 2) k N (0, I)</p>
    <p>&quot;</p>
    <p>Variational encoder</p>
    <p>Variational decoder</p>
    <p>Bi-GRUs Message Embedding Layer (d) VAEs</p>
    <p>h1</p>
    <p>Attention AttentionAttention</p>
    <p>(a) Variational Movement Decoder (VMD)</p>
    <p>Message Corpora</p>
    <p>Historical Prices</p>
    <p>Temporal Attention</p>
    <p>Training Objective</p>
    <p>y1 y2</p>
    <p>y3</p>
    <p>(c) Attentive Temporal Auxiliary (ATA)</p>
    <p>g1 g2 g3</p>
  </div>
  <div class="page">
    <p>Variational Movement Decoder</p>
    <p>I Goal: recurrently infer Z from X, y and decode y from X, Z I Challenge: posterior inference is intractable in our factorized model</p>
    <p>VAE solutions I Neural approximation and reparameterization I Recurrent ELBO I Adopt a posterior approximator</p>
    <p>q (zt|z&lt;t, xt, yt ) N(,2I)</p>
    <p>where  = {,}</p>
  </div>
  <div class="page">
    <p>Variational Movement Decoder</p>
    <p>I Goal: recurrently infer Z from X, y and decode y from X, Z I Challenge: posterior inference is intractable in our factorized model</p>
    <p>VAE solutions I Neural approximation and reparameterization I Recurrent ELBO I Adopt a posterior approximator</p>
    <p>q (zt|z&lt;t, xt, yt ) N(,2I)</p>
    <p>where  = {,}</p>
  </div>
  <div class="page">
    <p>StockNet architecture</p>
    <p>z1 z2 z3</p>
    <p>h2 h3</p>
    <p>Output</p>
    <p>hdec</p>
    <p>henc</p>
    <p>log 2</p>
    <p>z</p>
    <p>N (0, I)</p>
    <p>DKL  N (, 2) k N (0, I)</p>
    <p>&quot;</p>
    <p>Variational encoder</p>
    <p>Variational decoder</p>
    <p>Bi-GRUs Message Embedding Layer (d) VAEs</p>
    <p>h1</p>
    <p>Attention AttentionAttention</p>
    <p>(a) Variational Movement Decoder (VMD)</p>
    <p>Message Corpora</p>
    <p>Historical Prices</p>
    <p>Temporal Attention</p>
    <p>Training Objective</p>
    <p>y1 y2</p>
    <p>y3</p>
    <p>(c) Attentive Temporal Auxiliary (ATA)</p>
    <p>g1 g2 g3</p>
  </div>
  <div class="page">
    <p>Interface between VMD and ATA</p>
    <p>g2 g3g1</p>
    <p>Dependency Score</p>
    <p>Information Score</p>
    <p>Temporal Attention</p>
    <p>Training Objective</p>
    <p>gT</p>
    <p>yT</p>
    <p>I Integrate the deterministic feature ht and the latent variable zt</p>
    <p>gt = tanh(Wg [xt, h s t , zt ] + bg )</p>
    <p>I Decode movement hypothesis: first auxiliary targets, then main target</p>
    <p>I Temporal attention: v</p>
  </div>
  <div class="page">
    <p>Attentive Temporal Auxiliary</p>
    <p>I Break down the approximated L to temporal objectives f  RT1</p>
    <p>ft = log p (yt|xt, zt ) DKL [q (zt|z&lt;t, xt, yt )  p (zt|z&lt;t, xt )]</p>
    <p>I Reuse v to build the final temporal weight vector v  R1T</p>
    <p>v = [v, 1]</p>
    <p>where   [0, 1] controls the overall auxiliary effects I Recompose F</p>
    <p>F (,; X, y ) = 1 N</p>
    <p>N</p>
    <p>n</p>
    <p>v (n)f (n)</p>
  </div>
  <div class="page">
    <p>Experimental setup</p>
    <p>I Dataset Two-year daily price movements of 88 stocks Two components: a Twitter dataset and a historical price dataset Training: 20 months, 20,339 movements Development: 2 months, 2,555 movements Test: 2 months, 3,720 movements</p>
    <p>I Lag window: 5 I Metrics: accuracy and Matthews Correlation Coefficient (MCC) I Comparative study: five baselines from different genres and five StockNet variations</p>
  </div>
  <div class="page">
    <p>Baselines and variants</p>
    <p>Baselines I RAND: a naive predictor making</p>
    <p>random guess I ARIMA: Autoregressive Integrated</p>
    <p>Moving Average I RANDFOREST (Pagolu et al., 2016) I TSLDA (Nguyen and Shirai, 2015) I HAN (Hu et al., 2018)</p>
    <p>StockNet variants I HEDGEFUNDANALYST: fully-equipped I TECHNICALANALYST: from only prices I FUNDAMENTALANALYST: from only tweets I INDEPENDENTANALYST: optimizing only</p>
    <p>the main target I DISCRIMINATIVEANALYST: a discriminative</p>
    <p>variant</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Baseline models Acc. MCC RAND 50.89 -0.002266 ARIMA 51.39 -0.020588 RANDFOREST 53.08 0.012929 TSLDA 54.07 0.065382 HAN 57.64 0.051800</p>
    <p>StockNet variations Acc. MCC TECHNICALANALYST 54.96 0.016456 FUNDAMENTALANALYST 58.23 0.071704 INDEPENDENTANALYST 57.54 0.036610 DISCRIMINATIVEANALYST 56.15 0.056493 HEDGEFUNDANALYST 58.23 0.080796</p>
    <p>Baseline comparison I The accuracy of 56% is generally</p>
    <p>reported as a satisfying result (Nguyen and Shirai, 2015)</p>
    <p>I ARIMA: does not yield satisfying results</p>
    <p>I Two best baselines: TSLDA and HAN Variant comparison</p>
    <p>I Two information sources are integrated effectively</p>
    <p>I Generative framework incorporates randomness properly</p>
  </div>
  <div class="page">
    <p>Effects of temporal auxiliary</p>
    <p>Ac c.</p>
    <p>Acc. MCC</p>
    <p>Ac c.</p>
    <p>Acc. MCC</p>
    <p>M CC</p>
    <p>M CC0.05</p>
    <p>I The auxiliary weight   [0, 1] controls overall auxiliary effects</p>
    <p>v = [v, 1]</p>
    <p>I Our models do not linearly benefit from temporal auxiliary</p>
    <p>I Tweaking  acts as a trade-off between focusing on the main target and generalizing by denoising</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>I We demonstrated the effectiveness of deep generative approaches for stock movement prediction from social media</p>
    <p>I Outlook Better way to integrate fundamental information and technical indicators Other market signals, e.g. financial disclosures, periodic analyst reports and company profiles Investment simulation with modern portfolio theory</p>
    <p>I Dataset is available at https://github.com/yumoxu/stocknet-dataset</p>
  </div>
  <div class="page">
    <p>Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, and Tie-Yan Liu. 2018. Listening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, Los Angeles, California, USA, pages 261269.</p>
    <p>Ronny Luss and Alexandre dAspremont. 2015. Predicting abnormal returns from news using text classification. Quantitative Finance 15(6):9991012.</p>
    <p>Burton Gordon Malkiel. 1999. A random walk down Wall Street: including a life-cycle guide to personal investing. WW Norton &amp; Company.</p>
    <p>Thien Hai Nguyen and Kiyoaki Shirai. 2015. Topic modeling based sentiment analysis on social media for stock market prediction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing, China, volume 1, pages 13541364.</p>
    <p>Venkata Sasank Pagolu, Kamal Nayan Reddy, Ganapati Panda, and Babita Majhi. 2016. Sentiment analysis of twitter data for predicting stock market movements. In Proceedings of 2016 International Conference on Signal Processing, Communication, Power and Embedded System. IEEE, Rajaseetapuram, India, pages 13451350.</p>
  </div>
  <div class="page">
    <p>Appendix - Market Information Encoder</p>
    <p>Temporal input: xt = [ct, pt ]</p>
    <p>Corpus embedding ct I Multiple tweets with varied quality I Message embedding: Bi-GRU I Corpus embedding: messages</p>
    <p>composition with salience</p>
    <p>ut = softmax(w  u tanh(Wm,u Mt ))</p>
    <p>ct = Mt u  t</p>
    <p>Historical price vector pt I Price signals: the adjusted closing,</p>
    <p>highest and lowest</p>
    <p>pt = [ pct , p</p>
    <p>h t , p</p>
    <p>l t</p>
    <p>]</p>
    <p>I Normalization</p>
    <p>pt = pt/p c t1  1</p>
  </div>
  <div class="page">
    <p>Appendix - Variational Inference</p>
    <p>Latent factorization</p>
    <p>q (Z|X, y ) = T</p>
    <p>t=1</p>
    <p>q (zt|z&lt;t, xt, yt )</p>
    <p>Likelihood equation</p>
    <p>log p (y|X ) =DKL [q (Z|X, y )  p (Z|X, y )] +Eq(Z|X,y) [log p (y|X, Z )] DKL [q (Z|X, y )  p (Z|X )]</p>
    <p>Recurrent ELBO</p>
    <p>L(,; X, y )</p>
    <p>= T</p>
    <p>t=1</p>
    <p>Eq(zt|z&lt;t,xt,yt ) {</p>
    <p>log p (yt|xt, zt )</p>
    <p>DKL [q (zt|z&lt;t, xt, yt )  p (zt|z&lt;t, xt )] }</p>
    <p>log p (y|X )</p>
    <p>where the likelihood term</p>
    <p>p (yt|xt, zt ) = {</p>
    <p>p (yt|xt, zt ) , if t &lt; T p (yT |X, Z ) , if t = T .</p>
  </div>
  <div class="page">
    <p>Appendix - Attentive Temporal Auxiliary</p>
    <p>g2 g3g1</p>
    <p>Dependency Score</p>
    <p>Information Score</p>
    <p>Temporal Attention</p>
    <p>Training Objective</p>
    <p>gT</p>
    <p>yT I Information score</p>
    <p>vi = w  i tanh(Wg,i G</p>
    <p>)</p>
    <p>I Dependency score</p>
    <p>vd = g  T tanh(Wg,d G</p>
    <p>)</p>
    <p>I Integration</p>
    <p>v = (vi  vd )</p>
  </div>
  <div class="page">
    <p>Appendix - Trading-day Alignment</p>
    <p>I We reorganize our inputs, including the tweet corpora and historical prices, by aligning them to the T trading days in a lag</p>
    <p>I Specifically, on the t th trading day, we recognize market signals from the corpus Mt in [dt1, dt ) and the historical prices pt on dt1, for predicting the movement yt on dt</p>
  </div>
  <div class="page">
    <p>Appendix - Denoising Regularizer</p>
    <p>I Objective-level auxiliary can be regarded as a denoising regularizer: for a sample with a specific movement as the main target, the market source in the lag can be heterogeneous</p>
    <p>Example</p>
    <p>Affected by bad news, tweets on earlier days are negative but turn to positive due to timely crises management</p>
    <p>Without temporal auxiliary tasks, the model tries to identify positive signals on earlier days only for the main target of rise movement, which is likely to result in pure noise</p>
    <p>I Temporal auxiliary tasks help to Filter market sources in the lag as per their respective aligned auxiliary movements Encode more useful information into the latent driven factor Z</p>
  </div>
</Presentation>
