<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SOME NEW DIRECTIONS IN</p>
    <p>GRAPH-BASED SEMI</p>
    <p>SUPERVISED LEARNING</p>
    <p>XIAOJIN ZHU, ANDREW B. GOLDBERG, TUSHAR</p>
    <p>KHOT</p>
    <p>University of Wisconsin-Madison</p>
  </div>
  <div class="page">
    <p>Our position</p>
    <p>Current graph-based Semi-supervised</p>
    <p>learning (SSL) methods have three limitations:</p>
    <p>data is restricted to live on a single manifold</p>
    <p>learning must happen in batch mode</p>
    <p>the target label is assumed smooth on the</p>
    <p>manifold</p>
    <p>We propose three new directions:</p>
    <p>multiple manifolds learning</p>
    <p>Online SSL</p>
    <p>Compressive sensing for SSL</p>
  </div>
  <div class="page">
    <p>Background and notation</p>
    <p>Input: n labeled points {(xi,yi)}, m unlabeled {xi}</p>
    <p>Goal: learn f: XY</p>
    <p>Graph on n+m points, Wij edge weight</p>
    <p>Assumption: large edge weight  similar label</p>
    <p>Weight matrix W, degree matrix D, Laplacian matrix L=D-W</p>
    <p>Optimization:</p>
    <p>minimize the energy fLf,</p>
    <p>subject to given labels fiyi</p>
  </div>
  <div class="page">
    <p>Limitation 1: no intersecting</p>
    <p>manifolds</p>
    <p>Existing graph-based SSL works well on a</p>
    <p>single manifold:</p>
    <p>Or on multiple well-separated manifolds:</p>
    <p>Edge weight depends on simple (Euclidean)</p>
    <p>distance: the closer, the larger</p>
    <p>RBF weight:</p>
    <p>K nearest neighbor (1 if close, 0 otherwise)</p>
  </div>
  <div class="page">
    <p>Limitation 1: no intersecting</p>
    <p>manifolds</p>
    <p>But cannot handle intersecting manifolds:</p>
    <p>Euclidean-distance-based weights will mix up</p>
    <p>manifolds</p>
  </div>
  <div class="page">
    <p>Solution: local covariance</p>
    <p>The sample covariance matrix (ellipsoid)</p>
    <p>captures local geometry</p>
    <p>Similar nearby ellipsoids  large edge weight</p>
    <p>But how to measure covariance similarity?</p>
  </div>
  <div class="page">
    <p>A distance on covariance</p>
    <p>matrices</p>
    <p>Hellinger distance</p>
    <p>Symmetric, value in [0,1]</p>
    <p>Let p be the normal distribution at mean 0 with</p>
    <p>covariance 1, similarly for q</p>
    <p>Define the Hellinger distance between two</p>
    <p>covariance matrices as</p>
  </div>
  <div class="page">
    <p>Property of Hellinger distance</p>
    <p>Large value if the two covariance matrices are</p>
    <p>similar; close to 0, if they differ in density,</p>
    <p>dimensionality or orientation</p>
    <p>Ideal for tracing a manifold in a mixture of</p>
    <p>multiple manifolds</p>
  </div>
  <div class="page">
    <p>Hellinger distance for multi</p>
    <p>manifold</p>
    <p>Similar covariance  large weight</p>
    <p>Example: red=large weight, yellow=small</p>
    <p>weight</p>
    <p>Use this graph in manifold regularization  it</p>
    <p>will separate the manifolds.</p>
  </div>
  <div class="page">
    <p>Limitation 2: need all data at</p>
    <p>once</p>
    <p>In many applications, data stream in. Cannot</p>
    <p>store them all. Want:</p>
    <p>Online processing and then discard each</p>
    <p>incoming item</p>
    <p>Learn even when the item is unlabeled (different</p>
    <p>from standard online learning)</p>
    <p>Tolerate adversarial concept drifts (changes in</p>
    <p>XY)</p>
    <p>Theoretic guarantee</p>
    <p>Uses only finite memory budget</p>
  </div>
  <div class="page">
    <p>Online SSL setting</p>
    <p>necessarily iid, shows xt</p>
    <p>ft(xt)</p>
    <p>otherwise it abstains (unlabeled)</p>
    <p>given)</p>
  </div>
  <div class="page">
    <p>Solution: online convex</p>
    <p>programming</p>
    <p>Batch SSL minimizes a risk functional J(f) on</p>
    <p>all data</p>
    <p>If J can be decomposed into a sum of</p>
    <p>instantaneous convex risks Jt(f) on individual</p>
    <p>data item</p>
    <p>Then one can do gradient descent on Jt(f) at</p>
    <p>each step</p>
    <p>Even though each Jt(f) is different, one can</p>
    <p>show this gradient descent procedure</p>
    <p>optimizes something sensible: in particular,</p>
    <p>there is no regret</p>
  </div>
  <div class="page">
    <p>No-regret guarantee</p>
    <p>In online learning with concept drift, accuracy is not a good measure, because adversary can change the true labels arbitrarily often</p>
    <p>Instead, measure the difference to the best batch hypothesis f* (which will also be bad if concept drifts too often), known as the regret</p>
    <p>[Zinkevich03] the gradient descent procedure has zero regret asymptotically.</p>
  </div>
  <div class="page">
    <p>Online graph-based SSL</p>
    <p>This can be applied to graph-based SSL</p>
    <p>The instantaneous risk involves a subgraph</p>
    <p>from xt to all previous points</p>
    <p>Limited memory version: only keep a fixed</p>
    <p>length buffer, instead of all previous points</p>
    <p>Open questions: better ways to define the</p>
    <p>instantaneous risk, such that the manifold</p>
    <p>structure is summarized using finite memory.</p>
    <p>(on-going work)</p>
  </div>
  <div class="page">
    <p>Limitation 3: f has to be smooth</p>
    <p>Eigen value/vectors of Laplacian:</p>
    <p>Eigenvectors form orthonormal basis</p>
    <p>Any f can be decomposed as</p>
    <p>Existing SSL assumption: f uses a few low</p>
    <p>frequency eigenvectors, i.e., the corresponding</p>
    <p>i are large (non-zero).</p>
    <p>Low frequency eigenvectors: whose</p>
    <p>eigenvalues are close to zero</p>
  </div>
  <div class="page">
    <p>New assumption: sparsity</p>
    <p>Allow f to have high frequency eigenvectors,</p>
    <p>as long as  is sparse (a few large entries)</p>
    <p>Recent advances in compressive sensing</p>
    <p>determine when learning can happen</p>
    <p>The signal representation basis is</p>
    <p>The measurement basis is the canonical basis I</p>
    <p>(identity matrix)</p>
    <p>Labeled data in transductive learning =</p>
    <p>measurements made with random rows from I</p>
  </div>
  <div class="page">
    <p>SSL as compressive sensing</p>
    <p>Key quantity: coherence (I, )  max entry in</p>
    <p>Theorem: let there be n labeled points, m</p>
    <p>unlabeled points. Assume  has Sn+m non</p>
    <p>zero entries (but could be anywhere, both low</p>
    <p>and high freq) and f=  . Then</p>
    <p>labeled points is sufficient to exactly learn f.</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Unweighted ring graph with 1024 nodes</p>
    <p>Sparsity S=3, nonsmooth function</p>
    <p>Draw n random points to get label (true f values). Recovery f using L-1 minimization as standard in compressive sensing. Measure recovery error.</p>
    <p>Repeat several times for each n.</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Each trial is a dot</p>
    <p>Exact recovery happens when n&gt;35</p>
    <p>Compressive sensing  transductive learning for sparse but nonsmooth functions</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>We have presented three new research</p>
    <p>directions for graph-based SSL</p>
    <p>Multi-manifold learning</p>
    <p>Online SSL</p>
    <p>Compressive sensing</p>
    <p>We hope to inspire new research, making SSL</p>
    <p>an even more valuable tool for multimedia</p>
    <p>analysis.</p>
    <p>We thank the presenter, and you!</p>
  </div>
</Presentation>
