<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>GraphChi: Large-Scale Graph Computation on Just a PC</p>
    <p>Aapo Kyrl (CMU) Guy Blelloch (CMU)</p>
    <p>Carlos Guestrin (UW)</p>
    <p>OSDI12</p>
    <p>In co-opera+on with the GraphLab team.</p>
  </div>
  <div class="page">
    <p>BigData with Structure: BigGraph</p>
    <p>social graph social graph follow-graph consumer- products graph</p>
    <p>user-movie ra;ngs graph</p>
    <p>DNA interac;on</p>
    <p>graph</p>
    <p>WWW link graph</p>
    <p>etc.</p>
  </div>
  <div class="page">
    <p>Big Graphs != Big Data</p>
    <p>Data size: 140 billion connec;ons</p>
    <p>1 TB</p>
    <p>Not a problem!</p>
    <p>Computa;on:</p>
    <p>Hard to scale</p>
    <p>TwiQer network visualiza;on, by Akshay Java, 2009</p>
  </div>
  <div class="page">
    <p>Could we compute Big Graphs on a single machine?</p>
    <p>Disk-based Graph Computation</p>
    <p>Cant we just use the Cloud?</p>
  </div>
  <div class="page">
    <p>Wri;ng distributed applica;ons remains cumbersome.</p>
    <p>Cluster crash Crash in your IDE</p>
    <p>Distributed State is Hard to Program</p>
  </div>
  <div class="page">
    <p>Efficient Scaling</p>
    <p>Businesses need to compute hundreds of distinct tasks on the same graph  Example: personalized recommendations.</p>
    <p>Parallelize each task Parallelize across tasks</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task</p>
    <p>Task Task</p>
    <p>Complex Simple</p>
    <p>Expensive to scale</p>
  </div>
  <div class="page">
    <p>Other Benefits</p>
    <p>Costs  Easier management, simpler hardware.</p>
    <p>Energy Consumption  Full utilization of a single computer.</p>
    <p>Embedded systems, mobile devices  A basic flash-drive can fit a huge graph.</p>
  </div>
  <div class="page">
    <p>Research Goal</p>
    <p>Compute on graphs with billions of edges, in a reasonable *me, on a single PC.</p>
    <p>Reasonable = close to numbers previously reported for distributed systems in the literature.</p>
    <p>Experiment PC: Mac Mini (2012)</p>
  </div>
  <div class="page">
    <p>Computational Model</p>
    <p>GraphChi  Aapo Kyrola 9</p>
  </div>
  <div class="page">
    <p>Computational Model</p>
    <p>Graph G = (V, E)  directed edges: e = (source,</p>
    <p>destination)  each edge and vertex associated</p>
    <p>with a value (user-defined type)  vertex and edge values can be</p>
    <p>modified  (structure modification also</p>
    <p>supported) Data</p>
    <p>Data Data</p>
    <p>Data</p>
    <p>Data Data Data</p>
    <p>Data Data</p>
    <p>Data</p>
    <p>A B e</p>
    <p>Terms: e is an out-edge of A, and in-edge of B.</p>
  </div>
  <div class="page">
    <p>Data</p>
    <p>Data Data</p>
    <p>Data</p>
    <p>Data Data Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Vertex-centric Programming</p>
    <p>Think like a vertex  Popularized by the Pregel and GraphLab projects</p>
    <p>Historically, systolic computation and the Connection Machine</p>
    <p>MyFunc(vertex) { // modify neighborhood } Data</p>
    <p>Data Data</p>
    <p>Data</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>The Main Challenge of Disk-based Graph Computation:</p>
    <p>Random Access</p>
  </div>
  <div class="page">
    <p>Random Access Problem</p>
    <p>vertex in-neighbors out-neighbors</p>
    <p>....</p>
    <p>... or with file index pointers vertex in-neighbor-ptr out-neighbors</p>
    <p>....</p>
    <p>Random write</p>
    <p>Random read read</p>
    <p>synchronize</p>
    <p>Symmetrized adjacency file with values,</p>
    <p>For sufficient performance, millions of random accesses / second would be needed. Even for SSD, this is too much.</p>
  </div>
  <div class="page">
    <p>Possible Solutions</p>
    <p>[SSDAlloc, NSDI11]</p>
    <p>Too many small objects, need millions / sec.</p>
    <p>Associated values do not compress well, and are mutated.</p>
    <p>Expensive; The number of inter- cluster edges is big.</p>
    <p>Unpredictable performance.</p>
  </div>
  <div class="page">
    <p>Our Solution</p>
    <p>Parallel Sliding Windows (PSW)</p>
  </div>
  <div class="page">
    <p>Parallel Sliding Windows: Phases</p>
    <p>PSW processes the graph one sub-graph a time:</p>
    <p>In one iteration, the whole graph is processed.</p>
    <p>And typically, next iteration is started.</p>
  </div>
  <div class="page">
    <p>Vertices are numbered from 1 to n  P intervals, each associated with a shard on disk.  sub-graph = interval of vertices</p>
    <p>PSW: Shards and Intervals</p>
    <p>shard(1)</p>
    <p>interval(1) interval(2) interval(P)</p>
    <p>shard(2) shard(P)</p>
  </div>
  <div class="page">
    <p>PSW: Layout</p>
    <p>Shard 1</p>
    <p>Shards small enough to fit in memory; balance size of shards</p>
    <p>Shard: in-edges for interval of ver;ces; sorted by source-id</p>
    <p>in -e dg es fo</p>
    <p>r ve r; ce s 1. .1 00</p>
    <p>so rt ed</p>
    <p>b y so ur ce _i d</p>
    <p>Ver;ces 1..100</p>
    <p>Ver;ces 101..700</p>
    <p>Ver;ces 701..1000</p>
    <p>Ver;ces 1001..10000</p>
    <p>Shard 2 Shard 3 Shard 4 Shard 1</p>
  </div>
  <div class="page">
    <p>Ver;ces 1..100</p>
    <p>Ver;ces 101..700</p>
    <p>Ver;ces 701..1000</p>
    <p>Ver;ces 1001..10000</p>
    <p>Load all in-edges in memory</p>
    <p>Load subgraph for verJces 1..100</p>
    <p>What about out-edges? Arranged in sequence in other shards</p>
    <p>Shard 2 Shard 3 Shard 4</p>
    <p>PSW: Loading Sub-graph</p>
    <p>Shard 1</p>
    <p>in -e dg es fo</p>
    <p>r ve r; ce s 1. .1 00</p>
    <p>so rt ed</p>
    <p>b y so ur ce _i d</p>
  </div>
  <div class="page">
    <p>Shard 1</p>
    <p>Load all in-edges in memory</p>
    <p>Load subgraph for verJces 101..700</p>
    <p>Shard 2 Shard 3 Shard 4</p>
    <p>PSW: Loading Sub-graph</p>
    <p>Ver;ces 1..100</p>
    <p>Ver;ces 101..700</p>
    <p>Ver;ces 701..1000</p>
    <p>Ver;ces 1001..10000</p>
    <p>Out-edge blocks in memory</p>
    <p>in -e dg es fo</p>
    <p>r ve r; ce s 1. .1 00</p>
    <p>so rt ed</p>
    <p>b y so ur ce _i d</p>
  </div>
  <div class="page">
    <p>PSW Load-Phase</p>
    <p>Only P large reads for each interval.</p>
    <p>P2 reads on one full pass.</p>
  </div>
  <div class="page">
    <p>PSW: Execute updates</p>
    <p>Update-function is executed on intervals vertices  Edges have pointers to the loaded data blocks</p>
    <p>Changes take effect immediately  asynchronous.</p>
    <p>&amp;Data</p>
    <p>&amp;Data &amp;Data</p>
    <p>&amp;Data</p>
    <p>&amp;Data &amp;Data &amp;Data</p>
    <p>&amp;Data &amp;Data</p>
    <p>&amp;Data</p>
    <p>Block X</p>
    <p>Block Y</p>
    <p>Determinis;c scheduling prevents races between neighboring ver;ces.</p>
  </div>
  <div class="page">
    <p>PSW: Commit to Disk</p>
    <p>In write phase, the blocks are written back to disk  Next load-phase sees the preceding writes</p>
    <p>asynchronous.</p>
    <p>&amp;Data</p>
    <p>&amp;Data &amp;Data</p>
    <p>&amp;Data</p>
    <p>&amp;Data &amp;Data &amp;Data</p>
    <p>&amp;Data &amp;Data</p>
    <p>&amp;Data</p>
    <p>Block X</p>
    <p>Block Y</p>
    <p>In total: P2 reads and writes / full pass on the graph.  Performs well on both SSD and hard drive.</p>
  </div>
  <div class="page">
    <p>GraphChi: Implementation</p>
    <p>Evaluation &amp; Experiments</p>
  </div>
  <div class="page">
    <p>GraphChi</p>
    <p>C++ implementation: 8,000 lines of code  Java-implementation also available (~ 2-3x slower),</p>
    <p>with a Scala API.</p>
    <p>Several optimizations to PSW (see paper).</p>
    <p>Source code and examples: hQp://graphchi.org</p>
  </div>
  <div class="page">
    <p>EVALUATION: APPLICABILITY</p>
  </div>
  <div class="page">
    <p>Evaluation: Is PSW expressive enough?</p>
    <p>Graph Mining  Connected components  Approx. shortest paths  Triangle counting  Community Detection</p>
    <p>SpMV  PageRank  Generic</p>
    <p>Recommendations  Random walks</p>
    <p>Collaborative Filtering (by Danny Bickson)</p>
    <p>ALS  SGD  Sparse-ALS  SVD, SVD++  Item-CF</p>
    <p>Probabilistic Graphical Models  Belief Propagation</p>
    <p>Algorithms implemented for GraphChi (Oct 2012)</p>
  </div>
  <div class="page">
    <p>IS GRAPHCHI FAST ENOUGH?</p>
    <p>Comparisons to existing systems</p>
  </div>
  <div class="page">
    <p>Experiment Setting</p>
    <p>Mac Mini (Apple Inc.)  8 GB RAM  256 GB SSD, 1TB hard drive  Intel Core i5, 2.5 GHz</p>
    <p>Experiment graphs: Graph VerJces Edges P (shards) Preprocessing</p>
    <p>live-journal 4.8M 69M 3 0.5 min</p>
    <p>nevlix 0.5M 99M 20 1 min</p>
    <p>twiQer-2010 42M 1.5B 20 2 min</p>
    <p>uk-2007-05 106M 3.7B 40 31 min</p>
    <p>uk-union 133M 5.4B 50 33 min</p>
    <p>yahoo-web 1.4B 6.6B 50 37 min</p>
  </div>
  <div class="page">
    <p>Comparison to Existing Systems</p>
    <p>Notes: comparison results do not include +me to transfer the data to cluster, preprocessing, or the +me to load the graph from disk. GraphChi computes asynchronously, while all but GraphLab synchronously.</p>
    <p>PageRank</p>
    <p>See the paper for more comparisons.</p>
    <p>WebGraph Belief PropagaJon (U Kang et al.)</p>
    <p>Matrix FactorizaJon (Alt. Least Sqr.) Triangle CounJng</p>
    <p>GraphLab v1 (8 cores)</p>
    <p>GraphChi (Mac Mini)</p>
    <p>NeDlix (99B edges)</p>
    <p>Spark (50 machines)</p>
    <p>GraphChi (Mac Mini)</p>
    <p>TwiKer-2010 (1.5B edges)</p>
    <p>Pegasus / Hadoop (100</p>
    <p>machines)</p>
    <p>GraphChi (Mac Mini)</p>
    <p>Yahoo-web (6.7B edges)</p>
    <p>Hadoop (1636</p>
    <p>machines)</p>
    <p>GraphChi (Mac Mini)</p>
    <p>twiKer-2010 (1.5B edges)</p>
    <p>On a Mac Mini:  GraphChi can solve as big problems as exis;ng large-scale systems.</p>
    <p>Comparable performance.</p>
  </div>
  <div class="page">
    <p>PowerGraph Comparison  PowerGraph / GraphLab 2</p>
    <p>outperforms previous systems by a wide margin on natural graphs.</p>
    <p>With 64 more machines, 512 more CPUs:  Pagerank: 40x faster than</p>
    <p>GraphChi  Triangle counting: 30x faster</p>
    <p>than GraphChi.</p>
    <p>OSDI12</p>
    <p>GraphChi has state-of-the- art performance / CPU.</p>
    <p>vs.</p>
    <p>GraphChi</p>
  </div>
  <div class="page">
    <p>SYSTEM EVALUATION Sneak peek</p>
    <p>Consult the paper for a comprehensive evalua+on:  HD vs. SSD  Striping data across mul+ple</p>
    <p>hard drives  Comparison to an in-memory</p>
    <p>version  BoKlenecks analysis  Effect of the number of shards  Block size and performance.</p>
  </div>
  <div class="page">
    <p>Scalability / Input Size [SSD]</p>
    <p>Throughput: number of edges processed / second.</p>
    <p>Conclusion: the throughput remains roughly constant when graph size is increased.</p>
    <p>GraphChi with hard-drive is ~ 2x slower than SSD (if computa;onal cost low).</p>
    <p>Graph size</p>
    <p>domain</p>
    <p>twitter-2010</p>
    <p>uk-2007-05 uk-union</p>
    <p>yahoo-web</p>
    <p>PageRank -- throughput (Mac Mini, SSD)</p>
    <p>P er</p>
    <p>fo rm</p>
    <p>an ce</p>
    <p>Paper: scalability of other applica;ons.</p>
  </div>
  <div class="page">
    <p>Bottlenecks / Multicore</p>
    <p>Experiment on MacBook Pro with 4 cores / SSD.</p>
    <p>Computationally intensive applications benefit substantially from parallel execution.</p>
    <p>GraphChi saturates SSD I/O with 2 threads.</p>
    <p>J m e (s ec on</p>
    <p>ds )</p>
    <p>Number of threads</p>
    <p>Matrix Factoriza*on (ALS)</p>
    <p>Loading ComputaJon</p>
    <p>m e (s ec on</p>
    <p>ds )</p>
    <p>Number of threads</p>
    <p>Connected Components</p>
    <p>Loading ComputaJon</p>
  </div>
  <div class="page">
    <p>Evolving Graphs</p>
    <p>Graphs whose structure changes over time</p>
  </div>
  <div class="page">
    <p>Evolving Graphs: Introduction</p>
    <p>Most interesting networks grow continuously:  New connections made, some unfriended.</p>
    <p>Desired functionality:  Ability to add and remove edges in streaming</p>
    <p>fashion;  ... while continuing computation.</p>
    <p>Related work:  Kineograph (EuroSys 12), distributed system for</p>
    <p>computation on a changing graph.</p>
  </div>
  <div class="page">
    <p>PSW and Evolving Graphs</p>
    <p>Adding edges  Each (shard, interval) has an associated edge-buffer.</p>
    <p>Removing edges: Edge flagged as removed.</p>
    <p>interval(1)</p>
    <p>interval(2)</p>
    <p>interval(P)</p>
    <p>shard(j)</p>
    <p>edge-buffer(j, 1)</p>
    <p>edge-buffer(j, 2)</p>
    <p>edge-buffer(j, P)</p>
    <p>New edges (for example) TwiQer firehose</p>
  </div>
  <div class="page">
    <p>Recreating Shards on Disk</p>
    <p>When buffers fill up, shards a recreated on disk  Too big shards are split.</p>
    <p>During recreation, deleted edges are permanently removed.</p>
    <p>interval(1)</p>
    <p>interval(2)</p>
    <p>interval(P)</p>
    <p>shard(j)</p>
    <p>Re- create &amp; Split</p>
    <p>interval(1)</p>
    <p>interval(2)</p>
    <p>interval(P+1)</p>
    <p>shard(j)</p>
    <p>interval(1)</p>
    <p>interval(2)</p>
    <p>interval(P+1)</p>
    <p>shard(j+1)</p>
  </div>
  <div class="page">
    <p>EVALUATION: EVOLVING GRAPHS</p>
    <p>Streaming Graph experiment</p>
  </div>
  <div class="page">
    <p>Streaming Graph Experiment  On the Mac Mini:</p>
    <p>Streamed edges in random order from the twitter-2010 graph (1.5 B edges)</p>
    <p>With maximum rate of 100K or 200K edges/sec. (very high rate)</p>
    <p>Simultaneously run PageRank.  Data layout:</p>
    <p>Edges were streamed from hard drive  Shards were stored on SSD.</p>
    <p>edges</p>
  </div>
  <div class="page">
    <p>Ingest Rate</p>
    <p>Ed ge s / se c</p>
    <p>Time (hours)</p>
    <p>actual-100K actual-200K 100K 200K</p>
    <p>When graph grows, shard recrea;ons become more expensive.</p>
  </div>
  <div class="page">
    <p>Streaming: Computational Throughput</p>
    <p>Throughput varies strongly due to shard rewrites and asymmetric computa;on.</p>
    <p>Th ro uh</p>
    <p>gp ut E dg es /s ec</p>
    <p>Time (hours)</p>
    <p>Throughput (100K) Throughput (200K) Fixed graph</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Future Directions</p>
    <p>Come to the the poster on Monday to discuss!</p>
    <p>This work: small amount of memory.  What if have hundreds of GBs of RAM?</p>
    <p>Graph working memory (PSW) disk</p>
    <p>Computation 1 state Computation 2 state</p>
    <p>Computational state</p>
    <p>Graph working memory (PSW) disk</p>
    <p>Computation 1 state Computation 2 state</p>
    <p>Computational state</p>
    <p>Graph working memory (PSW) disk</p>
    <p>computational state</p>
    <p>Computation 1 state Computation 2 state</p>
    <p>RAM</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Parallel Sliding Windows algorithm enables processing of large graphs with very few nonsequential disk accesses.</p>
    <p>For the system researchers, GraphChi is a solid baseline for system evaluation  It can solve as big problems as distributed systems.</p>
    <p>Takeaway: Appropriate data structures as an alternative to scaling up.</p>
    <p>Source code and examples: http://graphchi.org License: Apache 2.0</p>
  </div>
  <div class="page">
    <p>Extra Slides</p>
  </div>
  <div class="page">
    <p>PSW is Asynchronous</p>
    <p>If V &gt; U, and there is edge (U,V, &amp;x) = (V, U, &amp;x), update(V) will observe change to x done by update(U):  Memory-shard for interval (j+1) will contain writes to shard(j) done</p>
    <p>on previous intervals.  Previous slide: If U, V in the same interval.</p>
    <p>PSW implements the Gauss-Seidel (asynchronous) model of computation  Shown to allow, in many cases, clearly faster convergence of</p>
    <p>computation than Bulk-Synchronous Parallel (BSP).  Each edge stored only once.</p>
    <p>V ti ( F (V t 0 , V</p>
    <p>t 1 , . . . , V</p>
    <p>t i1, V</p>
    <p>t1 i , V</p>
    <p>t1 i+1 , ...)</p>
    <p>Extended edi;on</p>
  </div>
  <div class="page">
    <p>Number of Shards</p>
    <p>Number of shards (P)</p>
    <p>Th ro</p>
    <p>ug hp</p>
    <p>ut (e</p>
    <p>dg es</p>
    <p>/s ec</p>
    <p>)</p>
    <p>Conn comp. (SSD)</p>
    <p>Pagerank (SSD)</p>
    <p>Pagerank (HD)</p>
    <p>Conn comp. (HD)</p>
    <p>Student Version of MATLAB</p>
    <p>If P is in the dozens, there is not much effect on performance.</p>
  </div>
  <div class="page">
    <p>I/O Complexity</p>
    <p>See the paper for theoretical analysis in the Aggarwal-Vitters I/O model.  Worst-case only 2x best-case.</p>
    <p>Intuition:</p>
    <p>shard(1)</p>
    <p>interval(1) interval(2) interval(P)</p>
    <p>shard(2) shard(P)</p>
    <p>Inter-interval edge is loaded from disk only once / itera;on.</p>
    <p>Edge spanning intervals is loaded twice / itera;on.</p>
  </div>
  <div class="page">
    <p>Multiple hard-drives (RAIDish)</p>
    <p>GraphChi supports striping shards to multiple disks  Parallel I/O.</p>
    <p>Pagerank Conn. components</p>
    <p>Se co nd</p>
    <p>s / ite</p>
    <p>ra ;o</p>
    <p>n</p>
    <p>Experiment on a 16-core AMD server (from year 2007).</p>
  </div>
  <div class="page">
    <p>Bottlenecks</p>
    <p>Disk IO Graph construc;on Exec. updates</p>
    <p>Connected Components on Mac Mini / SSD</p>
    <p>Cost of constructing the sub-graph in memory is almost as large as the I/O cost on an SSD  Graph construction requires a lot of random access in RAM  memory</p>
    <p>bandwidth becomes a bottleneck.</p>
  </div>
  <div class="page">
    <p>Computational Setting</p>
    <p>Constraints: A. Not enough memory to store the whole graph in</p>
    <p>memory, nor all the vertex values. B. Enough memory to store one vertex and its edges</p>
    <p>w/ associated values.</p>
    <p>Largest example graph used in the experiments:  Yahoo-web graph: 6.7 B edges, 1.4 B vertices</p>
    <p>Recently GraphChi has been used on a MacBook Pro to compute with the most recent TwiQer follow-graph (last year: 15 B edges)</p>
  </div>
</Presentation>
