<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>OASIS: Online Active Semi-Supervised Learning</p>
    <p>Andrew Goldberg, Xiaojin Zhu*, Alex Furger, Jun-Ming Xu</p>
    <p>University of Wisconsin-Madison</p>
    <p>AAAI 2011</p>
  </div>
  <div class="page">
    <p>The Problem We Consider</p>
  </div>
  <div class="page">
    <p>Is this</p>
    <p>semi-supervised learning?</p>
    <p>Yes, but sequential input, active query</p>
    <p>online learning?</p>
    <p>Yes, but learns on unlabeled items</p>
    <p>active learning?</p>
    <p>Yes, but learns on un-queried items</p>
    <p>OASIS = Online Active Semi-Supervised Learning</p>
  </div>
  <div class="page">
    <p>Main idea: Be Bayesian!</p>
    <p>Track all gaps with the posterior.</p>
    <p>semi-supervised learning</p>
    <p>online learning</p>
    <p>active learning</p>
    <p>all naturally follow.</p>
  </div>
  <div class="page">
    <p>The Margin in Supervised Learning</p>
    <p>E.g. SVM linear classifier</p>
  </div>
  <div class="page">
    <p>The Gap Assumption in SSL</p>
    <p>S3VM: find the largest unlabeled margin</p>
  </div>
  <div class="page">
    <p>The Need for a Multi-Modal Posterior</p>
    <p>There may be multiple candidate gaps</p>
  </div>
  <div class="page">
    <p>There may be multiple candidate gaps</p>
    <p>The Need for a Multi-Modal Posterior</p>
  </div>
  <div class="page">
    <p>There may be multiple candidate gaps</p>
    <p>The Need for a Multi-Modal Posterior</p>
  </div>
  <div class="page">
    <p>There may be multiple candidate gaps</p>
    <p>The Need for a Multi-Modal Posterior</p>
  </div>
  <div class="page">
    <p>Another Example of Multi-modal Posterior [courtesy of Kwang-Sung Jun]</p>
  </div>
  <div class="page">
    <p>Life is Easy Being Bayesian: Likelihood</p>
    <p>The null-category likelihood pushes w away from unlabeled points.  semi-supervised learning</p>
    <p>Inspired by [Lawrence &amp; Jordan NIPS04]</p>
  </div>
  <div class="page">
    <p>Life is Easy Being Bayesian: Update</p>
    <p>Sequential Bayesian update  online learning</p>
    <p>assume iid, not adversarial</p>
    <p>Cauchy prior</p>
  </div>
  <div class="page">
    <p>Life is Easy Being Bayesian: Predict</p>
    <p>Predict label</p>
    <p>Integrate out w</p>
    <p>If the posterior strongly disagree on xt, ask for its label  active learning</p>
  </div>
  <div class="page">
    <p>Life is Hard Being Bayesian!</p>
    <p>Particle filtering intractable</p>
  </div>
  <div class="page">
    <p>Particle Filtering Details</p>
    <p>Update weight bi by a multiplicative factor:</p>
    <p>if yt is revealed or queried</p>
    <p>if unlabeled</p>
    <p>Occasional resample-move to rejuvenize particles</p>
    <p>A single step of Metropolis-Hastings sampling</p>
  </div>
  <div class="page">
    <p>Active Learning using Particles</p>
    <p>Each incoming unlabeled point has a score:</p>
    <p>Query for label if score(x) &lt; s0</p>
  </div>
  <div class="page">
    <p>The Complete Algorithm</p>
    <p>If unlabeled and score(x)&lt;s0, query its label</p>
    <p>The null category likelihood for gap assumption</p>
    <p>Approximate MetropolisHastings with a small buffer</p>
  </div>
  <div class="page">
    <p>Experiments: List of Algorithms</p>
    <p>OSIS=Online Semi-Supervised Learning</p>
    <p>OS = Online Supervised learning</p>
    <p>AROW = Adaptive Regularization of Weight Vectors</p>
    <p>[Crammer et al. NIPS 09]</p>
  </div>
  <div class="page">
    <p>Experiments: Procedure</p>
    <p>20 trials of T iterations</p>
    <p>Start with 2 labeled points</p>
    <p>To control the total number of labels:</p>
    <p>First run OASIS, record the number of queries a</p>
    <p>Run other algorithms with 2+a labeled points</p>
    <p>Same exact x1  xT sequence across algorithms</p>
  </div>
  <div class="page">
    <p>Results on Letter</p>
  </div>
  <div class="page">
    <p>Results on Pendigits</p>
  </div>
  <div class="page">
    <p>Results on MNIST</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Online + active + semi-supervised learning</p>
    <p>Full Bayesian on gap assumption</p>
    <p>Particle filtering</p>
    <p>Future work:</p>
    <p>Theory</p>
    <p>Adversarial setting</p>
    <p>Acknowledgments: NSF IIS-0916038, AFOSR FA9550-09-1-0313, and NSF IIS-0953219. We thank Rob Nowak for helpful discussions.</p>
  </div>
</Presentation>
