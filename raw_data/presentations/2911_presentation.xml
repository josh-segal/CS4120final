<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FAST10, Feb 25, 2010</p>
    <p>Efficient Object Storage Journaling in a Distributed Parallel File System</p>
    <p>Sarp Oral, Feiyi Wang, David Dillow, Galen Shipman, Ross Miller, and Oleg Drokin</p>
    <p>Presented by Sarp Oral</p>
  </div>
  <div class="page">
    <p>A Demanding Computational Environment</p>
    <p>Jaguar XT5 18,688 Nodes</p>
    <p>Jaguar XT4 7,832 Nodes</p>
    <p>Frost (SGI Ice) 128 Node institutional cluster</p>
    <p>Smoky 80 Node software development cluster</p>
    <p>Lens 30 Node visualization and analysis cluster</p>
  </div>
  <div class="page">
    <p>Spider</p>
    <p>Demonstrated bandwidth of 240 GB/s on the center wide file system</p>
    <p>Fastest Lustre file system in the world</p>
    <p>Demonstrated stability and concurrent mounts on major OLCF systems  Jaguar XT5  Jaguar XT4  Opteron Dev Cluster (Smoky)  Visualization Cluster (Lens)</p>
    <p>Over 26,000 clients mounting the file system and performing I/O General availability on Jaguar XT5, Lens, Smoky, and GridFTP servers</p>
    <p>Largest scale Lustre file system in the world</p>
    <p>Cutting edge resiliency at scale</p>
    <p>Demonstrated resiliency features on Jaguar XT5  DM Multipath  Lustre Router failover</p>
  </div>
  <div class="page">
    <p>Designed to Support Peak Performance</p>
    <p>B an</p>
    <p>d w id th G B /s</p>
    <p>Timeline (January 2010)</p>
    <p>ReadBW GB/s WriteBW GB/s</p>
    <p>Max data rates (hourly) on  of available storage controllers</p>
  </div>
  <div class="page">
    <p>Motivations for a Center Wide File System</p>
    <p>Building dedicated file systems for platforms does not scale  Storage often 10% or more of new system cost  Storage often not poised to grow independently of attached machine  Different curves for storage and compute technology  Data needs to be moved between different compute islands</p>
    <p>Simulation platform to visualization platform  Dedicated storage is only accessible</p>
    <p>when its machine is available  Managing multiple file systems</p>
    <p>requires more manpower</p>
    <p>data sharing path</p>
    <p>Jaguar XT5</p>
    <p>Ewok</p>
    <p>Lens</p>
    <p>Smoky</p>
    <p>Jaguar XT4</p>
    <p>SION Network &amp; Spider System</p>
    <p>Jaguar XT4</p>
    <p>Jaguar XT5 Ewok</p>
    <p>LensSmoky</p>
  </div>
  <div class="page">
    <p>Spider: A System At Scale</p>
    <p>Over 10.7 PB of RAID 6 formatted capacity  13,440 1 TB drives  192 Lustre I/O servers  Over 3 TB of memory (on Lustre I/O servers)  Available to many compute systems through high-speed SION network</p>
    <p>Over 3,000 IB ports  Over 3 miles (5 kilometers) cables</p>
    <p>Over 26,000 client mounts for I/O  Peak I/O performance is 240 GB/s  Current Status</p>
    <p>in production use on all major OLCF computing platforms</p>
  </div>
  <div class="page">
    <p>Lustre File System</p>
    <p>Developed and maintained by CFS, then Sun, now Oracle  POSIX compliant, open source parallel file system, driven by DOE Labs  Metadata server (MDS) manages</p>
    <p>namespace</p>
    <p>Object storage server (OSS) manages Object storage targets (OST)</p>
    <p>OST manages block devices  ldiskfs on OSTs</p>
    <p>V. 1.6  superset of ext3  V. 1.8 +  superset of ext3 or ext4</p>
    <p>High-performance  Parallelism by object striping</p>
    <p>Highly scalable  Tuned for parallel block I/O</p>
    <p>Metadata Server</p>
    <p>(MDS)</p>
    <p>Metadata Target</p>
    <p>(MDT)</p>
    <p>Object Storage Servers</p>
    <p>(OSS)</p>
    <p>Object Storage Targets</p>
    <p>(OST)</p>
    <p>Lustre Clients</p>
    <p>High</p>
    <p>performance</p>
    <p>interconnect</p>
  </div>
  <div class="page">
    <p>Spider - Overview</p>
    <p>SION IB Network</p>
    <p>level 6 arrays (tiers)</p>
    <p>Controllers (Singlets)</p>
    <p>S A</p>
    <p>S c</p>
    <p>o n n e c ti o n s 1</p>
    <p>x D</p>
    <p>D R</p>
    <p>IB</p>
    <p>Jaguar XT5 segment</p>
    <p>Jaguar XT4 segment</p>
    <p>VIB VIB</p>
    <p>VIB</p>
    <p>Smoky</p>
    <p>VIB64 DDR 64 DDR</p>
    <p>VIB</p>
    <p>Spider</p>
    <p>Core 2Core 1</p>
    <p>Aggregation 1</p>
    <p>Aggregation 2</p>
    <p>Lens/Everest 60 DDR 5 DDR</p>
    <p>Currently providing high-performance scratch space to all major OLCF platforms</p>
  </div>
  <div class="page">
    <p>Spider - Speeds and Feeds</p>
    <p>Enterprise Storage controllers and large</p>
    <p>racks of disks are connected via InfiniBand.</p>
    <p>connections per pair</p>
    <p>Storage Nodes run parallel file system software and manage incoming FS traffic.</p>
    <p>SION Network provides connectivity</p>
    <p>between OLCF resources and</p>
    <p>primarily carries storage traffic.</p>
    <p>complex</p>
    <p>Lustre Router Nodes run parallel file system</p>
    <p>client software and forward I/O operations</p>
    <p>from HPC clients.</p>
    <p>Opteron nodes with 8 GB of RAM each</p>
    <p>Jaguar XT5</p>
    <p>Jaguar XT4</p>
    <p>XT5 SeaStar2+ 3D Torus</p>
    <p>InfiniBand 16 Gbit/sec</p>
    <p>Serial ATA 3 Gbit/sec</p>
    <p>Other Systems</p>
    <p>(Viz, Clusters)</p>
  </div>
  <div class="page">
    <p>Spider - Couplet and Scalable Cluster</p>
    <p>Disks 280 in 5 trays</p>
    <p>DDN Couplet (2 controllers)</p>
    <p>OSS (4 Dell nodes) 24 IB ports</p>
    <p>Flextronics Switch</p>
    <p>IB Ports</p>
    <p>Uplink to Cisco Core Switch</p>
    <p>Disks 280 in 5 trays</p>
    <p>DDN Couplet (2 controllers)</p>
    <p>OSS (4 Dell nodes) 24 IB ports</p>
    <p>Flextronics Switch</p>
    <p>IB Ports</p>
    <p>Uplink to Cisco Core Switch</p>
    <p>DDN S2A 9900 Couplet</p>
    <p>(2 controllers)</p>
    <p>Lustre I/O Servers (4 Dell nodes)</p>
    <p>IB Ports</p>
    <p>Uplink to Cisco Core Switch</p>
    <p>A Spider Scalable Cluster (SC)</p>
    <p>SC SC SC SC</p>
    <p>SC SC SC SC</p>
    <p>SC SC SC SC</p>
    <p>SC SC SC SC</p>
    <p>Unit 1 Unit 2</p>
    <p>Unit 3</p>
  </div>
  <div class="page">
    <p>Spider - DDN S2A9900 Couplet</p>
    <p>D1 D14</p>
    <p>Disk Enclosure 1</p>
    <p>DEM</p>
    <p>D15 D28... DEM</p>
    <p>D29 D42... DEM</p>
    <p>D56... DEM</p>
    <p>DEM</p>
    <p>DEM</p>
    <p>DEM</p>
    <p>DEM</p>
    <p>A1</p>
    <p>B1</p>
    <p>C1</p>
    <p>D1</p>
    <p>E1</p>
    <p>F1</p>
    <p>G1</p>
    <p>H1</p>
    <p>P1</p>
    <p>S1</p>
    <p>A1</p>
    <p>B1</p>
    <p>A2</p>
    <p>B2</p>
    <p>C2</p>
    <p>D2</p>
    <p>E2</p>
    <p>F2</p>
    <p>G2</p>
    <p>H2</p>
    <p>P2</p>
    <p>S2</p>
    <p>A2</p>
    <p>B2</p>
    <p>Controller2</p>
    <p>...</p>
    <p>Disk Enclosure 2</p>
    <p>Disk Enclosure 5</p>
    <p>Controller1</p>
    <p>...</p>
    <p>D43</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>IO</p>
    <p>Module</p>
    <p>Power Supply (House)</p>
    <p>Power Supply (UPS)</p>
    <p>Power Supply (House)</p>
    <p>Power Supply (UPS)</p>
    <p>Power Supply (House)</p>
    <p>Power Supply (UPS)</p>
  </div>
  <div class="page">
    <p>Spider - DDN S2A9900 (contd)</p>
    <p>D 14A</p>
    <p>D 1A</p>
    <p>D 2A</p>
    <p>...</p>
    <p>D 14B</p>
    <p>D 1B</p>
    <p>D 2B</p>
    <p>...</p>
    <p>D 14P</p>
    <p>D 1P</p>
    <p>D 2P</p>
    <p>...</p>
    <p>D14SD1S D2S ...</p>
    <p>D 28A</p>
    <p>D 15A</p>
    <p>D 16A</p>
    <p>...</p>
    <p>D 28B</p>
    <p>D 15B</p>
    <p>D 16B</p>
    <p>...</p>
    <p>D 28P</p>
    <p>D 15P</p>
    <p>D 16P</p>
    <p>D 28S</p>
    <p>D 15S</p>
    <p>D 16S</p>
    <p>...</p>
    <p>...</p>
    <p>Channel A</p>
    <p>Channel B</p>
    <p>Channel P</p>
    <p>Channel S</p>
    <p>Channel A</p>
    <p>Channel B</p>
    <p>Channel P</p>
    <p>Channel S</p>
    <p>Tier1 Tier 2 Tier 14</p>
    <p>...</p>
    <p>Tier 15 Tier 16 Tier 28</p>
    <p>Disk Controller 1 Disk Controller 2</p>
    <p>...</p>
    <p>RAID 6 (8+2)</p>
  </div>
  <div class="page">
    <p>Spider - How Did We Get Here?</p>
    <p>4 years project  We didnt just pick up phone and order</p>
    <p>a center-wide file system  No single vendor could deliver this system  Trail blazing was required</p>
    <p>Collaborative effort was key to success  ORNL  Cray  DDN  Cisco  CFS, SUN, and now Oracle</p>
  </div>
  <div class="page">
    <p>Spider  Solved Technical Challenges</p>
    <p>Fault tolerance design  Network  I/O servers  Storage arrays</p>
    <p>Infiniband support on XT SIO 0</p>
    <p>P e rc</p>
    <p>e n t o f o b s e rv</p>
    <p>e d p</p>
    <p>e a k {</p>
    <p>M B</p>
    <p>/s ,I O</p>
    <p>P S</p>
    <p>}</p>
    <p>Elapsed time (seconds)</p>
    <p>Hard bounce of 7844 nodes via 48 routers</p>
    <p>Bounce XT4 @ 206s</p>
    <p>I/O returns @ 435s</p>
    <p>Full I/O @ 524s</p>
    <p>RDMA Timeouts</p>
    <p>Bulk Timeouts</p>
    <p>OST Evicitions</p>
    <p>Combined R/W MB/s Combined R/W IOPS</p>
    <p>SeaStar Torus Congestion</p>
    <p>Performance  Asynchronous journaling  Network congestion avoidance</p>
    <p>Scalability  26,000 file system clients</p>
  </div>
  <div class="page">
    <p>ldiskfs Journaling Overhead</p>
    <p>Even sequential writes exhibit random I/O behavior due to journaling  Observed 4-8 KB writes along with 1 MB sequential writes on DDNs  DDN S2A9900s are not well tuned for small I/O access  For enhanced reliability write-back cache on DDNs are turned off</p>
    <p>Special file (contiguous block space) reserved for journaling on ldiskfs  Labeled as journal device  Beginning on physical disk layout</p>
    <p>Ordered mode  After file data portion committed on disk  journal meta data portion needs to</p>
    <p>be committed</p>
    <p>Extra head seek needed for every journal transaction commit!</p>
  </div>
  <div class="page">
    <p>ldiskfs Journaling Overhead (Contd)</p>
    <p>between two events. The write-back mode thus provides meta data consistency but does not provide any file data consistency. The ordered mode is the default journaling mode in Linux Ext3 file system. In this mode, though only updated meta data blocks are journaled the file data is guaranteed to be written to their fixed locations on disk before committing the meta data updates in the journal, thus providing an order between the meta data and file data commits on disk. The data journaling mode journals both the meta data and the file data.</p>
    <p>The ldiskfs file system by default performs journaling in ordered mode by first writing the data blocks to disk followed by meta data blocks to the journal. The journal is then written to disk and marked as committed. In the worst case, such as appending to a file, this can result in one 16 KB write (on average  for bitmap, inode block map, inode, and super block) and another 4 KB write for the journal commit record for every 1 MB write. These extra small writes cause at least two extra disk head seeks. Due to the poor IOP performance of SATA disks, these additional head seeks and small writes can substantially degrade the aggregate block I/O performance.</p>
    <p>A possible optimization (and perhaps the most obvious one) that would improve the journaling efficiency is to minimize the extra disk head seeks. This can be achieved by either a software or hardware optimization (or both).</p>
    <p>In order to obtain this baseline on the DDN S2A9900, the XDD benchmark utility was used. XDD allows multiple clients to exercise a parallel write or read operation synchronously. XDD can be run in sequential or random read or write mode. Our baseline tests focused on aggregate performance for sequential read or write workloads. Performance results using XDD from 4 hosts connected to the DDN via DDR IB are summarized in Fig. 1. The results presented are a summary of our testing and show performance of sequential read, sequential write, random read, and random write using 1MB transfers. These tests were run using a single host for the single LUN tests, and 4 hosts each with 7 LUNs for the 28 LUN test. Performance results presented are the best of 5 runs in each configuration.</p>
    <p>Table 1: XDD baseline performance !&quot;#$ %&amp;'(&quot;</p>
    <p>!&quot;#$&quot;%&amp;'() *+,-*. ./,-0,</p>
    <p>!&quot;#$&quot;%&amp;'() ,,76-5, ,*6+-5,</p>
    <p>)'*+,&quot;-./0</p>
    <p>After establishing a baseline of performance using XDD, we examined Lustre level performance using the IOR benchmark. Testing was conducted using 4 OSSes each with 7 OSTs on the DDN S2A9900. Our initial results showed very poor write performance of only 1398.99MB/sec using 28 clients where each client was writing to different OST. Lustre level write performance was a mere 24.9% of our baseline performance</p>
    <p>Block level benchmarking (writes) for 28 tiers  5608.15 MB/s (baseline)  File system level benchmark (obdfilter) gives 1398.99 MB/s</p>
    <p>24.9% of baseline bandwidth  One couplet, 4 OSS each with 7 OSTs  28 clients, one-to-one mapping with OSTs</p>
    <p>Analysis  Large number of 4KB writes in addition to 1MB writes  Traced back to ldiskfs journal updates</p>
  </div>
  <div class="page">
    <p>Minimizing extra disk head seeks</p>
    <p>Hardware solutions  External journal on an internal SAS tier  External journal on a network attached solid state device</p>
    <p>Software solution  Asynchronous journal commits</p>
    <p>Configura&gt;on Bandwidth MB/s (single couplet)</p>
    <p>Delta % from baseline</p>
    <p>Block level (28 @ers) 5608.15 0%</p>
    <p>Internal journals, SATA 1398.99 24.9%</p>
    <p>External, internal SAS @er 1978.82 35.2%</p>
    <p>External, sync to RAMSAN, solid state 3292.60 58.7%</p>
    <p>Internal, async journals, SATA 5222.95 93.1%</p>
  </div>
  <div class="page">
    <p>External journals on a solid state device</p>
    <p>Texas Memory Systems RamSan-400  Loaned by Vion Corp.  Non-volatile SSD  3 GB/s block I/O  400,000 IOPS  4 IB DDR ports w/ SRP</p>
    <p>28 LUNs  One-to-one mapping with</p>
    <p>DDN LUNs  Obtained 58.7% of baseline</p>
    <p>performance  Network round-trip latency or</p>
    <p>inefficiency on external journal code path might culprit</p>
    <p>Jaguar XT5 segment</p>
    <p>Jaguar XT4 segment</p>
    <p>VIB VIB</p>
    <p>VIB</p>
    <p>Smoky</p>
    <p>VIB64 DDR 64 DDR</p>
    <p>VIB</p>
    <p>Spider</p>
    <p>Core 2Core 1</p>
    <p>Aggregation 1</p>
    <p>Aggregation 2</p>
    <p>Lens/Everest 60 DDR 5 DDR</p>
    <p>TMS RamSan-400</p>
  </div>
  <div class="page">
    <p>Synchronous Journal Commits</p>
    <p>Running and closed transactions  Running transaction accepts new threads to join in and has all its data in memory  Closed transaction starts flushing updated metadata to journaling device. After flush is</p>
    <p>complete, the transaction state is marked as committed  Current running transaction cant be</p>
    <p>closed and committed until closed transaction fully commits to journaling device</p>
    <p>Congestion points  Slow disk  Journal size (1/4 of journal device)  Extra disk head seek for journal</p>
    <p>transaction commit  Write I/O operation for new threads is</p>
    <p>blocked on currently closed transaction that is committing</p>
    <p>RUNNING</p>
    <p>CLOSED COMMITTED</p>
    <p>The running transaction is marked as</p>
    <p>CLOSED in memory by Journaling</p>
    <p>Block Device (JBD) Layer</p>
    <p>File data is flushed</p>
    <p>from memory to</p>
    <p>disk</p>
    <p>The file data must be</p>
    <p>flushed to disk prior</p>
    <p>to committing the</p>
    <p>transaction</p>
    <p>Updated metadata</p>
    <p>blocks flushed to</p>
    <p>disk Updated metadata</p>
    <p>blocks are written from</p>
    <p>memory to journaling</p>
    <p>device</p>
  </div>
  <div class="page">
    <p>Asynchronous Journal Commits</p>
    <p>Change how Lustre uses the journal, not the operation of journal  Every server RPC reply has a special field (default, sync)</p>
    <p>id of the last transaction on stable storage  Client uses this to keep a list of completed, but not committed operations  In case of a server crash these could be resent (replayed) to the server</p>
    <p>Clients pin dirty and flushed pages to memory (default, sync)  Released only when server acks these are committed to stable storage</p>
    <p>Relax the commit sequence (async)  Add async flag to the RPC  Reply clients immediately after file data portion of RPC is committed to disk</p>
  </div>
  <div class="page">
    <p>Asynchronous Journal Commits (contd)</p>
    <p>Server sends a reply to client  JBD flushes updated metadata blocks to journaling device, writes commit record</p>
  </div>
  <div class="page">
    <p>Asynchronous Journal Commits (contd)</p>
    <p>A g g re</p>
    <p>g a te</p>
    <p>B lo</p>
    <p>c k I /O</p>
    <p>( M</p>
    <p>B /s</p>
    <p>)</p>
    <p>Number of Threads</p>
    <p>Hardware- and Software-based Journaling Solutions</p>
    <p>external journals on a tier of SAS disks external journals on RamSan-400 device</p>
    <p>internal journals on SATA disks async internal journals on SATA disks</p>
    <p>Async journaling achieves 5,223 MB/sec (at file system level) or 93% of baseline</p>
    <p>Cost effective  Requires only Lustre code change  Easy to implement and maintain</p>
    <p>Temporarily increases client memory consumption</p>
    <p>Clients have to keep more data in memory until the server acks the commit</p>
    <p>Does not change the failure semantics or reliability characteristics</p>
    <p>The guarantees about file system consistency at the local OST remain unchanged</p>
  </div>
  <div class="page">
    <p>Asynchronous Journal Commits Application Performance</p>
    <p>Up to 50% reduction in runtime  Might not be typical  Depends on the application</p>
    <p>Reduced number of small I/O requests  64% to 26.5%</p>
    <p>!&quot;!!#</p>
    <p>$!&quot;!!#</p>
    <p>%!&quot;!!#</p>
    <p>&amp;!&quot;!!#</p>
    <p>'!&quot;!!#</p>
    <p>(!&quot;!!#</p>
    <p>)!&quot;!!#</p>
    <p>*!&quot;!!#</p>
    <p>+!&quot;!!#</p>
    <p>,!&quot;!!#</p>
    <p>$!!&quot;!!#</p>
    <p>!#-#$%*# ($%#-#(%'# ,$%#-,%'# $!!+-$!&amp;)# $(&amp;)#-#$('+# %!'+#-#%!)!#</p>
    <p>. / 0 1 2 3# 4 5# 6 37 82 #9 :;</p>
    <p>#3 2 &lt; / 2 =8 =# &gt;?</p>
    <p>@#</p>
    <p>LMBN#O4/3BPQ=# R=MBN#O4/3BPQ=#</p>
    <p>!&quot;!#&quot;!$% !&quot;!&amp;&quot;'&amp;%</p>
    <p>!&quot;!'&quot;()%</p>
    <p>!&quot;!(&quot;)(%</p>
    <p>!&quot;!!&quot;!!%</p>
    <p>!&quot;!)&quot;($%</p>
    <p>!&quot;!(&quot;*+%</p>
    <p>!&quot;!'&quot;)#%</p>
    <p>!&quot;!*&quot;'$%</p>
    <p>!&quot;!,&quot;)(%</p>
    <p>!&quot;!&amp;&quot;+&amp;%</p>
    <p>!&quot;)!&quot;!*%</p>
    <p>$'%-./01%23%</p>
    <p>$'%-./01%23%</p>
    <p>)+''%-./01%23%</p>
    <p>)+''%-./01%23%</p>
    <p>@:A%896%:;&lt;01%</p>
    <p>Gyrokinetic Toroidal Code (GTC)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>A system at this scale, we cant just pick up the phone and order one  No problem is small when you scale it up  At Lustre file system level we obtained 24.9% of our baseline block level performance</p>
    <p>Tracked to ldiskfs journal updates</p>
    <p>Solutions  External journals on an internal SAS tier; achieved 35.2%  External journals on network attached SSD; achieved 58.7%  Asynchronous journal commits; achieved 93.1%</p>
    <p>Removed a bottleneck from critical write path  Decreased 4 KB I/O DDNs observed by 37.5%  Cost-effective, easy to deploy and maintain  Temporarily increases client memory consumption  Doesnt change failure characteristics or semantics</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Contact info</p>
    <p>Sarp Oral oralhs at ornl dot gov Technology Integration Group Oak Ridge Leadership Computing Facility Oak Ridge National Laboratory</p>
  </div>
</Presentation>
