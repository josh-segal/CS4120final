<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>CMPE 521 Database Systems</p>
    <p>QUERY SENSITIVE EMBEDDINGS Vassilis Athitsos, Marios Hadjieleftheriou,</p>
    <p>George Kollios and Stan Sclaroff</p>
    <p>DRS YILDIZ</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>General Information  Related Work  Required Background  Building Query Sensitive Embeddings  Evaluation of Method  Conclusion</p>
  </div>
  <div class="page">
    <p>General Information</p>
    <p>Motivation of such a work  Retrieve the most similar matches to a query object  Nearest neighbor methods are used currently  Existing solutions are too slow  Especially in domains where objects are compared using</p>
    <p>computationally expensive similarity (or distance) measures  Method offered by the paper:</p>
    <p>A novel method for approximating nearest neighbor retrieval for object spaces with expensive distance measures</p>
    <p>Embedding based : Maps objects to real vector space while preserving the proximity structure of original space</p>
    <p>Query sensitive (distance measure change according to query) measures while finding distances</p>
  </div>
  <div class="page">
    <p>General Information (cont.)</p>
    <p>Usage:  Estimating the properties of a biological sequence (like a protein,</p>
    <p>or DNA sequence)  Nearest neighbor classification used by pattern recognition</p>
    <p>techniques  For both cases a proximity measure is needed and this</p>
    <p>measure should be calculated easily.  Numerous methods have been proposed for speeding up</p>
    <p>nearest-neighbor retrieval but all are for metric spaces (coordinate space).</p>
    <p>Metric spaces have fixed dimensions. In case of variable dimensions existing methods do not help</p>
    <p>Query sensitive embeddings methods provide a way of nearest neighbor calculation for non-Euclidean (also non metric) spaces</p>
  </div>
  <div class="page">
    <p>General Information (cont.)</p>
    <p>Method Summary  Construct a function that maps objects into a real vector</p>
    <p>space  Find query sensitive coefficients  Measure the distance between objects by using the vector</p>
    <p>space calculated with L1 and L2 distances  L1 or L2 over vectors should be more efficient than finding</p>
    <p>the original distance (usually holds)  The results obtained (details later)</p>
    <p>The proposed method gives better results according to the two well-known existing embedding methods: BoostMap and FastMap</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Hashing and tree structures  Similarity indexing in multi-dimensional datasets  The performance degrades in high dimensions  Rely on Euclidean or metric properties, problem in non-metric</p>
    <p>spaces  Embedding based</p>
    <p>Lipschitz embeddings, FastMap, MetricMap, SparseMap, and BoostMap</p>
    <p>Speed up nearest neighbor retrieval  Construct an embedding, which maps objects into another space</p>
    <p>with a more efficient distance measure  BoostMap</p>
    <p>Preserves the original space proximity with original space (after the embedding)</p>
    <p>The proposed method is an extension to BoostMap</p>
  </div>
  <div class="page">
    <p>Required Background</p>
    <p>Embeddings  Classifiers</p>
  </div>
  <div class="page">
    <p>Embeddings</p>
    <p>1D Embeddings</p>
    <p>r is called the reference object  Dx is the (non-metric) distance between x and r</p>
    <p>Project x into the line x1x2 (Building block for FastMap)  1D Embeddings are considered weak classifiers (high error rate)  1D embeddings can be used as building blocks for constructing</p>
    <p>higher-dimensional embeddings  Embedding Types:</p>
    <p>Fx1,x2 is used to construct FastMap  Frs can be combined to form Lipschitz embeddings  AdaBoost combines many weak classifiers into a strong classifier</p>
  </div>
  <div class="page">
    <p>Embeddings (Cont.)</p>
  </div>
  <div class="page">
    <p>Embeddings (Cont.)</p>
  </div>
  <div class="page">
    <p>Classifiers</p>
    <p>Weak Classifiers  Given three objects q, a, b  X, is q closer to a or to b?  Find exact distances Dx(q,a) and Dx(q,b) !  Find checking if F(q) is closer to F(a) or to F(b) where F is a</p>
    <p>d dimensional embedding F:XRd  If the two methods stated above give the same result then F</p>
    <p>succeeds over (q,a,b) otherwise F fails over (q,a,b)  1D Embeddings act as weak classifiers because</p>
    <p>they have a high error rate  High dimensional Strong Classifiers can be built by</p>
    <p>using 1D Embeddings (Method of AdaBoost)  Purpose of weak classifiers : Reducing problem to</p>
    <p>find a strong classifier by using weak classifiers</p>
  </div>
  <div class="page">
    <p>Building Query Sensitive Embeddings</p>
    <p>Motivation for constructing Query Sensitive Embeddings</p>
    <p>Construction Method  Training Algorithm  Training Data</p>
  </div>
  <div class="page">
    <p>Motivation for constructing Query Sensitive Embeddings</p>
    <p>Lack of contrasting: Two high-dimensional objects are unlikely to be very similar in all the dimensions.</p>
    <p>Statistical sensitivity: The data is rarely distributed, and for a pair of objects there only relatively few coordinates that are significant for comparing those objects.</p>
    <p>Query sensitive embeddings are required:  Use weigths while finding distances  Those weights will be query specific values</p>
  </div>
  <div class="page">
    <p>Query Sensitive Embedding Example</p>
  </div>
  <div class="page">
    <p>Query Sensitive Embedding (2)</p>
    <p>F(x) = (Fr1(x), Fr2(x), Fr3(x))  L1 distance is used to compare the embeddings of</p>
    <p>two objects  20 database objects (3 reference points), 10 query</p>
    <p>objects  F fails on 23.5% of the 3800 triples (q, a, b) where q</p>
    <p>from query objects and a,b from database objects  1D embeddings Fr1, Fr2, Fr3 fail respectively on</p>
  </div>
  <div class="page">
    <p>Query Sensitive Embedding (3)</p>
    <p>If q = q1 then for triples(q1, a, b)  Fr1 does better than F  Fr1 fails on 5.8% of those triples  F fails on 11.6% of those triples</p>
    <p>Similarly if q = q2 and q = q3 respectively, Fr2 and Fr3 are more accurate than F.</p>
    <p>For query objects q1, q2, q3 it would be beneficial to use a query-sensitive weighted L1 measure, that would respectively use only the first, second, and third coordinate of F</p>
  </div>
  <div class="page">
    <p>Construction Method</p>
    <p>Specify a large family of 1D embeddings  Use 1D embeddings to define binary classifiers for estimating</p>
    <p>for object triples (q, a, b) if q is closer to a or to b.  These classifiers are expected to be better than a random</p>
    <p>classifier  Run AdaBoost to combine many classifiers into a single</p>
    <p>classifier (H)  H is significantly more accurate than the simple classifiers</p>
    <p>associated with 1D embeddings  Use H to define a d-dimensional embedding Fout, and a query</p>
    <p>sensitive weighted L1 distance measure Dout  H is equivalent to the combination of Fout and Dout  For three objects q, a, b  X, H predicts that q is closer to a than it</p>
    <p>is to b, then Fout(q) is closer to Fout(a) than it is to Fout(b)</p>
  </div>
  <div class="page">
    <p>Construction Method (cont.)</p>
    <p>The classifier:  1  if q is closer to a  -1  if q is closer to b  -1  if q is in equal distance (D type) to a,b  Classifier may fail if the reference is outside a region of the</p>
    <p>objects use Splitters</p>
    <p>The splitter (0,1):  V is a specified region or a threhold</p>
    <p>The Query Sensitive Classifer is combination of classifer and the splitter:</p>
  </div>
  <div class="page">
    <p>Training Algorithm</p>
    <p>Usage: Constructing an embedding and a query sensitive distance measure</p>
    <p>AdaBoost algortihm is used as the training algortihm  AdaBoost assumes that we have a weak learner module, which</p>
    <p>we can call at each round to obtain a new weak classifier.  The goal is to construct a strong classifier that achieves much</p>
    <p>higher accuracy than the individual weak classifiers.  At each step algorithm simply determines the appropriate weight</p>
    <p>for each weak classifier, and then adjusts the training weights  At each round mistakes from previous rounds are corected by</p>
    <p>some amount  Some changes and additions are done on the to adapt</p>
    <p>AdaBoost to this case</p>
  </div>
  <div class="page">
    <p>Training Data</p>
    <p>Xtr is a subset of X (object space)</p>
    <p>We need to compute Dx for all pairs of points in Xtr</p>
  </div>
  <div class="page">
    <p>Evaluation of Method</p>
    <p>Complexity Analysis  Embedding application: filter and refine</p>
    <p>retrieval  Experiments</p>
  </div>
  <div class="page">
    <p>Complexity Analysis</p>
    <p>At each training round measure m classifiers with t training triples in order to choose the best weak classifier</p>
    <p>The computational time per training round is O(mt)  Before we even start the training algorithm, we need to compute</p>
    <p>distances Dx from:  Every object in C to every object in C  Every object in C to every object in Xtr  C  the set of objects that we use to form 1D embeddings  Xtr  the set of objects from which we form training triples</p>
    <p>We also need all distances between pairs of objects in Xtr  Computing all those distances can sometimes be the most</p>
    <p>computationally expensive part of the algorithm, depending on the complexity of computing Dx</p>
    <p>Fortunately all those costs are just onetime preprocessing costs</p>
  </div>
  <div class="page">
    <p>Complexity Analysis (cont.)</p>
    <p>Computing the d-dimensional embedding of a query object takes O(d) time</p>
    <p>Also it requires O(d) evaluations of Dx  Result  Comparing the embedding of the query to the</p>
    <p>embeddings of n database objects takes time O(dn)  For a fixed d, these costs are similar to FastMap, SparseMap,</p>
    <p>and MetricMap  Incase of dynamic data change no performance penalty will be</p>
    <p>required until the underlying database structure changes (after too many inserts and deletions)</p>
  </div>
  <div class="page">
    <p>Embedding application: filter and refine retrieval</p>
    <p>In applications where we are interested in retrieving the k nearest neighbors for a query object q, a d-dimensional embedding F can be used in a filter-and-refine framework</p>
    <p>Compute and store vector F(x) for every database object x (offline step)</p>
    <p>For any new query q perform following:  Embedding step: compute F(q), by measuring the distances</p>
    <p>between q and the reference objects and/or pivot objects used to define F</p>
    <p>Filter step: Find the database objects whose associated vectors are the p most similar vectors to F(q)</p>
    <p>Refine step: sort those p candidates by evaluating the exact distance DX between q and each candidate</p>
    <p>The filter step discards most database objects by measuring distances between vectors</p>
    <p>The refine step applies Dx only to the top p candidates</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Comparison of the proposed method to the original BoostMap method and FastMap will be done</p>
    <p>We used two different datasets: the MNIST and a time series dataset</p>
    <p>MNIST:  Dataset of handwritten digits (from 0 to 9)  Shape Context Distance as the exact distance measure  A training set of 60,000 image</p>
    <p>Time Series:  Dynamic Time Warping as the exact distance measure  32,768 various sequences are obtained</p>
    <p>Both Shape Context Distance and Dynamic Time Warping are very hard to compute</p>
  </div>
  <div class="page">
    <p>Experiments (Cont)</p>
    <p>Parameter k will be used in experiments as the number of k-neighbor.</p>
    <p>Algorithms:  FastMap  Ra-QI  Original BoostMap  Se-QS  Proposed Method  Se-QI  Intermediate method (which incorporates our</p>
    <p>method of choosing training triples, but still constructs a query-insensitive embedding)</p>
    <p>We will try to achieve all k-neighbours with minimum number of comparisons</p>
  </div>
  <div class="page">
    <p>Experiments (MNIST- %90 accuracy)</p>
  </div>
  <div class="page">
    <p>Experiments (MNIST- %95 accuracy)</p>
  </div>
  <div class="page">
    <p>Experiments (Time Serials- %90 accuracy)</p>
  </div>
  <div class="page">
    <p>Experiments (Time Serials- %95 accuracy)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>The main difference of the proposed method with respect to existing embedding methods is that it constructs a query sensitive distance measure</p>
    <p>Such a distance measure captures the fact that different embedding coordinates are important for different queries, and thus leads to improved retrieval accuracy and speed by using smaller number of comparisions</p>
    <p>Embeddings is a family of methods that is not domain specific and can be applied for efficient retrieval in non-metric spaces</p>
  </div>
  <div class="page">
    <p>Thanks...</p>
  </div>
</Presentation>
