<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Transparent System Call Based Performance Debugging for Cloud Computing</p>
    <p>Nikhil Khadke, Michael P. Kasick, Soila P. Kavulya, Jiaqi Tan, and Priya Narasimhan,</p>
    <p>PARALLEL DATA LABORATORY Carnegie Mellon University</p>
  </div>
  <div class="page">
    <p>Introduction  Automated problem diagnosis is challenging</p>
    <p>Complex system interactions: Concurrency and distributed interactions can obscure root-cause</p>
    <p>Scale: Large volume of monitoring data  Production systems: Lack luxury to modify</p>
    <p>instrumentation in legacy systems  Focus of talk</p>
    <p>Explore feasibility of system calls as transparent instrumentation source for debugging of performance problems</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 2</p>
  </div>
  <div class="page">
    <p>Related Work: Instrumentation  End-to-end tracing of request flows</p>
    <p>Instrumentation frameworks [Sambasivan11, Sigelman10]  Black-box approach [Aguilera03]  Analysis of production logs [Kavulya12, Tan09]</p>
    <p>Performance and event log analysis  OS-level logs [Kasick10]  Generation of problem signatures [Bodik10,Cohen05]  Alarm correlation [Oliner10, Kandula09]</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 3</p>
  </div>
  <div class="page">
    <p>Why System Calls?  Captures interaction between application &amp; OS  Rich source of statistical and semantic data</p>
    <p>Statistical: Disk/network transfer times  Semantic: Programs/files accessed, hangs/failures</p>
    <p>Higher reliability than application logs  Quality of logs dependent on application developer  Logs can become obsolete as system evolves</p>
    <p>Transparency  No need to modify to application source code  No dependence on hardware architecture</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 4</p>
  </div>
  <div class="page">
    <p>Goals and Assumptions  Goals</p>
    <p>Learn profiles of normal behavior using system calls  Localize problematic node and type of problem</p>
    <p>Assumptions  Majority of nodes exhibit fault-free behavior  Workload evenly balanced across nodes  Hardware configurations similar across nodes  Clocks on nodes are synchronized</p>
    <p>Initial exploration  Diagnose resource contention in MapReduce systems</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 5</p>
  </div>
  <div class="page">
    <p>Target System: Hadoop  Hadoop: Open-source implementation of</p>
    <p>MapReduce  Long running jobs (&gt; 100s): Hard to label failures  Large, distributed: Hard to isolate failures  Leased cloud infrastructure: Slowdowns are costly</p>
    <p>MapReduce job consists of two main abstrac6ons  Maps: Process smaller par66on of large job in parallel  Reduces: Fetch, merge, and sort output of map tasks</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 6</p>
  </div>
  <div class="page">
    <p>Overview of Approach</p>
    <p>Instrument System</p>
    <p>Collect per-node system call traces</p>
    <p>Localize Problem</p>
    <p>Identify nodes whose system call profiles differ significantly from peers</p>
    <p>Studied two peer-comparison approaches</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 7</p>
  </div>
  <div class="page">
    <p>System Call Instrumentation  strace(), a UNIX utility captures system calls</p>
    <p>invoked by program  We record:</p>
    <p>Average and total time spent in each system call  Number of total and failed system call invocations  Percentage of time of spent in the system call  Verbose trace of system calls</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 8</p>
  </div>
  <div class="page">
    <p>System Call Scope and Choice  Network related:</p>
    <p>accept()  connect()  bind()  socket()</p>
    <p>File System related:  access()  stat()</p>
    <p>Process related:  execve()</p>
    <p>Omitted send/receive calls due to large variance in data processed</p>
    <p>Overhead of nave implementation: 1.2x</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 9</p>
  </div>
  <div class="page">
    <p>Diagnostic Approach</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 10</p>
    <p>Learn per-node profiles of normal behavior</p>
    <p>Two-step threshold for generating alarms  Local alarms: Accounts for normal variance between</p>
    <p>nodes (min. and max. bin counts for thresholds)  Global alarms: Pairwise histogram-comparison flags</p>
    <p>nodes which differ significantly from peers  Use heuristics to identify type of problem</p>
    <p>execve() stat() access() socket() S ys</p>
    <p>te m</p>
    <p>C al</p>
    <p>l C</p>
    <p>ou nt</p>
    <p>s</p>
    <p>System Calls</p>
  </div>
  <div class="page">
    <p>Diagnostic Approaches</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/&quot; 11</p>
    <p>Semantic diagnostic approach  Uses verbose system traces  Identify unique system call invocations</p>
    <p>Unique call  system calls, arguments, return code  E.g., stat(fileX) = -1 ENOENT (No such file or dir)</p>
    <p>Statistical diagnostic approach  Histogram of invocations of system call per time window</p>
  </div>
  <div class="page">
    <p>Experimental Setup  Experimental Setup</p>
    <p>5 identical machines: Single master and 4 slaves  Hadoop workloads</p>
    <p>wc  count the frequency of every word in a text corpus of 100,000 words</p>
    <p>sort  sorts 100,000 randomly generated numbers  Performance problem injection</p>
    <p>Disk-hog  write 2GB chunks continually to disk  Packet-loss  drop 5%, 20% and 50% of the</p>
    <p>network packets on a node</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 12</p>
  </div>
  <div class="page">
    <p>Results: Terminology  True positive</p>
    <p>Faulty node and root-cause correctly identified  False positive</p>
    <p>Incorrectly identify a node or provide a wrong rootcause</p>
    <p>Output node set  Set of all the nodes that are outputted by our</p>
    <p>algorithms (and believed to be faulty).</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 13</p>
  </div>
  <div class="page">
    <p>Results: Disk hog</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 14</p>
    <p>Statistical Semantic</p>
    <p>True positive rate (Non-speculative) True positive rate (Speculative) False positive rate (Non-speculative) False positive rate (Speculative)</p>
    <p>Disk hog signature: More time in stat() + less time in execve()</p>
  </div>
  <div class="page">
    <p>Results: Packet Loss</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 15</p>
    <p>Statistical Semantic</p>
    <p>True positive rate (Non-speculative) True positive rate (Speculative) False positive rate (Non-speculative) False positive rate (Speculative)</p>
    <p>Packet loss signature: More time in connect() + less time in accept()</p>
    <p>Results for 50% Packet Loss</p>
  </div>
  <div class="page">
    <p>Lessons Learned  Diagnostic approaches</p>
    <p>Statistical approach more effective semantic approach  Diagnostic accuracy independent of Hadoop workload  Speculative execution introduces variance that reduces</p>
    <p>diagnostic accuracy</p>
    <p>Performance Problems  Disk-related issues were easier to detect  Lower accuracy for network-related problems probably</p>
    <p>due to correlated problem manifestation across nodes</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 16</p>
  </div>
  <div class="page">
    <p>Future Work  How do cope with heterogeneous systems?</p>
    <p>Normalize behavior across heterogeneous nodes?  Limit peer comparison to nearest neighbors?</p>
    <p>How do we reduce instrumentation overhead?  Can we distinguish between application-level</p>
    <p>problems and infrastructural problems?</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 17</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 18</p>
  </div>
  <div class="page">
    <p>Related Work (1)  [Aguilera03] Performance debugging for distributed system of black</p>
    <p>boxes. M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthitacharoen. SOSP 2003</p>
    <p>[Barham04] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using Magpie for request extraction and workload modelling. SOSP 2004.</p>
    <p>[Bodik10]: Fingerprinting the datacenter: automated classification of performance crises. P. Bodk, M. Goldszmidt, A. Fox, D. B. Woodard, H. Andersen. EuroSys 2010.</p>
    <p>[Cohen05]: Capturing, indexing, clustering and retrieving system history. Ira Cohen, Steve Zhang, Moises Goldszmidt, Julie Symons, Terence Kelly, Armando Fox. SOSP, 2005.</p>
    <p>[Kandula09]: Detailed diagnosis in enterprise networks. Srikanth Kandula, Ratul Mahajan, Patrick Verkaik, Sharad Agarwal, Jitendra Padhye, Paramvir Bahl. SIGCOMM 2009.</p>
    <p>[Kasick10]: Black-Box Problem Diagnosis in Parallel File Systems. Michael P. Kasick, Jiaqi Tan, Rajeev Gandhi, Priya Narasimhan. FAST 2010.</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 19</p>
  </div>
  <div class="page">
    <p>Related Work (2)  [Kavulya12]: Draco: Statistical Diagnosis of Chronic Problems in Large</p>
    <p>Distributed Systems. Soila Kavulya, Scott Daniels, Kaustubh Joshi, Matti Hiltunen, Rajeev Gandhi, Priya Narasimhan. DSN 2012</p>
    <p>[Oliner2010] Using correlated surprise to infer shared influence. A. J. Oliner, A. V. Kulkarni, and A. Aiken. DSN 2010.</p>
    <p>[Sambasivan11]: Diagnosing Performance Changes by Comparing Request Flows. Raja R. Sambasivan, Alice X. Zheng, Michael De Rosa, Elie Krevat, Spencer Whitman, Michael Stroucken, William Wang, Lianghong Xu, and Gregory R. Ganger. NSDI 2011.</p>
    <p>[Sigelman10] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal, D. Beaver, S. Jaspan, and C. Shanbhagy. Dapper, a large-scale distributed systems tracing infrastructure. Google Technical Report, April 2010.</p>
    <p>[Tan09]: Mochi: Visual Log-Analysis Based Tools for Debugging Hadoop. J. Tan, X. Pan, S. P. Kavulya, R. Gandhi, P. Narasimhan. HotCloud09.</p>
    <p>MAD  Oct 12&quot;http://www.pdl.cmu.edu/ 20</p>
  </div>
</Presentation>
