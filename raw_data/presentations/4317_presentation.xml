<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Adversarial Contrastive Estimation ACL 2018 *AVISHEK (JOEY) BOSE, *HUAN LING, *YANSHUAI CAO</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 1 / 29</p>
  </div>
  <div class="page">
    <p>Contrastive Estimation</p>
    <p>Many Machine Learning models learn by trying to separate positive examples from negative examples.</p>
    <p>Positive Examples are taken from observed real data distribution (training set)</p>
    <p>Negative Examples are any other configurations that are not observed</p>
    <p>Data is in the form of tuples or triplets (x+,y+) and (x+,y) are positive and negative data points respectively.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 2 / 29</p>
  </div>
  <div class="page">
    <p>Easy Negative Examples with NCE</p>
    <p>Noise Constrastive Estimation samples negatives by taking p(y|x+) to be some unconditional pnce (y). Whats wrong with this?</p>
    <p>Negative y in (x,y) is not tailored toward x</p>
    <p>Difficult to choose hard negatives as training progresses</p>
    <p>Model doesnt learn discriminating features between positive and hard negative examples</p>
    <p>NCE negatives are easy !!!</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 3 / 29</p>
  </div>
  <div class="page">
    <p>Hard Negative Examples</p>
    <p>Informal Definition: Hard negative examples are data points that are extremely difficult for the training model to distinguish from positive examples.</p>
    <p>Hard Negatives result to higher losses and thus more more informative gradients</p>
    <p>Not necessarily closest to a positive datapoint in embedding space</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 4 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions</p>
    <p>Adversarial Contrastive Estimation: A general technique for hard negative mining using a Conditional GAN like setup.</p>
    <p>A novel entropy regularizer that prevents generator mode collapse and has good empirical benefits</p>
    <p>A strategy for handling false negative examples that allows training to progress</p>
    <p>Empirical validation across 3 different embedding tasks with state of the art results on some metrics</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 5 / 29</p>
  </div>
  <div class="page">
    <p>Problem: We want to generate negatives that ... fool a discriminative model into misclassifying.</p>
    <p>Solution: Use a Conditional GAN to sample hard negatives given x+. We can augment NCE with an adversarial sampler, pnce (y) + (1 )g(y|x).</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 6 / 29</p>
  </div>
  <div class="page">
    <p>Conditional GAN</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 7 / 29</p>
  </div>
  <div class="page">
    <p>, Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 8 / 29</p>
  </div>
  <div class="page">
    <p>The ACE Generator</p>
    <p>The ACE generator defines a categorical distribution over all possible y values</p>
    <p>Picking a negative example is a discrete choice and not differentiable</p>
    <p>Simplest way to train via Policy Gradients is the REINFORCE gradient estimator</p>
    <p>Learning is done via a GAN style min-max game</p>
    <p>min</p>
    <p>max</p>
    <p>V (,) = min</p>
    <p>max</p>
    <p>Ep+(x) L(,; x) (1)</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 9 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: GAN training can suffer from mode collapse? What happens if the generator collapses on its favorite few negative examples?</p>
    <p>Solution: Add a entropy regularizer term to the generators loss:</p>
    <p>Rent (x) = max(0,c H(g(y|x))) (2)</p>
    <p>H(g(y|x)) is the entropy of the categorical distribution c = log(k) is the entropy of a uniform distribution over k choices</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 10 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: The Generator can sample false negatives  gradient cancellation</p>
    <p>Solution: Apply an additional two-step technique, whenever computationally feasible.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 11 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: The Generator can sample false negatives  gradient cancellation</p>
    <p>Solution: Apply an additional two-step technique, whenever computationally feasible.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 12 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: The Generator can sample false negatives  gradient cancellation</p>
    <p>Solution: Apply an additional two-step technique, whenever computationally feasible.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 13 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: REINFORCE is known to have extremely high variance.</p>
    <p>Solution: Reduce Variance using the self-critical baseline. Other baselines and gradient estimators are also good options.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 14 / 29</p>
  </div>
  <div class="page">
    <p>Technical Contributions for effective training</p>
    <p>Problem: The generator is not learning from the NCE samples.</p>
    <p>Solution: Use Importance Sampling. Generator can leverage NCE samples for exploration in an off-policy scheme. The modified reward now looks like</p>
    <p>g(y |x)/pnce (y)</p>
    <p>.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 15 / 29</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 16 / 29</p>
  </div>
  <div class="page">
    <p>Contemporary Work</p>
    <p>GANs for NLP that are close to our work</p>
    <p>MaskGAN Fedus et. al 2018</p>
    <p>Incorporating GAN for Negative Sampling in Knowledge Representation Learning Wang et. al 2018</p>
    <p>KBGAN Cai and Wang 2017</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 17 / 29</p>
  </div>
  <div class="page">
    <p>Example: Knowledge Graph Embeddings</p>
    <p>Data in the form of triplets (head entity, relation, tail entity). For example {United states of America, partially contained by ocean, Pacific}</p>
    <p>Basic Idea: The embeddings for h,r,t should roughly satisfy h + r  t</p>
    <p>Link Prediction: Goal is to learn from observed positive entity relations and predict missing links.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 18 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Knowledge Graph Embeddings</p>
    <p>Positive Triplet: + = (h+,r+,t+)</p>
    <p>Negative Triplet: Either negative head or tail is sampled i.e.  = (h,r+,t+) or  = (h+,r+,t)</p>
    <p>Loss Function: L = max(0, + s(</p>
    <p>+)  s()) (3)</p>
    <p>ACE Generator: g(t |r+,h+) or g(h|r+,t+) parametrized by a feed</p>
    <p>forward neural net.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 19 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Knowledge Graph Embeddings</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 20 / 29</p>
  </div>
  <div class="page">
    <p>Experimental Result: Ablation Study</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 21 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Order Embeddings</p>
    <p>Hypernym Prediction: A hypernym pair is a pair of concepts where the first concept is a specialization or an instance of the second.</p>
    <p>Learning embeddings that are hierarchy preserving. The Root Node is at the origin and all other embeddings lie on the positive semi-space</p>
    <p>Constraint enforces the magnitude of the parents embedding to be smaller than childs in every dimension</p>
    <p>Sibling nodes are not subjected to this constraint.</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 22 / 29</p>
  </div>
  <div class="page">
    <p>Order Embeddings (Vendrov et. al 2016)</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 23 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Order Embeddings</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 24 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Word Embeddings: WordSim353</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 25 / 29</p>
  </div>
  <div class="page">
    <p>ACE for Word Embeddings: Stanford Rare Word</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 26 / 29</p>
  </div>
  <div class="page">
    <p>Discriminator Loss on NCE vs. Adversarial Examples</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 27 / 29</p>
  </div>
  <div class="page">
    <p>Nearest Neighbors for NCE vs. ACE</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 28 / 29</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>BlogPost: http://borealisai.com/2018/07/13/</p>
    <p>adversarial-contrastive-estimation-harder-better-faster-stronger/</p>
    <p>Avishek (Joey) Bose*, Huan Ling*, Yanshuai Cao* | Borealis AI, University of Toronto | August 2, 2018 29 / 29</p>
  </div>
</Presentation>
