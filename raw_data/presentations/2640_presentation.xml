<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Yiming Zhang, Chuanxiong Guo, Dongsheng Li,</p>
    <p>Rui Chu, Haitao Wu, Yongqiang Xiong</p>
    <p>NSDI 2015</p>
    <p>CubicRing: Enabling One-Hop Failure Detection/Recovery</p>
    <p>in Distributed In-Memory Storage Systems</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Traditional disk-based storage systems  Use RAM as a cache</p>
    <p>App servers + storage servers + cache servers</p>
    <p>Facebook keeps more than 75% of its online data in its memcached servers (2011)</p>
    <p>Disk-based storage issues  I/O latency and bandwidth</p>
    <p>Cache consistency</p>
  </div>
  <div class="page">
    <p>In-Memory Storage Systems</p>
    <p>Use RAM as persistent storage</p>
    <p>Data is kept entirely in the RAM</p>
    <p>Redis</p>
    <p>An in-memory key-value store with rich data</p>
    <p>model</p>
    <p>CMEM</p>
    <p>Tencents pubic in-memory key-value store service</p>
    <p>Stores several tens of TB of data of online games</p>
    <p>RAMCloud</p>
    <p>Uses InfiniBand to achieve 10-us level I/O latency</p>
    <p>Boosts the performance of online data-intensive app</p>
  </div>
  <div class="page">
    <p>Network Related Challenges for</p>
    <p>In-Memory Storage</p>
    <p>False failure detection</p>
    <p>Transient network problems vs. real server failures</p>
    <p>Recovery traffic congestion</p>
    <p>Thousands of recovery flows bring network congestion</p>
    <p>which results in long recovery time</p>
    <p>ToR switch failures</p>
    <p>When a ToR switch fails, all its servers are considered</p>
    <p>dead and several TB of data may need to be recovered</p>
  </div>
  <div class="page">
    <p>Solution: CubicRing</p>
    <p>One-hop failure detection</p>
    <p>Shorten the paths that heartbeats have to traverse</p>
    <p>Avoid traffic congestion</p>
    <p>Restrict the recovery traffic within the smallest</p>
    <p>possible range</p>
    <p>Avoid single failure for ToR switch</p>
    <p>Build the in-memory storage system on a multi</p>
    <p>homed cubic topology</p>
  </div>
  <div class="page">
    <p>Structure</p>
    <p>Failure Recovery</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Primary-Recovery-Backup</p>
    <p>Primary-recovery-backup</p>
    <p>Only one primary copy is stored in the RAM</p>
    <p>Redundant backup copies are stored on disks</p>
    <p>Fast recovery requires 10+ GB aggregate recovery</p>
    <p>throughput</p>
    <p>Also adopted by RAMCloud</p>
    <p>But cannot handle the network-related challenges</p>
    <p>Data center network</p>
    <p>Data center network</p>
    <p>write</p>
    <p>backup</p>
    <p>ack</p>
    <p>ack</p>
  </div>
  <div class="page">
    <p>Primary-Recovery-Backup</p>
    <p>Primary-recovery-backup</p>
    <p>Only one primary copy is stored in the RAM</p>
    <p>Redundant backup copies are stored on disks</p>
    <p>Fast recovery requires 10+ GB aggregate recovery</p>
    <p>throughput</p>
    <p>Also adopted by RAMCloud</p>
    <p>But cannot handle the network-related challenges</p>
    <p>Data center network</p>
    <p>read</p>
    <p>result</p>
  </div>
  <div class="page">
    <p>Primary-Recovery-Backup</p>
    <p>Primary-recovery-backup</p>
    <p>Only one primary copy is stored in the RAM</p>
    <p>Redundant backup copies are stored on disks</p>
    <p>Fast recovery requires 10+ GB aggregate recovery</p>
    <p>throughput</p>
    <p>Also adopted by RAMCloud</p>
    <p>But cannot handle the network-related challenges</p>
    <p>Data center network</p>
    <p>X recover</p>
  </div>
  <div class="page">
    <p>Directly-Connected-Tree</p>
  </div>
  <div class="page">
    <p>From Tree to Cube</p>
  </div>
  <div class="page">
    <p>CubicRing on BCube</p>
    <p>CubicRing  Three layer of rings: primary ring,</p>
    <p>recovery ring, backup ring</p>
    <p>Primary ring  All servers in BCube are primary</p>
    <p>servers.</p>
    <p>The whole key space is mapped into the primary ring</p>
  </div>
  <div class="page">
    <p>CubicRing on BCube(cont.)</p>
    <p>Recovery ring  Recovery servers of P are 1-hop to P</p>
    <p>Backup ring  Backup servers 1-hop to R and</p>
  </div>
  <div class="page">
    <p>CubicRing Property</p>
    <p>CubicRing for BCube(n, k)</p>
    <p># of primary servers</p>
    <p>P = nk+1</p>
    <p># of recovery servers for each primary server</p>
    <p>R = (n-1)(k+1)</p>
    <p># of backup servers for each primary server</p>
    <p>B = (n-1)2k(k+1)/2</p>
    <p>CubicRing has plenty of primary, recovery</p>
    <p>and backup servers</p>
    <p>BCube(16,2),</p>
    <p>P = 4096, R = 45, B = 675</p>
  </div>
  <div class="page">
    <p>Structure</p>
    <p>Failure Recovery</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Failure Detection</p>
    <p>Heartbeats</p>
    <p>Primary servers periodically send heartbeats to their recovery servers</p>
    <p>Confirmation of server failure</p>
    <p>If a recovery server does not receive heartbeats, it will report this server failure to a coordinator</p>
    <p>The coordinator verifies the server failure</p>
  </div>
  <div class="page">
    <p>Failure Recovery  Primary server</p>
    <p>failure</p>
  </div>
  <div class="page">
    <p>Failure Recovery  Primary server</p>
    <p>failure</p>
  </div>
  <div class="page">
    <p>Failure Recovery  Recovery server</p>
    <p>failure</p>
  </div>
  <div class="page">
    <p>Failure Recovery  Backup server</p>
    <p>failure</p>
  </div>
  <div class="page">
    <p>Single Server Failure Recovery</p>
    <p>Summarization</p>
    <p>Most of recoveries are 1-hop</p>
    <p>Concurrent recoveries have little contention</p>
  </div>
  <div class="page">
    <p>Structure</p>
    <p>Failure Recovery</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>MemCube  Memcached-1.4.15</p>
    <p>Linux (CentOS 6.4)</p>
    <p>MemCube components  Connection manager: Maintains the status of neighbors and</p>
    <p>interacts with other servers</p>
    <p>Storage manager: Handles RAM I/O requests and asynchronously writes backup data to disks</p>
    <p>Recovery manager: Reconstructs primary/backup data on the new primary/backup servers</p>
  </div>
  <div class="page">
    <p>Testbed</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>Server Failure Recovery</p>
    <p>Recovers 48 GB of data in 3.1 seconds</p>
    <p>Aggregate recovery throughput: 123.9 Gb/sec</p>
    <p>88.5% of the ideal aggregate bandwidth</p>
  </div>
  <div class="page">
    <p>Server Failure Recovery</p>
    <p>Simulations</p>
  </div>
  <div class="page">
    <p>Different # Recovery Servers</p>
    <p>Recovery bandwidth vs. fragmentation</p>
    <p>More recovery servers result in higher throughput</p>
    <p>And higher fragmentation</p>
  </div>
  <div class="page">
    <p>Different # Backup Servers</p>
    <p>Recovery bandwidth</p>
    <p>When # backup servers is small, their aggregate bandwidth may become the bottleneck</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>In-memory storage</p>
    <p>Redis</p>
    <p>CMEM</p>
    <p>RAMCloud</p>
    <p>Failure detection and recovery</p>
    <p>Phi-accrual detector</p>
    <p>Falcon and Pigeon</p>
    <p>Host failure recovery (e.g., microreboot)</p>
    <p>Flat Datacenter Storage  Locality-oblivious parallel recovery</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>CubicRing</p>
    <p>Exploits network proximity to restrict failure detection and recovery within 1-hop</p>
    <p>MemCube: in-memory key-value store</p>
    <p>Leverages the CubicRing structure for fast failure detection and recovery</p>
    <p>Maintains the CubicRing structure against failures</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
  </div>
</Presentation>
