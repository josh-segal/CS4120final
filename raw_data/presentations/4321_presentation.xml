<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation</p>
    <p>Tiancheng Zhao, Kyusong Lee and Maxine Eskenazi</p>
    <p>Language Technologies Institute, Carnegie Mellon University</p>
    <p>Code &amp; Data: github.com/snakeztc/NeuralDialog-LAED</p>
  </div>
  <div class="page">
    <p>Sentence Representation in Conversations</p>
    <p>Traditional System: hand-crafted semantic frame</p>
    <p>[Inform location=Pittsburgh, time=now]</p>
    <p>Not scalable to complex domains</p>
    <p>Neural dialog models: continuous hidden vectors</p>
    <p>Directly output system responses in words</p>
    <p>Hard to interpret &amp; control [Ritter et al 2011, Vinyals et al</p>
    <p>et al 2016, Zhao et al 2017]</p>
  </div>
  <div class="page">
    <p>Why discrete sentence representation?</p>
  </div>
  <div class="page">
    <p>Why discrete sentence representation?</p>
    <p>Our goal:</p>
    <p>X = What time do you want to</p>
    <p>travel?</p>
    <p>Recognition Model</p>
    <p>Z1Z2Z3</p>
    <p>Latent Actions</p>
    <p>Encoder Decoder Dialog System</p>
    <p>Scalability &amp; Interpretability</p>
  </div>
  <div class="page">
    <p>Baseline: Discrete Variational Autoencoder (VAE)</p>
    <p>M discrete K-way latent variables z with RNN recognition &amp; generation network.</p>
    <p>Reparametrization using Gumbel-Softmax [Jang et al., 2016; Maddison et al., 2016]</p>
    <p>p(z) e.g. uniform</p>
    <p>KL[ q(z|x) || p(z) ]</p>
  </div>
  <div class="page">
    <p>Baseline: Discrete Variational Autoencoder (VAE)</p>
    <p>M discrete K-way latent variables z with GRU encoder &amp; decoder.</p>
    <p>Reparametrization using Gumbel-Softmax [Jang et al., 2016; Maddison et al., 2016]</p>
    <p>FAIL to learn meaningful z because of posterior collapse (z is constant regardless of x)</p>
    <p>MANY prior solution on continuous VAE, e.g. (not exhaustive), yet still open-ended question</p>
    <p>KL-annealing, decoder word dropout [Bowman et a2015] Bag-of-word loss [Zhao et al 2017] Dilated CNN decoder</p>
    <p>[Yang, et al 2017] Wake-sleep [Shen et al 2017] 6</p>
  </div>
  <div class="page">
    <p>Anti-Info Nature in Evidence Lower Bound (ELBO)</p>
    <p>Write ELBO as an expectation over the whole dataset</p>
  </div>
  <div class="page">
    <p>Anti-Info Nature in Evidence Lower Bound (ELBO)</p>
    <p>Write ELBO as an expectation over the whole dataset</p>
    <p>Expand the KL term, and plug back in:</p>
    <p>Maximize ELBO  Minimize I(Z, X) to 0  Posterior collapse with powerful decoder.</p>
  </div>
  <div class="page">
    <p>Discrete Information VAE (DI-VAE)</p>
    <p>A natural solution is to maximize both data log likelihood &amp; mutual information.</p>
    <p>Match prior result for continuous VAE. [Mazhazni et al 2015, Kim et al 2017]</p>
  </div>
  <div class="page">
    <p>Discrete Information VAE (DI-VAE)</p>
    <p>A natural solution is to maximize both data log likelihood &amp; mutual information.</p>
    <p>Match prior result for continuous VAE. [Mazhazni et al 2015, Kim et al 2017]  Propose Batch Prior Regularization (BPR) to minimize KL [q(z)||p(z)] for discrete latent</p>
    <p>variables:</p>
    <p>N: mini-batch size. Fundamentally different from KL-annealing, since BPR is non-linear.</p>
  </div>
  <div class="page">
    <p>Learning from Context Predicting (DI-VST)</p>
    <p>Skip-Thought (ST) is well-known distributional sentence representation [Hill et al 2016]</p>
    <p>The meaning of sentences in dialogs is highly contextual, e.g. dialog acts.</p>
    <p>We extend DI-VAE to Discrete Information Variational Skip Thought (DI-VST).</p>
  </div>
  <div class="page">
    <p>Integration with Encoder-Decoders</p>
    <p>Encoder Decoder</p>
    <p>Recognition Network</p>
    <p>Dialog Context c</p>
    <p>Response x</p>
    <p>Response P(x|c, z)</p>
    <p>Training z</p>
    <p>z</p>
    <p>P(z|c)</p>
    <p>Optional: penalize decoder if generated x not exhibiting z [Hu et al 2017]</p>
    <p>Policy Network</p>
    <p>Generator</p>
  </div>
  <div class="page">
    <p>Integration with Encoder-Decoders</p>
    <p>Encoder Decoder</p>
    <p>Dialog Context c</p>
    <p>Response P(x|c, z)</p>
    <p>Testing</p>
    <p>z P(z|c)Policy Network</p>
  </div>
  <div class="page">
    <p>Evaluation Datasets</p>
    <p>a. Past evaluation dataset for text VAE [Bowman et al 2015]</p>
    <p>a. 3,031 Human-Woz dialog dataset from 3 domains: weather, navigation &amp; scheduling.</p>
    <p>a. 2,400 human-human telephone non-task-oriented dialogues about a given topic.</p>
    <p>a. 13,188 human-human non-task-oriented dialogs from chat room.</p>
  </div>
  <div class="page">
    <p>The Effectiveness of Batch Prior Regularization (BPR)</p>
    <p>For auto-encoding</p>
    <p>DAE: Autoencoder + Gumbel Softmax</p>
    <p>DVAE: Discrete VAE with ELBO loss</p>
    <p>DI-VAE: Discrete VAE + BPR</p>
    <p>For context-predicting</p>
    <p>DST: Skip thought + Gumbel Softmax</p>
    <p>DVST: Variational Skip Thought</p>
    <p>DI-VST: Variational Skip Thought + BPR</p>
    <p>Table 1: Results for various discrete sentence representations.</p>
  </div>
  <div class="page">
    <p>The Effectiveness of Batch Prior Regularization (BPR)</p>
    <p>For auto-encoding</p>
    <p>DAE: Autoencoder + Gumbel Softmax</p>
    <p>DVAE: Discrete VAE with ELBO loss</p>
    <p>DI-VAE: Discrete VAE + BPR</p>
    <p>For context-predicting</p>
    <p>DST: Skip thought + Gumbel Softmax</p>
    <p>DVST: Variational Skip Thought</p>
    <p>DI-VST: Variational Skip Thought + BPR</p>
    <p>Table 1: Results for various discrete sentence representations.</p>
  </div>
  <div class="page">
    <p>The Effectiveness of Batch Prior Regularization (BPR)</p>
    <p>For auto-encoding</p>
    <p>DAE: Autoencoder + Gumbel Softmax</p>
    <p>DVAE: Discrete VAE with ELBO loss</p>
    <p>DI-VAE: Discrete VAE + BPR</p>
    <p>For context-predicting</p>
    <p>DST: Skip thought + Gumbel Softmax</p>
    <p>DVST: Variational Skip Thought</p>
    <p>DI-VST: Variational Skip Thought + BPR</p>
    <p>Table 1: Results for various discrete sentence representations.</p>
  </div>
  <div class="page">
    <p>How large should the batch size be?</p>
    <p>&gt; When batch size N = 0</p>
    <p>= normal ELBO</p>
    <p>&gt; A large batch size leads to</p>
    <p>more meaningful latent action z</p>
    <p>Slowly increasing KL</p>
    <p>Improve PPL</p>
    <p>I(x,z) is not the final goal</p>
  </div>
  <div class="page">
    <p>Intropolation in the Latent Space</p>
  </div>
  <div class="page">
    <p>Differences between DI-VAE &amp; DI-VST</p>
    <p>DI-VAE cluster utterances based on the</p>
    <p>words:</p>
    <p>More fine-grained actions</p>
    <p>More error-prone since harder to predict</p>
    <p>DI-VST cluster utterances based on the</p>
    <p>context:</p>
    <p>Utterance used in the similar context</p>
    <p>Easier to get agreement.</p>
  </div>
  <div class="page">
    <p>Interpreting Latent Actions</p>
    <p>M=3, K=5. The trained R will map any utterance into a 1</p>
    <p>-a 2</p>
    <p>-a 3</p>
    <p>. E.g. How are you?  1-4-2</p>
    <p>Automatic Evaluation on SW &amp; DD</p>
    <p>Compare latent actions with</p>
    <p>human-annotations.</p>
    <p>Homogeneity [Rosenberg and</p>
    <p>Hirschberg, 2007].</p>
    <p>The higher the more correlated</p>
  </div>
  <div class="page">
    <p>Interpreting Latent Actions</p>
    <p>M=3, K=5. The trained R will map any utterance into a 1</p>
    <p>-a 2</p>
    <p>-a 3</p>
    <p>. E.g. How are you?  1-4-2</p>
    <p>Human Evaluation on SMD</p>
    <p>Expert look at 5 examples and give a</p>
    <p>name to the latent actions</p>
    <p>5 workers look at the expert name and</p>
    <p>another 5 examples.</p>
    <p>Select the ones that match the expert</p>
    <p>name.</p>
  </div>
  <div class="page">
    <p>Predict Latent Action by the Policy Network</p>
    <p>Provide useful measure about the</p>
    <p>complexity of the domain.</p>
    <p>Usr &gt; Sys &amp; Chat &gt; Task</p>
    <p>Predict latent actions from DI-VAE is harder</p>
    <p>than the ones from DI-VST</p>
    <p>Two types of latent actions has their own</p>
    <p>pros &amp; cons. Which one is better is</p>
    <p>application dependent.</p>
  </div>
  <div class="page">
    <p>Interpretable Response Generation</p>
    <p>Examples of interpretable dialog</p>
    <p>generation on SMD</p>
    <p>First time, a neural dialog system</p>
    <p>outputs both:</p>
    <p>target response</p>
    <p>high-level actions with</p>
    <p>interpretable meaning</p>
  </div>
  <div class="page">
    <p>Conclusions &amp; Future Work</p>
    <p>An analysis of ELBO that explains the posterior collapse issue for sentence VAE.</p>
    <p>DI-VAE and DI-VST for learning rich sentence latent representation and integration</p>
    <p>with encoder-decoders.</p>
    <p>Learn better context-based latent actions</p>
    <p>Encode human knowledge into the learning process.</p>
    <p>Learn structured latent action space for complex domains.</p>
    <p>Evaluate dialog generation performance in human-study.</p>
  </div>
  <div class="page">
    <p>Thank you! Code &amp; Data: github.com/snakeztc/NeuralDialog-LAED</p>
  </div>
  <div class="page">
    <p>Semantic Consistency of the Generation</p>
    <p>Use the recognition network as a classifier to predict the latent action z based on the generated response x.</p>
    <p>Report accuracy by comparing z and z.</p>
    <p>What we learned?</p>
    <p>DI-VAE has higher consistency than DI-VST</p>
    <p>L attr</p>
    <p>helps more in complex domain</p>
    <p>L attr</p>
    <p>helps DI-VST more than DI-VAE</p>
    <p>DI-VST is not directly helping generating x</p>
    <p>ST-ED doesnt work well on SW due to complex</p>
    <p>context pattern</p>
    <p>Spoken language and turn taking 27</p>
  </div>
  <div class="page">
    <p>What defines Interpretable Latent Actions</p>
    <p>Definition: Latent action is a set of discrete variable that define the high-level attributes of</p>
    <p>an utterance (sentence) X. Latent action is denoted as Z.</p>
    <p>Two key properties:</p>
    <p>Z should capture salient sentence-level features about the response X.</p>
    <p>The meaning of latent symbols Z should be independent of the context C.</p>
    <p>Why context-independent?</p>
    <p>If meaning of Z depends on C, then often impossible to interpret Z</p>
    <p>Since the possible space of C is huge!</p>
    <p>Conclusion: context-independent semantic ensures each assignment of z has the same</p>
    <p>meaning in all context.</p>
  </div>
</Presentation>
