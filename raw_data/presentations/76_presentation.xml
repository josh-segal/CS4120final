<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Eugene Agichtein and Silviu Cucerzan</p>
    <p>Microsoft Research</p>
    <p>Predicting Accuracy of Extracting Information from Unstructured Text Collections</p>
  </div>
  <div class="page">
    <p>Extracting and Managing Information in TextExtracting and Managing Information in Text</p>
    <p>Text Document Collections</p>
    <p>Web Documents</p>
    <p>Blogs News Alerts</p>
    <p>Information Extraction System</p>
    <p>Events</p>
    <p>--------------------</p>
    <p>Entities</p>
    <p>------------------------------------------------------------------------</p>
    <p>E1</p>
    <p>Relations</p>
    <p>E 2</p>
    <p>E3 E 4</p>
    <p>E4 E 1</p>
    <p>Varying properties Different Languages Varying consistency Noise/errors .</p>
    <p>Complex problem Usually many parameters Often tuning required</p>
    <p>Success ~ Accuracy</p>
  </div>
  <div class="page">
    <p>The Goal: Predict Extraction AccuracyThe Goal: Predict Extraction Accuracy</p>
    <p>Estimate the expected success of an IE system that relies on contextual patterns before</p>
    <p>running expensive experiments</p>
    <p>tuning parameters</p>
    <p>training the system</p>
    <p>Useful when adapting an IE system to  a new task</p>
    <p>a new document collection</p>
    <p>a new language</p>
  </div>
  <div class="page">
    <p>Specific Extraction TasksSpecific Extraction Tasks</p>
    <p>Named Entity Recognition (NER)</p>
    <p>Relation Extraction (RE)</p>
    <p>European champions Liverpool paved the way to the group stages of the Champions League taking a 3-1 lead over CSKA Sofia on Wednesday [...] Gerard Houllier's men started the match in Sofia on fire with Steven Gerrard scoring [...]</p>
    <p>Organization</p>
    <p>PersonLocation</p>
    <p>Misc</p>
    <p>Abraham Lincoln was born on Feb. 12, 1809, in a log cabin in Hardin (now Larue) County, Ky</p>
    <p>BORN Who When Where</p>
    <p>Abraham Lincoln Feb. 12, 1809 Hardin County, KY</p>
  </div>
  <div class="page">
    <p>Contextual CluesContextual Clues</p>
    <p>Left context Right context</p>
    <p>Left context Right contextMiddle context</p>
    <p>engineers Orville and Wilbur Wright built the first working airplane in 1903 .</p>
    <p>yesterday, Mrs Clinton told reporters the move to the East Room</p>
  </div>
  <div class="page">
    <p>Approach: Language ModellingApproach: Language Modelling</p>
    <p>Presence of contextual clues for a task appears related to extraction difficulty</p>
    <p>The more obvious the clues, the easier the task</p>
    <p>Can be modelled as unexpectedness of a word</p>
    <p>Use Language Modelling (LM) techniques to quantify intuition</p>
  </div>
  <div class="page">
    <p>Language Models (LM)Language Models (LM)</p>
    <p>An LM is summary of word distribution in text</p>
    <p>Can define unigram, bigram, trigram, n-gram models</p>
    <p>More complex models exist</p>
    <p>Distance, syntax, word classes</p>
    <p>But: not robust for web, other languages,</p>
    <p>LMs used in IR, ASR, Text Classification, Clustering:  Query Clarity: Predicting query performance</p>
    <p>[Cronen-Townsend et al, SIGIR 2002]</p>
    <p>Context Modelling for NER [Cucerzan et al., EMNLP 1999], [Klein et al. CoNLL 2003]</p>
  </div>
  <div class="page">
    <p>Document Language ModelsDocument Language Models</p>
    <p>A basic LM is a normalized word histogram for the document collection</p>
    <p>Unigram (word) models commonly used</p>
    <p>Higher-order n-grams (bigrams, trigrams) can be used</p>
    <p>word Freq</p>
    <p>the 0.0584</p>
    <p>to 0.0269</p>
    <p>and 0.0199</p>
    <p>said 0.0147</p>
    <p>. . . . . .</p>
    <p>'s 0.0018</p>
    <p>company 0.0014</p>
    <p>mrs 0.0003</p>
    <p>won 0.0003</p>
    <p>president 0.0003</p>
  </div>
  <div class="page">
    <p>Context Language ModelsContext Language Models</p>
    <p>Senator Christopher Dodd, D-Conn., named general chairman of the Democratic National Committee last week by President Bill Clinton , said it was premature to talk about lifting the U.S. embargo against Cuba</p>
    <p>Although the Clinton s health plan failed to make it through Congress this year , Mrs Clinton vowed continued support for the proposal.</p>
    <p>A senior White House official, who accompanied Clinton , told reporters</p>
    <p>By the fall of 1905, the Wright brothers  experimental period ended. With their third powered airplane , they now routinely made flights of several</p>
    <p>Against this backdrop, we see the Wright brothers efforts to develop an airplane</p>
  </div>
  <div class="page">
    <p>Key ObservationKey Observation</p>
    <p>If normally rare words consistently appear in contexts around entities, extraction task tends to be easier.</p>
    <p>Contexts for a task are an intrinsic property of collection and extraction task, and not restricted to a specific information extraction system.</p>
  </div>
  <div class="page">
    <p>Divergence MeasuresDivergence Measures</p>
    <p>Cosine Divergence:</p>
    <p>Relative entropy: KL Divergence</p>
    <p>CBG</p>
    <p>BGC BGC</p>
    <p>LMLM</p>
    <p>LMLM LMLM</p>
    <p>Vw BG</p>
    <p>C iCBGC</p>
    <p>wLM</p>
    <p>wLM wLMLMLM</p>
    <p>)(</p>
    <p>)( log)()||(KL</p>
  </div>
  <div class="page">
    <p>Interpreting Divergence: Reference LMInterpreting Divergence: Reference LM</p>
    <p>Need to calibrate the observed divergence  Compute Reference Model LMR :</p>
    <p>Pick K random non-stopwords R and compute the context language model around Ri.</p>
    <p>the five-star Hotel Astoria is a symbol of elegance and comfort. With an unbeatable location in St Isaac's Square in the heart of St Petersburg, ...</p>
    <p>Normalized KL(LMC)=</p>
    <p>Normalization corrects for bias introduced by small sample size</p>
    <p>)(</p>
    <p>)(</p>
    <p>R</p>
    <p>C</p>
    <p>LMKL</p>
    <p>LMKL</p>
  </div>
  <div class="page">
    <p>LMR converges to LMBG for large sample sizes</p>
    <p>Divergence of LMR substantial for small samples</p>
    <p>Reference LM (cont)Reference LM (cont)</p>
  </div>
  <div class="page">
    <p>Predicting Extraction Accuracy: The AlgorithmPredicting Extraction Accuracy: The Algorithm</p>
  </div>
  <div class="page">
    <p>Experimental EvaluationExperimental Evaluation</p>
    <p>How to measure success?</p>
    <p>Compare predicted ease of task vs. observed extraction accuracy</p>
    <p>Extraction Tasks: NER and RE</p>
    <p>NER: Datasets from the CoNLL 2002, 2003 evaluations</p>
    <p>RE: Binary relations between NEs and generic phrases</p>
  </div>
  <div class="page">
    <p>Extraction Task AccuracyExtraction Task Accuracy</p>
    <p>NER</p>
    <p>RE</p>
    <p>Relation Accuracy (%)</p>
    <p>strict partial Task Difficulty</p>
    <p>BORN 0.73 0.96 Easy</p>
    <p>DIED 0.34 0.97 Easy</p>
    <p>INVENT 0.35 0.64 Hard</p>
    <p>WROTE 0.12 0.50 Hard</p>
    <p>English Spanish Dutch</p>
    <p>LOC 90.21 79.84 79.19</p>
    <p>MISC 78.83 55.82 73.9</p>
    <p>ORG 81.86 79.69 69.48</p>
    <p>PER 91.47 86.83 78.83</p>
    <p>Overall 86.77 79.2 75.24</p>
  </div>
  <div class="page">
    <p>Document CollectionsDocument Collections</p>
    <p>Task Collection Size</p>
    <p>NER</p>
    <p>Reuters RCV1, 1/100 3,566,125 words</p>
    <p>Reuters RCV1, 1/10 35,639,471 words</p>
    <p>EFE newswire articles, May 2000 (Spanish) 367,589 words</p>
    <p>De Morgen articles (Dutch) 268,705 words</p>
    <p>RE Encarta document collection 64,187,912 words</p>
    <p>Note that Spanish and Dutch corpus sizes are much smaller</p>
  </div>
  <div class="page">
    <p>Predicting NER Performance (English)Predicting NER Performance (English)</p>
    <p>Florian et al. Chieu et al. Klein et al. Zhang et al. Carreras et al. Average</p>
    <p>LOC 91.15 91.12 89.98 89.54 89.26 90.21</p>
    <p>MISC 80.44 79.16 80.15 75.87 78.54 78.83</p>
    <p>ORG 84.67 84.32 80.48 80.46 79.41 81.86</p>
    <p>PER 93.85 93.44 90.72 90.44 88.93 91.47</p>
    <p>Overall 88.76 88.31 86.31 85.50 85.00 86.77</p>
    <p>Context size Absolute Normalized</p>
    <p>LOC 0.98 1.07</p>
    <p>MISC 1.29 1.40</p>
    <p>ORG 2.83 3.08</p>
    <p>PER 4.10 4.46</p>
    <p>RANDOM 0.92</p>
    <p>Absolute and Normalized KL-divergence</p>
    <p>LOC exception:</p>
    <p>Large overlap between locations in the training and test collections (i.e., simple gazetteers effective).</p>
    <p>Reuters 1/10, Context = 3 words, discard stopwords, avg</p>
  </div>
  <div class="page">
    <p>NER  Robustness / Different DimensionsNER  Robustness / Different Dimensions</p>
    <p>Counting stopwords (w) or not (w/o)</p>
    <p>Context Size</p>
    <p>Corpus size</p>
    <p>Reuters 1/100, context 3, avg</p>
    <p>Reuters 1/100, no stopwords, avg</p>
    <p>Reuters, context 3, no stopwords, avg</p>
    <p>LOC MISC ORG PER RAND</p>
    <p>F 90.2 78.8 81.9 91.5</p>
    <p>w 0.93 1.09 2.68 3.91 0.78</p>
    <p>w/o 1.48 1.83 3.81 5.62 1.27</p>
    <p>LOC MISC ORG PER RAND</p>
    <p>LOC MISC ORG PER RAND</p>
  </div>
  <div class="page">
    <p>Other Dimensions: Sample SizeOther Dimensions: Sample Size</p>
    <p>Normalized divergence of LMC remains high - Contrast with LMR for larger sample sizes</p>
  </div>
  <div class="page">
    <p>Other Dimensions: N-gram sizeOther Dimensions: N-gram size</p>
    <p>Higher order n-grams may help in some cases.</p>
    <p>LOC 90.21</p>
    <p>MISC 78.83</p>
    <p>ORG 81.86</p>
    <p>PER 91.47</p>
  </div>
  <div class="page">
    <p>Other LanguagesOther Languages</p>
    <p>Spanish</p>
    <p>Dutch Entity Actual</p>
    <p>LOC 79.19</p>
    <p>MISC 73.9</p>
    <p>ORG 69.48</p>
    <p>PER 78.83</p>
    <p>Context=1 Context=2 Context=3</p>
    <p>LOC 1.44 1.65 1.61</p>
    <p>MISC 1.97 2.02 1.91</p>
    <p>ORG 1.53 1.86 1.92</p>
    <p>PER 2.25 2.63 2.60</p>
    <p>RANDOM 2.59 1.89 1.71</p>
    <p>Context=1 Context=2 Context=3</p>
    <p>LOC 1.18 1.39 1.42</p>
    <p>MISC 1.73 2.12 2.35</p>
    <p>ORG 1.42 1.59 1.64</p>
    <p>PER 2.01 2.31 2.56</p>
    <p>RANDOM 2.42 1.82 1.53</p>
    <p>Entity Actual</p>
    <p>LOC 79.84</p>
    <p>MISC 55.82</p>
    <p>ORG 79.69</p>
    <p>PER 86.83</p>
    <p>Problem: very small collections</p>
  </div>
  <div class="page">
    <p>Predicting RE Performance (English)Predicting RE Performance (English)</p>
    <p>Relation Context size 1 Context size 2 Context size 3</p>
    <p>BORN 2.02 2.17 2.39</p>
    <p>DIED 1.89 1.86 1.83</p>
    <p>INVENT 1.94 1.75 1.72</p>
    <p>WROTE 1.59 1.59 1.53</p>
    <p>RANDOM 6.87 6.24 5.79</p>
    <p>Relation Accuracy (%)</p>
    <p>BORN 0.73 0.96</p>
    <p>DIED 0.34 0.97</p>
    <p>INVENT 0.35 0.64</p>
    <p>WROTE 0.12 0.50</p>
    <p>2- and 3- word contexts correctly distinguish between easy tasks (BORN, DIED), and difficult tasks (INVENT, WROTE).</p>
    <p>1-word context size appears not sufficient for predicting RE</p>
  </div>
  <div class="page">
    <p>Other Dimensions: Sample SizeOther Dimensions: Sample Size</p>
    <p>Divergence increases w/ sample size</p>
  </div>
  <div class="page">
    <p>Results SummaryResults Summary</p>
    <p>Context models can be effective in predicting the success of information extraction systems</p>
    <p>Even a small sample of available entities can be sufficient for making accurate predictions</p>
    <p>Available large collection size most important limiting factor</p>
  </div>
  <div class="page">
    <p>Other Applications and Future WorkOther Applications and Future Work</p>
    <p>Could use results for  Active learning/training IE  Improved boundary detection for NER  Improved confidence estimation of extraction</p>
    <p>e.g.: Culotta and McCallum [HLT 2004]</p>
    <p>For better results, could incorporate:  Internal contexts, gazeteers (e.g., for LOC entities)</p>
    <p>e.g.: Agichtein &amp; Ganti [KDD 2004], Cohen &amp; Sarawagi [KDD 2004]</p>
    <p>Syntax/logical distance  Coreference Resolution  Word classes</p>
  </div>
  <div class="page">
    <p>SummarySummary</p>
    <p>Presented the first attempt to predict information extraction accuracy for a given task and collection</p>
    <p>Developed a general, system-independent method utilizing Language Modelling techniques</p>
    <p>Estimates for extraction accuracy can help  Deploy information extraction systems  Port Information Extraction systems to new</p>
    <p>tasks, domains, collections, and languages</p>
  </div>
  <div class="page">
    <p>For More InformationFor More Information</p>
    <p>Text Mining, Search, and Navigation Group http://research.microsoft.com/tmsn/</p>
  </div>
</Presentation>
