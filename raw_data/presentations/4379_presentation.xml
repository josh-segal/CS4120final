<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Interpretable and Compositional</p>
    <p>Relation Learning by Joint Training</p>
    <p>with an Autoencoder</p>
    <p>Ryo Takahashi*1 Ran Tian*1 Kentaro Inui1,2</p>
    <p>(* equal contribution) 1Tohoku University 2RIKEN, Japan</p>
  </div>
  <div class="page">
    <p>Knowledge Bases (KBs) store a large amount of facts in</p>
    <p>the form of &lt;head entity, relation, tail entity&gt; triples:</p>
    <p>The Knowledge Base Completion (KBC) task aims to</p>
    <p>predict missing parts of an incomplete triple:</p>
    <p>Help discover missing facts in a KB</p>
    <p>July 18, 2018 2</p>
    <p>Task: Knowledge Base Completion</p>
    <p>The Matrix country_of_film Australia</p>
    <p>head entity</p>
    <p>tail entity relation</p>
    <p>Finding Nemo country_of_film</p>
    <p>?</p>
    <p>United States</p>
  </div>
  <div class="page">
    <p>Entity: represented by a low dimension vector (so</p>
    <p>that similar entities are</p>
    <p>close to each other)</p>
    <p>Relation: represented as transformation of the vector</p>
    <p>space, which can be:</p>
    <p>Vector Translation</p>
    <p>Linear map</p>
    <p>Non-linear map</p>
    <p>Up to design choice</p>
    <p>July 18, 2018 3</p>
    <p>Vector Based Approach</p>
    <p>The Matrix</p>
    <p>Finding Nemo</p>
    <p>US</p>
    <p>Australia</p>
    <p>A common approach to KBC is to model triples with a</p>
    <p>low dimension vector space, where</p>
  </div>
  <div class="page">
    <p>TransE [Bordes+13]  Relation as vector translation</p>
    <p>Intuitively suitable for 1-to-1</p>
    <p>relation</p>
    <p>Bilinear [Nickel+11]  Relation as linear</p>
    <p>transformation (matrix)</p>
    <p>Flexibly modeling N-to-N</p>
    <p>relation</p>
    <p>July 18, 2018 4</p>
    <p>2</p>
    <p>+</p>
    <p>same number of entities</p>
    <p>same distances within</p>
    <p>The Matrix</p>
    <p>Finding Nemo</p>
    <p>US</p>
    <p>Australia</p>
    <p>We follow</p>
    <p>country_of_film</p>
    <p>US</p>
    <p>Australia</p>
    <p>USD</p>
    <p>AUD currency</p>
    <p>of_country</p>
  </div>
  <div class="page">
    <p>More parameters compared to entity vector</p>
    <p>Objective is highly non-convex</p>
    <p>July 18, 2018 5</p>
    <p>Matrices are Difficult to Train</p>
    <p>2 vs. High dimension Low dimension</p>
    <p>entity</p>
    <p>vector</p>
    <p>relation</p>
    <p>matrix</p>
    <p>2</p>
  </div>
  <div class="page">
    <p>Propose jointly training relation matrices with an</p>
    <p>autoencoder:</p>
    <p>In order to reduce the high dimensionality</p>
    <p>Modified SGD with separated learning rates:</p>
    <p>In order to handle the highly non-convex</p>
    <p>training objective</p>
    <p>Use modified SGD to enhance joint training with</p>
    <p>autoencoder</p>
    <p>Other techniques for training relation matrices</p>
    <p>Achieve SOTA on standard KBC datasets</p>
    <p>July 18, 2018 6</p>
    <p>In this work:</p>
  </div>
  <div class="page">
    <p>TRAINING TECHNIQUES</p>
    <p>July 18, 2018 7</p>
  </div>
  <div class="page">
    <p>Proposed Train an autoencoder to</p>
    <p>reconstruct relation matrix from</p>
    <p>low dimension coding</p>
    <p>Base Model Represent relations as matrices in</p>
    <p>a bilinear model, can be</p>
    <p>extended with compositional</p>
    <p>training [Nickel+11, Guu+15, Tian+16]</p>
    <p>July 18, 2018 8</p>
    <p>Joint Training with an Autoencoder</p>
    <p>Finding 1. Reduce the high dimensionality of relation matrices</p>
    <p>2</p>
    <p>1</p>
    <p>2 2</p>
    <p>original reconstructed</p>
    <p>2</p>
    <p>2</p>
    <p>Train jointly</p>
    <p>2 2</p>
    <p>Different from usual</p>
    <p>autoencoders in which the</p>
    <p>original input is not updated</p>
  </div>
  <div class="page">
    <p>Proposed Train an autoencoder to</p>
    <p>reconstruct relation matrix from</p>
    <p>low dimension coding</p>
    <p>Base Model Represent relations as matrices in</p>
    <p>a bilinear model, can be</p>
    <p>extended with compositional</p>
    <p>training [Nickel+11, Guu+15, Tian+16]</p>
    <p>July 18, 2018 9</p>
    <p>Joint Training with an Autoencoder</p>
    <p>2</p>
    <p>1</p>
    <p>2 2</p>
    <p>original reconstructed</p>
    <p>2</p>
    <p>2</p>
    <p>Train jointly</p>
    <p>2 2</p>
    <p>Not easy to carry out</p>
    <p>Training objective is highly non-convex</p>
    <p>Easily fall into local minimums</p>
  </div>
  <div class="page">
    <p>Previous The common practice for setting</p>
    <p>learning rates of SGD [Bottou, 2012]:</p>
    <p>Modified Different parts in a neural network</p>
    <p>may have different learning rates</p>
    <p>July 18, 2018 10</p>
    <p>Modified SGD (Separated Learning Rates)</p>
    <p>: initial learning rate</p>
    <p>: coefficient of L2-regularizer</p>
    <p>: counter of trained examples</p>
    <p>KB   KB</p>
    <p>AE   AE</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>: counter of each entity</p>
    <p>: counter of each relation</p>
    <p>Our strategy Different learning rates for different parts of our model</p>
  </div>
  <div class="page">
    <p>Previous The common practice for setting</p>
    <p>learning rates of SGD [Bottou, 2012]:</p>
    <p>Modified Different parts in a neural network</p>
    <p>may have different learning rates</p>
    <p>July 18, 2018 11</p>
    <p>Modified SGD (Separated Learning Rates)</p>
    <p>: initial learning rate</p>
    <p>: coefficient of L2-regularizer</p>
    <p>KB   KB</p>
    <p>AE   AE</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>: counter of each entity</p>
    <p>: counter of each relation</p>
    <p>Our strategy Different learning rates for different parts of our model</p>
    <p>Learning rates for frequent entities</p>
    <p>and relations can decay more quickly</p>
  </div>
  <div class="page">
    <p>Modified Different parts in a neural network</p>
    <p>may have different learning rates</p>
    <p>July 18, 2018 12</p>
    <p>Modified SGD (Separated Learning Rates)</p>
    <p>KB   KB</p>
    <p>AE   AE</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>KB:  for KB-learning objective</p>
    <p>AE:  for autoencoder objective</p>
    <p>: counter of each entity</p>
    <p>: counter of each relation</p>
    <p>Our strategy Different learning rates for different parts of our model</p>
    <p>Rationale</p>
    <p>NN usually can be decomposed</p>
    <p>into several parts, each one is</p>
    <p>convex when other parts are fixed</p>
    <p>NN  joint co-training of many</p>
    <p>simple convex models</p>
    <p>Natural to assume different</p>
    <p>learning rate for each part</p>
  </div>
  <div class="page">
    <p>July 18, 2018 13</p>
    <p>Learning Rates for Joint Training Autoencoder</p>
    <p>KB</p>
    <p>()</p>
    <p>AE</p>
    <p>Autoencoder (AE)</p>
    <p>objective trying to fit to</p>
    <p>low dimension coding</p>
    <p>KB   KB</p>
    <p>KB objective trying</p>
    <p>to predict entities</p>
    <p>AE   AE</p>
    <p>Beginning of training</p>
    <p>AE is initialized randomly</p>
    <p>Does not make much sense</p>
    <p>to fit matrices to AE</p>
    <p>As the training proceeds</p>
    <p>KB and AE should balance</p>
  </div>
  <div class="page">
    <p>Normalization</p>
    <p>normalize relation</p>
    <p>matrices to  =  during training</p>
    <p>Regularization</p>
    <p>push  toward an orthogonal matrix</p>
    <p>Initialization</p>
    <p>initialize  as ( + )/2 instead of pure Gaussian</p>
    <p>July 18, 2018 14</p>
    <p>Other Training Techniques</p>
    <p>Minimize</p>
    <p>tr</p>
    <p>=</p>
    <p>on FB15k-237</p>
  </div>
  <div class="page">
    <p>EXPERIMENTS</p>
    <p>July 18, 2018 15</p>
  </div>
  <div class="page">
    <p>Dataset #Entity #Relation #Train #Valid #Test</p>
    <p>WN18RR [Dettmers+18]</p>
    <p>FB15k-237</p>
    <p>[Toutanova&amp;Chen15] 14,541 237 272,115 17,535 20,466</p>
    <p>July 18, 2018 16</p>
    <p>Datasets for Knowledge Base Completion</p>
    <p>WN18RR: subset of WordNet [Miller 95]</p>
    <p>FB15k-237: subset of Freebase [Bollacker+08]</p>
    <p>The previous WN18 and FB15k have an information</p>
    <p>leakage issue (refer our paper for test results)</p>
    <p>Evaluate models by how high the model ranks the gold</p>
    <p>test triples.</p>
  </div>
  <div class="page">
    <p>Model WN18RR FB15k-237</p>
    <p>MR MRR H10 MR MRR H10</p>
    <p>BASE 2447 .310 54.1 203 .328 51.5</p>
    <p>JOINT with AE 2268 .343 54.8 197 .331 51.6</p>
    <p>July 18, 2018 17</p>
    <p>Base Model vs. Joint Training with Autoencoder</p>
    <p>Joint training with an autoencoder</p>
    <p>improves upon the base bilinear model</p>
    <p>Metrics:  MR (Mean Rank):</p>
    <p>lower is better</p>
    <p>MRR (Mean Reciprocal Rank):</p>
    <p>higher is better</p>
    <p>H10 (Hits at 10):</p>
    <p>higher is better</p>
    <p>Models:  BASE: The bilinear model</p>
    <p>[Nickel+11]</p>
    <p>Proposed JOINT Training:</p>
    <p>Jointly train relation matrices</p>
    <p>with an autoencoder</p>
  </div>
  <div class="page">
    <p>Model WN18RR FB15k-237</p>
    <p>MR MRR H10 MR MRR H10</p>
    <p>Ours</p>
    <p>BASE 2447 .310 54.1 203 .328 51.5</p>
    <p>JOINT with AE 2268 .343 54.8 197 .331 51.6</p>
    <p>Re-experiments</p>
    <p>TransE [Bordes+13] 4311 .202 45.6 278 .236 41.6</p>
    <p>RESCAL [Nickel+11] 9689 .105 20.3 457 .178 31.9</p>
    <p>HolE [Nickel+16] 8096 .376 40.0 1172 .169 30.9</p>
    <p>Published results</p>
    <p>ComplEx [Trouillon+16] 5261 .440 51.0 339 .247 42.8</p>
    <p>ConvE [Dettmers+18] 5277 .460 48.0 246 .316 49.1</p>
    <p>July 18, 2018 18</p>
    <p>Compared to Previous Research</p>
    <p>Base model is competitive enough</p>
    <p>Our models achieved state-of-the-art results</p>
    <p>Normalization</p>
    <p>Regularization</p>
    <p>Initialization</p>
  </div>
  <div class="page">
    <p>Model WN18RR FB15k-237</p>
    <p>MR MRR H10 MR MRR H10</p>
    <p>Ours</p>
    <p>BASE 2447 .310 54.1 203 .328 51.5</p>
    <p>JOINT with AE 2268 .343 54.8 197 .331 51.6</p>
    <p>Re-experiments</p>
    <p>TransE [Bordes+13] 4311 .202 45.6 278 .236 41.6</p>
    <p>RESCAL [Nickel+11] 9689 .105 20.3 457 .178 31.9</p>
    <p>HolE [Nickel+16] 8096 .376 40.0 1172 .169 30.9</p>
    <p>Published results</p>
    <p>ComplEx [Trouillon+16] 5261 .440 51.0 339 .247 42.8</p>
    <p>ConvE [Dettmers+18] 5277 .460 48.0 246 .316 49.1</p>
    <p>July 18, 2018 19</p>
    <p>Compared to Previous Research</p>
    <p>Base model is competitive enough</p>
    <p>Our models achieved state-of-the-art results</p>
    <p>Normalization</p>
    <p>Regularization</p>
    <p>Initialization</p>
  </div>
  <div class="page">
    <p>July 18, 2018 20</p>
    <p>What Does the Trained Autoencoder Look Like?</p>
    <p>2 2</p>
    <p>Dimension 4 strongly</p>
    <p>correlates with film</p>
    <p>Sparse coding of relation matrices</p>
    <p>Interpretable to some extent</p>
    <p>Dimension 12 strongly</p>
    <p>correlates with currency</p>
  </div>
  <div class="page">
    <p>Composition of two relations in a KB coincide</p>
    <p>with a third relation:</p>
    <p>Extracted 154 examples of compositional</p>
    <p>relations from FB15k-237</p>
    <p>July 18, 2018 21</p>
    <p>Composition of Relations</p>
    <p>currency_of_country</p>
    <p>currency_of_film_budget</p>
    <p>country_of_film</p>
    <p>Film</p>
    <p>Country</p>
    <p>Currency</p>
  </div>
  <div class="page">
    <p>July 18, 2018 22</p>
    <p>Joint Training Helps Find Compositional Relations</p>
    <p>Model MR MRR</p>
    <p>BASE 1503 .0280.0010</p>
    <p>JOINT with AE 13027 .0481.0090</p>
    <p>Joint training with an autoencoder helps</p>
    <p>discovering compositional constraints</p>
    <p>currency_of_country</p>
    <p>currency_of_film_budget</p>
    <p>country_of_film</p>
    <p>country_of_film  currency_of_country  currency_of_film_budget</p>
    <p>If there is a composition Learned relation matrices to indeed</p>
    <p>comply with the composition</p>
  </div>
  <div class="page">
    <p>Task Knowledge Base Completion</p>
    <p>Approach Entities as low dimension vectors, relations as matrices</p>
    <p>Techniques Joint training relation matrices with autoencoder to reduce</p>
    <p>dimensionality</p>
    <p>Modified SGD: different learning rates for different parts</p>
    <p>Separated learning rates for updating relation matrices</p>
    <p>Normalization, Regularization, Initialization of relation matrices</p>
    <p>Results SOTA on WN18RR and FB15k-237</p>
    <p>Analysis Autoencoder learns sparse and interpretable low dimensional coding of</p>
    <p>relation matrices</p>
    <p>Dimension reduction helps find compositional relations</p>
    <p>Discussion Modern NNs have a lot of parameters</p>
    <p>Joint training with an autoencoder may reduce dimensionality while</p>
    <p>the NN is functioning</p>
    <p>More applications?</p>
    <p>July 18, 2018 23</p>
    <p>Conclusion and Discussion</p>
  </div>
</Presentation>
