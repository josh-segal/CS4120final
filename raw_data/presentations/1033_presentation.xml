<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scaling Distributed Filesystems in Resource-Harvesting Datacenters</p>
    <p>Pulkit A. Misra, igo Goiri, Jason Kace, Ricardo Bianchini</p>
  </div>
  <div class="page">
    <p>Resource-Harvesting Datacenters  Datacenters are under-utilized</p>
    <p>Provisioned for peak load, low tail latency  Harvest spare resources</p>
    <p>Co-locate services + batch jobs [Zhang, OSDI16]</p>
    <p>Enable datacenter-wide harvesting  Scale distributed file systems</p>
    <p>Server utilization distribution of a Google cluster</p>
  </div>
  <div class="page">
    <p>Scaling Distributed File Systems  More storage capacity demands</p>
    <p>Need bigger file system installations  Limitations to horizontal scaling</p>
    <p>Bottleneck at centralized components  Centralized metadata manager</p>
    <p>Manages namespace and blocks  Simplifies design and maintenance  Saturation: 4000 servers, ~40k reqs/sec</p>
    <p>Throughput vs Latency</p>
  </div>
  <div class="page">
    <p>Primary Tenants (PTs) own servers  Interactive services (e.g., Bing) are PTs</p>
    <p>Harvest resources from PTs  Avoid performance impact to the PT</p>
    <p>Challenges for distributed file systems</p>
    <p>Resource-Harvesting Challenges</p>
  </div>
  <div class="page">
    <p>Primary Tenants (PTs) own servers  Interactive services (e.g., Bing) are PTs</p>
    <p>Harvest resources from PTs  Avoid performance impact to the PT</p>
    <p>Challenges for distributed file systems</p>
    <p>Resource-Harvesting Challenges</p>
    <p>Time</p>
    <p>P T C</p>
    <p>P U</p>
    <p>U ti</p>
    <p>li za</p>
    <p>ti on Busy</p>
    <p>Threshold</p>
    <p>Busy servers fail accesses  lower availability</p>
  </div>
  <div class="page">
    <p>Primary Tenants (PTs) own servers  Interactive services (e.g., Bing) are PTs</p>
    <p>Harvest resources from PTs  Avoid performance impact to the PT</p>
    <p>Challenges for distributed file systems</p>
    <p>Resource-Harvesting Challenges</p>
    <p>Time</p>
    <p>P T C</p>
    <p>P U</p>
    <p>U ti</p>
    <p>li za</p>
    <p>ti on Busy</p>
    <p>Threshold</p>
    <p>Busy servers fail accesses  lower availability  Re-image disks  lower durability</p>
  </div>
  <div class="page">
    <p>Primary Tenants (PTs) own servers  Interactive services (e.g., Bing) are PTs</p>
    <p>Harvest resources from PTs  Avoid performance impact to the PT</p>
    <p>Challenges for distributed file systems</p>
    <p>Resource-Harvesting Challenges</p>
    <p>Time</p>
    <p>P T C</p>
    <p>P U</p>
    <p>U ti</p>
    <p>li za</p>
    <p>ti on Busy</p>
    <p>Threshold</p>
    <p>Busy servers fail accesses  lower availability  Re-image disks  lower durability</p>
    <p>Place replicas across PTs [Zhang,OSDI16]  Need diversity of PT servers in filesystem</p>
  </div>
  <div class="page">
    <p>Scaling Technique #1: ViewFS</p>
    <p>Partition of namespace on a subcluster  Mitigate metadata manager bottleneck  Users manually place data</p>
    <p>Unbalanced subclusters  Complex rebalance</p>
    <p>Need global view of the namespace, automated management</p>
    <p>Metadata Manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Metadata Manager</p>
    <p>Storage servers</p>
    <p>Subcluster s</p>
    <p>/apps/u se</p>
    <p>r</p>
  </div>
  <div class="page">
    <p>Single cluster with multiple strongly consistent metadata managers</p>
    <p>Global view of the namespace  More complex  No isolation from bugs or failures</p>
    <p>Need small independent subclusters for isolation [Verma, EuroSys15]</p>
    <p>Scaling Technique #2: Multiple Metadata Managers</p>
    <p>Storage servers</p>
    <p>Metadata Managers</p>
  </div>
  <div class="page">
    <p>Goals</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
    <p>/a pp</p>
    <p>/f ile</p>
    <p>.t xt</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
    <p>/a pp</p>
    <p>/f ile</p>
    <p>.t xt</p>
    <p>SC s</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
    <p>/a pp</p>
    <p>/f ile</p>
    <p>.t xt</p>
    <p>SC s</p>
    <p>/app/file.txt Storage server 1</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
    <p>/a pp</p>
    <p>/f ile</p>
    <p>.t xt</p>
    <p>SC s</p>
    <p>/app/file.txt Storage server 1</p>
  </div>
  <div class="page">
    <p>Our Solution: Datacenter-Harvesting File System</p>
    <p>Metadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster sMetadata manager</p>
    <p>Storage servers</p>
    <p>Subcluster 1</p>
    <p>Router 1 Router r</p>
    <p>State Store Rebalancer</p>
    <p>/a pp</p>
    <p>/f ile</p>
    <p>.t xt</p>
    <p>SC s</p>
    <p>/app/file.txt Storage server 1</p>
    <p>read</p>
  </div>
  <div class="page">
    <p>Goal #1: Transparent Scaling of File Systems  State Store</p>
    <p>Mount table: path  subcluster (SC)  Access load and capacity metrics  Router and rebalancer state</p>
    <p>Routers  Expose global namespace  Consult state store for path  sub cluster  Cache path resolutions</p>
    <p>user (1 TB, 15k)</p>
    <p>apps (4 TB, 30k)</p>
    <p>user_1 (2 TB, 15k)</p>
    <p>user_2 (2 TB, 10k)</p>
    <p>/ (5 TB, 15k)</p>
    <p>Mount table</p>
    <p>SC0 SC1 SC2</p>
  </div>
  <div class="page">
    <p>Goal #2: Enable Resource Harvesting  Provide high availability and durability</p>
    <p>Exploit behavioral diversity [Zhang,OSDI16]</p>
  </div>
  <div class="page">
    <p>Goal #2: Enable Resource Harvesting  Provide high availability and durability</p>
    <p>Exploit behavioral diversity [Zhang,OSDI16]</p>
    <p>Manual assignment</p>
    <p>Diversity in subclusters (# of Primary Tenants)</p>
    <p>Primary Tenant Manual: Primary Tenant  SC</p>
    <p>Less diversity in subclusters</p>
  </div>
  <div class="page">
    <p>Goal #2: Enable Resource Harvesting  Provide high availability and durability</p>
    <p>Exploit behavioral diversity [Zhang,OSDI16]</p>
    <p>Manual assignment</p>
    <p>Diversity in subclusters (# of Primary Tenants)</p>
    <p>Primary Tenant Manual: Primary Tenant  SC</p>
    <p>Less diversity in subclusters</p>
    <p>Consistent hashing: racks  SC  Randomization to promote diversity  Promote network locality (racks  SC)  Reduce data movement on SC add/remove</p>
    <p>Consistent hashing</p>
  </div>
  <div class="page">
    <p>Goal #3: Ensure Good Performance for Users</p>
    <p>Rebalancer as a minimization problem  Used capacity (&lt; 80% of available capacity)  Access load (&lt; 40k reqs/sec over a 5 minute period)  Amount of data moved for rebalancing  Mount table size</p>
  </div>
  <div class="page">
    <p>Goal #3: Ensure Good Performance for Users</p>
    <p>Rebalancer as a minimization problem  Used capacity (&lt; 80% of available capacity)  Access load (&lt; 40k reqs/sec over a 5 minute period)  Amount of data moved for rebalancing  Mount table size</p>
    <p>logs (1 TB, 15k)</p>
    <p>apps (4 TB, 35k)</p>
    <p>user (2 TB, 5k)</p>
    <p>user_1 (2 TB, 15k)</p>
    <p>user_2 (2 TB, 10k)</p>
    <p>/ (5 TB, 15k)</p>
  </div>
  <div class="page">
    <p>Goal #3: Ensure Good Performance for Users SC1: 8 TB, 60k reqs/sec</p>
    <p>Rebalancer as a minimization problem  Used capacity (&lt; 80% of available capacity)  Access load (&lt; 40k reqs/sec over a 5 minute period)  Amount of data moved for rebalancing  Mount table size</p>
    <p>logs (1 TB, 15k)</p>
    <p>apps (4 TB, 35k)</p>
    <p>user (2 TB, 5k)</p>
    <p>user_1 (2 TB, 15k)</p>
    <p>user_2 (2 TB, 10k)</p>
    <p>/ (5 TB, 15k)</p>
    <p>SC1 overloaded</p>
  </div>
  <div class="page">
    <p>Goal #3: Ensure Good Performance for Users SC1: 8 TB, 60k reqs/sec</p>
    <p>Rebalancer as a minimization problem  Used capacity (&lt; 80% of available capacity)  Access load (&lt; 40k reqs/sec over a 5 minute period)  Amount of data moved for rebalancing  Mount table size</p>
    <p>logs (1 TB, 15k)</p>
    <p>apps (4 TB, 35k)</p>
    <p>user (2 TB, 5k)</p>
    <p>user_1 (2 TB, 15k)</p>
    <p>user_2 (2 TB, 10k)</p>
    <p>/ (5 TB, 15k)</p>
    <p>Move to SC3</p>
  </div>
  <div class="page">
    <p>SC1: 4 TB, 35k reqs/sec SC3: 6 TB, 30k reqs/sec</p>
    <p>Goal #3: Ensure Good Performance for Users SC1: 8 TB, 60k reqs/sec</p>
    <p>Rebalancer as a minimization problem  Used capacity (&lt; 80% of available capacity)  Access load (&lt; 40k reqs/sec over a 5 minute period)  Amount of data moved for rebalancing  Mount table size</p>
    <p>logs (1 TB, 15k)</p>
    <p>apps (4 TB, 35k)</p>
    <p>user (6 TB, 30k)</p>
    <p>/ (5 TB, 15k)</p>
  </div>
  <div class="page">
    <p>Implementation</p>
  </div>
  <div class="page">
    <p>Datacenter-Harvesting HDFS (DH-HDFS)  Implement federation architecture over HDFS  Diversity-aware replica placement  Run independent instances in subclusters</p>
    <p>Implementation</p>
    <p>Subcluster s</p>
  </div>
  <div class="page">
    <p>Datacenter-Harvesting HDFS (DH-HDFS)  Implement federation architecture over HDFS  Diversity-aware replica placement  Run independent instances in subclusters</p>
    <p>Implementation</p>
    <p>Router r</p>
    <p>Subcluster s</p>
    <p>Load balancing for Routers</p>
  </div>
  <div class="page">
    <p>Datacenter-Harvesting HDFS (DH-HDFS)  Implement federation architecture over HDFS  Diversity-aware replica placement  Run independent instances in subclusters</p>
    <p>Implementation</p>
    <p>Router r</p>
    <p>State Store</p>
    <p>Subcluster s</p>
    <p>Load balancing for Routers  Zookeeper for State Store</p>
  </div>
  <div class="page">
    <p>Datacenter-Harvesting HDFS (DH-HDFS)  Implement federation architecture over HDFS  Diversity-aware replica placement  Run independent instances in subclusters</p>
    <p>Implementation</p>
    <p>Router r</p>
    <p>State Store</p>
    <p>Subcluster s</p>
    <p>Rebalancer</p>
    <p>Load balancing for Routers  Zookeeper for State Store  Rebalancer as a MapReduce job</p>
  </div>
  <div class="page">
    <p>Real deployment  4k servers divided into 4 subclusters  Deployment in production: 30k servers across 4 datacenters</p>
    <p>Large-scale simulation  Traces from production datacenters at Microsoft  Simulate full datacenters for 6 months</p>
    <p>HDFS trace from Yahoo!  700k files and 4 million accesses</p>
    <p>Evaluation</p>
  </div>
  <div class="page">
    <p>Baseline system: Groups of primary tenants  subclusters  Spectrum of primary tenant CPU utilization: low, mid and high  Significantly higher availability with DH-HDFS  Improvement in data durability (results in paper)</p>
    <p>Simulation: Availability (% Successful Accesses)</p>
  </div>
  <div class="page">
    <p>Baseline system: Groups of primary tenants  subclusters  Spectrum of primary tenant CPU utilization: low, mid and high  Significantly higher availability with DH-HDFS  Improvement in data durability (results in paper)</p>
    <p>Simulation: Availability (% Successful Accesses)</p>
    <p>Low utilization</p>
    <p>(up to 25%)</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Baseline system: Groups of primary tenants  subclusters  Spectrum of primary tenant CPU utilization: low, mid and high  Significantly higher availability with DH-HDFS  Improvement in data durability (results in paper)</p>
    <p>Simulation: Availability (% Successful Accesses)</p>
    <p>utilization (up to 50%)</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Baseline system: Groups of primary tenants  subclusters  Spectrum of primary tenant CPU utilization: low, mid and high  Significantly higher availability with DH-HDFS  Improvement in data durability (results in paper)</p>
    <p>Simulation: Availability (% Successful Accesses)</p>
    <p>utilization (up to 75%)</p>
    <p>Lower is better</p>
  </div>
  <div class="page">
    <p>Real Deployment: Router Performance</p>
    <p>Worst-case scenario: metadata-only operations workload  Block read latencies dominate in real-world workloads  Negligible router overhead in real workloads</p>
  </div>
  <div class="page">
    <p>Real Deployment: Router Performance</p>
    <p>Worst-case scenario: metadata-only operations workload  Block read latencies dominate in real-world workloads  Negligible router overhead in real workloads</p>
  </div>
  <div class="page">
    <p>Real Deployment: Router Performance</p>
    <p>Worst-case scenario: metadata-only operations workload  Block read latencies dominate in real-world workloads  Negligible router overhead in real workloads</p>
  </div>
  <div class="page">
    <p>Real Deployment: Router Performance</p>
    <p>Worst-case scenario: metadata-only operations workload  Block read latencies dominate in real-world workloads  Negligible router overhead in real workloads</p>
  </div>
  <div class="page">
    <p>Real Deployment: Rebalancer Performance</p>
    <p>13 TB data moved to balance subcluster 0  Average rebalance time: 6 mins</p>
    <p>100 ms to determine data to move  Primary tenant activity impacts data migration time (up to 4x)</p>
  </div>
  <div class="page">
    <p>Real Deployment: Rebalancer Performance</p>
    <p>13 TB data moved to balance subcluster 0  Average rebalance time: 6 mins</p>
    <p>100 ms to determine data to move  Primary tenant activity impacts data migration time (up to 4x)</p>
    <p>SC0 overloaded</p>
    <p>watermark: 2000 reqs/s</p>
  </div>
  <div class="page">
    <p>30k servers spread across 4 datacenter  Bootstrapping server  subcluster assignment</p>
    <p>Switch to consistent hashing caused massive reshuffling of servers  Restrict movement till servers are re-imaged or decommissioned</p>
    <p>Spread large data across subclusters  Users wanted data of batch jobs in a single folder  Create special folders with files distributed across subclusters</p>
    <p>More lessons in the paper</p>
    <p>Lessons from Production Deployment</p>
  </div>
  <div class="page">
    <p>Scale file systems to entire datacenter  Datacenter-Harvesting HDFS</p>
    <p>Runs independent subclusters  isolation  Federates subclusters transparently  global namespace  Higher durability and availability on harvested resources  Better file access performance via rebalancing</p>
    <p>Deployed in production datacenters  30k servers spread across 4 datacenters</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Questions?</p>
  </div>
</Presentation>
