<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Man vs. Machine: Adversarial Detection of Malicious Crowdsourcing Workers</p>
    <p>Gang Wang, Tianyi Wang, Haitao Zheng, Ben Y. Zhao</p>
    <p>UC Santa Barbara gangw@cs.ucsb.edu</p>
  </div>
  <div class="page">
    <p>Machine Learning for Security</p>
    <p>Machine learning (ML) to solve security problems  Email spam detection  Intrusion/malware detection  Authentication  Identifying fraudulent accounts (Sybils) and content</p>
    <p>Example: ML for Sybil detection in social networks</p>
    <p>Training Classifier</p>
    <p>Known samples</p>
    <p>Unknown Accounts</p>
  </div>
  <div class="page">
    <p>Adversarial Machine Learning</p>
    <p>Key vulnerabilities of machine learning systems  ML models derived from fixed datasets  Assuming similar distribution of training and real-world data</p>
    <p>Strong adversaries in ML systems  Aware of usage, reverse engineering ML systems  Adaptive evasion, temper with the trained model</p>
    <p>Practical adversarial attacks  What are the practical constrains for adversaries?  With constrains, how effective are adversarial attacks?</p>
  </div>
  <div class="page">
    <p>Context: Malicious Crowdsourcing</p>
    <p>New threat: malicious crowdsourcing = crowdturfing  Hiring a large army of real users for malicious attacks  Fake customer reviews, rumors, targeted spam  Most existing defenses fail against real users (CAPTCHA)</p>
  </div>
  <div class="page">
    <p>Online Crowdturfing Systems</p>
    <p>Online crowdturfing systems (services)  Connect customers with online users willing to spam for money  Sites located across the glob, e.g. China, US, India</p>
    <p>Crowdturfing in China  Largest crowdturfing sites: ZhuBaJie (ZBJ) and SanDaHa (SDH)  Million-dollar industry, tens of millions of tasks finished</p>
    <p>Customer</p>
    <p>Crowd workers</p>
    <p>Crowdturfing site Target Network</p>
  </div>
  <div class="page">
    <p>Machine Learning vs. Crowdturfing</p>
    <p>Machine learning to detect crowdturfing workers  Simple methods usually fail (e.g. CAPTCHA, rate limit)  Machine learning: more sophisticated modeling on user behaviors</p>
    <p>o You are how you click [USENIX13]</p>
    <p>Perfect context to study adversarial machine learning</p>
    <p>all worker behaviors</p>
  </div>
  <div class="page">
    <p>Goals and Questions</p>
    <p>Our goals  Develop defense against crowdturfing on Weibo (Chinese Twitter)  Understand the impact of adversarial countermeasures and the</p>
    <p>robustness of machine learning classifiers</p>
    <p>Key questions  What ML algorithms can accurately detect crowdturfing workers?  What are possible ways for adversaries to evade classifiers?  Can adversaries attack ML models by tampering with training data?</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Detection of Crowdturfing</p>
    <p>Adversarial Machine Learning Attacks</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Detect crowdturf workers on Weibo</p>
    <p>Adversarial machine learning attacks  Evasion Attack: workers evade classifiers  Poisoning Attack: crowdturfing admins tamper with training data</p>
    <p>Methodology</p>
    <p>Classifier</p>
    <p>Training Data</p>
    <p>Training (e.g. SVM)</p>
    <p>Poison AMack</p>
    <p>Evasion AMack</p>
  </div>
  <div class="page">
    <p>Ground-truth Dataset</p>
    <p>Crowdturfing campaigns targeting Weibo  Two largest crowdturfing sites ZBJ and SDH  Complete historical transaction records for 3 years (2009-2013)  20,416 Weibo campaigns: &gt; 1M tasks, 28,947 Weibo accounts</p>
    <p>Collect Weibo profiles and their latest tweets  Workers: 28K Weibo accounts used by ZBJ and SDH workers  Baseline users: snowball sampled 371K baseline users</p>
  </div>
  <div class="page">
    <p>Features to Detect Crowd-workers</p>
    <p>Search for behavioral features to detect workers</p>
    <p>Observations  Aged, well established accounts  Balanced follower-followee ratio  Using cover traffic</p>
    <p>Final set of useful features: 35  Baseline profile fields (9)  User interaction (comment, retweet) (8)  Tweeting device and client (5)  Burstiness of tweeting (12)  Periodical patterns (1)</p>
    <p>Task-driven nature</p>
    <p>Active at posting but have less bidirectional interactions</p>
  </div>
  <div class="page">
    <p>Performance of Classifiers</p>
    <p>Building classifiers on ground-truth data  Random Forests (RF)  Decision Tree (J48)  SVM radius kernel (SVMr)  SVM polynomial (SVMp)  Nave Bayes (NB)  Bayes Network (BN)</p>
    <p>Classifiers dedicated to detect professional workers</p>
    <p>Workers who performed &gt; 100 tasks  Responsible for 90% of total spam  More accurate to detect the professionals  99% accuracy</p>
    <p>RF J48 SVMr SVMp BN NB</p>
    <p>False Posi\ve Rate False Nega\ve Rate</p>
    <p>Random Forests: 95% accuracy</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation</p>
    <p>Detection of Crowdturfing</p>
    <p>Adversarial Machine Learning Attacks  Evasion attack</p>
    <p>Poisoning attack</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Classifier</p>
    <p>Training Data</p>
    <p>Training (e.g. SVM)</p>
    <p>Evasion AMack</p>
    <p>Detec\on Model Training</p>
  </div>
  <div class="page">
    <p>Attack #1: Adversarial Evasion</p>
    <p>Individual workers as adversaries  Workers seek to evade a classifier by mimicking normal users  Identify the key set of features to modify for evasion</p>
    <p>Attack strategy depends on workers knowledge on classifier  Learning algorithm, feature space, training data</p>
    <p>What knowledge is practically available? How does different knowledge level impact workers evasion?</p>
  </div>
  <div class="page">
    <p>A Set of Evasion Models</p>
    <p>Optimal evasion scenarios  Per-worker optimal: Each worker has perfect</p>
    <p>knowledge about the classifier</p>
    <p>Global optimal: knows the direction of the boundary  Feature-aware evasion: knows feature ranking</p>
    <p>Practical evasion scenario  Only knows normal users statistics  Estimate which of their features are most abnormal</p>
    <p>? ?</p>
    <p>? ?</p>
    <p>Prac\cal</p>
    <p>Op\mal</p>
    <p>Classifica\on boundary</p>
  </div>
  <div class="page">
    <p>Evasion Attack Results</p>
    <p>te (%</p>
    <p>)</p>
    <p>Number of Features Altered</p>
    <p>J48</p>
    <p>SVMp</p>
    <p>RF</p>
    <p>SVMr</p>
    <p>Evasion is highly effective with perfect knowledge, but less effective in practice</p>
    <p>Most classifiers are vulnerable to evasion  Random Forests are slightly more robust (J48 Tree the worst)</p>
    <p>Op@mal AAack</p>
    <p>te (%</p>
    <p>) Number of Features Altered</p>
    <p>J48</p>
    <p>SVMp</p>
    <p>RF</p>
    <p>SVMr</p>
    <p>Prac@cal AAack</p>
    <p>No single classifier is robust against evasion. The key is to limit adversaries knowledge</p>
    <p>Need to alter 20 features</p>
  </div>
  <div class="page">
    <p>Classifier</p>
    <p>Training Data</p>
    <p>Training (e.g. SVM)</p>
    <p>Poison AMack</p>
    <p>Detec\on Model Training</p>
  </div>
  <div class="page">
    <p>Attack #2: Poisoning Attack</p>
    <p>Crowdturfing site admins as adversaries  Highly motivated to protect their workers, centrally control workers  Tamper with the training data to manipulate model training</p>
    <p>Two practical poisoning methods  Inject mislabeled samples to training data  wrong classifier  Alter worker behaviors uniformly by enforcing system policies</p>
    <p>harder to train accurate classifiers</p>
    <p>Injec\on AMack</p>
    <p>Inject normal accounts, but labeled as worker</p>
    <p>Wrong model, false posi\ves!</p>
    <p>Altering AMack Difficult to classify!</p>
  </div>
  <div class="page">
    <p>Injecting Poison Samples</p>
    <p>Injecting benign accounts as workers into training data  Aim to trigger false positives during detection</p>
    <p>Fa ls e Po</p>
    <p>si @ ve R at e (%</p>
    <p>)</p>
    <p>Ra@o of Poison-to-Turfing</p>
    <p>Tree</p>
    <p>SVMp</p>
    <p>RF</p>
    <p>SVMr J48-Tree is more vulnerable than others</p>
    <p>Poisoning aMack is highly effec\ve More accurate classifier can be more vulnerable</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Key observations  Accurate machine learning classifiers can be highly vulnerable  No single classifier excels in all attack scenarios, Random Forests</p>
    <p>and SVM are more robust than Decision Tree.</p>
    <p>Adversarial attack impact highly depends on adversaries knowledge</p>
    <p>Moving forward: improve robustness of ML classifiers</p>
    <p>Multiple classifier in one detector (ensemble learning)  Adversarial analysis in unsupervised learning</p>
  </div>
  <div class="page">
    <p>Thank You! Questions?</p>
  </div>
</Presentation>
