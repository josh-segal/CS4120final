<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Hitchhikers Guide to Testing Statistical Significance in NLP</p>
    <p>Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart ACL 2018</p>
    <p>https://github.com/rtmdrr/testSignificanceNLP</p>
  </div>
  <div class="page">
    <p>I want to be state of the art Ingredients</p>
    <p>my new algorithm   current SOTA</p>
    <p>algorithm  Data  Evaluation measure</p>
    <p>Directions</p>
    <p>Apply algorithm on  Apply algorithm on  Test if</p>
  </div>
  <div class="page">
    <p>This is not enough!</p>
    <p>The difference between the performance of algorithm and could be coincidental!  We need to make sure that the probability of making a false claim is</p>
    <p>very small.  We can do so by</p>
    <p>Testing Statistical Significance!</p>
  </div>
  <div class="page">
    <p>NLP &amp; Hypothesis Testing  Survey ACL 2017</p>
    <p>180 experimental long papers  63 checked statistical significance  Only 42 mentioned the name of the statistical test  Only 36 used the correct statistical test - of all papers!</p>
    <p>OK!Checked significan</p>
    <p>ce180 experiment al papers</p>
  </div>
  <div class="page">
    <p>Simple Guide</p>
  </div>
  <div class="page">
    <p>Statistical Significance Hypothesis Testing</p>
    <p>Let: .</p>
  </div>
  <div class="page">
    <p>Statistical Significance Hypothesis Testing</p>
    <p>The smaller the p-value is, the higher the indication that the null hypothesis, , does not hold.</p>
    <p>We reject the null hypothesis if</p>
  </div>
  <div class="page">
    <p>Statistical Significance Hypothesis Testing</p>
    <p>Type I error  rejecting the null hypothesis when it is true</p>
    <p>Type II error not rejecting the null hypothesis when the alternative is true</p>
    <p>Significance level  probability of making type I error ()</p>
    <p>Significance Power  probability of not making type II error</p>
  </div>
  <div class="page">
    <p>So</p>
    <p>Lets all test for statistical significance! Why not?</p>
    <p>OK</p>
  </div>
  <div class="page">
    <p>NLP &amp; Hypothesis Testing - Problems</p>
    <p>Both algorithms are applied on the same data.</p>
    <p>What is the distribution of ?</p>
    <p>Data samples are not independent.</p>
  </div>
  <div class="page">
    <p>Paired Statistical Tests</p>
    <p>Both algorithms are applied on the same data  dependent</p>
    <p>Paired sample: sample selected from the first population is related to the corresponding sample from the second population</p>
    <p>Solution: apply paired-version of statistical test  Paired t-test, Wilcoxon signed-rank test, paired bootstrap</p>
  </div>
  <div class="page">
    <p>NLP &amp; Hypothesis Testing - Problems</p>
    <p>Both algorithms are applied on the same data.</p>
    <p>What is the distribution of ?</p>
    <p>Data samples are not independent.</p>
  </div>
  <div class="page">
    <p>Parametric Tests</p>
    <p>First case: the distribution of is Normal</p>
    <p>Parametric tests make assumptions about the test statistic distribution, particularly - normal distribution.</p>
    <p>When the parametric test meets assumptions it has high statistical power  Linear regression analyses  T-tests and analyses of variance on the difference of means  Normal curve Z-tests of the differences of means and proportions</p>
  </div>
  <div class="page">
    <p>Parametric Tests  Check for Normality  Shapiro-Wilk: tests if a sample comes from a normally distributed population</p>
    <p>scipy.stats.shapiro([a-b for a, b in zip(res_A, res_B)])</p>
    <p>Anderson-Darling: tests if a sample is drawn from a given distribution scipy.stats.anderson([a-b for a, b in zip(res_A, res_B)], 'norm')</p>
    <p>Kolmogorov-Smirnov: goodness of fit test. Samples are standardized and compared with a standard normal distribution.</p>
    <p>scipy.stats.kstest([a-b for a, b in zip(res_A, res_B)], 'norm')</p>
  </div>
  <div class="page">
    <p>Non-Parametric Tests</p>
    <p>Second case: the distribution of is unknown\not normal</p>
    <p>Non parametric tests do not assume anything about the test statistic distribution</p>
    <p>Two types  sampling-free and sampling-based tests</p>
  </div>
  <div class="page">
    <p>Sampling-Free Non-Parametric Tests</p>
    <p>Binomial\ Multinomial</p>
    <p>McNemar</p>
    <p>Cochrens Q</p>
    <p>Not Normal</p>
    <p>Sign</p>
    <p>Wilcoxon signed-rank</p>
  </div>
  <div class="page">
    <p>Sampling-Based Non-Parametric Tests</p>
    <p>Permutation tests: resamples drawn at random from the original data. Without replacements.  Paired design  consider all possible choices</p>
    <p>of signs to attach to each difference.</p>
    <p>Bootstrap: resamples drawn at random from the original data. With replacements.  Paired design  sample with repetitions from</p>
    <p>the set of all differences.</p>
  </div>
  <div class="page">
    <p>NLP &amp; Hypothesis Testing - Problems</p>
    <p>Both algorithms are applied on the same data.</p>
    <p>What is the distribution of ?</p>
    <p>Data samples are not independent.</p>
  </div>
  <div class="page">
    <p>NLP Data and I.I.D Assumption</p>
    <p>Many NLP datasets have dependent samples</p>
    <p>All statistical test assume independency =&gt; all tests are invalid, impact hard to quantify</p>
    <p>Solution: come up with statistical tests that allow dependencies</p>
  </div>
  <div class="page">
    <p>NLP &amp; Hypothesis Testing</p>
    <p>Both algorithms are applied on the same data.</p>
    <p>What is the distribution of ?</p>
    <p>Data samples are not independent.</p>
  </div>
  <div class="page">
    <p>Simple Guide</p>
  </div>
  <div class="page">
    <p>Thank You for Listening</p>
    <p>Questions?</p>
    <p>https://github.com/rtmdrr/testSignificanceNLP</p>
  </div>
</Presentation>
