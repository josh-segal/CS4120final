<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Deep-speare: A Joint Neural Model of Poetic Language, Meter and Rhyme</p>
    <p>Jey Han Lau1,2, Trevor Cohn2, Timothy Baldwin2, Julian Brooke3, and Adam Hammond4</p>
    <p>July 17, 2018</p>
  </div>
  <div class="page">
    <p>Creativity</p>
    <p>I Can machine learning models be creative?</p>
    <p>I Can these models compose novel and interesting narrative?</p>
    <p>I Creativity is a hallmark of intelligence  it often involves blending ideas from different domains.</p>
    <p>I We focus on sonnet generation in this work.</p>
  </div>
  <div class="page">
    <p>Sonnets</p>
    <p>Shall I compare thee to a summers day? Thou art more lovely and more temperate: Rough winds do shake the darling buds of May, And summers lease hath all too short a date:</p>
    <p>I A distinguishing feature of poetry is its aesthetic forms, e.g. rhyme and rhythm/meter.</p>
    <p>I Rhyme: {day , May}; {temperate, date}.</p>
    <p>I Stress (pentameter):</p>
    <p>S S+ S S+ S S+ S S+ S S+</p>
    <p>Shall I compare thee to a summers day?</p>
  </div>
  <div class="page">
    <p>Modelling Approach</p>
    <p>I We treat the task of poem generation as a constrained language modelling task.</p>
    <p>I Given a rhyming scheme, each line follows a canonical meter and has a fixed number of stresses.</p>
    <p>I We focus specifically on sonnets as it is a popular type of poetry (sufficient data) and has regular rhyming (ABAB, AABB or ABBA) and stress pattern (iambic pentameter).</p>
    <p>I We train an unsupervised model of language, rhyme and meter on a corpus of sonnets.</p>
  </div>
  <div class="page">
    <p>Sonnet Corpus</p>
    <p>I We first create a generic poetry document collection using GutenTag tool, based on its inbuilt poetry classifier.</p>
    <p>I We then extract word and character statistics from Shakespeares 154 sonnets.</p>
    <p>I We use the statistics to filter out all non-sonnet poems, yielding our sonnet corpus.</p>
    <p>Partition #Sonnets #Words</p>
    <p>Train 2685 367K Dev 335 46K Test 335 46K</p>
  </div>
  <div class="page">
    <p>Model Architecture</p>
    <p>(a) Language model (b) Pentameter model (c) Rhyme model</p>
  </div>
  <div class="page">
    <p>Language Model (LM)</p>
    <p>I LM is a variant of an LSTM encoderdecoder model with attention.</p>
    <p>I Encoder encodes preceding contexts, i.e. all sonnet lines before the current line.</p>
    <p>I Decoder decodes one word at a time for the current line, while attending to the preceding context.</p>
    <p>I Preceding context is filtered by a selective mechanism.</p>
    <p>I Character encodings are incorporated for decoder input words.</p>
    <p>I Input and output word embeddings are tied.</p>
  </div>
  <div class="page">
    <p>Pentameter Model (PM)</p>
    <p>I PM is designed to capture the alternating stress pattern.</p>
    <p>I Given a sonnet line, PM learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.</p>
    <p>T Attention Prediction</p>
    <p>... 8 Shall I compare thee to a summers day? S</p>
  </div>
  <div class="page">
    <p>Pentameter Model (PM)</p>
    <p>I PM fashioned as an encoderdecoder model.</p>
    <p>I Encoder encodes the characters of a sonnet line.</p>
    <p>I Decoder attends to the character encodings to predict the stresses.</p>
    <p>I Decoder states are not used in prediction.</p>
    <p>I Attention networks focus on characters whose position is monotonically increasing.</p>
    <p>I In addition to cross-entropy loss, PM is regularised further with two auxilliary objectives that penalise repetition and low coverage.</p>
  </div>
  <div class="page">
    <p>Pentameter Model (PM)</p>
  </div>
  <div class="page">
    <p>Rhyme Model</p>
    <p>I We learn rhyme in an unsupervised fashion for 2 reasons:</p>
    <p>I Extendable to other languages that dont have pronunciation dictionaries;</p>
    <p>I The language of our sonnets is not Modern English, so contemporary pronunciation dictionaries may not be accurate.</p>
    <p>I Assumption: rhyme exists in a quatrain.</p>
    <p>I Feed sentence-ending word pairs as input to the rhyme model and train it to separate rhyming word pairs from non-rhyming ones.</p>
  </div>
  <div class="page">
    <p>Rhyme Model</p>
    <p>Shall I compare thee to a summers day? ut Thou art more lovely and more temperate: ur Rough winds do shake the darling buds of May, ur+1 And summers lease hath all too short a date: ur+2</p>
    <p>Q = {cos(ut,ur ), cos(ut,ur+1), cos(ut,ur+2)} Lrm = max(0,  top(Q, 1) + top(Q, 2))</p>
    <p>I top(Q,k) returns the k-th largest element in Q.</p>
    <p>I Intuitively the model is trained to learn a sufficient margin that separates the best pair from all others, with the second-best being used to quantify all others.</p>
  </div>
  <div class="page">
    <p>Joint Training</p>
    <p>I All components trained together by treating each component as a sub-task in a multi-task learning setting.</p>
    <p>I Although the components (LM, PM and RM) appear to be disjointed, shared parameters allow the components to mutually influence each other during training.</p>
    <p>I If each component is trained separately, PM performs poorly.</p>
  </div>
  <div class="page">
    <p>Model Architecture</p>
    <p>(a) Language model (b) Pentameter model (c) Rhyme model</p>
  </div>
  <div class="page">
    <p>Evaluation: Crowdworkers</p>
    <p>I Crowdworkers are presented with a pair of poems (one machine-generated and one human-written), and asked to guess which is the human-written one.</p>
    <p>I LM: vanilla LSTM language model;</p>
    <p>I LM: LSTM language model that incorporates both character encodings and preceding context;</p>
    <p>I LM+PM+RM: the full model, with joint training of the language, pentameter and rhyme models.</p>
  </div>
  <div class="page">
    <p>Evaluation: Crowdworkers (2)</p>
    <p>Model Accuracy</p>
    <p>LM 0.742 LM 0.672</p>
    <p>LM+PM+RM 0.532</p>
    <p>LM+RM 0.532</p>
    <p>I Accuracy improves LM &lt; LM &lt; LM+PM+RM, indicating generated quatrains are less distinguishable.</p>
    <p>I Are workers judging poems using just rhyme?</p>
    <p>I Test with LM+RM reveals thats the case.</p>
    <p>I Meter/stress is largely ignored by laypersons in poetry evaluation.</p>
  </div>
  <div class="page">
    <p>Evaluation: Expert</p>
    <p>Model Meter Rhyme Read. Emotion</p>
    <p>LM 4.000.73 1.570.67 2.770.67 2.730.51 LM</p>
    <p>4.071.03 1.530.88 3.101.04 2.930.93 LM</p>
    <p>+PM+RM 4.100.91 4.430.56 2.700.69 2.900.79</p>
    <p>Human 3.871.12 4.101.35 4.800.48 4.370.71</p>
    <p>I A literature expert is asked to judge poems on the quality of meter, rhyme, readability and emotion.</p>
    <p>I Full model has the highest meter and rhyme ratings, even higher than human, reflecting that poets regularly break rules.</p>
    <p>I Despite excellent form, machine-generated poems are easily distinguished due to lower emotional impact and readability.</p>
    <p>I Vanilla language model (LM) captures meter surprisingly well.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>I We introduce a joint neural model that learns language, rhyme and stress in an unsupervised fashion.</p>
    <p>I We encode assumptions we have about the rhyme and stress in the architecture of the network.</p>
    <p>I Model can be adapted to poetry in other languages.</p>
    <p>I We assess the quality of generated poems using judgements from crowdworkers and a literature expert.</p>
    <p>I Our results suggest future research should look beyond forms, towards the substance of good poetry.</p>
    <p>I Code and data: https://github.com/jhlau/deepspeare</p>
  </div>
  <div class="page">
    <p>Untitled</p>
    <p>in darkness to behold him, with a light and him was filled with terror on my breast</p>
    <p>and saw its brazen ruler of the night but, lo! it was a monarch of the rest</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
