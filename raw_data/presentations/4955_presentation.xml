<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Numerical Computing: An Introduction</p>
    <p>Gyula Horvath Tom Verhoeff</p>
    <p>Horvath@inf.u-szeged.hu T.Verhoeff@TUE.NL</p>
    <p>University of Szeged Eindhoven University of Technology</p>
    <p>Hungary The Netherlands</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing1</p>
  </div>
  <div class="page">
    <p>IOI 2002: Bus Terminals</p>
    <p>A B</p>
    <p>Given a set of points with integer coordinates,</p>
    <p>select two points as hubs and</p>
    <p>assign each of the remaining points to a hub,</p>
    <p>while minimizing the maximum value (over all P, Q) of</p>
    <p>c(P, Q) = d(P, H(P )) + d(H(P ), H(Q)) + d(H(Q), Q)</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing2</p>
  </div>
  <div class="page">
    <p>Integer Computations</p>
    <p>Z = the set of integers</p>
    <p>How well is integer arithmetic implemented on a computer?</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing3</p>
  </div>
  <div class="page">
    <p>Non-Integer Numbers</p>
    <p>Fractions, percentages, fixed-point currency values</p>
    <p>Real numbers, complex numbers</p>
    <p>Scientific notation: 6.022142  1023</p>
    <p>Floating-point types in programming languages</p>
    <p>How well is non-integer arithmetic implemented on a computer?</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing4</p>
  </div>
  <div class="page">
    <p>Quote from Donald E. Knuth</p>
    <p>Floating point computation is by nature inexact, and programmers</p>
    <p>can easily misuse it so that the computed answers consist almost</p>
    <p>entirely of noise. One of the principal problems of numerical ana</p>
    <p>lysis is to determine how accurate the results of certain numerical</p>
    <p>methods will be. There is a credibility-gap : We dont know how</p>
    <p>much of the computers answers to believe. Novice computer users</p>
    <p>solve this problem by implicitly trusting in the computer as an infalli</p>
    <p>ble authority; they tend to believe that all digits of a printed answer</p>
    <p>are significant. Disillusioned computer users have just the opposite</p>
    <p>approach; they are constantly afraid that their answers are almost</p>
    <p>meaningless.</p>
    <p>The Art of Computer Programming, Vol. 2: Seminumerical Algorithms (3rd Ed.),</p>
    <p>Addison-Wesley, 1998, 4.2.2.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing5</p>
  </div>
  <div class="page">
    <p>Count Down</p>
    <p>Pascal C</p>
    <p>const D = 0.1;</p>
    <p>var x: Real;</p>
    <p>begin</p>
    <p>x := 1.0</p>
    <p>;</p>
    <p>while x &gt; 0.0 do</p>
    <p>x := x - D</p>
    <p>;</p>
    <p>writeln ( x:1:2 )</p>
    <p>end.</p>
    <p>#include &lt;stdio.h&gt;</p>
    <p>#define D 0.1</p>
    <p>int main ( void )</p>
    <p>{ double x = 1.0;</p>
    <p>while ( x &gt; 0.0 )</p>
    <p>x = x - D;</p>
    <p>printf ( &quot;%1.2f\n&quot;, x );</p>
    <p>}</p>
    <p>What value does this program print?</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing6</p>
  </div>
  <div class="page">
    <p>Euclidean Paths</p>
    <p>A B C D</p>
    <p>(2, 5, 31) (1, 2, 9) (0, 7, 27) (1, 8, 10)</p>
    <p>Consider the two V-shaped paths via the origin O: AOB and COD.</p>
    <p>Are the lengths of these two paths equal?</p>
    <p>If not, which is bigger?</p>
    <p>Now also tackle the case with</p>
    <p>A B C D</p>
    <p>(4, 12, 28) (2, 6, 14) (1, 1, 23) (1, 13, 19)</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing7</p>
  </div>
  <div class="page">
    <p>Parallel Resistors</p>
    <p>R1</p>
    <p>R2</p>
    <p>Write a program to compute the effective resistance ,</p>
    <p>given the non-negative values R1 and R2 as input.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing8</p>
  </div>
  <div class="page">
    <p>Quadratic Equation</p>
    <p>Consider the equation</p>
    <p>ax2 + bx + c = 0 (1)</p>
    <p>where parameters a, b, and c are given real constants and x is a real</p>
    <p>variable, whose value(s) satisfying (1) must be determined.</p>
    <p>What conditions to impose on the parameters to make this into a</p>
    <p>reasonable programming assignment?</p>
    <p>Solve your assignment.</p>
    <p>How to determine the quality of solver programs?</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing9</p>
  </div>
  <div class="page">
    <p>Floating-Point Numbers</p>
    <p>R = the set of real numbers</p>
    <p>Consider integers   2, t  1, emin  emax</p>
    <p>F(, t, emin, emax) = the set of floating-point numbers x of the form</p>
    <p>x =  f  e</p>
    <p>where fraction f and exponent e satisfy:</p>
    <p>f  t is an integer with f = 0 or 1  |f| &lt; , and</p>
    <p>e is an integer with emin  e  emax</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing10</p>
  </div>
  <div class="page">
    <p>Floating-Point Parameters</p>
    <p>is called the base of F; typically  = 2</p>
    <p>p = t + 1 = the number of bits in the binary representation of f;</p>
    <p>p is called the precision of F</p>
    <p>The smallest F-number larger than 1 is 1 +  with  = t ;  is called the machine epsilon of F.</p>
    <p>The interval from the smallest positive F-number Nmin = emin to the largest one Nmax = (  )emax is called the range of F.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing11</p>
  </div>
  <div class="page">
    <p>IEEE Standard: Normalized Binary Floating-Point Numbers</p>
    <p>Parameter values</p>
    <p>Type  t emin emax  Range</p>
    <p>Single 2 23 126 127 223  1.2  107  1038</p>
    <p>Double 2 52 1022 1023 252  2.2  1016  10308</p>
    <p>Sizes in bits</p>
    <p>Type  f e Total</p>
    <p>Single 1 23 8 32</p>
    <p>Double 1 52 11 64</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing12</p>
  </div>
  <div class="page">
    <p>Floating-Point Operations</p>
    <p>Most operations on R are not closed in F.</p>
    <p>When such operations are simulated on a computer, the result is</p>
    <p>forced into F, yielding an approximation of the exact result.</p>
    <p>This introduces a (small) rounding error into floating-point calcu</p>
    <p>lations. Subsequent operations on inexact results can magnify, or</p>
    <p>reduce, the error in non-intuitive ways.</p>
    <p>The aim of error analysis is to understand the propagation of errors</p>
    <p>in numerical algorithms, in particular to prove bounds on the error in</p>
    <p>the final result.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing13</p>
  </div>
  <div class="page">
    <p>Floating-Point Arithmetic</p>
    <p>Approximation function fl : R  F</p>
    <p>fl(x) is the floating-point number nearest to real number x</p>
    <p>For operation  on R, let  be its implementation on F</p>
    <p>IEEE Standard requires best results:</p>
    <p>x  y = fl(x  y)</p>
    <p>for all   { +, , , / } and x, y  F</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing14</p>
  </div>
  <div class="page">
    <p>Floating-Point Arithmetic: Limitations</p>
    <p>To what extent is F an adequate model of R?</p>
    <p>Which mathematical laws hold when translated from R to F?</p>
    <p>Rn fl n  Fn</p>
    <p>A A R</p>
    <p>fl  F</p>
    <p>For all   { +, , , / } and x, y  R</p>
    <p>fl(x  y) = fl(x)  fl(y)</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing15</p>
  </div>
  <div class="page">
    <p>Floating-Point Arithmetic: Examples</p>
    <p>Consider a machine working with two decimal digits (, t = 10, 1)</p>
    <p>fl(1.06 + 3.06) = fl(4.12) = 4.1</p>
    <p>fl(1.06) + fl(3.06) = 1.1 + 3.1 = 4.2</p>
    <p>How do the following expressions compare:</p>
    <p>Exact evaluation yields:</p>
    <p>Machine approximation yields:</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing16</p>
  </div>
  <div class="page">
    <p>Count Down: Analysis</p>
    <p>D = 0.1 has infinite repeating binary representation:</p>
    <p>(0.0001100110011001100110011001100 . . .)2 =</p>
    <p>k=1</p>
    <p>Cannot be represented exactly as a binary floating-point number</p>
    <p>In the program D = fl(0.1) 6= 0.1</p>
    <p>Double versus Single</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing17</p>
  </div>
  <div class="page">
    <p>Euclidean Paths: Analysis</p>
    <p>Pythagoras Theorem yields:</p>
    <p>AOB =</p>
    <p>778 +</p>
    <p>165  40.73788394062 . . .</p>
    <p>The two lengths coincide on the 12 most significant decimal digits,</p>
    <p>with a difference on the order of 1011.</p>
    <p>For the second pair we find</p>
    <p>AOB =</p>
    <p>531 +</p>
    <p>531  46.086874487211652 . . .</p>
    <p>where the difference is less than 1014.</p>
    <p>Are the lengths really different?</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing18</p>
  </div>
  <div class="page">
    <p>Euclidean Paths: Analysis</p>
    <p>For the second pair, factorization leads to a confirmation :</p>
    <p>For the first pair, three squarings lead to a contradiction :</p>
    <p>( 778  165</p>
    <p>990  86</p>
    <p>) 1332 = 4</p>
    <p>( 778  165  2</p>
    <p>778  165  990  86 + 990  86</p>
    <p>) 8</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing19</p>
  </div>
  <div class="page">
    <p>Parallel Resistors: Analysis</p>
    <p>Replacement resistance R for two parallel resistors R1 and R2:</p>
    <p>R = 1</p>
    <p>+ 1 R2</p>
    <p>= R1  R2</p>
    <p>R1 + R2</p>
    <p>What if R1 = 0 and/or R2 = 0?</p>
    <p>IEEE Standard supports well-behaved infinities :</p>
    <p>However, 0/0 is undefined, yielding a NaN (not-a-number)</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing20</p>
  </div>
  <div class="page">
    <p>Quadratic Equation: Analysis</p>
    <p>The well-knownn a, b, c-formula for solving quadratic equations:</p>
    <p>x1,2 = b</p>
    <p>b2  4ac 2a</p>
    <p>(2)</p>
    <p>Applying it to</p>
    <p>and evaluating it in IEEE single precision, yields</p>
    <p>x1,2 = 0.000000000, 1.000000000  10 8</p>
    <p>Should have been</p>
    <p>x1,2 = 1 .000000000, 1.000000000  10 8</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing21</p>
  </div>
  <div class="page">
    <p>Quadratic Equation: Analysis</p>
    <p>For our positive root, b and +</p>
    <p>b2  4ac have opposite signs and are of almost equal magnitude , because |4ac|  b2.</p>
    <p>When adding them, the (roundoff) error present in the computed</p>
    <p>value for b2  4ac is suddenly magnified enormously in relative size. This phenomena is known as cancellation .</p>
    <p>Cancellation is avoided in the less-known alternative formula:</p>
    <p>x1,2 = 2c</p>
    <p>b</p>
    <p>b2  4ac (4)</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing22</p>
  </div>
  <div class="page">
    <p>Measures for Accuracy</p>
    <p>Suppose the exact value x  R is approximated by x  F.</p>
    <p>The absolute error (in x for x) is defined as</p>
    <p>|x  x|</p>
    <p>The relative error is defined as</p>
    <p>|x  x| |x|</p>
    <p>Scientific and engineering applications often involve scaling, e.g. when</p>
    <p>converting values to other units.</p>
    <p>The relative error is preferred because it is invariant under scaling.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing23</p>
  </div>
  <div class="page">
    <p>Stability of Numerical Algorithm</p>
    <p>A numerical algorithm is called stable , when it produces answers</p>
    <p>whose accuracy is on the order of what can reasonably be expected</p>
    <p>for the problem at hand.</p>
    <p>Challenges in numerical mathematics are</p>
    <p>to determine what can reasonably be expected and</p>
    <p>to construct appropriate stable algorithms.</p>
    <p>For the positive root of (3), the a, b, c-formula (2) is unstable, whereas</p>
    <p>the alternative formula (4) is stable.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing24</p>
  </div>
  <div class="page">
    <p>Quadratic Equation: Further Analysis</p>
    <p>Cancellation is also possible in the subtraction b24ac when b2  4ac.</p>
    <p>In this case it is harder to circumvent, because it is inherent in the</p>
    <p>problem itself and not a consequence of a badly chosen algorithm.</p>
    <p>Determining the roots when they are nearly equal is said to be an</p>
    <p>ill-conditioned problem .</p>
    <p>The squaring b2, the multiplication 4ac, and the final division by 2a</p>
    <p>can produce (intermediate) results that fall outside the representable</p>
    <p>range. This is referred to as underflow or overflow .</p>
    <p>For b2 and 4ac this can happen even if the final results are represen</p>
    <p>table within the range of floating-point numbers.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing25</p>
  </div>
  <div class="page">
    <p>Quadratic Equation: Complications</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing26</p>
  </div>
  <div class="page">
    <p>Error Analysis</p>
    <p>Estimate quantitatively the error in a computation: e.g. give bounds</p>
    <p>Given floating-point numbers A, B, X, compute Y = AX + B.</p>
    <p>What can be said about the error in Y = A  X + B?</p>
    <p>F (A, B, X) = AX + B</p>
    <p>F (A, B, X) = A  X + B = AX(1 + ) + B</p>
    <p>= (AX(1 + ) + B) (1 + )</p>
    <p>with ||, ||  /2 c 2003, 2007, T. Verhoeff Numerical Computing27</p>
  </div>
  <div class="page">
    <p>Forward Error Analysis</p>
    <p>F (A, B, X) = (AX + B)(1 + ) + AX(1 + )</p>
    <p>= F (A, B, X) + (AX + B) + AX(1 + )</p>
    <p>F computes exact value plus a perturbation (forward error):</p>
    <p>(AX + B) + AX(1 + )</p>
    <p>Absolute error  AX( + ) + B: no reasonable bound</p>
    <p>Relative error  AX AX+B  + : no reasonable bound</p>
    <p>Error always small compared to B: false</p>
    <p>Error always small compared to AX: false</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing28</p>
  </div>
  <div class="page">
    <p>Error Analysis Is Not Easy</p>
    <p>Error propagation is a complex process</p>
    <p>Statistical analysis is not applicable if there are just a few steps</p>
    <p>It is not reliable (if there are many steps: law of large numbers),</p>
    <p>because errors need not be independent but can be correlated; in</p>
    <p>that case, statistical analysis is too optimistic</p>
    <p>Interval arithmetic often is (far) too pessimistic ; errors can and often do (partially) cancel each other</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing29</p>
  </div>
  <div class="page">
    <p>Backward Error Analysis</p>
    <p>F (A, B, X) = (AX(1 + ) + B) (1 + )</p>
    <p>= A(1 + )X(1 + ) + B(1 + )</p>
    <p>= F (A(1 + ), B(1 + ), X(1 + ))</p>
    <p>= F (A, B, X)</p>
    <p>where</p>
    <p>A = A(1 + )</p>
    <p>B = B(1 + )</p>
    <p>X = X(1 + )</p>
    <p>F computes exact solution for slightly perturbed input.</p>
    <p>Compare this error to the error already present in A, B, X.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing30</p>
  </div>
  <div class="page">
    <p>Other Areas in Numerical Mathematics</p>
    <p>Two additional sources of error:</p>
    <p>Data Uncertainty: the error already present in the input values</p>
    <p>E.g. by physical measurement</p>
    <p>Truncation Error: the error introduced by an inexact algorithm, which</p>
    <p>is known to produce incorrect answers when run on an ideal ma</p>
    <p>chine, with the purpose of obtaining accurate answers in less time</p>
    <p>E.g. by chopping off an infinite series or approximating a function</p>
    <p>by a polynomial.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing31</p>
  </div>
  <div class="page">
    <p>Recommendation 1</p>
    <p>Avoid floating-point numbers in computing whenever possible.</p>
    <p>To teachers: When designing programming problems, there are plen</p>
    <p>ty of possibilities without floating-point numbers.</p>
    <p>In fact, it is a good attitude to forbid your students to use</p>
    <p>floating-point numbers in their programs, because it is so hard to</p>
    <p>reason about floating-point programs.</p>
    <p>To students: Resist the temptation to use floating-point numbers</p>
    <p>when solving programming problems whose specification does not</p>
    <p>involve them.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing32</p>
  </div>
  <div class="page">
    <p>Recommendation 2</p>
    <p>If you do want to use floating-point numbers, study the literature.</p>
    <p>To teachers: When setting a programming problem involving floatingpoint numbers, the constraints must be expressed carefully and the problem must be solvable for all allowed inputs. Avoid illconditioned problems.</p>
    <p>To students: Before resorting to floating-point numbers, convince yourself that this is really necessary.</p>
    <p>Then, convince yourself that your program satisfies all constraints. In particular, check that you have not fallen into one of the standard traps giving rise to an unstable algorithm.</p>
    <p>In both cases, some form of error analysis is needed. c 2003, 2007, T. Verhoeff Numerical Computing33</p>
  </div>
  <div class="page">
    <p>Quote from Donald E. Knuth (continued)</p>
    <p>Many serious mathematicians have attempted to analyze</p>
    <p>a sequence of floating point operations rigorously, but have</p>
    <p>found the task so formidable that they have tried to be con</p>
    <p>tent with plausibility arguments instead.</p>
    <p>c 2003, 2007, T. Verhoeff Numerical Computing34</p>
  </div>
</Presentation>
