<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mimicking Word Embeddings using Subword RNNs Yuval Pinter, Robert Guthrie, Jacob Eisenstein</p>
    <p>@yuvalpi</p>
    <p>Presented at EMNLP</p>
    <p>September 2017, Copenhagen</p>
  </div>
  <div class="page">
    <p>The Word Embedding Pipeline</p>
    <p>Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Wikipedia</p>
    <p>GigaWord</p>
    <p>Reddit</p>
    <p>... 2</p>
  </div>
  <div class="page">
    <p>Embedding</p>
    <p>model</p>
    <p>(vectors)</p>
    <p>W2V</p>
    <p>GloVe</p>
    <p>Polyglot</p>
    <p>FastText</p>
    <p>...</p>
    <p>The Word Embedding Pipeline</p>
    <p>Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Wikipedia</p>
    <p>GigaWord</p>
    <p>Reddit</p>
    <p>... 2</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpus Supervised</p>
    <p>task corpus</p>
    <p>Penn TreeBank</p>
    <p>SemEval</p>
    <p>OntoNotes</p>
    <p>Univ. Dependencies</p>
    <p>...</p>
    <p>Embedding</p>
    <p>model</p>
    <p>(vectors)</p>
    <p>W2V</p>
    <p>GloVe</p>
    <p>Polyglot</p>
    <p>FastText</p>
    <p>...</p>
    <p>The Word Embedding Pipeline</p>
    <p>Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Wikipedia</p>
    <p>GigaWord</p>
    <p>Reddit</p>
    <p>... 2</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpus Supervised</p>
    <p>task corpus</p>
    <p>Penn TreeBank</p>
    <p>SemEval</p>
    <p>OntoNotes</p>
    <p>Univ. Dependencies</p>
    <p>...</p>
    <p>Embedding</p>
    <p>model</p>
    <p>(vectors)</p>
    <p>W2V</p>
    <p>GloVe</p>
    <p>Polyglot</p>
    <p>FastText</p>
    <p>...</p>
    <p>The Word Embedding Pipeline</p>
    <p>Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Wikipedia</p>
    <p>GigaWord</p>
    <p>Reddit</p>
    <p>...</p>
    <p>Tagging</p>
    <p>Parsing</p>
    <p>Sentiment</p>
    <p>NER</p>
    <p>...</p>
    <p>Task</p>
    <p>model</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Assumed Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
    <p>No pre-trained vectors</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
    <p>No pre-trained vectors</p>
    <p>Affects supervised tasks</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
    <p>No pre-trained vectors</p>
    <p>Affects supervised tasks</p>
    <p>Multiple treatments</p>
    <p>suggested</p>
  </div>
  <div class="page">
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>Actual Pattern</p>
    <p>Supervised</p>
    <p>task text</p>
    <p>No pre-trained vectors</p>
    <p>Affects supervised tasks</p>
    <p>Multiple treatments</p>
    <p>suggested</p>
    <p>Our method - compositional</p>
    <p>subword OOV model 5</p>
  </div>
  <div class="page">
    <p>Sources of OOVs</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names Chalabi has increasingly marginalized within Iraq, ...</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare morphological derivations</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
    <p>Without George Martin the Beatles would have been just another</p>
    <p>untalented band as Oasis.</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare morphological derivations</p>
    <p>Nonce words</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
    <p>Without George Martin the Beatles would have been just another</p>
    <p>untalented band as Oasis.</p>
    <p>What if Google morphed into GoogleOS?</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
    <p>Without George Martin the Beatles would have been just another</p>
    <p>untalented band as Oasis.</p>
    <p>What if Google morphed into GoogleOS?</p>
    <p>Well have four bands, and Big D is cookin. lots of fun and great prizes.</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
    <p>Without George Martin the Beatles would have been just another</p>
    <p>untalented band as Oasis.</p>
    <p>What if Google morphed into GoogleOS?</p>
    <p>Well have four bands, and Big D is cookin. lots of fun and great prizes.</p>
    <p>I dislike this urban society and I want to leave this whole enviroment.</p>
  </div>
  <div class="page">
    <p>Sources of OOVs  Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>Chalabi has increasingly marginalized within Iraq, ...</p>
    <p>Important species (...) include shrimp, (...) and some varieties of flatfish.</p>
    <p>This term was first used in German (Hochrenaissance),</p>
    <p>Without George Martin the Beatles would have been just another</p>
    <p>untalented band as Oasis.</p>
    <p>What if Google morphed into GoogleOS?</p>
    <p>Well have four bands, and Big D is cookin. lots of fun and great prizes.</p>
    <p>I dislike this urban society and I want to leave this whole enviroment.</p>
    <p>???</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>OOV</p>
    <p>???</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>One UNK to rule them all  Average existing embeddings</p>
    <p>Trained with embeddings (stochastic unking)</p>
    <p>OOV</p>
    <p>UNK</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>One UNK to rule them all  Average existing embeddings</p>
    <p>Trained with embeddings (stochastic unking)</p>
    <p>OOV</p>
    <p>UNK</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>One UNK to rule them all  Average existing embeddings</p>
    <p>Trained with embeddings (stochastic unking)</p>
    <p>OOV</p>
    <p>UNK</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>One UNK to rule them all  Average existing embeddings</p>
    <p>Trained with embeddings (stochastic unking)</p>
    <p>Add subword model during WE training  Bhatia et al. (2016), Wieting et al. (2016)</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpus</p>
    <p>Common OOV handling techniques  None (random init)</p>
    <p>One UNK to rule them all  Average existing embeddings</p>
    <p>Trained with embeddings (stochastic unking)</p>
    <p>Add subword model during WE training  Bhatia et al. (2016), Wieting et al. (2016)</p>
    <p>What if we dont have access to the original corpus? (e.g. FastText)</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>Char2Tag</p>
    <p>OOV</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
  </div>
  <div class="page">
    <p>Char2Tag  Add subword layer to supervised task</p>
    <p>Ling et al. (2015), Plank et al. (2016)</p>
    <p>OOV</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
  </div>
  <div class="page">
    <p>Char2Tag  Add subword layer to supervised task</p>
    <p>Ling et al. (2015), Plank et al. (2016)</p>
    <p>OOVs benefit from co-trained character model</p>
    <p>OOV</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
  </div>
  <div class="page">
    <p>Char2Tag  Add subword layer to supervised task</p>
    <p>Ling et al. (2015), Plank et al. (2016)</p>
    <p>OOVs benefit from co-trained character model</p>
    <p>Requires large supervised training set for efficient transfer to test set OOVs</p>
    <p>OOV</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
  </div>
  <div class="page">
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>What data do we have, post-unlabeled corpus?  Vector dictionary</p>
    <p>Orthography (the way words are spelled)</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>What data do we have, post-unlabeled corpus?  Vector dictionary</p>
    <p>Orthography (the way words are spelled)</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>What data do we have, post-unlabeled corpus?  Vector dictionary</p>
    <p>Orthography (the way words are spelled)</p>
    <p>Use the former as training objective, latter as input</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>What data do we have, post-unlabeled corpus?  Vector dictionary</p>
    <p>Orthography (the way words are spelled)</p>
    <p>Use the former as training objective, latter as input</p>
    <p>Pre-trained vectors as target  No need to access original unlabeled corpus</p>
    <p>Many training examples</p>
    <p>(No context) Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>What data do we have, post-unlabeled corpus?  Vector dictionary</p>
    <p>Orthography (the way words are spelled)</p>
    <p>Use the former as training objective, latter as input</p>
    <p>Pre-trained vectors as target  No need to access original unlabeled corpus</p>
    <p>Many training examples</p>
    <p>(No context)</p>
    <p>Subword units as inputs  Very extensible</p>
    <p>(Character inventory changes?)</p>
    <p>Supervised</p>
    <p>task corpusUnlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus Unlabeled</p>
    <p>corpus</p>
    <p>Unlabeled</p>
    <p>corpus</p>
    <p>OOV</p>
    <p>Enter MIMICK</p>
    <p>OOV</p>
  </div>
  <div class="page">
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>make</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>make</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>make</p>
    <p>Forward LSTM</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>make Backward</p>
    <p>LSTM</p>
    <p>Forward LSTM</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>All possible text</p>
    <p>Unlabeled text</p>
    <p>make Backward</p>
    <p>LSTM</p>
    <p>Forward LSTM</p>
    <p>Multilayered Perceptron</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Training</p>
    <p>m eka</p>
    <p>Pre-trained Embedding (Polyglot/FastText/etc.) make</p>
    <p>Loss (L2)</p>
    <p>Mimicked Embedding All possible text</p>
    <p>Unlabeled text</p>
    <p>make Backward</p>
    <p>LSTM</p>
    <p>Forward LSTM</p>
    <p>Multilayered Perceptron</p>
  </div>
  <div class="page">
    <p>Character embeddings</p>
    <p>MIMICK Inference</p>
    <p>b hal</p>
    <p>Mimicked Embedding All possible text</p>
    <p>Unlabeled text blah</p>
    <p>Backward LSTM</p>
    <p>Forward LSTM</p>
    <p>Multilayered Perceptron</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
    <p>Hebrew</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
    <p>Hebrew     (she/you-3p.sg.) will come true (she/you-3p.sg.) will solve</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
    <p>Hebrew     (she/you-3p.sg.) will come true (she/you-3p.sg.) will solve</p>
    <p>geometric (m.pl., nontrad. spelling) geometric (m.pl.)</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
    <p>Hebrew     (she/you-3p.sg.) will come true (she/you-3p.sg.) will solve</p>
    <p>geometric (m.pl., nontrad. spelling) geometric (m.pl.)</p>
    <p>Richardson Eustrach</p>
  </div>
  <div class="page">
    <p>Observation  Nearest Neighbors  English (OOV  Nearest in-vocab words)</p>
    <p>MCT  AWS, OTA, APT, PDM</p>
    <p>pesky  euphoric, disagreeable, horrid, ghastly</p>
    <p>lawnmower  tradesman, bookmaker, postman, hairdresser</p>
    <p>Hebrew     (she/you-3p.sg.) will come true (she/you-3p.sg.) will solve</p>
    <p>geometric (m.pl., nontrad. spelling) geometric (m.pl.)</p>
    <p>Richardson Eustrach</p>
    <p>Surface form  Syntactic properties  Semantics</p>
  </div>
  <div class="page">
    <p>Intrinsic Evaluation  RareWords</p>
  </div>
  <div class="page">
    <p>Intrinsic Evaluation  RareWords</p>
    <p>RareWords similarity task: morphologically-complex, mostly unseen words</p>
  </div>
  <div class="page">
    <p>Intrinsic Evaluation  RareWords</p>
    <p>RareWords similarity task: morphologically-complex, mostly unseen words</p>
  </div>
  <div class="page">
    <p>Intrinsic Evaluation  RareWords</p>
    <p>RareWords similarity task: morphologically-complex, mostly unseen words</p>
    <p>Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare(-ish) morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>... 15</p>
  </div>
  <div class="page">
    <p>Intrinsic Evaluation  RareWords</p>
    <p>RareWords similarity task: morphologically-complex, mostly unseen words</p>
    <p>Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare(-ish) morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>...</p>
    <p>Nearest:</p>
    <p>programmatic</p>
    <p>transformational</p>
    <p>mechanistic</p>
    <p>transactional</p>
    <p>contextual</p>
    <p>NN FUN!!!</p>
  </div>
  <div class="page">
    <p>Extrinsic Evaluation  POS + Attribute Tagging</p>
    <p>Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare(-ish) morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>... 16</p>
    <p>UD is annotated for POS and morphosyntactic attributes  Eng: his stated goals Tense=Past|VerbForm=Part</p>
    <p>Cze: osoby v pokroilm vku Animacy=Inan|Case=Loc|Degree=Pos|Gender=Masc|Negative=Pos|Number=Sing people of advanced age</p>
  </div>
  <div class="page">
    <p>Extrinsic Evaluation  POS + Attribute Tagging</p>
    <p>Names</p>
    <p>Domain-specific jargon</p>
    <p>Foreign words</p>
    <p>Rare(-ish) morphological derivations</p>
    <p>Nonce words</p>
    <p>Nonstandard orthography</p>
    <p>Typos and other errors</p>
    <p>... 16</p>
    <p>UD is annotated for POS and morphosyntactic attributes  Eng: his stated goals Tense=Past|VerbForm=Part</p>
    <p>Cze: osoby v pokroilm vku Animacy=Inan|Case=Loc|Degree=Pos|Gender=Masc|Negative=Pos|Number=Sing people of advanced age</p>
    <p>POS model from Ling et al. (2015)</p>
    <p>VBZ VBGNNDT</p>
    <p>Word embeddings</p>
    <p>the sittingiscat</p>
    <p>Forward LSTM</p>
    <p>Backward LSTM</p>
  </div>
  <div class="page">
    <p>UD is annotated for POS and morphosyntactic attributes  Eng: his stated goals Tense=Past|VerbForm=Part</p>
    <p>Cze: osoby v pokroilm vku Animacy=Inan|Case=Loc|Degree=Pos|Gender=Masc|Negative=Pos|Number=Sing people of advanced age</p>
    <p>POS model from Ling et al. (2015)</p>
    <p>Attributes - same as POS layer</p>
    <p>pres -----</p>
    <p>-- --sing-</p>
    <p>VBZ VBGNNDT</p>
    <p>Word embeddings</p>
    <p>the sittingiscat</p>
    <p>Forward LSTM</p>
    <p>Backward LSTM</p>
    <p>POS</p>
    <p>Number</p>
    <p>Tense</p>
    <p>Extrinsic Evaluation  POS + Attribute Tagging</p>
  </div>
  <div class="page">
    <p>UD is annotated for POS and morphosyntactic attributes  Eng: his stated goals Tense=Past|VerbForm=Part</p>
    <p>Cze: osoby v pokroilm vku Animacy=Inan|Case=Loc|Degree=Pos|Gender=Masc|Negative=Pos|Number=Sing people of advanced age</p>
    <p>POS model from Ling et al. (2015)</p>
    <p>Attributes - same as POS layer</p>
    <p>pres -----</p>
    <p>-- --sing-</p>
    <p>VBZ VBGNNDT</p>
    <p>Word embeddings</p>
    <p>the sittingiscat</p>
    <p>Forward LSTM</p>
    <p>Backward LSTM</p>
    <p>POS</p>
    <p>Number</p>
    <p>Tense</p>
    <p>Negative effect on POS</p>
    <p>Extrinsic Evaluation  POS + Attribute Tagging</p>
  </div>
  <div class="page">
    <p>UD is annotated for POS and morphosyntactic attributes  Eng: his stated goals Tense=Past|VerbForm=Part</p>
    <p>Cze: osoby v pokroilm vku Animacy=Inan|Case=Loc|Degree=Pos|Gender=Masc|Negative=Pos|Number=Sing people of advanced age</p>
    <p>POS model from Ling et al. (2015)</p>
    <p>Attributes - same as POS layer</p>
    <p>pres -----</p>
    <p>-- --sing-</p>
    <p>VBZ VBGNNDT</p>
    <p>Word embeddings</p>
    <p>the sittingiscat</p>
    <p>Forward LSTM</p>
    <p>Backward LSTM</p>
    <p>POS</p>
    <p>Number</p>
    <p>Tense</p>
    <p>Negative effect on POS</p>
    <p>Attribute evaluation metric  Micro F1</p>
    <p>Extrinsic Evaluation  POS + Attribute Tagging</p>
  </div>
  <div class="page">
    <p>Language Selection</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>10 from 8 non-IE branches</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>10 from 8 non-IE branches</p>
    <p>MRLs (e.g. Slavic languages)</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>10 from 8 non-IE branches</p>
    <p>MRLs (e.g. Slavic languages)  Much word-level data</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>10 from 8 non-IE branches</p>
    <p>MRLs (e.g. Slavic languages)  Much word-level data</p>
    <p>Relatively free word order Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection  |UD  Polyglot| = 44, we took 23</p>
    <p>Morphological structure  12 fusional</p>
    <p>3 analytic</p>
    <p>1 isolating</p>
    <p>7 agglutinative</p>
    <p>Geneological diversity  13 Indo-European (7 different branches)</p>
    <p>10 from 8 non-IE branches</p>
    <p>MRLs (e.g. Slavic languages)  Much word-level data</p>
    <p>Relatively free word order</p>
    <p>Institutional</p>
    <p>Entrepreneurial</p>
    <p>Linguistic</p>
    <p>Anatomical</p>
    <p>Ideological</p>
    <p>Minna Sundberg</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
    <p>Attribute-carrying tokens</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
    <p>Attribute-carrying tokens  Range from 0% (Vietnamese) to 92.4% (Hindi)</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
    <p>Attribute-carrying tokens  Range from 0% (Vietnamese) to 92.4% (Hindi)</p>
    <p>OOV rate (UD against Polyglot vocabulary)</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
    <p>Attribute-carrying tokens  Range from 0% (Vietnamese) to 92.4% (Hindi)</p>
    <p>OOV rate (UD against Polyglot vocabulary)  16.9%-70.8% type-level (median 29.1%)</p>
  </div>
  <div class="page">
    <p>Language Selection (contd.)  Script type</p>
    <p>7 in non-alphabetic scripts</p>
    <p>Ideographic (Chinese) - ~12K characters</p>
    <p>Hebrew, Arabic - no casing, no vowels, syntactic fusion</p>
    <p>Vietnamese - tokens are non-compositional syllables</p>
    <p>Attribute-carrying tokens  Range from 0% (Vietnamese) to 92.4% (Hindi)</p>
    <p>OOV rate (UD against Polyglot vocabulary)  16.9%-70.8% type-level (median 29.1%)</p>
    <p>2.2%-33.1% token-level (median 9.2%)</p>
  </div>
  <div class="page">
    <p>Evaluated Systems  NONE: Polyglots default UNK embedding</p>
    <p>the sittingisflatfish</p>
  </div>
  <div class="page">
    <p>Evaluated Systems  NONE: Polyglots default UNK embedding</p>
    <p>MIMICK</p>
    <p>the sittingisflatfish</p>
  </div>
  <div class="page">
    <p>Evaluated Systems  NONE: Polyglots default UNK embedding</p>
    <p>MIMICK</p>
    <p>CHAR2TAG - additional RNN layer  3x Training time</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>the sittingisflatfish</p>
  </div>
  <div class="page">
    <p>Evaluated Systems  NONE: Polyglots default UNK embedding</p>
    <p>MIMICK</p>
    <p>CHAR2TAG - additional RNN layer  3x Training time</p>
    <p>BOTH: MIMICK + CHAR2TAG</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>the sittingisflatfish</p>
  </div>
  <div class="page">
    <p>Evaluated Systems  NONE: Polyglots default UNK embedding</p>
    <p>MIMICK</p>
    <p>CHAR2TAG - additional RNN layer  3x Training time</p>
    <p>BOTH: MIMICK + CHAR2TAG</p>
    <p>POINT</p>
    <p>UNION</p>
    <p>ROAD</p>
    <p>LIGHT</p>
    <p>LONG</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>Char</p>
    <p>LSTM</p>
    <p>the sittingisflatfish</p>
  </div>
  <div class="page">
    <p>Results - Full Data</p>
    <p>POS tags (accuracy) Morpho. Attributes (micro F1) 21</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
  </div>
  <div class="page">
    <p>Results - 5,000 training tokens</p>
    <p>POS tags (accuracy) Morpho. Attributes (micro F1) 22</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
  </div>
  <div class="page">
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>Results - Language Types (5,000 tokens)</p>
    <p>Slavic languages POS 23</p>
  </div>
  <div class="page">
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>Results - Language Types (5,000 tokens)</p>
    <p>Slavic languages POS Agglutinative languages morpho. attribute F1 23</p>
  </div>
  <div class="page">
    <p>Results - Chinese</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
    <p>N O</p>
    <p>N E</p>
    <p>M IM</p>
    <p>IC K</p>
    <p>C H</p>
    <p>A R</p>
    <p>A G</p>
    <p>B O</p>
    <p>T H</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE! Code &amp; models: https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code compatible with w2v, Polyglot, FastText</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code compatible with w2v, Polyglot, FastText</p>
    <p>Models for Polyglot also on github</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code compatible with w2v, Polyglot, FastText</p>
    <p>Models for Polyglot also on github  &lt;1MB each, dynet format</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code compatible with w2v, Polyglot, FastText</p>
    <p>Models for Polyglot also on github  &lt;1MB each, dynet format</p>
    <p>Learn all OOVs in advance and add to param table, or</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>A Word (Model) from our Sponsor  Our extrinsic results are on tagging</p>
    <p>Please consider us for all your WE use cases!  Sentiment!</p>
    <p>Parsing!</p>
    <p>IE!</p>
    <p>QA!</p>
    <p>...</p>
    <p>Code compatible with w2v, Polyglot, FastText</p>
    <p>Models for Polyglot also on github  &lt;1MB each, dynet format</p>
    <p>Learn all OOVs in advance and add to param table, or</p>
    <p>Load into memory and infer on-line</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
    <p>Sore spots and Future Work</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
    <p>Sore spots and Future Work  Vietnamese - syllabic vocabulary</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
    <p>Sore spots and Future Work  Vietnamese - syllabic vocabulary</p>
    <p>Hebrew and Arabic - nontrivial tokenization, no case</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
    <p>Sore spots and Future Work  Vietnamese - syllabic vocabulary</p>
    <p>Hebrew and Arabic - nontrivial tokenization, no case</p>
    <p>Try other subword levels (morphemes, phonemes, bytes)</p>
  </div>
  <div class="page">
    <p>Conclusions  MIMICK: an OOV-extension embedding processing step for downstream tasks</p>
    <p>Compositional model complementing distributional artifact</p>
    <p>Powerful technique for low-resource scenarios</p>
    <p>Especially good for:  Morphologically-rich languages</p>
    <p>Large character vocabulary</p>
    <p>Sore spots and Future Work  Vietnamese - syllabic vocabulary</p>
    <p>Hebrew and Arabic - nontrivial tokenization, no case</p>
    <p>Try other subword levels (morphemes, phonemes, bytes)</p>
    <p>Improve morphosyntactic attribute tagging scheme</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Code &amp; models:</p>
    <p>https://github.com/yuvalpinter/Mimick</p>
    <p>Neglect</p>
    <p>Satisfaction</p>
    <p>Illness</p>
    <p>Espionage</p>
    <p>Bullying</p>
  </div>
</Presentation>
