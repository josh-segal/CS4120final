<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FARMER: Finding Interesting Rule Groups in Microarray Datasets</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background  Motivations  Algorithms  Performance Studies  Conclusions</p>
  </div>
  <div class="page">
    <p>Task  Mining association rules from dataset.</p>
    <p>a) Easy to understand b) Useful for classification</p>
    <p>What kind of dataset we deal with..</p>
    <p>a) Dataset D consists of a set of rows R={r1,r2..rn}</p>
    <p>b) There exists a set of items I={a,b,c,d}</p>
    <p>c) Each row ri consists of a sub set of I,</p>
    <p>d) There exists a set of class label C={c1,c2..ck}</p>
    <p>e) Each row ri contains a class label from C</p>
    <p>Iri</p>
  </div>
  <div class="page">
    <p>Dataset  In the microarray (biology) dataset:</p>
    <p>Each row in the dataset corresponds to a sample</p>
    <p>Each item value in the dataset corresponds to a distretized gene expression value.</p>
    <p>Class labels correspond to category of sample, (cancer / not cancer)</p>
  </div>
  <div class="page">
    <p>Association Rule  Association rule takes the form of</p>
    <p>LHS -&gt; Ci LHS is a set of items. Ci is a class label.</p>
    <p>Support of A: sup(A), the number of rows in dataset containing A.</p>
    <p>Support of rule r:LHS-&gt;Ci, sup(LHS Ci).  Confidence of rule r:LHS-&gt;Ci, sup(r)/sup(LHS)</p>
  </div>
  <div class="page">
    <p>Example  Rule r: {a,e,h} -&gt; C</p>
    <p>Support(r)=2</p>
    <p>Condifence(r)=66%</p>
  </div>
  <div class="page">
    <p>General solution</p>
    <p>Step1: Find all frequently occurred itemsets from dataset D.</p>
    <p>Step2: Generate rule in the form of itemset -&gt; C. Prune rules that do not have enough support an confidence.</p>
  </div>
  <div class="page">
    <p>Previous Algorithms</p>
    <p>Item enumeration: Search all the frequent itemsets by checking all possible combinations of items.</p>
    <p>{ }</p>
    <p>{a } {b } {c }</p>
    <p>{ab } {ac } {bc }</p>
    <p>{abc }</p>
    <p>We can simulate the search process in an item enumeration tree.</p>
  </div>
  <div class="page">
    <p>Previous Algorithms  Rule generation:</p>
    <p>e.g., we get frequent itemset {a,e,h}</p>
    <p>{a,e,h}-&gt;C, sup=2, confidence=66% {a,e,h}-&gt;-C, sup=1, confidence=33%</p>
  </div>
  <div class="page">
    <p>Microarray data</p>
    <p>Features of Microarray data  A few rows: 100-1000  A large number of items, 10000</p>
    <p>The space of all the combinations of items is large 210000.</p>
  </div>
  <div class="page">
    <p>Motivations--Challenges  Very slow for existing rule mining</p>
    <p>algorithms  Item search space is exponential to the</p>
    <p>number of item  use the idea of row enumeration to</p>
    <p>design new algorithm  The number of association rules are too</p>
    <p>huge even for a given consequent  mine Interesting rule groups</p>
  </div>
  <div class="page">
    <p>Definition  Row support set: Given a set of items</p>
    <p>I, we denote R(I) as the largest set of rows that contain I.</p>
    <p>Item support set: Given a set of rows R, we denote I(R) as the largest set of items that are common among rows in R.</p>
  </div>
  <div class="page">
    <p>Example  I={a,e,h}, then</p>
    <p>R(I)={r2,r3,r4}</p>
    <p>R={r2,r3}, then I(R)={a,e,h}</p>
  </div>
  <div class="page">
    <p>FARMER: Rule Group  What is rule group?</p>
    <p>Given a one row dataset: {a, b, c, d, e, Cancer}, 31 rules in the form of LHS  Cancer.</p>
    <p>the same row and the same confidence (100%).  1 upper bound and 5 lower bound</p>
    <p>Rule group: a set of association rules whose LHS itemsets occurs in a same set of rows.</p>
    <p>Rule group has a unique upper bound. abcde-&gt;Cancer</p>
  </div>
  <div class="page">
    <p>FARMER: Interesting Rule Group</p>
    <p>Consider two rules: abcd  Cancer (confidence 90%) ab  Cancer (confidence 95%).</p>
    <p>ab is a better indicator of Cancer than abcd  ab  Cancer has a higher confidence and  all rows covering abcd  Cancer must cover ab  Cancer</p>
    <p>IRG: Rule Group R is IRG if there exists no other IRG R whose upper bound rule ru is a subset of Rs upper bound ru and confidence(ru)&gt;confidence(ru)</p>
  </div>
  <div class="page">
    <p>IRG  Each IRG corresponds to a unique row set R.  Each IRG has a unique upper bound rule ru.</p>
    <p>Given a row set R, we can get a unique IRG whose upper bound rule is I(R) -&gt; C and whose corresponding row set is R(I(R))</p>
  </div>
  <div class="page">
    <p>Example  Given row set R={r2,r3}  I(R)={a,e,h}  IRG:</p>
    <p>ru=: {a,e,h}-&gt;C row set= R(I(R))=R({a,e,h})={r2,r3,r4}</p>
    <p>Support(ru)=2 Confidence(ru)=66%</p>
    <p>{a,e}-&gt;C, {a,h}-&gt;C</p>
  </div>
  <div class="page">
    <p>FARMER: Row Enumeration</p>
    <p>Search in the space of all combinations of rows. Smaller size 21000. (&lt;&lt;210000)</p>
    <p>Since each IRG -&gt; a unique rowset each rowset -&gt; a unique IRG</p>
    <p>We can get all IRG by row enumeration</p>
  </div>
  <div class="page">
    <p>Row Enumeration Tree</p>
  </div>
  <div class="page">
    <p>General Pruning method</p>
    <p>Estimate upper bound for pruning  Minimum support  Minimum confidence  Minimum chi square</p>
  </div>
  <div class="page">
    <p>IRG and Association Rules  From IRG, we can get the entire set of</p>
    <p>association rules found by any other item enumeration rule mining algorithms.</p>
    <p>Any association rule found in those algorithm belongs to one of the IRG we found.</p>
  </div>
  <div class="page">
    <p>IRG and Association Rules  Given an IRG with upper bound ru</p>
    <p>and lower bound set RL. Any rule r with belongs to IRG and has same support and confidence.</p>
    <p>IRG1: ru=: {a,e,h}-&gt;C RL={{e}-&gt;C, {h}-&gt;C,} {a,e}-&gt;C, {a,h}-&gt;Cbelongs to IRG1</p>
    <p>llul Rrrrr  ,'</p>
  </div>
  <div class="page">
    <p>Lower Bound  Lower Bound can be generated from</p>
    <p>upper bound.</p>
    <p>Find smallest subset of upper bound that occurs only in the row set of the IRG.</p>
    <p>Use incremental generate method</p>
  </div>
  <div class="page">
    <p>Experimental studies  Efficiency of FARMER</p>
    <p>On five real-life dataset  Varying the minimum support  Varying the minimum confidence  Varying the minimum chi-square</p>
    <p>Benchmark  CHARM  ColumnE</p>
    <p>Usefulness of IRGs  Classification</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>Clinical dataset: Prostate Cancer  136 rows, 12600 items  Class1 tumor, class2 normal</p>
  </div>
  <div class="page">
    <p>Example results--Prostate</p>
    <p>minimum suppo rt</p>
    <p>FA RM ER</p>
    <p>Co lumnE</p>
    <p>CHA RM</p>
  </div>
  <div class="page">
    <p>Example results--Prostate</p>
    <p>m inim um c o nfide nc e (%)</p>
    <p>FA RM ER:minsup=1:minchi=10</p>
    <p>FA RM ER:minsup =1</p>
  </div>
  <div class="page">
    <p>Classification results</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Proposed a novel algorithm to discover interesting rule group for given consequent.</p>
    <p>Much more efficient than previous item enumeration algorithms when handling microarray dataset.</p>
  </div>
  <div class="page">
    <p>Thank You</p>
  </div>
  <div class="page">
    <p>Prune Method 1  Removing items that appear</p>
    <p>in all tuples of transposed DB will not affect results</p>
    <p>r2 r3 {aeh}</p>
    <p>r4 has 100% support in the projected table of r2r3, therefore branch r2 r3r4 will be pruned.</p>
    <p>r2 r3 r4 {aeh}</p>
  </div>
  <div class="page">
    <p>Pruning method 2</p>
    <p>At a node, if an upper bound rule is detected to be discovered before, we can prune enumeration below this node  Because all upper bounds below this node</p>
    <p>has been discovered before  For example, at node 34, if we found that</p>
  </div>
  <div class="page">
    <p>Pruning method 3-Minimum Support</p>
    <p>Given X= {x1, x2, , xn} and TT|X (xn positive class)  Loose upper bound:</p>
    <p>obtained before scanning transposed table  |X|+LU, LU--the number of possible extensions of X in</p>
    <p>TT|X  Tight upper bound:</p>
    <p>obtained after scanning transposed table  |X|+TU, TU--the maximal number of possible extension in</p>
    <p>all tuples of TT|X</p>
  </div>
  <div class="page">
    <p>Pruning method 3-Minimum Confidence</p>
    <p>Given X= {x1, x2, , xn} and TT|X rule r  Conf(r) = a/(a+b)</p>
    <p>a  the occurrences of rule r in positive  b  the occurrences of rule r in negative  Is maximized at largest possible a and smallest b</p>
    <p>Loose upper bound:  The maximal possible a before scanning  b at the node X</p>
    <p>Tight upper bound  The maximal possible a after scanning  b at the node X</p>
  </div>
  <div class="page">
    <p>Pruning method 3 Minimum chi-square</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Figure 1: Example Table</p>
  </div>
  <div class="page">
    <p>Lower Bounds</p>
    <p>Algorithm of discovering lower bounds for an upper bound  Incremental method to update  Example: An upper bound rule with antecedent</p>
    <p>A=abcde and two rows (r1 : abcf ) and (r2 : cdeg)</p>
    <p>Initialize lower bounds {a, b, c, d, e}  add abc--- new lower {d ,e}  Add cde--- new lower {ad, bd, ae, be}</p>
  </div>
</Presentation>
