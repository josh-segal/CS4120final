<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Worst-Case Comparison Between Temporal Difference and Residual Gradient</p>
    <p>with Linear Function Approximation</p>
    <p>Lihong Li Rutgers Laboratory for Real-Life Reinforcement Learning (RL3)</p>
    <p>Presented at ICML 2008 Helsinki, Finland</p>
    <p>July 2008</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 2</p>
    <p>Outline</p>
    <p>The sequential online learning model  Related work  Theoretical results  Empirical evidence  Conclusions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 3</p>
    <p>Policy Evaluation  The problem: to predict long-term effect given short-term</p>
    <p>(immediate) feedback  Critical in (approximate) policy iteration</p>
    <p>Classic policy iteration (Howard, 60)  -policy iteration (Bertsekas &amp; Ioffe, 96)  Least-squares policy iteration (Lagoudakis &amp; Parr, 03)</p>
    <p>Curse of dimensionality (Bellman, 57)  Approximation is necessary  Linear function approximation</p>
    <p>Incremental algorithms  Temporal difference (Sutton, 88)  Residual gradient (Baird, 95)</p>
    <p>online fast</p>
    <p>incremental</p>
    <p>policy evaluation</p>
    <p>policy improvement</p>
    <p>API</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 4</p>
    <p>Sequential Online Learning Model: Notation</p>
    <p>L+++= ++ 2 2</p>
    <p>x1 x2 x3 xt r1 r2 rt xt+1</p>
    <p>ttty xw :Prediction =</p>
    <p>ttt yye  :error Prediction =</p>
    <p>tttttt xwxwrd += +1 :difference Temporal</p>
    <p>For an algorithm w/ linear func. approx.</p>
    <p>[Imagine xi = (si) in RL]</p>
    <p>== t</p>
    <p>tTD t</p>
    <p>tP de 22 and :losses Total ll</p>
  </div>
  <div class="page">
    <p>Learning Protocol LEARNER ENVIRONMENTDecide initial weight w1</p>
    <p>Announce input x1</p>
    <p>Predict 1 = w1  x1</p>
    <p>Announce reward r1 and x2</p>
    <p>Adjust weight to w2 ( w1 = w2  w1) Predict 2 = w2  x2</p>
    <p>Announce reward r2 and x3</p>
    <p>Adjust weight to w3 ( w2 = w3  w2)</p>
    <p>tim e</p>
    <p>No statistical assumption</p>
    <p>Unbounded computation power</p>
    <p>Can be Adversarial</p>
    <p>Error: e1 = y1 - 1</p>
    <p>TD: d1 = r1 + w1x2  w1x1</p>
    <p>Error: e2 = y2  2</p>
    <p>TD: d2 = r2 + w2x3  w2x2</p>
    <p>Predict 3 = w3  x3</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 6</p>
    <p>Sequential Online Learning Model: Notation</p>
    <p>== t</p>
    <p>tTD t</p>
    <p>tP de 22 and :losses Total ll</p>
    <p>L+++= ++ 2 2</p>
    <p>x1 x2 x3 xt r1 r2 rt xt+1</p>
    <p>ttty xw :Prediction =</p>
    <p>ttt yye  :error Prediction =</p>
    <p>tttttt xwxwrd += +1 :difference Temporal   = t</p>
    <p>ttP y 2u )xu(l</p>
    <p>+= + t</p>
    <p>tttTD r 2</p>
    <p>u for and boundupper to:Goal uu any TDTDPP llll</p>
    <p>For an algorithm w/ linear func. approx. compared to the best (in hindsight) weight u</p>
    <p>[Imagine xi = (si) in RL]</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 7</p>
    <p>Why the Two Metrics?</p>
    <p>Prediction error (et) :  Measures prediction accuracy  Directly affects policy quality in approximate</p>
    <p>policy iteration (Bertsekas &amp; Tsitsiklis, 96)</p>
    <p>Temporal difference (dt):  Measures prediction consistency  Related to Bellman error  More robust in policy improvement (Munos, 03)</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 8</p>
    <p>Three Online-Learning Algorithms</p>
    <p>TD(0) (Sutton, 88)</p>
    <p>TD*(0) (Schapire &amp; Warmuth, 96)</p>
    <p>RG (Baird, 95)</p>
    <p>tt tt</p>
    <p>tt d xx x1 ww</p>
    <p>+ +</p>
    <p>+</p>
    <p>)xx(ww 11 ++ + ttttt d</p>
    <p>tttt d xww 1 ++</p>
    <p>tttttt xwxwrd += +1 :difference Temporal  1 :equation&quot;Bellman &quot; ++= ttt yry</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 9</p>
    <p>Outline</p>
    <p>The sequential online learning model  Related work  Theoretical results  Empirical evidence  Conclusions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 10</p>
    <p>Related Work</p>
    <p>Sequential online learning model  First proposed by Schapire &amp; Warmuth (96)  They proposed TD*() and analyzed</p>
    <p>Online learning framework  Equivalently,  E.g., (Cesa-Bianchi, Long &amp; Warmuth, 96)  They analyzed for gradient descent</p>
    <p>Pl</p>
    <p>Pl</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 11</p>
    <p>Related Work</p>
    <p>TD(0) converges faster than RG in a small problem (Baird, 95)</p>
    <p>TD(0) converges provably faster (Schoknecht &amp; Merke, 03) when  Updates are synchronous  No function approximation is used  A certain matrix has real eigenvalues only</p>
    <p>In contrast, our analysis has no such restrictions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 12</p>
    <p>Outline</p>
    <p>The sequential online learning model  Related work  Theoretical results  Empirical evidence  Conclusions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 13</p>
    <p>Proof Ideas  Use ||wt - u||2 as the potential function to</p>
    <p>measure progress of learning in each step</p>
    <p>Lower bound ||wt  u||2 - ||wt+1  u||2 in terms of , , dt, et, and u</p>
    <p>Upper bound dt and et in terms of , , and u</p>
    <p>Upper bound t dt2 and t et2 by algebraic manipulations</p>
    <p>Optimize  to minimize these upper bounds</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 14</p>
    <p>TD*(0) and TD(0) yield smaller prediction errors  RG yields smaller temporal differences  The gaps become larger as  1</p>
    <p>Regret Analysis Summary</p>
    <p>(Schapire &amp; Warmuth, 96)</p>
    <p>(Cesa-Bianchi, Long, &amp; Warmuth, 96)</p>
    <p>discount</p>
    <p>Prediction Error Bound</p>
    <p>TD*(0) RG</p>
    <p>discount</p>
    <p>Temporal Difference Bound</p>
    <p>TD*(0) RG</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 15</p>
    <p>Regret Analysis Summary (contd)</p>
    <p>Results are valid for non-Markovian problems  Partially observable environments</p>
    <p>Often the case when function approximation is used</p>
    <p>Multi-agent systems  Time-varying situations</p>
    <p>But  these are upper bounds  Open question: how tight are they?</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 16</p>
    <p>Outline</p>
    <p>The sequential online learning model  Related work  Theoretical results  Empirical evidence  Conclusions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 17</p>
    <p>Ring Domain  Random features  k=5  E[yt] = 0   was optimized  500 runs  500 steps per run</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 18</p>
    <p>PuddleWorld Domain  RBF features  k = 16  Stochastic policy (N/E)   was optimized  500 runs  50 episodes per run</p>
    <p>From (Boyan &amp; Moore 95)</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 19</p>
    <p>Outline</p>
    <p>The sequential online learning model  Related work  Theoretical results  Empirical evidence  Conclusions</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 20</p>
    <p>Conclusions  We compared TD(0), TD*(0), and RG</p>
    <p>in sequential online learning model  when linear function approximation is used</p>
    <p>Theoretical implications  TD predictions are more accurate  RG predictions are more consistent  Supported by empirical evidence</p>
    <p>Online learning techniques can be useful in RL analysis</p>
  </div>
  <div class="page">
    <p>Lihong Li ICML, Helsinki, Finland, July 2008 21</p>
    <p>Extend analysis of temporal differences to TD() and TD*()</p>
    <p>Find non-trivial lower bounds  Ideally, matching lower bounds</p>
    <p>Analyze multiplicative forms of TD  E.g., (Kivinen &amp; Warmuth, 97; Precup &amp; Sutton, 97)  Could be useful when many features are irrelevant</p>
    <p>For instance, CMAC features</p>
    <p>Future Work</p>
  </div>
</Presentation>
