<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Enabling Flow-level Latency</p>
    <p>Measurements across Routers in Data</p>
    <p>Centers</p>
    <p>Parmjeet Singh, Myungjin Lee</p>
    <p>Sagar Kumar, Ramana Rao Kompella</p>
  </div>
  <div class="page">
    <p>Latency-critical applications in data centers</p>
    <p>Guaranteeing low end-to-end latency is important</p>
    <p>Web search (e.g., Googles instant search service)</p>
    <p>Retail advertising</p>
    <p>Recommendation systems</p>
    <p>High-frequency trading in financial data centers</p>
    <p>Operators want to troubleshoot latency anomalies</p>
    <p>End-host latencies can be monitored locally</p>
    <p>Detection, diagnosis and localization through a network: no</p>
    <p>native support of latency measurements in a router/switch</p>
  </div>
  <div class="page">
    <p>Prior solutions</p>
    <p>Lossy Difference Aggregator (LDA)</p>
    <p>Kompella et al. [SIGCOMM 09]</p>
    <p>Aggregate latency statistics</p>
    <p>Reference Latency Interpolation (RLI)</p>
    <p>Lee et al. [SIGCOMM 10]</p>
    <p>Per-flow latency measurements</p>
    <p>More suitable due to more fine-grained measurements</p>
  </div>
  <div class="page">
    <p>Deployment scenario of RLI</p>
    <p>Upgrading all switches/routers in a data center network</p>
    <p>Pros</p>
    <p>Provide finest granularity of latency anomaly localization</p>
    <p>Cons</p>
    <p>Significant deployment cost</p>
    <p>Possible downtime of entire production data centers</p>
    <p>In this work, we are considering partial deployment of RLI</p>
    <p>Our approach: RLI across Routers (RLIR)</p>
  </div>
  <div class="page">
    <p>Overview of RLI architecture</p>
    <p>Goal</p>
    <p>Latency statistics on a per-flow basis between interfaces</p>
    <p>Problem setting</p>
    <p>No storing timestamp for each packet at ingress and egress due to high storage and communication cost</p>
    <p>Regular packets do not carry timestamps</p>
    <p>Router</p>
    <p>Ingress I</p>
    <p>Egress E</p>
  </div>
  <div class="page">
    <p>Overview of RLI architecture</p>
    <p>Premise of RLI: delay locality</p>
    <p>Approach</p>
    <p>Latency</p>
    <p>Estimator</p>
    <p>Reference</p>
    <p>Packet</p>
    <p>Injector</p>
    <p>Ingress I Egress E 1 2 2 1</p>
    <p>R</p>
    <p>L</p>
    <p>D e la</p>
    <p>y</p>
    <p>Time</p>
    <p>R</p>
    <p>L</p>
    <p>Linear interpolation</p>
    <p>line</p>
    <p>delay</p>
  </div>
  <div class="page">
    <p>Full vs. Partial deployment</p>
    <p>Full deployment: 16 RLI sender-receiver pairs</p>
    <p>Partial deployment: 4 RLI senders + 2 RLI receivers</p>
    <p>81.25 % deployment cost reduction</p>
    <p>Switch 1 Switch 5</p>
    <p>Switch 2 Switch 4</p>
    <p>Switch 3</p>
    <p>Switch 6</p>
    <p>RLI Sender (Reference Packet Injector) RLI Receiver (Latency Estimator)</p>
  </div>
  <div class="page">
    <p>Case 1: Presence of cross traffic</p>
    <p>Issue: Inaccurate link utilization estimation at the sender</p>
    <p>leads to high reference packet injection rate</p>
    <p>Approach</p>
    <p>Not actively addressing the issue</p>
    <p>Evaluation shows no much impact on packet loss rate increase</p>
    <p>Details in the paper</p>
    <p>Switch 1 Switch 5</p>
    <p>Switch 2 Switch 4</p>
    <p>Switch 3</p>
    <p>Switch 6</p>
    <p>RLI Sender (Reference Packet Injector) RLI Receiver (Latency Estimator)</p>
    <p>Link utilization</p>
    <p>estimation on Switch 1 Bottleneck</p>
    <p>Link Cross</p>
    <p>Traffic</p>
  </div>
  <div class="page">
    <p>Case 2: RLI Sender side</p>
    <p>Issue: Traffic may take different routes at an intermediate</p>
    <p>switch</p>
    <p>Approach: Sender sends reference packets to all receivers</p>
    <p>Switch 1 Switch 5</p>
    <p>Switch 2 Switch 4</p>
    <p>Switch 3</p>
    <p>Switch 6</p>
    <p>RLI Sender (Reference Packet Injector) RLI Receiver (Latency Estimator)</p>
  </div>
  <div class="page">
    <p>Case 3: RLI Receiver side</p>
    <p>Issue: Hard to associate reference packets and regular packets that traversed the same path</p>
    <p>Approaches</p>
    <p>Packet marking: requires native support from routers</p>
    <p>Reverse ECMP computation: reverse engineer intermediate routes using ECMP hash function</p>
    <p>IP prefix matching at limited situation</p>
    <p>Switch 1 Switch 5</p>
    <p>Switch 2 Switch 4</p>
    <p>Switch 3</p>
    <p>Switch 6</p>
    <p>RLI Sender (Reference Packet Injector) RLI Receiver (Latency Estimator)</p>
  </div>
  <div class="page">
    <p>Deployment example in fat-tree topology</p>
    <p>RLI Sender (Reference Packet Injector) RLI Receiver (Latency Estimator)</p>
    <p>IP prefix matching Reverse ECMP computation /</p>
    <p>IP prefix matching</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Simulation setup</p>
    <p>Trace: regular traffic (22.4M pkts) + cross traffic (70M pkts)</p>
    <p>Simulator</p>
    <p>Results</p>
    <p>Accuracy of per-flow latency estimates</p>
    <p>Traffic</p>
    <p>Divider Switch1 Switch2</p>
    <p>Cross Traffic</p>
    <p>Injector</p>
    <p>Packet</p>
    <p>Trace</p>
    <p>RLI</p>
    <p>Receiver</p>
    <p>RLI</p>
    <p>Sender</p>
    <p>Cross</p>
    <p>Traffic</p>
    <p>Regular</p>
    <p>Traffic Reference</p>
    <p>packets</p>
    <p>injection rate</p>
  </div>
  <div class="page">
    <p>Accuracy of per-flow latency estimates</p>
    <p>Bottleneck link utilization: 93%</p>
    <p>Relative error</p>
    <p>C D</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Low latency applications in data centers</p>
    <p>Localization of latency anomaly is important</p>
    <p>RLI provides flow-level latency statistics, but full</p>
    <p>deployment (i.e., all routers/switches) cost is expensive</p>
    <p>Proposed a solution enabling partial deployment of RLI</p>
    <p>No too much loss in localization granularity (i.e., every other</p>
    <p>router)</p>
  </div>
  <div class="page">
    <p>Thank you! Questions?</p>
  </div>
</Presentation>
