<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Efficient classification for metric data</p>
    <p>Lee-Ad Gottlieb Hebrew U.</p>
    <p>Aryeh Kontorovich Ben Gurion U.</p>
    <p>Robert Krauthgamer Weizmann Institute</p>
  </div>
  <div class="page">
    <p>Classification problem  A fundamental problem in learning:</p>
    <p>Point space X</p>
    <p>Probability distribution P on X x {-1,1}  Learner observes sample S of n points (x,y) drawn iid ~P</p>
    <p>Wants to predict labels of other points in X</p>
    <p>Produces hypothesis h: X  {-1,1} with</p>
    <p>empirical error</p>
    <p>and true error</p>
    <p>Goal: uniformly over h in probability</p>
    <p>Efficient classification for metric data 2</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Classification problem  A fundamental problem in learning:</p>
    <p>Point space X</p>
    <p>Probability distribution P on X x {-1,1}  Learner observes sample S of n points (x,y) drawn iid ~P</p>
    <p>Wants to predict labels of other points in X</p>
    <p>Produces hypothesis h: X  {-1,1} with</p>
    <p>empirical error</p>
    <p>and true error</p>
    <p>Goal: uniformly over h in probability</p>
    <p>Efficient classification for metric data 3</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Classification problem  A fundamental problem in learning:</p>
    <p>Point space X</p>
    <p>Probability distribution P on X x {-1,1}  Learner observes sample S of n points (x,y) drawn iid ~P</p>
    <p>Wants to predict labels of other points in X</p>
    <p>Produces hypothesis h: X  {-1,1} with</p>
    <p>empirical error</p>
    <p>and true error</p>
    <p>Goal: uniformly over h in probability</p>
    <p>Efficient classification for metric data 4</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Generalization bounds  How do we upper bound the true error?</p>
    <p>Use a generalization bound. Roughly speaking (and whp)</p>
    <p>true error  empirical error + (complexity of h)/n</p>
    <p>More complex classifier  easier to fit to arbitrary data  VC-dimension: largest point set</p>
    <p>that can be shattered by h</p>
    <p>-1 +1</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Popular approach for classification Assume the points are in Euclidean space!  Pros</p>
    <p>Existence of inner product  Efficient algorithms (SVM)  Good generalization bounds (max margin)</p>
    <p>Cons  Many natural settings non-Euclidean  Euclidean structure is a strong assumption</p>
    <p>Recent popular focus  Metric space data</p>
    <p>Efficient classification for metric data 6</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 7</p>
    <p>Metric space  (X,d) is a metric space if</p>
    <p>X = set of points  d() = distance function</p>
    <p>nonnegative  symmetric  triangle inequality</p>
    <p>inner product  norm  norm  metric  But  doesnt hold</p>
    <p>208km</p>
  </div>
  <div class="page">
    <p>Classification for metric data? Advantage: often much more natural</p>
    <p>much weaker assumption  strings  Images (earthmover distance)</p>
    <p>Problem: no vector representation  No notion of dot-product (and no kernel)  What to do?</p>
    <p>Invent kernel (e.g. embed into Euclidean space)?.. Possible high distortion!  Use some NN heuristic?.. NN classifier has  VC-dim!</p>
    <p>Efficient classification for metric data 8</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 9</p>
    <p>Preliminaries: Lipschitz constant The Lipschitz constant L of a function f: X  R measures its</p>
    <p>smoothness  It is the smallest value L that satisfies for all points xi,xj in X</p>
    <p>Denoted by</p>
    <p>Suppose hypothesis h: S  {-1,1} is consistent with sample S  Its Lipschitz constant of h is determined by the closest pair of differently</p>
    <p>labeled points</p>
    <p>Or equivalently  2/d(S+,S)</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 10</p>
    <p>Preliminaries: Lipschitz extension Lipschitz extension:</p>
    <p>A classic problem in Analysis  given a function f: S  R for S in X, extend f to all of X without</p>
    <p>increasing the Lipschitz constant</p>
    <p>Example: Points on the real line  f(1) = 1  f(-1) = -1  credit: A. Oberman</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 11</p>
    <p>Classification for metric data  A powerful framework for metric classification was introduced</p>
    <p>by von Luxburg &amp; Bousquet (vLB, JMLR 04)</p>
    <p>Construction of h on S: The natural hypotheses (classifiers) to consider are maximally smooth Lipschitz functions</p>
    <p>Estimation of h on X: The problem of evaluating h for new points in X reduces to the problem of finding a Lipschitz function consistent with h  Lipschitz extension problem  For example</p>
    <p>f(x) = mini [f(xi) + 2d(x, xi)/d(S +,S)] over all (xi,yi) in S</p>
    <p>Evaluation of h reduces to exact Nearest Neighbor Search  Strong theoretical motivation for the NNS classification heuristic</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 14</p>
    <p>Two new directions  The framework of [vLB 04] leaves open two further questions:</p>
    <p>Constructing h: handling noise  Bias-Variance tradeoff  Which sample points in S should h ignore?</p>
    <p>Evaluating h on X  In arbitrary metric space, exact NNS</p>
    <p>requires (n) time  Can we do better?</p>
    <p>q</p>
    <p>~1</p>
    <p>~1</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 15</p>
    <p>Doubling dimension  Definition: Ball B(x,r) = all points within distance r from x.</p>
    <p>The doubling constant (of a metric M) is the minimum value  such that every ball can be covered by  balls of half the radius  First used by [Assoud 83], algorithmically by [Clarkson 97].  The doubling dimension is ddim(M)=log2(M)  A metric is doubling if its doubling dimension is constant  Euclidean: ddim(Rd) = O(d)</p>
    <p>Packing property of doubling spaces  A set with diameter diam and minimum</p>
    <p>inter-point distance a, contains at most</p>
    <p>(diam/a)O(ddim) points Here 7.</p>
  </div>
  <div class="page">
    <p>Applications of doubling dimension Major application to databases</p>
    <p>Recall that exact NNS requires (n) time in arbitrary metric space  There exists a linear size structure that supports approximate nearest neighbor search in</p>
    <p>time 2O(ddim) log n</p>
    <p>Database/network structures and tasks analyzed via the doubling dimension  Nearest neighbor search structure [KL 04, HM 06, BKL 06, CG 06]  Image recognition (Vision) [KG --]  Spanner construction [GGN 06, CG 06, DPP 06, GR 08a, GR 08b]  Distance oracles [Tal 04, Sli 05, HM 06, BGRKL 11]  Clustering [Tal 04, ABS 08, FM 10]  Routing [KSW 04, Sli 05, AGGM 06, KRXY 07, KRX 08]</p>
    <p>Further applications  Travelling Salesperson [Tal 04]  Embeddings [Ass 84, ABN 08, BRS 07, GK 11]  Machine learning [BLL 09, KKL 10, KKL --]</p>
    <p>Note: Above algorithms can be extended to nearly-doubling spaces [GK 10]</p>
    <p>Message: This is an active line of research</p>
  </div>
  <div class="page">
    <p>Our dual use of doubling dimension Interestingly, considering the doubling dimension yields</p>
    <p>contributes in two different areas</p>
    <p>Statistical: Function complexity  We bound the complexity of the hypothesis in terms of the doubling</p>
    <p>dimension of X and the Lipschitz constant of the classifier h</p>
    <p>Computational: efficient approximate NNS</p>
    <p>Efficient classification for metric data 19</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 20</p>
    <p>Statistical contribution  We provide generalization bounds for Lipschitz functions on</p>
    <p>spaces with low doubling dimension  vLB provided similar bounds using covering numbers and Rademacher</p>
    <p>averages</p>
    <p>Fat-shattering analysis:  L-Lipschitz functions shatter a set</p>
    <p>inter-point distance is at least 2/L  Packing property</p>
    <p>set has (diam L)O(ddim) points  This is the fat-shattering dimension</p>
    <p>of the classifier on the space, and is</p>
    <p>a good measure of its complexity.</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 21</p>
    <p>Statistical contribution  [BST 99]:</p>
    <p>For any f that classifies a sample of size n correctly, we have with probability at least 1  P {(x, y) : sgn(f(x))  y}  2/n (d log(34en/d) log2(578n) + log(4/)) .</p>
    <p>Likewise, if f is correct on all but k examples, we have with probability at least 1  P {(x, y) : sgn(f(x))  y}  k/n + [2/n (d ln(34en/d) log2(578n) + ln(4/))]1/2.</p>
    <p>In both cases, d is bound by the fat-shattering dimension,</p>
    <p>d  (diam L)ddim + 1</p>
    <p>Done with the statistical contribution  On to the computational contribution.</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 22</p>
    <p>Computational contribution  Evaluation of h for new points in X</p>
    <p>Lipschitz extension function  f(x) = mini [yi + 2d(x, xi)/d(S</p>
    <p>+,S)]  Requires exact nearest neighbor search, which can be expensive!</p>
    <p>New tool: (1+)-approximate nearest neighbor search  2O(ddim) log n + O(-ddim) time  [KL 04, HM 06, BKL 06, CG 06]</p>
    <p>If we evaluate f(x) using an approximate NNS, we can show that the result agrees with (the sign of) at least one of  g(x) = (1+) f(x) +   e(x) = (1+) f(x) -   Note that g(x)  f(x)  e(x)</p>
    <p>g(x) and e(x) have Lipschitz constant (1+)L, so they and the approximate function generalizes well</p>
    <p>g(x) f(x) e(x)</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 23</p>
    <p>Final problem: bias variance tradeof Which sample points in S should h ignore?</p>
    <p>If f is correct on all but k examples, we have with probability at least 1  P {(x, y):sgn(f(x))  y}  k/n+ [2/n (d ln(34en/d)log2(578n) +ln(4/))]1/2.  Where d  (diam L)ddim + 1</p>
    <p>-1 +1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 24</p>
    <p>Structural Risk Minimization  Algorithm</p>
    <p>Fix a target Lipschitz constant L  O(n2) possibilities</p>
    <p>Locate all pairs of points from S+ and S- whose distance is less than 2L  At least one of these points has to be taken as an error</p>
    <p>Goal: Remove as few points as possible</p>
    <p>-1</p>
    <p>+1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 25</p>
    <p>Structural Risk Minimization  Algorithm</p>
    <p>Fix a target Lipschitz constant L  O(n2) possibilities</p>
    <p>Locate all pairs of points from S+ and S- whose distance is less than 2L  At least one of these points has to be taken as an error</p>
    <p>Goal: Remove as few points as possible</p>
    <p>Minimum vertex cover  NP-Complete  Admits a 2-approximation in O(E) time</p>
    <p>-1</p>
    <p>+1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 26</p>
    <p>Structural Risk Minimization  Algorithm</p>
    <p>Fix a target Lipschitz constant L  O(n2) possibilities</p>
    <p>Locate all pairs of points from S+ and S- whose distance is less than 2L  At least one of these points has to be taken as an error</p>
    <p>Goal: Remove as few points as possible</p>
    <p>Minimum vertex cover  NP-Complete  Admits a 2-approximation in O(E) time</p>
    <p>Minimum vertex cover on a bipartite graph  Equivalent to maximum matching (Konigs theorem)  Admits an exact solution in O(n2.376) randomized time [MS 04]</p>
    <p>-1</p>
    <p>+1</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 27</p>
    <p>Efficient SRM  Algorithm:</p>
    <p>For each of O(n2) values of L  Run matching algorithm to find minimum error  Evaluate generalization bound for this value of L</p>
    <p>O(n4.376) randomized time</p>
    <p>Better algorithm  Binary search over O(n2) values of L  For each value</p>
    <p>Run greedy 2-approximation</p>
    <p>Approximate minimum error in O(n2 log n) time Evaluate approximate generalization bound for this value of L</p>
  </div>
  <div class="page">
    <p>Efficient classification for metric data 28</p>
    <p>Conclusion  Results:</p>
    <p>Generalization bounds for Lipschitz classifiers in doubling spaces  Efficient evaluation of the Lipschitz extension hypothesis using</p>
    <p>approximate NNS  Efficient Structural Risk Minimization</p>
    <p>Continuing research: Continuous labels  Risk bound via the doubling dimension  Classifier h determined via an LP  Faster LP: low-hop low-stretch spanners [GR 08a, GR 08b]  fewer</p>
    <p>constraints, each variable appears in bounded number of constraints.</p>
  </div>
  <div class="page">
    <p>Application: earthmover distance</p>
    <p>Efficient classification for metric data 29</p>
    <p>S T</p>
  </div>
</Presentation>
