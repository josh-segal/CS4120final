<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Building Algorithmically Nonstop Fault Tolerant MPI Programs</p>
    <p>Rui Wang, Erlin Yao, Pavan Balaji, Darius Buntinas, Mingyu Chen, and Guangming Tan</p>
    <p>Argonne National Laboratory, Chicago, USA ICT, Chinese Academy of Sciences, China</p>
  </div>
  <div class="page">
    <p>Hardware Resilience for large-scale systems  Resilience is a prominent becoming issue in large-scale</p>
    <p>supercomputers  Exascale systems that will be available in 2018-2020 will have close to</p>
    <p>a billion processing units  Even if each processing element fails once every 10,000 years, a</p>
    <p>system will have a fault once every 5 minutes  Some of these faults are correctable by hardware, while some</p>
    <p>are not  E.g., single bit flips are correctable by ECC memory, but double-bit flips</p>
    <p>are not  Even for cases where hardware corrections are technologically</p>
    <p>feasible, cost and other power constraints might make then practically infeasible</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Software Resilience</p>
    <p>Software resilience is cheaper with respect to cost investment, but has performance implications  The idea of most researchers working in this area is to understand this</p>
    <p>performance/resilience tradeof</p>
    <p>Classical software resilience technique: system checkpointing  Create a snapshot of the application image at some time interval and</p>
    <p>roll back to the last checkpoint if a failure occurs  Transparent to the user, but stresses the I/O subsystem</p>
    <p>SystemsU Perf. Ckpt time Source</p>
    <p>RoadRunner 1PF ~20 min. Panasas</p>
    <p>LLNL BG/L 500 TF &gt;20 min. LLNL</p>
    <p>Argonne BG/P 500 TF ~30 min. LLNL</p>
    <p>Total SGI Altix 100 TF ~40 min. estimation</p>
    <p>IDRIS BG/P 100 TF 30 min. IDRIS</p>
    <p>[Gibson, ICPP2007]</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Algorithm-based Fault Tolerance</p>
    <p>Recent research eforts in resilience have given birth to a new form of software resilience: Algorithmic-based Fault Tolerance (ABFT)  A.k.a. Algorithmic fault tolerance, application-based fault tolerance</p>
    <p>Key idea is to utilize mathematical properties in the computation being carried out to reconstruct data on a failure  No disk I/O phase, so the performance is independent of the file</p>
    <p>system bandwidth  Not 100% transparent  for most applications that use math libraries</p>
    <p>for their computation this can be transparent, but for others its not  This work has mostly been done in the context of dense matrix</p>
    <p>manipulation operations, but the concept is applicable to other contexts too</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>ABFT Recovery</p>
    <p>First proposed in 1987 to detect and correct instant errors at the VLSI layer</p>
    <p>Improved by Jack Dongarra to deal with node failures  Concept:</p>
    <p>Add redundant nodes to store encoded checksum of the original data  Re-design algorithm to compute original data and redundancy</p>
    <p>synchronously  Recover corrupted data upon failure</p>
    <p>D1 D2 D3 E</p>
    <p>D2 E D1 D3</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Deeper Dive into ABFT Recovery</p>
    <p>ABFT recovery pros:  Completely utilizes in-memory techniques, so no disk I/O is required  Utilizes additional computation to deal with node losses, so the</p>
    <p>amount of extra nodes required is fairly small (equal to the number of failures expected during the run)</p>
    <p>Important diference compared to in-memory checkpointing which requires twice the number of nodes</p>
    <p>ABFT recovery cons:  Failure recovery is non-trivial</p>
    <p>Requires additional computation  no problem; computation is free  Requires all processes to synchronize every time there is a failure</p>
    <p>synchronization is not free, especially when dealing with &gt;100,000 processes</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>In this paper</p>
    <p>This paper improves on ABFT Recovery to propose a new methodology called ABFT hot replacement</p>
    <p>Idea is to utilize additional mathematical properties to not require synchronization on a failure  Synchronization is eventually required, but can be delayed to a more</p>
    <p>natural synchronization point (such as the end of the program)  We demonstrate ABFT hot replacement with LU</p>
    <p>factorization in this paper, though the idea is relevant to other dense matrix computations as well  Might also work for sparse matrix computations, but is not as</p>
    <p>straightforward  Also demonstrate LINPACK with our proposed approach</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Motivation</p>
    <p>Requirements from MPI and improvements to MPICH2</p>
    <p>ABFT Hot Replacement</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Fault Tolerance in MPI</p>
    <p>Minimum set of fault-tolerance features required  Node failure will not cause the entire job to abort.  Communication operations involving a failed process will not hang and will</p>
    <p>eventually complete.  Communication operations will return an error code when it is afected by a</p>
    <p>failed process. This is needed to determine whether to re-send or re-receive messages</p>
    <p>The MPI implementation should provide a mechanism to query for failed processes.</p>
    <p>MPICH provides all these features and two forms of fault notification</p>
    <p>Asynchronous (through the process manager)  Synchronous (through the MPI communication operations)</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Process Management and Asynchronous Notification</p>
    <p>P0</p>
    <p>MPI Library</p>
    <p>P1</p>
    <p>MPI Library P2</p>
    <p>MPI Library</p>
    <p>Hydra proxy</p>
    <p>Hydra proxy</p>
    <p>mpiexec</p>
    <p>Node 0 Node 1</p>
    <p>SIGCHL D</p>
    <p>SIGUSR 1</p>
    <p>SIGUSR1</p>
    <p>FP List</p>
    <p>NULLP2</p>
    <p>FP List</p>
    <p>NULLP2</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Synchronous Notification: Point-to-point Communication</p>
    <p>If a communication operation fails, an MPI_ERR_OTHER is returned to the application  A message is sent to or a receive is posted for a message from a failed</p>
    <p>process  For nonblocking operations, the error can be returned during</p>
    <p>the subsequent WAIT operation that touches the request  Wildcard receives, i.e., using MPI_ANY_SOURCE create a</p>
    <p>special case, since we dont know who will send the data  In this case, all processes that posted a wildcard receive would get an</p>
    <p>error</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Synchronous Notification: Collective Communication  Collective operation does not hang, but some processes may have</p>
    <p>invalid results</p>
    <p>MPICH2 internally performs data error management  Mark the messages carrying invalid data by using a diferent tag value.  The process will continue performing the collective operation if a process</p>
    <p>receives a message marked as containing invalid data, but will mark any subsequent messages it sends as containing invalid data.</p>
    <p>From the application perspective:  The collective operation will return an error code or if it had received invalid</p>
    <p>data at any point during the operation; otherwise, returns MPI_SUCCESS.</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Motivation</p>
    <p>Requirements from MPI and improvements to MPICH2</p>
    <p>ABFT Hot Replacement</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>ABFT Hot-replacement</p>
    <p>Before the replacement,</p>
    <p>After the replacement,</p>
    <p>Assume D=DT</p>
    <p>D2D1 D3 E</p>
    <p>P1 P2 P3 P4</p>
    <p>ABFT Hot Replacement</p>
    <p>HiPC (12/20/2011)</p>
    <p>niii DDDDDD  111</p>
    <p>nii DEDDDD  111'</p>
    <p>T</p>
  </div>
  <div class="page">
    <p>High Performance Linpack (HPL)  benchmark for ranking supercomputers in top500  solve Ax = b</p>
    <p>C H</p>
    <p>E C</p>
    <p>K S</p>
    <p>U M</p>
    <p>C H</p>
    <p>E C</p>
    <p>K S</p>
    <p>U M</p>
    <p>C H</p>
    <p>E C</p>
    <p>K S</p>
    <p>U M</p>
    <p>Each process generates its local random matrix A for i = 0, 1,</p>
    <p>LU factorization Ai = LiUi ; computation</p>
    <p>Broadcast Li right ; communication</p>
    <p>Update the trailing sub-matrix U ; computation solve upper-triangular Ux = L-1b to obtain x ; back substitution phase</p>
    <p>checksum relationship maintained</p>
    <p>ABFT Hot Recovery in LINPACK</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Hot-Replacement  Replace dead process column by redundant process column</p>
    <p>Background Recovery  Recover the factorized data  Requires additional computation, but is only local</p>
    <p>Matrix U is not upper-triangular any more</p>
    <p>Failure Handling in Computation</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Failure Handling in Computation (contd.)</p>
    <p>Before hot-replacement</p>
    <p>After hot-replacement</p>
    <p>The correct solution x</p>
    <p>This phase requires a global synchronization, but can be done at the end of the application (or some natural synchronization point)</p>
    <p>HiPC (12/20/2011)</p>
    <p>bAx</p>
    <p>byA '</p>
    <p>Tyx</p>
  </div>
  <div class="page">
    <p>Failure Handling in Communication</p>
    <p>Broadcast phase : message forwarding  Robust broadcast mechanism</p>
    <p>None of the processes will block if a failure occurs (MPI provides this)  The error is notified to the application  at least one process will know</p>
    <p>if an error occurred anywhere (MPI provides this)  Either all non-failed processes receive the message successfully or</p>
    <p>none of them receive the message (MPI does not provide this yet)  Additional communication required to ensure the global view</p>
    <p>of the broadcast is consistent</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Motivation</p>
    <p>Requirements from MPI and improvements to MPICH2</p>
    <p>ABFT Hot Replacement</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Experimental Testbed</p>
    <p>Platform1:  17 nodes each with 4 quadcore 2.2 GHz Opteron processors (16-cores</p>
    <p>per node)  Connected by Gigabit Ethernet</p>
    <p>Platform II:  8 blades, 10 Intel Xeon X5650 processors per blade  Nodes in the same blade are connected by InfiniBand, while diferent</p>
    <p>blades are connected with each other by a single InfiniBand cable</p>
    <p>MPICH2:  The work done was based on an experimental version of MPICH2</p>
    <p>based on 1.3.2p1. The changes have been incorporated into MPICH2 releases as of 1.4 (and some more improvements incorporated into 1.5a1 and the upcoming 1.5a2)</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Performance Comparison of LINPACK</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Correctness Comparison</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Impact of Failure Occurrence</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Motivation</p>
    <p>Requirements from MPI and improvements to MPICH2</p>
    <p>ABFT Hot Replacement</p>
    <p>Experimental Evaluation</p>
    <p>Concluding Remarks</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Concluding Remarks</p>
    <p>Resilience is an important issue that needs to be addressed  Hardware resilience can only go so far, because of technology, power</p>
    <p>and price constraints  Software resilience required to augment places where hardware</p>
    <p>resilience is not sufficient  System checkpointing was the classical resilience method,</p>
    <p>but hard to scale to very large systems  ABFT-based methods gaining popularity</p>
    <p>Use mathematical properties to recompute data on failure  ABFT Recovery method previously proposed  problem is that it</p>
    <p>requires synchronization between all processes on failure  We proposed ABFT hot replacement, which deals with this problem</p>
    <p>HiPC (12/20/2011)</p>
  </div>
  <div class="page">
    <p>Thank You!</p>
    <p>Email: balaji@mcs.anl.gov Web: http://www.mcs.anl.gov/~balaji</p>
  </div>
</Presentation>
