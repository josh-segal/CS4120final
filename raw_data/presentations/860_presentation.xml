<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>ATC 2014 Philadelphia, PA!</p>
    <p>ShuffleWatcher : Shuffle-aware Scheduling in Mul5-tenant MapReduce Clusters</p>
    <p>Faraz Ahmad* Srimat T. Chakradhar Anand Raghunathan T. N. Vijaykumar</p>
    <p>!</p>
    <p>!</p>
    <p>*</p>
  </div>
  <div class="page">
    <p>Mul%-tenancy in MapReduce Clusters</p>
    <p>MAPREDUCE! JOBS!</p>
    <p>USERS!</p>
    <p>Challenges:! (1) Cluster throughput (2) Jobs latency! (3) Fairness among users!</p>
    <p>Our contribution: We achieve high throughput and low latency while maintaining fairness among users!</p>
    <p>Cost! Utilization! Data Sharing!</p>
  </div>
  <div class="page">
    <p>Significance of Shuffle in Mul%-tenant Clusters</p>
    <p>Shuffle  All-Map-to-All-Reduce communica%on</p>
    <p>Impact of Shuffle-heavy jobs  60% jobs in Yahoo, 20% jobs in</p>
    <p>Facebook are Shuffle-heavy  Shuffle-heavy jobs run much</p>
    <p>longer than Shuffle-light  high network traffic volume</p>
    <p>Impact of Mul%-tenancy  Mul%ple concurrent jobs shuffle  high network bisec%on pressure</p>
    <p>Net impact : Low cluster throughput / high jobs latency!</p>
    <p>map! tasks!</p>
    <p>reduce! tasks!</p>
    <p>shuffle!</p>
    <p>Input data (HDFS)!</p>
    <p>Output data (HDFS)!</p>
  </div>
  <div class="page">
    <p>Related Work: Mul%tenant Scheduling</p>
    <p>Targe%ng fairness  FIFO, Capacity, Fair (Hadoop)  Dominant Resource Fairness [NSDI 11]</p>
    <p>Targe%ng throughput  Delay Scheduler [EuroSys 10], Quincy [SOSP 09]  Op%mizes remote Map traffic</p>
    <p>Our contribution : We improve throughput by focusing on Shuffle while maintaining fairness among users!</p>
    <p>Traffic type Job Type Traffic volume (% of total) Remote Map Traffic Shuffle-heavy 5% Remote Map Traffic Shuffle-light 5% Shuffle Shuffle-heavy 78% Shuffle Shuffle-light 12%</p>
  </div>
  <div class="page">
    <p>ShuffleWatcher: Contribu%ons</p>
    <p>Achieves high throughput and low job latency by shaping and reducing Shuffle traffic</p>
    <p>Leverages factors unique to mul%-tenancy  Exploits trade-off : intra-job concurrency vs. Shuffle locality  Employs three mechanisms :</p>
    <p>Network-Aware Shuffle Scheduling (NASS) (traffic shaping)  Shuffle-Aware Map Placement (SAMP) (traffic reduc%on)  Shuffle-Aware Reduce Placement (SARP) (traffic reduc%on)</p>
    <p>Keeps the underlying fairness policy intact ShuffleWatcher achieves 46% higher throughput, 32% lower job latency, 48% reduced traffic compared to Delay Scheduler!</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduc%on  ShuffleWatcher mechanisms</p>
    <p>Network-aware Shuffle Scheduling (NASS)  Shuffle-aware Reduce Placement (SARP)  Shuffle-aware Map Placement (SAMP)</p>
    <p>Experimental Evalua%on  Conclusion</p>
  </div>
  <div class="page">
    <p>Network-Aware Shuffle Scheduling (NASS)</p>
    <p>Observa%on #1 :  Network not saturated all the %me</p>
    <p>40% jobs in Yahoo, 80% jobs in Facebook are Shuffle-light</p>
    <p>Provides an opportunity to shape traffic.</p>
    <p>Simple delaying a job  not an op%on  fairness compromised</p>
    <p>Shuffle profile in 100-node Amazon EC2 cluster (Fair Scheduler)!</p>
  </div>
  <div class="page">
    <p>Network-Aware Shuffle Scheduling (NASS) (contd.)</p>
    <p>Observa%on #2 :  Mul%-tenancy offers flexibility in Shuffle schedule</p>
    <p>map phase! shuffle! reduce phase!</p>
    <p>reduce scheduled! time!</p>
    <p>single-! tenancy!</p>
    <p>map phase! shuffle!</p>
    <p>reduce scheduled! time!</p>
    <p>multi-! tenancy!</p>
    <p>reduce phase!</p>
    <p>Shuffle may be delayed in multi-tenancy, if needed, without ! loss of communication-computation overlap!</p>
    <p>High intra-job! Map-Shuffle concurrency!</p>
    <p>Low intra-job Map-! Shuffle concurrency!</p>
    <p>traffic ! volume!</p>
    <p>tasks from other jobs</p>
    <p>tasks from other jobs</p>
  </div>
  <div class="page">
    <p>Network-Aware Shuffle Scheduling (NASS) (contd.)</p>
    <p>Contribu%on:  Delay a jobs shuffle at high network loads to shape traffic</p>
    <p>At high load, schedule other tasks of the same user that do not stress network.</p>
    <p>reduce ! Shuffle load!</p>
    <p>increase! Shuffle load!</p>
    <p>NASS achieves high throughput while maintaining fairness!</p>
  </div>
  <div class="page">
    <p>Shuffle-Aware Reduce Placement (SARP)</p>
    <p>Map ! execution!</p>
    <p>rack! server!</p>
    <p>Intermediate ! data !</p>
    <p>distribution!</p>
    <p>Single-tenancy! Multi-tenancy!</p>
  </div>
  <div class="page">
    <p>Shuffle-Aware Reduce Placement (SARP) (contd.)</p>
    <p>Observa%on:  A jobs intermediate data is likely to be skewed</p>
    <p>Mul%ple jobs run concurrently  Exploit NASSs delayed Shuffle to improve locality</p>
    <p>Intermediate data loca%ons are known</p>
    <p>Contribu%on:  Assign Reduce tasks to racks with more intermediate data</p>
    <p>Improves Shuffle locality  lowers cross-rack Shuffle traffic</p>
  </div>
  <div class="page">
    <p>Shuffle-Aware Map Placement (SAMP)</p>
    <p>Original Map Execution!</p>
    <p>Intermediate data distribution!rack!server!</p>
    <p>Ideal map placement for Shuffle locality!</p>
  </div>
  <div class="page">
    <p>Shuffle-Aware Map Placement (SAMP) (contd.)</p>
    <p>Observa%on:  Input data redundancy provides flexibility in Map scheduling</p>
    <p>Contribu%on:  Localize Map tasks to fewer racks to improve Shuffle locality</p>
    <p>Provides further opportuni%es for SARP to localize Shuffle  Lowers the sum (Remote Map Traffic + cross-rack Shuffle)</p>
    <p>May incur some remote map traffic for larger savings in Shuffle</p>
  </div>
  <div class="page">
    <p>ShuffleWatcher  Overall Picture</p>
    <p>SHUFFLEWATCHER!MAPREDUCE! JOBS!</p>
    <p>USERS!</p>
    <p>CLUSTER!</p>
    <p>Free! workers!</p>
    <p>Network- Aware Shuffle Scheduling (NASS)</p>
    <p>Shuffle- Aware Map Placement (SAMP)</p>
    <p>Shuffle- Aware Reduce</p>
    <p>Placement (SARP)</p>
    <p>NetSat!</p>
    <p>Network! Saturated?!</p>
    <p>task! assignment!</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduc%on  ShuffleWatcher mechanisms</p>
    <p>Network-aware Shuffle Scheduling (NASS)  Shuffle-aware Reduce Placement (SARP)  Shuffle-aware Map Placement (SAMP)</p>
    <p>Experimental Evalua%on  Conclusion</p>
  </div>
  <div class="page">
    <p>Experimental Methodology</p>
    <p>Baseline:  Fair Scheduler [Hadoop implementa%on]  DRF Scheduler [developed in-house as per NSDI 11]  Delay Scheduler [Eurosys 10, Hadoop implementa%on]</p>
    <p>Planorm:  100-node Amazon EC2 cluster</p>
    <p>4 virtual cores per node, 15 GB memory  10 racks of 10 nodes each  50 Mbps cross-rack per-node bisec%on bandwidth</p>
    <p>Workloads:  Job submission : exponen%al distribu%on (mean arrival rate : 40 s)  30 users, jobs from 12 MapReduc%ons, run for 4 hours</p>
    <p>40% shuffle-heavy, 20% shuffle-medium, 40% shuffle-light  Job sizes : mix of small, medium, large sizes vary from 100 MB-1 TB</p>
  </div>
  <div class="page">
    <p>Throughput Comparison</p>
    <p>N or m al iz ed</p>
    <p>T hr ou</p>
    <p>gh pu</p>
    <p>t</p>
    <p>ShuffleWatcher is independent of fairness policy and ! improves throughput (39%, 46%, 40%) over multiple schemes!</p>
  </div>
  <div class="page">
    <p>Jobs Latency (turn-around %me) Comparison</p>
    <p>N or m al iz ed</p>
    <p>ShuffleWatcher improves latency (27%, 32%, 29%) over multiple schemes!</p>
  </div>
  <div class="page">
    <p>Cross-rack Traffic Comparison</p>
    <p>t ra ffi c</p>
    <p>ShuffleWatcher achieves significant traffic reduction! (36%, 48%, 35%) over multiple schemes !</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Shuffle significantly impacts mul%-tenancy performance  ShuffleWatcher achieves high throughput by shaping and</p>
    <p>reducing Shuffle traffic while maintaining fairness  Leverages factors unique to mul%-tenancy  Trades-off intra-job concurrency for Shuffle locality</p>
    <p>Employs three mechanisms :  Network-Aware Shuffle Scheduling (NASS) for traffic shaping  Shuffle-Aware Map Placement (SAMP) and Shuffle-Aware Reduce Placement (SARP) for traffic reduc%on</p>
    <p>ShuffleWatcher achieves 46% higher throughput, 32% lower job latency, 48% reduced traffic compared to Delay Scheduler</p>
  </div>
  <div class="page">
    <p>Thanks</p>
  </div>
  <div class="page">
    <p>Back up slides</p>
  </div>
  <div class="page">
    <p>NASS Algorithm</p>
    <p>called when a worker requests for a task</p>
  </div>
  <div class="page">
    <p>SARP Algorithm</p>
    <p>Invoked when a job J schedules its Reduce tasks! 1. if (fraction of Maps completed for J &gt; MapCompletionThreshold) {! 2. for each rack R {! 3. PreferredReducesPerRack[R] = NumReduces *</p>
    <p>!(Intermediate data of job J on rack R / total Intermediate !! !data of job J)!</p>
    <p>}! } else {!</p>
    <p>}!</p>
  </div>
  <div class="page">
    <p>SAMP Algorithm</p>
    <p>!remote Map traffic + cross-rack Shuffle! 8. } while (minCrossRackTraffic decreases)! 9. PreferredMapRacks  TmpMapRacks! 10. TentativeReducesPerRack  TmpReducesPerRack!</p>
    <p>called when a new job J is submitted!</p>
  </div>
  <div class="page">
    <p>Network-aware Shuffle Scheduling (NASS)</p>
    <p>Network Saturated ?!</p>
    <p>Delay Shuffle-heavy ! Reduce tasks!</p>
    <p>Schedule Map tasks ! through SAMP!</p>
    <p>Pick a user based on fairness policy!</p>
    <p>Prioritize Shuffle-heavy ! Reduce tasks through SARP!</p>
    <p>Schedule Reduce tasks! through SARP!</p>
    <p>Schedule Map tasks ! through SAMP!</p>
    <p>Schedule Reduce tasks! through SARP!</p>
    <p>Yes! No!</p>
    <p>curbs concurrency!</p>
    <p>replenishes concurrency!</p>
    <p>improves ! Shuffle locality!</p>
    <p>Better critical resource (network) utilization -&gt; higher throughput!</p>
  </div>
  <div class="page">
    <p>Shuffle-aware Reduce Placement (SARP)</p>
    <p>Rack with more intermediate data -&gt; schedule more reduce tasks on that rack -&gt; high locality 10GB!</p>
    <p>intermediate data distribution!</p>
    <p>rack1! rack2!</p>
    <p>job1! job2!</p>
    <p>Shufflejob1 = 50GB (5 + 45)! Shufflejob2 = 50GB (45 + 5)!</p>
    <p>Shufflejob2 = 18GB (9 + 9)!</p>
    <p>Reduce task distribution (without SARP)!</p>
    <p>Reduce task distribution (with SARP)! Cross-rack traffic reduction : (100  36)/100 = 64%!</p>
  </div>
  <div class="page">
    <p>Shuffle-aware Map Placement (SAMP)</p>
    <p>Localize map tasks to fewer racks -&gt; high Shuffle locality  Minimize sum = remote Map traffic + cross-rack Shuffle</p>
    <p>Exploit input data redundancy 100%!</p>
    <p>(with replication)!</p>
    <p>job1! job2! 100%!</p>
    <p>rack1! rack2!</p>
    <p>(with SAMP)!</p>
    <p>intermediate data distribution! (without SAMP)!</p>
    <p>Cross-rack traffic reduction (with SARP) : (100  0)/100 = 100%!</p>
  </div>
  <div class="page">
    <p>Workload characteris%cs</p>
    <p>Exponen%al distribu%on  job size, job type picked at random at submission %me (consistent with Facebook, Yahoo traces [Mascots11]).</p>
    <p>Mean job arrival rate  Highest throughput for base case (delay scheduler)  40 seconds</p>
    <p>Job Category Benchmarks (Frequency)</p>
    <p>Shuffle- heavy</p>
    <p>Terasort(5%), Ranked-inv-index(10%), Self-join(10%), Word-sequence- count(10%), Adjacency-list(5%)</p>
    <p>Shuffle- medium</p>
    <p>Inverted-index(10%), Term- vector(10%)</p>
    <p>Shuffle- light</p>
    <p>Grep(15%), Wordcount(10%), Classifica%on(5%), Histo-movies(5%), Histo-ra%ngs(5%)</p>
    <p>Input Job Sizes %jobs</p>
    <p>Input Job Sizes %jobs</p>
    <p>&lt;100MB 20% 100-200GB 10%</p>
  </div>
  <div class="page">
    <p>Benchmarks</p>
    <p>Job Sizes Distribu%on</p>
    <p>Job Category Benchmarks (Frequency)</p>
    <p>Shuffle-heavy Terasort(5%), Ranked-inv-index(10%), Self-join(10%), Word- sequence-count(10%), Adjacency-list(5%)</p>
    <p>Shuffle-medium Inverted-index(10%), Term-vector(10%)</p>
    <p>Shuffle-light Grep(15%), Wordcount(10%), Classifica%on(5%), Histo-movies(5%), Histo-ra%ngs(5%)</p>
    <p>Input Job Sizes %jobs Input Job Sizes %jobs</p>
    <p>&lt;100MB 20% 100-200GB 10%</p>
  </div>
  <div class="page">
    <p>Performance Comparison (contd.)</p>
    <p>In tr a- jo b co nc ur re nc y</p>
    <p>Job Progress</p>
    <p>Intra-job Concurrency Reduce Map</p>
    <p>ShuffleWatcher achieves lower intra-job concurrency to improve Shuffle locality!</p>
  </div>
  <div class="page">
    <p>ShuffleWatcher impact on Traffic Shaping</p>
    <p>without ! ShuffleWatcher!</p>
    <p>with! ShuffleWatcher!</p>
  </div>
  <div class="page">
    <p>Throughput Impact of NASS, SARP and SAMP</p>
    <p>Shuffle- heavy</p>
    <p>Shuffle- medium</p>
    <p>Shuffle- light</p>
    <p>All</p>
    <p>N or m al iz ed</p>
    <p>T hr ou</p>
    <p>gh pu</p>
    <p>t w .r .t</p>
    <p>Fa ir</p>
    <p>NASS SARP SAMP</p>
    <p>All mechanisms contribute significantly to improve throughput!</p>
  </div>
  <div class="page">
    <p>Traffic Reduc%on achieved by SARP and SAMP</p>
    <p>a b c a b c a b c a b c</p>
    <p>N or m al iz ed</p>
    <p>C ro ss -r ac k Tr affi</p>
    <p>c w .r .t . F ai r</p>
    <p>Remote Map Traffic Cross-rack Shuffle</p>
    <p>Shuffle-heavy Shuffle-medium Shuffle-light All</p>
    <p>SARP alone reduces overall network traffic by 15% ! SARP+SAMP reduce overall network traffic by 36%!</p>
    <p>a: NASS! b: NASS + SARP! c: NASS + SARP + SAMP!</p>
  </div>
  <div class="page">
    <p>Impact of varia%on in number of jobs per user</p>
    <p>Th ro ug hp</p>
    <p>ut im</p>
    <p>pr ov em</p>
    <p>en t ov er F ai r</p>
    <p>Number of jobs per user</p>
    <p>ShuffleWatcher scales well with number of jobs per user and ! achieves higher improvement with higher number of jobs!</p>
  </div>
  <div class="page">
    <p>Sensi%vity to job mix</p>
    <p>mix1 mix2 mix3</p>
    <p>Th ro ug hp</p>
    <p>ut im</p>
    <p>pr ov em</p>
    <p>en t</p>
    <p>ov er F ai r</p>
    <p>mix1: 20%,20%,60% (Shuffle-heavy, Shuffle-medium, Shuffle-light)! mix2: 40%,20%,40% (default)! mix3: 60%, 20%,20%!</p>
    <p>ShuffleWatcher is effective over a wide range of job mixes!</p>
  </div>
  <div class="page">
    <p>Simpler algorithms to lower Shuffle wont work</p>
    <p>Need to consider the following aspects of scheduling:  jobs latency, fairness, cluster u%liza%on</p>
    <p>Scheduling Reduce tasks based on Intermediate data  sta%c decision  loses opportunity to overlap with own Map  poten%al increase in latency  fixed assignment to racks -&gt; reduced flexibility</p>
    <p>Scheduling Reduce tasks based on Input data distribu%on  cannot exploit addi%onal skew in intermediate data  fixed assignment to racks -&gt; reduced flexibility</p>
    <p>Traffic shaping ignored:  40% improvements due to traffic shaping</p>
  </div>
  <div class="page">
    <p>Bisec%on Bandwidth</p>
    <p>Bisec%on bandwidth is a global resource that is hard to scale up with the clusters compute and storage resources (CPU, memory, disk).  On the contrary, network switch and link bandwidth scale with hardware technology.</p>
    <p>Even with recent advances in data center networks, large clusters are typically provisioned for per-node bisec%on bandwidth that is 5-20 %mes lower than the within-rack bandwidth.  Full Bisec%on bandwidth is an expensive solu%on.  Bisec%on not saturated all the %me.  Same cost can be spent to buy addi%onal compute resources.</p>
  </div>
  <div class="page">
    <p>Full bisec%on vs. High over-subscrip%on</p>
    <p>-</p>
    <p>N et w or k in fr as tr uc tu re C os t (U SD</p>
    <p>)</p>
    <p>Number of Nodes</p>
  </div>
  <div class="page">
    <p>NetSat daemon</p>
    <p>Determines whether network is saturated or not.  Cross-rack link bandwidths are fixed to maintain higher oversubscrip%on ra%o</p>
    <p>(5:1) in our case.  E.g., 500Mbps if 10 nodes per rack (50 Mbps per node).  Nodes can share cross-rack link bandwidth</p>
    <p>Algorithm (determines whether link is saturated): i. Receive data from all running reduce tasks for all jobs periodically. ii. Receive data from NameNode for remote map and output write traffic iii. Update data transfers among racks (link usages). iv. Compare to cross-rack link bandwidth limit v. Return true if link usage (for n intervals) &gt; NWSatura%onThreshold (0.8 * Satura%onLimit)</p>
    <p>Shapes traffic more conserva%vely.  Rate limiters per rack.</p>
    <p>Specific topological informa%on or precise conges%on monitoring mechanisms can be overlaid</p>
    <p>Can be modified to account for traffic from other applica%ons (such as interac%ve workloads, MPI jobs) in the cluster.</p>
  </div>
  <div class="page">
    <p>ShuffleWatcher: where network is not cri%cal</p>
    <p>The concurrency  locality trade-off will likely remain in favor of achieving high concurrency.</p>
    <p>Traffic shaping not needed.  Traffic reduc%on s%ll helpful</p>
    <p>Localized shuffle s%ll faster than dispersed shuffle  Remote map tasks is s%ll slower than local map task  SARP will follow SAMP if Shuffle not delayed.</p>
    <p>ShuffleWatcher s%ll bezer than base case.  SAMP and SARP remain ac%ve.  No addi%onal overheads of ShuffleWatcher.</p>
  </div>
  <div class="page">
    <p>ShuffleWatcher implementa%on changes</p>
    <p>New mul%-tenant scheduler in a separate /contrib folder.  ~500-1000 lines of scheduler code.</p>
    <p>Data structures needed:  JobProfiling (Shuffle-heavy/medium/light) : updated at job comple%on</p>
    <p>ShuffleInputRa%o (0.3-0.8 -&gt;Shuffle-medium)  NetSat daemon (for NASS): updated periodically  currentShuffleJobs (for NASS): list of jobs in shuffle phase  IntermediateDataLoc (for SARP): updated as map tasks complete  PreferredReducePerRack (for SARP):updated before scheduling Reduce tasks or</p>
    <p>when %me-window expires for the job.  PreferredMapRacks (for SAMP):updated at job submission  TentaNveReducePerRack (for SAMP):updated at job submission  FairnessModule : based on fairness policy to be used.</p>
    <p>Shuffle data transfer informa%on from Reduce tasks  At every heartbeat (regular intervals)</p>
  </div>
  <div class="page">
    <p>DRF Scheduler</p>
    <p>Observa%on: Each job has different resource requirement  Fixed slots waste resources  Prepare fine granularity slots for nodes and allocate as many as are needed by the task.</p>
    <p>Task resource requirement is provided by the user.  Max-min fairness for mul%ple resources</p>
    <p>Equalize the dominant shares of users.  Works on CPU, Memory resources.</p>
    <p>Not straight-forward to apply to network resource.  Depends upon task loca%ons and data loca%ons which is dynamic.</p>
    <p>Worst case requirements do not u%lize network efficiently</p>
  </div>
  <div class="page">
    <p>Network Topology (ShuffleWatcher)</p>
    <p>EC2 does not provide precise topological informa%on.  Same observed node-to-node bandwidth for all nodes in our cluster.  We group 10 nodes per rack and limit cross-rack bandwidth to 500Mbps.</p>
    <p>Uses the hadoop-default no%ons of local, rack-local and cross-rack traffic.  Cross-rack is remote traffic which needs to be minimized.  We limit per-node cross-rack bandwidth to 50Mbps which is the expected limit per-node in a large cluster.</p>
    <p>Mechanisms applicable to a broader range of bisec%on bandwidths.  Schemes do not depend upon precise topological informa%on.</p>
    <p>Generally applicable to all clusters.  Topological informa%on can be overlaid if available.</p>
  </div>
</Presentation>
