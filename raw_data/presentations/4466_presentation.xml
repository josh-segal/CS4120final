<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Mining Reference Tables for Automatic Text Segmentation</p>
    <p>Eugene Agichtein Columbia University</p>
    <p>Venkatesh Ganti Microsoft Research</p>
  </div>
  <div class="page">
    <p>Scenarios</p>
    <p>Importing unformatted strings into a target structured database  Data warehousing  Data integration</p>
    <p>Requires each string to be segmented into the target relation schema</p>
    <p>Input strings are prone to errors (e.g., data warehousing, data exchange)</p>
  </div>
  <div class="page">
    <p>Current Approaches</p>
    <p>Rule-based  Hard to develop, maintain, and deploy</p>
    <p>comprehensive sets of rules for every domain</p>
    <p>Supervised  E.g., [BSD01]  Hard to obtain comprehensive datasets needed to</p>
    <p>train robust models</p>
  </div>
  <div class="page">
    <p>Our Approach</p>
    <p>Exploit large reference tables  Learn domain-specific dictionaries  Learn structure within attribute values</p>
    <p>Challenges  Order of attribute concatenation in future test</p>
    <p>input is unknown  Robustness to errors in test input after training on</p>
    <p>clean and standardized reference tables</p>
  </div>
  <div class="page">
    <p>Problem Statement</p>
    <p>Target schema: R[A1,,An]  For a given string s (a sequence of tokens)</p>
    <p>segment s into s1,,sn substrings at token boundaries  map s1,,sn to Ai1,,Ain  maximize P(Ai1|s1)**P(Ain|sn) among all possible</p>
    <p>segmentations of s  Product combination function handles arbitrary</p>
    <p>concatenation order of attribute values  P(Ai|x) that a string x belongs to Ai estimated by an</p>
    <p>Attribute Recognition Model ARMi  ARMs are learned from a reference relation r[A1,,An]</p>
  </div>
  <div class="page">
    <p>Segmentation Architecture</p>
  </div>
  <div class="page">
    <p>ARMs</p>
    <p>Design goals  Accurately distinguish an attribute value from</p>
    <p>other attributes  Generalize to unobserved/new attribute values  Robust to input errors  Able to learn over large reference tables</p>
  </div>
  <div class="page">
    <p>ARM: Instantiation of HMMs</p>
    <p>Purpose: Estimate probabilities of token sequences belonging to attributes</p>
    <p>ARM: instantiation of HMMs (sequential models)</p>
    <p>Acceptance probability: product of emission and transition probabilities</p>
  </div>
  <div class="page">
    <p>Instantiating HMMs</p>
    <p>Instantiation has to define  Topology: states &amp; transitions  Emission &amp; transition probabilities</p>
    <p>Current automatic approaches for topology search from among a pre-defined class of topologies are based on cross validation [FC00, BSD01]</p>
    <p>Expensive  Number of states in the ARM is small to keep the search</p>
    <p>space tractable</p>
  </div>
  <div class="page">
    <p>Intuition behind ARM Design</p>
    <p>Street address examples  [nw 57th St], [Redmond Woodinville Rd]</p>
    <p>Album names  [The best of eagles], [The fury of aquabats], [Colors Soundtrack]</p>
    <p>Large dictionaries (e.g., aquabats, soundtrack, st) to exploit  Begin and end tokens are very important to distinguish values</p>
    <p>of an attribute (nw, st, the,)  Can learn patterns on tokens (e.g., 57th generalizes to *th)  Need robustness to input errors</p>
    <p>[Best of eagles] for [The best of eagles], [nw 57th] for [nw 57th st]</p>
  </div>
  <div class="page">
    <p>Large Number of States</p>
    <p>Associate a state per token: Each state only emits a single base token  More accurate transition probabilities</p>
    <p>Model sizes for many large reference tables are still within a few megabytes  Not a problem with current main memory sizes!</p>
    <p>Prune the number of states (say, remove low frequency tokens) to limit the ARM size</p>
  </div>
  <div class="page">
    <p>BMT Topology: Relax Positional Specificity</p>
    <p>A single state per distinct symbol within a category -- emission probability of a symbol within a category is same</p>
  </div>
  <div class="page">
    <p>Feature Hierarchy: Relax Token Specificity [BSD01]</p>
  </div>
  <div class="page">
    <p>Example ARM for Address</p>
  </div>
  <div class="page">
    <p>Robustness Operations: Relax Sequential Specificity</p>
    <p>Make ARMs robust to common errors in the input, i.e., maintain high probability of acceptance despite these errors</p>
    <p>Common types of errors [HS98]  Token deletions  Token insertions  Missing values</p>
    <p>Intuition: Simulate the effects of such erroneous values over each ARM</p>
  </div>
  <div class="page">
    <p>Robustness Operations</p>
    <p>Simulating the effect of token insertions: token and corresponding transition probabilities are copied</p>
    <p>from BEGIN to MIDDLE state</p>
  </div>
  <div class="page">
    <p>Transition Probabilities</p>
    <p>Transitions from BM and BT and MM and MT allowed</p>
    <p>Learned from examples in reference table  Transition probabilities are also weighted by</p>
    <p>their ability to distinguish an attribute  A transition *  * which is common across</p>
    <p>many attributes gets low weight</p>
  </div>
  <div class="page">
    <p>Summary of ARM Instantiation</p>
    <p>BMT topology  Token hierarchy to generalize observed</p>
    <p>patterns  Robustness operations on HMMs to address</p>
    <p>input errors  One state per token in reference table to</p>
    <p>exploit large dictionaries</p>
  </div>
  <div class="page">
    <p>Attribute Order Determination</p>
    <p>If attribute order is known  Can use dynamic programming algorithm to segment [Rabiner89]</p>
    <p>If attribute order is unknown  Can ask the user to provide attribute order  Can discover attribute order</p>
    <p>Nave expensive strategy: evaluate all concatenation orders and segmentations for each input string</p>
    <p>Consistent Attribute Order Assumption: the attribute order is the same across a batch of input tuples</p>
    <p>Several datasets on the web satisfy this assumption  Allows us to efficiently</p>
    <p>Determine the attribute order over a batch of tuples  Segment input strings (using dynamic programming)</p>
  </div>
  <div class="page">
    <p>Segmentation Algorithm (runtime)</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>Reference relations from several domains  Addresses: 1,000,000 tuples</p>
    <p>[Name, #1, #2, Street Address, City, State, Zip]  Media: 280,000 tuples</p>
    <p>[ArtistName, AlbumName, TrackName]  Bibliography: 100,000 tuples</p>
    <p>[Title, Author, Journal, Volume, Month, Year]</p>
    <p>Compare CRAM (our system) with DataMold [BSD01]</p>
  </div>
  <div class="page">
    <p>Test Datasets</p>
    <p>Naturally erroneous datasets: unformatted input strings seen in operational databases</p>
    <p>Media  Customer addresses</p>
    <p>Controlled error injection:  Clean reference table tuples  [Inject errors]  Concatenate</p>
    <p>to generate input strings  Evaluate whether a segmentation algorithm recovered</p>
    <p>the original tuple  Accuracy Measure: % of attribute values correctly recognized</p>
  </div>
  <div class="page">
    <p>Overall Accuracy</p>
    <p>Addresses DBLP</p>
  </div>
  <div class="page">
    <p>Topology &amp; Robustness Operations</p>
    <p>Addresses</p>
  </div>
  <div class="page">
    <p>Training on Hypothetical Error Models</p>
  </div>
  <div class="page">
    <p>Exploiting Dictionaries</p>
    <p>Accuracy vs Reference Table size</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Reference tables leveraged for segmentation  Combining ARMs based on independence</p>
    <p>allows segmenting input strings with unknown attribute order</p>
    <p>ARM models learned over clean reference relations can accurately segment erroneous input strings  BMT topology  Robustness operations  Exploiting large dictionaries</p>
  </div>
  <div class="page">
    <p>Model Sizes &amp; Pruning</p>
    <p>Accuracy #States &amp; Transitions Model Size in MB</p>
  </div>
  <div class="page">
    <p>Order Determination Accuracy</p>
  </div>
  <div class="page">
    <p>Topology</p>
    <p>Media</p>
  </div>
  <div class="page">
    <p>Specificities of HMM Models</p>
    <p>Model specificity restricts accepted token sequences</p>
    <p>Positional specificity  Number ending in th|st can</p>
    <p>only be the 2nd token in an address value</p>
    <p>Token specificity  Last state only accepts st, rd,</p>
    <p>wy, blvd  Sequential specificity</p>
    <p>st, rd, wy, blvd have to follow a number in st|th</p>
  </div>
  <div class="page">
    <p>Robustness Operations</p>
    <p>Token insertion Token deletion Missing values</p>
  </div>
</Presentation>
