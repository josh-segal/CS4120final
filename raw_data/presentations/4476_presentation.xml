<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A General Optimization Framework for Smoothing Language Models on</p>
    <p>Graph Structures</p>
    <p>Qiaozhu Mei, Duo Zhang, ChengXiang Zhai</p>
    <p>University of Illinois at Urbana-Champaign</p>
  </div>
  <div class="page">
    <p>Kullback-Leibler Divergence Retrieval Method</p>
    <p>Document d</p>
    <p>A text mining paper</p>
    <p>data mining</p>
    <p>Doc Language Model (LM) d : p(w|d) text 4/100=0.04</p>
    <p>mining 3/100=0.03 clustering 1/100=0.01  data = 0 computing = 0</p>
    <p>Query q</p>
    <p>Data =0.5 Mining =0.5</p>
    <p>Query Language Model q : p(w|q)</p>
    <p>Data =0.4 Mining =0.4 Clustering =0.1</p>
    <p>? p(w|q)</p>
    <p>text =0.039 mining =0.028 clustering =0.01  data = 0.001 computing = 0.0005  Similarity</p>
    <p>function</p>
    <p>)|(</p>
    <p>)|( log)|()||(</p>
    <p>d</p>
    <p>q q</p>
    <p>Vw dq</p>
    <p>wp</p>
    <p>wp wpD</p>
    <p>Smoothed Doc LM d' : p(w|d)</p>
  </div>
  <div class="page">
    <p>Smoothing a Document Language Model</p>
    <p>Retrieval performance  estimate LM  smoothing LM</p>
    <p>text 4/100 = 0.04 mining 3/100 = 0.03 Assoc. 1/100 = 0.01 clustering 1/100=0.01  data = 0 computing = 0</p>
    <p>text = 0.039 mining = 0.028 Assoc. = 0.009 clustering =0.01  data = 0.001 computing = 0.0005</p>
    <p>Assign non-zero prob. to unseen words</p>
    <p>Estimate a more accurate distribution from sparse data</p>
    <p>text = 0.038 mining = 0.026 Assoc. = 0.008 clustering =0.01  data = 0.002 computing = 0.001</p>
    <p>)|( dMLE wP</p>
    <p>)|( collectionwP  )|()|()1()|( collectiondMLE wPwPdwP</p>
  </div>
  <div class="page">
    <p>Previous Work on Smoothing</p>
    <p>d</p>
    <p>Collection</p>
    <p>d</p>
    <p>Clusters</p>
    <p>d</p>
    <p>Nearest Neighbors</p>
    <p>Collection</p>
    <p>Cluster</p>
    <p>neighbors</p>
    <p>d</p>
    <p>d ~</p>
    <p>Interpolate MLE</p>
    <p>with Reference LM</p>
    <p>Estimate a Reference language model</p>
    <p>ref using the collection (corpus)</p>
    <p>ref</p>
    <p>)|()|()|( refdMLE wPwPdwP</p>
    <p>[Ponte &amp; Croft 98]</p>
    <p>[Liu &amp; Croft 04]</p>
    <p>[Kurland&amp; Lee 04]</p>
  </div>
  <div class="page">
    <p>Problems of Existing Methods</p>
    <p>Smoothing with Global Background  Ignoring collection structure</p>
    <p>Smoothing with Document Clusters  Ignoring local structures inside cluster</p>
    <p>Smoothing using Neighbor Documents  Ignoring global structure</p>
    <p>Different heuristics on ref and interpolation  No clear objective function for optimization</p>
    <p>No guidance on how to further improve the existing methods</p>
  </div>
  <div class="page">
    <p>Research Questions</p>
    <p>What is the right corpus structure to use?</p>
    <p>What are the criteria for a good smoothing method?  Accurate language model?</p>
    <p>What are we ending up optimizing?</p>
    <p>Could there be a general optimization framework?</p>
  </div>
  <div class="page">
    <p>Our Contribution</p>
    <p>Formulation of smoothing as optimization over graph structures</p>
    <p>A general optimization framework for smoothing both document LMs and query LMs</p>
    <p>Novel instantiations of the framework lead to more effective smoothing methods</p>
  </div>
  <div class="page">
    <p>A Graph-based Formulation of Smoothing</p>
    <p>A novel and general view of smoothing</p>
    <p>d</p>
    <p>P(w|d): MLE P(w|d): Smoothed</p>
    <p>P(w|d) = Surface on top of the Graph</p>
    <p>projection on a plain</p>
    <p>Smoothed LM = Smoothed Surface!</p>
    <p>Collection = Graph (of Documents)</p>
    <p>Collection</p>
    <p>P(w|d1) P(w|d2)</p>
    <p>d1 d2</p>
  </div>
  <div class="page">
    <p>Covering Existing Models</p>
    <p>d C1</p>
    <p>C2</p>
    <p>C3</p>
    <p>C4 Background</p>
    <p>Smoothing with Graph Structure</p>
    <p>Smoothing with Nearest Neighbor - Local Graph</p>
    <p>Smoothing with Document Clusters - Forest w/ Pseudo docs</p>
    <p>Smoothing with Global Background - Star graph</p>
    <p>Collection = Graph</p>
    <p>Smoothed LM = Smoothed Surfaces</p>
  </div>
  <div class="page">
    <p>Instantiations of the Formulation</p>
    <p>Language Models to be Smoothed</p>
    <p>Types of Graphs Document LM Query LM</p>
    <p>Star Graph w/ Background Node</p>
    <p>[Ponte &amp; Croft 98], [Hiemstra &amp; Kraaij 98], [Miller et al. 99], [ Zhai &amp; Lafferty 01]</p>
    <p>N/A</p>
    <p>Forest w/ Cluster roots [Liu and Croft 04] N/A</p>
    <p>Local kNN graph [Kurland and Lee 04] [Tao et al. 06]</p>
    <p>N/A</p>
    <p>Document Similarity Graph Novel N/A</p>
    <p>Word Similarity Graph Novel Novel</p>
    <p>Other graphs? ? ?</p>
    <p>Document Graphs</p>
  </div>
  <div class="page">
    <p>Smoothing over Word Graphs</p>
    <p>w</p>
    <p>P(wu|d)/Deg(u)</p>
    <p>Smoothed LM = Smoothed Surface!</p>
    <p>Similarity graph of words</p>
    <p>Given d, {P(w|d)} = Surface over the word graph!</p>
    <p>P(wu|d) P(wv|d)</p>
  </div>
  <div class="page">
    <p>The General Objective of Smoothing</p>
    <p>),(</p>
    <p>)(()1()(</p>
    <p>Evu</p>
    <p>vu Vu</p>
    <p>uu ffvuwffuwCO</p>
    <p>ufuf ~ 2)</p>
    <p>~ )(( uu ffuw</p>
    <p>Fidelity to MLE 2</p>
    <p>),(</p>
    <p>))(,(</p>
    <p>Evu</p>
    <p>vu ffvuw</p>
    <p>Smoothness of the surface</p>
    <p>)(uw</p>
    <p>Importance of vertices</p>
    <p>),( vuw</p>
    <p>- Weights of edges (1/dist.)</p>
  </div>
  <div class="page">
    <p>The Optimization Framework</p>
    <p>),(</p>
    <p>)(()1()(</p>
    <p>Evu</p>
    <p>vu Vu</p>
    <p>uu ffvuwffuwCO</p>
    <p>Criteria:  Fidelity: keep close to the MLE  Surface Smoothness: local and global consistency  Constraint:</p>
    <p>Unified optimization objective:</p>
    <p>Fidelity to MLE Smoothness of the surface</p>
    <p>)(minarg Find :Smoothing COf uf</p>
    <p>u</p>
    <p>w</p>
    <p>dwpd 1)|( ,</p>
  </div>
  <div class="page">
    <p>The Procedure of Smoothing</p>
    <p>Construct a document/word</p>
    <p>graph;</p>
    <p>d</p>
    <p>Vv vuuu</p>
    <p>u</p>
    <p>ffvuwffuDeg f</p>
    <p>CO ))(,(2)</p>
    <p>~ )(()1(2</p>
    <p>)(</p>
    <p>Vv</p>
    <p>vuu f uDeg</p>
    <p>vuw ff</p>
    <p>)(</p>
    <p>),(~ )1(  Iterative updating</p>
    <p>Define reasonable w(u)</p>
    <p>and w(u,v);</p>
    <p>Additional Dirichlet</p>
    <p>Smoothing</p>
    <p>Define reasonable fu</p>
    <p>smoothed</p>
    <p>Evuv</p>
    <p>vuwuDeguw ),(,</p>
    <p>),()()(</p>
    <p>Define graph</p>
    <p>Define surfaces</p>
    <p>Smooth surfaces</p>
  </div>
  <div class="page">
    <p>Smoothing Language Models using a Document Graph</p>
    <p>Construct a kNN graph of documents;</p>
    <p>d w(u): Deg(u) w(u,v): cosine</p>
    <p>Additional Dirichlet</p>
    <p>Smoothing</p>
    <p>fu= p(w|du); or fu= s(q, du);</p>
    <p>uf</p>
    <p>;)|( )(</p>
    <p>),( )|()1()|(</p>
    <p>Vv</p>
    <p>vuMLEu dwP uDeg</p>
    <p>vuw dwPdwP</p>
    <p>Vv</p>
    <p>vuu dqs uDeg</p>
    <p>vuw dqsdqs ),(</p>
    <p>)(</p>
    <p>),( ),(~)1(),(or</p>
    <p>Document language model:</p>
    <p>Alternative: Document relevance score: e.g., (Diaz 05)</p>
  </div>
  <div class="page">
    <p>Smoothing Language Models using a Word Graph</p>
    <p>Construct a kNN graph of</p>
    <p>words;</p>
    <p>w w(u): Deg(u) w(u,v): PMI</p>
    <p>Additional Dirichlet</p>
    <p>Smoothing</p>
    <p>fu=</p>
    <p>uf</p>
    <p>Document language model:</p>
    <p>Query Language Model</p>
    <p>)(</p>
    <p>)|(</p>
    <p>uDeg</p>
    <p>dwP u</p>
    <p>)(</p>
    <p>)|( or</p>
    <p>uDeg</p>
    <p>qwP u</p>
    <p>Vv</p>
    <p>vuMLEu dwP VDeg</p>
    <p>vuw dwPdwP )|(</p>
    <p>)(</p>
    <p>),( )|()1()|(</p>
    <p>Vv</p>
    <p>vuMLEu qwP VDeg</p>
    <p>vuw qwPqwP )|(</p>
    <p>)(</p>
    <p>),( )|()1()|(or</p>
  </div>
  <div class="page">
    <p>Intuitive Interpretation  Smoothing using Word Graph</p>
    <p>Vv</p>
    <p>vuMLu dwP VDeg</p>
    <p>vuw dwPdwP )|()</p>
    <p>)(</p>
    <p>),( )|()1(()|(</p>
    <p>w</p>
    <p>Stationary distribution of a Markov Chain</p>
    <p>w Writing a document = random walk on the word Markov chain; write down w whenever passing w</p>
    <p>)( uvP</p>
  </div>
  <div class="page">
    <p>Intuitive Interpretation  Smoothing using Document Graph</p>
    <p>d</p>
    <p>d1 0</p>
    <p>Vv</p>
    <p>vdwP uDeg</p>
    <p>vuw )|(</p>
    <p>)(</p>
    <p>),(  Absorption Probability</p>
    <p>to the 1 state</p>
    <p>Writing a word w in a document = random walk on the doc Markov chain; write down w if reaching 1</p>
    <p>)1( uP )0( uP</p>
    <p>)( vuP</p>
    <p>Act as neighbors do</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>Data Sets</p>
    <p># docs</p>
    <p>Avg doc length</p>
    <p>queries # relevant docs</p>
    <p>AP88-90 243k 273 51-150 21829</p>
    <p>LA 132k 290 301-400 2350</p>
    <p>SJMN 90k 266 51-150 4881</p>
    <p>TREC8 528k 477 401-450 4728</p>
    <p>Liu and Croft 04Tao 06</p>
    <p>Smooth Document LM on Document Graph (DMDG)</p>
    <p>Smooth Document LM on Word Graph (DMWG)</p>
    <p>Smooth relevance Score on Document Graph (DSDG)  Smooth Query LM on word graph (QMWG)  Evaluate using MAP</p>
  </div>
  <div class="page">
    <p>Effectiveness of the Framework</p>
    <p>Data Sets Dirichlet DMDG DMWG  DSDG QMWG</p>
    <p>AP88-90 0.217 0.254 *** (+17.1%)</p>
    <p>LA 0.247 0.258 ** (+4.5%)</p>
    <p>SJMN 0.204 0.231 *** (+13.2%)</p>
    <p>TREC8 0.257 0.271 *** (+5.4%)</p>
    <p>DMWG: reranking top 3000 results. Usually this yields to a reduced performance than ranking all the documents</p>
    <p>Wilcoxon test: *, **, *** means significance level 0.1, 0.05, 0.01</p>
    <p>Graph-based smoothing &gt;&gt; Baseline Smoothing Doc LM &gt;&gt; relevance score &gt;&gt; Query LM</p>
  </div>
  <div class="page">
    <p>Comparison with Existing Models</p>
    <p>Data Sets</p>
    <p>CBDM (Liu and Croft)</p>
    <p>DELM (Tao et al.)</p>
    <p>DMDG DMDG (1 iteration)</p>
    <p>AP88-90 0.233 0.250 0.254 0.252</p>
    <p>LA 0.259 0.265 0.260 0.258</p>
    <p>SJMN 0.217 0.227 0.235 0.229</p>
    <p>TREC8 N/A 0.267 0.271 0.270</p>
    <p>Graph-based smoothing &gt; state-of-the-art More iterations &gt; Single iteration (similar to DELM)</p>
  </div>
  <div class="page">
    <p>Combined with Pseudo-Feedback</p>
    <p>Data Sets FB FB+QMWG</p>
    <p>AP88-90 0.271 0.273</p>
    <p>LA 0.258 0.267</p>
    <p>SJMN 0.245 0.246</p>
    <p>TREC8 0.278 0.280</p>
    <p>Data Sets DMWG FB FB+DMWG</p>
    <p>AP88-90 0.252 0.266 0.271 **</p>
    <p>LA 0.257 0.257 0.267 **</p>
    <p>SJMN 0.229 0.241 0.249 **</p>
    <p>TREC8 0.271 0.278 0.292 ***</p>
    <p>d 1d</p>
    <p>q</p>
    <p>B F</p>
    <p>w</p>
    <p>smooth</p>
    <p>w</p>
    <p>smooth</p>
    <p>rerank Top docs</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Language modeling in Information Retrieval; smoothing using collection model  (Ponte &amp; Croft 98); (Hiemstra &amp; Kraaij 98); (Miller et al. 99); (Zhai &amp;</p>
    <p>Lafferty 01), etc.</p>
    <p>Smoothing using corpus structures  Cluster structure: (Liu &amp; Croft 04), etc.  Nearest Neighbors: (Kurland &amp; Lee 04), (Tao et al. 06)</p>
    <p>Relevance score propagation (Diaz 05), (Qin et al. 05)</p>
    <p>Graph-based learning  (Zhu et al. 03); (Zhou et al. 04), etc.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Smoothing language models using document/word graphs</p>
    <p>A general optimization framework  Various effective instantiations</p>
    <p>Improved performance over state-of-the-art</p>
    <p>Future Work:  Combine document graphs with word graphs</p>
    <p>Study alternative ways of constructing graphs</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Parameter Tuning</p>
    <p>Fast Convergence</p>
  </div>
</Presentation>
