<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Sockets Direct Protocol Over InfiniBand in Clusters: Is it Beneficial?</p>
    <p>P. Balaji, S. Narravula, K. Vaidyanathan, S. Krishnamoorthy, J. Wu and</p>
    <p>D. K. Panda</p>
    <p>Network Based Computing Laboratory</p>
    <p>The Ohio State University</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Background</p>
    <p>Sockets Direct Protocol (SDP)</p>
    <p>Multi-Tier Data-Centers</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Introduction  Advent of High Performance Networks</p>
    <p>Ex: InfiniBand, Myrinet, 10-Gigabit Ethernet</p>
    <p>High Performance Protocols: VAPI / IBAL, GM, EMP</p>
    <p>Good to build new applications</p>
    <p>Not so beneficial for existing applications</p>
    <p>Built around Portability: Should run on all platforms</p>
    <p>TCP/IP based Sockets: A popular choice</p>
    <p>Performance of Application depends on the Performance of Sockets</p>
    <p>Several GENERIC optimizations for sockets to provide high performance</p>
    <p>Jacobson Optimization: Integrated Checksum-Copy [Jacob89]</p>
    <p>Header Prediction for Single Stream data transfer</p>
    <p>[Jacob89]: An analysis of TCP Processing Overhead, D. Clark, V. Jacobson, J. Romkey and H. Salwen. IEEE Communications</p>
  </div>
  <div class="page">
    <p>Network Specific Optimizations</p>
    <p>Generic Optimizations Insufficient  Unable to saturate high performance networks</p>
    <p>Sockets can utilize some network features  Interrupt Coalescing (can be considered generic)</p>
    <p>Checksum Offload (TCP stack has to modified)</p>
    <p>Insufficient!</p>
    <p>Can we do better?  High Performance Sockets</p>
    <p>TCP Offload Engines (TOE)</p>
  </div>
  <div class="page">
    <p>High Performance Sockets</p>
    <p>High Performance Network</p>
    <p>Pseudo sockets layer</p>
    <p>Application or Library</p>
    <p>Hardware</p>
    <p>Kernel</p>
    <p>User Space</p>
    <p>Sockets OS Agent</p>
    <p>Network Native Protocol</p>
    <p>NIC</p>
    <p>IP</p>
    <p>TCP</p>
    <p>Sockets</p>
    <p>Application or Library</p>
    <p>Hardware</p>
    <p>Kernel</p>
    <p>User Space</p>
    <p>Traditional Berkeley Sockets High Performance Sockets</p>
  </div>
  <div class="page">
    <p>InfiniBand Architecture Overview</p>
    <p>Industry Standard</p>
    <p>Interconnect for connecting compute and I/O nodes</p>
    <p>Provides High Performance</p>
    <p>Low latency of lesser than 5us</p>
    <p>Over 840MBps uni-directional bandwidth</p>
    <p>Provides one-sided communication (RDMA, Remote Atomics)</p>
    <p>Becoming increasingly popular</p>
  </div>
  <div class="page">
    <p>Sockets Direct Protocol (SDP*)  IBA Specific Protocol for Data-Streaming</p>
    <p>Defined to serve two purposes:  Maintain compatibility for existing applications</p>
    <p>Deliver the high performance of IBA to the applications</p>
    <p>Two approaches for data transfer: Copy-based and Z-Copy</p>
    <p>Z-Copy specifies Source-Avail and Sink-Avail messages  Source-Avail allows destination to RDMA Read from source</p>
    <p>Sink-Avail allows source to RDMA Write to the destination</p>
    <p>Current implementation limitations:  Only supports the Copy-based implementation</p>
    <p>Does not support Source-Avail and Sink-Avail</p>
    <p>*SDP implementation from the Voltaire Software Stack</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Background</p>
    <p>Sockets Direct Protocol (SDP)</p>
    <p>Multi-Tier Data-Centers</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Multi-Tier Data-Centers</p>
    <p>Client Requests come over the WAN (TCP based + Ethernet Connectivity)</p>
    <p>Traditional TCP based requests are forwarded to the inner tiers</p>
    <p>Performance is limited due to TCP</p>
    <p>Can we use SDP to improve the data-center performance?</p>
    <p>SDP is not compatible with traditional sockets: Requires TCP termination!</p>
    <p>(Courtesy Mellanox Corporation)</p>
  </div>
  <div class="page">
    <p>Database ServersClients</p>
    <p>Application Servers</p>
    <p>Web Servers</p>
    <p>Proxy Nodes</p>
    <p>Tier 0</p>
    <p>Tier 1</p>
    <p>Tier 2</p>
    <p>Generate requests for both web servers and</p>
    <p>database servers</p>
    <p>TCP Termination Load Balancing</p>
    <p>Caching</p>
    <p>Caching</p>
    <p>Dynamic Content Caching Persistent Connections</p>
    <p>File System evaluation</p>
    <p>Caching Schemes</p>
    <p>Apache</p>
    <p>MySQL</p>
    <p>or</p>
    <p>DB2</p>
    <p>PHP</p>
    <p>Apache</p>
    <p>WAN</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Background</p>
    <p>Sockets Direct Protocol (SDP)</p>
    <p>Multi-Tier Data-Centers</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Experimental Evaluation</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Network</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Compute Node</p>
    <p>Meta-Data Manager</p>
    <p>I/O Server Node</p>
    <p>I/O Server Node</p>
    <p>I/O Server Node</p>
    <p>Meta Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Data</p>
    <p>Relies on Striping of data across different nodes</p>
    <p>Tries to aggregate I/O bandwidth from multiple nodes</p>
    <p>Utilizes the local file system on the I/O Server nodes</p>
  </div>
  <div class="page">
    <p>Parallel I/O in Clusters via PVFS</p>
    <p>PVFS: Parallel Virtual File System  Parallel: stripe/access data across multiple nodes  Virtual: exists only as a set of user-space daemons  File system: common file access methods (open, read/write)</p>
    <p>Designed by ANL and Clemson</p>
    <p>iod</p>
    <p>Local file systems</p>
    <p>iod</p>
    <p>Local file systems</p>
    <p>mgr</p>
    <p>Network</p>
    <p>Posix MPI-IO</p>
    <p>libpvfs</p>
    <p>Applications</p>
    <p>Posix MPI-IO</p>
    <p>libpvfs</p>
    <p>Applications</p>
    <p>Control Data</p>
    <p>PVFS over InfiniBand: Design and Performance Evaluation, Jiesheng Wu, Pete Wyckoff and D. K. Panda. International Conference on Parallel Processing (ICPP), 2003.</p>
  </div>
  <div class="page">
    <p>Presentation Layout</p>
    <p>Introduction and Background</p>
    <p>Sockets Direct Protocol (SDP)</p>
    <p>Multi-Tier Data-Centers</p>
    <p>Parallel Virtual File System (PVFS)</p>
    <p>Experimental Evaluation  Micro-Benchmark Evaluation</p>
    <p>Data-Center Performance</p>
    <p>PVFS Performance</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>Experimental Test-bed</p>
    <p>Eight Dual 2.4GHz Xeon processor nodes</p>
    <p>64-bit 133MHz PCI-X interfaces</p>
    <p>512KB L2-Cache and 400MHz Front Side Bus</p>
    <p>Mellanox InfiniHost MT23108 Dual Port 4x HCAs</p>
    <p>MT43132 eight 4x port Switch</p>
    <p>SDK version 0.2.0</p>
    <p>Firmware version 1.17</p>
  </div>
  <div class="page">
    <p>Latency and Bandwidth Comparison</p>
    <p>SDP achieves 500MBps bandwidth compared to 180MBps of IPoIB  Latency of 27us compared to 31us of IPoIB  Improved CPU Utilization</p>
  </div>
  <div class="page">
    <p>Hotspot Latency</p>
    <p>SDP is more scalable in hot-spot scenarios</p>
  </div>
  <div class="page">
    <p>Data-Center Response Time</p>
    <p>SDP shows very little improvement: Client network (Fast Ethernet) becomes the bottleneck</p>
    <p>Client network bottleneck reflected in the web server delay: up to 3 times improvement with SDP</p>
  </div>
  <div class="page">
    <p>Data-Center Response Time (Fast Clients)</p>
    <p>SDP performs well for large files; not very well for small files</p>
  </div>
  <div class="page">
    <p>Data-Center Response Time Split-up</p>
    <p>IPoIB SDP</p>
  </div>
  <div class="page">
    <p>Data-Center Response Time without Connection Time Overhead</p>
    <p>Without the connection time, SDP would perform well for all file sizes</p>
  </div>
  <div class="page">
    <p>PVFS Performance using ramfs</p>
  </div>
  <div class="page">
    <p>PVFS Performance with sync (ext3fs)</p>
    <p>Clients can push data faster to IODs using SDP; de-stage bandwidth remains the same</p>
  </div>
  <div class="page">
    <p>Conclusions  User-Level Sockets designed with two motives:</p>
    <p>Compatibility for existing applications</p>
    <p>High Performance for modern networks</p>
    <p>SDP was proposed recently along similar lines</p>
    <p>Sockets Direct Protocol: Is it Beneficial?  Evaluated it using micro-benchmarks and real applications</p>
    <p>Multi-Tier Data-Centers and PVFS</p>
    <p>Benefits in environments its good for</p>
    <p>Communication intensive environments such as PVFS</p>
    <p>Demonstrate environments its yet to mature for</p>
    <p>Connection overhead involving environments such as Data-Centers</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Connection Time bottleneck in SDP</p>
    <p>Using dynamic registered buffer pools, FMR techniques, etc</p>
    <p>Using QP pools</p>
    <p>Power-Law Networks</p>
    <p>Other applications: Streaming and Transaction</p>
    <p>Comparison with other high performance sockets</p>
  </div>
  <div class="page">
    <p>For more information, please visit the</p>
    <p>http://nowlab.cis.ohio-state.edu</p>
    <p>Network Based Computing Laboratory,</p>
    <p>The Ohio State University</p>
    <p>Thank You!</p>
    <p>NBC Home Page</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>TCP Termination in SDP</p>
    <p>OS by pass</p>
    <p>Ethernet</p>
    <p>IP</p>
    <p>TCP</p>
    <p>Proxy</p>
    <p>SDP</p>
    <p>Infiniband HW Ethernet</p>
    <p>IP</p>
    <p>TCP</p>
    <p>Sockets</p>
    <p>Browser</p>
    <p>Sockets</p>
    <p>SDP</p>
    <p>Infiniband HW</p>
    <p>Web ServerHTTP HTTP</p>
    <p>HTML HTML</p>
    <p>Personal Notebook/Computer Blade ServersNetwork Service Tier</p>
    <p>Ethernet Communication InfiniBand Communication</p>
  </div>
</Presentation>
