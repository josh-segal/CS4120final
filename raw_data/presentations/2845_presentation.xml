<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Measuring Congestion in High-Performance Datacenter Interconnects</p>
    <p>Saurabh Jha1, Archit Patke1, Jim Brandt2, Ann Gentile2, Benjamin Lim1, Mike Showerman1,3, Greg Bauer1,3, Larry Kaplan4, Zbigniew T. Kalbarczyk1 and William T. Kramer1,3, Ravishankar K. Iyer1,3</p>
  </div>
  <div class="page">
    <p>WRF: Largest Weather forecast simulation</p>
    <p>HPC solves critical science, finance, AI, and other problems</p>
    <p>Hurricane detector using AI (Courtesy: Nvidia)</p>
    <p>High-Performance Computing (HPC)</p>
  </div>
  <div class="page">
    <p>WRF: Largest Weather forecast simulation</p>
    <p>HPC solves critical science, finance, AI, and other problems</p>
    <p>Hurricane detector using AI (Courtesy: Nvidia)</p>
    <p>High-Performance Computing (HPC)</p>
    <p>HPC on Cloud HPC in Academic and National Labs</p>
    <p>NCSA (UIUC) Oakridge National Lab</p>
  </div>
  <div class="page">
    <p>High-Performance Computing (HPC)</p>
    <p>High-speed Networks (HSN)</p>
    <p>Low per-hop latency [1][2]  Low tail-latency variation  High bisection bandwidth</p>
    <p>[1] https://www.nextplatform.com/2018/03/27/in-modern-datacenters-the-latency-tail-wags-the-network-dog/ [2] https://blog.mellanox.com/2017/05/microsoft-enhanced-azure-cloud-efficiency/</p>
  </div>
  <div class="page">
    <p>Networking and Performance Variation</p>
    <p>Despite the low-latency, high-speed networks (HSN) are susceptible to high congestion</p>
    <p>Such congestion can cause up to 2-4X application performance variation in production settings</p>
    <p>R un ti m e (m in )</p>
    <p>RunNumber</p>
    <p>Up to . slowdown compared to median runtime of 282 minutes</p>
    <p>Up to 4 slowdown compared to the median loop iteration time of 2.5 sec</p>
  </div>
  <div class="page">
    <p>Networking and Performance Variation</p>
    <p>Despite low-latency, high-speed networks (HSN) susceptible to high congestion</p>
    <p>Such congestion can lead to up to 2-3X application performance variation in production settings</p>
    <p>Questions:  How often system/applications are experiencing congestion ? [Characterization]</p>
    <p>What are the culprits behind congestion? [Diagnostics]  How to avoid and mitigate effects of congestion ? [Network and System Design]</p>
  </div>
  <div class="page">
    <p>Highlights  Created data mining and ML-driven methodology and associated framework for</p>
    <p>Characterizing network design and congestion problems using empirical data  Identifying factors leading to the congestion on a live system  Checking if the application slowdown was indeed due to congestion</p>
    <p>Empirical evaluation of a real-world large-scale supercomputer: Blue Waters at NCSA  Largest 3D Torus network in the world  5 months of operational data  815,006 unique application runs  70 PB of data injected into the network</p>
    <p>Largest dataset on congestion (first on HPC networks)  Dataset (51 downloads and counting!) and code released</p>
  </div>
  <div class="page">
    <p>Key Findings  HSN congestion is the biggest contributor to app performance variation</p>
    <p>Continuous presence of high congestion regions  Long lived congestion (may persist for &gt;23 hours)</p>
    <p>Default congestion mitigation mechanism have limited efficacy  Only 8 % (261 of 3390 cases) of high congestion cases found using our framework were detected and</p>
    <p>acted by default congestion mitigation algorithm  In ~30% of the cases the default congestion mitigation algorithm was unable to alleviate congestion</p>
    <p>Congestion patterns and their tracking enables identification of culprits behind congestion  critical to system and application performance improvements  E.g., intra-app congestion can be fixed by changing allocation and mapping strategies</p>
    <p>congestion region</p>
  </div>
  <div class="page">
    <p>Congestion in credit-based flow control Network  Focus on evaluation of credit-based flow control transmission protocol</p>
    <p>Flit is the smallest unit of datum that can be transferred</p>
    <p>Flits are not dropped during congestion</p>
    <p>Backpressure (credits) provides congestion control</p>
    <p>link Switch 1 Switch 2</p>
    <p>FLIT</p>
    <p>If credit &gt; 0, flit can be sent</p>
    <p>FLITFLIT</p>
    <p>Available Credits: 3210</p>
    <p>FLIT</p>
    <p>If credit = 0, flit cannot be sent 8</p>
  </div>
  <div class="page">
    <p>Measuring Congestion</p>
    <p>link Switch 1 Switch 2</p>
    <p>Indicates flit waiting (no credit available, allocated buffer full)</p>
    <p>Indicates link is transmitting</p>
    <p>Time</p>
    <p>=</p>
    <p>= 100</p>
    <p>= 41.67 %</p>
    <p>Congestion measured using Percent time stalled (!&quot;)</p>
    <p>: # network cycles in $% measurement interval (fixed value)  : # total cycles the link was stalled in &amp; (i.e., flit was ready to be sent but no credits available.)</p>
  </div>
  <div class="page">
    <p>link 1 Switch 1 Switch 2</p>
    <p>FLIT</p>
    <p>If credit = 0, flit cannot be sent</p>
    <p>FLIT</p>
    <p>FLIT</p>
    <p>Available Credits: 0</p>
    <p>Congestion in credit-based flow control Network</p>
    <p>Switch 3</p>
    <p>Switch 4</p>
    <p>link 2</p>
    <p>link 3</p>
    <p>Insight: Congestion spreads locally (i.e., fans out from an origin point to other senders).</p>
  </div>
  <div class="page">
    <p>link 1 Switch 1 Switch 2</p>
    <p>FLIT</p>
    <p>If credit = 0, flit cannot be sent</p>
    <p>FLIT</p>
    <p>FLIT</p>
    <p>Available Credits: 0</p>
    <p>Congestion in credit-based flow control Network</p>
    <p>Switch 3</p>
    <p>Switch 4</p>
    <p>link 2</p>
    <p>link 3</p>
    <p>Insight: Congestion spreads locally (i.e., fans out from an origin point to other senders).</p>
    <p>Congestion Visualization</p>
    <p>PT S</p>
    <p>(% )</p>
  </div>
  <div class="page">
    <p>New unit for measuring congestion Measure congestion in terms of regions, their size and severity</p>
    <p>Unsupervised clustering</p>
    <p>distance is small: d(x,y)</p>
    <p>stall difference is small: d(xs ys)  p</p>
    <p>Low: 5% &lt; !&quot;  15%</p>
    <p>Med: 15% &lt; !&quot;  25%</p>
    <p>High: 25% &lt; !&quot;</p>
    <p>Neg: 0% &lt; !&quot;  5%</p>
    <p>Raw Congestion Visualization</p>
    <p>PT S</p>
    <p>(% )</p>
  </div>
  <div class="page">
    <p>Congestion Regions Proxy for Performance Evaluation</p>
    <p>Congestion-Informed Segmentation algorithm</p>
    <p>Congestion Regions (CRs) captures relation between congestion severity and application slowdown and therefore can be used for live forensics and debugging!</p>
    <p>(details in paper)</p>
    <p>E x</p>
    <p>e cu</p>
    <p>ti o n T</p>
    <p>im e (</p>
    <p>m in</p>
    <p>s)</p>
    <p>Max of average PTS across all regions overlapping the application topology</p>
    <p>Neg Low Med High</p>
    <p>Low: 5% &lt; '(  15%</p>
    <p>Med: 15% &lt; '(  25%</p>
    <p>High: 25% &lt; '(</p>
  </div>
  <div class="page">
    <p>Infiniband network (IB)</p>
    <p>Metadata Targets (MDT) [110]</p>
    <p>Object Storage Targets (OST) [1N]</p>
    <p>Metadata Servers (MDS)</p>
    <p>Object Storage Servers (OSS)</p>
    <p>MDS 1 MDS 2 OSS 1 OSS 2</p>
    <p>Failover pair Failover pair</p>
    <p>Login nodes</p>
    <p>Import export nodes 3-D Torus Cray Gemini Network</p>
    <p>N &lt;latexit sha1_base64=&quot;ptZDU7z4qZPBBidzKYRMGzDZSmQ=&quot;&gt;AAAB73icdVBNS8NAEN3Ur1q/qh69LBbBU0hqaOut6MWTVLAf0Iay2W7apZtN3J0IpfRPePGgiFf/jjf/jZu2goo+GHi8N8PMvCARXIPjfFi5ldW19Y38ZmFre2d3r7h/0NJxqihr0ljEqhMQzQSXrAkcBOskipEoEKwdjC8zv33PlOaxvIVJwvyIDCUPOSVgpE4PeMQ0vu4XS47tld2a5+GMVM6rtQWpVCvYtZ05SmiJRr/43hvENI2YBCqI1l3XScCfEgWcCjYr9FLNEkLHZMi6hkpi1vjT+b0zfGKUAQ5jZUoCnqvfJ6Yk0noSBaYzIjDSv71M/MvrphDW/CmXSQpM0sWiMBUYYpw9jwdcMQpiYgihiptbMR0RRSiYiAomhK9P8f+kVbbdM9u58Ur1i2UceXSEjtEpclEV1dEVaqAmokigB/SEnq0769F6sV4XrTlrOXOIfsB6+wQueJAS&lt;/latexit&gt;</p>
    <p>Cray Gemini Switch</p>
    <p>Topology: 3D Torus (24x24x24)  Compute nodes : 28K nodes  Avg. Bisection Bandwidth: 17550 GB/sec  Per hop latency: 105 ns [1]</p>
    <p>Courtesy: Cray Inc. (HP)</p>
    <p>System, Monitors, and Datasets</p>
    <p>Blue Waters Networks</p>
    <p>[1] https://wiki.alcf.anl.gov/parts/images/2/2c/Gemini-whitepaper.pdf</p>
  </div>
  <div class="page">
    <p>Network Failures</p>
    <p>Performance counters</p>
    <p>Workload</p>
    <p>Cray Network Monitors</p>
    <p>Lightweight Distributed Metric service (LDMS) [2]</p>
    <p>Scheduler</p>
    <p>Characterization (5 months)</p>
    <p>Live Analytics (60 seconds)</p>
    <p>~</p>
    <p>[2] A. Agelastos et al. Lightweight Distributed Metric Service: A Scalable Infrastructure for Continuous Monitoring of Large-scale Computing Systems and Applications. In SC14: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 154165, 2014.</p>
    <p>Infiniband network (IB)</p>
    <p>Metadata Targets (MDT) [110]</p>
    <p>Object Storage Targets (OST) [1N]</p>
    <p>Metadata Servers (MDS)</p>
    <p>Object Storage Servers (OSS)</p>
    <p>MDS 1 MDS 2 OSS 1 OSS 2</p>
    <p>Failover pair Failover pair</p>
    <p>Login nodes</p>
    <p>Import export nodes 3-D Torus Cray Gemini Network</p>
    <p>N &lt;latexit sha1_base64=&quot;ptZDU7z4qZPBBidzKYRMGzDZSmQ=&quot;&gt;AAAB73icdVBNS8NAEN3Ur1q/qh69LBbBU0hqaOut6MWTVLAf0Iay2W7apZtN3J0IpfRPePGgiFf/jjf/jZu2goo+GHi8N8PMvCARXIPjfFi5ldW19Y38ZmFre2d3r7h/0NJxqihr0ljEqhMQzQSXrAkcBOskipEoEKwdjC8zv33PlOaxvIVJwvyIDCUPOSVgpE4PeMQ0vu4XS47tld2a5+GMVM6rtQWpVCvYtZ05SmiJRr/43hvENI2YBCqI1l3XScCfEgWcCjYr9FLNEkLHZMi6hkpi1vjT+b0zfGKUAQ5jZUoCnqvfJ6Yk0noSBaYzIjDSv71M/MvrphDW/CmXSQpM0sWiMBUYYpw9jwdcMQpiYgihiptbMR0RRSiYiAomhK9P8f+kVbbdM9u58Ur1i2UceXSEjtEpclEV1dEVaqAmokigB/SEnq0769F6sV4XrTlrOXOIfsB6+wQueJAS&lt;/latexit&gt;</p>
    <p>Cray Gemini Switch</p>
    <p>Monitoring logs</p>
    <p>Topology: 3D Torus (24x24x24)  Compute nodes : 28K nodes  Avg. Bisection Bandwidth: 17550 GB/sec  Per hop latency: 105 ns [1]</p>
    <p>Courtesy: Cray Inc. (HP)</p>
    <p>System, Monitors, and Datasets</p>
    <p>Blue Waters Networks</p>
    <p>[1] https://wiki.alcf.anl.gov/parts/images/2/2c/Gemini-whitepaper.pdf</p>
  </div>
  <div class="page">
    <p>Long-lived congestion  Congestion Region can persist up to ~24</p>
    <p>hours (median: 9.7 hours)</p>
    <p>Congestion Region count decreases with increasing duration</p>
    <p># C R s</p>
    <p>CRDuration (mins)</p>
    <p>Low Med High</p>
    <p>Low: 5% &lt; )*  15% Med: 15% &lt; )*  25% High: 25% &lt; )*</p>
  </div>
  <div class="page">
    <p>Before congestion mitigation After congestion mitigation</p>
    <p>Default system congestion detection and mitigation</p>
    <p>Median: 7 hours</p>
    <p>#congestion mitigating triggered : 261</p>
    <p>Median time between events: 7 hours</p>
    <p>Failed to alleviate congestion in 29.8% cases</p>
    <p>Default mitigation throttles all NICs such that</p>
    <p>aggregate traffic injection bandwidth across all nodes &lt; single node bandwidth ejection</p>
  </div>
  <div class="page">
    <p>Only 8 % (261 of 3390 cases) of high congestion cases found by Monet were detected and acted by default congestion mitigation algorithm</p>
    <p>Default system congestion detection and mitigation</p>
    <p>Median: 7 hours, #events: 261</p>
    <p>Monet detection</p>
    <p>Median: 58 minutes, #events: 3390</p>
    <p>Default congestion mitigating triggered : 261</p>
    <p>Median time between events: 7 hours</p>
    <p>Failed to alleviate congestion in 29.8% of the cases</p>
  </div>
  <div class="page">
    <p>App traffic pattern changes</p>
    <p>System load changes</p>
    <p>Link failure</p>
    <p>[1] J Enos et al. Topology-aware job scheduling strategies for torus networks. In Proc. Cray User Group, 2014.</p>
    <p>Network design and congestionaware scheduling</p>
    <p>E.g., topology-aware scheduling [1] improved system throughput by 56% by tuning resource allocation strategies</p>
  </div>
  <div class="page">
    <p>App traffic pattern changes</p>
    <p>System load changes</p>
    <p>Link failure</p>
    <p>[2] Galvez et al. Automatic topology mapping of diverse large-scale parallel applications. In Proceedings of the International Conference on Supercomputing, ICS 17, pages 17:117:10, New York, NY, USA, 2017. ACM.</p>
    <p>Node mapping within the allocation reduces intra-app congestion</p>
    <p>E.g., TopoMapping [2] for finding optimal process rank mapping for the allocated resource</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Developed and validated the proposed methodology on production datasets</p>
    <p>Code and dataset online (51 downloads and counting!)  https://databank.illinois.edu/datasets/IDB-2921318  https://github.com/CSLDepend/monet</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Congestion Visualization on a production Cray Aries (DragonFly Network)</p>
    <p>Developing workload-aware high-speed networks  Inferring and meeting application demands  Optimizing congestion control and routing</p>
    <p>strategies</p>
    <p>Congestion avoidance and mitigation is an ongoing problem !</p>
    <p>Meet us at the poster session!</p>
    <p>Wednesday 6:30 PM - 8:00 PM Cypress Room</p>
  </div>
  <div class="page">
    <p>Questions?</p>
  </div>
</Presentation>
