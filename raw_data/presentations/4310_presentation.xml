<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Transformation Networks for Target-Oriented Sentiment Classification1</p>
    <p>Xin Li1, Lidong Bing2, Wai Lam1, Bei Shi1</p>
    <p>ACL 2018</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 2 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 3 / 25</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Target-Oriented Sentiment Classification (TOSC) is to detect the overall opinions / sentiments of the user review towards the given opinion target.</p>
    <p>TOSC is a supporting task of Target / Aspect-based Sentiment Analysis [5].</p>
    <p>TOSC has been investigated extensively in other names:</p>
    <p>Aspect-level Sentiment Classification [1, 7, 10, 11, 12].  Targeted Sentiment Prediction [6, 14].  Target-Dependent Sentiment Classification [2, 9].</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 4 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 5 / 25</p>
  </div>
  <div class="page">
    <p>Problem Formulation</p>
    <p>TOSC is a typical classification task but the input texts come from two sources:</p>
    <p>TOSC is to predict the overall sentiment of the context towards the target.</p>
    <p>Example</p>
    <p>[Boot time] is super fast, around anywhere from 35 seconds to 1 minute.</p>
    <p>This review conveys positive sentiment over the input Boot time.</p>
    <p>Great [food] but the [service] is dreadful.</p>
    <p>Given the target food, the sentiment polarity is positive while if the input target is service, it becomes negative.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 6 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 7 / 25</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Sentiments towards the targets are usually determined by key phrases.</p>
    <p>Example: This [dish] :::::::::: is my favorite and I always get it and never get</p>
    <p>tired of it. CNN whose aim is to capture the most informative n-grams (e.g., is my favorite) in the sentence should be a suitable model.</p>
    <p>Attention-based weighted combination of the entire word-level features may introduce some noises (e.g., never and tired in above sentence). We employ proximity-based CNN rather than attention-based RNN as the top-most feature extractor.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 8 / 25</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>Example: ::::</p>
    <p>great [food] but the [service] was ::::::: dreadful!</p>
    <p>CNN cannot fully explore the target information via vector concatenation.</p>
    <p>Combining context information and word embedding is an effective way to represent a word in the convolution-based architecture [4] Our Solution:</p>
    <p>(i) We propose a Target-Specific Transformation (TST) component to better consolidate the target information with word representations.</p>
    <p>(ii) We design two context-preserving mechanisms Adaptive Scaling (AS) and Loseless Forwarding (LF) to combine the contextualized representations and the transformed representations.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 9 / 25</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>In the target phrase, different words would not contribute equally to the target representation.</p>
    <p>For example, in amd turin processor, phrase head processor is more important than amd and turin. Our TST solves this problem in two steps:</p>
    <p>(i) Explicitly calculating the importance scores of the target words. (ii) Conducting word-level association between the target and its context.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 10 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 11 / 25</p>
  </div>
  <div class="page">
    <p>Model Overview</p>
    <p>1 2</p>
    <p>1 (0)</p>
    <p>2 (0)</p>
    <p>(0)</p>
    <p>1 ()</p>
    <p>2 ()</p>
    <p>()</p>
    <p>C</p>
    <p>o n</p>
    <p>v o</p>
    <p>lu ti</p>
    <p>o n</p>
    <p>L a</p>
    <p>y e r</p>
    <p>T r a</p>
    <p>n sf</p>
    <p>o r m</p>
    <p>a ti</p>
    <p>o n</p>
    <p>A r c h</p>
    <p>it e c tu</p>
    <p>r e</p>
    <p>B i</p>
    <p>d ir</p>
    <p>e c ti</p>
    <p>o n</p>
    <p>a l</p>
    <p>L S</p>
    <p>T M</p>
    <p>CPT CPT CPT</p>
    <p>CPT CPT CPT</p>
    <p>1 (1)</p>
    <p>2 (1)</p>
    <p>(1)</p>
    <p>Conv2d</p>
    <p>1  2</p>
    <p>TST</p>
    <p>LF/AS</p>
    <p>fully-connected</p>
    <p>CPT</p>
    <p>1  2</p>
    <p>()</p>
    <p>()</p>
    <p>(+1)</p>
    <p>Figure: Architecture of TNet.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 12 / 25</p>
  </div>
  <div class="page">
    <p>Model Overview</p>
    <p>The proposed TNet consists of the following three components:</p>
    <p>Generating contextualized word representations.</p>
    <p>Refining word-level representations with the input target and the contextual information.</p>
    <p>Introducing position information to detect the most salient features more accurately.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 13 / 25</p>
  </div>
  <div class="page">
    <p>Deep Transformation Architecture</p>
    <p>Deep Transformation Architecture stacks multiple Context-Preserving Transformation (CPT) layers</p>
    <p>Deeper network helps to learn more abstract features (He et al., CVPR 2016; Lecun et al., Nature 2015).</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 14 / 25</p>
  </div>
  <div class="page">
    <p>CPT Layer</p>
    <p>The functions of the CPT layer are two folds:</p>
    <p>Generating context-aware target representations ri conditioned on the i-th</p>
    <p>word representation h (l) i fed to the l-th layer:</p>
    <p>ri = m j=1</p>
    <p>hj F(h (l) i ,h</p>
    <p>j ),</p>
    <p>F(h(l)i ,h  j ) =</p>
    <p>exp (h (l)&gt; i h</p>
    <p>j )m</p>
    <p>k=1 exp (h (l)&gt; i h</p>
    <p>k ) ,</p>
    <p>Obtaining target-specific word representations</p>
    <p>h (l) i :</p>
    <p>h (l) i = g(W</p>
    <p>[h (l) i : r</p>
    <p>i ] + b</p>
    <p>),</p>
    <p>1  2</p>
    <p>TST</p>
    <p>fully-connected</p>
    <p>1  2</p>
    <p>()</p>
    <p>()</p>
    <p>Figure: Target-Specific Transformation (TST) component</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 15 / 25</p>
  </div>
  <div class="page">
    <p>CPT Layer</p>
    <p>information back to the transformed word features h (l) i</p>
    <p>(i) Adaptive Scaling (AS) (Similar to Highway Connection [8]):</p>
    <p>t (l) i = (Wtransh</p>
    <p>(l) i + btrans ),</p>
    <p>h (l+1) i = t</p>
    <p>(l) i  h</p>
    <p>(l) i + (1  t</p>
    <p>(l) i ) h</p>
    <p>(l) i .</p>
    <p>(ii) Lossless Forwarding (LF) (Similar to Residual Connection [3]):</p>
    <p>h (l+1) i = h</p>
    <p>(l) i + h</p>
    <p>(l) i .</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 16 / 25</p>
  </div>
  <div class="page">
    <p>Proximity-based Convolutional Feature Extractor</p>
    <p>This component aims to capture the most salient feature w.r.t. the current target for sentiment prediction.</p>
    <p>As observed in (Chen et al., 2017; Li and Lam, 2017), distance information is effective for better locating the salient features.</p>
    <p>Basic idea: Up-weighting the words close to the target and down-weighting those far away from the target.</p>
    <p>Convolutional neural network (Kim, 2014) is used to extract features from the weighted word representations.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 17 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 18 / 25</p>
  </div>
  <div class="page">
    <p>Settings</p>
    <p>Datasets</p>
    <p>LAPTOP, REST: datasets from SemEval14 ABSA challenge, containing the user reviews from laptop domain and restaurant domain respectively.</p>
    <p>TWITTER: a dataset built in (Dong et al., 2014), containing twitter posts and the opinion targets are annotated.</p>
    <p>Compared Models</p>
    <p>Traditional Models:  SVM (Kiritchenko et al., 2014).</p>
    <p>Attention-based Models:  ATAE-LSTM (Wang et al., 2016), MemNet (Tang et al., 2016), IAN</p>
    <p>(Ma et al., 2017), BILSTM-ATT-G (Liu and Zhang, 2017), RAM (Chen et al., 2017).</p>
    <p>Other Neural Models:  AdaRNN (Dong et al., 2014), TD-LSTM (Tang et al., 2016),</p>
    <p>AE-LSTM (Wang et al., 2016), CNN-ASP</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 19 / 25</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 20 / 25</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>Models LAPTOP REST TWITTER</p>
    <p>ACC Macro-F1 ACC Macro-F1 ACC Macro-F1</p>
    <p>TNet variants TNet-LF 76.01, 71.47, 80.79, 70.84 74.68, 73.36,</p>
    <p>TNet-AS 76.54, 71.75, 80.69, 71.27, 74.97, 73.60,</p>
    <p>Baselines</p>
    <p>SVM 70.49\ - 80.16\ - 63.40 63.30</p>
    <p>AdaRNN - - - - 66.30\ 65.90\</p>
    <p>AE-LSTM 68.90\ - 76.60\ - - ATAE-LSTM 68.70\ - 77.20\ - - IAN 72.10\ - 78.60\ - - CNN-ASP 72.46 65.31 77.82 65.11 73.27 71.77 TD-LSTM 71.83 68.43 78.00 66.73 66.62 64.01 MemNet 70.33 64.09 78.16 65.83 68.50 66.91 BILSTM-ATT-G 74.37 69.90 80.38 70.78 72.70 70.84 RAM 75.01 70.51 79.79 68.86 71.88 70.33</p>
    <p>The proposed TNet-LF and TNet-AS consistently outperform the baselines.</p>
    <p>TNet variants perform well on both user reviews (LAPTOP &amp; REST) and twitter posts (TWITTER).</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 21 / 25</p>
  </div>
  <div class="page">
    <p>Ablation Experiment</p>
    <p>Models LAPTOP REST TWITTER</p>
    <p>ACC Macro-F1 ACC Macro-F1 ACC Macro-F1</p>
    <p>TNet variants TNet-LF 76.01, 71.47, 80.79, 70.84 74.68, 73.36,</p>
    <p>TNet-AS 76.54, 71.75, 80.69, 71.27, 74.97, 73.60,</p>
    <p>CPT Alternatives LSTM-ATT-CNN 73.37 68.03 78.95 68.71 70.09 67.68 LSTM-FC-CNN-LF 75.59 70.60 80.41 70.23 73.70 72.82 LSTM-FC-CNN-AS 75.78 70.72 80.23 70.06 74.28 72.60</p>
    <p>Ablated TNet</p>
    <p>TNet w/o transformation 73.30 68.25 78.90 65.86 72.10 70.57 TNet w/o context 73.91 68.87 80.07 69.01 74.51 73.05 TNet-LF w/o position 75.13 70.63 79.86 69.69 73.83 72.49 TNet-AS w/o position 75.27 70.03 79.79 69.78 73.84 72.47</p>
    <p>Using attention (ATT) and fully-connected layer (FC) to replace CPT layer makes the performance worse.</p>
    <p>Each component / element in TNet contributes to the overall performance improvement.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 22 / 25</p>
  </div>
  <div class="page">
    <p>Impact of CPT layer number</p>
    <p>We conduct experiments on the held-out training data of LAPTOP and vary layer number L from 2 to 10, increased by 2.</p>
    <p>Ac cu</p>
    <p>ra cy</p>
    <p>(% )</p>
    <p>TNet-LF TNet-AS</p>
    <p>M ac</p>
    <p>ro -F</p>
    <p>)</p>
    <p>TNet-LF TNet-AS</p>
    <p>Increasing the layer number can increase the performance but the results will go down when L  4 due to the limited training data.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 23 / 25</p>
  </div>
  <div class="page">
    <p>Case Study</p>
    <p>Sentence BILSTM-ATT-G RAM TNet-LF TNet-AS 1. Air has higher [resolution]P but the [fonts]N are small . (N</p>
    <p>N7 N7 P P</p>
    <p>(P, O7, O7, P) (P, P, O7, P) (P, P, P, P) (P, P, P, P)</p>
    <p>P7 P7 N N</p>
    <p>(P, P, P) (P, P, P) (P, P, P) (P, P, P)</p>
    <p>Our TNet can make correct predictions when the opinion is target specific, e.g., long in the 5th and the 6th example.</p>
    <p>TNet can capture the salient features for target sentiment prediction accurately.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 24 / 25</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Our TNet employs CNN as feature extractor to detect the salient features, avoiding introducing the noises.</p>
    <p>Armed with target-specific word representation and proximity information, the TNet variants can predict the sentiment w.r.t. the target more accurately.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 25 / 25</p>
  </div>
  <div class="page">
    <p>References:</p>
    <p>[1] P. Chen, Z. Sun, L. Bing, and W. Yang. Recurrent attention network on memory for aspect sentiment analysis. In Proceedings of EMNLP, pages 463472, 2017.</p>
    <p>[2] L. Dong, F. Wei, C. Tan, D. Tang, M. Zhou, and K. Xu. Adaptive recursive neural network for target-dependent twitter sentiment classification. In Proceedings of ACL, pages 4954, 2014.</p>
    <p>[3] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of CVPR, pages 770778, 2016.</p>
    <p>[4] S. Lai, L. Xu, K. Liu, and J. Zhao. Recurrent convolutional neural networks for text classification. In Proceedings of AAAI, volume 333, pages 22672273, 2015.</p>
    <p>[5] B. Liu. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1167, 2012.</p>
    <p>[6] J. Liu and Y. Zhang. Attention modeling for targeted sentiment. In Proceedings of EACL, pages 572577, 2017.</p>
    <p>[7] D. Ma, S. Li, X. Zhang, and H. Wang. Interactive attention networks for aspect-level sentiment classification. In Proceedings of IJCAI, pages 40684074, 2017.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 25 / 25</p>
  </div>
  <div class="page">
    <p>[8] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.</p>
    <p>[9] D. Tang, B. Qin, X. Feng, and T. Liu. Effective lstms for target-dependent sentiment classification. In Proceedings of COLING, pages 32983307, 2016a.</p>
    <p>[10] D. Tang, B. Qin, and T. Liu. Aspect level sentiment classification with deep memory network. In Proceedings of EMNLP, pages 214224, 2016b.</p>
    <p>[11] Y. Tay, A. T. Luu, and S. C. Hui. Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis. arXiv preprint arXiv:1712.05403, 2017.</p>
    <p>[12] Y. Wang, M. Huang, x. zhu, and L. Zhao. Attention-based lstm for aspect-level sentiment classification. In Proceedings of EMNLP, pages 606615, 2016.</p>
    <p>[13] M. Yang, W. Tu, J. Wang, F. Xu, and X. Chen. Attention based lstm for target dependent sentiment classification. In Proceedings of AAAI, pages 50135014, 2017.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 25 / 25</p>
  </div>
  <div class="page">
    <p>[14] M. Zhang, Y. Zhang, and D.-T. Vo. Gated neural networks for targeted sentiment analysis. In Proceedings of AAAI, pages 30873093, 2016.</p>
    <p>Xin Li, Lidong Bing, Wai Lam, Bei Shi Transformation Networks for Target-Oriented Sentiment ClassificationACL 2018 25 / 25</p>
  </div>
</Presentation>
