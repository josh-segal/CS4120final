<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server</p>
    <p>Shucheng Wang1, Ziyi Lu1, Qiang Cao1, Hong Jiang3, Jie Yao2, Yuanyuan Dong4 and Puyuan Yang4</p>
  </div>
  <div class="page">
    <p>Outline  Background</p>
    <p>SSD-HDD hybrid storage</p>
    <p>Challenge and Motivation Unbalanced device utilization HDD buffered write behavior</p>
    <p>Design Buffer-Controlled Writes Mixed IO scheduler</p>
    <p>Evaluations 2</p>
  </div>
  <div class="page">
    <p>SSD-HDD hybrid storage</p>
    <p>SSD and HDD  SSD (Solid State Drive) has high-speed performance  HDD (Hard Disk Drive) has large capacity and low cost</p>
    <p>SSD-HDD hybrid storage  Applications</p>
    <p>Properties  Cost-effectiveness  High-speed and low latency</p>
  </div>
  <div class="page">
    <p>Unbalanced Device Utilization</p>
    <p>SSDs suffer from high write pressure</p>
    <p>Long tail latency  99th-percentile write latency &gt;</p>
    <p>99.9th-percentile write latency &gt;</p>
    <p>Large queue length  More than  blocked write requests in the queue</p>
    <p>Highly-intensive writes in each SSD  Peak write requests per second &gt;   Data written per day &gt;</p>
  </div>
  <div class="page">
    <p>Unbalanced Device Utilization</p>
    <p>A B C D</p>
    <p>A ve ra ge d ev ic e IO</p>
    <p>ut il iz at io n ( % )</p>
    <p>Workload Type</p>
    <p>SSD HDD</p>
    <p>HDDs are underutilized</p>
    <p>Low HDD utilization  /  / of SSD utilizations</p>
    <p>%  % of times are in idle state</p>
  </div>
  <div class="page">
    <p>We want to exploit the underutilized HDD to relieve the pressure of SSDs in hybrid storage nodes</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>Could the HDDs reach to -level write latency?</p>
    <p>?</p>
    <p>&lt; 1ms Method is feasible when:</p>
    <p>&lt;</p>
  </div>
  <div class="page">
    <p>Motivational Test  Issue sequential and continuous writes to HDDs</p>
    <p>(Append-only) (Close-loop)</p>
    <p>Tested HDD Devices</p>
    <p>Manufacturer Capacity Model Recording Technology Sequential Write Bandwidth (MB/s)</p>
    <p>West Digital 10TB WD100EFAX PMR 200 8TB WD8004FRYZ PMR 180 4TB WD40EZRZ PMR 180</p>
    <p>Seagate 8TB ST8000DM0004 PMR 180 4TB ST4000DM004 SMR 180</p>
  </div>
  <div class="page">
    <p>Results Overview</p>
  </div>
  <div class="page">
    <p>HDD write behaviors</p>
    <p>HDD can reach -level write latency, especially for</p>
    <p>small size requests</p>
  </div>
  <div class="page">
    <p>HDD write behaviors</p>
    <p>Reach us-level write latency, especially for small size requests</p>
    <p>-level latency spikes</p>
    <p>Higher than 10ms for some spikes</p>
  </div>
  <div class="page">
    <p>HDD write behaviors</p>
    <p>Reach to us-level write latency, especially for small size requests</p>
    <p>ms-level latency spikes</p>
    <p>Fixed write latency period</p>
    <p>The length of the fastest write stage is 16MB The interval between two highest latency spikes is 8MB</p>
  </div>
  <div class="page">
    <p>How does this happen?</p>
    <p>Built-in Buffer</p>
    <p>write IO</p>
    <p>HDD Device</p>
    <p>disc</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>HDDs can reach -level write latency</p>
    <p>? How to control these -level latency writes in HDDs?</p>
  </div>
  <div class="page">
    <p>M M M M M M SS</p>
    <p>Write Latency</p>
    <p>F F F F</p>
    <p>Sync</p>
    <p>M M M S</p>
    <p>HDD Buffered-Write Model</p>
    <p>Three types of HDD buffered writes  ast write (low-latency)  id write (mid-latency)  low write (high-latency spike)</p>
  </div>
  <div class="page">
    <p>HDD Buffered-Write Model</p>
    <p>M M M M M M SS</p>
    <p>Write Latency</p>
    <p>F F F F</p>
    <p>Buffered Write Sequence</p>
    <p>Sync</p>
    <p>M M M S</p>
    <p>Wf Wm</p>
    <p>Three types of HDD buffered writes  ast write (low-latency)  id write (mid-latency)  low write (high-latency spike)</p>
    <p>Buffered Write Sequence (after a Sync) Starts with a Fast stage, followed by one or more Slow-and-Mid stage-pairs  Fast stage lasts for  data written  Mid stage lasts for  data written  Slow stage contains an low write</p>
  </div>
  <div class="page">
    <p>Write-state Predictor</p>
    <p>The next HDD write state could be predicted, according to:  Write state of the current request</p>
    <p>Each write request can only be one of the , , or   Current , U and V values</p>
    <p>A:  &lt; U and V  U:   U and V</p>
    <p>operation  Takes the next write state back to</p>
    <p>F M</p>
    <p>Sync</p>
    <p>Write</p>
    <p>S</p>
    <p>Sync</p>
    <p>A</p>
    <p>U</p>
    <p>Sync</p>
    <p>A U</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)  Goal:</p>
    <p>Ensures user writes to be in the  or  write state, avoids writes in the low state.</p>
    <p>Steps:  Perform profiling for all key parameters (if unknown)  Invokes  operation when starting BCW</p>
    <p>To flush all data in the HDD build-in buffer</p>
    <p>Time</p>
    <p>Sync</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>Actively pads non-user data to HDD when there are no user requests  PF: for padding the  and  stage (i.e., 4KB)  PS: for padding the  stage (i.e., 64KB)</p>
    <p>Write user data to HDD when there are user requests in the queue</p>
    <p>Sync</p>
    <p>Sync</p>
    <p>Time</p>
    <p>Time</p>
    <p>USER</p>
    <p>F</p>
    <p>USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>U</p>
    <p>When current data written () in the  and  states are close to the  and  values  The next HDD write state will be predicted as</p>
    <p>S</p>
    <p>Sync</p>
    <p>Time</p>
    <p>USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>When current data written () in the  and  states are close to the  and  values  The next HDD write state will be predicted as   Stop receiving user requests</p>
    <p>Time</p>
    <p>Sync</p>
    <p>USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>U</p>
    <p>PS</p>
    <p>S</p>
    <p>When current data written () in the  and  states are close to the  and  values  The next HDD write state will be predicted as   Stop receiving user requests  Continuously pads PS, until a  write state is detected</p>
    <p>Time</p>
    <p>Sync</p>
    <p>USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>Sync</p>
    <p>When current data written () in the  and  states are close to the  and  values  The next HDD write state will be predicted as   Stop receiving user requests  Continuously pads PS, until a  write state is detected  Start receiving user requests</p>
    <p>Time</p>
    <p>PS</p>
    <p>S USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
  </div>
  <div class="page">
    <p>Buffer-Controlled Writes (BCW)</p>
    <p>V</p>
    <p>Same steps of padding PF and PS in  stages</p>
    <p>PS</p>
    <p>S</p>
    <p>PF</p>
    <p>M</p>
    <p>USER</p>
    <p>M</p>
    <p>PF</p>
    <p>M</p>
    <p>PS</p>
    <p>S</p>
    <p>USER</p>
    <p>M</p>
    <p>PF</p>
    <p>F</p>
    <p>Sync</p>
    <p>USER</p>
    <p>F</p>
    <p>PF</p>
    <p>F</p>
    <p>PF</p>
    <p>F Time</p>
  </div>
  <div class="page">
    <p>Challenges</p>
    <p>HDDs can reach -level write latency</p>
    <p>?</p>
    <p>BCW provides a proactive and controllable buffer writing approach</p>
    <p>How to leverage BCW effectively in the hybrid storage nodes?</p>
  </div>
  <div class="page">
    <p>Mixed IO scheduler (MIOS)  A scheduler that schedules mixed IOs at runtime  Architecture</p>
    <p>A request queue for each SSD and HDD, and monitors their length ()  ()  Log file in each HDD, a device file storing BCW writes in an append-only manner.</p>
    <p>Scheduling Strategy</p>
    <p>User writes</p>
    <p>SSD HDDSSD</p>
    <p>...</p>
    <p>... HDD</p>
    <p>...</p>
    <p>... Log file in HDD</p>
    <p>Request queue</p>
    <p>MIOS HDDflag( )l t</p>
  </div>
  <div class="page">
    <p>Mixed IO scheduler (MIOS)  Key parameters:</p>
    <p>Scheduling Strategy</p>
    <p>User writes</p>
    <p>SSD HDDSSD</p>
    <p>...</p>
    <p>... HDD</p>
    <p>...</p>
    <p>... Log file in HDD</p>
    <p>Request queue</p>
    <p>MIOS HDDflag( )l t</p>
    <p>Threshold :      kkl</p>
    <p>: is HDD available (BCW controlled)?</p>
  </div>
  <div class="page">
    <p>Scheduling strategies in MIOS</p>
    <p>MIOS_D: redirect SSD writes to HDDs when:   is higher than the threshold  AND  HDD is the  or  write state with BCW</p>
    <p>MIOS_E:  Same as MIOS_D when  &gt;   Further perform redirection with just  write state when  &lt;</p>
    <p>SSD HDD</p>
    <p>Redirect</p>
    <p>SSD HDD</p>
    <p>Redirect</p>
  </div>
  <div class="page">
    <p>Evaluation Setup</p>
    <p>System Linux version 4.15.0-52-generic CPU Intel Xeon E5-2696 v4 (2.20 GHz, 22 CPUs)</p>
    <p>Memory 128 GB</p>
    <p>HDDs West Digital 10TB (default)</p>
    <p>West Digital 4TB Seagate 4TB</p>
    <p>SSDs Samsung 960EVO 256GBNVMe, 2000MB/s</p>
    <p>Comparisons  Baseline: Pangu workload replay (writing all data into SSDs)  MIOS_D  MIOS_E</p>
    <p>Evaluation environment</p>
  </div>
  <div class="page">
    <p>Evaluation Setup</p>
    <p>Workload Types A B C D</p>
    <p>Business Cloud</p>
    <p>Computing Cloud Storage Structured Storage</p>
    <p>Structured Storage</p>
    <p>SSD Writes (GB) 14.7 61.2 7.2 7.5</p>
    <p>SSD Write Requests (millions) 0.43 4.4 4.8 4.7</p>
    <p>Note Lowest</p>
    <p>IO intensity Most</p>
    <p>written data</p>
    <p>Workload characteristics</p>
  </div>
  <div class="page">
    <p>Write Performance</p>
    <p>Average, 99qr and 99.9qr-percentile tail latency  Reduced by 65%, 85%, and 95% respectively in workload B  Reduced by 2%, 3.5% and 30% respectively in workload A</p>
  </div>
  <div class="page">
    <p>Queue Length Reduction</p>
    <p>Workload A Minimum (15%) reduction in queue lengths</p>
    <p>Workload B Maximum (95%) reduction in queue lengths</p>
  </div>
  <div class="page">
    <p>SSD Written Data Reduction  MIOS_D:</p>
    <p>Reduced 5.5% compared with Baseline in workload A  Reduced 15.3% and 16% in workload C and D</p>
    <p>MIOS_E:  Reduced 93.3% compared with Baseline in workload A  71% and 72% less than Baseline in workload C and D</p>
  </div>
  <div class="page">
    <p>Write Performance: MIOS_D vs MIOS_E  MIOS_E leads to worse latency performance  MIOS_E leads to 1.4x higher average latency, 1.7x higher 99qr-percentile latency, 6.6x higher 99.9qr-percentile latency than MIOS_D in workload A</p>
    <p>Tradeoff</p>
  </div>
  <div class="page">
    <p>Experiment with other HDDs</p>
    <p>Different types of HDDs do not have a significant impact  The maximum difference of average and tail latency is less than 3%  Amount of data written to and number of requests processed in SSD with different HDDs is less than 5%</p>
    <p>Baseline WD10TB WD 4TB</p>
    <p>SE 4TB</p>
    <p>SSD written data (GB) 61.2 4.1 4.2 4.4</p>
    <p>SSD write requests</p>
    <p>(thousands) 4453 720 724 769</p>
  </div>
  <div class="page">
    <p>Thanks! Q&amp;A</p>
    <p>Email: wsczq@hust.edu.cn</p>
  </div>
</Presentation>
