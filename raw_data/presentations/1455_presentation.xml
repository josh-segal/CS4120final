<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Panache: A Parallel Filesystem Cache for Global file Access</p>
    <p>Renu Tewari (with Dean Hildebrand, Marc Eshel, Manoj Naik, Frank Schmuck, Roger Haskin)</p>
    <p>IBM Almaden</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Global File Access</p>
    <p>Data Source</p>
    <p>Data Centers</p>
    <p>Compute Farms</p>
    <p>Clients</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Characteristics</p>
    <p>High latency links  ~150 ms.</p>
    <p>Bandwidth across data centers is decent  Across data centers OC-48/192  Teragrid 10-30Gb/s</p>
    <p>WAN Network is not reliable  Multiple exchange points, outages, packet loss</p>
    <p>Predominantly large files  Virtual machine images, application virtual disks  Large satellite images  Youtube videos</p>
    <p>Global access but local speeds</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Global performance to match local? With 25 years of Internet experience, weve learned exactly one way to deal with exponential growth:</p>
    <p>Caching. Data has to find local sources near consumers rather than always coming from the place it was originally</p>
    <p>produced -- Van Jacobson, 1995</p>
    <p>A. W. Burks, H. Goldstine, John von Neumann &quot;Preliminary Discussion of the Logical Design of an Electronic Computing Instrument, 1946.</p>
    <p>Ideally one would desire an indefinitely large memory capacity such that any particular word would be immediately available. We are forced to recognize the possibility of constructing a hierarchy of memories, each of which has greater capacity than the preceding but which is less quickly accessible.</p>
    <p>Cache is not only local but also scalable</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>pNFS and parallel Reads 2</p>
    <p>Panache Architecture1</p>
    <p>Summary and Conclusions6</p>
    <p>Namespace Caching5</p>
    <p>Dependent Metadata operations4</p>
    <p>Asynchronous updates 3</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>/home/appl/data/web/spreadsheet.xls</p>
    <p>/home/appl/data/web/drawing.ppt</p>
    <p>Panache Overview /home</p>
    <p>/appl</p>
    <p>/data</p>
    <p>/web</p>
    <p>/home/appl/data/web/drawing.ppt</p>
    <p>GPFS Panache</p>
    <p>Scale out cache</p>
    <p>Storage Array</p>
    <p>Storage node</p>
    <p>Storage node</p>
    <p>Interface node</p>
    <p>Interface node</p>
    <p>/home/appl/data/web/spreadsheet.xls</p>
    <p>Remote user reads local edge device for file</p>
    <p>On demand-read from home site</p>
    <p>Local cache to disk</p>
    <p>ReadCan run disconnected</p>
    <p>Panache</p>
    <p>NF S</p>
    <p>NFS CIFS HTTP VFS</p>
    <p>pNFS</p>
    <p>Gateway node</p>
    <p>Home Site Cluster</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>How did that work</p>
    <p>home</p>
    <p>appl</p>
    <p>data</p>
    <p>web</p>
    <p>home</p>
    <p>appl</p>
    <p>data</p>
    <p>web</p>
    <p>Home directory treeCache directory tree</p>
    <p>Inode: 100 Inode attrs: &lt;  &gt; Remote state: &lt;id: 1024 attrs: mtime, ctime &gt;</p>
    <p>Inode 1024 Inode attrs: &lt;  &gt;</p>
    <p>Independent filesystems  Separate inode space  Home FS is unchanged  Cache is a standalone clustered FS</p>
    <p>LOOKUP GETATTR</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>GPFS GPFS Panache Panache</p>
    <p>User space</p>
    <p>Kernel</p>
    <p>VFS Panache</p>
    <p>GPFS</p>
    <p>pNFS client</p>
    <p>To home site</p>
    <p>Device layer</p>
    <p>Cluster Node1</p>
    <p>VFS</p>
    <p>Panache</p>
    <p>GPFS pNFS client</p>
    <p>Device layer</p>
    <p>Cluster Node2</p>
    <p>To home site</p>
    <p>User space daemon User space daemon</p>
    <p>Panache Internal Architecture</p>
    <p>.</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Key Features</p>
    <p>Miss throughput at WAN bandwidth  Hit throughput matches local access  Writes and metadata updates match local speeds</p>
    <p>Asynchronous write back  Assume high latency to disconnected network by design</p>
    <p>All operations are parallel  Parallel ingest  Parallel access  Parallel update  Parallel data write-back  Parallel metadata write-back (non dependent operations)</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>pNFS and parallel Reads 2</p>
    <p>Panache Architecture1</p>
    <p>Summary and Conclusions6</p>
    <p>Namespace Caching5</p>
    <p>Dependent Metadata operations4</p>
    <p>Asynchronous updates 3</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Storage Back-end</p>
    <p>SAN</p>
    <p>File-based NFS Clients</p>
    <p>GPFS cluster Filesystem</p>
    <p>Data and Metadata Servers</p>
    <p>pNFS Overview</p>
    <p>Linux</p>
    <p>AIX</p>
    <p>Sun</p>
    <p>NFSv4.1 Metadata</p>
    <p>NFSv4.1 Parallel I/O</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Why use pNFS for Data Transfer</p>
    <p>ior reads 8GB files, 8 node server cluster completely saturates the network</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Home clusterCache cluster</p>
    <p>NFSv4</p>
    <p>pNFS</p>
    <p>pNFS client pNFS data servers</p>
    <p>NFS clients NFS servers</p>
    <p>pNFS clients pNFS data servers</p>
    <p>pNFS</p>
    <p>Parallel Ingest Options pNFS metaddataserver</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Cache Hit Performance</p>
    <p>ior reads 8GB files, 10 node cache cluster (5 app + 5 gw), hits match local access</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Cache Miss Performance</p>
    <p>ior reads 8GB files, 10 node cache cluster (5 app + 5 gw), 5 node remote cluster miss limited by network bandwidth</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>pNFS and parallel Reads 2</p>
    <p>Panache Architecture1</p>
    <p>Summary and Conclusions6</p>
    <p>Namespace Caching5</p>
    <p>Dependent Metadata operations4</p>
    <p>Asynchronous updates 3</p>
    <p>Outline</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>/home/appl/data/web/spreadsheet.xls</p>
    <p>/home/appl/data/web/drawing.ppt</p>
    <p>Asynchronous write back</p>
    <p>/home</p>
    <p>/appl</p>
    <p>/data</p>
    <p>/web</p>
    <p>/home/appl/data/web/drawing.ppt</p>
    <p>Storage node</p>
    <p>Storage node</p>
    <p>Interface node</p>
    <p>Interface node</p>
    <p>/home/appl/data/web/spreadsheet.xls</p>
    <p>Remote user writes file to local edge</p>
    <p>device</p>
    <p>Local cache to disk Log write to memory Q</p>
    <p>nw is connected</p>
    <p>Panache scale out cache</p>
    <p>Panache</p>
    <p>Home cluster</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Cache Write Performance</p>
    <p>ior writes 8GB files, 10 node cache cluster (5app+5gw), writes match local access</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Dependent Metadata Operations</p>
    <p>home Tiger woods</p>
    <p>/</p>
    <p>Panache Cluster</p>
    <p>mkdir 200, Tigerwoods2</p>
    <p>mkdir 100, home1</p>
    <p>write 400, 0, 4M4</p>
    <p>create 300, video3</p>
    <p>operation Q</p>
    <p>Home not found (ENOENT)</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Dependent Metadata Operations with Token Chaining</p>
    <p>home Tiger woods</p>
    <p>/</p>
    <p>Panache Cluster</p>
    <p>mkdir 200, Tigerwoods2</p>
    <p>mkdir 100, home1</p>
    <p>write 400, 0, 4M4</p>
    <p>create 300, video3</p>
    <p>operation Q Rename, hard links cross multiple inodes</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Parallel Metadata Throughput</p>
    <p>ops/sec</p>
    <p>Number of Nodes (Appln and Gateway)</p>
    <p>mdtest Metadata Throughput (creates/sec)</p>
    <p>mdtest 1000 creates/node, remote cluster size grows in tandem with gw nodes of cache cluster</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Metadata Flush Throughput (creates/sec)</p>
    <p>Number of Nodes (Appln and Gateway)</p>
    <p>o p s / s e c</p>
    <p>mdtest 1000 creates/node, remote cluster size grows in tandem with gw nodes of cache cluster</p>
    <p>Metadata Flush Throughput</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Consistency Definitions</p>
    <p>Local consistency  Read from a node of the cache cluster returns the last write from any</p>
    <p>node of the cache cluster  Validity Lag</p>
    <p>Time delay between a read at the cache site reflecting the last write at the remote site</p>
    <p>Synchronization Lag   Time delay between a read at the remote site reflecting the last write at</p>
    <p>the cache site.  Close-to-open consistency</p>
    <p>For Open  = 0  For Close  = 0</p>
    <p>When disconnected for time T   T   T</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Conflicts</p>
    <p>In the absence of cross-site locking   In the presence of concurrent updates   Conflicts will happen  Conflict detection</p>
    <p>Based on &lt;ctime, mtime, inode#&gt;</p>
    <p>Conflict resolution  Policy driven per dir tree  No data loss copy to .conflicts dir</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Caching Large Namespaces</p>
    <p>Directory Tree created on demand  Readdir (e.g., ls)</p>
    <p>&lt;name, inode number&gt;</p>
    <p>Readdir plus attrs (e.g., ls l)  &lt; inode attrs&gt; for each dir entry</p>
    <p>Inodes allocated but not created  Directory entry contains orphan inode  create the inode on a later lookup</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Namespace Caching Results</p>
    <p>Readdir from cache</p>
    <p>Readdir w/ orphan inodes</p>
    <p>Readdir w/ creates</p>
    <p>Files per dir</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Summary</p>
    <p>For global file access across data centers  cache should be scalable  Parallel data fetching from/to multiple nodes</p>
    <p>Miss throughput limited by WAN bandwidth</p>
    <p>Hit throughput matches local data access  Update throughput matches local access  All data/metadata updates are asynchronous</p>
    <p>Handle intermittent network connectivity by design</p>
  </div>
  <div class="page">
    <p>2008 IBM Corporation</p>
    <p>Questions?</p>
  </div>
</Presentation>
