<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Scott Wen-tau Yih Joint work with Ming-Wei Chang, Chris Meek, Andrzej Pastusiak</p>
    <p>Microsoft Research</p>
    <p>The 51st Annual Meeting of the Association for Computational Linguistics (ACL-2013)</p>
    <p>Question Answering Using Enhanced Lexical Semantic Models</p>
  </div>
  <div class="page">
    <p>Task  Answer Sentence Selection Given a factoid question, find the sentence that</p>
    <p>Contains the answer Can sufficiently support the answer</p>
    <p>Q: Who won the best actor Oscar in 1973? S1: Jack Lemmon was awarded the Best Actor Oscar for Save</p>
    <p>the Tiger (1973). S2: Academy award winner Kevin Spacey said that Jack</p>
    <p>Lemmon is remembered as always making time for others.</p>
  </div>
  <div class="page">
    <p>Lemmon was awarded the Best Supporting Actor Oscar in 1956 for Mister Roberts (1955) and the Best Actor Oscar for Save the Tiger (1973), becoming the first actor to achieve this rare double</p>
    <p>Source: Jack Lemmon -- Wikipedia</p>
    <p>Who won the best actor Oscar in 1973?</p>
  </div>
  <div class="page">
    <p>Dependency Tree Matching Approaches</p>
    <p>Tree edit-distance [Punyakanok, Roth &amp; Yih, 2004] Represent question and sentence using their dependency trees Measure their distance by the minimal number of edit operations: change, delete &amp; insert</p>
    <p>Quasi-synchronous grammar [Wang et al., 2007] Tree-edit CRF [Wang &amp; Manning, 2010] Discriminative learning on tree-edit features [Heilman &amp; Smith, 2010; Yao et al., 2013]</p>
  </div>
  <div class="page">
    <p>Issues of Dependency Tree Matching</p>
    <p>Dependency tree captures mostly syntactic relations.</p>
    <p>Tree matching is complicated. High run-time cost Computational complexity: [Tai, 1997]</p>
    <p>and are the numbers of nodes respectively of trees and and are the maximum depths respectively of trees and</p>
  </div>
  <div class="page">
    <p>Match the Surface Forms Directly Q: Who won the best actor Oscar in 1973?</p>
    <p>S: Jack Lemmon was awarded the Best Actor Oscar.</p>
    <p>Can matching Q &amp; S directly perform comparably?</p>
  </div>
  <div class="page">
    <p>Match the Surface Forms Directly Q: Who won the best actor Oscar in 1973?</p>
    <p>S: Jack Lemmon was awarded the Best Actor Oscar.</p>
    <p>Using a simple word alignment setting Link words in Q that are related to words in S Determine whether two words can be semantically associated using recently developed lexical semantic models</p>
  </div>
  <div class="page">
    <p>Main Results</p>
    <p>Investigate unstructured and structured models that incorporate rich lexical semantic information</p>
    <p>Enhanced lexical semantic models (beyond WordNet) are crucial in improving performance Simple unstructured BoW models become very competitive</p>
    <p>Outperform previous tree-matching approaches</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction Problem definition Lexical semantic models QA matching models Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Problem Definition</p>
    <p>Supervised setting Question set: Each question is associated with a list of labeled candidate answer sentences:</p>
    <p>Goal: Learn a classifier</p>
  </div>
  <div class="page">
    <p>Assume that there is an underlying structure Describe which words in and can be associated</p>
    <p>What is the fastest car in the world?</p>
    <p>The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.</p>
    <p>Word Alignment View</p>
    <p>h</p>
    <p>Words that are semantically related</p>
    <p>[Harabagiu &amp; Moldovan, 2001]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction Problem definition Lexical semantic models</p>
    <p>Synonymy/Antonymy Hypernymy/Hyponymy (the Is-A relation) Semantic word similarity</p>
    <p>QA matching models Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Synonymy/Antonymy Synonyms can be easily found in a thesaurus Degree of synonymy provides more information</p>
    <p>ship vs. boat</p>
    <p>Polarity Inducing LSA (PILSA) [Yih, Zweig &amp; Platt, EMNLP-CoNLL-12] A vector space model that encodes polarity information Synonyms cluster together in this space Antonyms lie at the opposite ends of a unit sphere</p>
    <p>hot burning</p>
    <p>cold freezing</p>
  </div>
  <div class="page">
    <p>Polarity Inducing Latent Semantic Analysis [Yih, Zweig &amp; Platt, EMNLP-CoNLL-12]</p>
    <p>Acrimony: rancor, conflict, bitterness; goodwill, affection Affection: goodwill, tenderness, fondness; acrimony, rancor</p>
    <p>acrimony rancor goodwill affection</p>
    <p>Group 1: acrimony 4.73 6.01 -5.81 -4.86</p>
    <p>Group 2: affection -3.78 -5.23 6.21 5.15</p>
    <p>Inducing polarity</p>
    <p>Cosine Score:</p>
  </div>
  <div class="page">
    <p>Hypernymy/Hyponymy (the Is-A relation)</p>
    <p>Issues of WordNet taxonomy Limited or skewed concept distribution (e.g., cat woman) Lack of coverage (e.g., apple company, jaguar car)</p>
    <p>Q: What color is Saturn? S: Saturn is a giant gas planet with brown and beige clouds.</p>
    <p>Q: Who wrote Moonlight Sonata? S: Ludwig van Beethoven composed the Moonlight Sonata in 1801.</p>
  </div>
  <div class="page">
    <p>Probase [Wu et al. 2012]</p>
    <p>A KB that contains 2.7 million concepts Relations discovered by Hearst patterns from 1.68 billion Web pages Degree of relations based on frequency of term co-occurrences</p>
    <p>Evaluated on SemEval-12 Relational Similarity [Zhila et al., NAACL-HLT-2013]</p>
    <p>Y is a kind of X  What is the most illustrative example word pair?</p>
    <p>X Y</p>
    <p>automobile van</p>
    <p>wheat bread</p>
    <p>weather rain</p>
    <p>politician senator</p>
    <p>Probase correlates well with human annotations  Spearmans rank correlation coefficient</p>
    <p>(vs. of the previous best system)</p>
  </div>
  <div class="page">
    <p>Semantic Word Similarity A back-off solution when the exact lexical relation is unclear</p>
    <p>Measuring Semantic Word Similarity Vector space model (VSM) Similarity score is derived by cosine</p>
    <p>Heterogeneous VSMs [Yih &amp; Qazvinian, HLT-NAACL-2012] Wikipedia context vectors RNN language model word embedding [Mikolov et al., 2010] Clickthrough-based latent semantic model [Gao et al., SIGIR-2011]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Introduction Problem definition Lexical semantic models QA matching models</p>
    <p>Bag-of-words model Learning latent structures</p>
    <p>Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Bag-of-Words Model (1/2) Word Alignment  Complete bipartite matching</p>
    <p>Every word in question maps to every word in sentence</p>
    <p>What is the fastest car in the world?</p>
    <p>The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.</p>
  </div>
  <div class="page">
    <p>Bag-of-Words Model (2/2) Example is a pair of question and sentence</p>
    <p>,</p>
    <p>Given word relation functions , create a feature vector</p>
    <p>Learning algorithms Logistic Regression (LR) &amp; Boosted Decision Trees (BDT)</p>
  </div>
  <div class="page">
    <p>Latent Word Alignment Structures (1/2)</p>
    <p>Issue of the bag-of-words models Unrelated parts of sentence will be paired with words in question</p>
    <p>Q: Which was the first movie that James Dean was in? S: James Dean, who began as an actor on TV dramas, didnt</p>
    <p>make his screen debut until 1951s Fixed Bayonet.</p>
  </div>
  <div class="page">
    <p>Latent Word Alignment Structures (2/2)</p>
    <p>The latent structure: word alignment with the many-to-one constraints</p>
    <p>Each word in  needs to be linked to a word in . Each word in  can be linked to zero or more words in .</p>
    <p>What is the fastest car in the world?</p>
    <p>The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.</p>
  </div>
  <div class="page">
    <p>Learning Latent Word Alignment Structures</p>
    <p>LCLR Framework [Chang et al., NAACL-HLT 2010] Change the decision function from to</p>
    <p>Candidate sentence  correctly answers question  if and only if the decision can be supported by the best alignment .</p>
    <p>Feature Design</p>
    <p>Objective function</p>
  </div>
  <div class="page">
    <p>Outline Introduction Problem definition Lexical semantic models QA matching models Experiments</p>
    <p>Dataset Evaluation metrics Results</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Dataset [Wang et al., EMNLP-CoNLL-2007]</p>
    <p>Created based on TREC QA data Manual judgment for each question/answer-sentence pair</p>
    <p>Training  Q/A pairs from TREC 8-12 Clean: 5,919 manually judged Q/A pairs (100 questions)</p>
    <p>Development and Test: Q/A pairs from TREC 13 Dev: 1,374 Q/A pairs (84 questions) Test: 1,866 Q/A pairs (100 questions)</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>For each question, rank the candidate sentences Sentences with more than 40 words are excluded Questions with only positive or only negative sentences are excluded (only 68 questions in the test set left)</p>
    <p>Metrics Mean Average Precision (MAP)</p>
    <p>Average Precision: area under the precision-recall curve</p>
    <p>Mean Reciprocal Rank (MRR) =</p>
  </div>
  <div class="page">
    <p>Implementation Details</p>
    <p>Simple tricks that improve the models Removing stop words Features are weighted by the inverse document frequency (IDF) of the question word</p>
    <p>Capturing the importance of words in questions</p>
    <p>Evaluation script Previous work compared results of 68 questions to labels of 72 questions (highest MAP &amp; MRR 0.9444) We have updated results following the same setting.</p>
  </div>
  <div class="page">
    <p>Results  BDT vs. LCLR</p>
    <p>I&amp;L +WN +LS +NER&amp;AnsType 0.55</p>
    <p>BDT LCLR</p>
    <p>M e</p>
    <p>a n</p>
    <p>A ve</p>
    <p>ra g</p>
    <p>e P</p>
    <p>re ci</p>
    <p>si o</p>
    <p>n (</p>
    <p>M A</p>
    <p>P )</p>
    <p>I&amp;L: Identical Word &amp; Lemma Match</p>
  </div>
  <div class="page">
    <p>Results  BDT vs. LCLR</p>
    <p>I&amp;L +WN +LS +NER&amp;AnsType 0.55</p>
    <p>BDT LCLR</p>
    <p>M e</p>
    <p>a n</p>
    <p>A ve</p>
    <p>ra g</p>
    <p>e P</p>
    <p>re ci</p>
    <p>si o</p>
    <p>n (</p>
    <p>M A</p>
    <p>P )</p>
    <p>WN: WordNet Syn, Ant, Hyper/Hypo</p>
  </div>
  <div class="page">
    <p>Results  BDT vs. LCLR</p>
    <p>I&amp;L +WN +LS +NER&amp;AnsType 0.55</p>
    <p>BDT LCLR</p>
    <p>M e</p>
    <p>a n</p>
    <p>A ve</p>
    <p>ra g</p>
    <p>e P</p>
    <p>re ci</p>
    <p>si o</p>
    <p>n (</p>
    <p>M A</p>
    <p>P )</p>
    <p>LS: Enhanced Lexical Semantics</p>
  </div>
  <div class="page">
    <p>Results  BDT vs. LCLR</p>
    <p>I&amp;L +WN +LS +NER&amp;AnsType 0.55</p>
    <p>BDT LCLR</p>
    <p>M e</p>
    <p>a n</p>
    <p>A ve</p>
    <p>ra g</p>
    <p>e P</p>
    <p>re ci</p>
    <p>si o</p>
    <p>n (</p>
    <p>M A</p>
    <p>P )</p>
    <p>NER&amp;AnsType: Named Entity &amp; Answer Type Checking</p>
  </div>
  <div class="page">
    <p>Results  LCLR vs. TED-based Methods</p>
    <p>LCLR* Heilman &amp; Smith, 2010 Yao et al., 2013 0.5</p>
    <p>MAP MRR</p>
    <p>*Updated numbers; different from the version in the proceedings</p>
  </div>
  <div class="page">
    <p>Limitation of Word Matching Models</p>
    <p>Three reasons/sources of errors Uncovered or inaccurate entity relations Lack of robust question analysis Need of high-level semantic representation and inference</p>
    <p>Q: In what film is Gordon Gekko the main character? S: He received a best actor Oscar in 1987 for this role as</p>
    <p>Gordon Gekko in Wall Street.</p>
  </div>
  <div class="page">
    <p>Conclusions Answer sentence selection using word alignment</p>
    <p>Leveraging enhanced lexical semantic models to find semantically related words</p>
    <p>Key findings Rich lexical semantic information improves both unstructured (BoW) and structured (LCLR) models Outperform the dependency tree matching approaches</p>
    <p>Future Work Applications in community QA, paraphrasing, textual entailment High-level semantic representations</p>
  </div>
</Presentation>
