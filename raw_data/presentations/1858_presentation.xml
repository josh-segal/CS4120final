<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Output Regularized Projection</p>
    <p>Kai Yu1, Shipeng Yu12, Volker Tresp1</p>
    <p>kai.yu@siemens.com spyu@dbs.informatik.uni-muenchen.de volker.tresp@siemens.com</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation: A supervised algorithm for dimensionality reduction</p>
    <p>PCA: Unsupervised Projection</p>
    <p>Multi-Output Regularized Projection (MORP)</p>
    <p>Idea: Minimize reconstruction errors of both input and output</p>
    <p>Primal form: Linear mappings</p>
    <p>Dual form: Non-linear mappings</p>
    <p>How to choose between primal and dual form</p>
    <p>Connections to Related Work</p>
    <p>Experimental Results</p>
    <p>Visualization, User preference prediction, multi-label classification</p>
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Motivation</p>
    <p>We are dealing with high-dimensional data in pattern recognition.</p>
    <p>What are the problems?</p>
    <p>Noisy dimensions: Only a small number of dimensions suffice</p>
    <p>Learnability: curse of dimensionality</p>
    <p>Inefficiency: Computational cost is too high</p>
    <p>How to solve these problems? Dimensionality Reduction</p>
    <p>Feature selection: Select part of the dimensions</p>
    <p>Feature transformation/projection: Learn a mapping that maps from the high-dimensional input space into a low-dimensional latent space</p>
    <p>Some notations: We have N documents</p>
    <p>Document i is denoted as xi  X  RM , with output yi  Y  RL</p>
    <p>X  RNM = [x1, . . . , xN ]T , Y  RNL = [y1, . . . , yN ]T</p>
    <p>We aim to derive a mapping  : X 7 V such that V  RK , K &lt; M</p>
  </div>
  <div class="page">
    <p>Principal Component Analysis</p>
    <p>A well-known unsupervised feature transformation method.</p>
    <p>Some formulations</p>
    <p>Mapping directions with largest data covariance</p>
    <p>Best rank K approximation to the data matrix X</p>
    <p>An optimization problem to minimize the reconstruction error:</p>
    <p>min A,V</p>
    <p>X  VA2F s.t. V&gt;V = I,</p>
    <p>with V  RNK the latent semantics, and A  RKM the factor loadings. Drawbacks of PCA:</p>
    <p>PCA is unsupervised and may not be beneficial to supervised learning</p>
    <p>No inter-correlation between X and Y is considered in the mapping</p>
    <p>No intra-correlation between dimensions of Y (if multiple outputs) is considered in the mapping</p>
  </div>
  <div class="page">
    <p>MORP</p>
    <p>The optimization problem solved by MORP (with 0    1):</p>
    <p>min A,B,V</p>
    <p>(1  )X  VA2F + Y  VB 2 F</p>
    <p>s.t. V&gt;V = I, V = XW.</p>
    <p>We are minimizing the reconstruction errors of both X and Y</p>
    <p>We are constraining the mappings to be linear in X</p>
    <p>Denote K = (1  )XX&gt; + YY&gt;. Let [v1, . . . , vN ] be its eigenvectors with eigenvalues 1  . . .  N . We obtain at the optimum,</p>
    <p>A = V&gt;X, B = V&gt;Y;</p>
    <p>V = [v1, . . . , vK]R where R is an arbitrary K  K orthogonal matrix;</p>
    <p>The optimum of the cost function is N</p>
    <p>i=K+1 i;</p>
    <p>Denote W = [w1, . . . , wK], each w solves the optimization problem:</p>
    <p>max wRM</p>
    <p>w&gt;X&gt;KXw s.t. w&gt;X&gt;Xw = 1.</p>
  </div>
  <div class="page">
    <p>MORP: Primal Form</p>
    <p>The optimization problem for w is ill-posed when rank(X) &lt; M.</p>
    <p>One way to deal with this problem is to introduce Tickhonov regularizer:</p>
    <p>min wRM</p>
    <p>w&gt;X&gt;K1Xw + w2</p>
    <p>s.t. w&gt;X&gt;Xw = 1.</p>
    <p>We summarize the primal form of the MORP solution:</p>
    <p>Calculate K = (1  )XX&gt; + YY&gt;; Solve a generalized eigenvalue problem</p>
    <p>[X&gt;K1X + I]w = X&gt;Xw,</p>
    <p>obtain generalized eigenvectors w1, . . . , wK with smallest K eigenvalues 1  . . .  K ; Form mapping functions j(x) =</p>
    <p>jw</p>
    <p>&gt; j x,j = 1, . . . ,K, and finally</p>
    <p>(x) = [1(x), . . . ,K(x)] &gt; defines the mapping .</p>
  </div>
  <div class="page">
    <p>MORP: Dual Form</p>
    <p>Non-linear mappings are obtained by applying representer theorem and defining dual variable  as</p>
    <p>w = X&gt;.</p>
    <p>We summarize the dual form of the MORP solution:</p>
    <p>Calculate Kx, Ky using kernel functions x, y, and K = (1)Kx+Ky; Solve a generalized eigenvalue problem</p>
    <p>[KxK 1Kx + Kx] = K</p>
    <p>obtain generalized eigenvectors 1, . . . , K with smallest K eigenvalues 1  . . .  K ; Form mapping functions j(x) =</p>
    <p>j</p>
    <p>N i=1(j)ix(xi, x), and finally</p>
    <p>(x) = [1(x), . . . ,K(x)] &gt; defines the mapping .</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Which form to choose in real world applications?</p>
    <p>Primal MORP solves an M  M generalized eigenvalue problem  is more efficient when M &lt; N and only learns a linear mapping for X</p>
    <p>Dual MORP solves an N  N generalized eigenvalue problem  is more efficient when N &lt; M for linear mappings</p>
    <p>can learn non-linear mappings with carefully chosen kernel function x</p>
    <p>Two extreme cases of MORP:</p>
    <p>When  = 0, MORP is identical to PCA (primal) and kernel PCA (dual)</p>
    <p>When  = 1, MORP shows similar spirit with kernel dependency estimation (KDE), but is better since MORP has one unified optimization framework</p>
    <p>Other supervised projection methods</p>
    <p>FDA: only focuses on single output with binary classification</p>
    <p>CCA: minimizes inter-correlation but ignores self-correlations</p>
    <p>PLS: is a penalized CCA and focuses on the regression of known outputs</p>
  </div>
  <div class="page">
    <p>Experiment 1: User Preference Prediction</p>
    <p>The Goal: Evaluate projection methods with prediction performance.</p>
    <p>We extract 642 paintings from 47 artists and collect 190 user preference data from an online survey. We select some training users and make predictions for test users based on low-level image features and ratings of other users. For projection methods, a linear SVM classifier is trained on the 50-dimensional latent space.</p>
    <p>MORP is consistently better than other methods</p>
  </div>
  <div class="page">
    <p>Experiment 2: Multi-label Classification</p>
    <p>The Goal: Evaluate projection methods in terms of classification.</p>
    <p>Data set: 1021 images from Corel, with 491 features and 37 categories.</p>
    <p>We manually labeled the data and it has a multi-label setting, i.e., each document can belong to multiple categories.</p>
    <p>We test the following two settings:</p>
    <p>Setting (A): We pick up 70% categories for classification and employ 5-fold cross-validation with one fold training and 4 folds testing</p>
    <p>Setting (B): Evaluate the classification performance on the rest 30% categories for previously unseen data with newly derived features</p>
    <p>For projection methods, linear SVMs are trained on the (non-linearly) projected feature space. For Original Features an SVM with RBF kernel is trained.</p>
  </div>
  <div class="page">
    <p>Results: (setting A: top; setting B: bottom)</p>
    <p>MORP achieves the best performance</p>
    <p>CCA can only obtain effective dimensions less than the number of categories</p>
    <p>Only MORP can obtain significantly better performance than Original Features</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>MORP has the following advantages:</p>
    <p>It is supervised and takes PCA as a special case (when  = 0)</p>
    <p>It considers both the inter-correlation between X and Y, and the intracorrelation of Y</p>
    <p>Both linear and non-linear mappings are easy to derive</p>
    <p>It handles multiple outputs simultaneously</p>
    <p>Experimental results are very encouraging.</p>
  </div>
</Presentation>
