<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Optimizing Network Performance in Distributed Machine Learning</p>
    <p>Luo Mai Chuntao Hong Paolo Costa</p>
  </div>
  <div class="page">
    <p>Machine Learning</p>
    <p>Successful in many fields  Online advertisement</p>
    <p>Spam filtering</p>
    <p>Fraud detection</p>
    <p>Image recognition</p>
    <p>One of the most important workloads in data centers</p>
  </div>
  <div class="page">
    <p>Industry Scale Machine Learning</p>
    <p>More data, higher accuracy</p>
    <p>Scales of industry problems  100 Billions samples, 1TBs  1PBs data</p>
    <p>10 Billions parameters, 1GBs  1TBs data</p>
    <p>Distributed execution  100s  1000s machines</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers</p>
    <p>W1 W2 W3 W4 Data partitions</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers 5</p>
    <p>W1 + 0.1 W2 + 0.2 W3  0.3 W4 +1.2 W1  0.9 W2 + 0.5 W3  0.1 W4  0.5</p>
    <p>gradient</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers</p>
    <p>Parameter server</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers</p>
    <p>Parameter server</p>
    <p>W1 + g1 W2 + g2 W3 + g3 W4 + g4</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers</p>
    <p>Parameter servers</p>
    <p>W3 W4W1 W2</p>
    <p>Use multiple PS to avoid</p>
    <p>bottleneck</p>
  </div>
  <div class="page">
    <p>Distributed Machine Learning</p>
    <p>Data partitions</p>
    <p>Model replicas</p>
    <p>Workers</p>
    <p>Parameter servers</p>
    <p>Bottleneck</p>
  </div>
  <div class="page">
    <p>Inbound Congestion</p>
    <p>Network Core</p>
    <p>Inbound congestion</p>
  </div>
  <div class="page">
    <p>Outbound Congestion</p>
    <p>Outbound congestion</p>
    <p>Network Core</p>
  </div>
  <div class="page">
    <p>Network Core Congestion</p>
    <p>Over-subscribed Network Core</p>
    <p>Congestion in the core in case of over-subscribed</p>
    <p>networks</p>
  </div>
  <div class="page">
    <p>Existing Approaches</p>
    <p>Over-provisioning network Expensive</p>
    <p>Limited deployment scale</p>
    <p>Not available in public clouds Training algorithm</p>
    <p>Fast network H/W e.g., Infiniband and</p>
    <p>RoCE</p>
  </div>
  <div class="page">
    <p>Existing Approaches</p>
    <p>Over-provisioning network Expensive</p>
    <p>Limited deployment scale</p>
    <p>Not available in public Clouds</p>
    <p>Asynchronous training algorithm Training efficiency</p>
    <p>Might not converge</p>
    <p>Asynchronous training algorithm</p>
    <p>Network H/W</p>
  </div>
  <div class="page">
    <p>Rethinking the Network Design</p>
    <p>Training algorithm</p>
    <p>Network H/W</p>
    <p>MLNet</p>
    <p>MLNet is a communication layer designed for distributed machine learning systems</p>
    <p>Improves communication efficiency</p>
    <p>Orthogonal to existing approaches</p>
  </div>
  <div class="page">
    <p>Rethinking the Network Design</p>
    <p>Training algorithm</p>
    <p>Network H/W</p>
    <p>MLNet</p>
    <p>MLNet is a communication layer designed for distributed machine learning systems Improves communication efficiency</p>
    <p>Orthogonal to existing approaches</p>
    <p>Optimizations: Traffic reduction</p>
    <p>Flow prioritization</p>
  </div>
  <div class="page">
    <p>Traffic Reduction</p>
  </div>
  <div class="page">
    <p>Aggregate the gradients from 6 workers</p>
    <p>1 = 11 + 12 + 13 + 14 + 15 + 16</p>
    <p>Traffic Reduction: Key Insight</p>
    <p>Workers</p>
    <p>Parameter server</p>
    <p>Aggregation is commutative and associative</p>
  </div>
  <div class="page">
    <p>+  +  +  +</p>
    <p>Aggregate the gradients from 6 workers</p>
    <p>Traffic Reduction: Key Insight</p>
    <p>Aggregate gradients incrementally does not change the final result</p>
  </div>
  <div class="page">
    <p>Traffic Reduction: Design</p>
    <p>Intercept the push message from the worker to the PS</p>
  </div>
  <div class="page">
    <p>Traffic Reduction: Design</p>
    <p>Redirect the messages to a local worker for partial aggregation</p>
  </div>
  <div class="page">
    <p>Traffic Reduction: Design</p>
    <p>Send the partial results to the PS for final aggregation</p>
  </div>
  <div class="page">
    <p>More details on the paper: 1. Traffic reduction in pull request 2. Asynchronous communication</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization: Key Insight</p>
    <p>Job 1 Job 2 Job 3 Job 4</p>
    <p>These four TCP flows share a bottleneck link and each of</p>
    <p>them gets 25% of its bandwidth</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization: Key Insight</p>
    <p>Flow Completion Time (FCT)</p>
    <p>Average completion time is 4 Model 1 Model 2 Model 3 Model 4</p>
    <p>Job 1</p>
    <p>Job 2</p>
    <p>Job 3</p>
    <p>Job 4</p>
    <p>All flows are delayed! TCP per-flow fairness is harmful in distributed</p>
    <p>machine learning.</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization: Key Insight</p>
    <p>MLNet prioritizes the competing flows to minimize the average training time</p>
    <p>Job 1 Job 2 Job 3 Job 4</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization: Key Insight</p>
    <p>Job 1</p>
    <p>Job 2</p>
    <p>Job 3</p>
    <p>Job 4</p>
    <p>Flow Completion Time (FCT)</p>
    <p>Average completion time is 2Model 1 Model 2 Model 3 Model 4</p>
    <p>Shorten average FCT can largely improve average training time</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Simulate common network topology in data centers  Classic 10Gbps 1024-node data center topology [Fat-Tree, SIGCOMM08]</p>
    <p>Training large scale logistic regression  65B parameters, 141TB dataset [Parameter Server, OSDI14]</p>
    <p>800 workers [Parameter Server, OSDI14]</p>
    <p>With production trace  Data processing rate: uniform(100, 200) MBps</p>
    <p>Synchronize every 30 seconds</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
  </div>
  <div class="page">
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Baseline</p>
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Rack reduces 48% completion time</p>
  </div>
  <div class="page">
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Baseline</p>
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Deploying more parameter servers resolve edge network bottlenecks</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Deploying more parameter servers to reduce training time (1) uses more machines (2) only possible with non-oversubscribed networks</p>
  </div>
  <div class="page">
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Baseline</p>
    <p>Traffic Reduction (1:4 Oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>MLNet reduces congestion in the network core.</p>
    <p>Reduces training time by &gt;70%</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization</p>
    <p>20 jobs running in the same cluster</p>
    <p>C D</p>
    <p>F</p>
    <p>Training time (Hours)</p>
    <p>Baseline Prioritization</p>
    <p>Everyone finish (almost) at the same time</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization</p>
    <p>C D</p>
    <p>F</p>
    <p>Training time (Hours)</p>
    <p>Baseline Prioritization</p>
    <p>Improve the median by 25%</p>
    <p>Delay the tail by 2%</p>
    <p>Better Worse</p>
  </div>
  <div class="page">
    <p>Traffic Prioritization + Traffic Reduction</p>
    <p>C D</p>
    <p>F</p>
    <p>Training time (Hours)</p>
    <p>Baseline Priori. + Red. Reduction</p>
    <p>Improve the median by 60%</p>
    <p>Improve the tail by 54%</p>
    <p>Better Worse</p>
  </div>
  <div class="page">
    <p>More details on the paper: 1. Binary tree aggregation 2. More analysis</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>MLNet can significantly improve the network performance of distributed machine learning  Traffic reduction</p>
    <p>Flow prioritization</p>
    <p>Drop-in solution</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Relaxed fault-tolerance?  When worker fails, drop that portion of data</p>
    <p>Adaptive communication  Reduce synchronization when network is busy?</p>
    <p>Hybrid network infrastructure?  Some with 10GE, some with 40GE ROCE, etc.</p>
    <p>Degree of tree?</p>
  </div>
  <div class="page">
    <p>Traffic Reduction: Design</p>
    <p>Is the local aggregator a new bottleneck?</p>
    <p>Example: 15 workers in a rack</p>
  </div>
  <div class="page">
    <p>Traffic Reduction: Design</p>
    <p>Build a balanced aggregation structure such as a binary tree.</p>
    <p>Example: 15 workers in a rack Binary tree aggregation</p>
  </div>
  <div class="page">
    <p>Traffic Reduction</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g ti</p>
    <p>m e</p>
    <p>(H o</p>
    <p>u rs</p>
    <p>)</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Binary Tree and Rack reduces 78% and 48% completion time</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Deploying more parameter servers resolve edge network bottlenecks</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (Non-oversubscribed Net.)</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Deploying more parameter servers to reduce training time (1) needs more machines (2) only possible with non-oversubscribed networks</p>
  </div>
  <div class="page">
    <p>Traffic Reduction (1:4 Oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
  </div>
  <div class="page">
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Traffic Reduction (1:4 Oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>MLNet reduces congestion in the network core</p>
  </div>
  <div class="page">
    <p>Tr a</p>
    <p>in in</p>
    <p>g t</p>
    <p>im e</p>
    <p>( H</p>
    <p>o u</p>
    <p>rs )</p>
    <p>Number of parameter servers</p>
    <p>Rack Binary Baseline</p>
    <p>Traffic Reduction (1:4 Oversubscribed Net.)</p>
    <p>Better</p>
    <p>Worse</p>
    <p>Cost-effective Expensive</p>
    <p>Binary is consistently consuming more bandwidth than Rack</p>
  </div>
  <div class="page">
    <p>Example: Training a Neural Network</p>
    <p>Random init weight</p>
    <p>Truth: {cat, dog, cat,}</p>
    <p>Calculate error/gradient</p>
    <p>W: {w1, w2, w3, w4}</p>
    <p>G: {g1, g2, g3, g4}</p>
    <p>W: {w1, w2, w3, w4}</p>
    <p>Update weights</p>
  </div>
  <div class="page">
    <p>Example: Neural Network</p>
    <p>Dog : 99%</p>
    <p>Cat : 1%</p>
    <p>Model</p>
    <p>W1</p>
    <p>W4</p>
    <p>W2 W3</p>
    <p>Train Apply</p>
  </div>
  <div class="page">
    <p>Model Training</p>
    <p>Model</p>
    <p>W1</p>
    <p>W4</p>
    <p>W2 W3</p>
    <p>Refine model</p>
    <p>W1</p>
    <p>W4</p>
    <p>W2 W3</p>
    <p>Random Init Final Model</p>
    <p>W1</p>
    <p>W4</p>
    <p>W2 W3</p>
    <p>Converge</p>
  </div>
</Presentation>
