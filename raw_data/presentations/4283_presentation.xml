<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Zhen Yang, Wei Chen, Feng Wang and Bo Xu</p>
    <p>Institute of Automation, Chinese Academy of Sciences</p>
    <p>Unsupervised NMT with Weight Sharing</p>
  </div>
  <div class="page">
    <p>Contents</p>
    <p>Background</p>
    <p>The proposed model</p>
    <p>Experiments and results</p>
    <p>Related and future work 4</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Assumption: different languages can be mapped into one shared-latent space</p>
  </div>
  <div class="page">
    <p>Initialize the model with inferred bilingual dictionary</p>
    <p>Unsupervised word embedding mapping</p>
    <p>Learn strong language model</p>
    <p>De-noising Auto-Encoding</p>
    <p>Convert Unsupervised setting into a supervised one</p>
    <p>Back-translation</p>
    <p>Constrain the latent representation produced by encoders to a shared space</p>
    <p>fully-shared encoder fixed mapped embedding GAN</p>
    <p>Techniques based on</p>
  </div>
  <div class="page">
    <p>We find</p>
    <p>The shared encoder is a bottleneck for unsupervised NMT</p>
    <p>The shared encoder is weak in keeping the unique and internal characteristics of each language, such as the style, terminology and sentence structure. Since each language has its own</p>
    <p>characteristics, the source and target language should be encoded and learned independently.</p>
    <p>Fixed word embedding also weakens the performance (not included in the paper)</p>
    <p>If you are interested about this part, you can find some discussions in our github code:</p>
    <p>https://github.com/ZhenYangIACAS/unsupervised-NMT</p>
  </div>
  <div class="page">
    <p>The proposed model:</p>
    <p>The local GAN is utilized to constrain the source and target latent representations to</p>
    <p>have the same distribution (embedding-reinforced encoder is also designed for this</p>
    <p>purpose, see our paper for detail).</p>
    <p>The global GAN is utilized to fine tune the whole model.</p>
  </div>
  <div class="page">
    <p>Experiment setup:</p>
    <p>Training sets:</p>
    <p>WMT16En-de, WMT14En-Fr, LDC Zn-En</p>
    <p>Note: The monolingual data is built by selecting the front half of the source</p>
    <p>language and the back half of the target language.</p>
    <p>Test sets:</p>
    <p>newstest2016En-de, newstest2014En-Fr, NIST02En-Zh</p>
    <p>Model Architecture:</p>
    <p>Word Embedding:</p>
    <p>applying the Word2vec to pre-train the word embedding</p>
    <p>utilizing Vecmap to map these embedding to a shared-latent space</p>
  </div>
  <div class="page">
    <p>Experimental results:</p>
    <p>Layers for</p>
    <p>sharing</p>
    <p>En-de En-Fr Zh-En</p>
    <p>The effects of the weight-sharing layer number</p>
    <p>Sharing one layer achieves the best translation performance.</p>
  </div>
  <div class="page">
    <p>Experimental results:</p>
    <p>The BLEU results of the proposed model:</p>
    <p>Baseline 1: the word-by-word translation according to the similarity of the word embedding</p>
    <p>Baseline 2: unsupervised NMT with monolingual corpora only proposed by Facebook.</p>
    <p>Upper Bound: the supervised translation on the same model.</p>
  </div>
  <div class="page">
    <p>Experimental results:</p>
    <p>Ablation study</p>
    <p>We perform an ablation study by training multiple versions of our model with some</p>
    <p>missing components: the local GAN, global GAN, the directional self-attention, the</p>
    <p>weight-sharing and the embedding-reinforced encoder.</p>
    <p>We do not test the importance of the auto-encoding, back-translation and the pre-trained</p>
    <p>embeddings since they have been widely tested in previous works.</p>
  </div>
  <div class="page">
    <p>Semi-supervised NMT (with 0.2M parallel data)</p>
    <p>Continue training the model after unsupervised training on the</p>
    <p>parallel data</p>
    <p>From scratch, training the model on monolingual data for one</p>
    <p>epoch, and then on parallel data for one epoch, and another one on</p>
    <p>monolingual data, on and on.</p>
    <p>Models BLEU</p>
    <p>Only with parallel data 11.59</p>
    <p>Fully unsupervised training 10.48</p>
    <p>Continuing Training on supervised data 14.51</p>
    <p>Jointly training on monolingual and parallel data 15.79</p>
  </div>
  <div class="page">
    <p>G. Lample, A. Conneau, L. Denoyer, and M. Ranzato. 2018.</p>
    <p>Unsupervised machine translation using monolingual corpora only.</p>
    <p>In International Conference on Learning Representations (ICLR).</p>
    <p>Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018.</p>
    <p>Unsupervised neural machine translation.</p>
    <p>In International Conference on Learning Representations (ICLR).</p>
    <p>G. Lample, A. Conneau, L. Denoyer, and M. Ranzato. 2018</p>
    <p>Phrase-Based &amp; Neural Unsupervised Machine Translation (arxiv)</p>
    <p>* The newest paper (third one) proposes the shared BPE method for unsupervised</p>
    <p>NMT, its effectiveness is to be verified (around +10 BLEU points improvement is</p>
    <p>presented).</p>
    <p>Related works:</p>
  </div>
  <div class="page">
    <p>Future work:</p>
    <p>Continuing testing the unsupervised NMT and seeking to</p>
    <p>find its optimal configurations.</p>
    <p>Testing the performance of semi-supervised NMT with a</p>
    <p>little amount of bilingual data.</p>
    <p>Investigating more effective approach for utilizing the</p>
    <p>monolingual data in the framework of unsupervised NMT.</p>
  </div>
  <div class="page">
    <p>Code and new results can be found at:</p>
    <p>https://github.com/ZhenYangIACAS/unsupervised-NMT</p>
  </div>
</Presentation>
