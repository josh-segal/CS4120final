<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning to Extract Relations from the Web using Minimal Supervision</p>
    <p>Razvan C. Bunescu Machine Learning Group Department of Computer</p>
    <p>Sciences University of Texas at Austin</p>
    <p>razvan@cs.utexas.edu</p>
    <p>Raymond J. Mooney Machine Learning Group Department of Computer</p>
    <p>Sciences University of Texas at Austin</p>
    <p>mooney@cs.utexas.com</p>
  </div>
  <div class="page">
    <p>Introduction: Relation Extraction</p>
    <p>People are often interested in finding relations between entities:  What proteins interact with IRAK1?</p>
    <p>Which companies were acquired by Google?</p>
    <p>In which city was Mozart born?</p>
    <p>Relation Extraction (RE) is the task of automatically locating predefined types of relations in text documents.</p>
  </div>
  <div class="page">
    <p>Relation Examples: 1) Protein Interactions:</p>
    <p>Introduction: Relation Extraction</p>
    <p>The phosphorylation of Pellino2 by activated IRAK1 could trigger the translocation of IRAKs from complex I to II.</p>
    <p>Search engine giant Google has bought video-sharing website YouTube in a controversial $1.6 billion deal.</p>
    <p>Wolfgang Amadeus Mozart was born to Leopold and Ana Maria</p>
    <p>Mozart, in the front room of Getreidegasse 9 in Salzburg.</p>
  </div>
  <div class="page">
    <p>Motivation: Minimal Supervision</p>
    <p>Developing an RE system usually requires a significant amount of human effort:  Extraction patterns designed by a human expert [Blaschke et al.,</p>
    <p>examples [Zelenko et al., 2003; Culotta and Sorensen, 2004].</p>
    <p>A different RE approach:  Extraction patterns learned from weak supervision derived from a</p>
    <p>significantly reduced amount of human supervision.</p>
  </div>
  <div class="page">
    <p>Relation Extraction with Minimal Supervision</p>
    <p>Human supervision  a handful of pairs of entities known to exhibit (+) or not exhibit () a particular relation.</p>
    <p>Weak supervision  bags of sentences containing the pairs, automatically extracted from a very large corpus.</p>
    <p>Use bags of sentences in a Multiple Instance Learning framework [Dietterich et al., 1997] to train a relation extraction model.</p>
  </div>
  <div class="page">
    <p>Types of Supervision for RE</p>
    <p>Single Instance Learning (SIL):  A corpus of positive and negative sentence examples, with the two</p>
    <p>entity names annotated.  A sentence example is positive iff it explicitly asserts the target</p>
    <p>relationship between the two annotated entities.</p>
    <p>Multiple Instance Learning (MIL):  A corpus of positive and negative bags of sentences.  A bag is positive iff it contains at least one positive sentence</p>
    <p>example.</p>
  </div>
  <div class="page">
    <p>RE from Web with Minimal Supervision</p>
    <p>+/ Argument a1 Argument a2 + Google YouTube</p>
    <p>+ Adobe Systems Macromedia</p>
    <p>+ Viacom DreamWorks</p>
    <p>+ Novartis Eon Labs</p>
    <p>Yahoo Microsoft</p>
    <p>Pfizer Teva</p>
    <p>Example pairs of named entities for R  Corporate Acquisitions.</p>
  </div>
  <div class="page">
    <p>Minimal Supervision: Positive bags</p>
    <p>Use a search engine to extract bags of sentences containing both entities in a pair.</p>
    <p>Google, YouTube S1 Search engine giant Google has bought video-sharing website YouTube in a</p>
    <p>controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
  </div>
  <div class="page">
    <p>Minimal Supervision: Positive bags</p>
    <p>Use a search engine to extract bags of sentences containing both entities in a pair.</p>
    <p>Google, YouTube S1 Search engine giant Google has bought video-sharing website YouTube in a</p>
    <p>controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
  </div>
  <div class="page">
    <p>Minimal Supervision: Positive bags</p>
    <p>Use a search engine to extract bags of sentences containing both entities in a pair.</p>
    <p>Google, YouTube S1 Search engine giant Google has bought video-sharing website YouTube in a</p>
    <p>controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
  </div>
  <div class="page">
    <p>Minimal Supervision: Negative Bags</p>
    <p>Use a search engine to extract bags of sentences containing both entities in a pair.</p>
    <p>Yahoo, Microsoft S1 Yahoo is starting to look more like Microsoft and less like the innovative,</p>
    <p>unified service that got my loyalty in the first place.</p>
    <p>S2 Whatever it is, Yahoo is dashing in front, with Microsoft close behind.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Yahoo and Microsoft teamed up on October 12 to make their instant messaging software compatible.</p>
  </div>
  <div class="page">
    <p>Minimal Supervision: Negative Bags</p>
    <p>Use a search engine to extract bags of sentences containing both entities in a pair.</p>
    <p>Yahoo, Microsoft S1 Yahoo is starting to look more like Microsoft and less like the innovative,</p>
    <p>unified service that got my loyalty in the first place.</p>
    <p>S2 Whatever it is, Yahoo is dashing in front, with Microsoft close behind.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Yahoo and Microsoft teamed up on October 12 to make their instant messaging software compatible.</p>
  </div>
  <div class="page">
    <p>MIL Background: Domains</p>
    <p>Originally introduced to solve a Drug Activity prediction problem in biochemistry [Dietterich et al., 1997]  Each molecule has a limited set of low energy conformations</p>
    <p>bags of 3D conformations.  A bag is positive is at least one of the conformations binds to a</p>
    <p>predefined target.  MUSK dataset [Dietterich et al., 1997]</p>
    <p>A bag is positive if the molecule smells musky.</p>
    <p>Content Based Image Retrieval [Zhang et al., 2002]  Text categorization [Andrews et al., 03], [Ray et al., 05].</p>
  </div>
  <div class="page">
    <p>MIL Background: Algorithms</p>
    <p>Axis Parallel Rectangles [Dietterich, 1997]</p>
    <p>Diverse Density [Maron, 1998]</p>
    <p>Multiple Instance Logistic Regression [Ray &amp; Craven, 05]</p>
    <p>Multi-Instance SVM kernels of [Gartner et al., 2002]</p>
    <p>Normalized Set Kernel.</p>
    <p>Statistic Kernel.</p>
  </div>
  <div class="page">
    <p>MIL for Relation Extraction</p>
    <p>Focus on SVM approaches  Through kernels, can work efficiently with instances that implicitly</p>
    <p>belong to a high-dimensional feature spaces.</p>
    <p>Can reuse existing relation extraction kernels.</p>
    <p>Multi-Instance kernels of [Gartner et al., 2002] not appropriate when very few bags:  Bags (not instances) are considered as training examples.</p>
    <p>The number of SVs is upper bounded by the number of bags</p>
    <p>Very few bags  very few SVs  insufficient capacity.</p>
  </div>
  <div class="page">
    <p>MIL for Relation Extraction</p>
    <p>A simple approach to MIL is to transform it into a standard supervised learning problem:  Apply the bag label to all instances inside the bag.  Train a standard supervised algorithm on the transformed dataset.  Despite class noise, obtains competitive results [Ray &amp; Craven, 05]</p>
    <p>Google, YouTube</p>
    <p>S1 Search engine giant Google has bought video-sharing website YouTube in a controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
  </div>
  <div class="page">
    <p>MIL for Relation Extraction</p>
    <p>A simple approach to MIL is to transform it into a standard supervised learning problem:  Apply the bag label to all instances inside the bag.  Train a standard supervised algorithm on the transformed dataset.  Despite class noise, obtains competitive results [Ray &amp; Craven, 05]</p>
    <p>Google, YouTube</p>
    <p>S1 Search engine giant Google has bought video-sharing website YouTube in a controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>. .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Sn Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to:</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to: Regularization term</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to:</p>
    <p>Error on positive bags</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to:</p>
    <p>Error on negative bags</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to:</p>
    <p>cp, cn &gt; 0, cp+ cn = 1, controls the relative influence that false negative vs. false positives have on the objective function.</p>
    <p>want cp &lt; 0.5 (penalize false negatives less than false positives); used cp = 0.1</p>
  </div>
  <div class="page">
    <p>SVM Framework with MIL Supervision</p>
    <p>np X Xx x</p>
    <p>p n</p>
    <p>X Xx x</p>
    <p>n p</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>L c</p>
    <p>L</p>
    <p>C wJ</p>
    <p>,1)(</p>
    <p>,1)(</p>
    <p>x</p>
    <p>nx</p>
    <p>px</p>
    <p>Xxbxw</p>
    <p>Xxbxw</p>
    <p>minimize:</p>
    <p>subject to:</p>
    <p>Dual formulation  kernel between bag instances K(x1,x2)  (x1)(x2).</p>
    <p>Use SSK  a subsequence kernel customized for relation extraction. [Bunescu &amp; Mooney, 2005]</p>
  </div>
  <div class="page">
    <p>The Subsequence Kernel for Relation Extraction</p>
    <p>Implicit features are sequences of words anchored at the two entity names.</p>
    <p>e1  bought  e2  billion  deal.</p>
    <p>s  a word sequence</p>
    <p>Google has bought video-sharing website YouTube in a controversial $1.6 billion deal.</p>
    <p>g1 1 g2  3 g3  4 g4  0</p>
    <p>x  an example sentence, containing s as a subsequence</p>
    <p>[Bunescu &amp; Mooney, 2005].</p>
    <p>s(x)  the value of feature s in example x 0431),()(   xsgap</p>
    <p>g</p>
    <p>s ix</p>
  </div>
  <div class="page">
    <p>The Subsequence Kernel for Relation Extraction</p>
    <p>K(x1,x2)  (x1)(x2)  the number of common anchored subsequences between x1 and x2, weighted by their total gap.</p>
    <p>Many relations require at least one content word  modify kernel to optionally ignore sequences formed exclusively of stop words and punctuation signs.</p>
    <p>Kernel is computed efficiently by a generalized version of the dynamic programming procedure from [Lodhi et al., 2002].</p>
    <p>[Bunescu &amp; Mooney, 2005].</p>
  </div>
  <div class="page">
    <p>Two Types of Bias</p>
    <p>The MIL approach to RE differs from other MIL problems in two respects:  The training dataset contains very few bags.  The bags can be very large.</p>
    <p>These properties lead to two types of bias:  [Type I] Combinations of words that are correlated to the two</p>
    <p>relation arguments are given too much weight in the learned model.</p>
    <p>[Type II] Words specific to a particular relation instance are given too much weight.</p>
  </div>
  <div class="page">
    <p>Type I Bias</p>
    <p>Google, YouTube S1 Search engine giant Google has bought video-sharing website YouTube</p>
    <p>in a controversial $1.6 billion deal.</p>
    <p>S2 The companies will merge Google's search expertise with YouTube's video expertise, pushing what executives believe is a hot emerging market of video offered over the Internet.</p>
    <p>Overweighted Patterns:</p>
    <p>search  e1  video  e2</p>
    <p>e1  video  e2</p>
    <p>e1  search  e2</p>
    <p>e1  search  e2  video</p>
  </div>
  <div class="page">
    <p>Type II Bias</p>
    <p>Google, YouTube</p>
    <p>S1</p>
    <p>Ever since Google paid $1.65 billion for YouTube in October , plenty of pundits  from Mark Cuban to yours truly  have been waiting for the other shoe to drop.</p>
    <p>S2 Google Gobbles Up YouTube for $1.6 BILLION  October 9, 2006</p>
    <p>S3 Google has acquired social media company YouTube for $1.65 billion in a stock-for-stock transaction as announced by Google Inc. on October 9, 2006.</p>
    <p>Overweighted Patterns:</p>
    <p>e1  for  e2  October</p>
    <p>e1  has  e2  October</p>
  </div>
  <div class="page">
    <p>A Solution for Type I Bias</p>
    <p>Use the SSK approach, with new feature weight:</p>
    <p>sw</p>
    <p>xsgap s wx )()(</p>
    <p>),( ),()( xsgaps x</p>
    <p>Modify subsequence kernel computations to use word weights (w).</p>
    <p>Want small (w) for words w correlated with either of the two relation arguments.</p>
  </div>
  <div class="page">
    <p>A Solution for Type I Bias: Word Weights</p>
    <p>),(</p>
    <p>)()..|(),( )( 21</p>
    <p>wXC</p>
    <p>XCaXaXwPwXC w</p>
    <p>Use a formula for word weights (w) that discounts the effect of correlations of w with either of the two arguments a1 and a2.</p>
  </div>
  <div class="page">
    <p>A Solution for Type I Bias: Word Weights</p>
    <p>),(</p>
    <p>)()..|(),( )( 21</p>
    <p>wXC</p>
    <p>XCaXaXwPwXC w</p>
    <p>The # of sentences in bag X.</p>
  </div>
  <div class="page">
    <p>A Solution for Type I Bias: Word Weights</p>
    <p>),(</p>
    <p>)()..|(),( )( 21</p>
    <p>wXC</p>
    <p>XCaXaXwPwXC w</p>
    <p>The # of sentences in bag X that contain word w.</p>
  </div>
  <div class="page">
    <p>A Solution for Type I Bias: Word Weights</p>
    <p>),(</p>
    <p>)()..|(),( )( 21</p>
    <p>wXC</p>
    <p>XCaXaXwPwXC w</p>
    <p>The probability that the word w appears in a sentence due only to the presence of X.a1 or X.a2, assuming X.a1 and X.a2 are independent causes for w.</p>
    <p>)).|(1()).|(1(1)..|( 2121 aXwPaXwPaXaXwP</p>
    <p>).|().|().|().|( 2121 aXwPaXwPaXwPaXwP</p>
    <p>P(w|a) is the probability that w appears in a sentence due to the presence of a.  Estimate P(w|a) using counts from a separate bag of sentences containing a.</p>
  </div>
  <div class="page">
    <p>MIL Relation Extraction Datasets</p>
    <p>Given two arguments a1 and a2, submit query string a1 * * * * * * * a2 to Google.</p>
    <p>Download the resulting documents (less than 1000).</p>
    <p>Split text into sentences and tokenize using the OpenNLP package.</p>
    <p>Keep only sentences containing both a1 and a2.</p>
    <p>Replace closest occurrences of a1 and a2 with generic tags e1 and e2 .</p>
  </div>
  <div class="page">
    <p>MIL Relation Extraction Datasets</p>
    <p>+/ Argument a1 Argument a2 Bag size</p>
    <p>+ Google YouTube 1375</p>
    <p>+ Adobe Systems Macromedia 622</p>
    <p>+ Viacom DreamWorks 323</p>
    <p>+ Novartis Eon Labs 311</p>
    <p>Yahoo Microsoft 163</p>
    <p>Pfizer Teva 247</p>
    <p>+ Pfizer Rinat Neuroscience 50 (41)</p>
    <p>+ Yahoo Inktomi 433 (115)</p>
    <p>Google Apple 281</p>
    <p>Viacom NBC 231</p>
    <p>Training Pairs</p>
    <p>Testing Pairs manually labeled all bag sentences</p>
    <p>Corporate Acquisitions Dataset</p>
  </div>
  <div class="page">
    <p>MIL Relation Extraction Datasets</p>
    <p>+/ Argument a1 Argument a2 Bag size</p>
    <p>+ Franz Kafka Prague 522</p>
    <p>+ Andre Agassi Las Vegas 386</p>
    <p>+ Charlie Chaplin London 292</p>
    <p>+ George Gershwin New York 260</p>
    <p>Luc Besson New York 74</p>
    <p>W. A. Mozart Vienna 288</p>
    <p>+ Luc Besson Paris 126 (6)</p>
    <p>+ Marie Antoinette Vienna 39 (10)</p>
    <p>Charlie Chaplin Hollywood 266</p>
    <p>George Gershwin London 104</p>
    <p>Training Pairs</p>
    <p>PersonBirthplace Dataset</p>
    <p>Testing Pairs manually labeled all bag sentences</p>
  </div>
  <div class="page">
    <p>Experimental Results: Systems</p>
    <p>[SSK-MIL] MIL formulation using the original SSK.</p>
    <p>[SSK-T1] MIL formulation with the SSK modified to use word weights in order to reduce Type I bias.</p>
    <p>[BW-MIL] MIL formulation using a bag-of-words kernel.</p>
    <p>[SSK-SIL] SIL formulation using the original subsequence kernel:</p>
    <p>Use manually labeled instances from the test bags.</p>
    <p>Train on instances from one positive bag and one negative bag, test on instances from the other two bags.</p>
    <p>Average results over all four combinations.</p>
  </div>
  <div class="page">
    <p>Experimental Results: Evaluation</p>
    <p>vary a threshold on the extraction confidence.</p>
  </div>
  <div class="page">
    <p>Company Acquisitions</p>
  </div>
  <div class="page">
    <p>PersonBirthplace</p>
  </div>
  <div class="page">
    <p>Experimental Results: AUC</p>
    <p>SSK-T1 is significantly more accurate than SSK-MIL.</p>
    <p>SSK-T1 is competitive with SSK-SIL, however:</p>
    <p>SSK-T1 supervision  only 6 pairs (4 positive).</p>
    <p>SSK-SIL average supervision:</p>
    <p>~500 manually labeled sentences (78 positive) for Acquisitions.</p>
    <p>~300 manually labeled sentences (22 positive) for Birthplaces.</p>
    <p>Dataset SSK-MIL SSK-T1 BW-MIL SSK-SIL</p>
    <p>Company Acquisitions 76.9% 81.1% 45.8% 80.4%</p>
    <p>People Birthplace 72.5% 78.2% 69.2% 73.4%</p>
  </div>
  <div class="page">
    <p>Applications &amp; Extensions</p>
    <p>A Google Sets system for relation extraction  Ideally, the user provides only positive pairs.  Likely negative examples are created by pairing the argument</p>
    <p>entity with other named entities in the same sentence.  Any pair of entities different from the relation pair is likely to be</p>
    <p>negative  implicit negative evidence.</p>
    <p>Google YouTube</p>
    <p>Adobe Systems Macromedia</p>
    <p>Viacom DreamWorks</p>
    <p>Novartis Eon Labs</p>
    <p>Pfizer Rinat Neuroscience</p>
    <p>Yahoo Inktomi . .</p>
    <p>. .</p>
    <p>. .</p>
    <p>Input Output 42</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>Investigate methods for reducing Type II bias.</p>
    <p>Experiment with other, more sophisticated MIL algorithms.</p>
    <p>Explore the effect of Type I and Type II bias when using dependency information in the relation extraction kernel.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Presented a new approach to Relation Extraction, trained using only a handful of pairs of entities known to exhibit or not exhibit the target relationship.</p>
    <p>Extended an existing subsequence kernel to resolve problems caused by the minimal supervision provided.</p>
    <p>The new MIL approach is competitive with its SIL counterpart that uses significantly more human supervision.</p>
  </div>
</Presentation>
