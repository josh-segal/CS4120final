<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Probabilistic Near-Duplicate Detection Using Simhash</p>
    <p>Probabilistic NearProbabilistic Near--Duplicate Duplicate Detection Using SimhashDetection Using Simhash</p>
    <p>Sadhan Sood, Dmitri Loguinov Presented by Matt SmithPresented by Matt Smith</p>
    <p>Internet Research LabInternet Research Lab Department of Computer Science and EngineeringDepartment of Computer Science and Engineering</p>
    <p>Texas A&amp;M UniversityTexas A&amp;M University</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>IntroductionIntroductionIntroduction</p>
    <p>Similarity matching is a common task in data mining; we are often interested in knowing which documents of a collection are similar</p>
    <p>to each other</p>
    <p>Usually involves representing documents by ddimensional feature vectors and comparing those,</p>
    <p>but all-to-all comparison is infeasible for large collections</p>
    <p>Approximation algorithms such as simhash, trading some precision and recall for speed, are a promising technique for use on large collections</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>IntroductionIntroductionIntroduction</p>
    <p>Simhash replaces a documents feature vector with a fixed-size fingerprint that preserves cosine similarity of the original vector space</p>
    <p>Main challenge: quickly find all pairs of fingerprints within a certain Hamming distance h of each other</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>MotivationMotivationMotivation</p>
    <p>A</p>
    <p>feature vector represents the subset of features present in a given document u of the collection, each feature being described by a real-valued weight</p>
    <p>Given typical values for average feature count and storage required per feature (e.g., 141 and 8 bytes respectively), all-to-all comparisons are completely infeasible</p>
    <p>A conversion to a fixed-size fingerprint of the feature vector (as done by Simhash) helps with storage and computational complexity concerns</p>
    <p>Manku et al. [2007] showed that 64 bits is generally enough to capture similarity of much larger feature vectors</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>MotivationMotivationMotivation</p>
    <p>Even with the much faster Hamming distance calculation on this fingerprint, a sub-quadratic technique will be very desirable:</p>
    <p>(Table is across all n pairs of crawled webpages)</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>ObjectivesObjectivesObjectives</p>
    <p>We consider two classes of matching problems</p>
    <p>Clustering: given one page, find all of its matches or near-duplicates</p>
    <p>Duplicate elimination: determine if there exists at least one match in the collection, without finding all matching documents</p>
    <p>Can allow us to improve performance significantly by skipping the exhaustive search</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>SimhashSimhashSimhash</p>
    <p>The simhash algorithm operates as follows:</p>
    <p>Initialize a vector W of weights to 0</p>
    <p>Each feature i (word on a webpage, etc) is hashed with a uniformly random function</p>
    <p>For each bit j of hash i</p>
    <p>, add or subtract the feature weight wi to/from Wj based on whether the bit is 0 or 1</p>
    <p>Example: Feature Hash weight word1 0101 0.05 -0.05 +0.05 -0.05 +0.05</p>
    <p>word2 1101 0.02 +0.02 +0.02 -0.02 +0.02</p>
    <p>word3 0001 0.01 -0.01 -0.01 -0.01 +0.01</p>
    <p>word4 1110 0.03 +0.03 +0.03 +0.03 -0.03</p>
    <p>word5 0100 0.05 -0.05 +0.05 -0.05 -0.05</p>
    <p>word6 0011 0.09 -0.09 -0.09 +0.09 +0.09</p>
    <p>weight -0.15 +0.05 -0.01 +0.09</p>
    <p>simhash 0 1 0 1</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>SimhashSimhashSimhash</p>
    <p>Next, we examine the issue of which bits are likely to differ between similar</p>
    <p>documents</p>
    <p>Or, put another way, how likely it is for a given bit in the simhash to flip given minor changes to a document</p>
    <p>Details of the model can be found in the paper; we just give an illustrative example here</p>
    <p>Main observation: examining the simhash weight vector, typically discarded, gives us insight into the bit-flipping question</p>
    <p>The bit with the smallest absolute weight value is the one most likely to be flipped by small changes to the document</p>
    <p>called a weak</p>
    <p>bit</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>SimhashSimhashSimhash</p>
    <p>Consider making changes to the document represented in the previous table with simhash value 0101:</p>
    <p>Feature Hash weight word1 0101 0.05 -0.05 +0.05 -0.05 +0.05 word2 1101 0.02 +0.02 +0.02 -0.02 +0.02 word3 0001 0.01 -0.01 -0.01 -0.01 +0.01 word4 1110 0.03 +0.03 +0.03 +0.03 -0.03 word5 0100 0.05 -0.05 +0.05 -0.05 -0.05 word6 0011 0.09 -0.09 -0.09 +0.09 +0.09</p>
    <p>weight -0.15 +0.05 -0.01 +0.09 simhash 0 1 0 1</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>SimhashSimhashSimhash</p>
    <p>Removing two unimportant features (e.g., with the lowest weights):</p>
    <p>Feature Hash weight word1 0101 0.05 -0.05 +0.05 -0.05 +0.05 word2 1101 0.02 +0.02 +0.02 -0.02 +0.02 word3 0001 0.01 -0.01 -0.01 -0.01 +0.01 word4 1110 0.03 +0.03 +0.03 +0.03 -0.03 word5 0100 0.05 -0.05 +0.05 -0.05 -0.05 word6 0011 0.09 -0.09 -0.09 +0.09 +0.09</p>
    <p>weight -0.16 +0.04 +0.02 +0.06 simhash 0 1 1 1</p>
    <p>single bit</p>
    <p>change</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>SimhashSimhashSimhash</p>
    <p>Removing two important features (e.g., with higher than average weights):</p>
    <p>Note that bit 3 still flips, as last time</p>
    <p>Feature Hash weight word1 0101 0.05 -0.05 +0.05 -0.05 +0.05 word2 1101 0.02 +0.02 +0.02 -0.02 +0.02 word3 0001 0.01 -0.01 -0.01 -0.01 +0.01 word4 1110 0.03 +0.03 +0.03 +0.03 -0.03 word5 0100 0.05 -0.05 +0.05 -0.05 -0.05 word6 0011 0.09 -0.09 -0.09 +0.09 +0.09</p>
    <p>weight -0.05 -0.05 +0.09 +0.09 simhash 0 0 1 1</p>
    <p>two-bit</p>
    <p>change</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>If we only want to search to a Hamming distance h = 1; the problem is trivial</p>
    <p>Simply generate a table with the simhash entries sorted by increasing absolute value of bit weight</p>
    <p>However, in practice we want a larger maximum distance</p>
    <p>so how do we determine which is the</p>
    <p>optimal second bit to flip?</p>
    <p>E.g., given an initial single bit flip with weight 0.01, do we next try the bit with -0.5, or the two-bit combination (-1.9, 0.01)?</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>Here we sort the bits of document us hash according to the absolute value of their weight, and for convenience refer to bit i</p>
    <p>as the bit with the i-th</p>
    <p>lowest weight</p>
    <p>We then build a Volatility Ordered Set Heap (VOSH), which sorts bit combinations according to flip probability</p>
    <p>Height of this heap corresponds to b, the # of hash bits</p>
    <p>Details and algorithm are in the full paper, Section 5.1-5.2</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>Main properties of this heap:</p>
    <p>A parent node represents a better flip combination than its children; i.e., more likely to flip given small changes to u</p>
    <p>Left child increments the last bit of the parent</p>
    <p>Right child, if exists, increments the bit to the left of any gap in bit positions of the parent</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>We must decide at runtime which of the siblings at a given level in this heap is optimal, when we know the weight vector of the query simhash</p>
    <p>Additional max-heap is used to represent the frontier</p>
    <p>of yet-unexplored nodes</p>
    <p>By calculating the expected change in value for flipping the bits represented in each node</p>
    <p>At each step, the higher value node (i.e., the sibling that lost</p>
    <p>in the comparison) is placed, along with its children, in</p>
    <p>the max-heap</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>Using b = 64 and h between 1 and 3, we examine the VOSH-based approach on 8M simhash pairs and compare it to randomly flipping bits</p>
    <p>For h = 1, 30% of matches are found after only one flip; 80% after 4 flips, and 100% in 17 or fewer (vs. 64)</p>
    <p>h = 2, 100% of matches found in 152 vs. 2016 flips</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Bit OrderBit OrderBit Order</p>
    <p>Similar results for h = 3 (675 flips vs. 41,664)</p>
    <p>This difference increases with h, and as recall decreases</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Experiments and ResultsExperiments and ResultsExperiments and Results</p>
    <p>Dataset: 70M web pages from IRLbot web crawl (April 2008)</p>
    <p>Feature weights calculated by normalized TF-IDF score of each word i on page u</p>
    <p>Simhash fingerprint calculated with 64-bit MurmurHash function</p>
    <p>We compare our approach (PSM) to Block Permuted Hamming Search (BPHS), using the parameters suggested in the Manku paper</p>
    <p>We normalize our RAM usage to BPHS</p>
    <p>number of tables metric, see section 8.4 in the paper</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Experiments and ResultsExperiments and ResultsExperiments and Results</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Experiments and ResultsExperiments and ResultsExperiments and Results</p>
    <p>Scalability as dataset increases in size:</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Experiments and ResultsExperiments and ResultsExperiments and Results</p>
    <p>Batch mode throughput</p>
    <p>RAM usage is less important, but still smaller than BPHS</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>AgendaAgendaAgenda</p>
    <p>Introduction</p>
    <p>Motivation and Objectives</p>
    <p>Simhash</p>
    <p>Bit Order</p>
    <p>Experiments and Results</p>
    <p>Conclusions and Future Work</p>
  </div>
  <div class="page">
    <p>C o</p>
    <p>m p</p>
    <p>ut e</p>
    <p>r S c</p>
    <p>ie nc</p>
    <p>e , T</p>
    <p>e xa</p>
    <p>s A</p>
    <p>&amp; M</p>
    <p>U ni</p>
    <p>ve rs</p>
    <p>ity</p>
    <p>Conclusions and Future WorkConclusions and Future WorkConclusions and Future Work</p>
    <p>By utilizing the weight vector usually discarded during simhash calculation, we can generate a model to predict which bits will be most likely to be flipped in near-duplicates</p>
    <p>Result is a huge decrease in search time vs. exhaustive search, and the gap only widens if were willing to sacrifice a little recall</p>
    <p>Future work involves analysis of feature selection techniques to improve clustering results, further overhead reduction</p>
  </div>
</Presentation>
