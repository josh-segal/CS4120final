<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Incremental Syntactic Language Models for Phrase-based Translation</p>
    <p>Lane Schwartz Chris Callison-Burch William Schuler Stephen Wu</p>
    <p>Air Force Research Lab lane.schwartz@wpafb.af.mil</p>
    <p>Johns Hopkins University ccb@cs.jhu.edu</p>
    <p>Ohio State University schuler@ling.ohio-state.edu</p>
    <p>Mayo Clinic wu.stephen@mayo.edu</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in Statistical Machine Translation</p>
    <p>Translation Model vs Language Model</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Translation Model</p>
    <p>Abeille et al., 1990; Poutsma, 1998; Poutsma, 2000; Yamada &amp; Knight, 2001; Yamada &amp; Knight, 2002; Eisner, 2003; Gildea, 2003; Hearne &amp; Way, 2003; Poutsma, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl &amp; Knight, 2004; Melamed, 2004; Ding &amp; Palmer, 2005; Hearne, 2005; Quirk et al., 2005; Cowan et al., 2006; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006; Marcu et al., 2006; Zollmann &amp; Venugopal, 2006; Bod, 2007; DeNeefe et al., 2007; Liu et al., 2007; Chiang et al., 2008; Lavie et al., 2008; Mi &amp; Huang, 2008; Mi et al., 2008; Resnik, 2008; Shen et al., 2008; Zhou et al., 2008; Chiang, 2009; Hanneman &amp; Lavie, 2009; Liu et al., 2009; Chiang, 2010; Huang &amp; Mi, 2010; . . .</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Translation Model vs Language Model</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>Phrase-based decoder produces translation in the target language incrementally from left-to-right Phrase-based syntactic LM parser should parse target language hypotheses incrementally from left-to-right Related work:</p>
    <p>Galley &amp; Manning (2009) obtained 1-best dependency parse using a greedy dependency parser</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>Phrase-based decoder produces translation in the target language incrementally from left-to-right Phrase-based syntactic LM parser should parse target language hypotheses incrementally from left-to-right Related work:</p>
    <p>Galley &amp; Manning (2009) obtained 1-best dependency parse using a greedy dependency parser</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>Phrase-based decoder produces translation in the target language incrementally from left-to-right Phrase-based syntactic LM parser should parse target language hypotheses incrementally from left-to-right Related work:</p>
    <p>Galley &amp; Manning (2009) obtained 1-best dependency parse using a greedy dependency parser</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>We use a standard HHMM parser (Schuler et al., 2010)</p>
    <p>Engineering simple model, equivalent to PPDA Engineering linear-time parsing Algorithmic elegant fit into phrase-based decoder</p>
    <p>Cognitive nice psycholinguistic properties Other parsers Roark (2001), Henderson (2004),</p>
    <p>Huang &amp; Sagae (2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Syntax in the Language Model</p>
    <p>Definition An incremental syntactic language model uses an incremental statistical parser to define a probability model over the dependency or phrase structure of target language strings.</p>
    <p>We use a standard HHMM parser (Schuler et al., 2010)</p>
    <p>Engineering simple model, equivalent to PPDA Engineering linear-time parsing Algorithmic elegant fit into phrase-based decoder</p>
    <p>Cognitive nice psycholinguistic properties Other parsers Roark (2001), Henderson (2004),</p>
    <p>Huang &amp; Sagae (2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Incremental Parsing</p>
    <p>S</p>
    <p>NP</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP</p>
    <p>VB</p>
    <p>meets</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>PP</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Incremental Parsing</p>
    <p>S</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP</p>
    <p>VB</p>
    <p>meets</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>} S/VP</p>
    <p>VP/NN</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Transform right-expanding sequences of constituents into left-expanding sequences of incomplete constituents (Johnson 1998)</p>
    <p>S</p>
    <p>NP</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP</p>
    <p>VB</p>
    <p>meets</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>PP</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Incomplete constituents can be processed incrementally using a Hierarchical Hidden Markov Model parser. (Murphy &amp; Paskin, 2001; Schuler et al. 2010)</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Transform right-expanding sequences of constituents into left-expanding sequences of incomplete constituents (Johnson 1998)</p>
    <p>S</p>
    <p>NP</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP</p>
    <p>VB</p>
    <p>meets</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>PP</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Incomplete constituents can be processed incrementally using a Hierarchical Hidden Markov Model parser. (Murphy &amp; Paskin, 2001; Schuler et al. 2010)</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Transform right-expanding sequences of constituents into left-expanding sequences of incomplete constituents (Johnson 1998)</p>
    <p>S</p>
    <p>NP</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP</p>
    <p>VB</p>
    <p>meets</p>
    <p>NP</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>PP</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Incomplete constituents can be processed incrementally using a Hierarchical Hidden Markov Model parser. (Murphy &amp; Paskin, 2001; Schuler et al. 2010)</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Hierarchical Hidden Markov Model</p>
    <p>Circles denote hidden random variables</p>
    <p>Edges denote conditional dependencies</p>
    <p>Shaded circles denote observed values</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=The =president =meets =the =board =on =Friday</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>Hierarchical Hidden Markov Model</p>
    <p>Circles denote hidden random variables</p>
    <p>Edges denote conditional dependencies</p>
    <p>Shaded circles denote observed values</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=The =president =meets =the =board =on =Friday</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP =S/PP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP =S/PP</p>
    <p>=IN</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP =S/PP</p>
    <p>=IN</p>
    <p>=S/NP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP =S/PP</p>
    <p>=IN</p>
    <p>=S/NP</p>
    <p>=NP</p>
  </div>
  <div class="page">
    <p>Incremental Parsing using HHMM (Schuler et al. 2010)</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Analogous to Maximally Incremental CCG Parsing</p>
    <p>Equivalent to Probabilistic Push-Down Automata</p>
    <p>Isomorphic Tree  Path</p>
    <p>S</p>
    <p>S/NP</p>
    <p>S/PP</p>
    <p>S/VP</p>
    <p>NP</p>
    <p>NP/NN</p>
    <p>DT</p>
    <p>The</p>
    <p>NN</p>
    <p>president</p>
    <p>VP</p>
    <p>VP/NN</p>
    <p>VP/NP</p>
    <p>VB</p>
    <p>meets</p>
    <p>DT</p>
    <p>the</p>
    <p>NN</p>
    <p>board</p>
    <p>IN</p>
    <p>on</p>
    <p>NP</p>
    <p>Friday</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1 =The</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2 =president</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3 =meets</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4 =the</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5 =board</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6 =on</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7 =Friday</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=DT</p>
    <p>=NP/NN</p>
    <p>=NN</p>
    <p>=NP</p>
    <p>=S/VP</p>
    <p>=VB</p>
    <p>=VP/NP</p>
    <p>=DT =VP/NN</p>
    <p>=NN</p>
    <p>=VP =S/PP</p>
    <p>=IN</p>
    <p>=S/NP</p>
    <p>=NP</p>
    <p>=S</p>
  </div>
  <div class="page">
    <p>Phrase-Based Translation</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Der Prasident trifft am Freitag den Vorstand The president meets the board on Friday</p>
    <p>Stack 0</p>
    <p>s</p>
    <p>s the</p>
    <p>s that</p>
    <p>s president</p>
    <p>Stack 1</p>
    <p>the president</p>
    <p>that president</p>
    <p>president Friday</p>
    <p>Stack 2</p>
    <p>president meets</p>
    <p>Obama met</p>
    <p>Stack 3</p>
  </div>
  <div class="page">
    <p>Phrase-Based Translation</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Der Prasident trifft am Freitag den Vorstand The president meets the board on Friday</p>
    <p>Stack 0</p>
    <p>s</p>
    <p>s the</p>
    <p>s that</p>
    <p>s president</p>
    <p>Stack 1</p>
    <p>the president</p>
    <p>that president</p>
    <p>president Friday</p>
    <p>Stack 2</p>
    <p>president meets</p>
    <p>Obama met</p>
    <p>Stack 3</p>
  </div>
  <div class="page">
    <p>Phrase-Based Translation with Syntactic LM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Definition th represents parses of the partial translation at node h in stack t</p>
    <p>Stack 0</p>
    <p>s</p>
    <p>s the</p>
    <p>s that</p>
    <p>s president</p>
    <p>Stack 1</p>
    <p>the president</p>
    <p>that president</p>
    <p>president Friday</p>
    <p>Stack 2</p>
    <p>president meets</p>
    <p>Obama met</p>
    <p>Stack 3</p>
  </div>
  <div class="page">
    <p>Phrase-Based Translation with Syntactic LM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Definition th represents parses of the partial translation at node h in stack t</p>
    <p>Stack 0</p>
    <p>s 0</p>
    <p>s the 11</p>
    <p>s that 12</p>
    <p>s president 13</p>
    <p>Stack 1</p>
    <p>the president</p>
    <p>21</p>
    <p>that president</p>
    <p>22</p>
    <p>president Friday</p>
    <p>23</p>
    <p>Stack 2</p>
    <p>president meets</p>
    <p>31</p>
    <p>Obama met 32</p>
    <p>Stack 3</p>
  </div>
  <div class="page">
    <p>Integrate Parser into Phrase-based Decoder</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>s11</p>
    <p>s21</p>
    <p>s31</p>
    <p>e1</p>
    <p>r12</p>
    <p>r22</p>
    <p>r32</p>
    <p>s12</p>
    <p>s22</p>
    <p>s32</p>
    <p>e2</p>
    <p>r13</p>
    <p>r23</p>
    <p>r33</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5</p>
    <p>r16</p>
    <p>r26</p>
    <p>r36</p>
    <p>s16</p>
    <p>s26</p>
    <p>s36</p>
    <p>e6</p>
    <p>r17</p>
    <p>r27</p>
    <p>r37</p>
    <p>s17</p>
    <p>s27</p>
    <p>s37</p>
    <p>e7</p>
    <p>r18</p>
    <p>r28</p>
    <p>r38</p>
    <p>=The =president =meets =the =board =on =Friday</p>
    <p>s the 11</p>
    <p>the president</p>
    <p>21</p>
    <p>president meets</p>
    <p>31</p>
    <p>meets the 41</p>
  </div>
  <div class="page">
    <p>Integrate Parser into Phrase-based Decoder</p>
    <p>president meets</p>
    <p>31</p>
    <p>the board</p>
    <p>51</p>
    <p>s13</p>
    <p>s23</p>
    <p>s33</p>
    <p>e3</p>
    <p>r14</p>
    <p>r24</p>
    <p>r34</p>
    <p>s14</p>
    <p>s24</p>
    <p>s34</p>
    <p>e4</p>
    <p>r15</p>
    <p>r25</p>
    <p>r35</p>
    <p>s15</p>
    <p>s25</p>
    <p>s35</p>
    <p>e5=meets =the =board</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Direct Maximum Entropy Model of Translation</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>e = argmax e</p>
    <p>exp  j jhj(e,f)</p>
    <p>= Set of j feature weights</p>
    <p>h =</p>
    <p>Phrase-based translation model n-gram LM Distortion model ... Syntactic LM P(th)</p>
    <p>Stack 0</p>
    <p>s 0</p>
    <p>s the 11</p>
    <p>Stack 1</p>
    <p>the president</p>
    <p>21</p>
    <p>Stack 2</p>
    <p>president meets</p>
    <p>31</p>
    <p>Stack 3</p>
  </div>
  <div class="page">
    <p>Does an Incremental Syntactic LM Help Translation?</p>
    <p>Thats nice...</p>
    <p>but will it make my BLEU score go up?</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Perplexity Results</p>
    <p>Language models trained on WSJ Treebank corpus</p>
    <p>LM In-domain Perplexity</p>
    <p>Out-of-domain Perplexity</p>
    <p>WSJ 5-gram LM 232 1262 WSJ Syntactic LM 385 529</p>
    <p>Interpolated WSJ 5-gram + WSJ SynLM</p>
    <p>Gigaword 5-gram 258 312 Interpolated Gigaword 5-gram + WSJ SynLM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Perplexity Results</p>
    <p>Language models trained on WSJ Treebank corpus</p>
    <p>LM In-domain Perplexity</p>
    <p>Out-of-domain Perplexity</p>
    <p>WSJ 5-gram LM 232 1262 WSJ Syntactic LM 385 529 Interpolated WSJ 5-gram + WSJ SynLM</p>
    <p>Gigaword 5-gram 258 312 Interpolated Gigaword 5-gram + WSJ SynLM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Perplexity Results</p>
    <p>Language models trained on WSJ Treebank corpus ...and n-gram model for larger English Gigaword corpus.</p>
    <p>LM In-domain Perplexity</p>
    <p>Out-of-domain Perplexity</p>
    <p>WSJ 5-gram LM 232 1262 WSJ Syntactic LM 385 529 Interpolated WSJ 5-gram + WSJ SynLM</p>
    <p>Gigaword 5-gram 258 312 Interpolated Gigaword 5-gram + WSJ SynLM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Does an Incremental Syntactic LM Help Translation?</p>
    <p>Thats nice...</p>
    <p>but will it make my BLEU score go up?</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Does an Incremental Syntactic LM Help Translation?</p>
    <p>Moses with LM(s) BLEU Using n-gram LM only 18.78 Using n-gram LM + Syntactic LM 19.78</p>
    <p>Experiment NIST OpenMT 2008 Urdu-English data set Moses with standard phrase-based translation model Tuning and testing restricted to sentences  20 words long Results reported on devtest set n-gram LM is WSJ 5-gram LM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Does an Incremental Syntactic LM Help Translation?</p>
    <p>Moses with LM(s) BLEU Using n-gram LM only 18.78 Using n-gram LM + Syntactic LM 19.78</p>
    <p>Experiment NIST OpenMT 2008 Urdu-English data set Moses with standard phrase-based translation model Tuning and testing restricted to sentences  20 words long Results reported on devtest set n-gram LM is WSJ 5-gram LM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Straightforward general framework for incorporating any Incremental Syntactic LM into Phrase-based Translation We used an Incremental HHMM Parser as Syntactic LM</p>
    <p>Syntactic LM shows substantial decrease in perplexity on out-of-domain data over n-gram LM when trained on same data Syntactic LM interpolated with n-gram LM shows even greater decrease in perplexity on both in-domain and out-of-domain data, even when n-gram LM is trained on substantially larger corpus +1 BLEU on Urdu-English task with Syntactic LM</p>
    <p>All code is open source and integrated into Moses</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>Incremental Syntactic Language Models for Phrase-based Translation</p>
    <p>Lane Schwartz Chris Callison-Burch William Schuler Stephen Wu</p>
    <p>Air Force Research Lab lane.schwartz@wpafb.af.mil</p>
    <p>Johns Hopkins University ccb@cs.jhu.edu</p>
    <p>Ohio State University schuler@ling.ohio-state.edu</p>
    <p>Mayo Clinic wu.stephen@mayo.edu</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>This looks a lot like CCG</p>
    <p>Our parser performs some CCG-style operations:</p>
    <p>Forward function application NP/NN NN  NP</p>
    <p>Type raising NP  S/VP</p>
    <p>Type raising in conjunction with forward function composition</p>
    <p>DT  NP/NN VP/NP NP/NN  VP/NN</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Why not just use CCG?</p>
    <p>No probablistic version of incremental CCG Our parser is constrained (we dont have backward composition) We do use those components of CCG (forward function application and forward function composition) which are useful for probabilistic incremental parsing</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Speed Results</p>
    <p>Mean per-sentence decoding time</p>
    <p>Sentence length</p>
    <p>Moses +SynLM beam=50</p>
    <p>+SynLM beam=2000</p>
    <p>Parser beam sizes are indicated for the syntactic LM Parser runs in linear time, but were parsing all paths through the Moses lattice as they are generated by the decoder More informed pruning, but slower decoding</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
  <div class="page">
    <p>Phrase-Based Translation with Syntactic LM</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
    <p>Definition</p>
    <p>e def = string of n target language words e1. . .en</p>
    <p>et def = the first t words in e, where tn</p>
    <p>t def = set of all incremental parses of et</p>
    <p>t def = subset of parses t that remain after parser pruning</p>
    <p>e argmax</p>
    <p>P( |e)  t1</p>
    <p>et</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Acknowledgments</p>
    <p>This research was supported by NSF CAREER/PECASE award 0447685, NSF grant IIS-0713448, and the European Commission through the EuroMatrixPlus project. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the sponsors or the United States Air Force. Paper cleared for public release (Case Number 88ABW-2010-6489) on 10 Dec 2010. Presentation cleared for public release (Case Number 88ABW-2011-2970) on 26 May 2011.</p>
    <p>Motivation Syntactic LM Decoder Integration Results Questions?</p>
  </div>
</Presentation>
