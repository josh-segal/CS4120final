<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>On the Limitations of Unsupervised Bilingual Dictionary Induction</p>
    <p>Sebastian Ruder Ivan VuliAnders Sgaard</p>
  </div>
  <div class="page">
    <p>Background: Unsupervised MT</p>
  </div>
  <div class="page">
    <p>Recently: Unsupervised neural machine translation (Artetxe et al., ICLR 2018; Lample et al., ICLR 2018)</p>
    <p>Background: Unsupervised MT</p>
  </div>
  <div class="page">
    <p>Recently: Unsupervised neural machine translation (Artetxe et al., ICLR 2018; Lample et al., ICLR 2018)</p>
    <p>Background: Unsupervised MT</p>
  </div>
  <div class="page">
    <p>Recently: Unsupervised neural machine translation (Artetxe et al., ICLR 2018; Lample et al., ICLR 2018)</p>
    <p>Background: Unsupervised MT</p>
    <p>Key component: Initialization via unsupervised cross-lingual alignment of word embedding spaces</p>
  </div>
  <div class="page">
    <p>Background: Cross-lingual word embeddings</p>
  </div>
  <div class="page">
    <p>Cross-lingual word embeddings enable cross-lingual transfer</p>
    <p>Background: Cross-lingual word embeddings</p>
  </div>
  <div class="page">
    <p>Cross-lingual word embeddings enable cross-lingual transfer</p>
    <p>Most common approach: Project one word embedding space into another by learning a transformation matrix between source embeddings and their translations</p>
    <p>Background: Cross-lingual word embeddings</p>
    <p>W xi yin</p>
  </div>
  <div class="page">
    <p>Cross-lingual word embeddings enable cross-lingual transfer</p>
    <p>Most common approach: Project one word embedding space into another by learning a transformation matrix between source embeddings and their translations</p>
    <p>(Mikolov et al., 2013)</p>
    <p>Background: Cross-lingual word embeddings</p>
    <p>n</p>
    <p>i=1</p>
    <p>Wxi  yi 2</p>
    <p>W xi yin</p>
  </div>
  <div class="page">
    <p>Cross-lingual word embeddings enable cross-lingual transfer</p>
    <p>Most common approach: Project one word embedding space into another by learning a transformation matrix between source embeddings and their translations</p>
    <p>(Mikolov et al., 2013)</p>
    <p>More recently: Use an adversarial setup to learn an unsupervised mapping</p>
    <p>Background: Cross-lingual word embeddings</p>
    <p>n</p>
    <p>i=1</p>
    <p>Wxi  yi 2</p>
    <p>W xi yin</p>
  </div>
  <div class="page">
    <p>Cross-lingual word embeddings enable cross-lingual transfer</p>
    <p>Most common approach: Project one word embedding space into another by learning a transformation matrix between source embeddings and their translations</p>
    <p>(Mikolov et al., 2013)</p>
    <p>More recently: Use an adversarial setup to learn an unsupervised mapping</p>
    <p>Assumption: Word embedding spaces are approximately isomorphic, i.e. same number of vertices, connected the same way.</p>
    <p>Background: Cross-lingual word embeddings</p>
    <p>n</p>
    <p>i=1</p>
    <p>Wxi  yi 2</p>
    <p>W xi yin</p>
  </div>
  <div class="page">
    <p>How similar are embeddings across languages?</p>
  </div>
  <div class="page">
    <p>Nearest neighbour (NN) graphs of top 10 most frequent words in English and German are not isomorphic.</p>
    <p>How similar are embeddings across languages?</p>
  </div>
  <div class="page">
    <p>Nearest neighbour (NN) graphs of top 10 most frequent words in English and German are not isomorphic.</p>
    <p>NN graphs of top 10 most frequent English words and their translations into German</p>
    <p>How similar are embeddings across languages?</p>
    <p>English German</p>
  </div>
  <div class="page">
    <p>Nearest neighbour (NN) graphs of top 10 most frequent words in English and German are not isomorphic.</p>
    <p>NN graphs of top 10 most frequent English words and their translations into German</p>
    <p>How similar are embeddings across languages?</p>
    <p>English German  Not isomorphic</p>
  </div>
  <div class="page">
    <p>How similar are embeddings across languages?</p>
  </div>
  <div class="page">
    <p>NN graphs of top 10 most frequent English nouns and their translations</p>
    <p>How similar are embeddings across languages?</p>
    <p>English German</p>
  </div>
  <div class="page">
    <p>NN graphs of top 10 most frequent English nouns and their translations</p>
    <p>How similar are embeddings across languages?</p>
    <p>English German</p>
    <p>Not isomorphic</p>
  </div>
  <div class="page">
    <p>NN graphs of top 10 most frequent English nouns and their translations</p>
    <p>How similar are embeddings across languages?</p>
    <p>English German</p>
    <p>Not isomorphic</p>
    <p>Word embeddings are not approximately isomorphic across languages.</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>: adjacency matrices of</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
    <p>A1, A2 G1, G2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>: adjacency matrices of</p>
    <p>: degree matrices of</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
    <p>A1, A2 G1, G2</p>
    <p>G1, G2D1, D2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>: adjacency matrices of</p>
    <p>: degree matrices of</p>
    <p>: Laplacians of</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
    <p>A1, A2 G1, G2</p>
    <p>G1, G2D1, D2</p>
    <p>L1 = D1  A1, L2 = D2  A2 G1, G2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>: adjacency matrices of</p>
    <p>: degree matrices of</p>
    <p>: Laplacians of</p>
    <p>: eigenvalues (spectra) of</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
    <p>A1, A2 G1, G2</p>
    <p>G1, G2D1, D2</p>
    <p>L1 = D1  A1, L2 = D2  A2 G1, G2</p>
    <p>1, 2 L1, L2</p>
  </div>
  <div class="page">
    <p>Need a metric to measure how similar two NN graphs and of different languages are</p>
    <p>Propose eigenvector similarity</p>
    <p>: adjacency matrices of</p>
    <p>: degree matrices of</p>
    <p>: Laplacians of</p>
    <p>: eigenvalues (spectra) of</p>
    <p>How do we quantify similarity?</p>
    <p>G1 G2</p>
    <p>A1, A2 G1, G2</p>
    <p>G1, G2D1, D2</p>
    <p>L1 = D1  A1, L2 = D2  A2 G1, G2</p>
    <p>1, 2 L1, L2</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>Quantifies how much two NN graphs are isospectral, i.e. they have the same spectrum (same sets of eigenvalues).</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>Quantifies how much two NN graphs are isospectral, i.e. they have the same spectrum (same sets of eigenvalues).</p>
    <p>Isomorphic isospectral, but isospectral isomorphic</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>Quantifies how much two NN graphs are isospectral, i.e. they have the same spectrum (same sets of eigenvalues).</p>
    <p>Isomorphic isospectral, but isospectral isomorphic</p>
    <p>: G1, G2  [0,)</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>Quantifies how much two NN graphs are isospectral, i.e. they have the same spectrum (same sets of eigenvalues).</p>
    <p>Isomorphic isospectral, but isospectral isomorphic</p>
    <p>: are isospectral (very similar)</p>
    <p>: G1, G2  [0,)</p>
    <p>= 0 G1, G2</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>How do we quantify similarity?</p>
    <p>Quantifies how much two NN graphs are isospectral, i.e. they have the same spectrum (same sets of eigenvalues).</p>
    <p>Isomorphic isospectral, but isospectral isomorphic</p>
    <p>: are isospectral (very similar)</p>
    <p>: become less similar</p>
    <p>: G1, G2  [0,)</p>
    <p>= 0 G1, G2</p>
    <p>G1, G2</p>
    <p>= k</p>
    <p>i=1</p>
    <p>(1i  2i) 2 k = min</p>
    <p>j {</p>
    <p>k i=1 ji</p>
    <p>n i=1 ji</p>
    <p>&gt; 0.9} Metric: where</p>
  </div>
  <div class="page">
    <p>Unsupervised cross-lingual learning assumptions</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>May or may not scale to low-resource languages</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>May or may not scale to low-resource languages</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
    <p>Conneau et al. (2018) This work</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>May or may not scale to low-resource languages</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
    <p>Conneau et al. (2018) This work</p>
    <p>Languages Dependent-marking, fusional and isolating</p>
    <p>Agglutinative, many cases</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>May or may not scale to low-resource languages</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
    <p>Conneau et al. (2018) This work</p>
    <p>Languages Dependent-marking, fusional and isolating</p>
    <p>Agglutinative, many cases</p>
    <p>Corpora Comparable (Wikipedia) Different domains</p>
  </div>
  <div class="page">
    <p>Besides isomorphism, several other implicit assumptions</p>
    <p>May or may not scale to low-resource languages</p>
    <p>Unsupervised cross-lingual learning assumptions</p>
    <p>Conneau et al. (2018) This work</p>
    <p>Languages Dependent-marking, fusional and isolating</p>
    <p>Agglutinative, many cases</p>
    <p>Corpora Comparable (Wikipedia) Different domains</p>
    <p>Algorithms/ hyperparameters</p>
    <p>Same Different</p>
  </div>
  <div class="page">
    <p>Conneau et al. (2018)</p>
  </div>
  <div class="page">
    <p>Conneau et al. (2018)</p>
    <p>X Y</p>
  </div>
  <div class="page">
    <p>Conneau et al. (2018)</p>
    <p>X Y</p>
    <p>W WX Y</p>
  </div>
  <div class="page">
    <p>Conneau et al. (2018)</p>
    <p>W W</p>
  </div>
  <div class="page">
    <p>Conneau et al. (2018)</p>
    <p>W W</p>
  </div>
  <div class="page">
    <p>A simple weakly supervised method</p>
  </div>
  <div class="page">
    <p>Extract identically spelled words in both languages</p>
    <p>A simple weakly supervised method</p>
  </div>
  <div class="page">
    <p>Extract identically spelled words in both languages</p>
    <p>Use these as bilingual seed words</p>
    <p>A simple weakly supervised method</p>
  </div>
  <div class="page">
    <p>Extract identically spelled words in both languages</p>
    <p>Use these as bilingual seed words</p>
    <p>Run refinement step of Conneau et al. (2018)</p>
    <p>A simple weakly supervised method</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
    <p>Given a list of source language words, find the closest target language word in the cross-lingual embedding space</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
    <p>Given a list of source language words, find the closest target language word in the cross-lingual embedding space</p>
    <p>Compare against a gold standard dictionary</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
    <p>Given a list of source language words, find the closest target language word in the cross-lingual embedding space</p>
    <p>Compare against a gold standard dictionary</p>
    <p>Metric: Precision at 1 (P@1)</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
    <p>Given a list of source language words, find the closest target language word in the cross-lingual embedding space</p>
    <p>Compare against a gold standard dictionary</p>
    <p>Metric: Precision at 1 (P@1)</p>
    <p>Use fastText monolingual embeddings</p>
  </div>
  <div class="page">
    <p>Experiments: Bilingual dictionary induction</p>
    <p>Given a list of source language words, find the closest target language word in the cross-lingual embedding space</p>
    <p>Compare against a gold standard dictionary</p>
    <p>Metric: Precision at 1 (P@1)</p>
    <p>Use fastText monolingual embeddings</p>
    <p>Conneau et al. (2018) This work</p>
    <p>Languages (English to)</p>
    <p>French, German, Chinese, Russian,</p>
    <p>Spanish</p>
    <p>Estonian (ET), Finnish (FI), Greek (EL), Hungarian</p>
    <p>(HU), Polish (PL), Turkish</p>
  </div>
  <div class="page">
    <p>Impact of language similarity P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR ET-FI</p>
    <p>Unsupervised (Adversarial) Weakly supervised (Identical strings)</p>
  </div>
  <div class="page">
    <p>Impact of language similarity P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR ET-FI</p>
    <p>Unsupervised (Adversarial) Weakly supervised (Identical strings)</p>
  </div>
  <div class="page">
    <p>Impact of language similarity</p>
    <p>Unsupervised approaches are challenged by languages that are not isolating and not dependent marking</p>
    <p>P@ 1</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR ET-FI</p>
    <p>Unsupervised (Adversarial) Weakly supervised (Identical strings)</p>
  </div>
  <div class="page">
    <p>Impact of language similarity</p>
    <p>Unsupervised approaches are challenged by languages that are not isolating and not dependent marking</p>
    <p>Naive supervision leads to competitive performance on similar language pairs and better results for dissimilar pairs</p>
    <p>P@ 1</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR ET-FI</p>
    <p>Unsupervised (Adversarial) Weakly supervised (Identical strings)</p>
  </div>
  <div class="page">
    <p>Impact of language similarity</p>
    <p>Ei ge</p>
    <p>nv ec</p>
    <p>to r s</p>
    <p>im ila</p>
    <p>rit y</p>
    <p>BDI performance</p>
  </div>
  <div class="page">
    <p>Impact of language similarity</p>
    <p>Ei ge</p>
    <p>nv ec</p>
    <p>to r s</p>
    <p>im ila</p>
    <p>rit y</p>
    <p>BDI performance</p>
  </div>
  <div class="page">
    <p>Impact of language similarity</p>
    <p>Eigenvector similarity strongly correlates with BDI performance</p>
    <p>Ei ge</p>
    <p>nv ec</p>
    <p>to r s</p>
    <p>im ila</p>
    <p>rit y</p>
    <p>BDI performance</p>
    <p>(  0.89)</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Spanish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Spanish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Spanish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Spanish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
    <p>Unsupervised approaches break down when domains are dissimilar</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Spanish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Source and target embeddings induced on 3 corpora: EuroParl (EP), Wikipedia (Wiki), Medical (EMEA)</p>
    <p>Unsupervised approaches break down when domains are dissimilar</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Finnish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Finnish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Finnish</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Domain differences may exacerbate difficulties of generalising across dissimilar languages</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Hungarian</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Hungarian</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
  </div>
  <div class="page">
    <p>Impact of domain differences</p>
    <p>English-Hungarian</p>
    <p>P@ 1</p>
    <p>D om</p>
    <p>ai n</p>
    <p>si m</p>
    <p>ila rit</p>
    <p>y</p>
    <p>EP-EP EP-Wiki EP-EMEA Wiki-EP Wiki-Wiki Wiki-EMEA EMEA-EP EMEA-Wiki EMEA-EMEA</p>
    <p>Domain similarity Unsupervised Weakly supervised</p>
    <p>Weak supervision helps to bridge domain differences, but performance still deteriorates</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters</p>
    <p>Settings: English with skipgram, win=2, ngrams=3-6</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters</p>
    <p>Settings: English with skipgram, win=2, ngrams=3-6</p>
    <p>Vary hyper-parameters of Spanish embeddings</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters P@</p>
    <p>== win=10 ngrams=2-7 win=10, ngrams=2-7</p>
    <p>English-Spanish (skipgram) English-Spanish (cbow)</p>
    <p>Settings: English with skipgram, win=2, ngrams=3-6</p>
    <p>Vary hyper-parameters of Spanish embeddings</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters P@</p>
    <p>== win=10 ngrams=2-7 win=10, ngrams=2-7</p>
    <p>English-Spanish (skipgram) English-Spanish (cbow)</p>
    <p>Settings: English with skipgram, win=2, ngrams=3-6</p>
    <p>Vary hyper-parameters of Spanish embeddings</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters P@</p>
    <p>== win=10 ngrams=2-7 win=10, ngrams=2-7</p>
    <p>English-Spanish (skipgram) English-Spanish (cbow)</p>
    <p>Settings: English with skipgram, win=2, ngrams=3-6</p>
    <p>Vary hyper-parameters of Spanish embeddings</p>
  </div>
  <div class="page">
    <p>Impact of hyper-parameters P@</p>
    <p>== win=10 ngrams=2-7 win=10, ngrams=2-7</p>
    <p>English-Spanish (skipgram) English-Spanish (cbow)</p>
    <p>Different algorithms introduce embedding spaces with wildly different structures.</p>
  </div>
  <div class="page">
    <p>Impact of dimensionality P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR</p>
  </div>
  <div class="page">
    <p>Impact of dimensionality P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR</p>
  </div>
  <div class="page">
    <p>Worse performance overall, but better performance for dissimilar language pairs (Estonian, Finnish, Greek).</p>
    <p>Impact of dimensionality P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR</p>
  </div>
  <div class="page">
    <p>Worse performance overall, but better performance for dissimilar language pairs (Estonian, Finnish, Greek).</p>
    <p>Monolingual word embeddings may overfit to rare peculiarities of languages. 20</p>
    <p>Impact of dimensionality P@</p>
    <p>EN-ES EN-ET EN-FI EN-EL EN-HU EN-PL EN-TR</p>
  </div>
  <div class="page">
    <p>Impact of evaluation procedure</p>
  </div>
  <div class="page">
    <p>Impact of evaluation procedure</p>
    <p>Part-of-speech: Performance on verbs is lowest across the board.</p>
  </div>
  <div class="page">
    <p>Impact of evaluation procedure</p>
    <p>Part-of-speech: Performance on verbs is lowest across the board.</p>
    <p>Frequency: Sensitivity to frequency for Hungarian, but less so for Spanish.</p>
  </div>
  <div class="page">
    <p>Impact of evaluation procedure</p>
    <p>Part-of-speech: Performance on verbs is lowest across the board.</p>
    <p>Frequency: Sensitivity to frequency for Hungarian, but less so for Spanish.</p>
    <p>Homographs: Lower precision due to loan words/proper names. High precision for free with weak supervision.</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Eigenvector similarity strongly correlates with unsupervised bilingual dictionary induction performance.</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Eigenvector similarity strongly correlates with unsupervised bilingual dictionary induction performance.</p>
    <p>Limitations of unsupervised bilingual dictionary induction:</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Eigenvector similarity strongly correlates with unsupervised bilingual dictionary induction performance.</p>
    <p>Limitations of unsupervised bilingual dictionary induction:</p>
    <p>Morphologically rich languages.</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Eigenvector similarity strongly correlates with unsupervised bilingual dictionary induction performance.</p>
    <p>Limitations of unsupervised bilingual dictionary induction:</p>
    <p>Morphologically rich languages.</p>
    <p>Corpora from different domains.</p>
    <p>Takeaways</p>
  </div>
  <div class="page">
    <p>Word embedding spaces are not approximately isomorphic across languages.</p>
    <p>We can use eigenvector similarity to characterise the relatedness of two monolingual vector spaces.</p>
    <p>Eigenvector similarity strongly correlates with unsupervised bilingual dictionary induction performance.</p>
    <p>Limitations of unsupervised bilingual dictionary induction:</p>
    <p>Morphologically rich languages.</p>
    <p>Corpora from different domains.</p>
    <p>Different word embedding algorithms. 22</p>
    <p>Takeaways</p>
  </div>
</Presentation>
