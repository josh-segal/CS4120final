<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>FlexServe: Deployment of PyTorch Models as Flexible REST Endpoints</p>
    <p>Edward Verenich1,2, Alvaro Velasquez2, M.G. Sarwar Murshed1, and Faraz Hussain1</p>
  </div>
  <div class="page">
    <p>Scenario</p>
    <p>Need on-demand image classification / object detection from sensor imagery.</p>
    <p>Inference capability needs to plug-in into existing systems.</p>
    <p>Rapidly changing target classes with possible geometric and photometric variations.</p>
    <p>Need for dynamic classification sensitivity threshold to control number of false negatives.</p>
  </div>
  <div class="page">
    <p>Why PyTorch Models?</p>
    <p>Large collection of pretrained PyTorch CNNs used as base models for convolutional feature extraction and transfer learning.</p>
    <p>PyTorchs dynamic graph architecture makes the network easier to debug.</p>
  </div>
  <div class="page">
    <p>Frequent Model Retraining and Deployment</p>
    <p>Target classes change frequently -&gt; use transfer learning for novel classes with limited training samples.</p>
    <p>Images from sensors may contain geometric and photometric variations (noise, angle, lighting, etc.) -&gt; train several architectures for the same target class to take advantage of different inductive biases.</p>
    <p>Inference service must be decoupled from the larger system -&gt; deploy trained models as REST services.</p>
  </div>
  <div class="page">
    <p>Options for Deployment</p>
    <p>TensorFlow Serving  requires a two-phase conversion process to an intermediate format (ONNX) and then a static computational graph. Not all PyTorch features are properly translated (nn.Module subclassing for ensemble models).</p>
    <p>KFServing for Kubernetes  requires that Kubernetes be installed, depends on a separate ingress gateway.</p>
    <p>TorchServe ( April 2020)  released after FlexServe by AWS and Facebook, requires Java 8 (not always permitted)</p>
  </div>
  <div class="page">
    <p>FlexServe</p>
    <p>Minimal dependencies outside of PyTorch due to restrictive and heterogeneous host computational environments.</p>
    <p>Deployment of multiple models behind a single endpoint.</p>
    <p>Multiple models share GPU resource for inference.</p>
    <p>Accept flexible data batch sizes during inference.</p>
    <p>Adjust inference endpoint sensitivity dynamically.</p>
  </div>
  <div class="page">
    <p>FlexServe Model Ensembles in Shared GPU Memory</p>
    <p>Multiple architectures trained for target class.</p>
    <p>Shared GPU memory for ensemble inference.</p>
    <p>Single REST endpoint.</p>
    <p>JSON response from ensemble: {model 1: [ALPHA],</p>
    <p>model 2: [ALPHA],</p>
    <p>model N: [BETA]}</p>
    <p>Single container with all dependencies</p>
  </div>
  <div class="page">
    <p>Flexible Batch Size Inference Endpoint</p>
    <p>Pool 1 to N samples in a single request, each model in the ensemble returns batched inference results.</p>
    <p>Requesting Application</p>
    <p>FlexServe Model Ensemble as a</p>
    <p>Service</p>
    <p>HTTP POST</p>
    <p>JSON</p>
    <p>{ model 1: [ALPHA, ALPHA, BETA], model 2: [ALPHA, BETA, ALPHA], model N: [ALPHA, ALPHA, BETA] }</p>
  </div>
  <div class="page">
    <p>Ensemble: Mixture of Experts and Sensitivity</p>
    <p>Consuming applications have different detection sensitivity policies that depend on target classes, surveillance area, etc.</p>
    <p>Ensemble inference allows the consuming application to employ its own sensitivity policy by combining output from FlexServe ensemble responses as needed (vote, average, probability, threshold, etc.)</p>
    <p>FlexServe Ensemble Models as a Service</p>
    <p>App Client 1 (vote policy)</p>
    <p>App Client 2 (threshold policy)</p>
    <p>App Client 3 (average policy)</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Existing PyTorch model deployment solutions did not meet our requirements for rapidly fielding RESTful inference services.</p>
    <p>FlexServe was designed to be a lightweight, flexible, and open deployment module for the growing PyTorch user community.</p>
    <p>Major contributors (Amazon, Facebook) to the PyTorch ecosystem recognized some of these limitations and released a scalable model deployment library (TorchServe).</p>
    <p>FlexServe is still the simplest solution with minimal requirements that is suitable for a wide range of computational environments.</p>
  </div>
  <div class="page">
    <p>Questions</p>
    <p>Edward Verenich  verenie@clarkson.edu, edward.verenich.2@us.af.mil</p>
    <p>Alvaro Velasquez  alvaro.velasquez.1@us.af.mil</p>
    <p>M.G. Sarwar Murshed  murshem@clarkson.edu</p>
    <p>Faraz Hussain  fhussain@clarkson.edu</p>
    <p>https://github.com/verenie/flexserve</p>
  </div>
</Presentation>
