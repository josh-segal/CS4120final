<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Multi-Relational Latent Semantic Analysis</p>
    <p>Kai-Wei Chang Joint work with Scott Wen-tau Yih, Chris Meek</p>
    <p>Microsoft Research</p>
  </div>
  <div class="page">
    <p>Natural Language Understanding</p>
    <p>Build an intelligent system that can interact with human using natural language</p>
    <p>Research challenge Meaning representation of text Support useful inferential tasks</p>
    <p>Semantic word representation is the foundation Language is compositional Word is the basic semantic unit</p>
  </div>
  <div class="page">
    <p>Continuous Semantic Representations</p>
    <p>A lot of popular methods for creating word vectors! Vector Space Model [Salton &amp; McGill 83] Latent Semantic Analysis [Deerwester+ 90] Latent Dirichlet Allocation [Blei+ 01] Deep Neural Networks [Collobert &amp; Weston 08]</p>
    <p>Encode term co-occurrence information Measure semantic similarity well</p>
  </div>
  <div class="page">
    <p>Continuous Semantic Representations</p>
    <p>sunny rainy</p>
    <p>windycloudy</p>
    <p>car</p>
    <p>wheel</p>
    <p>cab sad</p>
    <p>joy</p>
    <p>emotion</p>
    <p>feeling</p>
  </div>
  <div class="page">
    <p>Semantics Needs More Than Similarity</p>
    <p>Tomorrow will be rainy.</p>
    <p>Tomorrow will be sunny.</p>
    <p>rainy, sunny?</p>
    <p>rainy, sunny?</p>
  </div>
  <div class="page">
    <p>Leverage Linguistic Resources</p>
    <p>Cant we just use the existing linguistic resources? Knowledge in these resources is never complete Often lack of degree of relations</p>
    <p>Create a continuous semantic representation that Leverages existing rich linguistic resources Discovers new relations Enables us to measure the degree of multiple relations (not just similarity)</p>
  </div>
  <div class="page">
    <p>Roadmap</p>
    <p>Introduction Background</p>
    <p>Latent Semantic Analysis (LSA) Polarity Inducing LSA (PILSA)</p>
    <p>Multi-Relational Latent Semantic Analysis (MRLSA) Encoding multi-relational data in a tensor Tensor decomposition &amp; measuring degree of a relation</p>
    <p>Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Roadmap</p>
    <p>Introduction Background</p>
    <p>Latent Semantic Analysis (LSA) Polarity Inducing LSA (PILSA)</p>
    <p>Multi-Relational Latent Semantic Analysis (MRLSA) Encoding multi-relational data in a tensor Tensor decomposition &amp; measuring degree of a relation</p>
    <p>Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Latent Semantic Analysis [Deerwester+ 1990]</p>
    <p>Data representation Encode single-relational data in a matrix</p>
    <p>Co-occurrence (e.g., from a general corpus) Synonyms (e.g., from a thesaurus)</p>
    <p>Factorization Apply SVD to the matrix to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors</p>
  </div>
  <div class="page">
    <p>Cosine Score</p>
    <p>Encode Synonyms in Matrix Input: Synonyms from a thesaurus</p>
    <p>Joyfulness: joy, gladden Sad: sorrow, sadden</p>
    <p>joy gladden sorrow sadden goodwill</p>
    <p>Group 1: joyfulness 1 1 0 0 0</p>
    <p>Group 2: sad 0 0 1 1 0</p>
    <p>Group 3: affection 0 0 0 0 1</p>
    <p>Target word: row-vector Term: column-vector</p>
  </div>
  <div class="page">
    <p>SVD generalizes the original data Uncovers relationships not explicit in the thesaurus Term vectors projected to -dim latent space</p>
    <p>Word similarity: cosine of two column vectors in</p>
    <p>Mapping to Latent Space via SVD</p>
    <p>terms</p>
  </div>
  <div class="page">
    <p>Problem: Handling Two Opposite Relations Synonyms &amp; Antonyms</p>
    <p>LSA cannot distinguish antonyms [Landauer 2002] Distinguishing synonyms and antonyms is still perceived as a difficult open problem. [Poon &amp; Domingos 09]</p>
  </div>
  <div class="page">
    <p>Polarity Inducing LSA [Yih, Zweig, Platt 2012]</p>
    <p>Data representation Encode two opposite relations in a matrix using polarity</p>
    <p>Synonyms &amp; antonyms (e.g., from a thesaurus)</p>
    <p>Factorization Apply SVD to the matrix to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors</p>
  </div>
  <div class="page">
    <p>joy gladden sorrow sadden goodwill</p>
    <p>Group 1: joyfulness 1 1 -1 -1 0</p>
    <p>Group 2: sad -1 -1 1 1 0</p>
    <p>Group 3: affection 0 0 0 0 1</p>
    <p>Encode Synonyms &amp; Antonyms in Matrix</p>
    <p>Joyfulness: joy, gladden; sorrow, sadden Sad: sorrow, sadden; joy, gladden</p>
    <p>Inducing polarity</p>
    <p>Cosine Score:</p>
    <p>Target word: row-vector</p>
  </div>
  <div class="page">
    <p>joy gladden sorrow sadden goodwill</p>
    <p>Group 1: joyfulness 1 1 -1 -1 0</p>
    <p>Group 2: sad -1 -1 1 1 0</p>
    <p>Group 3: affection 0 0 0 0 1</p>
    <p>Encode Synonyms &amp; Antonyms in Matrix</p>
    <p>Joyfulness: joy, gladden; sorrow, sadden Sad: sorrow, sadden; joy, gladden</p>
    <p>Inducing polarity Target word: row-vector</p>
    <p>Cosine Score:</p>
  </div>
  <div class="page">
    <p>Limitation of the matrix representation Each entry captures a particular type of relation between two entities, or Two opposite relations with the polarity trick</p>
    <p>Encoding other binary relations Is-A (hyponym)  ostrich is a bird Part-whole  engine is a part of car</p>
    <p>Problem: How to Handle More Relations?</p>
    <p>Encode multiple relations in a 3-way tensor (3-dim array)!</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Represent word relations using a tensor Each slice encodes a relation between terms and target words.</p>
    <p>jo y</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>el in</p>
    <p>g</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>jo y</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>el in</p>
    <p>g</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Synonym layer Antonym layer</p>
    <p>Construct a tensor with two slices</p>
    <p>Encode Multiple Relations in Tensor</p>
  </div>
  <div class="page">
    <p>Can encode multiple relations in the tensor</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>jo y</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>el in</p>
    <p>g</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Hyponym layer</p>
    <p>Encode Multiple Relations in Tensor</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Derive a low-rank approximation to generalize the data and to discover unseen relations Apply Tucker decomposition and reformulate the results</p>
    <p>Tensor Decomposition  Analogy to SVD</p>
    <p>1,</p>
    <p>2,  ,</p>
    <p>1,2 ,,</p>
    <p>~~</p>
    <p>1 ,2,,</p>
    <p>1,</p>
    <p>2,  ,</p>
    <p>latent representation of words</p>
  </div>
  <div class="page">
    <p>1,</p>
    <p>2,  ,</p>
    <p>1,2 ,,</p>
    <p>~~</p>
    <p>1 ,2,,</p>
    <p>Derive a low-rank approximation to generalize the data and to discover unseen relations Apply Tucker decomposition and reformulate the results</p>
    <p>~~</p>
    <p>1 ,</p>
    <p>latent representation of words</p>
    <p>latent representation of a relation</p>
    <p>Tensor Decomposition  Analogy to SVD</p>
  </div>
  <div class="page">
    <p>Multi-Relational LSA</p>
    <p>Data representation Encode multiple relations in a tensor</p>
    <p>Synonyms, antonyms, hyponyms (is-a),  (e.g., from a linguistic knowledge base)</p>
    <p>Factorization Apply tensor decomposition to the tensor to find latent components</p>
    <p>Measuring degree of relation Cosine of latent vectors after projection</p>
  </div>
  <div class="page">
    <p>Measure Degree of Relation</p>
    <p>Similarity Cosine of the latent vectors</p>
    <p>Other relation (both symmetric and asymmetric) Take the latent matrix of the pivot relation (synonym) Take the latent matrix of the relation Cosine of the latent vectors after projection</p>
  </div>
  <div class="page">
    <p>( joy ,sadden )=cos (: , joy , ,: ,sadden ,)</p>
    <p>jo y</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>lli ng</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>jo y</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>lli ng</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Synonym layer Antonym layer</p>
    <p>Measure Degree of Relation Raw Representation</p>
  </div>
  <div class="page">
    <p>( joy ,sadden )=cos (: , joy , ,: ,sadden ,)</p>
    <p>jo y</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>lli ng</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>jo y</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>lli ng</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Synonym layer Antonym layer</p>
    <p>Measure Degree of Relation Raw Representation</p>
  </div>
  <div class="page">
    <p>( joy ,feeling )=cos (: , joy , ,: ,feeling ,h)</p>
    <p>jo y</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>lli ng</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Synonym layer</p>
    <p>Estimate the Degree of a Relation Raw Representation</p>
    <p>gl ad</p>
    <p>de n</p>
    <p>jo y</p>
    <p>sa dd</p>
    <p>en fe</p>
    <p>el in</p>
    <p>g</p>
    <p>joyfulness</p>
    <p>gladden</p>
    <p>sad</p>
    <p>anger</p>
    <p>Hypernym layer</p>
  </div>
  <div class="page">
    <p>(w,w)=cos (: ,w ,,: ,w ,)</p>
    <p>Measure Degree of Relation Raw Representation</p>
    <p>w w</p>
    <p>Synonym layer</p>
    <p>The slice of the specific relation</p>
  </div>
  <div class="page">
    <p>Cos ( , )</p>
    <p>~~</p>
    <p>1,2,,</p>
    <p>, 2 , ,</p>
    <p>~~</p>
    <p>(w,w)=cos (:,: ,  ,:  ,: ,: ,  ,:</p>
    <p>)</p>
    <p>Measure Degree of Relation Latent Representation</p>
    <p>w w</p>
    <p>R</p>
    <p>R v v</p>
  </div>
  <div class="page">
    <p>Roadmap</p>
    <p>Introduction Background</p>
    <p>Latent Semantic Analysis (LSA) Polarity Inducing LSA (PILSA)</p>
    <p>Multi-Relational Latent Semantic Analysis (MRLSA) Encoding multi-relational data in a tensor Tensor decomposition &amp; measuring degree of a relation</p>
    <p>Experiments Conclusions</p>
  </div>
  <div class="page">
    <p>Experiment: Data for Building MRLSA Model</p>
    <p>Encarta Thesaurus Record synonyms and antonyms of target words Vocabulary of 50k terms and 47k target words</p>
    <p>WordNet Has synonym, antonym, hyponym, hypernym relations Vocabulary of 149k terms and 117k target words</p>
    <p>Goals: MRLSA generalizes LSA to model multiple relations Improve performance by combing heterogeneous data</p>
  </div>
  <div class="page">
    <p>Example Antonyms Output by MRLSA</p>
    <p>Target High Score Words</p>
    <p>inanimate alive, living, bodily, in-the-flesh, incarnate</p>
    <p>alleviate exacerbate, make-worse, in-flame, amplify, stir-up</p>
    <p>relish detest, abhor, abominate, despise, loathe</p>
    <p>* Words in blue are antonyms listed in the Encarta thesaurus.</p>
  </div>
  <div class="page">
    <p>Results  GRE Antonym Test Task: GRE closest-opposite questions</p>
    <p>Which is the closest opposite of adulterate? (a) renounce (b) forbid (c) purify (d) criticize (e) correct</p>
    <p>A c c u</p>
    <p>ra c y</p>
  </div>
  <div class="page">
    <p>Example Hyponyms Output by MRLSA</p>
    <p>Target High Score Words</p>
    <p>bird ostrich, gamecock, nighthawk, amazon, parrot</p>
    <p>automobile minivan, wagon, taxi, minicab, gypsy cab</p>
    <p>vegetable buttercrunch, yellow turnip, romaine, chipotle, chilli</p>
  </div>
  <div class="page">
    <p>Results  Relational Similarity (SemEval-2012) Task: Class-Inclusion Relation ( is-a kind of )</p>
    <p>Most/least illustrative word pairs (a) art:abstract (b) song:opera (c) footwear:boot (d) hair:brown</p>
    <p>A c c u</p>
    <p>ra c y</p>
  </div>
  <div class="page">
    <p>Conclusions Continuous semantic representation that</p>
    <p>Leverages existing rich linguistic resources Discovers new relations Enables us to measure the degree of multiple relations</p>
    <p>Approaches Better data representation Matrix/Tensor decomposition</p>
    <p>Challenges &amp; Future Work Capture more types of knowledge in the model Support more sophisticated inferential tasks</p>
  </div>
</Presentation>
