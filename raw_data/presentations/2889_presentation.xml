<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>TonY: An Orchestrator for Distributed Machine Learning Jobs</p>
  </div>
  <div class="page">
    <p>Agenda  Background: TensorFlow and YARN  What is TonY?  Why use TonY for distributed training?  Next steps</p>
  </div>
  <div class="page">
    <p>Machine Learning process</p>
    <p>Productionizing machine learning requires many steps</p>
    <p>The focus of this talk will be model training</p>
    <p>Data Ingestion</p>
    <p>Data Preparation</p>
    <p>Model Training</p>
    <p>Model Deployment</p>
    <p>Model Serving</p>
  </div>
  <div class="page">
    <p>Background What is TensorFlow?</p>
  </div>
  <div class="page">
    <p>Background What is TensorFlow?</p>
    <p>Visualisation with TensorBoard https://learningtensorflow.com/Visualisation/</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Worker task</p>
    <p>Worker task</p>
    <p>Worker task</p>
    <p>Parameter server task</p>
    <p>Parameter server task</p>
    <p>Worker + Parameter Server Model</p>
    <p>Worker task</p>
    <p>Worker task</p>
    <p>Worker task</p>
    <p>Worker task</p>
    <p>Ring All-Reduce Model</p>
    <p>What is distributed TensorFlow?</p>
  </div>
  <div class="page">
    <p>How to run distributed TensorFlow?  Distribute code/data artifacts across multiple machines in distributed job  Allow tasks in the same distributed job to talk to each other (e.g. tell each</p>
    <p>worker where all other worker/parameter servers are)  Ensure your task compute requirements are met before starting</p>
    <p>distributed job  Or, have a framework do all of the above for you (Hadoop!)</p>
  </div>
  <div class="page">
    <p>Background What is Hadoop?</p>
  </div>
  <div class="page">
    <p>Background What is Hadoop?</p>
    <p>Distributed File System</p>
  </div>
  <div class="page">
    <p>Background What is Hadoop?</p>
    <p>Distributed File System</p>
    <p>Yet Another Resource Negotiator</p>
  </div>
  <div class="page">
    <p>Background How to work with YARN?</p>
  </div>
  <div class="page">
    <p>Background How to work with YARN?</p>
  </div>
  <div class="page">
    <p>Background How to work with YARN?</p>
  </div>
  <div class="page">
    <p>What is TonY?</p>
  </div>
  <div class="page">
    <p>What is TonY?  Orchestrates running distributed TensorFlow on Hadoop  Acquires compute resources from Hadoop (memory, CPU, GPU)  Sets up and launches distributed TensorFlow jobs on Hadoop clusters  Manages application</p>
    <p>lifecycle  Fault tolerance  Job monitoring</p>
  </div>
  <div class="page">
    <p>TonY Architecture</p>
  </div>
  <div class="page">
    <p>TonY Architecture  Entry point for TonY jobs  Package users configurations, users</p>
    <p>model code and submit as YARN application</p>
  </div>
  <div class="page">
    <p>TonY Architecture  Job setup and lifecycle</p>
    <p>management  Negotiates compute resources</p>
    <p>from Hadoop  Sets up container environment  Launches and monitors containers</p>
  </div>
  <div class="page">
    <p>TonY Architecture  Container = Task Executor  Launches users provided python</p>
    <p>script  Heartbeats to Application Master</p>
    <p>for liveness</p>
  </div>
  <div class="page">
    <p>Why use TonY for distributed training?</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Leverage YARNs fine-grained resource management and multi-tenancy</p>
    <p>Logical resource isolation via queues  Hardware-based physical resource partitioning (CPU, K80, V100)  User-based resource limits</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Native GPU resource awareness  Ensures GPU resource isolation and scheduling</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  One-click TensorBoard access for monitoring training progress</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Fault tolerance  More workers = more failures</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Fault tolerance  More workers = more failures  First attempt periodically saves model checkpoints to HDFS</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Fault tolerance  More workers = more failures  First attempt periodically saves model checkpoints to HDFS  Worker failure -&gt; tear down and restart application</p>
  </div>
  <div class="page">
    <p>Scaling distributed TensorFlow on Hadoop  Fault tolerance  More workers = more failures  First attempt periodically saves model checkpoints to HDFS  Worker failure -&gt; tear down and restart application  Read checkpoints from HDFS, resume from where previous attempt</p>
    <p>left off</p>
  </div>
  <div class="page">
    <p>Open Sourced!  https://github.com/linkedin/TonY  Engineering blog post: https://bit.ly/2O6L5WD</p>
    <p>Contributions Welcome!</p>
  </div>
  <div class="page">
    <p>Next steps  Dr. Elephant integration  TonY portal for notebook, job history, cross-execution monitoring</p>
  </div>
  <div class="page">
    <p>Q &amp; A</p>
  </div>
</Presentation>
