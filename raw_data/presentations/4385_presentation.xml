<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Yue Gu*, Kangning Yang, Shiyu Fu, Shuhong Chen, Xinyu Li, Ivan Marsic</p>
    <p>Multimedia Image Processing Lab</p>
    <p>Electrical and Computer Engineering Department</p>
    <p>Rutgers, The State University of New Jersey</p>
    <p>Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment</p>
  </div>
  <div class="page">
    <p>Question and Answer</p>
    <p>Recommendation System</p>
    <p>AI Assistant</p>
    <p>Why the affective analysis is necessary?</p>
    <p>Human Speech</p>
    <p>Accurate Response</p>
    <p>Affects</p>
  </div>
  <div class="page">
    <p>Progress of Affective Computing</p>
    <p>Emotion RecognitionEmotion Recognition</p>
    <p>Happy, Excited</p>
    <p>Sadness</p>
    <p>Anger</p>
    <p>Neutral</p>
    <p>Frustration</p>
    <p>Happy, Excited</p>
    <p>Sadness</p>
    <p>Anger</p>
    <p>Neutral</p>
    <p>Frustration</p>
    <p>Sentiment AnalysisSentiment Analysis</p>
    <p>Strong Positive</p>
    <p>Positive</p>
    <p>Neutral</p>
    <p>Negative</p>
    <p>Strong Negative</p>
    <p>Strong Positive</p>
    <p>Positive</p>
    <p>Neutral</p>
    <p>Negative</p>
    <p>Strong Negative</p>
    <p>Affective AnalysisAffective Analysis</p>
    <p>MFCCs</p>
    <p>Prosody</p>
    <p>Vocal Quality</p>
    <p>MFCCs</p>
    <p>Prosody</p>
    <p>Vocal Quality</p>
    <p>Speech Signal</p>
    <p>Processing</p>
    <p>Speech Signal</p>
    <p>Processing</p>
    <p>BoW</p>
    <p>POS</p>
    <p>CNNs, LSTMs</p>
    <p>BoW</p>
    <p>POS</p>
    <p>CNNs, LSTMs</p>
    <p>Natural Language</p>
    <p>Processing</p>
    <p>Natural Language</p>
    <p>ProcessingMulti-Modality</p>
  </div>
  <div class="page">
    <p>Is multi-modality needed?</p>
    <p>Vocal signal prominence</p>
    <p>Oh you dont like you are west-siderOh you dont like that you are west-sider</p>
    <p>Neutral Frustrationor</p>
    <p>that</p>
  </div>
  <div class="page">
    <p>Is multi-modality needed?</p>
    <p>Vocal signal prominence</p>
    <p>Oh you dont like that you are west-siderOh you dont like that you are west-sider</p>
    <p>Neutral Frustrationor</p>
    <p>Oh you dont like that you are west-sider</p>
    <p>Happy</p>
  </div>
  <div class="page">
    <p>Is multi-modality needed?</p>
    <p>Vocal signal prominence</p>
    <p>Oh you dont like that you are west-siderOh you dont like that you are west-sider</p>
    <p>Neutral Frustrationor</p>
    <p>Oh you dont like that you are west-sider</p>
    <p>Happy</p>
    <p>I love this city! I hate this city!</p>
    <p>Acoustic ambiguity</p>
  </div>
  <div class="page">
    <p>Challenges: Feature Extraction</p>
    <p>Gap between features and actual affective states</p>
    <p>Lack of high-level associations</p>
    <p>Not all parts contribute equally</p>
  </div>
  <div class="page">
    <p>Challenges: Modality Fusion</p>
    <p>Decision-level Fusion</p>
    <p>Lack of mutual association learning</p>
    <p>Feature-level Fusion</p>
    <p>Fail to learn time-dependent interactions</p>
    <p>Lack of consistency</p>
  </div>
  <div class="page">
    <p>Proposed Solutions</p>
    <p>Feature Extraction</p>
    <p>Hierarchical attention based bidirectional GRUs</p>
    <p>Modality Fusion</p>
    <p>Word-level fusion with attention</p>
    <p>An End-to-End multimodal network</p>
  </div>
  <div class="page">
    <p>Data Pre-processing</p>
    <p>Text Branch</p>
    <p>Word Embedding: word2vec</p>
    <p>Audio Branch</p>
    <p>Mel-frequency spectral coefficients (MFSCs)</p>
    <p>Synchronization</p>
    <p>Word-level forced alignment 10</p>
  </div>
  <div class="page">
    <p>BiGRU</p>
    <p>BiGRU</p>
    <p>I mean guys</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>1 2</p>
    <p>_21 _22 _2</p>
    <p>_2and _2</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>BiGRU</p>
    <p>Audio (MFSC)</p>
    <p>Frame-level Acoustic</p>
    <p>Attention</p>
    <p>Word-level Acoustic</p>
    <p>Attention</p>
    <p>Word-level Textual</p>
    <p>Attention</p>
    <p>Text (Embedded)</p>
    <p>CNN</p>
    <p>Te x</p>
    <p>t A</p>
    <p>u d</p>
    <p>io F</p>
    <p>u si</p>
    <p>o n</p>
    <p>S o</p>
    <p>ftm a</p>
    <p>x La</p>
    <p>y e</p>
    <p>r</p>
    <p>_ =  _ +</p>
    <p>_ = (_</p>
    <p>)</p>
    <p>=1  (_</p>
    <p>)</p>
    <p>R e</p>
    <p>su lt</p>
    <p>Word-level Fusion</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
    <p>_1 _2 _</p>
  </div>
  <div class="page">
    <p>Word-level Fusion</p>
    <p>_ _ _ _</p>
    <p>Dense Layer</p>
    <p>_</p>
    <p>c</p>
    <p>_ _ _ _</p>
    <p>Dense Layer</p>
    <p>_</p>
    <p>c</p>
    <p>_</p>
    <p>Attention Layer</p>
    <p>(b) Vertical Fusion (c) Fine-tuning Attention Fusion</p>
    <p>__ _</p>
    <p>_</p>
    <p>Dense Layer</p>
    <p>_</p>
    <p>(a) Horizontal Fusion</p>
    <p>_</p>
    <p>_ = (_</p>
    <p>)</p>
    <p>=1  (_</p>
    <p>) + _ 12</p>
    <p>__ __</p>
    <p>Word-level acoustic attention distribution Word-level textual attention distribution</p>
    <p>Word-level acoustic contextual state Word-level textual contextual state</p>
    <p>__ __</p>
  </div>
  <div class="page">
    <p>Baselines</p>
    <p>Sentiment Analysis</p>
    <p>BL-SVM, LSTM-SVM</p>
    <p>C-MKL, TFN, LSTM(A)</p>
    <p>Emotion Recognition</p>
    <p>SVM Trees, GSV-eVector</p>
    <p>C-MKL, H-DMS</p>
    <p>Fusion</p>
    <p>Decision-level, Feature-level (utterance-level) 13</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Result</p>
    <p>MOSI</p>
    <p>Weighted Accuracy Weighted F1</p>
  </div>
  <div class="page">
    <p>Emotion Recognition Result</p>
    <p>IEMOCAP</p>
    <p>Weighted Accuracy Unweighted Accuracy 15</p>
  </div>
  <div class="page">
    <p>Multimodal architecture is needed</p>
    <p>T A T+A</p>
    <p>MOSI</p>
    <p>Weighted Accuracy Weighted F1</p>
    <p>T A T+A</p>
    <p>IEMOCAP</p>
    <p>Weighted Accuracy Weighted F1</p>
  </div>
  <div class="page">
    <p>Generalization</p>
    <p>Ours-HF Ours-VF Ours-HAF</p>
    <p>MOSI to YouTube</p>
    <p>Weighted Accuracy Weighted F1</p>
    <p>Ours-HF Ours-VF Ours-HAF</p>
    <p>IEMOCAP to EmotiW</p>
    <p>Weighted Accuracy Weighted F1</p>
  </div>
  <div class="page">
    <p>Attention Visualization</p>
    <p>What about the business what the hell is this</p>
    <p>Label: anger</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>Word-level acoustic attention distribution Word-level textual attention distribution</p>
    <p>Shared attention distribution Fine-tuning attention distribution</p>
    <p>Carry representative information in both text and audio</p>
    <p>Successfully combine both textual and acoustic attentions</p>
  </div>
  <div class="page">
    <p>Attention Visualization</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>__ __</p>
    <p>Word-level acoustic attention distribution Word-level textual attention distribution</p>
    <p>Shared attention distribution Fine-tuning attention distribution</p>
    <p>Capture emphasis and importance variation</p>
    <p>Vocal signal prominence</p>
    <p>Oh you dont like that youre west-sider</p>
    <p>Label: happy</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>A hierarchical attention based multimodal structure</p>
    <p>The word-level fusion strategies</p>
    <p>Word-level attention visualization</p>
  </div>
  <div class="page">
    <p>Thank you !</p>
    <p>Email: yg202@scarletmail.rutgers.edu</p>
    <p>Homepage: www.ieyuegu.com</p>
  </div>
</Presentation>
