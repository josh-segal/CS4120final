<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Strong Baselines for Neural Semi-supervised Learning</p>
    <p>under Domain Shift</p>
    <p>Sebastian Ruder Barbara Plank</p>
  </div>
  <div class="page">
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>State-of-the-art domain adaptation approaches</p>
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>State-of-the-art domain adaptation approaches</p>
    <p>leverage task-specific features</p>
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>State-of-the-art domain adaptation approaches</p>
    <p>leverage task-specific features</p>
    <p>evaluate on proprietary datasets or on a single benchmark</p>
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>State-of-the-art domain adaptation approaches</p>
    <p>leverage task-specific features</p>
    <p>evaluate on proprietary datasets or on a single benchmark</p>
    <p>Only compare against weak baselines</p>
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>State-of-the-art domain adaptation approaches</p>
    <p>leverage task-specific features</p>
    <p>evaluate on proprietary datasets or on a single benchmark</p>
    <p>Only compare against weak baselines</p>
    <p>Almost none evaluate against approaches from the extensive semi-supervised learning (SSL) literature</p>
    <p>Learning under Domain Shift</p>
  </div>
  <div class="page">
    <p>Revisiting Semi-Supervised Learning Classics in a Neural World</p>
  </div>
  <div class="page">
    <p>How do classics in SSL compare to recent advances?</p>
    <p>Revisiting Semi-Supervised Learning Classics in a Neural World</p>
  </div>
  <div class="page">
    <p>How do classics in SSL compare to recent advances?</p>
    <p>Can we combine the best of both worlds?</p>
    <p>Revisiting Semi-Supervised Learning Classics in a Neural World</p>
  </div>
  <div class="page">
    <p>How do classics in SSL compare to recent advances?</p>
    <p>Can we combine the best of both worlds?</p>
    <p>How well do these approaches work on out-of-distribution data?</p>
    <p>Revisiting Semi-Supervised Learning Classics in a Neural World</p>
  </div>
  <div class="page">
    <p>Bootstrapping algorithms</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>Bootstrapping algorithms</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>(Co-training)</p>
    <p>Bootstrapping algorithms</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>(Co-training)</p>
    <p>Tri-training</p>
    <p>Bootstrapping algorithms</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>(Co-training)</p>
    <p>Tri-training</p>
    <p>Tri-training with disagreement</p>
    <p>Bootstrapping algorithms</p>
  </div>
  <div class="page">
    <p>Self-training</p>
  </div>
  <div class="page">
    <p>Self-training</p>
  </div>
  <div class="page">
    <p>Self-training</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>- Er ror</p>
    <p>amp lific</p>
    <p>atio n</p>
  </div>
  <div class="page">
    <p>Self-training</p>
    <p>- Er ror</p>
    <p>amp lific</p>
    <p>atio n</p>
  </div>
  <div class="page">
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Calibration</p>
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Calibration</p>
    <p>Output probabilities in neural networks are poorly calibrated.</p>
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Calibration</p>
    <p>Output probabilities in neural networks are poorly calibrated.</p>
    <p>Throttling (Abney, 2007), i.e. selecting the top n highest confidence unlabeled examples works best.</p>
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Calibration</p>
    <p>Output probabilities in neural networks are poorly calibrated.</p>
    <p>Throttling (Abney, 2007), i.e. selecting the top n highest confidence unlabeled examples works best.</p>
    <p>Online learning</p>
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Calibration</p>
    <p>Output probabilities in neural networks are poorly calibrated.</p>
    <p>Throttling (Abney, 2007), i.e. selecting the top n highest confidence unlabeled examples works best.</p>
    <p>Online learning</p>
    <p>Training until convergence on labeled data and then on unlabeled data works best.</p>
    <p>Self-training variants</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Tri-training Tri-training</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training x</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training</p>
    <p>y = 1</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training</p>
    <p>y = 1 y = 0</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training</p>
    <p>y = 1y = 1 y = 0</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Tri-training Tri-training</p>
    <p>Tri-training</p>
    <p>y = 1y = 1 y = 0</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>x</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>y = 1 x</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>y = 1 x</p>
    <p>y = 1</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>y = 0</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>y = 0</p>
  </div>
  <div class="page">
    <p>Tri-training with disagreement</p>
    <p>Tri-training with disagreement</p>
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>y = 0 - 3</p>
    <p>inde pend</p>
    <p>ent mod</p>
    <p>els</p>
  </div>
  <div class="page">
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Sampling unlabeled data</p>
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Sampling unlabeled data</p>
    <p>Producing predictions for all unlabeled examples is expensive</p>
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Sampling unlabeled data</p>
    <p>Producing predictions for all unlabeled examples is expensive</p>
    <p>Sample number of unlabeled examples</p>
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Sampling unlabeled data</p>
    <p>Producing predictions for all unlabeled examples is expensive</p>
    <p>Sample number of unlabeled examples</p>
    <p>Confidence thresholding</p>
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Sampling unlabeled data</p>
    <p>Producing predictions for all unlabeled examples is expensive</p>
    <p>Sample number of unlabeled examples</p>
    <p>Confidence thresholding</p>
    <p>Not effective for classic approaches, but essential for our method</p>
    <p>Tri-training hyper-parameters</p>
  </div>
  <div class="page">
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>x</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>y = 1 x</p>
    <p>y = 1</p>
    <p>Multi-task tri-training</p>
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>Multi-task Tri-training</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2 m3</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2 m3 m1 m2 m3</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2 m3 m1 m2 m3 m1 m2 m3</p>
    <p>Multi-task Tri-training</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2 m3 m1 m2 m3 m1 m2 m3</p>
    <p>orthogonality constraint (Bousmalis et al., 2016)</p>
    <p>Multi-task Tri-training</p>
    <p>Lorth = W  m1Wm2</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>BiLSTM</p>
    <p>w2 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w1 char</p>
    <p>BiLSTM</p>
    <p>BiLSTM</p>
    <p>w3 char</p>
    <p>BiLSTM</p>
    <p>m1 m2 m3 m1 m2 m3 m1 m2 m3</p>
    <p>orthogonality constraint (Bousmalis et al., 2016)</p>
    <p>Multi-task Tri-training</p>
    <p>Lorth = W  m1Wm2</p>
    <p>L() =   i</p>
    <p>1,..,n</p>
    <p>log Pmi(y | h ) + LorthLoss:</p>
    <p>(Plank et al., 2016)</p>
  </div>
  <div class="page">
    <p>Data &amp; Tasks</p>
  </div>
  <div class="page">
    <p>Data &amp; Tasks</p>
    <p>Two tasks: Domains:</p>
  </div>
  <div class="page">
    <p>Data &amp; Tasks</p>
    <p>Two tasks: Domains:</p>
    <p>Sentiment analysis on Amazon reviews dataset (Blitzer et al, 2006)</p>
  </div>
  <div class="page">
    <p>Data &amp; Tasks</p>
    <p>Two tasks: Domains:</p>
    <p>Sentiment analysis on Amazon reviews dataset (Blitzer et al, 2006)</p>
    <p>POS tagging on SANCL 2012 dataset (Petrov and McDonald, 2012)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Results A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Avg over 4 target domains</p>
    <p>VFAE* DANN* Asym* Source only Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>* result from Saito et al., (2017)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Results A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Avg over 4 target domains</p>
    <p>VFAE* DANN* Asym* Source only Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>* result from Saito et al., (2017)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Results A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Avg over 4 target domains</p>
    <p>VFAE* DANN* Asym* Source only Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>* result from Saito et al., (2017)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Results A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Avg over 4 target domains</p>
    <p>VFAE* DANN* Asym* Source only Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>* result from Saito et al., (2017)</p>
  </div>
  <div class="page">
    <p>Sentiment Analysis Results A</p>
    <p>cc ur</p>
    <p>ac y</p>
    <p>Avg over 4 target domains</p>
    <p>VFAE* DANN* Asym* Source only Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>* result from Saito et al., (2017)</p>
    <p>has higher variance.</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>Trained on 10% labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>Source (+embeds) Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>Trained on 10% labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>Source (+embeds) Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>Trained on 10% labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>Source (+embeds) Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>Trained on 10% labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>Source (+embeds) Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>Trained on 10% labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>Source (+embeds) Self-training Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>Tri-training with disagreement works best with little data.</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
    <p>Trained on full labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>TnT Stanford* Source (+embeds) Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
    <p>Trained on full labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>TnT Stanford* Source (+embeds) Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
    <p>Trained on full labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>TnT Stanford* Source (+embeds) Tri-training Tri-training-Disagr. MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Results</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
    <p>Trained on full labeled data (WSJ)</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>Avg over 5 target domains</p>
    <p>TnT Stanford* Source (+embeds) Tri-training Tri-training-Disagr. MT-Tri</p>
    <p>Tri-training works best in the full data setting.</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on out-of-vocabulary (OOV) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>O O</p>
    <p>V to</p>
    <p>ke ns</p>
    <p>% O</p>
    <p>O V</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs OOV tokens Src Tri MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on out-of-vocabulary (OOV) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>O O</p>
    <p>V to</p>
    <p>ke ns</p>
    <p>% O</p>
    <p>O V</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs OOV tokens Src Tri MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on out-of-vocabulary (OOV) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>O O</p>
    <p>V to</p>
    <p>ke ns</p>
    <p>% O</p>
    <p>O V</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs OOV tokens Src Tri MT-Tri</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on out-of-vocabulary (OOV) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>O O</p>
    <p>V to</p>
    <p>ke ns</p>
    <p>% O</p>
    <p>O V</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs OOV tokens Src Tri MT-Tri</p>
    <p>Classic tri-training works best on OOV tokens.</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on out-of-vocabulary (OOV) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>O O</p>
    <p>V to</p>
    <p>ke ns</p>
    <p>% O</p>
    <p>O V</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs OOV tokens Src Tri MT-Tri</p>
    <p>Classic tri-training works best on OOV tokens.</p>
    <p>MT-Tri does worse than source-only baseline on OOV.</p>
  </div>
  <div class="page">
    <p>POS accuracy per binned log frequency</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y de</p>
    <p>lta v</p>
    <p>s. s</p>
    <p>rc -o</p>
    <p>nl y</p>
    <p>ba se</p>
    <p>lin e</p>
    <p>Binned frequency</p>
    <p>MT-Tri Tri</p>
    <p>POS Tagging Analysis</p>
  </div>
  <div class="page">
    <p>POS accuracy per binned log frequency</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y de</p>
    <p>lta v</p>
    <p>s. s</p>
    <p>rc -o</p>
    <p>nl y</p>
    <p>ba se</p>
    <p>lin e</p>
    <p>Binned frequency</p>
    <p>MT-Tri Tri</p>
    <p>POS Tagging Analysis</p>
  </div>
  <div class="page">
    <p>POS accuracy per binned log frequency</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y de</p>
    <p>lta v</p>
    <p>s. s</p>
    <p>rc -o</p>
    <p>nl y</p>
    <p>ba se</p>
    <p>lin e</p>
    <p>Binned frequency</p>
    <p>MT-Tri Tri  Tri-training works best on low-frequency tokens (leftmost</p>
    <p>bins).</p>
    <p>POS Tagging Analysis</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>very di fficult c</p>
    <p>ases</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>very di fficult c</p>
    <p>ases</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>very di fficult c</p>
    <p>ases</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>No bootstrapping method works well on unknown wordtag combinations.</p>
    <p>very di fficult c</p>
    <p>ases</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>POS Tagging Analysis Accuracy on unknown word-tag (UWT) tokens</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y on</p>
    <p>U W</p>
    <p>T to</p>
    <p>ke ns</p>
    <p>% U</p>
    <p>W T</p>
    <p>to ke</p>
    <p>ns</p>
    <p>Answers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*</p>
    <p>No bootstrapping method works well on unknown wordtag combinations.</p>
    <p>Less lexicalized FLORS approach is superior.</p>
    <p>very di fficult c</p>
    <p>ases</p>
    <p>* result from Schnabel &amp; Schtze (2014)</p>
  </div>
  <div class="page">
    <p>Takeaways</p>
    <p>Tri-training</p>
  </div>
  <div class="page">
    <p>Classic tri-training works best: outperforms recent state-of-the-art methods for sentiment analysis.</p>
    <p>Takeaways</p>
    <p>Tri-training</p>
  </div>
  <div class="page">
    <p>Classic tri-training works best: outperforms recent state-of-the-art methods for sentiment analysis.</p>
    <p>We address the drawback of tri-training (space &amp; time complexity) via the proposed MT-Tri model</p>
    <p>MT-Tri works best on sentiment, but not for POS.</p>
    <p>Takeaways</p>
    <p>Tri-training</p>
  </div>
  <div class="page">
    <p>Classic tri-training works best: outperforms recent state-of-the-art methods for sentiment analysis.</p>
    <p>We address the drawback of tri-training (space &amp; time complexity) via the proposed MT-Tri model</p>
    <p>MT-Tri works best on sentiment, but not for POS.</p>
    <p>Importance of:</p>
    <p>Comparing neural methods to classics (strong baselines)</p>
    <p>Evaluation on multiple tasks &amp; domains</p>
    <p>Takeaways</p>
    <p>Tri-training</p>
  </div>
</Presentation>
