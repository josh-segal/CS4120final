<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Terminal Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks under Hardware Fault Attacks</p>
    <p>Sanghyun Hong1, Pietro Frigo2, Yiitcan Kaya1, Cristiano Guiffrida2, Tudor Dumitra1</p>
  </div>
  <div class="page">
    <p>Sanghyun Hong, http://hardwarefail.ml 2</p>
  </div>
  <div class="page">
    <p>Sanghyun Hong, http://hardwarefail.ml 3</p>
  </div>
  <div class="page">
    <p>DNNs Resilience  False Sense of Security</p>
    <p>Techniques that rely on the graceful degradation  Parameter pruning1: to reduce the inference cost  Parameter quantization2: to compress the network size  Blend noises to parameters3: to improve the robustness</p>
    <p>Prior work showed it is difficult to cause the accuracy drop  Indiscriminate poisoning4: blend a lot of poisons  11% drop  Storage media errors5: a lot of random bit errors  5% drop  Hardware fault attacks6,7: a lot of random faults  7% drops</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 6</p>
    <p>They focus on the best-case or the average-case perturbations</p>
  </div>
  <div class="page">
    <p>Sanghyun Hong, http://hardwarefail.ml 7</p>
    <p>What is the WORST-CASE perturbation (a bit-flip) that inflicts a SIGNIFICANT accuracy drop exceeding 10%?</p>
  </div>
  <div class="page">
    <p>Illustration: How DNN Computes  Accuracy: 98.53%</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 8</p>
    <p>Conv [1, 10, 5x5] Conv [10, 20, 5x5] FC [320, 50] FC [50, 10]</p>
  </div>
  <div class="page">
    <p>Prior Work: Optimal Brain Damage</p>
    <p>Accuracy:</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 9</p>
    <p>Conv [1, 10, 5x5] Conv [10, 20, 5x5] FC [320, 50] FC [50, 10]</p>
    <p>The unimportant parameters</p>
  </div>
  <div class="page">
    <p>Prior Work: Hardware Fault Attacks  Accuracy: 98.53%</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 10</p>
    <p>Conv [1, 10, 5x5] Conv [10, 20, 5x5] FC [320, 50] FC [50, 10]</p>
    <p>Memory (RAM)</p>
    <p>weight + bias weight + bias weight + bias weight + bias</p>
  </div>
  <div class="page">
    <p>Prior Work: Hardware Fault Attacks</p>
    <p>Accuracy:</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 11</p>
    <p>Conv [1, 10, 5x5] Conv [10, 20, 5x5] FC [320, 50] FC [50, 10]</p>
    <p>Memory (RAM)</p>
    <p>weight + bias</p>
    <p>Sign Exponent Mantissa</p>
  </div>
  <div class="page">
    <p>Can We Find a Worst-case Bit-flip?</p>
    <p>Accuracy:</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 12</p>
    <p>Conv [1, 10, 5x5] Conv [10, 20, 5x5] FC [320, 50] FC [50, 10]</p>
    <p>Memory (RAM)</p>
    <p>weight + bias 1.2E+38: 1.401 x 2&quot;#$: 0 | 1011 1101 | 011 0011 0110 1111 1101 0001</p>
    <p>Sign Exponent Mantissa</p>
  </div>
  <div class="page">
    <p>Research Questions  RQ-1: How vulnerable are DNNs to a single bit-flip?</p>
    <p>RQ-2: What properties influence this vulnerability?</p>
    <p>RQ-3: Can an attacker exploit this vulnerability?</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation?</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 16</p>
  </div>
  <div class="page">
    <p>Research Questions  RQ-1: How vulnerable are DNNs to a single bit-flip?</p>
    <p>RQ-2: What properties influence this vulnerability?</p>
    <p>RQ-3: Can an attacker exploit this vulnerability?</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation?</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 17</p>
  </div>
  <div class="page">
    <p>RQ-1: How Vulnerable are DNNs to a Bit-flip?</p>
    <p>Metric</p>
    <p>Relative Accuracy Drop [RAD] = !&quot;&quot;#$%&amp;' ( !&quot;&quot;#)**+,-%.!&quot;&quot;#$%&amp;'</p>
    <p>Methodology  Flip (01 and 10) each bit in all parameters of a model  Measure the RAD over the entire validation set, each time  Achilles bit: when the bit flips, the flip inflicts RAD &gt; 10%</p>
    <p>Vulnerability  Max RAD: the maximum RAD that an Achilles bit can inflict  Ratio: the percentage of vulnerable parameters in a model</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 20</p>
  </div>
  <div class="page">
    <p>Network Acc. # Params Max RAD Ratio</p>
    <p>B(ase) 95.71 21,840 98 % 50%</p>
    <p>B-Wide 98.46 85,670 99 % 50%</p>
    <p>B-PReLU 98.13 21,843 99 % 99%</p>
    <p>B-Dropout 96.86 21,840 99 % 49%</p>
    <p>B-DP-Norm 97.97 21,962 99 % 51%</p>
    <p>L(eNet)5 98.81 61,706 99 % 47%</p>
    <p>L5-Dropout 98.72 61,706 99 % 45%</p>
    <p>L5-D-Norm 99.05 62,598 98 % 49%</p>
    <p>RQ-1: Vulnerability Analysis in MNIST</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 23</p>
    <p>Maximum RAD  98% in all models</p>
    <p>&gt; 45% of params are vulnerable in all the MNIST models</p>
  </div>
  <div class="page">
    <p>RQ-1: How Vulnerable Are Larger Models?  Metric</p>
    <p>Relative Accuracy Drop [RAD] = !&quot;&quot;#$%&amp;' ( !&quot;&quot;#)**+,-%.!&quot;&quot;#$%&amp;'</p>
    <p>Methodology  Flip (01 and 10) each bit in all parameters of a model  Measure the RAD over the entire validation set, each time</p>
    <p>[e.g. VGG16-ImageNet: examine 138M parameters  942 days]</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 24</p>
  </div>
  <div class="page">
    <p>RQ-1: How Vulnerable Are Larger Models?</p>
    <p>Metric</p>
    <p>Relative Accuracy Drop [RAD] = !&quot;&quot;#$%&amp;' ( !&quot;&quot;#)**+,-%.!&quot;&quot;#$%&amp;'</p>
    <p>Methodology  Flip (01 and 10) each bit in all parameters of a model  Measure the RAD over the entire validation set, each time</p>
    <p>Speed-up heuristics  Sampled validation set (SV): use 10% of the validation set  Inspect only specific bits (SB): the exponents or their MSBs  Sampled parameters (SP): uniformly sample 20k parameters</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 27</p>
  </div>
  <div class="page">
    <p>RQ-1: Vulnerability Analysis in Large Models</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 28</p>
    <p>Dataset Network Acc. # Params SV SB SP Max RAD Ratio</p>
    <p>C IF</p>
    <p>A R</p>
    <p>-1 0</p>
    <p>B(ase) 83.74 776K  !&quot;#  B-Slim 82.19 197K  !&quot;#  B-Dropout 81.18 776K  !&quot;#  B-D-Norm 80.17 778K  !&quot;#  AlexNet 83.96 2.5M  !&quot;#  VGG16 91.34 14.7M  !&quot;#</p>
    <p>Im ag</p>
    <p>eN et</p>
    <p>AlexNet 79.07 61.1M  $%&amp;'  (20K) VGG16 90.38 138.4M  $%&amp;'  (20K) ResNet50 92.86 25.6M  $%&amp;'  (20K) DenseNet161 93.56 28.9M  $%&amp;'  (20K) InceptionV3 88.65 27.2M  $%&amp;'  (20K)</p>
  </div>
  <div class="page">
    <p>RQ-1: Vulnerability Analysis in Large Models</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 29</p>
    <p>Dataset Network Acc. # Params SV SB SP Max RAD Ratio</p>
    <p>C IF</p>
    <p>A R</p>
    <p>-1 0</p>
    <p>B(ase) 83.74 776K  !&quot;#  94 % 46.8% B-Slim 82.19 197K  !&quot;#  93 % 46.7% B-Dropout 81.18 776K  !&quot;#  94 % 40.5% B-D-Norm 80.17 778K  !&quot;#  97 % 45.9% AlexNet 83.96 2.5M  !&quot;#  96 % 47.3% VGG16 91.34 14.7M  !&quot;#  99 % 46.2%</p>
    <p>Im ag</p>
    <p>eN et</p>
    <p>AlexNet 79.07 61.1M  $%&amp;'  (20K) 100 % 47.3% VGG16 90.38 138.4M  $%&amp;'  (20K) 99 % 42.1% ResNet50 92.86 25.6M  $%&amp;'  (20K) 100 % 47.8% DenseNet161 93.56 28.9M  $%&amp;'  (20K) 100 % 49.0% InceptionV3 88.65 27.2M  $%&amp;'  (20K) 100 % 40.8%</p>
  </div>
  <div class="page">
    <p>Research Questions  RQ-1: How vulnerable are DNNs to a single bit-flip?</p>
    <p>RQ-2: What properties influence this vulnerability?</p>
    <p>RQ-3: Can an attacker exploit this vulnerability?</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation?</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 30</p>
  </div>
  <div class="page">
    <p>RQ-2: Properties that Influence the Vulnerability  (Network-level) DNN-properties  (Parameter-level) Bitwise representation</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 31</p>
  </div>
  <div class="page">
    <p>RQ-2: Impact of the Common Techniques</p>
    <p>(Network-level) DNN-properties  The dropout and batch-norm do not affect the vulnerability</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 33</p>
    <p>Dataset Network Base acc. # Params SV SB SP Max RAD Ratio</p>
    <p>M IN</p>
    <p>IS T</p>
    <p>L(eNet)5 98.81 61,706    99 % 47% L5-Dropout 98.72 61,706    99 % 45% L5-D-Norm 99.05 62,598    98 % 49%</p>
    <p>CI FA</p>
    <p>R -1</p>
  </div>
  <div class="page">
    <p>RQ-2: Impact of the Other DNN Properties  (Network-level) DNN-properties  The dropout and batch-norm cannot reduce the vulnerability  The vulnerability increases proportionally with the width  The activation with negative values doubles the vulnerability  The vulnerability is consistent across 19 DNNs architectures</p>
    <p>[8 MNIST, 5 CIFAR-10, and 5 ImageNet architectures]</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 36</p>
  </div>
  <div class="page">
    <p>RQ-2: Impact of the Parameter Sign</p>
    <p>(Parameter-level) Bitwise representation  Flip the MSB of the exponents mostly lead to [RAD &gt; 10%]  The only (01) flip direction leads to [RAD &gt; 10%]  The positive parameters are likely to be vulnerable</p>
    <p>to bit-flips than the negative parameters</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 39</p>
  </div>
  <div class="page">
    <p>Research Questions  RQ-1: How vulnerable are DNNs to a single bit-flip?</p>
    <p>RQ-2: What properties influence this vulnerability?</p>
    <p>RQ-3: Can an attacker exploit this vulnerability?</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation?</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 40</p>
  </div>
  <div class="page">
    <p>RQ-3: Threat Model  Attackers Capability</p>
    <p>Capability  Surgical: can cause a bit-flip at an intended location  Blind: cannot control the location of a bit-flip</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 41</p>
  </div>
  <div class="page">
    <p>RQ-3: Threat Model  Attackers Knowledge</p>
    <p>Capability  Surgical: can cause a bit-flip at an intended location  Blind: cannot control the location of a bit-flip</p>
    <p>Knowledge:  White-box: knows the victim model internals  Black-box: has no knowledge of the victim model</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 43</p>
  </div>
  <div class="page">
    <p>RQ-3: Threat Model  Single Bit Adversary</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 46</p>
    <p>White-boxBlack-box</p>
    <p>Strongest attacker !(#$% &amp;' ()#$**+, -$%)  100%</p>
    <p>Weakest attacker ! #$% &amp;' ()#$**+, -$%  / [VGG16: 42.1% / 32-bits  1.32%]</p>
    <p>Blind</p>
    <p>Surgical</p>
  </div>
  <div class="page">
    <p>RQ-3: Practical Weapon  Rowhammer  Rowhammer attacks  Single-bit corruption primitives at DRAM-level  Software-induced hardware fault attacks</p>
    <p>[The attacker only requires a user-level access to memory]</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 48</p>
    <p>Row Buffer</p>
    <p>Double-sided Rowhammer attack</p>
  </div>
  <div class="page">
    <p>RQ-3: Practical Weapon  Rowhammer  Rowhammer attacks  Single-bit corruption primitives at DRAM-level  Software-induced hardware fault attacks</p>
    <p>[The attacker only requires a user-level access to memory]</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 49</p>
    <p>Double-sided Rowhammer attack</p>
    <p>Row Buffer</p>
  </div>
  <div class="page">
    <p>RQ-3: Threat Model (Re-visited)</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 51</p>
    <p>White-boxBlack-box</p>
    <p>Blind Surgical</p>
    <p>Strongest attacker</p>
    <p>Weakest attacker ! &quot;#$ %&amp; '(&quot;#))*+ ,#$  [VGG16: 42.1% / 32-bits  1.32%]</p>
    <p>!(&quot;#$ %&amp; '(&quot;#))*+ ,#$)  100%</p>
  </div>
  <div class="page">
    <p>RQ-3: If Our Adversary Can Flip Multiple-Bits</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 52</p>
    <p>White-boxBlack-box</p>
    <p>Blind Surgical</p>
    <p>Strongest attacker</p>
    <p>(Weakest) Stronger attacker &quot; #$% &amp;' ()#$**+, -$%  1.3%</p>
    <p>&quot;(#$% &amp;' ()#$**+, -$%)  100%</p>
  </div>
  <div class="page">
    <p>Evaluation  MLaaS scenario: a VM runs under the Rowhammer pressure</p>
    <p>A Python process that constantly queries the VGG16 ImageNet model  Make bit-flips to the process memory: both on the code and data</p>
    <p>[Consequences: RAD &gt; 10%, process crash, or RAD &lt;= 10%]</p>
    <p>Method: Hammertime1 DB  Explore Rowhammer effects systematically in 12 different DRAM chips</p>
    <p>[Vulnerability of DRAM: based on the number of bits subjected to flip]</p>
    <p>Experiments  25 experiments for each of 12 different DRAM chips  300 cumulative bit-flip attempts for each experiment</p>
    <p>RQ-3: The Weakest Attacker with Rowhammer</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 55</p>
  </div>
  <div class="page">
    <p>Blind attack results  The attacker can inflict the Terminal Brain Damage (RAD &gt;</p>
    <p>It is Challenging to Detect the blind attacker  Only 6 crashes observed over the entire 7.5k bit-flip attempts</p>
    <p>RQ-3: The Weakest Attacker with Rowhammer</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 57</p>
    <p>Blind Rowhammer attack is practical against DNN models</p>
  </div>
  <div class="page">
    <p>Research Questions  RQ-1: How vulnerable are DNNs to single bit-flips?</p>
    <p>RQ-2: What properties influence this vulnerability?</p>
    <p>RQ-3: Can an attacker exploit this vulnerability?</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation?</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 58</p>
  </div>
  <div class="page">
    <p>RQ-4: Rowhammer Defenses  Hardware-supported defenses to fault attack  ECC: Error correcting code in memory1</p>
    <p>Detection based on hardware performance counters2</p>
    <p>System-level defenses to fault attack  CATT: Memory isolation of the kernel and user space3</p>
    <p>ZebRAM: Software-based isolation of every DRAM row4</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 59</p>
    <p>They require infrastructure-wide changes, or they are not effective against other hardware faults</p>
  </div>
  <div class="page">
    <p>RQ-4: Can We Mitigate this Vulnerability?</p>
    <p>Investigate DNN-level defenses:  Restrict activation magnitudes: Tanh or ReLU6  Use low-precision numbers: quantization or binarization</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 60</p>
  </div>
  <div class="page">
    <p>RQ-4: Pros and Cons of Our Defenses  Pros  Both the directions reduce the # of vulnerable parameters</p>
    <p>Cons  Require to re-train a whole model from scratch</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 61</p>
  </div>
  <div class="page">
    <p>RQ-4: Pros and Cons of Our Defenses</p>
    <p>Pros  Both directions reduce the # of vulnerable parameters  Substitute activation functions without re-training</p>
    <p>Cons  Require to re-train a whole model from scratch  Expect the accuracy drop of a model without re-training</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 62</p>
  </div>
  <div class="page">
    <p>Summary of Our Results  RQ-1: How vulnerable are DNNs to single bit-flips?</p>
    <p>All DNNs have a bit whose flip causes RAD up to 100% 40-50% of all parameters in a model are vulnerable</p>
    <p>RQ-2: What properties influence this vulnerability? The vulnerability is consistent across multiple DNNs</p>
    <p>RQ-3: Can an attacker exploit this vulnerability? Blind Rowhammer attacker can exploit this practically</p>
    <p>RQ-4: Can we utilize DNN-level mechanisms for mitigation? We reduce the vulnerable parameters in a model; but ours degrade the performance or require the re-training</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 66</p>
  </div>
  <div class="page">
    <p>Conclusions and Implications  DNNs are not resilient to worst-case parameter perturbations  Re-examine techniques relying on graceful degradations with security lens</p>
    <p>The vulnerability of DNNs to !-arch. attacks is under-studied  Explore and evaluate new attacks, particularly thought hard  These attacks may be inflicted with weak attackers, e.g. blind Rowhammer</p>
    <p>For AI systems, system-level defenses are not sufficient  Consider additional model-level defenses that account for DNN properties</p>
    <p>Sanghyun Hong, http://hardwarefail.ml 69</p>
  </div>
  <div class="page">
    <p>Thank you! Sanghyun Hong shhong@cs.umd.edu</p>
    <p>http://hardwarefail.ml</p>
  </div>
</Presentation>
