<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Using Syslog Message Sequences for Predicting Disk</p>
    <p>Failures</p>
    <p>Wes Featherstun and Errin Fulp</p>
    <p>Wake Forest University</p>
    <p>Department of Computer Science</p>
    <p>Winston-Salem, NC USA</p>
    <p>November 11, 2010</p>
  </div>
  <div class="page">
    <p>HPC Trends and System Events</p>
    <p>Computing improvements achieved by adding more processors</p>
    <p>IBM Blue Gene at LLNL has 212,992 processors</p>
    <p>System failures will become more problematic</p>
    <p>As systems become larger, frequency of critical events will increase</p>
    <p>Hardware failure, software failure, and user error</p>
    <p>Lower overall system utilization</p>
    <p>Cannot easily improve failure rates; can we manage failures?</p>
    <p>Minimize the impact of failures</p>
    <p>Smarter scheduling of applications and services</p>
    <p>Accurate event predictions are key for event management</p>
    <p>Are accurate predictions possible?</p>
    <p>Need system status information to make predictions</p>
  </div>
  <div class="page">
    <p>System Status Information</p>
    <p>Almost every computer maintains a system log file</p>
    <p>Provide information about system events</p>
    <p>An event represents a change in system state</p>
    <p>Include hardware failures, software failures, and security</p>
    <p>Host Facility Level Tag Time Message</p>
    <p>Entries contain information such as: time, message, and tag</p>
    <p>Time identifies when the message was recorded</p>
    <p>Message describes the event, typically natural language</p>
    <p>Tag represents criticality, low values are more important</p>
    <p>Trying to predict future events</p>
  </div>
  <div class="page">
    <p>Example System Event to Predict</p>
    <p>An interesting event is disk failure</p>
    <p>By 2018 [large systems] could</p>
    <p>have 300 concurrent</p>
    <p>reconstructions at any time</p>
    <p>[SG07]</p>
    <p>Predicting disk failure is</p>
    <p>important</p>
    <p>Easy to identify event in the</p>
    <p>log...</p>
    <p>Predict failure as early as</p>
    <p>possible</p>
    <p>Min depth d and max lead l M</p>
    <p>depth</p>
    <p>lead</p>
    <p>time</p>
  </div>
  <div class="page">
    <p>Previous Work</p>
    <p>Hammerly et al. used a naive Bayesian classifier to predict with</p>
    <p>IBM achieved over 80% accuracy, but with a specialized logging</p>
    <p>system [LZXS07]</p>
    <p>Broadwell achieved 100% accuracy in predicting SCSI cable</p>
    <p>failures, but the approach is not easily scalable [Bro02]</p>
    <p>Turnbull et al. used an approach similar to that in this presentation</p>
    <p>to predict system board failures [TA03]</p>
  </div>
  <div class="page">
    <p>Support Vector Machines</p>
    <p>Support Vector Machine (SVM) is a classification algorithm</p>
    <p>Consider a set of samples from two different classes</p>
    <p>Each vector consists of features describing the sample</p>
    <p>SVM finds a hyperplane separating the classes in hyperspace</p>
    <p>The vectors closest to the plane are the support vectors</p>
  </div>
  <div class="page">
    <p>Spectrum Kernel</p>
    <p>Assume two symbols {A, B} and sequence length k = 2</p>
    <p>There are 2k possible sequences (features) (AA, AB, BA, BB)</p>
    <p>Value of a feature is the number of occurrences</p>
    <p>M = {A, A, B, A, A, B, B, A}</p>
    <p>AA: 2</p>
    <p>AB: 2</p>
    <p>BA: 2</p>
    <p>BB: 1 The spectrum kernel uses a sliding window to create sequences</p>
    <p>There are bk possible sequences, where b is number of symbols</p>
    <p>Used to provide context to each item</p>
    <p>How does this work for syslog messages?</p>
  </div>
  <div class="page">
    <p>tag Sequences</p>
    <p>Each message has a tag that indicates criticality</p>
    <p>Sequence of messages represented by sequence of tag values</p>
    <p>Need to reduce number of symbols, assume three levels</p>
    <p>high (tag &lt; 10), medium (10 &lt;tag&lt; 140), low (tag&gt; 140)</p>
    <p>Given a series of messages M, process using a sliding window</p>
    <p>Count the number of occurrences of k-length sequences</p>
  </div>
  <div class="page">
    <p>Example of Tag Sequences</p>
    <p>Let M = {148, 148, 158, 40, 5}</p>
    <p>Assume b = 3 and k = 3, then 33 = 27 possible features</p>
    <p>Feature number is ft+1 = mod (b  ft, b k) + e</p>
    <p>Vector for M would be (5:1, 141:1, 148:2, 158:1)</p>
    <p>tag Encoding(e) Sequence f(base10)</p>
  </div>
  <div class="page">
    <p>System Data Used for Experiments</p>
    <p>About 24 months of syslog file from 1024 node Linux cluster</p>
    <p>Averaged 3.24 message an hour (78 a day) per machine</p>
    <p>Observed 125 disk failure events</p>
    <p>Tag values ranged from 0 to 189</p>
    <p>tag value</p>
    <p>p e</p>
    <p>rc e</p>
    <p>n t</p>
    <p>o f</p>
    <p>a ll</p>
    <p>m e</p>
    <p>ss a</p>
    <p>g e</p>
    <p>s</p>
    <p>Distribution of Tag Values</p>
    <p>tag number</p>
    <p>p e</p>
    <p>rc e</p>
    <p>n t</p>
    <p>o f</p>
    <p>a ll</p>
    <p>m e</p>
    <p>ss a</p>
    <p>g e</p>
    <p>s</p>
    <p>Distribution of Message tags and Intervals Used</p>
    <p>Non-fail disk Fail disk</p>
  </div>
  <div class="page">
    <p>Experimental Setup Tags</p>
    <p>Use an SVM based on tag count and sequence count</p>
    <p>Determine optimal lead time</p>
    <p>Determine optimal window size</p>
    <p>Use the window size and lead time results to find the best sequence</p>
    <p>length</p>
    <p>Analyze results using accuracy, precision, and recall</p>
    <p>Accuracy: the number of correct classifications</p>
    <p>Precision: how many predicted failures actually occurred?</p>
    <p>Recall: of all failures, how many were predicted?</p>
  </div>
  <div class="page">
    <p>Tag Results</p>
    <p>Lead time best at about 100 messages</p>
    <p>Optimal window size is about 800 messages</p>
    <p>Lead Time</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
    <p>The Effect of Lead Time On SVM Performance</p>
    <p>Accuracy Precision Recall</p>
    <p>(a) The accuracy, precision and re</p>
    <p>call of tag-based methods as lead time</p>
    <p>changes</p>
    <p>Window Size</p>
    <p>P e</p>
    <p>rc e</p>
    <p>n ta</p>
    <p>g e</p>
    <p>Accuracy Precision Recall</p>
    <p>(b) The accuracy, precision, and recall</p>
    <p>of tag-based methods as the window</p>
    <p>size changes</p>
  </div>
  <div class="page">
    <p>Changing Sequence Length</p>
    <p>Increasing the sequence length adds more contextual information</p>
    <p>Do longer sequences improve effectiveness?</p>
    <p>Longer sequences exponentially increase feature space...</p>
    <p>Experiments performed with window size of 800 messages and 100</p>
    <p>messages of lead time</p>
    <p>Sequence Length Accuracy Precision Recall</p>
    <p>Table: A comparison of sequence lengths</p>
    <p>k = 5 provides best balance of speed and effectiveness</p>
  </div>
  <div class="page">
    <p>Timing Information</p>
    <p>Perhaps a change in message frequency is an indicator of imminent</p>
    <p>failure</p>
    <p>Keep track of the time (in seconds) during which each k-length</p>
    <p>sequence arrives</p>
    <p>Feature Space Accuracy Precision Recall</p>
    <p>Sequences Using Tags 79.9993 82.8838 79.0012</p>
    <p>Sequences Using Tags and Time 77.8329 82.2338 71.667</p>
    <p>Table: Comparing performance between features using only tags and features including time information using sequences of length 5</p>
    <p>Feature Space Accuracy Precision Recall</p>
    <p>Sequences Using Tags 80.999 85.4837 78.668</p>
    <p>Sequences Using Tags and Time 81.1661 86.9337 76.005</p>
    <p>Table: Comparing performance between features using only tags and features including time information using sequences of length 7</p>
  </div>
  <div class="page">
    <p>Keyword Motivation</p>
    <p>Tag numbers arent always available</p>
    <p>Instead, try to classify on message field</p>
    <p>Create a dictionary of words (any space delineated string)</p>
    <p>Alphabet is far too large</p>
    <p>Created dictionaries of keywords (originally 52, reduced to 24)</p>
    <p>Convert each keyword to a number</p>
    <p>Use sliding window of 2+ words to create word sequences</p>
    <p>keyword Encoding(e) Sequence f</p>
    <p>disk 0 0</p>
    <p>hpc 1 01</p>
    <p>error 2 012 5</p>
    <p>lustre 3 0123 18</p>
  </div>
  <div class="page">
    <p>Keywords Results</p>
    <p>The 54 keyword dictionary contains specific node names</p>
    <p>Does the SVM just train on nodes which tend to fail?</p>
    <p>Create a reduced 24-keyword dictionary</p>
    <p>All identifiers of a certain type are assigned to the same number</p>
    <p>Training on general categories instead of specific nodes, IPs, etc</p>
    <p>Dictionary Accuracy Precision Recall</p>
    <p>Table: A comparison of keystring dictionaries</p>
    <p>Sequence Length Accuracy Precision Recall</p>
    <p>Table: Performance as sequence length increases</p>
  </div>
  <div class="page">
    <p>Keywords with Timing Information</p>
    <p>Timing information hurt performance for tag based methods</p>
    <p>Since keywords ignore message bounds, might timing info add useful</p>
    <p>context?</p>
    <p>Experiment Accuracy Precision Recall</p>
    <p>Without Time Info 79.4996 82.9838 80.6676</p>
    <p>With Time Info 80.1657 85.567 78.6679</p>
    <p>Table: A comparison of the 24-keystring dictionary with and without the</p>
    <p>addition of time information</p>
    <p>k = 4, despite k = 5 performing better without timing information</p>
    <p>Training time for k = 5 with timing information can be massive...</p>
  </div>
  <div class="page">
    <p>Combination Features</p>
    <p>Both the tag and keyword approaches work well in isolation</p>
    <p>Can the effectiveness of predictions be increased by combining</p>
    <p>methods?</p>
    <p>Approach Accuracy Precision Recall</p>
    <p>Tags Without Time 80.999 85.4837 78.668</p>
    <p>Keystrings With Time 80.1657 85.567 78.6679</p>
    <p>Combination Without Time 77.9995 82.317 74.334</p>
    <p>Combination With Time 80.6664 88.567 74.6673</p>
    <p>Table: A comparison of tag based, keystring based, and combination methods</p>
    <p>Combination approach has fewer false positives, but also fewer true positives</p>
  </div>
  <div class="page">
    <p>Conclusions and Future Work</p>
    <p>Best method depends on priorities</p>
    <p>High recall: tags without time or 24-keywords with time</p>
    <p>High precision: combining tags, keywords, and timing information</p>
    <p>Several areas for improvement</p>
    <p>Try different classification algorithms</p>
    <p>Different data sets</p>
    <p>Can this approach be used for other failure events?</p>
    <p>Questions? Interested? Email:</p>
    <p>Dr. Errin Fulp: fulp@wfu.edu</p>
    <p>Wes Feathesrtun: wes.featherstun@gmail.com</p>
  </div>
  <div class="page">
    <p>Further Reading</p>
    <p>Peter Broadwell.</p>
    <p>Component failure prediction using supervised naive bayesian classification, December 2002.</p>
    <p>Available at: http://citeseerx.ist.psu.edu/viewdoc/summary?doi= 10.1.1.3.4641. Accessed on April 19, 2010.</p>
    <p>Greg Hamerly and Charles Elkan.</p>
    <p>Bayesian approaches to failure prediction for disk drives.</p>
    <p>In Proceedings of the Eighteenth International Conference on Machine Learning (ICLM), June 2001.</p>
    <p>Yinglung Liang, Yanyong Zhang, Hui Xiong, and Ramendra Sahoo.</p>
    <p>Failure prediction in ibm bluegene/l event logs.</p>
    <p>In Proceedings of the Sevent IEEE International Conference on Data Mining, 2007.</p>
    <p>Bianca Schroeder and Garth A Gibson.</p>
    <p>Understanding failures in petascale computers.</p>
    <p>Journal of Physics: Conference Series, (28), 2007.</p>
    <p>Doug Turnbull and Neil Alldrin.</p>
    <p>Failure prediction in hardware systems, 2003.</p>
    <p>Available at: http://www.cs.ucsd.edu/ dturnbul/Papers/ServerPrediction.pdf. Accessed on: April 19, 2010.</p>
  </div>
</Presentation>
