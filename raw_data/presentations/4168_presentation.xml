<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Data-Driven Text Simplification</p>
    <p>Sanja tajner and Horacio Saggion</p>
    <p>Sanja@informatik.uni-mannheim.de</p>
    <p>http://web.informatik.uni-mannheim.de/sstajner/index.html</p>
    <p>University of Mannheim, Germany</p>
    <p>Horacio.saggion@upf.edu</p>
    <p>https://www.upf.edu/web/horacio-saggion</p>
    <p>University Pompeu Fabra, Spain</p>
    <p>COLING 2018 Tutorial</p>
    <p>#TextSimplification2018</p>
  </div>
  <div class="page">
    <p>Presenters</p>
    <p>Horacio Saggion  http://www.dtic.upf.edu/~hsaggion</p>
    <p>https://www.linkedin.com/pub/horacio</p>
    <p>saggion/16/9b9/174</p>
    <p>https://twitter.com/h_saggion</p>
    <p>Large Scale Text Understanding</p>
    <p>Systems Lab / TALN group</p>
    <p>Sanja Stajner  http://web.informatik.uni</p>
    <p>mannheim.de/sstajner</p>
    <p>https://www.linkedin.com/in/sanja</p>
    <p>stajner-a6904738</p>
    <p>Data and Web Science Group  https://dws.informatik.uni- Systems Lab / TALN group</p>
    <p>http://www.taln.upf.edu</p>
    <p>Department of Information &amp;</p>
    <p>Communication Technologies</p>
    <p>Universitat Pompeu Fabra, Barcelona,</p>
    <p>Spain</p>
    <p>https://dws.informatik.uni</p>
    <p>mannheim.de/en/home/</p>
    <p>University of Mannheim, Germany</p>
    <p>2018 by S. tajner &amp; H. Saggion 2</p>
  </div>
  <div class="page">
    <p>Tutorial antecedents</p>
    <p>Previous tutorials on the topic given at:</p>
    <p>IJCNLP 2013 and RANLP 2015 (H. Saggion)</p>
    <p>RANLP 2017 (S. tajner)</p>
    <p>Automatic Text Simplification. H. Saggion. 2017. Morgan &amp;  Automatic Text Simplification. H. Saggion. 2017. Morgan &amp; Claypool. Synthesis Lectures on Human Language Technologies Series.</p>
    <p>https://www.morganclaypool.com/doi/abs/10.2200/S00700ED1V01Y201602 HLT032</p>
    <p>2018 by S. tajner &amp; H. Saggion 3</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Motivation for ATS  Automatic text simplification</p>
    <p>TS projects</p>
    <p>TS resources</p>
    <p>Neural text simplification Neural text simplification</p>
    <p>2018 by S. tajner &amp; H. Saggion 4</p>
  </div>
  <div class="page">
    <p>PART 1</p>
    <p>Motivation for Text SimplificationMotivation for Text Simplification</p>
    <p>2018 by S. tajner &amp; H. Saggion 5</p>
  </div>
  <div class="page">
    <p>Text Simplification (TS)</p>
    <p>During simplification, complex sentences are split into simple ones and uncommon vocabulary is replaced by more common expressions</p>
    <p>The process of transforming a text into an equivalent which is more readable and/or understandable by a target audience</p>
    <p>uncommon vocabulary is replaced by more common expressions</p>
    <p>TS is a complex task which encompasses a number of operations applied at different linguistic levels:</p>
    <p>Lexical</p>
    <p>Syntactic</p>
    <p>Discourse</p>
    <p>Started to attract the attention of natural language processing some years ago (1996) mainly as a pre-processing step</p>
    <p>2018 by S. tajner &amp; H. Saggion 6</p>
  </div>
  <div class="page">
    <p>This is human simplification</p>
    <p>Original Text</p>
    <p>Amnesty International accused the U.S. authorities of providing an &quot;inhuman&quot; treatment to Bradley Manning, a soldier accused of</p>
    <p>Adapted Text (by trained editor)</p>
    <p>United States treats a soldier in prison very badly.</p>
    <p>The soldier is called Bradley Manning. Manning, a soldier accused of leaking &quot;wires&quot; of American diplomacy to the website Wikileaks.</p>
    <p>Bradley Manning is in prison for giving information about the Government of the United States to Wikileaks.</p>
    <p>Wikileaks is a website which provides information on matters of public interest.</p>
    <p>2018 by S. tajner &amp; H. Saggion 7</p>
  </div>
  <div class="page">
    <p>Why text simplification?</p>
    <p>It is an interesting research problem for the NLP community</p>
    <p>Identify and measure sources of complexity / difficulty</p>
    <p>Create a paraphrase which is easy to read</p>
    <p>Several NLP expertises involved: summarization, natural language generation, sentence compression, word sense disambiguation, generation, sentence compression, word sense disambiguation, machine translation, etc...</p>
    <p>It is socially relevant</p>
    <p>Unprecedented democratization of information (e.g. Web)</p>
    <p>Information is not equally accessible to everyone</p>
    <p>UN Enable: make information and information services accessible to different groups of persons with disability</p>
    <p>2018 by S. tajner &amp; H. Saggion 8</p>
  </div>
  <div class="page">
    <p>Simplification users</p>
    <p>Deaf people (Inui &amp; al., 2003; Chung et al., 2013)</p>
    <p>Blind people (Grefenstette, 1998)</p>
    <p>People with low-literacy (Williams &amp; Reiter, 2008; Alusio &amp; al., 2008)</p>
    <p>People with autism (Mitkov, 2012; Barbu et al., 2013; Orasan People with autism (Mitkov, 2012; Barbu et al., 2013; Orasan et al, 2013; Dornescu et al., 2013)</p>
    <p>Second language learners (Petersen and Ostendorf, 2007; Burstein et al., 2013; Eskenazi et al. 2013)</p>
    <p>Dyslexic people (Matausch &amp; Pebck, 2010, Rello et al., 2013)</p>
    <p>People with aphasia (Carroll et al., 1999)</p>
    <p>2018 by S. tajner &amp; H. Saggion 9</p>
  </div>
  <div class="page">
    <p>English (Chandrasekar et al., 1996; Siddharthan, 2002; Carroll et al. 1998, Bouayad-Agha et al., 2009; Zhu et al., 2010; Coster &amp; Kauchak, 2011; Yatskar et al., 2011)</p>
    <p>French (Seratan, 2012;Franois &amp; Fairon, 2012)</p>
    <p>Portuguese (Alusio et al., 2008; Specia, 2010)</p>
    <p>Japanese (Inui et al.,2003), Arabic (Al-Subaihin and Al-Khalifa, 2011)</p>
    <p>Languages</p>
    <p>Danish (Klerke &amp; Sgaard, 2012)</p>
    <p>Swedish (Smith et al., 2010; Keskisrkk, 2012)</p>
    <p>Spanish (Saggion et al. 2011; Bautista et al., 2012; Rello et al, 2013; Mosquera et al., 2013)</p>
    <p>Italian (DellOrletta et al. 2011, Tonelli et al. 2012)</p>
    <p>Basque (Aranzabe et al, 2012)</p>
    <p>Korean (Chung et al, 2013)</p>
    <p>2018 by S. tajner &amp; H. Saggion 10</p>
  </div>
  <div class="page">
    <p>NLP as a simplification user</p>
    <p>Dealing with complex sentences:</p>
    <p>Initial simplification application (Chandresakar et al., 1996)</p>
    <p>Improve results in IE (Jonnalagadda &amp; Gonzalez, 2011; Evans, 2011; Minard et al., 2012)</p>
    <p>Assist in question generation (Bernhard et al, 2012)</p>
    <p>Text summarization (Grefenstette, 1998 , Siddharthan et al, 2004)</p>
    <p>Improve results in MT (tajner and Popovi, 2016)</p>
    <p>2018 by S. tajner &amp; H. Saggion 11</p>
  </div>
  <div class="page">
    <p>Where to find simple texts?</p>
    <p>Opera is a drama set to music.</p>
    <p>An opera is a play in which everything</p>
    <p>is sung instead of spoken.</p>
    <p>Operas are usually performed in</p>
    <p>opera houses.</p>
    <p>2018 by S. tajner &amp; H. Saggion 12</p>
  </div>
  <div class="page">
    <p>Where to find simple texts?</p>
    <p>Opera is an art form in which singers</p>
    <p>and musicians perform a dramatic work</p>
    <p>combining text (called a libretto) and</p>
    <p>musical score.</p>
    <p>The performance is typically given in an</p>
    <p>opera house, accompanied by an</p>
    <p>orchestra or smaller musical ensemble.</p>
    <p>2018 by S. tajner &amp; H. Saggion 13</p>
  </div>
  <div class="page">
    <p>Where to find simple texts?</p>
    <p>2018 by S. tajner &amp; H. Saggion 14</p>
  </div>
  <div class="page">
    <p>Where to find simple texts?</p>
    <p>EASY NEWS</p>
    <p>2018 by S. tajner &amp; H. Saggion 15</p>
  </div>
  <div class="page">
    <p>Where to find simple texts? 8 Pages News Paper</p>
    <p>(Swedish)</p>
    <p>2018 by S. tajner &amp; H. Saggion 16</p>
  </div>
  <div class="page">
    <p>Klartale News Paper</p>
    <p>(Norwegian)</p>
    <p>Where to find simple texts?</p>
    <p>2018 by S. tajner &amp; H. Saggion 17</p>
  </div>
  <div class="page">
    <p>LEssentiel News Paper</p>
    <p>(French)</p>
    <p>Where to find simple texts?</p>
    <p>2018 by S. tajner &amp; H. Saggion 18</p>
  </div>
  <div class="page">
    <p>Dueparole News Paper</p>
    <p>(Italian)</p>
    <p>Where to find simple texts?</p>
    <p>2018 by S. tajner &amp; H. Saggion 19</p>
  </div>
  <div class="page">
    <p>LiteracyWorks Web site</p>
    <p>(English)</p>
    <p>Where to find simple texts?</p>
    <p>2018 by S. tajner &amp; H. Saggion 20</p>
  </div>
  <div class="page">
    <p>Simplification tasks</p>
    <p>Lexical simplification</p>
    <p>Replace complicated words and expressions by easier to read/understand substitutes (e.g. synonyms)</p>
    <p>Explain complicated words expressions by providing definitions/explanationsdefinitions/explanations</p>
    <p>Syntactic simplification</p>
    <p>Transform long and complex sentences into syntactic equivalents which could be easier to read/understand</p>
    <p>Could be addressed independently or jointly</p>
    <p>2018 by S. tajner &amp; H. Saggion 21</p>
  </div>
  <div class="page">
    <p>Simplification tasks</p>
    <p>The play was magnificent. =&gt; The play was brilliant.</p>
    <p>The boy had tuberculosis. =&gt; The boy had tuberculosis, a</p>
    <p>disease of the lungs.</p>
    <p>The festival was held in New Orleans, which was recovering</p>
    <p>from the hurricane. =&gt; The festival was held in New Orleans. from the hurricane. =&gt; The festival was held in New Orleans.</p>
    <p>New Orleans was recovering from the hurricane.</p>
    <p>The city was destroyed by the hurricane. =&gt; The hurricane</p>
    <p>destroyed the city.</p>
    <p>2018 by S. tajner &amp; H. Saggion 22</p>
  </div>
  <div class="page">
    <p>PART 2</p>
    <p>Automatic text simplificationAutomatic text simplification</p>
    <p>2018 by S. tajner &amp; H. Saggion 23</p>
  </div>
  <div class="page">
    <p>ATS systems</p>
    <p>Data-driven Rule-based</p>
    <p>Approaches to ATS</p>
    <p>Not enough parallel data!</p>
    <p>Lex Syn Data-driven</p>
    <p>MT</p>
    <p>(SMT and NMT)</p>
    <p>Tree Transduction</p>
    <p>Rule-based</p>
    <p>Language and genre specific!</p>
    <p>Time-consuming!</p>
    <p>Hybrid Lex Syn</p>
    <p>2018 by S. tajner &amp; H. Saggion 24</p>
  </div>
  <div class="page">
    <p>Stages of ATS</p>
    <p>Stage 1: Detection of necessary transformations</p>
    <p>Stage 2: Building ATS systems</p>
    <p>Stage 3: Evaluation of ATS systems Stage 3: Evaluation of ATS systems</p>
    <p>2018 by S. tajner &amp; H. Saggion 25</p>
  </div>
  <div class="page">
    <p>Detection of Necessary Transformations</p>
    <p>For humans:  Psycholinguistic theories (rule-based systems)</p>
    <p>Learning from parallel data (data-driven systems)</p>
    <p>Eye-tracking (tajner et al., 2017, BEA)</p>
    <p>Crowdsourcing (Yiman et al., 2017, RANLP)Crowdsourcing (Yiman et al., 2017, RANLP)</p>
    <p>Treat everything as potentially complex (Glava and tajner, 2015, ACL)</p>
    <p>For NLP applications:  Ideally from systems mistakes</p>
    <p>In practice: this step is skipped</p>
    <p>2018 by S. tajner &amp; H. Saggion 26</p>
  </div>
  <div class="page">
    <p>Building ATS Systems</p>
    <p>Modular Systems:  Lexical simplifier:</p>
    <p>Modular (CWI module, candidate generation, ranking, substitution)</p>
    <p>All-in-one</p>
    <p>Syntactic simplifier</p>
    <p>Adding information (e.g. definitions) Adding information (e.g. definitions)</p>
    <p>Elimination (content reduction)</p>
    <p>End-to-end systems:  MT-based systems (including the NMT-based system)</p>
    <p>Tree transduction systems</p>
    <p>2018 by S. tajner &amp; H. Saggion 27</p>
  </div>
  <div class="page">
    <p>Evaluation of ATS Systems</p>
    <p>Quality of the output</p>
    <p>Quality of the output</p>
    <p>Effectiveness/usefulness</p>
    <p>Reading speed</p>
    <p>Evaluation of ATS systems</p>
    <p>Automatic evaluation Human evaluation</p>
    <p>Readability measures</p>
    <p>MT evaluation metrics</p>
    <p>(BLEU, METEOR, TERp)</p>
    <p>(document level)</p>
    <p>Grammaticality</p>
    <p>Meaning preservation</p>
    <p>Simplicity</p>
    <p>(sentence level, 1-5 scale)</p>
    <p>Reading speed</p>
    <p>Comprehension</p>
    <p>2018 by S. tajner &amp; H. Saggion 28</p>
  </div>
  <div class="page">
    <p>Quality of the Output (Human Evaluation)</p>
    <p>Sentence G M S</p>
    <p>Madrid was occupied by French troops during the Napoleonic Wars, and Napoleons</p>
    <p>brother Joseph was installed on the throne. 5 / 4</p>
    <p>Madrid was occupied by French his soldiers during the Napoleonic Wars, and Napoleons</p>
    <p>brother Joseph was installed on the throne. 4 4 4</p>
    <p>brother Joseph was installed on the throne.</p>
    <p>Madrid was occupied by French troops during the Napoleonic Wars, and Napoleons</p>
    <p>brother Joseph was put on the throne. 5 5 5</p>
    <p>Madrid was occupied by French troops during the Napoleonic Wars, and Napoleons</p>
    <p>brother Joseph was RRB- installed on them on the throne. 3 3 3</p>
    <p>2018 by S. tajner &amp; H. Saggion 29</p>
  </div>
  <div class="page">
    <p>QATS Shared Task (LREC 2016)</p>
    <p>20 classification systems</p>
    <p>12 raw metrics</p>
    <p>The best systems (tajner, Popovic and Bchara, 2016) combine MT evaluation metrics (BLEU, METEOR, ) and 17 baseline MT quality evaluation metrics (BLEU, METEOR, ) and 17 baseline MT quality estimation features</p>
    <p>The highest weighted F-measure:</p>
    <p>Grammaticality: 71.84 (majority class  65.89)</p>
    <p>Meaning preservation: 68.07 (majority class  42.51)</p>
    <p>Simplicity: 56.42 (majority class  39.68)</p>
    <p>2018 by S. tajner &amp; H. Saggion 30</p>
  </div>
  <div class="page">
    <p>First steps: manual rules</p>
    <p>Rules over syntactic representations (Chandrasekar et al. 1996)  Superficial analysis (ckunking) to identify noun and verb groups  Rules: W X:NP, RELPRO Y, Z. =&gt; W X:NP Z. X:NP Y. (manually developed)</p>
    <p>Xi Jinping, who is the current Paramount Leader of the Peoples Republic of China, was visiting the USA.</p>
    <p>China, was visiting the USA.  W =</p>
    <p>X: = Xi Jinping</p>
    <p>RELPRO:=who</p>
    <p>Y = is the current Paramount Leader of the Peoples Republic of China</p>
    <p>Z= was visiting the USA</p>
    <p>Xi Jinping was visiting the USA. Xi Jinping is the current Paramount Leader of the Peoples Republic of China.</p>
    <p>2018 by S. tajner &amp; H. Saggion 31</p>
  </div>
  <div class="page">
    <p>First steps: rule learning</p>
    <p>Learning to transform from complex to simple (Chandrasekar &amp; Srinivas, 1996)</p>
    <p>(O) Talwinder Singh, who masterminded the 1984 Kanishka crash, was killed in a fierce</p>
    <p>two-hour encounter.</p>
    <p>(S) Talwinder Singh was killed in a fierce two-hour encounter. Talwinder Singh</p>
    <p>masterminded the 1984 Kanishka crash.</p>
    <p>ORIGINAL SIMPLIFICATION</p>
    <p>CUT COPY</p>
    <p>was killed</p>
    <p>the crash</p>
    <p>Talwinder</p>
    <p>Singh</p>
    <p>who</p>
    <p>mastermined</p>
    <p>in  encounter</p>
    <p>relative</p>
    <p>ORIGINAL was killed</p>
    <p>the crash</p>
    <p>Talwinder</p>
    <p>Singh</p>
    <p>mastermined</p>
    <p>in  encounter</p>
    <p>Talwinder</p>
    <p>Singh</p>
    <p>SIMPLIFICATION</p>
    <p>2018 by S. tajner &amp; H. Saggion 32</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>Lexical simplification is concerned with replacing words or short phrases by simpler variants in a context aware fashion (generally synonyms), which can be understood by a wider range of readers</p>
    <p>What is needed?</p>
    <p>Procedure to identify which words should be simplified</p>
    <p>Procedure to identify appropriate synonyms of the difficult words Procedure to identify appropriate synonyms of the difficult words</p>
    <p>Procedure to choose the simplest and more appropriate in the context</p>
    <p>Procedure to adjust context to comply with the changes</p>
    <p>First ever lexical simplifier was implemented in the PSET project</p>
    <p>WordNet was used as a source of synonyms</p>
    <p>No word sense disambiguation carried out</p>
    <p>Kucera-Francis frequency list used to chose simpler replacement</p>
    <p>2018 by S. tajner &amp; H. Saggion 33</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>(De Belder &amp; al. 2010) lexicon &amp; language model combined</p>
    <p>Given a word in a text, two lists of words are generated</p>
    <p>L1: list of synonyms from the lexical database (authoritative source)</p>
    <p>L2: list of alternative words obtained from a latent words language model (learning from non annotated data)model (learning from non annotated data)</p>
    <p>A probabilistic model estimates the probability of replacement of one word (original) by another word</p>
    <p>P1(w|w_original)=P2(w|w_original,context)*P3(easy|w)</p>
    <p>P2 is a language model that w fits in the given context</p>
    <p>P3 can be modelled in different ways: frequency, morphosyntactic properties, complexity based on database, etc.</p>
    <p>2018 by S. tajner &amp; H. Saggion 34</p>
  </div>
  <div class="page">
    <p>Lexical Simplification</p>
    <p>Extracting lexical simplifications using Wikipedia &amp; Simple Wikipedia (Yatskar et al. 2010) through edit history / versions</p>
    <p>Hypothesis: changes in Simple Wikipedia correspond to simplifications the author is making.(not always!)</p>
    <p>Objective: extract rules such as A  a, where A and a are synonyms and a is easier than A</p>
    <p>A mechanism is needed to model when the change of one word by another is due to a simplification  A mechanism is needed to model when the change of one word by another is due to a simplification operation</p>
    <p>Language model:</p>
    <p>One model computes the probability that the change of a word A by word a is due to: correction, simplification, etc.</p>
    <p>It is assumed that in the normal Wikipedia simplification changes are negligible</p>
    <p>It is also assumed that the proportion of corrections in Simple Wikipedia is equal to those in normal Wikipedia</p>
    <p>The probability of changing A by a p(a|A) is approximated by frequencies</p>
    <p>The model outputs the most probable replacement for A</p>
    <p>Point-wise Information Model:</p>
    <p>Search for replacements corresponding to simplify (editor explicitly saying so!) A and a is stronger using PMI (point-wise mutual information)</p>
    <p>2018 by S. tajner &amp; H. Saggion 35</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>Two baseline methods proposed:</p>
    <p>Frequency model: use the most frequent substitution</p>
    <p>Random model: chose a random valid substitution</p>
    <p>Compare with a list created automatically</p>
    <p>Human list of replacements &gt; Language Model &gt; PMI &gt; FREQ &gt;= RANDOM</p>
    <p>2018 by S. tajner &amp; H. Saggion 36</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>Biran et al. (2011) also use English Wikipedia (EW) and Simple</p>
    <p>English Wikipedia (ESW)</p>
    <p>EW is used to extract context vectors for each word (cooccurrences)</p>
    <p>A similarity measure can be used to identify which words can be replaced by which words</p>
    <p>The cosine between vector representations is used in this work</p>
    <p>Some filtering applied using WordNet</p>
    <p>2018 by S. tajner &amp; H. Saggion 37</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>Implementing the simplicity of a word: example canine and dog</p>
    <p>check occurrences of both words in EW and SEW</p>
    <p>canine appears 9620 times in EW</p>
    <p>canine appears 62 times in SEW canine appears 62 times in SEW</p>
    <p>dog appears 171000 times in EW</p>
    <p>dog appears 1360 times in SEW</p>
    <p>complexity(canine) = 9620/62 = 155</p>
    <p>complexity(dog) = 171000/1360 = 125</p>
    <p>2018 by S. tajner &amp; H. Saggion 38</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>The length of the word is also taken as a measure of complexity</p>
    <p>len(canine)=6, len(dog)=3</p>
    <p>final_complexity=complexity*len</p>
    <p>fc(canine)=155*6=930 fc(canine)=155*6=930</p>
    <p>fc(dog)=125*3=375</p>
    <p>canine is more difficult than dog</p>
    <p>So canine can be simplifies by dog, but dog can not be simplified with canine</p>
    <p>2018 by S. tajner &amp; H. Saggion 39</p>
  </div>
  <div class="page">
    <p>Lexical simplification</p>
    <p>Grammaticality: generate all equivalent pairs, if word in past tense then its simplification in past tense, etc.</p>
    <p>Chose as simplification of the target word w a replacement x that fits in the context</p>
    <p>Baseline: replace a word by its more frequent synonym Baseline: replace a word by its more frequent synonym</p>
    <p>Evaluation is a non-realistic scenario in the sense that only one word is simplified in the sentence  chose a sentence where only one word has been replaced by the</p>
    <p>method</p>
    <p>three variables evaluated: simplification(yes/no), grammaticality (bad/ok/good), sense (yes/no)</p>
    <p>the proposed method is better than the baseline</p>
    <p>2018 by S. tajner &amp; H. Saggion 40</p>
  </div>
  <div class="page">
    <p>Dealing with numbers</p>
    <p>Original Simplification</p>
    <p>Cerca de 1,9 millones de personas asistieron al Casi 2 millones de personas asistieron al concierto</p>
    <p>Simplification of numerical expressions in text (Bautista et al. 2012)</p>
    <p>(Bautista &amp; Saggion, 2014) studies the problem of how to make numbers and</p>
    <p>numerical expressions simpler by the use of rounding and addition/change of</p>
    <p>modifiers for Spanish</p>
    <p>Cerca de 1,9 millones de personas asistieron al</p>
    <p>concierto (About 1.9 million people attended the</p>
    <p>concert)</p>
    <p>Casi 2 millones de personas asistieron al concierto</p>
    <p>(Nearly 2 million people attended the concert)</p>
    <p>Slo se ha vendido un cuarto de las entradas (Only a</p>
    <p>quarter of the tickets have been sold)</p>
    <p>Slo se ha vendido  de las entradas (Only  of the</p>
    <p>tickets have been sold)</p>
    <p>Uno de cada cuatro nios hablan chino (One in four</p>
    <p>children speak Chinese)</p>
    <p>speak Chinese)</p>
    <p>Asistieron un 57% de la clase (57% of the class</p>
    <p>attended.)</p>
    <p>Asistieron mas de la mitad de la clase (More than</p>
    <p>half of the class attended)</p>
    <p>Aprobaron el 98% (98% passed.) Aprobaron casi todos (Almost everyone passed)</p>
    <p>2018 by S. tajner &amp; H. Saggion 41</p>
  </div>
  <div class="page">
    <p>Syntactic Simplification</p>
    <p>Siddharthan (2006) was concerned with generation issues during text simplification  sentence order, word choice, generation of referring expressions</p>
    <p>(1) Mr. Anthony, who runs an employment agency, decries program trading, but he isn't sure it should be strictly regulated.</p>
    <p>? (2a) Mr. Anthony decries program trading. (2b) Mr. Anthony runs an employment</p>
    <p>agency. (2c) But he isn't sure it should be strictly regulated. agency. (2c) But he isn't sure it should be strictly regulated.</p>
    <p>Tree stage approach: analysis, transformation, regeneration</p>
    <p>analysis: text chunking</p>
    <p>transformation: set of hand crafted rules</p>
    <p>regeneration: sentence ordering, anaphora, conjunctive cohesion (choice of</p>
    <p>connectives)</p>
    <p>More recently (Siddharthan, 2011) argues for the use of dependency relations in text simplification allowing him to better model and learn lexical transformations (Siddhartan &amp; Angrosh 2014)</p>
    <p>2018 by S. tajner &amp; H. Saggion 42</p>
  </div>
  <div class="page">
    <p>Learning simplification from parsing trees</p>
    <p>Based on a corpus of comparable documents &lt;C,S&gt; of complex and simplified versions (Zhu et al. 2010)</p>
    <p>English Wikipedia/Simple English Wikipedia</p>
    <p>Align EW &amp; SEW using a TF*IDF method and allow 1 to n alignments (PWKP dataset)alignments (PWKP dataset)</p>
    <p>This work models the following aspects:</p>
    <p>replacement of words and phrases</p>
    <p>syntactic simplification seen as composition of the following operations on a tree (Split, Drop, Copying, Reordering)</p>
    <p>2018 by S. tajner &amp; H. Saggion 43</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VPVP</p>
    <p>SBAR</p>
    <p>S</p>
    <p>PHRASE STRUCTURE OF COMPLEXT SENTENCE</p>
    <p>Learning simplification from parsing trees</p>
    <p>Augus</p>
    <p>t</p>
    <p>wa</p>
    <p>s</p>
    <p>th</p>
    <p>e</p>
    <p>mont</p>
    <p>h</p>
    <p>sixth</p>
    <p>i</p>
    <p>n</p>
    <p>th</p>
    <p>e</p>
    <p>ancien</p>
    <p>t</p>
    <p>Roman calenda</p>
    <p>r</p>
    <p>whic</p>
    <p>h</p>
    <p>starte</p>
    <p>d i</p>
    <p>n 735BC</p>
    <p>NP</p>
    <p>NP PP</p>
    <p>NP</p>
    <p>WHNP</p>
    <p>VPVP</p>
    <p>PP</p>
    <p>August was the sixth month in the ancient Roman calendar which started in 735BC.</p>
    <p>2018 by S. tajner &amp; H. Saggion 44</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VPVP</p>
    <p>SBAR</p>
    <p>S</p>
    <p>SPLIT PROBABILITIES</p>
    <p>ASSOCIATED TO</p>
    <p>CUTTING/SPLIT POINTS</p>
    <p>Learning simplification from parsing trees</p>
    <p>Augus</p>
    <p>t</p>
    <p>was th</p>
    <p>e</p>
    <p>mont</p>
    <p>h</p>
    <p>sixth</p>
    <p>i</p>
    <p>n</p>
    <p>th</p>
    <p>e</p>
    <p>ancien</p>
    <p>t</p>
    <p>Roman calenda</p>
    <p>r</p>
    <p>whic</p>
    <p>h</p>
    <p>starte</p>
    <p>d i</p>
    <p>n 735BC</p>
    <p>NP</p>
    <p>NP PP</p>
    <p>NP</p>
    <p>WHNP</p>
    <p>VPVP</p>
    <p>PP</p>
    <p>2018 by S. tajner &amp; H. Saggion 45</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VPVP</p>
    <p>S</p>
    <p>COPYING SUBJECTS</p>
    <p>NP</p>
    <p>PROBABILITIES</p>
    <p>OF COPYING</p>
    <p>A COMPONENT</p>
    <p>Learning simplification from parsing trees</p>
    <p>Augus</p>
    <p>t</p>
    <p>wa</p>
    <p>s</p>
    <p>i</p>
    <p>n th</p>
    <p>e</p>
    <p>starte</p>
    <p>d i</p>
    <p>n 735BC</p>
    <p>NP</p>
    <p>NP PP</p>
    <p>VPVP</p>
    <p>PP</p>
    <p>ancien</p>
    <p>t</p>
    <p>Roman calenda</p>
    <p>r</p>
    <p>NP th</p>
    <p>e mont</p>
    <p>h</p>
    <p>sixth</p>
    <p>ancien</p>
    <p>t Roman</p>
    <p>calenda</p>
    <p>r</p>
    <p>NP</p>
    <p>th</p>
    <p>e</p>
    <p>2018 by S. tajner &amp; H. Saggion 46</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VPVP</p>
    <p>S</p>
    <p>DELETE AND REORDERING</p>
    <p>COMPONENTS</p>
    <p>NP</p>
    <p>PROBABILITIES</p>
    <p>OF DELETING AND</p>
    <p>REORDERING</p>
    <p>Learning simplification from parsing trees</p>
    <p>Augus</p>
    <p>t</p>
    <p>wa</p>
    <p>s</p>
    <p>i</p>
    <p>n th</p>
    <p>e</p>
    <p>starte</p>
    <p>d i</p>
    <p>n 735BC</p>
    <p>NP</p>
    <p>NP PP</p>
    <p>VPVP</p>
    <p>PP</p>
    <p>ancien</p>
    <p>t</p>
    <p>calenda</p>
    <p>r</p>
    <p>NP th</p>
    <p>e mont</p>
    <p>h</p>
    <p>sixth</p>
    <p>ancien</p>
    <p>t calenda</p>
    <p>r</p>
    <p>NP</p>
    <p>th</p>
    <p>e</p>
    <p>2018 by S. tajner &amp; H. Saggion 47</p>
  </div>
  <div class="page">
    <p>S</p>
    <p>VPVP</p>
    <p>S</p>
    <p>WORD SUBSTITUTION</p>
    <p>NP</p>
    <p>PROBABILITY OF</p>
    <p>REPLACING A WORD</p>
    <p>Learning simplification from parsing trees</p>
    <p>Augus</p>
    <p>t</p>
    <p>wa</p>
    <p>s</p>
    <p>i</p>
    <p>n th</p>
    <p>e</p>
    <p>starte</p>
    <p>d i</p>
    <p>n 735BC</p>
    <p>NP</p>
    <p>NP PP</p>
    <p>VPVP</p>
    <p>PP</p>
    <p>old calenda</p>
    <p>r</p>
    <p>NP th</p>
    <p>e mont</p>
    <p>h</p>
    <p>sixt</p>
    <p>h</p>
    <p>old</p>
    <p>calenda</p>
    <p>r</p>
    <p>NP</p>
    <p>th</p>
    <p>e</p>
    <p>August was the sixth month in the old calendar. The old calendar started in 735BC.</p>
    <p>2018 by S. tajner &amp; H. Saggion 48</p>
  </div>
  <div class="page">
    <p>Syntactic Simplification by Optimization</p>
    <p>Woodsend &amp; Lapata (2011) propose two components to learn to simplify English  Learn from corpora simplification transformations</p>
    <p>Optimize rule application  Given a sentence produce all possible simplifications licensed by the</p>
    <p>grammar</p>
    <p>select the simplest one using a number of constraints</p>
    <p>Quasi-synchronous grammars allow them to model non-isomorphic transformations  lexical rules and splitting rules are extracted from aligned corpus</p>
    <p>Integer Linear programming (ILP) is used to select an optimal simplification  cost function: grammaticality + readability</p>
    <p>ILP is also applied to English by De Belder (2014) and to French by Brouwers et al. (2014)</p>
    <p>2018 by S. tajner &amp; H. Saggion 49</p>
  </div>
  <div class="page">
    <p>Event-Based ATS System (EventSimplify)</p>
    <p>The core idea:  Events constitute relevant information in news  Descriptive (parts of) sentences not denoting events are</p>
    <p>informationally less relevant</p>
    <p>Semantic content reduction based on information relevance (opposed to traditional lexical and syntactic simplification)(opposed to traditional lexical and syntactic simplification)</p>
    <p>Two event-based simplification schemes:  Sentence-wise  Event-wise</p>
    <p>Evaluation:</p>
    <p>readability (automated),  grammaticality and information relevance (human)</p>
    <p>2018 by S. tajner &amp; H. Saggion 50</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Original</p>
    <p>Baset al-Megrahi, the Libyan intelligence officer who was</p>
    <p>convicted in the 1988 Lockerbie bombing has died at his home in</p>
    <p>Tripoli, nearly three years after he was released from a Scottish</p>
    <p>prison. There were complications from prostate cancer and his</p>
    <p>funeral would take place on Monday.funeral would take place on Monday.</p>
    <p>Simplification</p>
    <p>Baset al-Megrahi, convicted in the 1998 Lockerbie bombing has</p>
    <p>died at his home in Tripoli. Three years earlier he was released</p>
    <p>from a Scottish prison.</p>
    <p>2018 by S. tajner &amp; H. Saggion 51</p>
  </div>
  <div class="page">
    <p>EventSimplify</p>
    <p>Build upon state-of-the-art event extraction system (Glava and najder, 2013)</p>
    <p>Extract only factual events  Non-factual events (negated, uncertain) generally contain less  Non-factual events (negated, uncertain) generally contain less</p>
    <p>important information</p>
    <p>Two step process:  Supervised extraction of factual event mentions</p>
    <p>Application of event-centred simplification schemes (two different schemes)</p>
    <p>2018 by S. tajner &amp; H. Saggion 52</p>
  </div>
  <div class="page">
    <p>Simplification Example</p>
    <p>Original</p>
    <p>Baset al-Megrahi, the Libyan intelligence officer who was convicted in the 1988</p>
    <p>Lockerbie bombing has died at his home in Tripoli, nearly three years after he was</p>
    <p>released from a Scottish prison.</p>
    <p>Sentence-wise simplification</p>
    <p>Baset al-Megrahi was convicted in the 1988 Lockerbie bombing has died at hisBaset al-Megrahi was convicted in the 1988 Lockerbie bombing has died at his</p>
    <p>home after he was released from a Scottish prison.</p>
    <p>Event-wise simplification</p>
    <p>Baset al-Megrahi was convicted in the 1988 Lockerbie bombing. Baset al Megrahi</p>
    <p>has died at his home. He was released from a Scottish prison.</p>
    <p>Event-wise with pron. anaphora resolution</p>
    <p>Baset al-Megrahi was convicted in the 1988 Lockerbie bombing. Baset al-Megrahi</p>
    <p>has died at his home. Baset al-Megrahi was released from a Scottish prison.</p>
    <p>2018 by S. tajner &amp; H. Saggion 53</p>
  </div>
  <div class="page">
    <p>Evaluation of EventSimplify</p>
    <p>Readability (automatically)</p>
    <p>Grammaticality (human)</p>
    <p>Information relevance (human)</p>
    <p>Evaluated on text snippets (280 in total)</p>
    <p>Baseline: retains only the main clause of a sentence and discards all subordinate clauses</p>
    <p>2018 by S. tajner &amp; H. Saggion 54</p>
  </div>
  <div class="page">
    <p>Human Evaluation</p>
    <p>Aspect Weighted kappa Pearson MAE</p>
    <p>Grammaticality 0.68 0.77 0.18</p>
    <p>Meaning 0.53 0.67 0.37</p>
    <p>Simplicity 0.54 0.60 0.28</p>
    <p>IAA</p>
    <p>Relevance = harmonic mean of Meaning and Simplicity</p>
    <p>Scheme Grammaticality (1  3) Relevance (1  3)</p>
    <p>Baseline 2.57  0.79 1.90  0.64</p>
    <p>Sentence-wise 1.98  0.80 2.12  0.61</p>
    <p>Event-wise 2.70  0.52 2.30  0.54</p>
    <p>Pronominal anaphora 2.68  0.56 2.39  0.57</p>
    <p>2018 by S. tajner &amp; H. Saggion 55</p>
  </div>
  <div class="page">
    <p>PART 3</p>
    <p>TS ProjectsTS Projects</p>
    <p>2018 by S. tajner &amp; H. Saggion 56</p>
  </div>
  <div class="page">
    <p>First ever project to address needs of people with language impairment  Targeted simplification for people with aphasia</p>
    <p>Aphasia: language impairment following brain injury</p>
    <p>People with aphasia may benefit from Augmentative, Alternative Communication (AAC) technology</p>
    <p>Complex language in newspapers</p>
    <p>PSET Project</p>
    <p>Complex language in newspapers  Twenty-five-year-old blonde-haired mother-of-two Jane Smith....</p>
    <p>Pronouns may be difficult to interpret</p>
    <p>Passive-voice constructions may be difficult to understand</p>
    <p>2018 by S. tajner &amp; H. Saggion 57</p>
  </div>
  <div class="page">
    <p>In PSET sentence for aphasic people should (Devlin and Tait, 1998):  Follow the Subject-Verb-Object (SVO) pattern</p>
    <p>Be in active voice</p>
    <p>Be as short as possible</p>
    <p>Contain only one adjective per noun</p>
    <p>Be chronologically ordered in the text</p>
    <p>Be semantically non-reversible</p>
    <p>PSET Project</p>
    <p>Be semantically non-reversible</p>
    <p>PSET system components  Syntactic simplifier</p>
    <p>Anaphora resolver/substitution component</p>
    <p>Lexical simplifier</p>
    <p>2018 by S. tajner &amp; H. Saggion 58</p>
  </div>
  <div class="page">
    <p>The Simplext Project (Saggion et al., 2011; 2015)</p>
    <p>A text simplification system in Spanish for people with cognitive disabilities (e.g. Down syndrome)</p>
    <p>Lexical and syntactic simplification components implemented</p>
    <p>System developed based on creation/analysis of a parallel corpus of original (short) news and their simplified versions: the Simplext corpus</p>
    <p>ORIGINAL &amp; SIMPLIFIED</p>
    <p>SENTENCES IN SIMPLEXT</p>
    <p>2018 by S. tajner &amp; H. Saggion 59</p>
  </div>
  <div class="page">
    <p>Estados Unidos trata muy mal a un soldado</p>
    <p>detenido.</p>
    <p>El soldado se llama Bradley Manning.</p>
    <p>Bradley Manning est detenido por</p>
    <p>INFORMATIO</p>
    <p>N</p>
    <p>PROVIDER</p>
    <p>SIMPLIFICATIO</p>
    <p>N</p>
    <p>EXPERT COMPLEX</p>
    <p>SENTENCE</p>
    <p>Amnista Internacional acus a las</p>
    <p>autoridades estadounidenses de Bradley Manning est detenido por</p>
    <p>dar informacin del gobierno de Estados Unidos</p>
    <p>a Wikileaks.</p>
    <p>Wikileaks es una pgina web donde se da</p>
    <p>informacin sobre asuntos de inters pblico.</p>
    <p>.</p>
    <p>SENTENCES</p>
    <p>autoridades estadounidenses de</p>
    <p>proporcionar un &quot;trato</p>
    <p>inhumano&quot; a Bradley Manning,</p>
    <p>un soldado acusado de filtrar</p>
    <p>cables de la diplomacia</p>
    <p>norteamericana al portal</p>
    <p>Wikileaks.</p>
    <p>2018 by S. tajner &amp; H. Saggion 60</p>
  </div>
  <div class="page">
    <p>ALIGNMENT</p>
    <p>PROGRAM</p>
    <p>Amnista Internacional acus a las</p>
    <p>autoridades estadounidenses de</p>
    <p>proporcionar un &quot;trato inhumano&quot; a</p>
    <p>Bradley Manning, un soldado</p>
    <p>acusado de filtrar cables de la</p>
    <p>diplomacia norteamericana al portal</p>
    <p>Wikileaks.</p>
    <p>Estados Unidos trata muy mal a un soldado detenido. El soldado se llama Bradley Manning. Bradley Manning est detenido por dar informacin del gobierno de</p>
    <p>ALIGNMENT</p>
    <p>TABLES</p>
    <p>BI-TEXT</p>
    <p>EDITOR</p>
    <p>CORPUS</p>
    <p>for</p>
    <p>RESEARCH</p>
    <p>GATE bi-text editor</p>
    <p>2018 by S. tajner &amp; H. Saggion 61</p>
  </div>
  <div class="page">
    <p>Syntactic simplification approach (Bott et al. 2012)</p>
    <p>Rule-based approach that transforms dependency graphs into new sentens</p>
    <p>Performs various sentence splitting operations on subordinate and coordinate structures</p>
    <p>Copies subjects and verbs (e.g. relative pronoun replaced by  Copies subjects and verbs (e.g. relative pronoun replaced by lexical NP)</p>
    <p>Orders the various clauses</p>
    <p>Produces the output from the resulting dependency graphs</p>
    <p>Tools: dependency parser (Bohnet, 2009) and MATE framework (Bohnet et al., 2000)</p>
    <p>2018 by S. tajner &amp; H. Saggion 62</p>
  </div>
  <div class="page">
    <p>Working on Dependencies</p>
    <p>2018 by S. tajner &amp; H. Saggion 63</p>
  </div>
  <div class="page">
    <p>Other simplification operations (Drndarevic &amp; Saggion, 2012)</p>
    <p>Transformation of nouns/adjectives of nationality</p>
    <p>Tunisian authorities =&gt; authorities of Tunisia</p>
    <p>Transformation of numerical expressions</p>
    <p>year clarification 2010 =&gt; the year 2010</p>
    <p>deletion end of may of 2010 =&gt; 2010 deletion end of may of 2010 =&gt; 2010</p>
    <p>small numbers in digits seven books =&gt; 7 books</p>
    <p>replace by definition (2 decades =&gt; 20 years)</p>
    <p>Normalization of reporting verbs to the form decir (say)</p>
    <p>X explained that Y =&gt; X said that Y</p>
    <p>Removal of information in parenthesis</p>
    <p>2018 by S. tajner &amp; H. Saggion 64</p>
  </div>
  <div class="page">
    <p>Simplext simplification portal</p>
    <p>2018 by S. tajner &amp; H. Saggion 65</p>
  </div>
  <div class="page">
    <p>Simplext full evaluation</p>
    <p>Evaluation of syntactic simplification module, rule-based simplification, and whole system (Drndarevic et al., 2013; Saggion et al. 2015)</p>
    <p>tested the degree of simplification achieved =&gt; readability indicesindices</p>
    <p>tested the grammaticality and meaning preservation =&gt; human evaluation</p>
    <p>readability + understandability =&gt; target user evaluation</p>
    <p>2018 by S. tajner &amp; H. Saggion 66</p>
  </div>
  <div class="page">
    <p>Readability Assessment</p>
    <p>Degree of simplification</p>
    <p>a set of readability indices</p>
    <p>2018 by S. tajner &amp; H. Saggion 67</p>
  </div>
  <div class="page">
    <p>Readability Assessment</p>
    <p>Differences between text conditions</p>
    <p>In general manual and automatic simplification reduce the value of the readability indexes</p>
    <p>2018 by S. tajner &amp; H. Saggion 68</p>
  </div>
  <div class="page">
    <p>Simplext full evaluation</p>
    <p>Grammaticality and meaning preservation</p>
    <p>25 human evaluators asked to read and assess original and simplified sentences</p>
    <p>Questionnaires with 38 pairs of original (O) and simplified (S) sentencessentences</p>
    <p>Pairs O-S contained at least one lexical change and one syntactic change</p>
    <p>Order was altered random to counterbalance the sequence effect</p>
    <p>2018 by S. tajner &amp; H. Saggion 69</p>
  </div>
  <div class="page">
    <p>Non-target User Evaluation</p>
    <p>Grammaticality and meaning preservation</p>
    <p>Questions:</p>
    <p>Answers on Likert scale: 1 completely disagree  5 completely agree Answers on Likert scale: 1 completely disagree  5 completely agree</p>
    <p>2018 by S. tajner &amp; H. Saggion 70</p>
  </div>
  <div class="page">
    <p>Able to Include</p>
    <p>To bring for people with intellectual disabilities application which integrates:</p>
    <p>Text Simplification</p>
    <p>Text to Pictogram / Pictogram to Text Translation</p>
    <p>Speech Synthesis Speech Synthesis</p>
    <p>Text Simplification</p>
    <p>Spanish: technology from Simplext</p>
    <p>English: new software based on Simplext</p>
    <p>2018 by S. tajner &amp; H. Saggion 71</p>
  </div>
  <div class="page">
    <p>Syntactic Simplification System</p>
    <p>Linguistically motivated approach</p>
    <p>Identification of phenomena known to cause reading/understanding problems</p>
    <p>NLP pipeline composed of the following modules:</p>
    <p>Document Analysis  Tokenization and Sentence Splitting Tokenization and Sentence Splitting</p>
    <p>Mate Parser (Bohnet, 2010) trained on the CoNLL dataset</p>
    <p>A set of grammars implemented in the JAPE language</p>
    <p>identify complex syntactic constructions</p>
    <p>annotate the text with useful information to facilitate re-writing</p>
    <p>Sentence Generation / re-writing  Java programs deal with the transformation of the sentence (copying,</p>
    <p>reordering, capitalization, etc.)</p>
    <p>Iterative process</p>
    <p>2018 by S. tajner &amp; H. Saggion 72</p>
  </div>
  <div class="page">
    <p>passive constructions a. The release was accompanied by a number of TV appearances, including</p>
    <p>a full hour on On the Record.</p>
    <p>b. A number of TV appearances, including a full hour on On the Record accompanied the release.</p>
    <p>appositive constructions a. The moon is named after Portia, the heroine of William Shakespeare's</p>
    <p>play The Merchant of Venice.</p>
    <p>b. The moon is named after Portia. Portia is the heroine of William Shakespeare's play The Merchant of Venice.</p>
    <p>relative clauses a. The festival was held in New Orleans, which was recovering from</p>
    <p>Hurricane Katrina.</p>
    <p>S y</p>
    <p>n ta</p>
    <p>c ti</p>
    <p>c</p>
    <p>P h</p>
    <p>e n</p>
    <p>o m</p>
    <p>e n</p>
    <p>a</p>
    <p>a. The festival was held in New Orleans, which was recovering from Hurricane Katrina.</p>
    <p>b. The festival was held in New Orleans. New Orleans was recovering from Hurricane Katrina.</p>
    <p>coordinated constructions a. Tracy killed 71 people, caused $ 837 million in damage and destroyed</p>
    <p>more than 70 percent of Darwin's buildings, including 80 percent of houses.</p>
    <p>b. Tracy killed 71 people. Tracy caused $ 837 million in damage. And Tracy destroyed more than 70 percent of Darwin's buildings, including 80 percent of houses.</p>
    <p>S y</p>
    <p>n ta</p>
    <p>c ti</p>
    <p>c</p>
    <p>P h</p>
    <p>e n</p>
    <p>o m</p>
    <p>e n</p>
    <p>a</p>
    <p>2018 by S. tajner &amp; H. Saggion 73</p>
  </div>
  <div class="page">
    <p>correlated correlatives</p>
    <p>a. A hypothesis requires more work by the researcher in order to either confirm or disprove it.</p>
    <p>b. A hypothesis requires more work by the researcher in order to confirm it. Or a hypothesis requires more work by the researcher in order to disprove it.</p>
    <p>subordinate clauses</p>
    <p>a. He is perhaps best known for his design for the Natural History Museum in London, although he also built a wide variety of other buildings throughout the country.</p>
    <p>b. He also built a wide variety of other buildings thought</p>
    <p>S y</p>
    <p>n ta</p>
    <p>c ti</p>
    <p>c</p>
    <p>P h</p>
    <p>e n</p>
    <p>o m</p>
    <p>e n</p>
    <p>a</p>
    <p>b. He also built a wide variety of other buildings thought the country. But he is perhaps best known for his design for the Natural History Museum in London.</p>
    <p>adverbial clauses</p>
    <p>a. Oxfordshire is a county in the South East England region, bordering on Northamptonshire, Buckinghamshire, Berkshire, Wiltshire, Gloucestershire and Warwickshire.</p>
    <p>b. Oxfordshire is a county in the South East England region. Oxfordshire borders on Northamptonshire, Buckinghamshire, Berkshire, Wiltshire, Gloucestershire and Warwickshire.</p>
    <p>S y</p>
    <p>n ta</p>
    <p>c ti</p>
    <p>c</p>
    <p>P h</p>
    <p>e n</p>
    <p>o m</p>
    <p>e n</p>
    <p>a</p>
    <p>2018 by S. tajner &amp; H. Saggion 74</p>
  </div>
  <div class="page">
    <p>Identification and Annotation</p>
    <p>Left Hand Right Side</p>
    <p>(regular pattern)(regular pattern)</p>
    <p>2018 by S. tajner &amp; H. Saggion 75</p>
  </div>
  <div class="page">
    <p>Identification and Annotation</p>
    <p>Murrurundi railway station is located on the Main North railway line, 352 km from</p>
    <p>Sydney</p>
    <p>Murrurundi railway station is located on the Main North railway line. The Main North</p>
    <p>railway line is 352 km from Sydney.</p>
    <p>2018 by S. tajner &amp; H. Saggion 76</p>
  </div>
  <div class="page">
    <p>English Rule-based System</p>
    <p>Appositions: 1 rule</p>
    <p>Relative clauses: 17 rules</p>
    <p>Coordination: 10 rules</p>
    <p>Correlatives: 4 rules</p>
    <p>Subordination: 8 rules Subordination: 8 rules</p>
    <p>Adverbial: 12 rules</p>
    <p>Passive: 14 rules</p>
    <p>Rules are applied iteratively until no more simplifications are fired</p>
    <p>2018 by S. tajner &amp; H. Saggion 77</p>
  </div>
  <div class="page">
    <p>Evaluation of English Rules</p>
    <p>Right Wrong Ignored</p>
    <p>Apposition 79% 21% 0%</p>
    <p>Relative Clause 79% 14% 7%</p>
    <p>Coordination 56% 6% 38%</p>
    <p>Subordination 72% 25% 3%</p>
    <p>E Parser Rules</p>
    <p>Apposition 19 2</p>
    <p>Relative Clause 14 0</p>
    <p>Coordination 4 2</p>
    <p>Subordination 7 18</p>
    <p>ERRORSRULES</p>
    <p>Subordination 72% 25% 3%</p>
    <p>Passive 85% 6% 9%</p>
    <p>Total 74% 14% 11%</p>
    <p>Subordination 7 18</p>
    <p>Passive 5 1</p>
    <p>Total 49 23</p>
    <p>Evaluation Dataset (English Wikipedia Simplification Dataset)</p>
    <p>100 sentences for each phenomena</p>
    <p>right: all required elements correctly identified</p>
    <p>wrong: at least one element is missing</p>
    <p>ignored: structure not identified</p>
    <p>2018 by S. tajner &amp; H. Saggion 78</p>
  </div>
  <div class="page">
    <p>Lexical Simplification System</p>
    <p>System architecture:</p>
    <p>Document analysis</p>
    <p>Complex word detection/identification</p>
    <p>Word sense disambiguation</p>
    <p>Synonym ranking Synonym ranking</p>
    <p>Language realization</p>
    <p>2018 by S. tajner &amp; H. Saggion 79</p>
  </div>
  <div class="page">
    <p>Document Analysis</p>
    <p>Objective: Linguistically analyze the input document</p>
    <p>Approach: Make use of available Natural Language Processing Tools</p>
    <p>GATE (Cunningham et al. 2002)  well known library for Natural Language Processing Language Processing</p>
    <p>ANNIE pipeline from the GATE system  Tokenization  identify words</p>
    <p>Sentence Splitting  identify sentences</p>
    <p>Part-of-Speech (POS) tagging  identity the category of each word</p>
    <p>Lemmatization  obtain the lemma of each word</p>
    <p>Named Entity Recognition and Classification  recognize names of people, places, etc.</p>
    <p>2018 by S. tajner &amp; H. Saggion 80</p>
  </div>
  <div class="page">
    <p>Complex Word Detection</p>
    <p>Goal: to detect which words might be complex for the target audience</p>
    <p>Approach: rely on available psycholinguistic data  Age-of-Acquisition (AoA) norms (Kuperman et al., 2012)</p>
    <p>Ratings of the age at which words are learned Ratings of the age at which words are learned</p>
    <p>English words: nouns, verbs, and adjectives</p>
    <p>30121 rating (51715 inflected words)</p>
    <p>Kucera-Francis word frequency counts (Kucera &amp; Francis, 1967)  43299 words (all POS categories)</p>
    <p>Extracted from the Brown Corpus (over 1M words)</p>
    <p>2018 by S. tajner &amp; H. Saggion 81</p>
  </div>
  <div class="page">
    <p>Word Sense Disambiguation</p>
    <p>Goal: obtaining the most appropriate sense for a given word in a given context</p>
    <p>Approach: Vector Space Model approach for Lexical Semantics</p>
    <p>Dictionary of Target Words and Senses: OpenThesaurus (transformed)</p>
    <p>bill, measure  &gt; sense_1</p>
    <p>We model each target word in the dictionary (and each sense) as a vector of context words extracted from text collections (e.g. Simple Wikipedia)</p>
    <p>Given a complex word in a sentence we compare its context against the vectors in the dictionary and select the most appropriate list of synonyms</p>
    <p>bill, measure  &gt; sense_1</p>
    <p>bill, note, banknote - &gt; sense_2</p>
    <p>bill, invoice -&gt; sense_3</p>
    <p>bill</p>
    <p>2018 by S. tajner &amp; H. Saggion 82</p>
  </div>
  <div class="page">
    <p>Synonym Ranking</p>
    <p>Goal: obtain the simplest and most appropriate synonym word for the given context</p>
    <p>Approach: Ranking of synonyms by lexical simplicity</p>
    <p>Measures used: word frequency (more frequent means  Measures used: word frequency (more frequent means simpler)</p>
    <p>Resources</p>
    <p>British National Corpus (BNC) frequency list, Google Web 1T Corpus unigram frequencies, Simple English Wikipedia : frequency counts, Normal English Wikipedia: frequency counts, Kucera-Francis norms, Age-of-Acquisition rankings</p>
    <p>2018 by S. tajner &amp; H. Saggion 83</p>
  </div>
  <div class="page">
    <p>Language Realization</p>
    <p>Goal: Generate the correct inflected forms of the final selected synonym word substitutes in context</p>
    <p>Approach: Use the SimpleNLG Java API</p>
    <p>Default SimpleNLG Lexicon used</p>
    <p>Rules that use lemmas, and Part-of-Speech tags of the simple  Rules that use lemmas, and Part-of-Speech tags of the simple word and context</p>
    <p>Complement this module with heuristics to repair contexts</p>
    <p>an automobile -&gt; a car</p>
    <p>2018 by S. tajner &amp; H. Saggion 84</p>
  </div>
  <div class="page">
    <p>Overall architecture</p>
    <p>2018 by S. tajner &amp; H. Saggion 85</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>The poem was composed by the renowned artist</p>
    <p>compose and renowned detected as complex words because of their low frequency</p>
    <p>compose has various meanings: {write} {compile} {pen, write, indite}.</p>
    <p>renowned has one meaning: {celebrated, famous,.notable, noted}</p>
    <p>write selected as substitute of compose</p>
    <p>famous selected as substitute of renowned</p>
    <p>2018 by S. tajner &amp; H. Saggion 86</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>The poem was composed by the renowned artist</p>
    <p>Passive rule activated</p>
    <p>The poem was written by the famous artist</p>
    <p>SUB JNMO</p>
    <p>D</p>
    <p>LGS</p>
    <p>PMOD</p>
    <p>NMO D</p>
    <p>NMO D</p>
    <p>Passive rule activated</p>
    <p>passive subject (the poem), by-agent (the famous artist)</p>
    <p>verb is re-written: was written =&gt; wrote</p>
    <p>elements re-ordered</p>
    <p>Result: The famous artist wrote the poem.</p>
    <p>PMOD</p>
    <p>2018 by S. tajner &amp; H. Saggion 87</p>
  </div>
  <div class="page">
    <p>Demonstrations</p>
    <p>Simplext / Able to Include (interface)</p>
    <p>Simplification embedded in e-mail application (interface)</p>
    <p>Multilingual lexical simplifier (console)</p>
    <p>2018 by S. tajner &amp; H. Saggion 88</p>
  </div>
  <div class="page">
    <p>PorSimples project</p>
    <p>A text simplification system Brazilian Portuguese aimed at people with low-literacy (11% of Brazilian population)</p>
    <p>Mainly syntactic simplification developed</p>
    <p>Corpus developed for the project containing newspaper articles and two different simple versions (natural and strong)articles and two different simple versions (natural and strong)</p>
    <p>2018 by S. tajner &amp; H. Saggion 89</p>
  </div>
  <div class="page">
    <p>PorSimples</p>
    <p>Simplifications in the PorSimples corpus</p>
    <p>Appositions; relative clauses; subordinate clauses; coordinate clauses; sentences w/non-finite verbs; passive voice</p>
    <p>Rule-based syntactic simplification</p>
    <p>Hand-made developed procedures applied in cascade  Passive voice, apposition, subordination, non-restrictive clauses,  Passive voice, apposition, subordination, non-restrictive clauses,</p>
    <p>restrictive relatives, and coordination</p>
    <p>Iterative process: sentence is re-parsed after each rule application</p>
    <p>Treatment of relative clauses:  Input: The book, which John gave me, belongs to Paul</p>
    <p>Output: The book belongs to Paul. John gave me the book.</p>
    <p>2018 by S. tajner &amp; H. Saggion 90</p>
  </div>
  <div class="page">
    <p>PorSimples</p>
    <p>Learning components in PorSimples  Sentence split algorithm using Support Vector Machine classifier (features</p>
    <p>from basic text analysis + rhetorical relation inspired features)</p>
    <p>Simplification as machine translation using the corpus  Specia (2010)  First to cast text simplification as a kind of translation problem  The MOSES standard phrase-based Statistical Machine Translation system used for training a model  CorpusCorpus</p>
    <p>3383 sentence pairs  500 sentence pairs for additional tuning  500 sentence pairs for testing</p>
    <p>Results in terms of BLUE metric 0.60  System is very cautious when performing simplification resulting in an output too similar to the input</p>
    <p>SIMPLIFICA is a project related to PorSimples whose aim is to provide an authoring tool for the production of adapted texts</p>
    <p>Incorporates readability-assessment prediction</p>
    <p>Proposes simplifications to the user who can correct them to achieve good text quality</p>
    <p>2018 by S. tajner &amp; H. Saggion 91</p>
  </div>
  <div class="page">
    <p>Simplification for people with autism</p>
    <p>The FIRST project Flexible Interactive Reading Support Tool (Mitkov, 2012; Martn-Valdivia et al., 2014)</p>
    <p>Reading obstacles faced by people with autism:  complex sentences, ambiguity, figurative language, rare and specialized</p>
    <p>terms, etc.</p>
    <p>Open Book a multilingual (Spanish, Bulgarian, English) tool simplifies  Open Book a multilingual (Spanish, Bulgarian, English) tool simplifies and summarizes input text, replace or explains difficult vocabulary, provides navigation mechanisms, add pictograms for better understanding....</p>
    <p>Simplification is not only for the final user but also for their carers who can adapt input text using the tool</p>
    <p>(Dornescu et al, 2014) describe how relative clauses and other syntactic phenomena are dealt with in FIRST</p>
    <p>Rule-based + sequence learning (CRFs) to identify complex constructions</p>
    <p>2018 by S. tajner &amp; H. Saggion 92</p>
  </div>
  <div class="page">
    <p>More on simplifying for end users</p>
    <p>Adapting texts for people with dyslexia (Rello et al. 2013)  People with dyslexia have problems with word recognition</p>
    <p>Word frequency and length are factors which influence readability and comprehensibility in people with dyslexia</p>
    <p>Adapting texts to poor readers (Williams and Reiter, 2005) Adapting texts to poor readers (Williams and Reiter, 2005)  Natural Language Generation techniques addressing word selection,</p>
    <p>sentence generation, and discourse</p>
    <p>Easy to understand words, short sentences, simple syntactic structures, easy to understand discourse connectives, etc.</p>
    <p>2018 by S. tajner &amp; H. Saggion 93</p>
  </div>
  <div class="page">
    <p>Simplifying for NLP</p>
    <p>To improve parsing results (Jonnalagadda et al. 2009)  Concerned with problems when parsing biomedical texts with parsers developed</p>
    <p>for newspaper articles</p>
    <p>Simplifying named entities (e.g. gene names into placeholders), rudimentary segmentation of sentences based on puctuation</p>
    <p>To improve summarization output (Lal and Rger, 2002; Siddarthan et al. 2004)2004)  Lexical simplification of the output summary</p>
    <p>Simplification before content selection</p>
    <p>To improve information extraction (Evans, 2011)  Classification algorithm to detect and segment different types of coordination</p>
    <p>Simple IE patterns over simplified input work better than complex IE patterns over non-simplified input</p>
    <p>To improve comprehension of patent documents (Bouyad-Aga et al, 2009)  Segmentation of long claim sentences and re-generation as simpler sentences</p>
    <p>2018 by S. tajner &amp; H. Saggion 94</p>
  </div>
  <div class="page">
    <p>TS Helps Machine Translation</p>
    <p>Focus: English to Serbian MT (tajner and Popovi, 2016)</p>
    <p>Two different automated TS systems:</p>
    <p>2018 by S. tajner &amp; H. Saggion 95</p>
  </div>
  <div class="page">
    <p>Fluency and Adequacy (Example 1)</p>
    <p>Original (A = 2, F = 3):</p>
    <p>As we emerge from a decade of conflict abroad and economic crisis at home, its time to renew America, Obama said, speaking against a backdrop of armored vehicles and a U.S. flag.</p>
    <p>TS-C (A = 4, F = 4):</p>
    <p>Speaking against a backdrop of armored vehicles and a U.S. flag, Obama said its time to renew America as we emerge from a decade of conflict abroad and economic crisis at home.</p>
    <p>2018 by S. tajner &amp; H. Saggion 96</p>
  </div>
  <div class="page">
    <p>Fluency and Adequacy (Example 2)</p>
    <p>Original (A = 2, F = 3): Several Israeli security delegations have visited Egypt during the past two months to decide on a new embassy location.</p>
    <p>TS-C (A = 4, F = 4):TS-C (A = 4, F = 4): Several Israeli security delegations have visited Egypt during the past two months to choose a new embassy location.</p>
    <p>2018 by S. tajner &amp; H. Saggion 97</p>
  </div>
  <div class="page">
    <p>Fluency and Adequacy (Example 3)</p>
    <p>Original (A = 4, F = 3): A Florida mother shot her four children early Tuesday morning before turning the gun on herself at her home in Port St. John, police said.</p>
    <p>TS-A (A = 5, F = 5):TS-A (A = 5, F = 5): A Florida mother shot her four children early Tuesday morning. After that, the Florida mother turned the gun on herself at her home.</p>
    <p>2018 by S. tajner &amp; H. Saggion 98</p>
  </div>
  <div class="page">
    <p>PART 4</p>
    <p>Existing resources for text simplificationExisting resources for text simplification</p>
    <p>2018 by S. tajner &amp; H. Saggion 99</p>
  </div>
  <div class="page">
    <p>Text Simplification Resources</p>
    <p>Lexical Resources for Simplification</p>
    <p>Synonym inventories in several languages: Word Nets / Multilingual Central Repository; various Open Thesaurus (Spanish, English, Catalan, etc.)</p>
    <p>Compiled lists of frequencies: Kucera-Francis (Kucera &amp; Francis, 1967), Age of Acquisition (Kuperman et al., 2012)</p>
    <p>Compiled lists of frequencies: Kucera-Francis (Kucera &amp; Francis, 1967), Age of Acquisition (Kuperman et al., 2012)</p>
    <p>Lists of familiar words (Dale &amp; Chall, 1948)</p>
    <p>Corpora</p>
    <p>Comparable corpus: Wikipedia  Simple Wikipedia; Edit histories</p>
    <p>Parallel corpora: Newsela (Xu et al, 2015), Simplext (Saggion et al. 2015) , PorSimples (Aluisio et al. 2008), FIRST (Stajner et al, 2014)</p>
    <p>2018 by S. tajner &amp; H. Saggion 100</p>
  </div>
  <div class="page">
    <p>Based on the SemEval Lexical Substitution the English Lexical Simplification dataset is created (Specia et al., 2012)</p>
    <p>Based on the lexical substitution dataset (McCarthy and Navigli, 2009)</p>
    <p>Lexical Simplification Resources</p>
    <p>201 words in 10 different contexts</p>
    <p>2018 by S. tajner &amp; H. Saggion 101</p>
  </div>
  <div class="page">
    <p>Also based on the lexical substitution dataset, (De Belder and Moens, 2012) created a similar dataset. Difficulty based on grades provided by informants.</p>
    <p>Lexical Simplification Resources</p>
    <p>2018 by S. tajner &amp; H. Saggion 102</p>
  </div>
  <div class="page">
    <p>Horn et al. (2014) created a 500 sentences, crowd-sourced lexical substitution resource sampled from alignments between English Wikipedia and Simple English Wikipedia</p>
    <p>Lexical Simplification Resources</p>
    <p>2018 by S. tajner &amp; H. Saggion 103</p>
  </div>
  <div class="page">
    <p>Lexical Resources</p>
    <p>CASSA (Baeza-Yates et al., 2015) is a lexical database created automatically from the Spanish Open Thesaurus and the 5gram Google Books Ngram Corpus</p>
    <p>2018 by S. tajner &amp; H. Saggion 104</p>
  </div>
  <div class="page">
    <p>Lexical Resources</p>
    <p>French lexicon annotated with degrees of difficulties (Gala et al, 2013)</p>
    <p>Words extracted from spoken transcriptions of Parkinsons disease affected people  considered a sample of simple text productions</p>
    <p>Words graded based on two existing lexical resources in French: Manulex: lexicon with grades extracted from educational resources, and JeuxDeMots: semantic network with synonymic and other lexical relationslexical relations</p>
    <p>A classifier is trained to associate grades to JdM entries not present in Manulex</p>
    <p>Brooke et al (2012) presents a method to create a readability lexicon with three levels of difficulty from a small (15k) set of seed words.</p>
    <p>A regression model to associate a score and a classification algorithm to discriminate two given words are used to expand the lexicon</p>
    <p>2018 by S. tajner &amp; H. Saggion 105</p>
  </div>
  <div class="page">
    <p>Simple English Wikipedia Dataset</p>
    <p>Called PWPK dataset, it has been compiled by Zhu et al. (2010)</p>
    <p>65K articles from SEW aligned to EW</p>
    <p>Sentences aligned using tf*idf + cosine similarity</p>
    <p>Final dataset contains 108K sentence pairs</p>
    <p>2018 by S. tajner &amp; H. Saggion 106</p>
  </div>
  <div class="page">
    <p>Newsela corpus (English + Spanish)</p>
    <p>Xu et al. (2015) heavily criticizes PWKP since it has</p>
    <p>alignment errors and contains inadequate simplifications</p>
    <p>50% of pairs in PWKP are not simplifications</p>
    <p>Newsela is controlled for quality Newsela is controlled for quality</p>
    <p>1,130 news articles re-written 4 times for children at different grade levels</p>
    <p>Freely available for research purposes upon request at: https://newsela.com/data/</p>
    <p>2018 by S. tajner &amp; H. Saggion 107</p>
  </div>
  <div class="page">
    <p>Newsela corpus (English + Spanish)</p>
    <p>(Newsela, 2016)</p>
    <p>2018 by S. tajner &amp; H. Saggion 108</p>
  </div>
  <div class="page">
    <p>Sentence and paragraph alignment</p>
    <p>English Newsela corpus manually aligned (Xu et al., 2016)</p>
    <p>English Newsela corpus automatically aligned (tajner et al., 2017; Paetzold et al., 2017)</p>
    <p>English and Spanish Newsela corpus automatically aligned  English and Spanish Newsela corpus automatically aligned (tajner et al., 2018)</p>
    <p>2018 by S. tajner &amp; H. Saggion 109</p>
  </div>
  <div class="page">
    <p>Automatic alignment tools</p>
    <p>CATS (tajner et al., 2017; tajner et al., 2018):</p>
    <p>three text similarity measures</p>
    <p>two alignment strategies (preserving order or not)</p>
    <p>http://cats-demo.informatik.uni-mannheim.de/demo.jsp</p>
    <p>Freely available: https://github.com/neosyon/SimpTextAlign Freely available: https://github.com/neosyon/SimpTextAlign</p>
    <p>MASSAlign (Paetzold et al., 2017):</p>
    <p>https://github.com/ghpaetzold/massalign</p>
    <p>2018 by S. tajner &amp; H. Saggion 110</p>
  </div>
  <div class="page">
    <p>PorSimples corpus (Brazilian Portuguese)</p>
    <p>Alusio and Gasperin (2010) Parallel corpus of news articles (Zero Hora) together with human simplifications</p>
    <p>Two simplifications: natural and strong</p>
    <p>Sentences: 2,116 original; 3104 natural simplifications; 3,537 strong simplifications</p>
    <p>2018 by S. tajner &amp; H. Saggion 111</p>
  </div>
  <div class="page">
    <p>Simplext Corpus (Saggion et al. 2015)</p>
    <p>200 short news articles from Spanish news agency</p>
    <p>Each simplified by a text adaptation expert</p>
    <p>Corpus aligned at sentence level (Bott &amp; Saggion, 2011) automatically and manually correctedmanually corrected</p>
    <p>Sentences: 1,149 original; 1,808 simplified</p>
    <p>Most are 1-to-1 alignments with content reduction</p>
    <p>Splits (1-to-2 and 1-to-n) are the second most frequent alignment</p>
    <p>Also deletions and insertions are observed</p>
    <p>2018 by S. tajner &amp; H. Saggion 112</p>
  </div>
  <div class="page">
    <p>PART 5</p>
    <p>Neural ApproachesNeural Approaches</p>
    <p>2018 by S. tajner &amp; H. Saggion 113</p>
  </div>
  <div class="page">
    <p>Lexical Simplification Systems</p>
    <p>Two main types:</p>
    <p>Modular approach (Paetzold and Specia, 2016)</p>
    <p>All-in-one (Horn et al., 2014; Glava and tajner, 2015)</p>
    <p>Modular approach: Modular approach:</p>
    <p>Complex word identification (CWI)</p>
    <p>Substitution candidate generation</p>
    <p>Substitution candidate ranking</p>
    <p>Substitution</p>
    <p>2018 by S. tajner &amp; H. Saggion 114</p>
  </div>
  <div class="page">
    <p>LS Approaches</p>
    <p>Devlin and Tait (1998): uses WordNet (rule-based)</p>
    <p>Yatskar et al. (2010): uses EW meta-data (unsupervised)</p>
    <p>Biran et al. (2011): uses co-occurrence statistics of SEW (unsupervised)</p>
    <p>Horn et al. (2014): uses sentence-aligned EW-SEW (supervised)</p>
    <p>Glava and tajner (2015): uses word embeddings (unsupervised) Glava and tajner (2015): uses word embeddings (unsupervised)</p>
    <p>Paetzold and Specia (2016): uses word embeddings with POS (unsupervised)</p>
    <p>Implementation of many LS systems: http://ghpaetzold.github.io/LEXenstein/</p>
    <p>2018 by S. tajner &amp; H. Saggion 115</p>
  </div>
  <div class="page">
    <p>Light-LS (Glava and tajner, 2015)</p>
    <p>Pros:</p>
    <p>No need for parallel data or simplified data</p>
    <p>Better coverage than other LS systems</p>
    <p>Cons:</p>
    <p>Simplifying only single words (no multi-word expressions)</p>
    <p>Problem with antonyms (due to word embeddings)</p>
    <p>2018 by S. tajner &amp; H. Saggion 116</p>
  </div>
  <div class="page">
    <p>Light-LS: Main Idea</p>
    <p>Simple words are also present in non-simple texts</p>
    <p>We need:</p>
    <p>Good semantic similarity measure (to retrieve substitution candidates)candidates)</p>
    <p>Good measure of word complexity (to rank substitution candidates)</p>
    <p>2018 by S. tajner &amp; H. Saggion 117</p>
  </div>
  <div class="page">
    <p>Light-LS (Glava and tajner, 2015)</p>
    <p>Simplification candidate selection:</p>
    <p>Using only content words</p>
    <p>Using 200-dimensional GloVe vectors pretrained on English Wikipedia and Gigaword 5</p>
    <p>For each content word select 10 most similar content words (cosine  For each content word select 10 most similar content words (cosine similarity) excluding morphological derivations</p>
    <p>Ranking:</p>
    <p>Context similarity (symmetric window of size 3)</p>
    <p>Simplicity (frequency in a large corpora)</p>
    <p>Fluency (language model)</p>
    <p>2018 by S. tajner &amp; H. Saggion 118</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Automatic evaluation on two datasets:</p>
    <p>Replacement task (Horn et al., 2014)</p>
    <p>Ranking task (SemEval-2012 Task 1)</p>
    <p>Human evaluation on a 1  5 Likert scale: Human evaluation on a 1  5 Likert scale:</p>
    <p>Grammaticality</p>
    <p>Meaning preservation</p>
    <p>Simplicity</p>
    <p>2018 by S. tajner &amp; H. Saggion 119</p>
  </div>
  <div class="page">
    <p>Replacement Task Results</p>
    <p>Precision: the percentage of correct simplifications (i.e. the system simplification was found in the list of manual simplifications)</p>
    <p>Accuracy: the percentage of correct simplifications out of all words that should have been simplified</p>
    <p>Changed: the percentage of target words changed by the system Changed: the percentage of target words changed by the system</p>
    <p>Model Precision Accuracy Changed</p>
    <p>Biran et al. (2011) 71.4 3.4 5.2</p>
    <p>Horn et al. (2014) 76.1 66.3 86.3</p>
    <p>LIGHT-LS 71.0 68.2 96.0</p>
    <p>2018 by S. tajner &amp; H. Saggion 120</p>
  </div>
  <div class="page">
    <p>Ranking Task Results</p>
    <p>Task: for each target word (one per sentence) and three given substitution candidates, rank the substitution candidates from simplest to most complex</p>
    <p>Evaluation: the official SemEval-2012 Task 1 script for calculating Cohens kappa</p>
    <p>Model Cohens kappa</p>
    <p>Baseline-random 0.013</p>
    <p>Baseline-frequency 0.471</p>
    <p>Jauhar and Specia (2012) 0.496</p>
    <p>LIGHT-LS 0.540</p>
    <p>2018 by S. tajner &amp; H. Saggion 121</p>
  </div>
  <div class="page">
    <p>Results of Human Evaluation</p>
    <p>Source G Smp MP Ch</p>
    <p>Original 4.90 3.36 -- -</p>
    <p>Manual 4.83 3.95 4.71 76.3%Manual 4.83 3.95 4.71 76.3%</p>
    <p>Biran et al. 4.63 3.24 4.65 17.5%</p>
    <p>LIGHT-LS 4.60 3.76 4.13 68.6%</p>
    <p>Biran et al. Ch. 3.97 2.86 3.57 -</p>
    <p>LIGHT-LS Ch. 4.57 3.55 3.75 -</p>
    <p>2018 by S. tajner &amp; H. Saggion 122</p>
  </div>
  <div class="page">
    <p>Example</p>
    <p>Source Sentence</p>
    <p>Original The contrast between a high level of education and a low level of</p>
    <p>political rights was particularly great in Aarau, and the city refused to</p>
    <p>send troops to defend the Bernese border.</p>
    <p>Biran et al. The separate between a high level of education and a low level of Biran et al. The separate between a high level of education and a low level of</p>
    <p>political rights was particularly great in Aarau, and the city refused to</p>
    <p>send troops to defend the Bernese border.</p>
    <p>LIGHT-LS The contrast between a high level of education and a low level of</p>
    <p>political rights was especially great in Aarau, and the city asked to send</p>
    <p>troops to protect the Bernese border.</p>
    <p>2018 by S. tajner &amp; H. Saggion 123</p>
  </div>
  <div class="page">
    <p>LS-NNS (Paetzold and Specia, 2016)</p>
    <p>Similar idea of using word embeddings for unsupervised LS</p>
    <p>Difference: context-aware word embeddings (POS tags instead of sense labels)</p>
    <p>Difference: modular approach</p>
    <p>Difference: used a corpus of subtitles Difference: used a corpus of subtitles</p>
    <p>(Paetzold and Specia, 2016)</p>
    <p>Model Precision Accuracy Changed</p>
    <p>Biran 0.121 0.121 1.000</p>
    <p>Kauchak 0.364 0.172 0.808</p>
    <p>Glavas 0.456 0.197 0.741</p>
    <p>LS-NNS 0.464 0.226 0.762</p>
    <p>2018 by S. tajner &amp; H. Saggion 124</p>
  </div>
  <div class="page">
    <p>Exploring Neural TS Models (Nisioi et al., 2017)</p>
    <p>First attempt at using sequence to sequence neural networks to model text simplification</p>
    <p>The model simultaneously performs lexical simplification and content reduction</p>
    <p>Almost perfect grammaticality and meaning preservation</p>
    <p>Higher level of simplification than state-of-the-art ATS systems</p>
    <p>2018 by S. tajner &amp; H. Saggion 125</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>EW-SEW (Hwang et al., 2015): 150,000 full matches and 130,000 partial matches</p>
    <p>Manually created multi-reference development and test set (Xu et al., 2016): 2,000 + 359 (each with eight references)</p>
    <p>High number of named entities</p>
    <p>High lexical richness Original Simplified</p>
    <p>Locations 158,394 127,349</p>
    <p>Persons 161,808 127,742</p>
    <p>Organisations 130,679 101,239</p>
    <p>Misc 95,168 71,138</p>
    <p>Vocabulary 187,137 144,132</p>
    <p>Tokens 7,400,499 5,634,834</p>
    <p>2018 by S. tajner &amp; H. Saggion 126</p>
  </div>
  <div class="page">
    <p>NTS System</p>
    <p>OpenNMT framework</p>
    <p>Two LSTM layers</p>
    <p>Hidden states of size 500</p>
    <p>500 hidden units</p>
    <p>A 0.3 dropout probability A 0.3 dropout probability</p>
    <p>Vocabulary: 50,000</p>
    <p>(Nisioi et al., 2017)</p>
    <p>2018 by S. tajner &amp; H. Saggion 127</p>
  </div>
  <div class="page">
    <p>Training</p>
    <p>Training the model for 15 epochs with plain SGD optimiser</p>
    <p>After epoch 8, halve the learning rate</p>
    <p>At the end of every epoch save the current state of the model and predict perplexity values of the models on the dev set</p>
    <p>Early-stopping and selecting the model with best perplexity Early-stopping and selecting the model with best perplexity</p>
    <p>Parameters initialised over uniform distribution with support [-0.1, 0.1]</p>
    <p>Global attention in combination with input feeding for the decoder</p>
    <p>The architecture configuration, data, and pretrained models available at: https://github.com/senisioi/NeuralTextSimplification</p>
    <p>2018 by S. tajner &amp; H. Saggion 128</p>
  </div>
  <div class="page">
    <p>Whats New Here?</p>
    <p>Word embeddings</p>
    <p>Kauchak (2013) showed that adding original language to the simple language in LMs improves ATS</p>
    <p>Encoder: original English + Google News (word2vec) Encoder: original English + Google News (word2vec)</p>
    <p>Decoder: simplified English + Google News (word2vec)</p>
    <p>2018 by S. tajner &amp; H. Saggion 129</p>
  </div>
  <div class="page">
    <p>What About the OoV Words?</p>
    <p>Vocabulary size: 50,000</p>
    <p>Those not present in the vocabulary are replaced with UNK symbols during training</p>
    <p>At the prediction time, we replace unknown words with the highest probability score from the attention layer</p>
    <p>2018 by S. tajner &amp; H. Saggion 130</p>
  </div>
  <div class="page">
    <p>How to Find the Best Hypothesis?</p>
    <p>We generate first two candidate hypotheses from each beam size from 5 to 12</p>
    <p>Try to find the best beam size and hypothesis based on:</p>
    <p>BLEU with NIST smoothing (Bird et al., 2009) BLEU with NIST smoothing (Bird et al., 2009)</p>
    <p>SARI (Xu et al., 2016)</p>
    <p>Development dataset (2,000 sentence pairs) is used for finding the best beam size and hypothesis according to BLEU and SARI</p>
    <p>2018 by S. tajner &amp; H. Saggion 131</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>First 70 sentences from the test set (Xu et al., 2016)</p>
    <p>Automatic evaluation (BLEU and SARI)</p>
    <p>Human evaluation: Human evaluation:</p>
    <p>Number of changes</p>
    <p>Correctness of changes</p>
    <p>Grammaticality</p>
    <p>Meaning preservation</p>
    <p>Relative simplicity</p>
    <p>2018 by S. tajner &amp; H. Saggion 132</p>
  </div>
  <div class="page">
    <p>Comparison with the State of the Art</p>
    <p>SBMT system (Xu et al., 2016)</p>
    <p>Unsupervised s.o.t.a. LS system LightLS (Glava and tajner, 2015)</p>
    <p>PBSMT system with output reranking (Wubben et al., 2012) PBSMT system with output reranking (Wubben et al., 2012)</p>
    <p>We use original systems in all three cases</p>
    <p>2018 by S. tajner &amp; H. Saggion 133</p>
  </div>
  <div class="page">
    <p>Correctness and Number of Changes</p>
    <p>The whole phrase counted as one change:</p>
    <p>e.g. become defunct  was dissolved</p>
    <p>If grammatically correct and preserves the meaning (2 native speakers) and if it makes the sentence easier to understand (2 non-speakers) and if it makes the sentence easier to understand (2 nonnave speakers)  correct</p>
    <p>Where not agreed, we asked a third annotator and used the majority vote</p>
    <p>2018 by S. tajner &amp; H. Saggion 134</p>
  </div>
  <div class="page">
    <p>Grammaticality and Meaning Preservation</p>
    <p>1  5 Likert scale (1  very bad, 5  very good)</p>
    <p>3 native English speakers</p>
    <p>Quadratic Cohens kappa: Quadratic Cohens kappa:</p>
    <p>0.78 for Grammaticality</p>
    <p>0.63 for Meaning preservation</p>
    <p>2018 by S. tajner &amp; H. Saggion 135</p>
  </div>
  <div class="page">
    <p>Simplicity</p>
    <p>Evaluated by 3 non-native but fluent English speakers</p>
    <p>Relative simplicity (evaluating sentence pairs Original  Simplified):</p>
    <p>+2 much simpler+2 much simpler</p>
    <p>+1 somewhat simpler</p>
    <p>-1 somewhat more difficult</p>
    <p>-2 much more difficult</p>
    <p>Quadratic Cohens kappa: 0.66</p>
    <p>2018 by S. tajner &amp; H. Saggion 136</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>Approac</p>
    <p>h</p>
    <p>Training LM Changes Scores Rank</p>
    <p>Dataset Size (sent) Corpus Size (sent)</p>
    <p>Total Correc t</p>
    <p>G M S</p>
    <p>NTS Default (beam 5, hypothesis 1) 36 72.2% 4.92 4.31 0.46</p>
    <p>NTS Best SARI (beam 5, hypothesis 2) 72 51.6% 4.19 3.62 0.38</p>
    <p>NTS Best BLEU (beam 12, hypothesis 1) 44 73.7% 4.77 4.15 0.92</p>
    <p>NTS-w2v Default (beam 5, hypothesis 1) 31 54.8% 4.79 4.17 0.21</p>
    <p>(Nisioi et al., 2017)</p>
    <p>NTS-w2v Best SARI (beam 12, hypothesis 2) 110 68.1% 4.53 3.83 0.63</p>
    <p>NTS-w2v Best BLEU (beam 12, hypothesis 1) 61 76.9% 4.67 4.00 0.40</p>
    <p>PBSMT Wiki</p>
    <p>(Good+Partial) 284,499 Wiki 391,572 76 35.5% 4.09 3.31 0.26</p>
    <p>PBSMT Newsela</p>
    <p>(neighb.)+Wiki 593,947</p>
    <p>Newsel a+Wiki</p>
    <p>PBSMT Newsela (all) +</p>
    <p>Wiki 764,571</p>
    <p>Newsel a+Wiki</p>
    <p>s.o.t.a.</p>
    <p>PBSMT-R (Wubben et al., 2012) 171 41.0% 3.10 2.71 -0.55</p>
    <p>Supervised SBMT (PPDB+SARI)(Xu et al., 2016) 143 34.3% 4.28 3.57 0.03</p>
    <p>Unsupervised (LightLS) (Glava and tajner, 2015) 132 26.6% 4.47 2.67 -0.01</p>
    <p>2018 by S. tajner &amp; H. Saggion 137</p>
  </div>
  <div class="page">
    <p>NTS vs. State-of-the-Art ATS</p>
    <p>NTS models have higher percentage of correct changes</p>
    <p>NTS models have more simplified output than any other ATS system</p>
    <p>NTS with custom word2vec embeddings, ranked with SARI: NTS with custom word2vec embeddings, ranked with SARI:</p>
    <p>the highest total number of changes among NTS models</p>
    <p>one of the highest number of correct changes</p>
    <p>the second highest simplicity score</p>
    <p>solid grammaticality and meaning preservation scores</p>
    <p>2018 by S. tajner &amp; H. Saggion 138</p>
  </div>
  <div class="page">
    <p>Customised NTS Models</p>
    <p>Ranking predictions with SARI  the highest number of changes</p>
    <p>Ranking predictions with BLEU  the highest number of correct changes</p>
    <p>Customised word embeddings in combination with SARI seem to work best among all our NTS systems</p>
    <p>2018 by S. tajner &amp; H. Saggion 139</p>
  </div>
  <div class="page">
    <p>(Nisioi et al., 2017)</p>
    <p>2018 by S. tajner &amp; H. Saggion 140</p>
  </div>
  <div class="page">
    <p>Reinforcement Learning for TS (Zhang and Lapata, 2017)</p>
    <p>Encoder-decoder model embedded in reinforcement learning framework (DRESS)</p>
    <p>Motivation: generic encoder-decoder models favorise copying over rewriting</p>
    <p>DRESS rewards: simplicity, relevance, and fluency DRESS rewards: simplicity, relevance, and fluency</p>
    <p>Trained and evaluated on three datasets: WikiSmall (Zhu et al., 2010), WikiLarge (Kauchak, 2013), and Newsela (Xu et al., 2015)</p>
    <p>Output compared with: PBMT-R (Wubben et al, 2010), Hybrid (Narayan and Gardent, 2014), SBMT-SARI (Xu et al., 2016),</p>
    <p>2018 by S. tajner &amp; H. Saggion 141</p>
  </div>
  <div class="page">
    <p>Output Examples</p>
    <p>(Zhang and Lapata, 2017)</p>
    <p>2018 by S. tajner &amp; H. Saggion 142</p>
  </div>
  <div class="page">
    <p>Comparison of NTS Systems (tajner and Nisioi, 2018)</p>
    <p>Systems evaluated for:</p>
    <p>The percentage of sentences which undergone at least one change;</p>
    <p>The total number of changes;</p>
    <p>The percentage of correct changes;</p>
    <p>Grammaticality of the simplified sentence;</p>
    <p>Meaning preservation of the simplified sentence;</p>
    <p>Relative simplicity of the simplified sentence in comparison to the original sentence.</p>
    <p>2018 by S. tajner &amp; H. Saggion 143</p>
  </div>
  <div class="page">
    <p>Evaluation results (tajner and Nisioi, 2018)</p>
    <p>2018 by S. tajner &amp; H. Saggion 144</p>
  </div>
  <div class="page">
    <p>Outputs</p>
    <p>(tajner and Nisioi, 2018)  2018 by S. tajner &amp; H. Saggion 145</p>
  </div>
  <div class="page">
    <p>Comparison of Fully-fledged Systems: Example 1</p>
    <p>Original, Angrosh et al. (2014), Woodsend and Lapata (2011):</p>
    <p>They drove a patrol car onto the lawn in an attempt to rescue her.</p>
    <p>EvLex, LexEv:</p>
    <p>They drove a police car onto the lawn. content reduction</p>
    <p>EventSimplify + Light LS = EvLex</p>
    <p>LightLS + EventSimplify = LexEv</p>
    <p>Angrosh et al. (2014) is a hybrid system</p>
    <p>Woodsend and Lapata (2011a) is a supervised system based on EW-SEW</p>
    <p>2018 by S. tajner &amp; H. Saggion 146</p>
  </div>
  <div class="page">
    <p>Comparison of Fully-fledged Systems: Example 2</p>
    <p>Original, Woodsend and Lapata (2011):</p>
    <p>Jonson was rushed to hospital but died from her wounds, Goodyear said.</p>
    <p>Angrosh et al. (2014):</p>
    <p>Goodyear said Jonson was rushed to hospital but died from her wounds.Goodyear said Jonson was rushed to hospital but died from her wounds.</p>
    <p>EvLex, LexEv:</p>
    <p>Jonson was rushed to hospital. Jonson died from her injuries.</p>
    <p>syntactic (reordering)</p>
    <p>syntactic (sentence splitting)</p>
    <p>2018 by S. tajner &amp; H. Saggion 147</p>
  </div>
  <div class="page">
    <p>Comparison of Fully-fledged Systems: Example 3</p>
    <p>Original:</p>
    <p>The ambassadors arrival has not been announced and he flew in complete secrecy. the official said.</p>
    <p>Woodsend and Lapata (2011):</p>
    <p>The ambassadors arrival has not been announced., the official said. He The ambassadors arrival has not been announced., the official said. He flew in complete secrecy.</p>
    <p>Angrosh et al. (2014):</p>
    <p>The official said The ambassadors arrival has not been announced. And he flew in complete secrecy.</p>
    <p>EvLex, LexEv:</p>
    <p>He arrived in complete secrecy.</p>
    <p>syntactic (reordering)</p>
    <p>content reduction</p>
    <p>2018 by S. tajner &amp; H. Saggion 148</p>
  </div>
  <div class="page">
    <p>Summary of the tutorial</p>
    <p>Text simplification is a complex task which requires considerable linguistic and world knowledge</p>
    <p>Automatic text simplification, although still imperfect, has the potential to serve a variety of users with special needs</p>
    <p>Text simplification has been addressed with a variety of techniques including rule-based methods, unsupervised approaches, and including rule-based methods, unsupervised approaches, and current/innovative data-driven techniques</p>
    <p>The techniques will depend on several factors such as availability of resources or what you are aiming for (e.g. just try a new approach or create a system for an end user)</p>
    <p>2018 by S. tajner &amp; H. Saggion 149</p>
  </div>
  <div class="page">
    <p>Summary of the tutorial</p>
    <p>For the time being, and except for few works, text simplification is being approached at word and sentence, neglecting discourse issues such as cohesion and coherence</p>
    <p>There is much to be done to take text simplification research to the next level</p>
    <p>2018 by S. tajner &amp; H. Saggion 150</p>
  </div>
  <div class="page">
    <p>Data-Driven Text Simplification</p>
    <p>Sanja tajner and Horacio Saggion</p>
    <p>Many thanks for attending the tutorial !!!!Many thanks for attending the tutorial !!!!</p>
    <p>COLING 2018 Tutorial</p>
    <p>#TextSimplification2018</p>
  </div>
</Presentation>
