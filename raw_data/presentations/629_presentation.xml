<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Towards a Robust Query Optimizer: A Principled and Practical Approach</p>
    <p>ACM Sigmod Conference, 2005 Babcock B. and Chaudhuri S.</p>
    <p>Presented by: Esma Kl</p>
  </div>
  <div class="page">
    <p>Outline  Introduction</p>
    <p>Query Optimizer  An Alternative Approach to Query Optimization</p>
    <p>Cardinality Estimation  Related Work  The New Approach</p>
    <p>The Performance/Predictability Tradeoff  Incorporating the Probability Distribution</p>
    <p>Analysis of the Approach  Experiments  Future Work  Conclusion</p>
  </div>
  <div class="page">
    <p>Introduction Query Optimizer</p>
    <p>The task: To select a low-cost query plan.  The difficulty: Only incomplete and imprecise</p>
    <p>information about query plan cost is available to the optimizer at query compilation time.</p>
    <p>The standard approach:  First, generate rough guesses as the values of the</p>
    <p>relevant cost model parameters using some statistics of the data.</p>
    <p>Next, using rough guesses as inputs, invoke a search algorithm to find the least costly plan.</p>
  </div>
  <div class="page">
    <p>Introduction An Alternative Approach to Query Optimization</p>
    <p>The problem: The standard approach ignores the uncertainty about the values of important cost parameters while calculating unique values.</p>
    <p>The solution: The new approach uses probabilistic reasoning to acknowledge uncertainties in the query planning process in a principled manner.</p>
    <p>Therefore, it is capable of producing query plans that are more robust to estimation errors and changes in the running environment.</p>
    <p>It focuses on the cardinality estimation phase of optimization.</p>
  </div>
  <div class="page">
    <p>Cardinality Estimation, a central subproblem in query optimization</p>
    <p>The time that a particular query plan takes to execute is crucially dependent on the sizes of the relations accessed in the query for that plan.  However, they cannot generally be computed</p>
    <p>exactly without first executing the query plan.  Therefore, query optimizer needs to estimate</p>
    <p>them to produce cost estimation of the plan.  The problem of producing accurate size estimates for</p>
    <p>intermediate results is known as the cardinality estimation problem.</p>
  </div>
  <div class="page">
    <p>Cardinality Estimation Estimating The Selectivities</p>
    <p>Typically, the sizes of base relations are known; the challenging part of cardinality estimation involves  estimating the selectivities of the various selection</p>
    <p>conditions and  estimating join predicates in a query.</p>
    <p>From the optimizers point of view, the best case for the selectivity estimation would be  to produce a probability distribution over possible</p>
    <p>selectivities  instead of a point estimate of selectivity.</p>
  </div>
  <div class="page">
    <p>Related Work  Various estimation techniques have been proposed</p>
    <p>(histograms, Fourier transformations, sampling, ..).</p>
    <p>But they use AVI assumption to simplify calculations.</p>
    <p>Under the attribute value independence (AVI) assumption, predicates on different attributes are assumed to be independent of each other.</p>
    <p>e.g. P(A=a and B=b) = P(A=a) . P(B=b)</p>
    <p>In fact, AVI assumption is violated frequently in real practice and causes serious query optimization errors.</p>
  </div>
  <div class="page">
    <p>Related Work  New techniques for modeling correlated multidimensional</p>
    <p>distributions, such as multidimensional histograms, have been proposed.  But, none of them have yet been adopted in</p>
    <p>commercial DBMS and  they still provide a single-point estimate of cardinality</p>
    <p>without providing any information about the uncertainty of the estimate.</p>
    <p>The degree of uncertainty can be quite important in selecting the most appropriate query plan.</p>
  </div>
  <div class="page">
    <p>The New Approach  The proposed method: A cardinality estimation procedure</p>
    <p>that is based on Bayesian inference from pre-computed random samples.</p>
    <p>The advantages:  By using Bayes rule, the problematic AVI assumption</p>
    <p>of previous studies is avoided.  An appropriate trade-off between predictability and</p>
    <p>performance is expressed based on user or application preferences.</p>
    <p>The procedure is compatible with the architecture of existing query optimizers.</p>
  </div>
  <div class="page">
    <p>The Performance/Predictability Tradeoff</p>
    <p>An example query optimizer problem :  A single query Q running against a table with N rows  Choose an access method to retrieve records from relation</p>
    <p>R that satisfy the predicate (A=a) and (B=b), where A and B are two indexed attributes of R.</p>
    <p>Two different plans, P1 and P2, exist:  Index intersection plan  Sequential scan plan</p>
    <p>The two plans have different degrees of dependency on the (unknown) query selectivity.</p>
  </div>
  <div class="page">
    <p>The Performance/Predictability Tradeoff</p>
    <p>Costs of the plans:  Index intersection plan identifies the qualifying records</p>
    <p>based on the indexes and then retrieves just those records. If the number of records to be retrieved is low, it performs well.</p>
    <p>However, since the index intersection plan requires one random disk read per record, it overcomes poorly when the selectivity is high.</p>
    <p>Sequential scan plans cost is not directly depends the query selectivity.</p>
  </div>
  <div class="page">
    <p>The Performance/Predictability Tradeoff</p>
    <p>Which plan is preferable?  Plan-1, if query selectivity is less than 26%.  Plan-2, if the query selectivity is greater that 26%.</p>
  </div>
  <div class="page">
    <p>The Performance/Predictability Tradeoff</p>
    <p>As selectivity varies, execution cost of Plan-2 does not vary so much, there is no risk for it to have high cost.</p>
    <p>However, cost of Plan-1 varies so much as selectivity varies. It may have very small or very large cost value, depending on the selectivity.</p>
    <p>When the probability distribution for selectivity is collapsed to its expected value, as done in previous studies, the information is lost. Therefore, it should not be claimed that the query plan that has least expected cost is selected.</p>
  </div>
  <div class="page">
    <p>Reasoning About Uncertainity</p>
    <p>How can knowledge of the probability distribution for selectivity be used to improve the query optimizer?</p>
    <p>How can such a probability distribution be estimated?</p>
  </div>
  <div class="page">
    <p>Incorporating the Probability Distribution</p>
    <p>Which plan is preferable?  Which part of the probability distribution is more important?</p>
    <p>The middle part, i.e. the typical behavior  The right-hand tail, i.e. the realistic worst case behavior</p>
  </div>
  <div class="page">
    <p>Incorporating the Probability Distribution</p>
    <p>Which plan is preferable?  Plan-1, if minimizing expected cost is the overriding concern.  Plan-2, if users are more risk-averse,</p>
    <p>since the slightly higher expected cost of Plan-2 has greater predictability, the risk that the query will take much longer than expected to execute is reduced.</p>
  </div>
  <div class="page">
    <p>Incorporating the Probability Distribution Confidence Threshold</p>
    <p>The desired tradeoff between performance and predictability is expressed by means of an user/application-specified parameter called as confidence threshold, T%.</p>
    <p>T% can be alternatively (and equivalently) described in terms of the cumulative distribution function (cdf) for query execution cost.</p>
    <p>T% = cdf(c) c = cdf-1(T%)</p>
  </div>
  <div class="page">
    <p>Incorporating the Probability Distribution Derivation of Execution Cost Distribution</p>
    <p>pdf(c) can be derived from  pdf(s) where pdf(s)= (# of occurence of s ) / (# of</p>
    <p>tuples)  cost function c=g(s) that is available implicit form</p>
    <p>through the cost estimation module  But deriving cost for various selectivity could be</p>
    <p>an expensive task.  Under the assumption that query execution cost is</p>
    <p>monotonically increasing function of selectivity, it is enough to invert the cdf for selectivity. s=cdf-1(T%)</p>
  </div>
  <div class="page">
    <p>Selectivity Estimation via Sampling</p>
    <p>In this study, the selectivity estimation is performed using uniform random samples of the relations in the database.  It improves robustness of the query process.</p>
    <p>During query optimization, the cardinality estimation module is invoked for each relational subexpression.</p>
    <p>For each such expression, how many tuples satisfy the expression is counted.</p>
    <p>The number of satisfying tuples and the overall number of tuples in the sample are used in pdf calculation.</p>
  </div>
  <div class="page">
    <p>Selectivity Estimation via Sampling</p>
    <p>Random sampling has some advantages over most selectivity estimation techniques:  It does not use the AVI assumption, so</p>
    <p>estimation errors are lessen  The dimensionality of the data does not affect</p>
    <p>the accuracy of random sampling  It is simple to implement</p>
  </div>
  <div class="page">
    <p>Deriving the Probability Distribution</p>
    <p>Suppose database consists of N tuples</p>
    <p>The predicate is satisfied by pN tuples</p>
    <p>(fraction of p of the database satisfies the predicate and 1-p do not)</p>
    <p>Probability of selecting k tuples out of N satisfying the predicate</p>
    <p>P(X|p): Bernoulli distribution = pk.(1-p)(N-k)</p>
  </div>
  <div class="page">
    <p>Deriving the Probability Distribution</p>
    <p>The conditional density: (Bayes Rule)</p>
    <p>pdf(p=z|X) = P(X|p=z).pdf(p=z) / P(X)</p>
    <p>posterior = likelihood*prior / evidence</p>
    <p>The unknown quantity p is treated as a random variable</p>
    <p>Aim is to find the conditional probability distribution for p=z, given the observed data X</p>
  </div>
  <div class="page">
    <p>Deriving the Probability Distribution</p>
    <p>Bayes Rule: pdf(p=z|X) = P(X|p=z).pdf(p=z) / P(X)</p>
    <p>P(X): Does no depend on z, common for all p=z  P(X|p=z) = zk.(1-z)(N-k)</p>
    <p>pdf(p=z): Prior knowledge about query workload is lacking  is assumed to be uniform distribution f(z)=1 for</p>
    <p>OR  is assumed to be beta distribution (most widely accepted non</p>
    <p>informative prior) f(z)  z -1/2 .(1-z) -1/2</p>
  </div>
  <div class="page">
    <p>Summary of the Estimation Procedure</p>
    <p>The optimizer searches the optimal plan through many possible query plans.</p>
    <p>During this search, the optimizer makes a number of subroutine calls to the cardinality estimation module to estimate the size of various intermediate query results.</p>
    <p>In this study, the cardinality estimation module is modified to estimate the selectivity of each query predicate:</p>
  </div>
  <div class="page">
    <p>Summary of the Estimation Procedure</p>
    <p>s = cdf-1(T%).</p>
  </div>
  <div class="page">
    <p>Analysis An example</p>
    <p>Assume a simple linear cost model: the execution time for query plan Pi = vi*pN + fi  pN: the number of tuples satisfying the query</p>
    <p>predicate,  p: the query selectivity  N: total number of rows</p>
    <p>vi is the incremental cost per tuple for plan Pi  fi is the fixed overhead, independent of query</p>
    <p>selectivity, for plan Pi</p>
  </div>
  <div class="page">
    <p>Analysis An example</p>
    <p>Plan-1 and Plan-2 roughly resemble an index intersection plan and a sequential scan plan.</p>
    <p>The parameter values that are chosen empirically:  N = 6.000.000,  f1 = 5, v1 = 3.510</p>
    <p>-3</p>
    <p>f2 = 35, v2 = 3.510 -6</p>
    <p>The crossover point where plan Plan-2 becomes better than plan Plan-1 is at a selectivity of;</p>
    <p>pc = (f1-f2) / (v2N-v1N) = 0.14%</p>
  </div>
  <div class="page">
    <p>Analysis An example</p>
    <p>If the query optimizer knew the actual query selectivity exactly, it would always choose</p>
    <p>plan Plan-2 whenever the selectivity p &lt; the crossover point pc and</p>
    <p>plan Plan-1 when p &gt; pc 0.14%.</p>
    <p>Instead, the estimated selectivity s=cdf-1(T%) will be compared with pc</p>
  </div>
  <div class="page">
    <p>Analytical Results Higher T%  the query optimizer more prone to overestimation. (The optimizer choses P2 when P1 would be better)</p>
    <p>Lower T% the query optimizer more prone to underestimation. (The optimizer chooses P1 when P2 would be better)</p>
  </div>
  <div class="page">
    <p>Analytical Results</p>
    <p>The higher T%  the less variability occurs in the query optimization time</p>
    <p>The moderate settings T% are better than extremely high or low settings at producing low expected execution times.</p>
  </div>
  <div class="page">
    <p>Experiments  The query optimizer of a commercial DBMS, Microsoft</p>
    <p>SQL Server, is modified by replacing the existing histogram-based cardinality estimation module with the proposed module.</p>
    <p>Larger selectivities are easier to estimate accurately, so estimation error is less when the selectivity is higher. Therefore, low-selectivity queries are concentrated.</p>
    <p>The TPC-H benchmark dataset (~1 GB) is used.</p>
  </div>
  <div class="page">
    <p>Experiments Single-Table Query Scenario</p>
    <p>The template used for this scenario is SELECT SUM(extendedprice) FROM lineitem</p>
    <p>WHERE shipdate BETWEEN 07/01/97 AND 09/30/97</p>
    <p>AND receiptdate BETWEEN (07/01/97+?) AND (09/30/97+?)</p>
    <p>The degree of overlap between the set of rows satisfying the condition on shipdate and receiptdate  is controlled by the value of ?  varied so that the overall query selectivity was between</p>
  </div>
  <div class="page">
    <p>Experiments Single-Table Query Scenario</p>
    <p>The commercial DBMSs standard estimation module is worse!.. Because it always selects the index intersection plan, which performed poorly at higher selectivities.</p>
  </div>
  <div class="page">
    <p>Experiments Single-Table Query Scenario</p>
    <p>The variance in execution cost decreases steadily as T % increases.</p>
    <p>Commercial DBMSs standard estimation is significantly worse!.. Because it always uses AVI assumption even with correlated data instead of Bayes Rule.</p>
  </div>
  <div class="page">
    <p>Experiments Three-Table Join Scenario</p>
    <p>In this query scenario, the query template consists of a natural join between  the lineitem table,  the orders table, and  the part table.</p>
    <p>There is an additional selection condition on the part table which is used as the free parameter that is varied in this scenario.</p>
    <p>All relations clustered on their primary keys, with additional indexes on the foreign key columns.</p>
  </div>
  <div class="page">
    <p>Experiments Three-Table Join Scenario  In this query scenario, the optimal query plan has one of three</p>
    <p>different structures, depending on the number of rows selected from the part table that satisfy the predicate:</p>
    <p>At low selectivities, the best plan is to first join part to lineitem using indexed nested loops join, and then hash join with orders.</p>
    <p>When the selectivity gets a little higher, the best plan is a sequence of hash joins, first between lineitem and part and then with orders.</p>
    <p>When the query selectivity goes higher than approximately 10%, the optimal plan is to first merge join the two larger relations, lineitem and orders, and then hash join with part.</p>
  </div>
  <div class="page">
    <p>Experiments Three-Table Join Scenario</p>
    <p>This shows that the properties of the proposed method is applicable for a broader class of queries.</p>
    <p>Although the types of queries involved in two scenarios are quite different, each of subfigures is similar to its counterpart from the first scenario.</p>
  </div>
  <div class="page">
    <p>Experimental Conclusions</p>
    <p>A confidence threshold of 80% achieves both good performance (low average execution time) and good predictability (little variability in execution time).</p>
    <p>A confidence threshold of 95% leads to very stable query plans so it is good for situations where predictability is the major concern.</p>
    <p>Confidence thresholds below 50% are rather speculative and are likely to be of limited applicability.</p>
  </div>
  <div class="page">
    <p>Future Work  As join expressions, only foreign-key joins are used.</p>
    <p>Extending the techniques to work with the full generality of SQL is a direction for future work.</p>
    <p>The time spent in query optimization was about 30%40% more than time using standard histograms.  Because the implementation lacks even basic</p>
    <p>optimization such as memorizing.  Validation of experimental conclusions will be</p>
    <p>checked through additional experimentation.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>By one definition, a robust query optimizer is one that generates plans that work reasonably well even when optimizer assumptions fail to hold.</p>
    <p>In this study, a novel cardinality estimation procedure is developed that manages uncertainty in a principled way by reasoning probabilistically about selectivity.</p>
    <p>How can we increase the robustness of query optimizers?</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Users should be allowed to prioritize performance criterions.</p>
    <p>In the estimation technique in this study, the query planning process is incorporated with user or application preferences about the predictability vs. performance tradeoff, explicitly expressed through the setting of a single parameter.</p>
  </div>
  <div class="page">
    <p>Thanks for listeningThanks for listening</p>
    <p>QuestionsQuestions</p>
  </div>
</Presentation>
