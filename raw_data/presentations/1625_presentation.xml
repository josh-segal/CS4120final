<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 1</p>
    <p>Institut fr Signal- und Informationsverarbeitung</p>
    <p>Signal and Information Processing Laboratory</p>
    <p>Some Remarks on Factor Graphs</p>
    <p>Hans-Andrea Loeliger</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 2</p>
    <p>Why Factor Graphs?</p>
    <p>Factor graphs are a language for system models.</p>
    <p>Well known: A large variety of algorithms in coding, signal processing, and artificial intelligence can be explained as special cases of summary-product message passing on a factor graph: BCJR forward-backward algorithm, Viterbi algorithm, iterative decoding of turbo codes and low-density parity check codes, belief propagation in Bayesian networks, Kalman filtering and smoothing, . . . .</p>
    <p>More important: Factor graphs are highly useful for the development of new algorithms for a wide variety of real-world detection/estimation problems. In particular, they allow to combine and to extend most prior art, including gradient methods, EMtype algorithms, particle filters, . . . .</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 3</p>
    <p>Outline</p>
    <p>Forney-style factor graphs  Some simple remarks  Dealing with continuous variables  On gradient methods</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 4</p>
    <p>Forney-Style Factor Graphs (FFG)</p>
    <p>represent the factorization of a function of several variables.</p>
    <p>(D. Forney: Codes on graphs: normal realizations, 2001)</p>
    <p>Example:</p>
    <p>f (x1, x2, x3, x4, x5) = fA(x1, x2, x3)  fB(x3, x4, x5)  fC(x4).</p>
    <p>x1</p>
    <p>fA</p>
    <p>x2</p>
    <p>x3</p>
    <p>fB</p>
    <p>fC</p>
    <p>x4 x5</p>
    <p>Rules:</p>
    <p>A node for every factor.  An edge or half-edge for every variable.  Node g is connected to edge x iff variable x appears in factor g.</p>
    <p>What if some variable appears in more than 2 factors?</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 5</p>
    <p>Example:</p>
    <p>Markov Chain</p>
    <p>pXY Z(x, y, z) = pX(x)  pY |X(y|x)  pZ|Y (z|y).</p>
    <p>pX</p>
    <p>X</p>
    <p>pY |X</p>
    <p>Y</p>
    <p>pZ|Y</p>
    <p>Z</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 6</p>
    <p>Example:</p>
    <p>Low-Density Parity Check Code</p>
    <p>Code</p>
    <p>B B B</p>
    <p>J J J</p>
    <p>B B B</p>
    <p>J J J</p>
    <p>B B B</p>
    <p>J J J</p>
    <p>random connections</p>
    <p>= J</p>
    <p>J J</p>
    <p>= . . . J</p>
    <p>J J</p>
    <p>= J</p>
    <p>J J</p>
    <p>X1 X2 . . . Xn</p>
    <p>Channel Model</p>
    <p>p(y1|x1)</p>
    <p>Y1</p>
    <p>p(y2|x2) . . .</p>
    <p>Y2 . . .</p>
    <p>p(yn|xn)</p>
    <p>Yn</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 7</p>
    <p>Example:</p>
    <p>Classical Linear State Space Models</p>
    <p>Xk = AXk1 + BUk</p>
    <p>Zk = CXk</p>
    <p>Yk = Zk + Wk</p>
    <p>Wk = white Gaussian noise.</p>
    <p>. . .</p>
    <p>Xk1 ? Uk</p>
    <p>? Zk</p>
    <p>? Yk</p>
    <p>Xk ?</p>
    <p>Uk+1</p>
    <p>? Zk+1</p>
    <p>? Yk+1</p>
    <p>Xk+1</p>
    <p>. . .</p>
    <p>Xk1</p>
    <p>A</p>
    <p>?</p>
    <p>Uk</p>
    <p>B</p>
    <p>?</p>
    <p>+ - =</p>
    <p>?</p>
    <p>C</p>
    <p>?Zk</p>
    <p>Xk</p>
    <p>Wk ?</p>
    <p>Zk</p>
    <p>+</p>
    <p>?Yk</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 8</p>
    <p>Example:</p>
    <p>Channel with Unknown Parameters</p>
    <p>Code</p>
    <p>X1 X2 Xn</p>
    <p>Channel Model</p>
    <p>Y1 Y2 Yn 1 n</p>
    <p>Model for Channel Parameters</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 9</p>
    <p>Superimposed Signals</p>
    <p>. . .</p>
    <p>Xk</p>
    <p>Xk+1</p>
    <p>?</p>
    <p>Yk</p>
    <p>+</p>
    <p>?</p>
    <p>Zk</p>
    <p>?</p>
    <p>Yk+1</p>
    <p>+</p>
    <p>?</p>
    <p>Zk+1</p>
    <p>. . .</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 10</p>
    <p>Summary-Product Message Update Rule</p>
    <p>Each message is a function (usually a scaled pdf) of the variable associated with the corresponding edge. This function is a summary of everything behind this edge.</p>
    <p>H H</p>
    <p>H H</p>
    <p>H HH</p>
    <p>X1</p>
    <p>.</p>
    <p>.</p>
    <p>.</p>
    <p>Xn</p>
    <p>f Y</p>
    <p>H HHj</p>
    <p>1</p>
    <p>*</p>
    <p>n</p>
    <p>out</p>
    <p>out(y) 4 =  x1</p>
    <p>. . .  xn</p>
    <p>f (x1, . . . , xn, y) 1(x1)   n(xn)</p>
    <p>or (e.g.)</p>
    <p>out(y) 4 = max</p>
    <p>x1 . . . max</p>
    <p>xn f (x1, . . . , xn, y) 1(x1)   n(xn)</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 11</p>
    <p>Example</p>
    <p>g</p>
    <p>X =</p>
    <p>X</p>
    <p>?</p>
    <p>X</p>
    <p>h</p>
    <p>Open box:</p>
    <p>f (x, x, x) = g(x)h(x)(x x)(x x)</p>
    <p>X = X = X for all valid configurations.</p>
    <p>Closed box (= message out of the box):</p>
    <p>f (x) =  x</p>
    <p>x</p>
    <p>g(x)h(x)(x x)(x x) = g(x)h(x)</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 12</p>
    <p>Example: Addition / Convolution</p>
    <p>g</p>
    <p>X +</p>
    <p>X</p>
    <p>?</p>
    <p>X</p>
    <p>h</p>
    <p>Open box:</p>
    <p>f (x, x, x) = g(x)h(x)(x + x x)</p>
    <p>X + X = X for all valid configurations.</p>
    <p>Closed box (= message out of the box):</p>
    <p>f (x) =  x</p>
    <p>x</p>
    <p>g(x)h(x)(x + x x) =  x</p>
    <p>g(x)h(x  x)</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 13</p>
    <p>Outline</p>
    <p>Forney-style factor graphs  Some simple remarks  Dealing with continuous variables  On gradient methods</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 14</p>
    <p>Simple Remark 1:</p>
    <p>Combining Information</p>
    <p>Code 1</p>
    <p>=</p>
    <p>=</p>
    <p>Code 2</p>
    <p>Xk</p>
    <p>Xk+1</p>
    <p>Channel</p>
    <p>...</p>
    <p>Yk</p>
    <p>Yk+1 ...</p>
    <p>The correct handling of extrinsic/intrinsic information is automatic from the summary-product rule.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 15</p>
    <p>Simple Remark 2:</p>
    <p>Mappers and Such</p>
    <p>f</p>
    <p>XA</p>
    <p>XB Z</p>
    <p>g 01 g 00 g 10 g 11</p>
    <p>In the FFG, the mapper becomes a factor node with the local function</p>
    <p>f (xA, xB, z) 4 =</p>
    <p>{ 1, if f (xA, xB) = z 0, else</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 16</p>
    <p>Simple Remark 3:</p>
    <p>Hybrid Equality Node</p>
    <p>X</p>
    <p>finite set X =</p>
    <p>Y</p>
    <p>R</p>
    <p>(x  y) is a Kronecker delta in x and a Dirac delta in y.</p>
    <p>Messages:</p>
    <p>outY (y) =</p>
    <p>xX (y  x) inX(x), a sum of weighted Dirac deltas.</p>
    <p>outX(x) =</p>
    <p>y (x  y) inY (y) = inY (x),</p>
    <p>sampling the incoming density inY at the elements of X .</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 17</p>
    <p>Signal Processing by Message Passing</p>
    <p>Design choices:</p>
    <p>Modeling = choice of graph.  Choice of message types and of the corresponding update rules</p>
    <p>for continuous variables.</p>
    <p>Scheduling of the message computations.</p>
    <p>Most good known signal processing techniques can be used, combined, and extended.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 18</p>
    <p>Example:</p>
    <p>Channel with Unknown Parameters</p>
    <p>Code</p>
    <p>X1 X2 Xn</p>
    <p>Channel Model</p>
    <p>Y1 Y2 Yn 1 n</p>
    <p>Model for Channel Parameters</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 19</p>
    <p>Continuous Variables: Message Types</p>
    <p>The following message types are widely applicable.</p>
    <p>Hard-decision estimate.  Quantization of variables.</p>
    <p>Obvious, but infeasible in higher dimensions.</p>
    <p>Function value and derivative (or gradient) at a point selected by the receiving node. This data type allows gradient algorithms (e.g., LMS).</p>
    <p>Mean and variance, often with an underlying assumption of Gaussianity. This is the realm of Kalman filtering.</p>
    <p>List of samples. A pdf can be represented by a list of samples. This data type is the basis of the particle filter, but it can be used as a generic data type in a graphical model.</p>
    <p>Compound messages. The product of other message types.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 20</p>
    <p>Kalman Filtering and Smoothing</p>
    <p>A</p>
    <p>? ?</p>
    <p>B</p>
    <p>? ?</p>
    <p>+</p>
    <p>=</p>
    <p>?</p>
    <p>C</p>
    <p>? 6</p>
    <p>A</p>
    <p>?</p>
    <p>?</p>
    <p>B</p>
    <p>?</p>
    <p>+</p>
    <p>=</p>
    <p>?</p>
    <p>C</p>
    <p>? 6</p>
    <p>Messages consist of mean vector m and covariance matrix V or W = V 1.</p>
  </div>
  <div class="page">
    <p>Sum-Product Rules for Gaussian Messages</p>
    <p>(x  y)(x  z)</p>
    <p>=X- Z</p>
    <p>Y 6</p>
    <p>mZ = ( WX + WY</p>
    <p>)# (WXmX + WY mY )</p>
    <p>WZ = WX + WY</p>
    <p>VZ = VX ( VX + VY</p>
    <p>)# VY</p>
    <p>(x + y + z)</p>
    <p>+-X-  Z6</p>
    <p>Y 6</p>
    <p>mZ = mX  mY VZ = VX + VY</p>
    <p>WZ = WX ( WX + WY</p>
    <p>)# WY</p>
    <p>(y  Ax)</p>
    <p>AX</p>
    <p>-Y</p>
    <p>mY = AmX</p>
    <p>VY = AVXA H</p>
    <p>Problem with WY if A has not full row rank!</p>
    <p>(x  Ay)</p>
    <p>A X</p>
    <p>Y</p>
    <p>mY = ( AHWXA</p>
    <p>)# AHWXmX</p>
    <p>WY = A HWXA</p>
    <p>If A has full row rank:</p>
    <p>mY = A H ( AAH</p>
    <p>)1 mX</p>
  </div>
  <div class="page">
    <p>Rules for Composite Blocks</p>
    <p>The Heart of Kalman Filtering and RLS</p>
    <p>=X- Z</p>
    <p>? W 6</p>
    <p>A</p>
    <p>?Y 6</p>
    <p>mZ = mX + VXA HG (mY  AmX )</p>
    <p>VZ = VX  VXAHGAVX with G</p>
    <p>H )1</p>
    <p>+-X-  Z6</p>
    <p>W 6</p>
    <p>A</p>
    <p>Y 6</p>
    <p>mZ = mX  AmY WZ = WX  WXAHAHWX with H</p>
    <p>HWXA )1</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 23</p>
    <p>Data Type List of Samples: Particle Filter</p>
    <p>A probability distribution can be represented by a list of samples from the distribution.</p>
    <p>This data type is the basis of the particle filter, which has recently gained much attention in the statistics literature as a means to overcome the limitations of Kalman filtering.</p>
    <p>Its use for message passing in general factor graphs seems to be largely unexplored, but promising.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 24</p>
    <p>Continuous Variables: Message Types</p>
    <p>The following message types are widely applicable.</p>
    <p>Hard-decision estimate.  Quantization of variables.</p>
    <p>Obvious, but infeasible in higher dimensions.</p>
    <p>Function value and derivative (or gradient) at a point selected by the receiving node. This data type allows gradient algorithms (e.g., LMS).</p>
    <p>Mean and variance, often with an underlying assumption of Gaussianity. This is the realm of Kalman filtering.</p>
    <p>List of samples. A pdf can be represented by a list of samples. This data type is the basis of the particle filter, but it can be used as a generic data type in a graphical model.</p>
    <p>Compound messages. The product of other message types.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 25</p>
    <p>On Gradient Methods</p>
    <p>fA PPPPPPPPPP</p>
    <p>fB</p>
    <p>=</p>
    <p>f () = fA()fB()</p>
    <p>Suppose we wish to find</p>
    <p>4 = argmax</p>
    <p>f ()</p>
    <p>(and/or f ()) by solving</p>
    <p>d</p>
    <p>d</p>
    <p>( log f ()</p>
    <p>) = 0</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 26</p>
    <p>On Gradient Methods (contd)</p>
    <p>fA PPPPPPPPPP PPi</p>
    <p>PPq</p>
    <p>fB</p>
    <p>)</p>
    <p>1 =</p>
    <p>and/or fA()fB()</p>
    <p>( log fA()</p>
    <p>) =</p>
    <p>new = old + s  d</p>
    <p>d</p>
    <p>( log f ()</p>
    <p>) =old</p>
    <p>= old + s</p>
    <p>( d</p>
    <p>d</p>
    <p>( log fA()</p>
    <p>) =old</p>
    <p>+ d</p>
    <p>d</p>
    <p>( log fB()</p>
    <p>) =old</p>
    <p>) where s  R is a step-size parameter.</p>
  </div>
  <div class="page">
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>%</p>
    <p>&amp;</p>
    <p>$</p>
    <p>% Brest 2003 27</p>
    <p>Conclusions</p>
    <p>Factor graphs help to develop practical algorithms for complex real-world detection and estimation problems.</p>
    <p>Such algorithms may include combinations and extensions of most prior art, including gradient methods, Kalman filtering, particle filters, . . . .</p>
    <p>There are parallel developments (with similar graphical models) in statistics, artificial intelligence, and neural networks.</p>
    <p>Forney-style factor graphs are especially elegant.</p>
  </div>
</Presentation>
