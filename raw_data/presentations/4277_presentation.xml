<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Conclusions</p>
    <p>Learning Topic-Sensitive Word Representations</p>
    <p>Results (Generalized Average Precision)</p>
    <p>Marzieh Fadaee Arianna Bisazza Christof Monz Informatics Institute, University of Amsterdam</p>
    <p>Evaluation: Lexical Substitution Task</p>
    <p>c</p>
    <p>Topic Model: Hierarchical Dirichlet Process (HDP)Motivation</p>
    <p>Topic-Sensitive Representation Models Hard Topic-Labeled Representation Hard Topic-Labeled + Generic Word Representation Soft Topic-Labeled Representation</p>
    <p>Example: Nearest Neighbors of bat Topic-Sensitive Skipgram (HTLE)</p>
    <p>bats batting Bat catcher fielder hitter balls</p>
    <p>While the team at bat is trying to score runs, the team in the field is attempting to record outs.</p>
    <p>So that in one way things in the distressed areas are not as bad as they might be .</p>
    <p>direction  respect  route  aspect  some</p>
    <p>Pre-trained Glove</p>
    <p>Pre-trained Skipgram</p>
    <p>ball pitchout batter toss-for umpire batting fielder</p>
    <p>bats batting hitter batsman batted hoary Batting</p>
    <p>1 vespertilionidae heran hipposideros sorenseni luctus coxi kerivoula</p>
    <p>2</p>
    <p>Sim(ws, wt) = cos(h(w  s ), h(w</p>
    <p>0</p>
    <p>t )) +</p>
    <p>P c cos(h(w</p>
    <p>s ), o(wc))</p>
    <p>C</p>
    <p>Having one representation per word fails to capture polysemy</p>
    <p>We propose an approach to learn multiple representations per word by topic-modeling the context with HDP</p>
    <p>Polysemous word &quot;Diverse contexts  Distinct topic distributions</p>
    <p>When context is available, multiple representations per word perform best in capturing the underlying meaning</p>
    <p>Our topic-sensitive representations:</p>
    <p>capture different word senses</p>
    <p>work as good as Skipgram with 6 times fewer dimensions</p>
    <p>obtain improvements in the lexical substitution task, performing best in Noun substitution</p>
    <p>Example:</p>
    <p>Sim(ws, wt)</p>
    <p>Ranking</p>
    <p>Sim(ws, wt) = X</p>
    <p>,0</p>
    <p>p() p(0)cos(h(ws ), h(w 0</p>
    <p>t )) +</p>
    <p>P ,c cos(h(w</p>
    <p>s ), o(wc)) p()</p>
    <p>C</p>
    <p>Sampling</p>
    <p>hHTLE</p>
    <p>owi+j</p>
    <p>wkiwi k</p>
    <p>r(wki )</p>
    <p>owi+j</p>
    <p>hSTLE</p>
    <p>wi   1 K</p>
    <p>w1i , w 2 i , ..., w</p>
    <p>K i</p>
    <p>r00(w1i ) r 00(w2i ) r</p>
    <p>owi+j</p>
    <p>r0(wi)</p>
    <p>hHTLEadd</p>
    <p>r0(wki )</p>
    <p>wi k</p>
    <p>wki , wi</p>
    <p>document-specific topic distribution</p>
    <p>Topics</p>
    <p>The bat wing is a membrane stretched across four &quot;extremely&quot; elongated fingers.</p>
    <p>Example: The word bat in two different sentences:</p>
    <p>Model</p>
    <p>LS-SE07 LS-CIC</p>
    <p>SGE + C MSSG HTLE HTLEadd STLE</p>
    <p>HTLE HTLEadd STLE</p>
    <p>Inference</p>
    <p>N/A</p>
    <p>Sampled</p>
    <p>Expected</p>
    <p>Uses the hard topic labels resulting from HDP sampling to learn representations</p>
    <p>Uses the sum of the hard topic-labeled representation and the generic (i.e. unlabeled) representation</p>
    <p>Uses the topic distribution to compute a weighted sum over the word-topic representations</p>
    <p>Skipgram</p>
    <p>uroderma magnirostrum sorenseni miniopterus promops luctus micronycteris</p>
    <p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 639.021.646</p>
  </div>
</Presentation>
