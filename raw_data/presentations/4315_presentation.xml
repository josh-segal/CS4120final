<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module</p>
    <p>Juan Pavez, Hctor Allende, Hctor Allende-Cid</p>
    <p>1</p>
  </div>
  <div class="page">
    <p>Reasoning for Question Answering Reasoning is crucial for building systems that can dialogue with humans in natural language.</p>
    <p>Reasoning: The process of forming conclusions, judgments, or inferences from facts or premises.</p>
    <p>Examples:</p>
    <p>Inferential Reasoning: Premise 1, Premise 2 -&gt; Conclusion</p>
    <p>John is in the kitchen, John has the ball -&gt; The ball is in the kitchen</p>
    <p>Relational Reasoning: Reason about relations between entities and their properties (Santoro et al.)</p>
    <p>Causal Reasoning, Logical Reasoning,  2</p>
  </div>
  <div class="page">
    <p>bAbI Dataset (Weston et al., 2015)</p>
    <p>One of the earliest datasets to measure the reasoning abilities of ML systems.</p>
    <p>Synthetic. Not NLP.</p>
    <p>Easy to evaluate different reasoning capabilities.</p>
    <p>Noiseless tasks: Separates reasoning analysis from natural language understanding.</p>
    <p>A thorough analysis can be found in (Lee et al., 2016)</p>
    <p>Category 2: Two Supporting Facts. 01: Mary went to the kitchen. 02: Sandra journeyed to the office 03: Mary got the football there. 04: Mary travelled to the garden. 05: Where is the football? garden 3 4</p>
    <p>Category 4: Path Finding. 01: The bedroom is south of the hallway.. 02: The bathroom is east of the office. 03: The kitchen is west of the garden. 04: The garden is south of the office. 05: The office is south of the bedroom. 05: How do you go from the garden to the bedroom?? n,n 4 5</p>
    <p>3</p>
  </div>
  <div class="page">
    <p>bAbI Dataset (Weston et al., 2015)</p>
    <p>One of the earliest datasets to measure the reasoning abilities of ML systems.</p>
    <p>Synthetic. Not NLP.</p>
    <p>Easy to evaluate different reasoning capabilities.</p>
    <p>Noiseless tasks: Separates reasoning analysis from natural language understanding.</p>
    <p>A thorough analysis can be found in (Lee et al., 2016)</p>
    <p>Category 2: Two Supporting Facts. 01: Mary went to the kitchen. 02: Sandra journeyed to the office 03: Mary got the football there. 04: Mary travelled to the garden. 05: Where is the football? garden 3 4</p>
    <p>Category 4: Path Finding. 01: The bedroom is south of the hallway.. 02: The bathroom is east of the office. 03: The kitchen is west of the garden. 04: The garden is south of the office. 05: The office is south of the bedroom. 05: How do you go from the garden to the bedroom?? n,n 4 5</p>
    <p>Has(Mary, Football), Is(Mary, Garden)  Is(Football, Garden)</p>
    <p>4</p>
  </div>
  <div class="page">
    <p>bAbI Dataset (Weston et al., 2015)</p>
    <p>One of the earliest datasets to measure the reasoning abilities of ML systems.</p>
    <p>Synthetic. Not NLP.</p>
    <p>Easy to evaluate different reasoning capabilities.</p>
    <p>Noiseless tasks: Separates reasoning analysis from natural language understanding.</p>
    <p>A thorough analysis can be found in (Lee et al., 2016)</p>
    <p>Category 2: Two Supporting Facts. 01: Mary went to the kitchen. 02: Sandra journeyed to the office 03: Mary got the football there. 04: Mary travelled to the garden. 05: Where is the football? garden 3 4</p>
    <p>Category 4: Path Finding. 01: The bedroom is south of the hallway.. 02: The bathroom is east of the office. 03: The kitchen is west of the garden. 04: The garden is south of the office. 05: The office is south of the bedroom. 05: How do you go from the garden to the bedroom?? n,n 4 5</p>
    <p>S(Garden, Office), S(Office, Bedroom)</p>
    <p>N, N &amp; N = S1</p>
    <p>5</p>
  </div>
  <div class="page">
    <p>Memory Augmented Neural Networks</p>
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015) Process a set of inputs and store them in memory. Then, at each hop, an important part of the memory is retrieved and used to retrieve more memories. Finally, the last retrieved memory is used to compute the answer.</p>
    <p>m1 = wdaniel + wwent + . . .</p>
    <p>m1 03: Mary got the football there. 04: Mary travelled to the garden</p>
    <p>u = wwhere + wis + . . .</p>
    <p>q:Where is the football. 6</p>
  </div>
  <div class="page">
    <p>Memory Augmented Neural Networks</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>i = Softmax(u Tmi)</p>
    <p>o1 =  i</p>
    <p>imi</p>
    <p>o1</p>
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015) Process a set of inputs and store them in memory. Then, at each hop, an important part of the memory is retrieved and used to retrieve more memories. Finally, the last retrieved memory is used to compute the answer.</p>
    <p>7</p>
  </div>
  <div class="page">
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015) Process a set of inputs and store them in memory. Then, at each hop, an important part of the memory is retrieved and used to retrieve more memories. Finally, the last retrieved memory is used to compute the answer.</p>
    <p>Memory Augmented Neural Networks</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>o1</p>
    <p>m1m2 m3m4</p>
    <p>o2</p>
    <p>i = Softmax(o T 1 mi)</p>
    <p>o2 =  i</p>
    <p>imi</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>8</p>
  </div>
  <div class="page">
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015) Process a set of inputs and store them in memory. Then, at each hop, an important part of the memory is retrieved and used to retrieve more memories. Finally, the last retrieved memory is used to compute the answer.</p>
    <p>Memory Augmented Neural Networks</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>o1</p>
    <p>m1m2 m3m4</p>
    <p>o2</p>
    <p>Hop 1</p>
    <p>Hop 2 a = Softmax(Wo2)</p>
    <p>aW</p>
    <p>9</p>
  </div>
  <div class="page">
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015) Process a set of inputs and store them in memory. Then, at each hop, an important part of the memory is retrieved and used to retrieve more memories. Finally, the last retrieved memory is used to compute the answer.</p>
    <p>Memory Augmented Neural Networks</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>o1</p>
    <p>m1m2 m3m4</p>
    <p>o2</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>aW</p>
    <p>10</p>
    <p>y</p>
    <p>L(y, y) =   i</p>
    <p>yiln( yi)</p>
  </div>
  <div class="page">
    <p>Memory Augmented Neural Networks</p>
    <p>Memory Networks (Weston et al. 2014, Sukhbaatar et al. 2015)</p>
    <p>Some weaknesses:</p>
    <p>The attention mechanism is simple</p>
    <p>The attention mechanism relies on embeddings.</p>
    <p>It may be nice to separate embedding learning from attention learning (modularization, reusability).</p>
    <p>The answer computation is too simple, it only uses one retrieved memory. Hard to see how can produce more complex reasoning based on memories.</p>
    <p>11</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017) Neural Network with an inductive bias to learn pairwise relations of the input objects and their properties. A type of Graph Neural Networks.</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>memories</p>
    <p>12</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017) Neural Network with an inductive bias to learn pairwise relations of the input objects and their properties. A type of Graph Neural Networks.</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>memories memories pairs with question</p>
    <p>13</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017) Neural Network with an inductive bias to learn pairwise relations of the input objects and their properties. A type of Graph Neural Networks.</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>memories memories pairs with question</p>
    <p>oi,j = g([mi; mj; u])</p>
    <p>g g g</p>
    <p>o1,2 o1,3</p>
    <p>o3,4</p>
    <p>14</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017) Neural Network with an inductive bias to learn pairwise relations of the input objects and their properties. A type of Graph Neural Networks.</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>memories memories pairs with question</p>
    <p>a = f(  i,j</p>
    <p>oi,j)</p>
    <p>g g g</p>
    <p>o1,2 o1,3</p>
    <p>o3,4  a</p>
    <p>f 01: Daniel went to the bathroom. 02: Sandra journeyed to the office. 03: Mary got the football there. 04: Mary travelled to the garden</p>
    <p>15</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017) Neural Network with an inductive bias to learn pairwise relations of the input objects and their properties. A type of Graph Neural Networks.</p>
    <p>m1</p>
    <p>q:Where is the football.</p>
    <p>m2 m3m4</p>
    <p>u</p>
    <p>memories memories pairs with question g</p>
    <p>g g</p>
    <p>o1,2 o1,3</p>
    <p>o3,4  a</p>
    <p>f 01: Daniel went to the bathroom. 02: Sandra journeyed to the office. 03: Mary got the football there. 04: Mary travelled to the garden</p>
    <p>16</p>
    <p>y</p>
    <p>L(y, y) =   i</p>
    <p>yiln( yi)</p>
  </div>
  <div class="page">
    <p>Relational Neural Networks</p>
    <p>Relation Networks (Santoro et al. 2017)</p>
    <p>Some weaknesses:</p>
    <p>The model needs to process pairs where N is the number of memories.</p>
    <p>500 memories would require 250k backward and forward computations!</p>
    <p>Can not filter out unuseful objects that can produce spurious relations.</p>
    <p>N2</p>
    <p>17</p>
  </div>
  <div class="page">
    <p>Reasoning ModuleAttention ModuleShort-term Memory Module</p>
    <p>Working Memory Network (Pavez et al., 2018) A Memory Network model with a new working memory buffer and relational reasoning module. Produces state-of-the-art results in reasoning tasks. Inspired by the Multi-component model of working memory.</p>
    <p>m1 = GRU([Wdaniel; Wwent; . . . ])</p>
    <p>m1 03: Mary got the football there. 04: Mary travelled to the garden</p>
    <p>q:Where is the football.</p>
    <p>u = GRU([Wwhere; Wis; . . . ])</p>
    <p>u</p>
    <p>Working Memory Networks</p>
    <p>18</p>
  </div>
  <div class="page">
    <p>Reasoning ModuleAttention ModuleShort-term Memory Module Attention Module</p>
    <p>q:Where is the football.</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m</p>
    <p>Multi-head attention (Vaswani et al. 2017)</p>
    <p>mli = W l mmi</p>
    <p>li = Softmax((u T mli / d))</p>
    <p>hl =  j</p>
    <p>lj m l j</p>
    <p>o1 = [h1; h2; . . . ]Wo</p>
    <p>u</p>
    <p>o1</p>
    <p>Working Memory Networks</p>
    <p>19</p>
    <p>Working Memory Network (Pavez et al., 2018) A Memory Network model with a new working memory buffer and relational reasoning module. Produces state-of-the-art results in reasoning tasks. Inspired by the Multi-component model of working memory.</p>
  </div>
  <div class="page">
    <p>Working Memory Network (Pavez et al., 2018) A Memory Network model with a new working memory buffer and relational reasoning module. Produces state-of-the-art results in reasoning tasks. Inspired by the Multi-component model of working memory.</p>
    <p>Reasoning ModuleAttention ModuleShort-term Memory Module</p>
    <p>Multi-head attention (Vaswani et al. 2017)</p>
    <p>q:Where is the football.</p>
    <p>mli = W l mmi</p>
    <p>li = Softmax((( ft(o1) T mli / d))</p>
    <p>hl =  j</p>
    <p>lj m l j</p>
    <p>o1 = [h1; h2; . . . ]Wo</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m</p>
    <p>u</p>
    <p>o1</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  o2</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>Working Memory Networks</p>
    <p>20</p>
  </div>
  <div class="page">
    <p>Attention ModuleShort-term Memory Module</p>
    <p>ft</p>
    <p>q:Where is the football.</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m</p>
    <p>u</p>
    <p>o1</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  o2</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  oH</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>Hop H</p>
    <p>Reasoning Module</p>
    <p>Working memory buffer</p>
    <p>21</p>
  </div>
  <div class="page">
    <p>Reasoning ModuleAttention ModuleShort-term Memory Module</p>
    <p>q:Where is the football.</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m</p>
    <p>u</p>
    <p>o1</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  o2</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  oH</p>
    <p>memories pairs with question</p>
    <p>g o1,2  a</p>
    <p>f a</p>
    <p>g o3,4</p>
    <p>g o4,H</p>
    <p>a = f(  i,j</p>
    <p>g([oi; oj; u])</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>Hop H</p>
    <p>Working memory buffer</p>
    <p>22</p>
  </div>
  <div class="page">
    <p>Reasoning ModuleAttention ModuleShort-term Memory Module</p>
    <p>q:Where is the football.</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m</p>
    <p>u</p>
    <p>o1</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  o2</p>
    <p>m1 m2 m3 m4</p>
    <p>W1m  oH</p>
    <p>memories pairs with question</p>
    <p>g o1,2  a</p>
    <p>f a</p>
    <p>g o3,4</p>
    <p>g o4,H</p>
    <p>Hop 1</p>
    <p>Hop 2</p>
    <p>Hop H</p>
    <p>Working memory buffer</p>
    <p>23</p>
    <p>y</p>
    <p>L(y, y) =   i</p>
    <p>yiln( yi)</p>
  </div>
  <div class="page">
    <p>Results - Jointly trained bAbI-10k.  Results on jointly trained bAbI-10k: Train a single model on all tasks simultaneously.</p>
    <p>Note that EntNet (Henaff et al.) solves all tasks in the per-task version: A single model for each task.</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>LSTM MemNN MemNN-S RN SDNC WMemNN WMemNN*</p>
    <p>24(Rae et al.)(Santoro et al.)(Sukhbaatar et al.) (Pavez et al.) (Pavez et al.)(Sukhbaatar et al.)(Sukhbaatar et al.)</p>
  </div>
  <div class="page">
    <p>Results - Jointly trained bAbI-10k.  Results on jointly trained bAbI-10k: Train a single model on all tasks simultaneously.</p>
    <p>Note that EntNet (Henaff et al.) solves all tasks in the per-task version: A single model for each task.</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>MemNN MemNN-S RN SDNC WMemNN WMemNN*</p>
    <p>25(Rae et al.)(Santoro et al.)(Sukhbaatar et al.) (Pavez et al.) (Pavez et al.)(Sukhbaatar et al.)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Results - Jointly trained bAbI-10k.  Results on jointly trained bAbI-10k: Train a single model on all tasks simultaneously.</p>
    <p>Note that EntNet (Henaff et al.) solves all tasks in the per-task version: A single model for each task.</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>MemNN MemNN-S RN SDNC WMemNN WMemNN*</p>
    <p>26(Rae et al.)(Santoro et al.)(Sukhbaatar et al.) (Pavez et al.) (Pavez et al.)(Sukhbaatar et al.)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Results - Jointly trained bAbI-10k.  Results on jointly trained bAbI-10k: Train a single model on all tasks simultaneously.</p>
    <p>Note that EntNet (Henaff et al.) solves all tasks in the per-task version: A single model for each task.</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>MemNN MemNN-S RN SDNC WMemNN WMemNN*</p>
    <p>27(Rae et al.)(Santoro et al.)(Sukhbaatar et al.) (Pavez et al.) (Pavez et al.)(Sukhbaatar et al.)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Results - Jointly trained bAbI-10k.  Results on jointly trained bAbI-10k: Train a single model on all tasks simultaneously.</p>
    <p>Note that EntNet (Henaff et al.) solves all tasks in the per-task version: A single model for each task.</p>
    <p>A cc</p>
    <p>ur ac</p>
    <p>y</p>
    <p>MemNN MemNN-S RN SDNC WMemNN WMemNN*</p>
    <p>28(Rae et al.)(Santoro et al.)(Sukhbaatar et al.) (Pavez et al.) (Pavez et al.)(Sukhbaatar et al.)</p>
    <p>?</p>
  </div>
  <div class="page">
    <p>Ablations</p>
    <p>29</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>co un</p>
    <p>tin g</p>
    <p>ba si</p>
    <p>c in</p>
    <p>du ct</p>
    <p>io n</p>
    <p>si ze</p>
    <p>re as</p>
    <p>on in</p>
    <p>g</p>
    <p>po si</p>
    <p>tio na</p>
    <p>l re</p>
    <p>as on</p>
    <p>in g</p>
    <p>pa th</p>
    <p>fi nd</p>
    <p>in g</p>
    <p>MemNN (Sukhbaatar et al.)</p>
    <p>MemNN(S) (Sukhbaatar et al.</p>
    <p>RN (Santoro et al.)</p>
    <p>WMemNN (no multi-head)</p>
    <p>WMemNN</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>multiple relations complex attention patterns</p>
  </div>
  <div class="page">
    <p>Ablations</p>
    <p>30</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>co un</p>
    <p>tin g</p>
    <p>ba si</p>
    <p>c in</p>
    <p>du ct</p>
    <p>io n</p>
    <p>si ze</p>
    <p>re as</p>
    <p>on in</p>
    <p>g</p>
    <p>po si</p>
    <p>tio na</p>
    <p>l re</p>
    <p>as on</p>
    <p>in g</p>
    <p>pa th</p>
    <p>fi nd</p>
    <p>in g</p>
    <p>MemNN (Sukhbaatar et al.)</p>
    <p>MemNN(S) (Sukhbaatar et al.</p>
    <p>RN (Santoro et al.)</p>
    <p>WMemNN (no multi-head)</p>
    <p>WMemNN</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>multiple relations complex attention patterns</p>
  </div>
  <div class="page">
    <p>Ablations</p>
    <p>31</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>co un</p>
    <p>tin g</p>
    <p>ba si</p>
    <p>c in</p>
    <p>du ct</p>
    <p>io n</p>
    <p>si ze</p>
    <p>re as</p>
    <p>on in</p>
    <p>g</p>
    <p>po si</p>
    <p>tio na</p>
    <p>l re</p>
    <p>as on</p>
    <p>in g</p>
    <p>pa th</p>
    <p>fi nd</p>
    <p>in g</p>
    <p>MemNN (Sukhbaatar et al.)</p>
    <p>MemNN(S) (Sukhbaatar et al.</p>
    <p>RN (Santoro et al.)</p>
    <p>WMemNN (no multi-head)</p>
    <p>WMemNN</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>multiple relations complex attention patterns</p>
  </div>
  <div class="page">
    <p>Ablations</p>
    <p>32</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>co un</p>
    <p>tin g</p>
    <p>ba si</p>
    <p>c in</p>
    <p>du ct</p>
    <p>io n</p>
    <p>si ze</p>
    <p>re as</p>
    <p>on in</p>
    <p>g</p>
    <p>po si</p>
    <p>tio na</p>
    <p>l re</p>
    <p>as on</p>
    <p>in g</p>
    <p>pa th</p>
    <p>fi nd</p>
    <p>in g</p>
    <p>MemNN (Sukhbaatar et al.)</p>
    <p>MemNN(S) (Sukhbaatar et al.</p>
    <p>RN (Santoro et al.)</p>
    <p>WMemNN (no multi-head)</p>
    <p>WMemNN</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>multiple relations complex attention patterns</p>
  </div>
  <div class="page">
    <p>Ablations</p>
    <p>33</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>co un</p>
    <p>tin g</p>
    <p>ba si</p>
    <p>c in</p>
    <p>du ct</p>
    <p>io n</p>
    <p>si ze</p>
    <p>re as</p>
    <p>on in</p>
    <p>g</p>
    <p>po si</p>
    <p>tio na</p>
    <p>l re</p>
    <p>as on</p>
    <p>in g</p>
    <p>pa th</p>
    <p>fi nd</p>
    <p>in g</p>
    <p>MemNN (Sukhbaatar et al.)</p>
    <p>MemNN(S) (Sukhbaatar et al.</p>
    <p>RN (Santoro et al.)</p>
    <p>WMemNN (no multi-head)</p>
    <p>WMemNN</p>
    <p>pp or</p>
    <p>tin g</p>
    <p>fa</p>
    <p>ct s</p>
    <p>multiple relations complex attention patterns</p>
  </div>
  <div class="page">
    <p>Time comparisons for a forward and backward pass for a single batch of size 32.</p>
    <p>For 30 memories there is a speedup of almost 20x.</p>
    <p>34</p>
    <p>Time comparison</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>35</p>
    <p>We presented the Working Memory Neural Network, a Memory Network model augmented with a new working memory buffer and relational reasoning module.</p>
    <p>It retains the relational reasoning capabilities of the relation network while reducing it computation times considerably.</p>
    <p>We hope that this contribution may help applying the relation network in larger problems.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>36</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>37</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>38</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
    <p>Reasoning module</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>39</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
    <p>Reasoning module</p>
    <p>CNN</p>
    <p>GRU Multi-head attention</p>
    <p>There is exactly one black triangle not touching any edge</p>
    <p>Relational Reasoning Module</p>
    <p>NLVR: dev/test</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>40</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
    <p>Reasoning module</p>
    <p>CNN</p>
    <p>GRU Multi-head attention</p>
    <p>There is exactly one black triangle not touching any edge</p>
    <p>Relational Reasoning Module</p>
    <p>16. Why, what are YOUR shoes done with? 17. said the Gryphon. 18. I mean, what makes then so shiny? 19. Alice looked down at then, and considered a little before she gave her answer. 20. They are done with blacking, I believe .</p>
    <p>q. Boots and shoes under the sea.  the _______ went in a deep voice, are done with a whiting.</p>
    <p>biGRU</p>
    <p>biGRU Scaled Dot-product Attention</p>
    <p>Attention Sum Reasoning Module</p>
    <p>NLVR: dev/test</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>41</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
    <p>Reasoning module</p>
    <p>CNN</p>
    <p>GRU Multi-head attention</p>
    <p>There is exactly one black triangle not touching any edge</p>
    <p>Relational Reasoning Module</p>
    <p>16. Why, what are YOUR shoes done with? 17. said the Gryphon. 18. I mean, what makes then so shiny? 19. Alice looked down at then, and considered a little before she gave her answer. 20. They are done with blacking, I believe .</p>
    <p>q. Boots and shoes under the sea.  the _______ went in a deep voice, are done with a whiting.</p>
    <p>biGRU</p>
    <p>biGRU Attention Sum Reasoning Module</p>
    <p>Elmo LSTM</p>
    <p>ResNet BiAtt AoA</p>
    <p>GA Attention SumGraph NNDecoder</p>
    <p>Scaled Dot-product Attention</p>
    <p>NLVR: dev/test</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>42</p>
    <p>It is a very general framework. We argue that it should include:</p>
    <p>Embedding + Short-term storage</p>
    <p>Attentional controller + Working memory buffer</p>
    <p>Reasoning module</p>
    <p>CNN</p>
    <p>GRU Multi-head attention</p>
    <p>There is exactly one black triangle not touching any edge</p>
    <p>Relational Reasoning Module</p>
    <p>16. Why, what are YOUR shoes done with? 17. said the Gryphon. 18. I mean, what makes then so shiny? 19. Alice looked down at then, and considered a little before she gave her answer. 20. They are done with blacking, I believe .</p>
    <p>q. Boots and shoes under the sea.  the _______ went in a deep voice, are done with a whiting.</p>
    <p>biGRU</p>
    <p>biGRU Attention Sum Reasoning Module</p>
    <p>Elmo LSTM</p>
    <p>ResNet BiAtt AoA</p>
    <p>GA Attention SumGraph NNDecoder</p>
    <p>Scaled Dot-product Attention</p>
    <p>NLVR: dev/test</p>
  </div>
  <div class="page">
    <p>juan.pavezs@alumnos.usm.cl</p>
    <p>Code: https://github.com/jgpavez/Working-Memory-Networks</p>
    <p>Thanks!</p>
    <p>43</p>
    <p>@juanpavez</p>
  </div>
</Presentation>
