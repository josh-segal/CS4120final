<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hamming Embedding and Weak Geometry Consistency for large scale image search</p>
    <p>Herv Jgou, Matthijs Douze &amp; Cordelia Schmid</p>
  </div>
  <div class="page">
    <p>Large scale object/scene recognition</p>
    <p>Each image described by approximately 2000 descriptors  2 109 descriptors to index!</p>
    <p>Database representation in RAM:  Raw size of descriptors : 1 TB, search+memory intractable</p>
    <p>Fast search with LSH: 800 GB, memory intractable</p>
    <p>Image search system</p>
    <p>ranked image list</p>
    <p>Image dataset: &gt; 1 million images</p>
    <p>query</p>
  </div>
  <div class="page">
    <p>State-of-the-art: Bag-of-features (BOF) [Sivic &amp; Zisserman03]</p>
    <p>Hessian-Affine regions + SIFT descriptors</p>
    <p>Bag-of-features processing</p>
    <p>+tf-idf weighting</p>
    <p>querying</p>
    <p>sparse frequency vector</p>
    <p>centroids (visual words)</p>
    <p>Inverted file</p>
    <p>[Mikolajezyk &amp; Schmid 04] [Lowe 04]</p>
    <p>ranked image short-list</p>
    <p>Set of SIFT descriptors</p>
    <p>Query image</p>
    <p>Geometric verification</p>
    <p>Re-ranked list</p>
    <p>[Lowe 04, Chum &amp; al 2007]</p>
    <p>visual words:  1 word (index) per local descriptor</p>
    <p>only images ids in inverted file</p>
    <p>=&gt; 8 GB fits!</p>
    <p>[Sivic &amp; al 03, Nister &amp; al 04, ]</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Bag-of-features: approximate nearest neighbors (ANN) search interpretation</p>
    <p>Hamming Emdedding</p>
    <p>Weak geometry consistency</p>
  </div>
  <div class="page">
    <p>Bag-of-features as an ANN search algorithm</p>
    <p>Matching function of descriptors : k-nearest neighbors or -search</p>
    <p>Bag-of-features matching function where q(x) is a quantizer, i.e., assignment to visual word and</p>
    <p>a,b is the Kronecker operator (a,b=1 iff a=b)</p>
    <p>fq(x;y) = q(x);q(y)</p>
  </div>
  <div class="page">
    <p>Approximate nearest neighbor search evaluation</p>
    <p>ANN algorithms usually returns a short-list of nearest neighbors  this short-list is supposed to contain the NN with high probability</p>
    <p>exact search may be performed to re-order this short-list</p>
    <p>Proposed quality evaluation of ANN search: trade-off between</p>
    <p>Accuracy: NN recall = probability that the NN is in this list</p>
    <p>against</p>
    <p>Ambiguity removal = proportion of vectors in the short-list  the lower this proportion, the more information we have about the vector</p>
    <p>the lower this proportion, the lower the complexity if we perform exact search on the short-list</p>
    <p>ANN search algorithms usually have some parameters to handle this trade-off</p>
  </div>
  <div class="page">
    <p>ANN evaluation of bag-of-features</p>
    <p>ANN algorithms returns a list of potential neighbors</p>
    <p>Accuracy: NN recall = probability that the NN is in this list</p>
    <p>Ambiguity removal: = proportion of vectors in the short-list</p>
    <p>In BOF, this trade-off is managed by the number of clusters k</p>
    <p>N N</p>
    <p>r ec</p>
    <p>al l</p>
    <p>rate of points retrieved</p>
    <p>k=100</p>
    <p>BOF</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Bag-of-features: voting and ANN interpretation</p>
    <p>Hamming Embedding</p>
    <p>Weak geometry consistency</p>
  </div>
  <div class="page">
    <p>State-of-the art: First issue</p>
    <p>The intrinsic matching scheme performed by BOF is weak  for a small visual dictionary: too many false matches</p>
    <p>for a large visual dictionary: many true matches are missed</p>
    <p>No good trade-off between small and large !  either the Voronoi cells are too big</p>
    <p>or these cells cant absorb the descriptor noise</p>
    <p>intrinsic approximate nearest neighbor search of BOF is not sufficient</p>
  </div>
  <div class="page"/>
  <div class="page"/>
  <div class="page">
    <p>State-of-the art: First issue</p>
    <p>Need to fight against the quantization noise  several recent paper proposed methods to have a richer representation of (sets of) descriptors</p>
    <p>In image search: multiple or soft assignment of descriptors to visual words  Jegou et al, A contextual dissimilarity measure for accurate and efficient</p>
    <p>image search, CVPR2007</p>
    <p>Philbin et al., Lost in quantization: improving particular object retrieval in large scale image databases, CVPR2008</p>
    <p>these methods reduce the sparsity of the BOF representation  negative impact on the search efficiency</p>
  </div>
  <div class="page">
    <p>Hamming Embedding</p>
    <p>Representation of a descriptor x  Vector-quantized to q(x) as in standard BOF</p>
    <p>+ short binary vector b(x) for an additional localization in the Voronoi cell</p>
    <p>Two descriptors x and y match iif</p>
    <p>where h(a,b) is the Hamming distance</p>
    <p>Nearest neighbors for Hamming distance  those for Euclidean distance  a metric in the embedded space reduces dimensionality curse effects</p>
    <p>Efficiency</p>
    <p>Hamming distance = very few operations</p>
    <p>Fewer random memory accesses: 3 x faster that standard BOF with same dictionary size!</p>
    <p>q(x) = q(y) h(b(x); b(y)) &lt;</p>
  </div>
  <div class="page">
    <p>Hamming Embedding</p>
    <p>Off-line (given a quantizer)</p>
    <p>draw an orthogonal projection matrix P of size db  d</p>
    <p>this defines db random projection directions</p>
    <p>for each Voronoi cell and projection direction, compute the median value for a learning set</p>
    <p>On-line: compute the binary signature b(x) of a given descriptor</p>
    <p>project x onto the projection directions as z(x) = (z1,zdb)</p>
    <p>bi(x) = 1 if zi(x) is above the learned median value, otherwise 0</p>
  </div>
  <div class="page">
    <p>Indexing structure filters 99.9995%</p>
    <p>of the descriptors (for k=200000)</p>
    <p>filters 98.8% of the remaining descriptors</p>
    <p>(for ht=22)</p>
  </div>
  <div class="page">
    <p>Hamming and Euclidean neighborhood</p>
    <p>trade-off between memory usage and accuracy</p>
    <p>more bits yield higher accuracy</p>
    <p>We used 64 bits (8 bytes)</p>
    <p>ra te</p>
    <p>o f</p>
    <p>N r</p>
    <p>et ri</p>
    <p>ev ed</p>
    <p>( re</p>
    <p>ca ll</p>
    <p>)</p>
    <p>rate of cells points retrieved</p>
    <p>ar bi</p>
    <p>tra r d</p>
    <p>es cr</p>
    <p>ip to</p>
    <p>r f ilt</p>
    <p>er in</p>
    <p>g (ra</p>
    <p>nd om</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Hamming Embedding: filtering matches</p>
    <p>re ca</p>
    <p>ll</p>
    <p>Hamming distance threshold</p>
    <p>all descriptors</p>
    <p>b(x) {0,1}64</p>
  </div>
  <div class="page">
    <p>ANN evaluation of Hamming Embedding</p>
    <p>N N</p>
    <p>r ec</p>
    <p>al l</p>
    <p>rate of points retrieved (=1-filtering rate)</p>
    <p>k=100</p>
    <p>ht=16</p>
    <p>HE+BOF BOF</p>
    <p>compared to BOF: at least 10 times less points in the short-list for the same level of accuracy</p>
    <p>Hamming Embedding provides a much better trade-off between recall and ambiguity removal</p>
  </div>
  <div class="page">
    <p>Hamming Embedding: Example</p>
  </div>
  <div class="page">
    <p>Compared with 20K dictionary : false matches</p>
  </div>
  <div class="page">
    <p>Hamming Embedding: Example</p>
  </div>
  <div class="page">
    <p>Compared with 200K visual word: good matches missed</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Bag-of-features: voting and ANN interpretation</p>
    <p>Hamming Emdedding</p>
    <p>Weak geometry consistency</p>
  </div>
  <div class="page">
    <p>State-of-the art: Second issue</p>
    <p>Re-ranking based on full geometric verification  works very well</p>
    <p>but performed on a short-list only (typically, 100 images)</p>
    <p>for very large datasets, the number of distracting images is so high that relevant images are not even short-listed!</p>
    <p>ra te</p>
    <p>o f r</p>
    <p>el ev</p>
    <p>an t i</p>
    <p>m ag</p>
    <p>es s</p>
    <p>ho rt</p>
    <p>-li st</p>
    <p>ed 20 images 100 images</p>
    <p>short-list size:</p>
    <p>[Lowe 04, Chum &amp; al 2007]</p>
  </div>
  <div class="page">
    <p>Weak geometry consistency</p>
    <p>Weak geometric information used for all images (not only the short-list)</p>
    <p>Each invariant interest region detection has a scale and rotation angle associated, here characteristic scale and dominant gradient orientation</p>
    <p>Scale change 2 Rotation angle ca. 20 degrees</p>
    <p>Each matching pair results in a scale and angle difference</p>
    <p>For the global image scale and rotation changes are roughly consistent</p>
  </div>
  <div class="page">
    <p>Pisa tower: Let analyze the dominent orientation difference of matching descriptors</p>
  </div>
  <div class="page">
    <p>Max = rotation angle between images</p>
    <p>Orientation consistency</p>
    <p>FILTERED!</p>
  </div>
  <div class="page">
    <p>PEAK</p>
    <p>FILTERED!</p>
  </div>
  <div class="page">
    <p>WGC: scale consistency</p>
    <p>PEAK</p>
    <p>FILTERED!</p>
  </div>
  <div class="page">
    <p>WGC: scale consistency</p>
    <p>FILTERED!</p>
    <p>PEAK</p>
  </div>
  <div class="page">
    <p>Weak geometry consistency</p>
    <p>Integrate the geometric verification into the BOF representation  votes for an image projected onto two quantized subspaces, that is vote for</p>
    <p>an image at a given angle &amp; scale</p>
    <p>these subspace are shown to be independent</p>
    <p>a score sj for all quantized angle and scale differences for each image</p>
    <p>final score: filtering for each parameter (angle and scale) and min selection</p>
    <p>Only matches that do agree with the main difference of orientation and scale will be taken into account in the final score</p>
    <p>Re-ranking using full geometric transformation still adds information in a final stage</p>
  </div>
  <div class="page">
    <p>Integrating geometric a priori information</p>
    <p>Images orientation difference is strongly non uniform  natural orientation for many images (but still /2 rotation ambiguity)</p>
    <p>human tendency to use the same orientation for the same place</p>
    <p>non matching images</p>
    <p>matching images</p>
    <p>ra te</p>
    <p>o f</p>
    <p>m at</p>
    <p>ch es</p>
    <p>pi/2</p>
    <p>w ei</p>
    <p>gh ti</p>
    <p>n g</p>
    <p>quantized angle difference</p>
    <p>prior: pi/2 rotation</p>
    <p>PRIOR</p>
  </div>
  <div class="page">
    <p>Experimental results</p>
    <p>Evaluation for the INRIA holidays dataset, 1491 images  500 query images + 991 annotated true positives</p>
    <p>Most images are holiday photos of friends and family</p>
    <p>1 million distractor images from Flickr</p>
    <p>Dataset size 1.001.491 images</p>
    <p>Vocabulary construction on a different Flickr set</p>
    <p>Almost real-time search speed, see retrieval demo</p>
    <p>Evaluation metric: mean average precision (in [0,1], bigger = better)</p>
    <p>Average over precision/recall curve</p>
  </div>
  <div class="page">
    <p>Holidays dataset  example queries (out of 500)</p>
  </div>
  <div class="page">
    <p>Example query  response : Venice Channel</p>
    <p>Query</p>
    <p>Base 4Base 3</p>
    <p>Base 2Base 1</p>
  </div>
  <div class="page">
    <p>Example query  response : San Marco square</p>
    <p>Query Base 1 Base 3Base 2</p>
    <p>Base 9Base 8</p>
    <p>Base 4 Base 5 Base 7Base 6</p>
  </div>
  <div class="page">
    <p>Results  Venice Channel</p>
    <p>Base 1 Flickr</p>
    <p>Flickr Base 4</p>
    <p>Base 3</p>
    <p>Query</p>
  </div>
  <div class="page">
    <p>BOF 2 Ours 1</p>
    <p>BOF 43064 Ours 5</p>
    <p>Query</p>
    <p>BOF 5890 Ours 4</p>
  </div>
  <div class="page">
    <p>Results  San Marco</p>
    <p>Query</p>
    <p>Base 01 Base 03</p>
    <p>Base 02 Base 06</p>
    <p>Flickr Flickr</p>
  </div>
  <div class="page">
    <p>Comparison with state-of-the-art</p>
    <p>Evaluation on our holidays dataset, 500 query images, 1 million images in total</p>
    <p>Metric: mean average precision (in [0,1], bigger = better)</p>
    <p>Average query time (4 CPU cores)</p>
    <p>m A</p>
    <p>P</p>
    <p>database size</p>
    <p>baseline WGC</p>
    <p>HE WGC+HE</p>
    <p>+re-ranking</p>
  </div>
  <div class="page">
    <p>Trecvid video copy detection task: evaluation results</p>
    <p>NDCR measure: the lower the best (0 = perfect)</p>
    <p>See our excellent results for all types of transformations below  circles: our result  squares: best results  dashed: medians of all runs (22 participants)</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>HE: state-of-the-art representation of local descriptors</p>
    <p>WGC: use partial geometry information for billions descriptors</p>
    <p>See Internet demo (from http://lear.inrialpes.fr)</p>
    <p>Trecvid copyright detection task: we obtained excellent results</p>
    <p>DEMO AND QUESTIONS</p>
  </div>
</Presentation>
