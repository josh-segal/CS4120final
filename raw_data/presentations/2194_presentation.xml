<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>OPEN ADDRESSABLE DEVICE TIERS Andy Kowles, Seagate Design Center, Longmont, Colorado | July 2017 | HotStorage</p>
    <p>andrew.kowles@seagate.com</p>
  </div>
  <div class="page">
    <p>WHY? The lack of determinism and the opaque nature of the existing Drive Managed Shingled Magnetic Recording designs have proven fatal for broad acceptance of shingled disks in enterprise storage systems. The Skylight paper (FAST 15) is recommended reading.</p>
    <p>Stop doing things behind our back</p>
    <p>In response to this, Zone Block Device schema such as Host Managed have emerged. Such highly restrictive access models improve the drive latency responses and predictability, versus Drive Managed.</p>
    <p>A complementary, wild and crazy way to address HDDs is offered here: Using LBA addressing tricks and explicit policies, allow storage software developers to access internal device tiers which have been, until now, hidden and autonomous. No T10/T13 changes are strictly required.</p>
  </div>
  <div class="page">
    <p>WHAT IS IT? PART 1: SHADOW ADDRESSING</p>
    <p>This addressing scheme is but one option for direct addressing of device tiers.</p>
  </div>
  <div class="page">
    <p>WHAT? POLICIES A tier on a device defined similarly as one in a system: It is a storage area of a particular size, cost, performance, availability, and reliability.</p>
    <p>Policies for OATS are undiscovered country. What policies would you prefer to see? Here are some possibilities</p>
    <p>Latency/Performance: Cleaning from the tier NEVER occurs. Strict LRU replacement policies inside the tier.</p>
    <p>Latency/Performance: Cleaning from the tier NEVER occurs. Once Shadow address space or tier is full, Shadow Addressing has no effect.</p>
    <p>Reliability: Data is Duplicated when directed to Shadow.</p>
    <p>ACID write transactions without Flush Cache (fsync) or Disable Write Cache, or FUA, are performed on data directed to Shadow addresses.</p>
    <p>With multiple tiers, different data can be directed to tiers with the appropriate reliability and performance properties.</p>
    <p>f</p>
  </div>
  <div class="page">
    <p>EXAMPLE: BATCHED METADATA UPDATES</p>
    <p>Analogous to the way ext4-lazy (FAST 17) improved SMR disk performance by using a large journal and avoiding excessive inplace inode updates.</p>
    <p>Could this reduce the required size of NAND layers above the HDD in a data center?</p>
  </div>
  <div class="page">
    <p>System Rollback: A Trim/Unmap command using the Shadow addresses, followed by a read, would return Version (N-1) across the device.</p>
  </div>
  <div class="page">
    <p>QUESTIONS AND FURTHER STUDY  What are the most useful use cases to storage software developers?  How many upper LBA bits would be used, and for what? (48bits of</p>
    <p>LBA addressing for 4KiB blocks provides for 1125 PB of capacity. At 512B blocks, 140 PB)</p>
    <p>1 bit for Shadow A  70PB  1 bit for Policy X or Alternate Tier B  35PB  1 bit for Duplication/Reliability Settings.</p>
    <p>Can Host Managed or Host Aware Zone Block Devices be improved?  Further Study: Implement an openly addressable device using an</p>
    <p>existing DM-SMR or SSHD device. Incorporate Shadow Policies derived from use cases into the device. Effect changes to the block stack to harness the Openly Addressable Tier for the desired use case(s). Test and measure and report.</p>
    <p>Very interested to hear you ideas, in person or otherwise. Seagate Global Firmware andrew.kowles@seagate.com | 720.684.8469</p>
  </div>
</Presentation>
