<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Gemini: A Computation-Centric Distributed Graph Processing System</p>
    <p>Xiaowei Zhu Wenguang Chen Weimin Zheng Xiaosong Ma Tsinghua University Qatar Computing Research Institute</p>
  </div>
  <div class="page">
    <p>Graphs, Platforms, and Systems</p>
    <p>Shared-memory Ligra[PPoPP13], Galois[SOSP 13] Efficiency Scalability</p>
    <p>Distributed PowerGraph[OSDI 12], GraphX[OSDI 14]</p>
    <p>Efficiency Scalability</p>
  </div>
  <div class="page">
    <p>Far from saturated for IB EDR (100Gbps)</p>
    <p>(38.1*8/2/26.9/8=0.708Gbps)</p>
    <p>Connecting Shared-Memory and Distributed</p>
    <p>Nodes System</p>
    <p>Runtime (s) 19.3 26.9</p>
    <p>Instructions 482G 6.06T</p>
    <p>Memory references 23.4G 87.2G</p>
    <p>Communication (GB) - 38.1</p>
    <p>Instructions per cycle 0.414 0.655</p>
    <p>L3 cache miss rate 49.7% 54.9%</p>
    <p>CPU utilization 96.8% 68.4%</p>
    <p>common design Bottleneck in computation!</p>
    <p>More instructions and memory traffic</p>
    <p>Poorer access locality</p>
    <p>Lower multi-core utilization</p>
    <p>Profiling existing systems</p>
    <p>Distributed overhead</p>
    <p>Sub-optimal computation</p>
  </div>
  <div class="page">
    <p>We Propose: Gemini</p>
    <p>Build scalability on top of efficiency  Avoid unnecessary distributed side-effects  Optimize computation on partitioned sub-graphs</p>
    <p>Shift of design focus  Designed for distributed, but computation-centric</p>
    <p>Modern clusters have fast interconnects  Computation-communication overlap in place</p>
    <p>Major optimizations  Efficiency</p>
    <p>Adaptive push-/pull-style computation  Hierarchical chunk-based partitioning</p>
    <p>Scalability  Locality-aware chunking  Chunk-based work-stealing</p>
  </div>
  <div class="page">
    <p>Dual Mode: BFS Example (1)</p>
    <p>Active edge set</p>
    <p>|Active edge set| / |E| &lt; threshold  Sparse mode</p>
    <p>Push operations</p>
    <p>Active vertex set</p>
    <p>Selective scheduling: only access out-edges from active vertices</p>
    <p>Locks/atomic operations required for correctness of concurrent updates</p>
  </div>
  <div class="page">
    <p>Dual Mode: BFS Example (2)</p>
    <p>Active edge set</p>
    <p>|Active edge set| / |E| &gt; threshold  Dense mode  Pull operations</p>
    <p>Vertices pulling along in-edges Contention-free updating</p>
    <p>Useless computation</p>
  </div>
  <div class="page">
    <p>Push vs. Pull: Performance Impact</p>
    <p>Single-Source2Shortest2Paths</p>
    <p>Sparse Dense</p>
  </div>
  <div class="page">
    <p>Distributed Dual-Mode Computation</p>
    <p>Node0</p>
    <p>Node1</p>
  </div>
  <div class="page">
    <p>Node1</p>
    <p>Node0</p>
    <p>When Distributed to 2 Nodes</p>
    <p>Master</p>
    <p>Mirror</p>
    <p>Inter-node message passing</p>
  </div>
  <div class="page">
    <p>Geminis Distributed Push</p>
    <p>Node0</p>
    <p>Node1</p>
    <p>Masters message mirrors, who update their local neighbors</p>
  </div>
  <div class="page">
    <p>Geminis Distributed Pull</p>
    <p>Node0</p>
    <p>Node1</p>
    <p>Mirrors pull updates from neighbors, then message masters</p>
  </div>
  <div class="page">
    <p>Chunking  Divide vertex set into contiguous chunks</p>
    <p>Dual-mode edge data distributed accordingly</p>
    <p>Geminis Choice of Graph Partitioning</p>
    <p>Node0</p>
    <p>Node1</p>
    <p>Node0 Node1</p>
  </div>
  <div class="page">
    <p>Why Chunk-Based Partitioning?</p>
    <p>It preserves locality!  Fact: locality exists in many real-world graphs</p>
    <p>Vertices semantically ordered</p>
    <p>Preprocessing affordable when vertices unordered  E.g., BFS[Algorithms 09], LLP[WWW 11]</p>
    <p>FacebookCountry Adjacency Matrix1</p>
    <p>UK Web (2005) Adjacency Matrix</p>
  </div>
  <div class="page">
    <p>More Benefits of Chunking</p>
    <p>Low-overhead distributed designs</p>
    <p>Applied recursively at different levels  More on this later</p>
    <p>Global VID  Local VIDGlobal VID  Local VID</p>
  </div>
  <div class="page">
    <p>Challenges with Distributed Chunking</p>
    <p>When scaling out  CSR/CSC vertex indices can become very sparse  Solution: compress vertex indices</p>
    <p>Modern servers built upon NUMA architecture  Interleaved layout sub-optimal  Solution: apply inter-socket sub-partitioning</p>
    <p>Chunking as vertex-centric (edge-cut) scheme  Vertex-centric solutions are not good at load balancing natural graphs  Solution: balancing workload in locality-aware manner</p>
  </div>
  <div class="page">
    <p>Locality-Aware Chunking</p>
    <p>Gemini considers both vertex and edge  Edge: the amount of work to be processed  Vertex: the processing speed of work (locality)  Hybrid metric: |Vi| + |Ei|</p>
    <p>Chunk size affects random access efficiency!</p>
    <p>Balancing by edges?</p>
  </div>
  <div class="page">
    <p>node</p>
    <p>socket</p>
    <p>core</p>
    <p>Data partitioning till socket-level</p>
    <p>Work partitioning within socket</p>
    <p>cluster</p>
    <p>Per-node partition</p>
    <p>Per-socket partition w. NUMA-aware placement</p>
    <p>Per-core work chunk</p>
    <p>Mini-chunk for work-stealing</p>
    <p>Locality-aware</p>
    <p>Fine grain (64-vertex)</p>
    <p>Chunking, Chunking All the Way</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Platform: 8-node cluster  Intel Xeon E5-2670 v3 (12-core CPU), 30MB L3 cache  2 sockets sharing 128 GB RAM (DDR4 2133MHz)  Network: Mellanox Infiniband EDR 100Gbps</p>
    <p>Applications  PageRank (PR) (20 iterations)  Connected Components (CC)  Single-Source Shortest Paths (SSSP)  Breadth-First Search (BFS)  Betweenness Centrality (BC)</p>
    <p>Graph |V| |E| enwiki-2013 4,206,785 101,355,853 twitter-2010 41,652,330 1,468,365,182 uk-2007-05 105,896,555 3,738,733,648 weibo-2013 72,393,453 6,431,150,494 clueweb-12 978,048,098 42,574,107,469</p>
    <p>Input graphs</p>
  </div>
  <div class="page">
    <p>Single-Node Efficiency</p>
    <p>* uses different algorithms.</p>
    <p>Application Ligra Galois Gemini</p>
    <p>PR 21.2 19.3 12.7</p>
    <p>CC 6.51 3.59* 4.93</p>
    <p>SSSP 2.81 3.33 3.29</p>
    <p>BFS 0.347 0.528 0.468</p>
    <p>BC 2.45 3.94* 1.88 Runtime in seconds (twitter-2010)</p>
    <p>NUMA-aware memory accesses</p>
    <p>More iterations</p>
    <p>More instructions</p>
    <p>System Ligra Gemini Remote access ratio 50.1% 9.10% L3 cache miss rate 52.6% 40.1%</p>
    <p>Averageaccess latency 183ns 125ns Memory performance (BC)</p>
  </div>
  <div class="page">
    <p>Multi-Node Scalability: Larger Graphs</p>
    <p>Application PowerLyra Gemini PR</p>
    <p>Out of memory</p>
    <p>Graph |V| |E| enwiki-2013 4,206,785 101,355,853 twitter-2010 41,652,330 1,468,365,182 uk-2007-05 105,896,555 3,738,733,648 weibo-2013 72,393,453 6,431,150,494 clueweb-12 978,048,098 42,574,107,469</p>
    <p>Runtime in seconds (clueweb-12)</p>
  </div>
  <div class="page">
    <p>Multi-Node Scalability: Faster Speeds</p>
    <p>uk-2007-05 weibo-2013</p>
    <p>N or m al iz ed R un</p>
    <p>ti m e</p>
    <p>Nodes</p>
    <p>PageRank</p>
    <p>PowerLyra Gemini</p>
    <p>Connected-Components</p>
    <p>PowerLyra Gemini</p>
    <p>PageRank</p>
    <p>PowerLyra Gemini</p>
    <p>Connected-Components</p>
    <p>PowerLyra Gemini</p>
  </div>
  <div class="page">
    <p>Closing Remarks</p>
    <p>What have we learned  Computation efficiency highlighted by fast network  Existing guidelines may not apply</p>
    <p>Chunking works!  Multi-fold benefits  Enables series of optimizations</p>
    <p>Gemini Search GeminiGraph on Github</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
    <p>Q &amp; A</p>
  </div>
</Presentation>
