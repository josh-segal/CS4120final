<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Tag line, tag line</p>
    <p>Alexandros Batsakis NetApp* Randal Burns Johns Hopkins University Arkady Kanevsky NetApp James Lentini NetApp Thomas Talpey NetApp</p>
    <p>A Congestion-Aware Network File System</p>
  </div>
  <div class="page">
    <p>What Is This Talk About ?</p>
    <p>a framework for scheduling client operations in a distributed file system based on servers congestion</p>
  </div>
  <div class="page">
    <p>What Is Wrong Now ? Selfishness</p>
    <p>clients try to maximize their own throughput  send requests to the server greedily  each request incurs a cost to the system</p>
    <p>network, memory, disk  do not care about social impact (externalities)</p>
    <p>C1</p>
    <p>C2</p>
    <p>S</p>
    <p>clients really bound only by flow rate (their benefit)</p>
  </div>
  <div class="page">
    <p>Clients Have (Good) Excuses</p>
    <p>server takes all responsibility (system-design)  clients are</p>
    <p>oblivious to server load  oblivious to other client population</p>
    <p>our objective is to teach clients to behave better  to care about the social impact of their actions  to become congestion-aware !</p>
    <p>implementation: CA-NFS: Congestion-aware Network File System</p>
  </div>
  <div class="page">
    <p>CA-NFS Building Blocks</p>
    <p>monitor system usage and quantify congestion  schedule client operations</p>
  </div>
  <div class="page">
    <p>Assessing System Load / Congestion</p>
    <p>how can one measure congestion ?  throughput, latency, time, cpu%, ... ???  black box, grey box, ... ???</p>
    <p>how can one compare load across  heterogeneous workloads ?  heterogeneous devices ?</p>
    <p>80% CPU usage vs 100 pending disk I/Os ?  heterogeneous machines ?</p>
  </div>
  <div class="page">
    <p>Congestion Pricing</p>
    <p>unify congestion under a single metric based on B. Awerbuch, Y. Azar, and S. Plotkin, Throughput-competitive online routing , FOCS 93</p>
    <p>congestion price = exp function of the resource utilization  we adapt it to fit storage systems [auction model - proof in the paper]</p>
    <p>ui utilization of resource i  Pi, Pmax price of resource i, max price  ki degradation factor as the load-increases</p>
    <p>device-specific e.g. disk vs network</p>
    <p>tions that uses an online auction model to price congestion in a resource independent way. We adapt this theory to distributed file systems by considering the path of file system operations, from the clients memory to servers disk, as a short-lived circuit.</p>
    <p>CA-NFS uses a reverse auction model. In a reverse auction, the buyer advertises a need for a service and the sellers place bids, like a regular auction. However, the seller who places the lowest bid wins the auction. Accordingly in CA-NFS, when the client is about to issue a request, it compares its local price with the server price. Depending on who offers the lower price the client accelerates, or defers the operation.</p>
    <p>We start by describing an auction for a single resource. We then build a pricing function for each resource and assemble these functions into a price for each NFS operation.</p>
    <p>For each resource, we define a simple auction in an online setting in which the bids arrive sequentially and unpredictably. In a way, a bid represents the clients willingness to pay for the use of the resource, i.e. the clients local price. A bid will be accepted immediately if it is higher than the price of the resource at that time.</p>
    <p>Our goal is to find an online algorithm that is competitive to the optimal offline algorithm in any future request sequence. The performance degradation of an online algorithm (competitive ratio) is</p>
    <p>offline online in which offline is the benefit from the offline optimal algorithm and online the benefit from the online algorithm. Awerbuch et al. [4] establish the lower bound at in which is the ratio between the maximum and minimum benefit realized by the online algorithm over all inputs. The lower bound is achieved when reserving of the resource doubles the price.</p>
    <p>The worst case occurs when the offline algorithm sells the entire resource at the maximum bid , which was rejected by the online algorithm. For the online algorithm to reject this bid, it must have set the price greater than , which means it has already sold of the resource for at least .</p>
    <p>online and</p>
    <p>offline online</p>
    <p>Increasing price exponentially with increased utilization leads to a competitive ratio logarithmic in .</p>
    <p>ably competitive with the optimal offline algorithm in the maximum usage of each resource. It has a weak ( , not constant) competitive ratio, but even this weak ratio is unprecedented in the storage systems literature. The online algorithm knows nothing about the future, assumes no correlation between past and future requests, and is only aware of the current system state.</p>
    <p>Based on the theoretical framework, we define the pricing function for an individual resource in our framework as</p>
    <p>in which the utilization varies between and so that the price varies between and .</p>
    <p>The parameter represents the performance degradation experienced by the end user as the resource becomes congested. Thus, appropriate values of should provide incremental feedback as the resource usage increases.</p>
    <p>The heterogeneous resources of distributed file systems complicate parameter selection. Different resources become congested at different levels of utilization, which dictates that parameters need to be set individually. With very large , the price function stays near zero until the utilization is almost 1. Then the price goes up very quickly. With very small , the resource becomes expensive at lower utilization, which throttles usage prior to congestion. The network exhibits few negative effects from increased utilization until near its capacity and, thus, calls for a higher setting of . Similarly, memory works well until its nearly full at which point it experiences congestion in the form of fragmentation and synchronous stalls from out-of-memory conditions. Disks, on the other hand, require smaller values of , because each additional I/O interferes with all subsequent (and some previous) I/Os, increasing the service time by increasing queue lengths and potentially moving the head out of position.</p>
    <p>CA-NFS users do not need to set the value of explicitly, as it is precomputed for most existing device types. The pricing mechanism is robust to small hardware variations, e.g to different device brands. During various CANFS deployments, we experimented extensively with the value of . (We do not present all these experiments as they are quite tedious.)</p>
    <p>We approximate the cumulative cost of all resources by the highest cost (most congested) resource. The highest cost resource corresponds well with the system bottleneck. is the same for all server resources and the exponential nature of the pricing functions ensures that resources under load become expensive quickly.</p>
  </div>
  <div class="page">
    <p>Resource Monitoring</p>
    <p>the theory makes no assumptions about the devices that are monitored  an expression of the utilization</p>
    <p>real devices:  network, CPU, memory, disk</p>
    <p>virtual devices (heuristics):  read-ahead effectiveness  cache effectiveness {Batsakis et al Awol at FAST 08}</p>
    <p>can be extended to any device  SSDs, Infiniband,</p>
  </div>
  <div class="page">
    <p>Operation Scheduling</p>
    <p>NFS servers operate under false assumptions  client benefit increases with server throughput  all client operations are equally important</p>
    <p>client operations come at different priorities  explicitly: low-priority processes</p>
    <p>out-of-protocol handling (QoS etc.)  see... future work</p>
    <p>implicitly: synchronous vs asynchronous ops</p>
  </div>
  <div class="page">
    <p>Client Operations &amp; Implicit Priorities</p>
    <p>synchronous versus asynchronous ops  synchronous operations:</p>
    <p>reads, metadata  must be performed on-demand (applications block)</p>
    <p>asynchronous operations:  write, read-ahead  can be time-shifted depending on the client state</p>
    <p>memory usage, application needs, ...</p>
    <p>our goal is to schedule client ops so that non-time critical (async) I/O traffic does not interfere with on-demand (sync) requests</p>
  </div>
  <div class="page">
    <p>CA-NFS Operation  Reverse Auction</p>
    <p>clients and servers encode their resource constraints by increasing or decreasing their prices</p>
    <p>servers advertise their congestion prices to clients</p>
    <p>clients compare the server prices with their local prices and they decide to:  issue read-aheads prudently or aggressively  defer or accelerate a write</p>
  </div>
  <div class="page">
    <p>Write Acceleration</p>
    <p>CA-NFS clients notify the server to sync the data immediately upon a WRITE</p>
    <p>no client buffering is needed  preserves the cache contents of the client</p>
    <p>(maintain hit rate)  if the server load is low, why sync later ?  saves client memory :)</p>
    <p>no double buffering -- maintains client cache  consumes server resources immediately :-(</p>
  </div>
  <div class="page">
    <p>Write Deferral</p>
    <p>CA-NFS clients keep data in local memory only and do not copy them to the server  if the server load is high postpone the write  saves server memory, disk and network I/O :-)  consumes clients memory :-(  faces the risk of higher latency for subsequent</p>
    <p>COMMIT operations upon close  but they would be slow anyways (high load)  some heuristics to throttle small write deferral</p>
  </div>
  <div class="page">
    <p>CA-NFS in Practice</p>
    <p>Client 1 Client 2memory: 80%RA eff: 10% hit rate: 40%</p>
    <p>price Writes Reads 85 4</p>
    <p>memory: 10% RA eff: 85% hit rate: 90%</p>
    <p>price Writes Reads 20 17</p>
    <p>Server memory: 65%network: 20% disk: 90%</p>
    <p>hit rate: 40% price Writes Reads</p>
  </div>
  <div class="page">
    <p>CA-NFS in Practice</p>
    <p>Client 1 Client 2memory: 50%RA eff: 10% hit rate: 40%</p>
    <p>price Writes Reads 64 4</p>
    <p>memory: 50% RA eff: 85% hit rate: 90%</p>
    <p>price Writes Reads 60 17</p>
    <p>Server memory: 65%network: 20% disk: 60%</p>
    <p>hit rate: 40% price Writes Reads</p>
    <p>CA-NFS exchanges resource congestion among clients and the server</p>
  </div>
  <div class="page">
    <p>Experimental Analysis</p>
    <p>two different workloads (filebench)  fileserver: 1000s of real NFS traces</p>
    <p>creates, deletes, reads, writes, etc.  many asynchronous operations</p>
    <p>oltp: based on a database I/O model  many small random reads and writes  mostly synchronous operations</p>
  </div>
  <div class="page">
    <p>Fileserver  Results I</p>
  </div>
  <div class="page">
    <p>Fileserver  Results II</p>
  </div>
  <div class="page">
    <p>OLTP - Results</p>
  </div>
  <div class="page">
    <p>To Do (or Not To Do)</p>
    <p>smart scheduling of async operations is just a proof-of-concept  policies &amp; priorities for synchronous operations</p>
    <p>e.g. if price &gt; 0.8 then stop application X  fairness over time</p>
    <p>one client may drive prices up for everybody  resource reservations by differentiating prices  proportional sharing based on salaries  holistic flow control</p>
  </div>
  <div class="page">
    <p>Parting Thoughts</p>
    <p>contribution: a framework to build performance management based on congestion</p>
    <p>case study of an economic anomaly  client benefit does not always increase with</p>
    <p>throughput  client requests come at different priorities</p>
    <p>server cost always increases with load  benefit-based vs cost-based system design</p>
    <p>Thank You Questions?</p>
  </div>
</Presentation>
