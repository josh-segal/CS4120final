<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>A Crowd-Annotated Spanish Corpus for Humor Analysis</p>
    <p>SantiagoCastro, LuisChiruzzo, AialaRos, DiegoGaratandGuillermo Moncecchi July 20th, 2018</p>
    <p>Grupo de Procesamiento de Lenguaje Natural, Universidad de la Repblica  Uruguay</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Background</p>
    <p>Extraction</p>
    <p>Annotation</p>
    <p>Dataset</p>
    <p>Analysis</p>
    <p>Conclusion</p>
    <p>HAHA Task</p>
  </div>
  <div class="page">
    <p>Background</p>
  </div>
  <div class="page">
    <p>Background i</p>
    <p>Humor Detection is about telling if a text is humorous (e.g., a joke).</p>
    <p>My grandpa came to America looking for freedom, but it didnt work out, in the next flight my grandma was coming.</p>
    <p>ITS REALLY HOT</p>
  </div>
  <div class="page">
    <p>Background ii</p>
    <p>Some previous work, such as Barbieri and Saggion (2014), Mihalcea and Strapparava (2005), and Sjbergh and Araki (2007), created binary Humor Classifiers for short texts written in English.</p>
    <p>They extracted one-liners from the Internet and from Twitter, such as:</p>
    <p>Beauty is in the eye of the beer holder.</p>
    <p>Castro et al. (2016) worked on Spanish tweets since our group is interested in leveraging tools for Spanish.</p>
    <p>Back then, we conceived the first and only Spanish dataset to study Humor.</p>
  </div>
  <div class="page">
    <p>Background iii</p>
    <p>Castro et al. (2016) corpus provided 40k tweets from 18 accounts, with 34k annotations. The annotators decided if the tweets were humorous or not, and if so they rated them from 1 to 5.</p>
    <p>However, the dataset has some issues: 1. low inter-annotator agreement (Fleiss  = 0.3654) 2. limited variety of sources (humorous: 9 Twitter accounts, non-humorous: 3 about news accounts, 3 about inspirational thoughts and 3 about curious facts)</p>
  </div>
  <div class="page">
    <p>Background iv</p>
  </div>
  <div class="page">
    <p>Related work</p>
    <p>Potash, Romanov, and Rumshisky (2017) built a corpus based on tweets in English that aims to distinguish the degree of funniness in a given tweet. They used the tweet set issued in response to a TV game show, labeling which tweets were considered humorous by the show. Used in SemEval 2017 Task 6  #HashtagWars.</p>
  </div>
  <div class="page">
    <p>Extraction</p>
  </div>
  <div class="page">
    <p>Extraction i</p>
  </div>
  <div class="page">
    <p>Extraction ii</p>
  </div>
  <div class="page">
    <p>Annotation</p>
  </div>
  <div class="page">
    <p>Annotation i</p>
    <p>We built a web page, similar to the one used by Castro et al. (2016):</p>
  </div>
  <div class="page">
    <p>Annotation ii</p>
    <p>clasificahumor.com</p>
  </div>
  <div class="page">
    <p>Annotation iii</p>
    <p>Tweets were randomly shown to annotators, but avoiding duplicates (by using web cookies).</p>
    <p>We wanted UI to be the more intuitive and self-explanatory as possible, trying not to induce any bias on users and letting them come up with their own definition of humor.</p>
    <p>The simple and friendly interface is meant to keep the users engaged and having fun while classifying tweets.</p>
  </div>
  <div class="page">
    <p>Annotation iv</p>
    <p>People annotated from March 8th to 27th, 2018.</p>
    <p>The first tweets shown to every session were the same: 3 tweets for which we know a clear answer.</p>
    <p>During the annotation process, we added around 4,500 tweets coming from humorous accounts to help the balance.</p>
  </div>
  <div class="page">
    <p>Dataset</p>
  </div>
  <div class="page">
    <p>Dataset i</p>
    <p>The dataset consists of two CSV files: tweets and annotations.</p>
    <p>tweet ID origin</p>
    <p>tweet ID session ID date value</p>
  </div>
  <div class="page">
    <p>Dataset ii</p>
    <p>27,282 tweets</p>
    <p>117,800 annotations (including 2,959 skips)</p>
    <p>107,634 high quality annotations (excluding skips)</p>
  </div>
  <div class="page">
    <p>Analysis</p>
  </div>
  <div class="page">
    <p>Annotation Distribution</p>
    <p>Number of annotations</p>
    <p>Tw ee ts</p>
  </div>
  <div class="page">
    <p>Class Distribution</p>
    <p>Excellent Good Regular Little Funny Not Funny Not Humorous</p>
  </div>
  <div class="page">
    <p>Annotators Distribution</p>
    <p>Annotators</p>
    <p>An no ta tio ns</p>
  </div>
  <div class="page">
    <p>Agreement</p>
    <p>Krippendorffs  = 0.5710 (vs. 0.3654)  If we include the low quality,  = 0.5512  Funniness:  = 0.1625  If we only consider the 11 annotators who tagged more than a 1,000 times (who tagged 50,939 times in total), the humor and funniness agreement are respectively 0.6345 and 0.2635.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We created a better version of a dataset to study Humor in Spanish. 27,282 tweets coming from multiple sources, with 107,634 annotations high quality annotations.</p>
    <p>Significant inter-annotator agreement value.  It is also a first step to study subjectivity. Although more annotations per tweet would be appropriate, there is a subset of a thousand tweets with at least six annotations that could be used to study peoples opinion on the same instances.</p>
  </div>
  <div class="page">
    <p>HAHA Task</p>
  </div>
  <div class="page">
    <p>HAHA Task</p>
    <p>An IberEval 2018 task.  Two subtasks: Humor Classification and Funniness Average Prediction.</p>
    <p>Subset of 20k tweets.  3 participants,  7 and 2 submissions respectively.</p>
  </div>
  <div class="page">
    <p>Analysis</p>
    <p>Category Votes Hits</p>
    <p>Humorous 3/5 52.25% 4/5 75.33% 5/5 85.04%</p>
    <p>Not humorous 3/5 68.54% 4/5 80.83% 5/5 82.42%</p>
  </div>
  <div class="page">
    <p>References i</p>
    <p>References</p>
    <p>Barbieri, Francesco and Horacio Saggion (2014). Automatic Detection of Irony and Humour in Twitter. In: ICCC, pp. 155162.</p>
    <p>Castro, Santiago et al. (2016). Is This a Joke? Detecting Humor in Spanish Tweets. In: Ibero-American Conference on Artificial Intelligence. Springer, pp. 139150. doi: 10.1007/978-3-319-47955-2_12.</p>
  </div>
  <div class="page">
    <p>References ii</p>
    <p>Fleiss, Joseph L (1971). Measuring nominal scale agreement among many raters. In: Psychological bulletin 76.5, p. 378. doi: 10.1037/h0031619.</p>
    <p>Krippendorff, Klaus (2012). Content analysis: An introduction to its methodology. Sage. doi: 10.1111/j.1468-4446.2007.00153_10.x.</p>
    <p>Mihalcea, Rada and Carlo Strapparava (2005). Making Computers Laugh: Investigations in Automatic Humor Recognition. In: Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. HLT 05. Vancouver, British Columbia, Canada: Association for Computational Linguistics, pp. 531538. doi: 10.3115/1220575.1220642.</p>
  </div>
  <div class="page">
    <p>References iii</p>
    <p>Potash, Peter, Alexey Romanov, and Anna Rumshisky (2017). SemEval-2017 Task 6:# HashtagWars: Learning a sense of humor. In: Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 4957. doi: 10.18653/v1/s17-2004.</p>
    <p>Sjbergh, Jonas and Kenji Araki (2007). Recognizing Humor Without Recognizing Meaning. In: WILF. Ed. by Francesco Masulli, Sushmita Mitra, and Gabriella Pasi. Vol. 4578. Lecture Notes in Computer Science. Springer, pp. 469476. isbn: 978-3-540-73399-7. doi: 10.1007/978-3-540-73400-0_59.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>https://pln-fing-udelar.github.io/humor/</p>
  </div>
</Presentation>
