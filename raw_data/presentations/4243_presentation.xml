<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>IBM Research AI</p>
    <p>A Co-Matching Model for Multi-choice Reading</p>
    <p>Comprehension Shuohang Wang1, Mo Yu2, Shiyu Chang2, Jing Jiang1</p>
  </div>
  <div class="page">
    <p>Reading Comprehension</p>
    <p>The task: to answer questions given a passage of text  Datasets</p>
    <p>CNN/Daily Mail [Hermann et al. 2015]  Childrens Book Test [Hill et al. 2016]</p>
    <p>SQuAD [Rajpurkar 2016]</p>
    <p>MCTest [Richardson et al. 2013]  RACE [Lai et al. 2017]</p>
    <p>NarrativeQA [Kocisky et al. 2018]</p>
    <p>Cloze Style</p>
    <p>Answer Span Extraction</p>
    <p>Multi-choice Questions</p>
  </div>
  <div class="page">
    <p>RACE  Passage: My father wasnt a king, he was a taxi driver, but</p>
    <p>I met Blandy at a party and he asked if Id like to buy the island. Of course I said yes but I had no money-I was just an art teacher. I tried to find some business partners, who all thought I was crazy. So I sold some of my possessions, put my savings together and bought it ...</p>
    <p>Question: How did the author get the island?  a. It was a present from Blandy.  b. The king sold it to him.  c. He bought it from Blandy.  d. He inherited from his father.</p>
    <p>Challenge: to jointly model passage,</p>
    <p>question and candidate answers</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Converted to sequence pair matching [Yin et al., 2016]  Each candidate answer is concatenated with the question  The concatenated sequences are matched against the passage</p>
    <p>Passage</p>
    <p>a.</p>
    <p>b.</p>
    <p>c.</p>
    <p>d.</p>
    <p>ranking scores</p>
    <p>matching Limitation:</p>
    <p>Question and answers are not clearly</p>
    <p>separated. Interaction information between a question and an answer</p>
    <p>is lost.</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Matching sequences pair by pair [Lai et al., 2017]  Match passage and question first  Then this representation is used to match candidate answers</p>
    <p>Passage</p>
    <p>a.</p>
    <p>b.</p>
    <p>c.</p>
    <p>d.</p>
    <p>matching</p>
    <p>Question</p>
    <p>Q-specific passage representation</p>
    <p>matching</p>
    <p>ranking scores</p>
    <p>Limitation: Matching P &amp; Q may not</p>
    <p>give meaningful representations for</p>
    <p>questions like Which statement of the following</p>
    <p>is true?07/18/2018 5</p>
  </div>
  <div class="page">
    <p>Our Solution  Co-match each sentence in the passage with the question and</p>
    <p>the candidates answers separately.  Make use of the alignments between sequences as follows:</p>
    <p>Passage</p>
    <p>Candidate Answer: He bought it from Blandy</p>
    <p>Question: How did the author get the island?</p>
    <p>Hierarchically aggregate the co-matching representations of (sentence, question, answer) triplets for final scoring.</p>
  </div>
  <div class="page">
    <p>Co-Matching  For every word in sentence, we match it with the attention-weighted vectors</p>
    <p>computed based on the question and the candidate answer, respectively.</p>
    <p>Passage</p>
    <p>Candidate Answer: He bought it from Blandy</p>
    <p>Question: How did the author get the island?</p>
    <p>bought</p>
    <p>get</p>
    <p>Co-Matching state</p>
    <p>+</p>
    <p>+</p>
    <p>Attention mechanism</p>
  </div>
  <div class="page">
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>Co-Matching states</p>
    <p>1 Embedding</p>
    <p>LSTM states</p>
    <p>Attention weights</p>
    <p>Attentionweighted vectors</p>
    <p>Matching states</p>
    <p>Question Candidate answer</p>
    <p>Framework</p>
  </div>
  <div class="page">
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>1</p>
    <p>h</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>2</p>
    <p>h1 h 2 h</p>
    <p>Nth Sentence of Passage</p>
    <p>Co-Matching states</p>
    <p>Hierarchical LSTM</p>
    <p>1   2    Embedding</p>
    <p>LSTM states</p>
    <p>Question Candidate answer</p>
    <p>Framework Representation for ranking candidates</p>
    <p>Attention weights</p>
    <p>Attentionweighted vectors</p>
    <p>Matching states</p>
  </div>
  <div class="page">
    <p>Experiments</p>
    <p>We studied two key factors: (1) the co-matching module (2) the hierarchical aggregation</p>
    <p>approach</p>
    <p>Our Hier-Co-Matching achieved the best performance compared with previous work.</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>We proposed a hierarchical co-matching model for answering multi-choice reading comprehension questions.  We showed that our model could achieve state-of-the-art</p>
    <p>performance on the RACE dataset.  There is still much room for improvement on RACE given the</p>
    <p>low absolute performance.  Latest results by OpenAI: 59%</p>
    <p>Paper: Code:</p>
  </div>
  <div class="page">
    <p>References</p>
    <p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale reading comprehension dataset from examinations. In Proceedings of EMNLP.  Wenpeng Yin, Sebastian Ebert, and Hinrich Schutze. 2016. Attention-based</p>
    <p>convolutional neural network for machine comprehension. In Proceedings of NAACL.  Soham Parikh, Ananya B. Sai, Preksha Nema, Mitesh M. Khapra. 2018.</p>
    <p>ElimiNet: A Model for Eliminating Options for Reading Comprehension with Multiple Choice Questions. In Proceedings of IJCAI.  Haichao Zhou, Wei Furu, Qin Bing, and Liu Ting. 2018. Hierarchical attention</p>
    <p>flow for multiple-choice reading comprehension. In Proceedings of AAAI.  Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen, and Xiaodong Liu. 2017.</p>
    <p>Towards human-level machine reading comprehension: Reasoning and inference with multiple strategies. 2018. arXiv.</p>
  </div>
</Presentation>
