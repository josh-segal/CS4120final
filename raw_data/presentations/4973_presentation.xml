<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Hybrid Parallel Programming with MPI and Unified Parallel C</p>
    <p>James Dinan*, Pavan Balaji, Ewing Lusk, P. Sadayappan*, Rajeev Thakur</p>
    <p>*The Ohio State University Argonne National Laboratory</p>
  </div>
  <div class="page">
    <p>MPI Parallel Programming Model</p>
    <p>SPMD exec. model</p>
    <p>Two-sided messaging</p>
    <p>#include &lt;mpi.h&gt;</p>
    <p>int main(int argc, char **argv) { int me, nproc, b, i; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(WORLD,&amp;me); MPI_Comm_size(WORLD,&amp;nproc);</p>
    <p>// Divide work based on rank  b = mxiter/nproc; for (i = me*b; i &lt; (me+1)*b; i++)</p>
    <p>work(i);</p>
    <p>// Gather and print results</p>
    <p>MPI_Finalize(); return 0;</p>
    <p>}</p>
    <p>Send(buf, N)</p>
    <p>Recv(buf, 1)</p>
  </div>
  <div class="page">
    <p>Unified Parallel C</p>
    <p>Unified Parallel C (UPC) is:  Parallel extension of ANSI C  Adds PGAS memory model  Convenient, shared memory</p>
    <p>-like programming  Programmer controls/exploits</p>
    <p>locality, one-sided communication</p>
    <p>Tunable approach to performance  High level: Sequential C  Shared memory  Medium level: Locality, data distribution, consistency  Low level: Explicit one-sided communication</p>
    <p>X[M][M][N]</p>
    <p>X[1..9] [1..9][1..9]X</p>
    <p>get()</p>
  </div>
  <div class="page">
    <p>Why go Hybrid?</p>
    <p>UPC asynchronous global address space  Aggregates memory of multiple nodes  Operate on large data sets  More space than OpenMP (limited to one node)</p>
    <p>Use multiple global address spaces for replication  Groups apply to static arrays</p>
    <p>Most convenient to use</p>
  </div>
  <div class="page">
    <p>Why not use MPI-2 One-Sided?</p>
    <p>MPI-2 provides one-sided messaging</p>
    <p>Not quite the same as a global address space</p>
    <p>Does not assume coherence: Extremely portable</p>
    <p>No access to performance/programmability gains on machines with coherent memory subsystem</p>
    <p>Accesses must be locked using coarse grain window locks  Can only access a location once per epoch if written to  No overlapping local/remote accesses  No pointers, window objects cannot be shared,</p>
    <p>UPC provides fine-grain asynchronous global addr. space</p>
    <p>Makes some assumptions about memory subsystem  Assumptions fit most HPC systems</p>
  </div>
  <div class="page">
    <p>Hybrid MPI+UPC Programming Model</p>
    <p>Many possible ways to combine MPI</p>
    <p>Focus on:</p>
    <p>Flat: One global address space</p>
    <p>Nested: Multiple global address spaces (UPC groups)</p>
    <p>Hybrid MPI+UPC Process UPC Process</p>
    <p>Flat Nested-Funneled Nested-Multiple</p>
  </div>
  <div class="page">
    <p>Flat Hybrid Model</p>
    <p>UPC Threads  MPI Ranks 1:1  Every process can use MPI and UPC</p>
    <p>Benefit:  Add one large global address space to MPI codes</p>
    <p>Allow UPC programs to use MPI libs: ScaLAPACK, etc</p>
    <p>Some support from Berkeley UPC for this model  upcc -uses-mpi tells BUPC to be compatible with MPI</p>
  </div>
  <div class="page">
    <p>Nested Hybrid Model</p>
    <p>Launch multiple UPC spaces connected by MPI</p>
    <p>Static UPC groups  Applied to static distributed</p>
    <p>shared arrays (e.g. shared [bsize] double x[N][N][N];)</p>
    <p>Proposed UPC groups cant be applied to static arrays</p>
    <p>Group size (THREADS) determined at launch or compile time</p>
    <p>Useful for:  Improving performance of locality constrained UPC codes</p>
    <p>Increasing the storage for MPI groups (multilevel parallelism)</p>
    <p>Nested: Only thread 0 performs MPI communication</p>
    <p>Multiple: All threads perform MPI communication</p>
  </div>
  <div class="page">
    <p>Experimental Evaluation</p>
    <p>Software setup:  GCCUPC compiler</p>
    <p>Berkeley UPC runtime 2.8.0, IBV conduit  SSH bootstrap (default is MPI)</p>
    <p>MVAPICH with MPICH2's Hydra process manager</p>
    <p>Hardware setup:  Glenn cluster at the Ohio Supercomputing Center</p>
    <p>877 Node IBM 1350 cluster  Two dual-core 2.6 GHz AMD Opterons and 8GB RAM per node  Infiniband interconnect</p>
  </div>
  <div class="page">
    <p>Random Access Benchmark</p>
    <p>Threads access random elements of distributed shared array</p>
    <p>UPC Only: One copy distribute across all procs. No local accesses.</p>
    <p>Hybrid: Array is replicated on every group. All accesses are local.</p>
    <p>shared double data[8]:</p>
    <p>shared double data[8]: shared double data[8]:</p>
    <p>P0 P1 P2 P3</p>
    <p>P0 P1 P2 P3</p>
    <p>Affinity</p>
  </div>
  <div class="page">
    <p>Random Access Benchmark</p>
    <p>Performance decreases with system size  Locality decreases, data is more dispersed</p>
    <p>Nested-multiple model  Array is replicated across UPC groups</p>
  </div>
  <div class="page">
    <p>Barnes-Hut n-Body Simulation</p>
    <p>Simulate motion and gravitational interactions of n astronomical bodies over time</p>
    <p>Represents 3-d space using an oct-tree  Space is sparse</p>
    <p>Summarize distant interactions using center of mass</p>
    <p>Colliding Antennae Galaxies (Hubble Space Telescope)</p>
  </div>
  <div class="page">
    <p>Hybrid Barnes-Hut Algorithm</p>
    <p>UPC</p>
    <p>for i in 1..t_max</p>
    <p>t &lt;- new octree()</p>
    <p>forall b in bodies</p>
    <p>insert(t, b)</p>
    <p>summarize_subtrees(t)</p>
    <p>forall b in bodies</p>
    <p>compute_forces(b, t)</p>
    <p>forall b in bodies</p>
    <p>advance(b)</p>
    <p>Hybrid UPC+MPI</p>
    <p>for i in 1..t_max</p>
    <p>t &lt;- new octree()</p>
    <p>forall b in bodies</p>
    <p>insert(t, b)</p>
    <p>summarize_subtrees(t)</p>
    <p>our_bodies &lt;</p>
    <p>partion(group id, bodies)</p>
    <p>forall b in our_bodies</p>
    <p>compute_forces(b, t)</p>
    <p>forall b in our_bodies</p>
    <p>advance(b)</p>
    <p>Allgather(our_bodies, bodies)</p>
  </div>
  <div class="page">
    <p>Hybrid MPI+UPC Barnes-Hut</p>
    <p>Nested-funneled model</p>
    <p>Tree is replicated across UPC groups</p>
    <p>51 new lines of code (2% increase)</p>
    <p>Distribute work and collect results 14</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Hybrid MPI+UPC offers interesting possibilities  Improve locality of UPC codes through replication</p>
    <p>Increase storage space MPI codes with UPCs GAS</p>
    <p>Random Access Benchmark:  1.33x performance with groups spanning two nodes</p>
    <p>Barnes-Hut:  2x performance with groups spanning four nodes</p>
    <p>2% increase in codes size</p>
    <p>Contact: James Dinan &lt;dinan@cse.ohio-state.edu&gt;</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>The PGAS Memory Model</p>
    <p>Global Address Space  Aggregates memory of multiple nodes  Logically partitioned according to affinity  Data access via one-sided get(..) and put(..) operations  Programmer controls data distribution and locality</p>
    <p>PGAS Family: UPC (C), CAF (Fortran), Titanium (Java), GA (library)</p>
    <p>Shared</p>
    <p>G lo</p>
    <p>b a l</p>
    <p>a d</p>
    <p>d re</p>
    <p>s s</p>
    <p>s p</p>
    <p>a c e</p>
    <p>Private</p>
    <p>Proc0 Proc1 Procn</p>
    <p>X[M][M][N]</p>
    <p>X[1..9] [1..9][1..9]</p>
    <p>X</p>
  </div>
  <div class="page">
    <p>Who is UPC</p>
    <p>UPC is an open standard, latest is v1.2 from May, 2005  Academic and Government Institutions</p>
    <p>George Washington University</p>
    <p>Laurence Berkeley National Laboratory</p>
    <p>University of California, Berkeley</p>
    <p>University of Florida</p>
    <p>Michigan Technological University</p>
    <p>U.S. DOE, Army High Performance Computing Research Center  Commercial Institutions</p>
    <p>Hewlett-Packard (HP)</p>
    <p>Cray, Inc</p>
    <p>Intrepid Technology, Inc.</p>
    <p>IBM</p>
    <p>Etnus, LLC (Totalview) 18</p>
  </div>
  <div class="page">
    <p>Why not use MPI-2 one-sided?</p>
    <p>Problem: Ordering  A location can only be accessed once per epoch if written to</p>
    <p>Problem: Concurrency in accessing shared data  Must always lock to declare an access epoch</p>
    <p>Concurrent local/remote accesses to the same window are an error even if regions do not overlap</p>
    <p>Locking is performed at (coarse) window granularity  Want multiple windows to increase concurrency</p>
    <p>Problem: Multiple windows  Window creation (i.e. dynamic object allocation) is collective</p>
    <p>MPI_Win objects can't be shared</p>
    <p>Shared pointers require bookkeeping</p>
  </div>
  <div class="page">
    <p>Mapping UPC Thread Ids to MPI Ranks</p>
    <p>How to identify a process?</p>
    <p>Group ID</p>
    <p>Group rank</p>
    <p>Group ID = MPI rank of thread 0</p>
    <p>Group rank = MYTHREAD</p>
    <p>Thread IDs not contiguous</p>
    <p>Must be renumbered</p>
    <p>MPI_Comm_split(0, key)</p>
    <p>Key = MPI rank of thread 0 * THREADS + MYTHREAD</p>
    <p>Result: contiguous renumbering</p>
    <p>MYTHREAD = MPI rank % THREADS</p>
    <p>Group ID = Thread 0 rank = MPI rank/THREADS</p>
  </div>
  <div class="page">
    <p>Launching Nested-Multiple Applications</p>
    <p>Example: launch hybrid app with two UPC groups of size 8  MPMD-style launch</p>
    <p>$ mpiexec -env HOSTS=hosts.0 upcrun -n 8 hybrid-app : -env HOSTS=hosts.1 upcrun -n 8 hybrid-app</p>
    <p>Mpiexec launches two tasks  Each MPI task runs UPC's launcher</p>
    <p>Provide different arguments (host file) to each task</p>
    <p>Under nested-multiple model, all processes are hybrid  Each instance of hybrid_app calls MPI_Init(), requests a rank</p>
    <p>Problem: MPI thinks it launched a two-task job!</p>
    <p>Solution: Flag: --ranks-per-proc=8  Added to Hydra process manager in MPICH2</p>
  </div>
  <div class="page">
    <p>Dot Product  Flat</p>
    <p>#include &lt;upc.h&gt;</p>
    <p>#include &lt;mpi.h&gt;</p>
    <p>#define N 100*THREADS</p>
    <p>shared double v1[N], v2[N];</p>
    <p>int main(int argc, char **argv) {</p>
    <p>int i, rank, size;</p>
    <p>double sum = 0.0, dotp;</p>
    <p>MPI_Comm hybrid_comm;</p>
    <p>MPI_Init(&amp;argc, &amp;argv);</p>
    <p>MPI_Comm_split(MPI_COMM_WORLD, 0,</p>
    <p>MYTHREAD, &amp;hybrid_comm);</p>
    <p>MPI_Comm_rank(hybrid_comm, &amp;rank);</p>
    <p>MPI_Comm_size(hybrid_comm, &amp;size);</p>
    <p>upc_forall(i = 0; i &lt; N; i++; i)</p>
    <p>sum += v1[i]*v2[i];</p>
    <p>MPI_Reduce(&amp;sum, &amp;dotp, 1, MPI_DOUBLE,</p>
    <p>MPI_SUM, 0, hybrid_comm);</p>
    <p>if (rank == 0) printf(&quot;Dot = %f\n&quot;, dotp);</p>
    <p>MPI_Finalize();</p>
    <p>return 0;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Dot Product  Nested Funneled</p>
    <p>#include &lt;upc.h&gt;</p>
    <p>#include &lt;mpi.h&gt;</p>
    <p>#define N 100*THREADS</p>
    <p>shared double v1[N], v2[N];</p>
    <p>shared double our_sum = 0.0;</p>
    <p>shared double my_sum[THREADS];</p>
    <p>shared int me, np;</p>
    <p>int main(int argc, char **argv) {</p>
    <p>int i, B;</p>
    <p>double dotp;</p>
    <p>if (MYTHREAD == 0) {</p>
    <p>MPI_Init(&amp;argc, &amp;argv);</p>
    <p>MPI_Comm_rank(MPI_COMM_WORLD, (int*)&amp;me);</p>
    <p>MPI_Comm_size(MPI_COMM_WORLD, (int*)&amp;np);</p>
    <p>}</p>
    <p>upc_barrier; 23</p>
    <p>B = N/np;</p>
    <p>my_sum[MYTHREAD] = 0.0;</p>
    <p>upc_forall(i=me*B;i&lt;(me+1)*B;i++; &amp;v1[i])</p>
    <p>my_sum[MYTHREAD] += v1[i]*v2[i];</p>
    <p>upc_all_reduceD(&amp;our_sum,&amp;my_sum[MYTHREAD],</p>
    <p>UPC_ADD, 1, 0, NULL,</p>
    <p>UPC_IN_ALLSYNC | UPC_OUT_ALLSYNC);</p>
    <p>if (MYTHREAD == 0) {</p>
    <p>MPI_Reduce(&amp;our_sum,&amp;dotp,1,MPI_DOUBLE,</p>
    <p>MPI_SUM, 0, MPI_COMM_WORLD);</p>
    <p>if (me == 0) printf(&quot;Dot = %f\n&quot;, dotp);</p>
    <p>MPI_Finalize();</p>
    <p>}</p>
    <p>return 0;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Dot Product  Nested Multiple</p>
    <p>#include &lt;upc.h&gt;</p>
    <p>#include &lt;mpi.h&gt;</p>
    <p>#define N 100*THREADS</p>
    <p>shared double v1[N], v2[N];</p>
    <p>shared int r0;</p>
    <p>int main(int argc, char **argv) {</p>
    <p>int i, me, np;</p>
    <p>double sum = 0.0, dotp;</p>
    <p>MPI_Comm hybrid_comm</p>
    <p>MPI_Init(&amp;argc, &amp;argv);</p>
    <p>MPI_Comm_rank(MPI_COMM_WORLD, &amp;me);</p>
    <p>MPI_Comm_size(MPI_COMM_WORLD, &amp;np);</p>
    <p>if (MYTHREAD == 0) r0 = me;</p>
    <p>upc_barrier;</p>
    <p>MPI_Comm_split(MPI_COMM_WORLD, 0,</p>
    <p>r0*THREADS+MYTHREAD, &amp;hybrid_comm);</p>
    <p>MPI_Comm_rank(hybrid_comm, &amp;me);</p>
    <p>MPI_Comm_size(hybrid_comm, &amp;np);</p>
    <p>int B = N/(np/THREADS); // Block size</p>
    <p>upc_forall(i=me*B;i&lt;(me+1)*B;i++; &amp;v1[i])</p>
    <p>sum += v1[i]*v2[i];</p>
    <p>MPI_Reduce(&amp;sum, &amp;dotp, 1, MPI_DOUBLE,</p>
    <p>MPI_SUM, 0, hybrid_comm);</p>
    <p>if (me == 0) printf(&quot;Dot = %f\n&quot;, dotp);</p>
    <p>MPI_Finalize();</p>
    <p>return 0;</p>
    <p>}</p>
  </div>
</Presentation>
