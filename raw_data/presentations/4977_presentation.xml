<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>DMA-Assisted, Intranode Communication in GPU-Accelerated Systems</p>
    <p>Feng Ji*, Ashwin M. Aji, James Dinan, Darius Buntinas, Pavan Balaji, Rajeev Thakur, Wu-chun Feng, Xiaosong Ma*&amp; * Department of Computer Science, North Carolina State University  Department of Computer Science, Virginia Tech  Mathematics and Computer Science Division, Argonne National Laboratory &amp; Computer Science and Mathematics Division, Oak Ridge National Laboratory</p>
  </div>
  <div class="page">
    <p>Background: CPU-GPU Clusters</p>
    <p>Graphics Processing Units (GPUs)  Many-core architecture for high p</p>
    <p>erformance and efficiency (FLOPs , FLOPs/Watt, FLOPs/$)</p>
    <p>Prog. Models: CUDA, OpenCL, Op enACC</p>
    <p>Explicitly managed global memor y and separate address spaces</p>
    <p>CPU clusters  Most popular parallel prog. mode</p>
    <p>l: Message Passing Interface (MP I)</p>
    <p>Host memory only</p>
    <p>Disjoint Memory Spaces!</p>
    <p>MPI rank 0</p>
    <p>MPI rank 1</p>
    <p>MPI rank 2</p>
    <p>MPI rank 3</p>
    <p>NIC</p>
    <p>Main memory</p>
    <p>CPU</p>
    <p>Global memory</p>
    <p>Shared memory</p>
    <p>Multiprocessor GPU</p>
    <p>PCIe, HT/QPI</p>
  </div>
  <div class="page">
    <p>GPU-Accelerated High Performance Computing</p>
    <p>Clusters with GPUs are becoming common  Multiple GPUs per node  Non-uniform node architecture  Node topology plays role in performa</p>
    <p>nce</p>
    <p>New programmability and performan ce challenges for programming mode ls and runtime systems</p>
  </div>
  <div class="page">
    <p>Programming CPU-GPU Clusters (e.g: MPI+CUDA)</p>
    <p>GPU device</p>
    <p>memory</p>
    <p>GPU device</p>
    <p>memory</p>
    <p>CPU main</p>
    <p>memory</p>
    <p>CPU main</p>
    <p>memory</p>
    <p>PCIe PC Ie</p>
    <p>HT/QPI</p>
    <p>Rank = 0 Rank = 1</p>
    <p>if(rank == 0) { cudaMemcpy(host_buf, dev_buf, D2H) MPI_Send(host_buf, .. ..) }</p>
    <p>if(rank == 1) { MPI_Recv(host_buf, .. ..) cudaMemcpy(dev_buf, host_buf, H2D) }</p>
  </div>
  <div class="page">
    <p>Current Limitations of Programming CPU-GPU Clust ers (e.g: MPI+CUDA)</p>
    <p>Manual blocking copy between host and GPU memory serializes PCIe, Inte rconnect</p>
    <p>Manual non-blocking copy is better, but will incur protocol overheads mul tiple times</p>
    <p>Programmability/Productivity: Manual data movement leading to comple x code; Non-portable codes</p>
    <p>Performance: Inefficient and non-portable performance optimizations</p>
  </div>
  <div class="page">
    <p>Goal of Programming CPU-GPU Clusters (e.g: MPI+ Any accelerator)</p>
    <p>GPU device</p>
    <p>memory</p>
    <p>GPU device</p>
    <p>memory</p>
    <p>CPU main</p>
    <p>memory</p>
    <p>CPU main</p>
    <p>memory</p>
    <p>PCIe PC Ie</p>
    <p>HT/QPI</p>
    <p>Rank = 0 Rank = 1</p>
    <p>if(rank == 0) { MPI_Send(any_buf, .. ..); }</p>
    <p>if(rank == 1) { MPI_Recv(any_buf, .. ..); }</p>
  </div>
  <div class="page">
    <p>MPI-ACC: Integrated and Optimized Data Movemen t MPI-ACC: integrates accelerator awareness with MPI for all da</p>
    <p>ta movement  Programmability/Productivity: supports multiple accelerators and pro</p>
    <p>g. models (CUDA, OpenCL)  Performance: allows applications to portably leverage system-specific</p>
    <p>and vendor-specific optimizations</p>
    <p>GPU</p>
    <p>CPU CPU</p>
    <p>GPU GPU GPU</p>
    <p>Network</p>
  </div>
  <div class="page">
    <p>MPI-ACC: Integrated and Optimized Data Movemen t  MPI-ACC: integrates accelerator awareness with MPI for all da</p>
    <p>ta movement  MPI-ACC: An Integrated and Extensible Approach to Data Movement in A</p>
    <p>ccelerator-Based Systems [HPCC 12]  Intranode Optimizations</p>
    <p>DMA-Assisted, Intranode Communication in GPU-Accelerated Systems, F eng Ji, Ashwin M. Aji, James Dinan, Darius Buntinas, Pavan Balaji, Rajeev T hakur, Wu-chun Feng and Xiaosong Ma [This paper]</p>
    <p>Efficient Intranode Communication in GPU-Accelerated Systems, Feng Ji, Ashwin M. Aji, James Dinan, Darius Buntinas, Pavan Balaji, Wu-Chun Feng and Xiaosong Ma. [AsHES 12]</p>
    <p>Noncontiguous Datatypes  Enabling Fast, Noncontiguous GPU Data Movement in Hybrid MPI+GPU E</p>
    <p>nvironments, John Jenkins, James Dinan, Pavan Balaji, Nagiza F. Samatova , and Rajeev Thakur. Under review at IEEE Cluster 2012.</p>
  </div>
  <div class="page">
    <p>Intranode Optimizations: shared memory protocol [ASHES 12]</p>
    <p>Directly connect PCIe data copies into MPI internal shared me mory</p>
    <p>Transparently doing chunking and pipelining</p>
    <p>GPU Global Mem</p>
    <p>Host Main Mem</p>
    <p>Process 0 Process 1</p>
    <p>Shared Mem</p>
    <p>Process 0 MPIGPU_Send(d_ptr, , MPIGPU_BUF_GPU);</p>
    <p>Process 1 MPIGPU_Recv(d_ptr, , MPIGPU_BUF_GPU);</p>
  </div>
  <div class="page">
    <p>GPUDirect + CUDAIPC</p>
    <p>GPUDirect: DMA-driven peer GPU copy  CUDAIPC: exporting a GPU buffer to a different process</p>
    <p>Device Mem</p>
    <p>Main Mem</p>
    <p>Process 0 cudaIpcGetMemHandle(&amp;handle, d_ptr);</p>
    <p>Process 1 cudaIpcOpenMemHandl(&amp;d_ptr_src, handle); cudaMemcpy(d_ptr, d_ptr_src, );</p>
    <p>Direct copy</p>
    <p>Handle</p>
  </div>
  <div class="page">
    <p>DMA-assisted Intranode GPU data transfer</p>
    <p>Motivation  Eliminate going through MPI host-side shared memory  Reduce the complexity of doing the same thing in the application level</p>
    <p>Need NOT re-invent synchronization mechanism</p>
    <p>Process 0 cudaIpcGetMemHandle(&amp;handle, d_ptr); MPI_Send(handle, ); MPI_Recv(Msg_done, );</p>
    <p>Process 1 MPI_Recv(handle, ); cudaIpcOpenMemHandl(&amp;d_ptr_src, handle); cudaMemcpy(d_ptr, d_ptr_src, ); MPI_Send(Msg_done, );</p>
  </div>
  <div class="page">
    <p>DMA-assisted Intranode GPU data transfer</p>
    <p>Challenges  GPUDirect requires GPU peer accessibility</p>
    <p>Same IO/Hub: Yes  Different IO/Hub: Yes for AMD (HT); No for Intel (QPI)</p>
    <p>Overhead of handle open/close  MPI is unaware of GPU device topology</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 2</p>
    <p>Intel (with QPI) AMD (with HT)</p>
  </div>
  <div class="page">
    <p>Extend Large Message Transport (LMT) protocol</p>
    <p>LMT Protocol supports PUT/GET/C OOP modes</p>
    <p>Handles carried in packets/cookies  Sender passes GPU info (with hand</p>
    <p>le) to Receiver  Sender always tries to get the han</p>
    <p>dle  Getting a handle is a light-weight o</p>
    <p>p. (according to NVIDIA)  Receiver can decide GPU peer acce</p>
    <p>ssibility  Receiver passes the decision in CT</p>
    <p>S to Sender  Fall back option: via shared memor</p>
    <p>y [AsHES 12]</p>
    <p>PUT mode</p>
    <p>Sender Receiver</p>
    <p>RTS</p>
    <p>CTS</p>
    <p>Done</p>
    <p>PUT</p>
    <p>Get Src handle</p>
    <p>Decide accessibility</p>
    <p>Get Dest handle</p>
    <p>Open Dest handle</p>
  </div>
  <div class="page">
    <p>Extend Large Message Transport (LMT) protocol</p>
    <p>GET Mode  COOP Mode</p>
    <p>Sender Receiver</p>
    <p>RTS</p>
    <p>CTS</p>
    <p>Done</p>
    <p>Done</p>
    <p>Get Src handle</p>
    <p>Decide accessibility</p>
    <p>Get Dest handle</p>
    <p>Open Src handle</p>
    <p>Open Dest handle</p>
    <p>Sender Receiver</p>
    <p>RTS</p>
    <p>CTS</p>
    <p>Done</p>
    <p>Get Src handle</p>
    <p>Decide accessibility</p>
    <p>Open Src handle</p>
  </div>
  <div class="page">
    <p>IPC Open/Close overhead</p>
    <p>Getting a handle is light-weight operation  Open and close operations on the handle are NOT  Do not close a handle when a pair of MPI_Send/Recv is done</p>
    <p>Many MPI program reuse buffers (including GPU buffers)  Lazy close option to avoid overhead</p>
    <p>Cache opened handles &amp; their addresses locally  Next time: try to find the handle in the local cache</p>
    <p>If found, no need to reopen it, but use its address</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Systems Keeneland (@GaTech)</p>
    <p>Magellan (@Argonne)</p>
    <p>CPU Intel AMD NUMA Nodes 2 4 Interconnect Intel QPI AMD HT GPUs 3 2 GPU Topology GPU 0: N0;</p>
    <p>GPU 1,2: N1 GPU 0: N0; GPU 1: N3</p>
    <p>GPU Peer accessibility Only GPU 1 ~ 2 Yes Distance same I/O Hub (Near) 2 PCIe + 1 HT (Far)</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 2 Contact: Feng Ji (fji@ncsu.edu)</p>
  </div>
  <div class="page">
    <p>Near case: Intel @ Keeneland</p>
    <p>Bandwidth nearly reaches the peak bandwidth of the system</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
    <p>GPU 2</p>
  </div>
  <div class="page">
    <p>Far case: AMD @ Magellan</p>
    <p>Better to adopt shared memory approach</p>
    <p>GPU 1</p>
    <p>GPU 0</p>
  </div>
  <div class="page">
    <p>Stencil2D (SHOC) on Intel @ Keeneland</p>
    <p>Compared against using previous shared memory based approach  Avg 4.7% improvement (single pr</p>
    <p>ecision) &amp; 2.3% (double precisio n)</p>
    <p>Computation increases O(n2) with problem size, thus communicatio n reduces</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Accelerators are getting ubiquitous  Exciting new opportunities for systems researchers  Requires evolution of HPC software stack &amp; more openness of GPU sys</p>
    <p>tem stack  Integrated accelerator-awareness with MPI</p>
    <p>Supported multiple accelerators and programming models  Goals are productivity and performance</p>
    <p>Optimized Intranode communication  Utilized GPU DMA engine  Eliminated going through MPI main memory buffer</p>
    <p>Questions? Contact Feng Ji (fji@ncsu.edu) Ashwin Aji (aaji@cs.vt.edu) Pavan Balaji (balaji@mcs.anl.gov)</p>
  </div>
</Presentation>
