<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>An Analysis of Linux Scalability to Many Cores</p>
    <p>Silas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Morris, and Nickolai Zeldovich</p>
    <p>MIT CSAIL</p>
  </div>
  <div class="page">
    <p>What is scalability?</p>
    <p>Application does N times as much work on N cores as it could on 1 core</p>
    <p>Scalability may be limited by Amdahl's Law:  Locks, shared data structures, ...  Shared hardware (DRAM, NIC, ...)</p>
  </div>
  <div class="page">
    <p>Why look at the OS kernel?</p>
    <p>Many applications spend time in the kernel  E.g. On a uniprocessor, the Exim mail server</p>
    <p>spends 70% in kernel</p>
    <p>These applications should scale with more cores</p>
    <p>If OS kernel doesn't scale, apps won't scale</p>
  </div>
  <div class="page">
    <p>Speculation about kernel scalability</p>
    <p>Several kernel scalability studies indicate existing kernels don't scale well</p>
    <p>Speculation that fixing them is hard  New OS kernel designs:</p>
    <p>Corey, Barrelfish, fos, Tessellation,</p>
    <p>How serious are the scaling problems?  How hard is it to fix them?  Hard to answer in general, but we shed some light on</p>
    <p>the answer by analyzing Linux scalability</p>
  </div>
  <div class="page">
    <p>Analyzing scalability of Linux</p>
    <p>Use a off-the-shelf 48-core x86 machine  Run a recent version of Linux</p>
    <p>Used a lot, competitive baseline scalability</p>
    <p>Scale a set of applications  Parallel implementation  System intensive</p>
  </div>
  <div class="page">
    <p>Contributions</p>
    <p>Analysis of Linux scalability for 7 real apps.  Stock Linux limits scalability  Analysis of bottlenecks</p>
    <p>Fixes: 3002 lines of code, 16 patches  Most fixes improve scalability of multiple apps.  Remaining bottlenecks in HW or app  Result: no kernel problems up to 48 cores</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Method</p>
    <p>Run application  Use in-memory file system to avoid disk bottleneck</p>
    <p>Find bottlenecks  Fix bottlenecks, re-run application</p>
    <p>Stop when a non-trivial application fix is required, or bottleneck by shared hardware (e.g. DRAM)</p>
  </div>
  <div class="page">
    <p>Off-the-shelf 48-core server</p>
    <p>6 core x 8 chip AMD</p>
    <p>DRAM DRAM DRAM</p>
    <p>DRAMDRAMDRAMDRAM</p>
    <p>DRAM</p>
  </div>
  <div class="page">
    <p>Exim memcached</p>
    <p>Apache PostgreSQL</p>
    <p>gmake Psearchy</p>
    <p>Metis</p>
    <p>Poor scaling on stock Linux kernel</p>
    <p>Y-axis: (throughput with 48 cores) / (throughput with one core)</p>
    <p>perfect scaling</p>
    <p>terrible scaling</p>
  </div>
  <div class="page">
    <p>Exim on stock Linux: collapse</p>
    <p>Throughput</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t (m</p>
    <p>e s s a g</p>
    <p>e s /s</p>
    <p>e c o</p>
    <p>n d</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Exim on stock Linux: collapse</p>
    <p>Throughput</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t (m</p>
    <p>e s s a g</p>
    <p>e s /s</p>
    <p>e c o</p>
    <p>n d</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Exim on stock Linux: collapse</p>
    <p>Throughput Kernel time</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t (m</p>
    <p>e s s a g</p>
    <p>e s /s</p>
    <p>e c o</p>
    <p>n d</p>
    <p>)</p>
    <p>K e</p>
    <p>rn e</p>
    <p>l C P</p>
    <p>U t im</p>
    <p>e (</p>
    <p>m ill</p>
    <p>is e</p>
    <p>c o</p>
    <p>n d</p>
    <p>s /m</p>
    <p>e s s a g</p>
    <p>e )</p>
  </div>
  <div class="page">
    <p>Oprofile shows an obvious problem</p>
    <p>samples % app name symbol name 2616 7.3522 vmlinux radix_tree_lookup_slot 2329 6.5456 vmlinux unmap_vmas 2197 6.1746 vmlinux filemap_fault 1488 4.1820 vmlinux __do_fault 1348 3.7885 vmlinux copy_page_c 1182 3.3220 vmlinux unlock_page 966 2.7149 vmlinux page_fault</p>
    <p>samples % app name symbol name 13515 34.8657 vmlinux lookup_mnt 2002 5.1647 vmlinux radix_tree_lookup_slot 1661 4.2850 vmlinux filemap_fault 1497 3.8619 vmlinux unmap_vmas 1026 2.6469 vmlinux __do_fault 914 2.3579 vmlinux atomic_dec 896 2.3115 vmlinux unlock_page</p>
  </div>
  <div class="page">
    <p>Oprofile shows an obvious problem</p>
    <p>samples % app name symbol name 2616 7.3522 vmlinux radix_tree_lookup_slot 2329 6.5456 vmlinux unmap_vmas 2197 6.1746 vmlinux filemap_fault 1488 4.1820 vmlinux __do_fault 1348 3.7885 vmlinux copy_page_c 1182 3.3220 vmlinux unlock_page 966 2.7149 vmlinux page_fault</p>
    <p>samples % app name symbol name 13515 34.8657 vmlinux lookup_mnt 2002 5.1647 vmlinux radix_tree_lookup_slot 1661 4.2850 vmlinux filemap_fault 1497 3.8619 vmlinux unmap_vmas 1026 2.6469 vmlinux __do_fault 914 2.3579 vmlinux atomic_dec 896 2.3115 vmlinux unlock_page</p>
  </div>
  <div class="page">
    <p>Oprofile shows an obvious problem</p>
    <p>samples % app name symbol name 2616 7.3522 vmlinux radix_tree_lookup_slot 2329 6.5456 vmlinux unmap_vmas 2197 6.1746 vmlinux filemap_fault 1488 4.1820 vmlinux __do_fault 1348 3.7885 vmlinux copy_page_c 1182 3.3220 vmlinux unlock_page 966 2.7149 vmlinux page_fault</p>
    <p>samples % app name symbol name 13515 34.8657 vmlinux lookup_mnt 2002 5.1647 vmlinux radix_tree_lookup_slot 1661 4.2850 vmlinux filemap_fault 1497 3.8619 vmlinux unmap_vmas 1026 2.6469 vmlinux __do_fault 914 2.3579 vmlinux atomic_dec 896 2.3115 vmlinux unlock_page</p>
  </div>
  <div class="page">
    <p>Bottleneck: reading mount table</p>
    <p>sys_open eventually calls: struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); return mnt; }</p>
  </div>
  <div class="page">
    <p>Bottleneck: reading mount table</p>
    <p>sys_open eventually calls: struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); return mnt; }</p>
  </div>
  <div class="page">
    <p>Bottleneck: reading mount table</p>
    <p>sys_open eventually calls: struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); return mnt; }</p>
    <p>Critical section is short. Why does it cause a scalability bottleneck?</p>
  </div>
  <div class="page">
    <p>Bottleneck: reading mount table</p>
    <p>sys_open eventually calls: struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); return mnt; }</p>
    <p>Critical section is short. Why does it cause a scalability bottleneck?</p>
    <p>spin_lock and spin_unlock use many more cycles than the critical section</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Allocate a ticket</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Allocate a ticket</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Allocate a ticket</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Allocate a ticket</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Allocate a ticket</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Spin until it's my turn</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Linux spin lock implementation void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Update the ticket value</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>} 500  4000 cycles!!</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
    <p>Previous lock holder notifies next lock holder after</p>
    <p>sending out N/2 replies</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Scalability collapse caused by non-scalable locks [Anderson 90]</p>
    <p>void spin_lock(spinlock_t *lock) {</p>
    <p>t = atomic_inc(lock-&gt;next_ticket); while (t != lock-&gt;current_ticket)</p>
    <p>; /* Spin */ }</p>
    <p>void spin_unlock(spinlock_t *lock) {</p>
    <p>lock-&gt;current_ticket++; }</p>
    <p>struct spinlock_t { int current_ticket; int next_ticket;</p>
    <p>}</p>
  </div>
  <div class="page">
    <p>Bottleneck: reading mount table</p>
    <p>sys_open eventually calls: struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); return mnt; }</p>
    <p>Well known problem, many solutions  Use scalable locks [MCS 91]  Use message passing [Baumann 09]  Avoid locks in the common case</p>
  </div>
  <div class="page">
    <p>Solution: per-core mount caches  Observation: mount table is rarely modified struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; if ((mnt = hash_get(percore_mnts[cpu()], path))) return mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); hash_put(percore_mnts[cpu()], path, mnt); return mnt; }</p>
  </div>
  <div class="page">
    <p>Solution: per-core mount caches  Observation: mount table is rarely modified struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; if ((mnt = hash_get(percore_mnts[cpu()], path))) return mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); hash_put(percore_mnts[cpu()], path, mnt); return mnt; }</p>
  </div>
  <div class="page">
    <p>Solution: per-core mount caches</p>
    <p>Common case: cores access per-core tables  Modify mount table: invalidate per-core tables</p>
    <p>struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; if ((mnt = hash_get(percore_mnts[cpu()], path))) return mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); hash_put(percore_mnts[cpu()], path, mnt); return mnt; }</p>
    <p>Observation: mount table is rarely modified</p>
  </div>
  <div class="page">
    <p>Solution: per-core mount caches</p>
    <p>Common case: cores access per-core tables  Modify mount table: invalidate per-core tables</p>
    <p>struct vfsmount *lookup_mnt(struct path *path) { struct vfsmount *mnt; if ((mnt = hash_get(percore_mnts[cpu()], path))) return mnt; spin_lock(&amp;vfsmount_lock); mnt = hash_get(mnts, path); spin_unlock(&amp;vfsmount_lock); hash_put(percore_mnts[cpu()], path, mnt); return mnt; }</p>
    <p>Observation: mount table is rarely modified</p>
  </div>
  <div class="page">
    <p>Per-core lookup: scalability is better</p>
    <p>Throughput with per-core lookup Throughput of stock Linux</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>(m e</p>
    <p>s s a</p>
    <p>g e</p>
    <p>s /s</p>
    <p>e c o</p>
    <p>n d )</p>
  </div>
  <div class="page">
    <p>Per-core lookup: scalability is better</p>
    <p>Throughput with per-core lookup Throughput of stock Linux</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>(m e</p>
    <p>s s a</p>
    <p>g e</p>
    <p>s /s</p>
    <p>e c o</p>
    <p>n d )</p>
  </div>
  <div class="page">
    <p>No obvious bottlenecks samples % app name symbol name 3319 5.4462 vmlinux radix_tree_lookup_slot 3119 5.2462 vmlinux unmap_vmas 1966 3.3069 vmlinux filemap_fault 1950 3.2800 vmlinux page_fault 1627 2.7367 vmlinux unlock_page 1626 2.7350 vmlinux clear_page_c 1578 2.6542 vmlinux kmem_cache_free</p>
    <p>samples % app name symbol name 4207 5.3145 vmlinux radix_tree_lookup_slot 4191 5.2943 vmlinux unmap_vmas 2632 3.3249 vmlinux page_fault 2525 3.1897 vmlinux filemap_fault 2210 2.7918 vmlinux clear_page_c 2131 2.6920 vmlinux kmem_cache_free 2000 2.5265 vmlinux dput</p>
    <p>Functions execute more slowly on 48 cores</p>
  </div>
  <div class="page">
    <p>No obvious bottlenecks samples % app name symbol name 3319 5.4462 vmlinux radix_tree_lookup_slot 3119 5.2462 vmlinux unmap_vmas 1966 3.3069 vmlinux filemap_fault 1950 3.2800 vmlinux page_fault 1627 2.7367 vmlinux unlock_page 1626 2.7350 vmlinux clear_page_c 1578 2.6542 vmlinux kmem_cache_free</p>
    <p>samples % app name symbol name 4207 5.3145 vmlinux radix_tree_lookup_slot 4191 5.2943 vmlinux unmap_vmas 2632 3.3249 vmlinux page_fault 2525 3.1897 vmlinux filemap_fault 2210 2.7918 vmlinux clear_page_c 2131 2.6920 vmlinux kmem_cache_free 2000 2.5265 vmlinux dput</p>
    <p>Functions execute more slowly on 48 cores</p>
  </div>
  <div class="page">
    <p>No obvious bottlenecks samples % app name symbol name 3319 5.4462 vmlinux radix_tree_lookup_slot 3119 5.2462 vmlinux unmap_vmas 1966 3.3069 vmlinux filemap_fault 1950 3.2800 vmlinux page_fault 1627 2.7367 vmlinux unlock_page 1626 2.7350 vmlinux clear_page_c 1578 2.6542 vmlinux kmem_cache_free</p>
    <p>samples % app name symbol name 4207 5.3145 vmlinux radix_tree_lookup_slot 4191 5.2943 vmlinux unmap_vmas 2632 3.3249 vmlinux page_fault 2525 3.1897 vmlinux filemap_fault 2210 2.7918 vmlinux clear_page_c 2131 2.6920 vmlinux kmem_cache_free 2000 2.5265 vmlinux dput</p>
    <p>Functions execute more slowly on 48 cores</p>
  </div>
  <div class="page">
    <p>No obvious bottlenecks samples % app name symbol name 3319 5.4462 vmlinux radix_tree_lookup_slot 3119 5.2462 vmlinux unmap_vmas 1966 3.3069 vmlinux filemap_fault 1950 3.2800 vmlinux page_fault 1627 2.7367 vmlinux unlock_page 1626 2.7350 vmlinux clear_page_c 1578 2.6542 vmlinux kmem_cache_free</p>
    <p>samples % app name symbol name 4207 5.3145 vmlinux radix_tree_lookup_slot 4191 5.2943 vmlinux unmap_vmas 2632 3.3249 vmlinux page_fault 2525 3.1897 vmlinux filemap_fault 2210 2.7918 vmlinux clear_page_c 2131 2.6920 vmlinux kmem_cache_free 2000 2.5265 vmlinux dput</p>
    <p>dput is causing other functions to slow down</p>
    <p>Functions execute more slowly on 48 cores</p>
  </div>
  <div class="page">
    <p>Bottleneck: reference counting</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>Ref count indicates if kernel can free object  File name cache (dentry), physical pages, ...</p>
  </div>
  <div class="page">
    <p>Bottleneck: reference counting  Ref count indicates if kernel can free object</p>
    <p>File name cache (dentry), physical pages, ...</p>
    <p>A single atomic instruction limits scalability?!</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
  </div>
  <div class="page">
    <p>Bottleneck: reference counting</p>
    <p>Reading the reference count is slow  Reading the reference count delays memory</p>
    <p>operations from other cores</p>
    <p>Ref count indicates if kernel can free object  File name cache (dentry), physical pages, ...</p>
    <p>A single atomic instruction limits scalability?!</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
  </div>
  <div class="page">
    <p>Reading reference count is slow void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Reading reference count is slow void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Reading reference count is slow void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};120  4000 cycles depending on congestion</p>
  </div>
  <div class="page">
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>}; Hardware cache</p>
    <p>line lock</p>
  </div>
  <div class="page">
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Contention on a reference count congests the interconnect</p>
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Contention on a reference count congests the interconnect</p>
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Contention on a reference count congests the interconnect</p>
    <p>Reading the reference count delays memory operations from other cores</p>
    <p>void dput(struct dentry *dentry) { if (!atomic_dec_and_test(&amp;dentry-&gt;ref)) return; dentry_free(dentry); }</p>
    <p>struct dentry {  int ref;</p>
    <p>};</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>shared</p>
    <p>Core 0 Core 1</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>per-coreper-core</p>
    <p>Core 0 Core 1</p>
    <p>shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>rm /tmp/foo Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>Be more abstract. If you want to free, disable sloppy counter, ...</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
    <p>rm /tmp/foo</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
    <p>rm /tmp/foo</p>
  </div>
  <div class="page">
    <p>Solution: sloppy counters</p>
    <p>Observation: kernel rarely needs true value of ref count  Each core holds a few spare references</p>
    <p>Core 0 Core 1</p>
    <p>per-coreper-core shared</p>
    <p>dentry sloppy counter</p>
    <p>rm /tmp/foo</p>
  </div>
  <div class="page">
    <p>Properties of sloppy counters</p>
    <p>Simple to start using:  Change data structure  atomic_inc  sloppy_inc</p>
    <p>Scale well: no cache misses in common case  Memory usage: O(N) space  Related to: SNZI [Ellen 07] and distributed</p>
    <p>counters [Appavoo 07]</p>
  </div>
  <div class="page">
    <p>Sloppy counters: more scalability</p>
    <p>Throughput with sloppy counters Throughput with per-core lookup Throughput of stock Linux</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>(m e</p>
    <p>s s a</p>
    <p>g e</p>
    <p>s /s</p>
    <p>e c o</p>
    <p>n d )</p>
  </div>
  <div class="page">
    <p>Sloppy counters: more scalability</p>
    <p>Throughput with sloppy counters Throughput with per-core lookup Throughput of stock Linux</p>
    <p>Cores</p>
    <p>T h ro</p>
    <p>u g h p u t</p>
    <p>(m e</p>
    <p>s s a</p>
    <p>g e</p>
    <p>s /s</p>
    <p>e c o</p>
    <p>n d )</p>
  </div>
  <div class="page">
    <p>Summary of changes</p>
    <p>3002 lines of changes to the kernel  60 lines of changes to the applications</p>
    <p>A p</p>
    <p>a c h</p>
    <p>e</p>
    <p>Mount tables X X Open file table X X Sloppy counters X X X</p>
    <p>X X X X</p>
    <p>Super pages X DMA buffer allocation X X Network stack false sharing X X X Parallel accept X Application modifications X X X</p>
    <p>m e</p>
    <p>m c a</p>
    <p>c h</p>
    <p>e d</p>
    <p>E xi</p>
    <p>m</p>
    <p>P o</p>
    <p>s tg</p>
    <p>re S</p>
    <p>Q L</p>
    <p>g m</p>
    <p>a k e</p>
    <p>P s e</p>
    <p>a rc</p>
    <p>h y</p>
    <p>M e</p>
    <p>ti s</p>
    <p>inode allocation Lock-free dentry lookup</p>
  </div>
  <div class="page">
    <p>Handful of known techniques [Cantrill 08]</p>
    <p>Lock-free algorithms  Per-core data structures  Fine-grained locking  Cache-alignment  Sloppy counters</p>
  </div>
  <div class="page">
    <p>Better scaling with our modifications</p>
    <p>Y-axis: (throughput with 48 cores) / (throughput with one core)</p>
    <p>Exim memcached</p>
    <p>Apache PostgreSQL</p>
    <p>gmake Psearchy</p>
    <p>Metis</p>
    <p>Most of the scalability is due to the Linux community's efforts</p>
  </div>
  <div class="page">
    <p>Current bottlenecks</p>
    <p>Application Bottleneck</p>
    <p>memcached HW: transmit queues on NIC Apache HW: receive queues on NIC Exim App: contention on spool directories gmake App: serial stages and stragglers PostgreSQL App: spin lock Psearchy HW: cache capacity Metis HW: DRAM throughput</p>
    <p>Kernel code is not the bottleneck  Further kernel changes might help apps. or hw</p>
  </div>
  <div class="page">
    <p>Limitations</p>
    <p>Results limited to 48 cores and small set of applications</p>
    <p>Looming problems  fork/virtual memory book-keeping  Page allocator  File system  Concurrent modifications to address space</p>
    <p>In-memory FS instead of disk  48-core AMD machine  single 48-core chip</p>
  </div>
  <div class="page">
    <p>Related work  Linux and Solaris scalability studies [Yan 09,10]</p>
    <p>[Veal 07] [Tseng 07] [Jia 08] ...  Scalable multiprocessor Unix variants</p>
    <p>Flash, IBM, SGI, Sun,   100s of CPUs</p>
    <p>Linux scalability improvements  RCU, NUMA awareness,</p>
    <p>Our contribution:  In-depth analysis of kernel intensive applications</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Linux has scalability problems  They are easy to fix or avoid up to 48 cores</p>
    <p>http://pdos.csail.mit.edu/mosbench</p>
  </div>
  <div class="page"/>
</Presentation>
