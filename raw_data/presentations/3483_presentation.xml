<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Lancaster A at SemEval-2017 Task 5:</p>
    <p>Evaluation metrics matter: predicting</p>
    <p>sentiment from financial news headlines</p>
    <p>Andrew Moore and Paul Rayson</p>
    <p>February 27, 2018</p>
    <p>School of Computing and Communications, Lancaster University.</p>
  </div>
  <div class="page">
    <p>Task</p>
  </div>
  <div class="page">
    <p>The task</p>
    <p>Example sentence</p>
    <p>Why AstraZeneca plc &amp; Dixons Carphone PLC Are Red-Hot Growth</p>
    <p>Stars!</p>
    <p>Sentiment scale</p>
    <p>Data</p>
    <p>Training data: 1142 samples, 960 headlines/sentences.</p>
    <p>Testing data: 491 samples, 461 headlines/sentences.</p>
  </div>
  <div class="page">
    <p>Approach</p>
  </div>
  <div class="page">
    <p>Models</p>
  </div>
  <div class="page">
    <p>Pre-Processing and Additional data used</p>
    <p>Pre-Processing</p>
    <p>Word2Vec model</p>
    <p>Used 189, 206 financial articles (e.g. Financial Times) that were</p>
    <p>manually downloaded from Factiva1 to create a Word2Vec model [5]2.</p>
    <p>These were created using Gensim3.</p>
  </div>
  <div class="page">
    <p>Support Vector Regression (SVR) [1]</p>
    <p>Features and settings that we changed</p>
  </div>
  <div class="page">
    <p>Word Replacements</p>
    <p>Example Sentence</p>
    <p>AstraZeneca PLC had an improved performance where as Dixons</p>
    <p>performed poorly</p>
    <p>companyname had an posword performance where as companyname</p>
    <p>performed negword</p>
  </div>
  <div class="page">
    <p>Two BLSTM models</p>
    <p>Standard Model (SLSTM)</p>
    <p>Drop out between layers and connections.</p>
    <p>25 times trained over the data (epoch of 25).</p>
    <p>Early stopping model</p>
    <p>(ELSTM)</p>
    <p>Drop out between layers only.</p>
    <p>Early stopping used to determine the epoch.</p>
  </div>
  <div class="page">
    <p>BLSTM loss function</p>
    <p>Loss function</p>
    <p>Mean Square Error (MSE)</p>
    <p>Y</p>
    <p>Y i=1</p>
    <p>(yi y)2 (1)</p>
  </div>
  <div class="page">
    <p>Findings and Results</p>
  </div>
  <div class="page">
    <p>SVR best features</p>
    <p>Features</p>
    <p>Using uni-grams and bi-grams to be the best. 2.4% improvement over uni-grams.</p>
    <p>Using a tokeniser always better. Affects bi-gram results the most. 1% improvement using Unitok5 over whitespace.</p>
    <p>SVR parameter settings important 8% difference between using C=0.1 and C=0.01.</p>
    <p>Incorporating the target aspect increased performance. 0.3% improvement.</p>
    <p>Using all word replacements. N=10 for POS and NEG words and N=0 for company. 0.8% improvement using company and 0.2% for</p>
    <p>POS and NEG.</p>
  </div>
  <div class="page">
    <p>The three different metrics</p>
    <p>Cosine Similarity (CS)</p>
    <p>Metric 1</p>
    <p>K i=1</p>
    <p>yi yi K i=1</p>
    <p>y2i</p>
    <p>K i=1</p>
    <p>y2i</p>
    <p>(2)</p>
    <p>Metric 2 N n=1 CS(yn, yn)</p>
    <p>N (3)</p>
    <p>Metric 3</p>
    <p>N n=1</p>
    <p>{ len(yn)  CS(yn, yn), if len(yn) &gt; 1 1 |y  yn|, if yny  0</p>
    <p>K (4)</p>
    <p>K = Total number of samples.</p>
    <p>N = Total number of sentences.</p>
  </div>
  <div class="page">
    <p>Results across the different metrics</p>
    <p>Metric</p>
    <p>Model 1 2 3</p>
    <p>SVR 62.14 54.59 62.34</p>
    <p>SLSTM 72.89 61.55 68.64</p>
    <p>ELSTM 73.20 61.98 69.24</p>
    <p>Fortia-FBK[4] 74.50 -</p>
    <p>Metric 1 was the final metric used.</p>
  </div>
  <div class="page">
    <p>Error Analysis</p>
    <p>uk stocks little changed as ashtead gains, housing shares drop</p>
    <p>Predicted: -0.43, Real: 0.23</p>
    <p>standard life chief agrees 600000 bonus cut</p>
    <p>Predicted: -0.54, Real: 0.08</p>
    <p>why i would put j sainsbury plc in my trolley before wm morrison</p>
    <p>supermarkets ...</p>
    <p>Predicted: 0.11, Real: 0.76</p>
  </div>
  <div class="page">
    <p>Future Work</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>et al. [7].</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>LSTMs.</p>
  </div>
  <div class="page">
    <p>Questions?</p>
    <p>a.moore@lancaster.ac.uk</p>
    <p>p.rayson@lancaster.ac.uk</p>
    <p>@apmoore94</p>
    <p>@perayson</p>
    <p>All the code can be found here6</p>
    <p>Presentation can be found here 7</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>H. Drucker, C. J. Burges, L. Kaufman, A. Smola, V. Vapnik, et al.</p>
    <p>Support vector regression machines.</p>
    <p>Advances in neural information processing systems, 9:155161, 1997.</p>
    <p>A. Graves and J. Schmidhuber.</p>
    <p>Framewise phoneme classification with bidirectional lstm and</p>
    <p>other neural network architectures.</p>
    <p>Neural Networks, 18(5):602610, 2005.</p>
    <p>S. Hochreiter and J. Schmidhuber.</p>
    <p>Long short-term memory.</p>
    <p>Neural computation, 9(8):17351780, 1997.</p>
  </div>
  <div class="page">
    <p>References II</p>
    <p>Y. Mansar, L. Gatti, S. Ferradans, M. Guerini, and J. Staiano.</p>
    <p>Fortia-fbk at semeval-2017 task 5: Bullish or bearish? inferring</p>
    <p>sentiment towards brands from financial news headlines.</p>
    <p>In Proceedings of the 11th International Workshop on Semantic</p>
    <p>Evaluation (SemEval-2017), pages 817822. Association for</p>
    <p>Computational Linguistics, 2017.</p>
    <p>T. Mikolov, K. Chen, G. Corrado, and J. Dean.</p>
    <p>Efficient estimation of word representations in vector space.</p>
    <p>arXiv preprint arXiv:1301.3781, 2013.</p>
    <p>Q. Qian, M. Huang, J. Lei, and X. Zhu.</p>
    <p>Linguistically regularized lstm for sentiment classification.</p>
    <p>In Proceedings of the 55th Annual Meeting of the Association for</p>
    <p>Computational Linguistics (Volume 1: Long Papers), pages</p>
  </div>
  <div class="page">
    <p>References III</p>
    <p>Y. Wang, M. Huang, x. zhu, and L. Zhao.</p>
    <p>Attention-based lstm for aspect-level sentiment classification.</p>
    <p>In Proceedings of the 2016 Conference on Empirical Methods in</p>
    <p>Natural Language Processing, pages 606615. Association for</p>
    <p>Computational Linguistics, 2016.</p>
  </div>
</Presentation>
