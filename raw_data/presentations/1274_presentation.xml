<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>BatchCrypt: Efficient Homomorphic Encryption for Cross-Silo Federated Learning</p>
    <p>Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, Yang Liu* Hong Kong University of Science and Technology</p>
    <p>University of Nevada, Reno</p>
    <p>* WeBank</p>
  </div>
  <div class="page">
    <p>Federated Learning</p>
    <p>Emerging challenge: small &amp; fragmented data</p>
    <p>Privacy concerns  Data breaches</p>
    <p>Government regulations  GDPR  CCPA</p>
    <p>Solution: Federated Learning Collaborative Machine Learning without Centralized Training Data [1]</p>
    <p>Data Silos</p>
  </div>
  <div class="page">
    <p>Target Scenario: Cross-Silo Horizontal FL</p>
    <p>Cross-Silo: among organizations / institutions</p>
    <p>o Banks, hospitals</p>
    <p>o Reliable communication and computation</p>
    <p>o Strong privacy requirements</p>
    <p>o As opposed to cross-device: edge devices</p>
    <p>Hospital A Hospital B Hospital C</p>
  </div>
  <div class="page">
    <p>Target Scenario: Cross-Silo Horizontal FL</p>
    <p>Horizontal: datasets share same feature space [2]</p>
    <p>Objective: train a model together without revealing private data to third</p>
    <p>party (aggregator) and each other [2] Yang, Qiang, et al. &quot;Federated machine learning: Concept and applications.&quot; ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (2019): 1-19.</p>
  </div>
  <div class="page">
    <p>Repurpose datacenter distributed training?</p>
    <p>Gradients are not safe to share in plaintext [3]</p>
  </div>
  <div class="page">
    <p>Federated Learning Approaches</p>
    <p>[4] Gehrke, Johannes, Edward Lui, and Rafael Pass. &quot;Towards privacy for social networks: A zero-knowledge based definition of privacy.&quot; TCC 2011. [5] Bagdasaryan, Eugene, Omid Poursaeed, and Vitaly Shmatikov. &quot;Differential privacy has disparate impact on model accuracy.&quot; NIPS. 2019.</p>
    <p>[6] Du, Wenliang, Yunghsiang S. Han, and Shigang Chen. Privacy-preserving multivariate statistical analysis: Linear regression and classification. SDM 2004. [7] Bonawitz, Keith, et al. Practical secure aggregation for privacy-preserving machine learning. CCS 2017.</p>
    <p>Method Differential Privacy</p>
    <p>Secure Multi Party Comput.</p>
    <p>Secure Aggregation [7]</p>
    <p>Homomorphic Encryption</p>
    <p>Efficiency  [6]   Strong Privacy  [4]</p>
    <p>No accuracy loss  [5]</p>
  </div>
  <div class="page">
    <p>Additively Homomorphic Encryption for FL</p>
    <p>Allow computation over ciphertexts decrypt(encrypt(a) + encrypt(b)) = a + b</p>
    <p>Enables oblivious aggregation</p>
    <p>Client N</p>
    <p>Aggregator</p>
    <p>! Aggregation</p>
    <p>Single Client Gradients</p>
    <p>Aggregated Gradients</p>
    <p>HE Public Key</p>
    <p>HE Private Key</p>
    <p>&quot; Encryption</p>
    <p># Gradient computation</p>
    <p>$ Decryption</p>
    <p>% Model update</p>
    <p>Client A</p>
    <p>&quot; Encryption</p>
    <p># Gradient computation</p>
    <p>$ Decryption</p>
    <p>% Model update</p>
    <p>Client B 1. Clients produce gradients 2. Encrypt gradients and upload them to Aggregator 3. Aggregator summarizes all gradient ciphertexts 4. Clients receive aggregated gradients 5. Clients decrypt and apply model update [8]</p>
  </div>
  <div class="page">
    <p>Characterization: FL with HE</p>
    <p>Why is HE expensive:  Computation  Communication</p>
    <p>Plaintext: 32bit -&gt; ciphertext: 2000+ bit</p>
    <p>Key Size</p>
    <p>Plaintext Ciphertext Encryption Decryption</p>
    <p>Paillier HETime breakdown of one iteration Run on FATE, models are FMNIST, CIFAR10, and LSTM</p>
  </div>
  <div class="page">
    <p>Potential Solutions</p>
    <p>Accelerate HE operations o Limited parallelism: 3X with FPGA [9] o Communication stays the same</p>
    <p>Reduce encryption operations o One operation multiple data o batching gradient values o Compact plaintext, less inflation</p>
    <p>plaintext: 2000 bit -&gt; ciphertext 2000bit</p>
    <p>Challenge: Maintain HEs additively property</p>
    <p>Decrypting the sum of 2 batched ciphertexts =</p>
    <p>Adding pairs separately</p>
    <p>+</p>
    <p>=</p>
    <p>[9] San, Ismail, et al. &quot;Efficient paillier cryptoprocessor for privacy-preserving data mining.&quot; Security and communication networks 9.11 (2016): 1535-1546..</p>
  </div>
  <div class="page">
    <p>Gradient Batching is non-trivial</p>
    <p>All ciphertexts at aggregator: no differentiation, no permutation, no shifting Only bit-wise additions on underlying plaintexts</p>
    <p>Gradients are floating numbers: exponent aligning is required for addition [9]</p>
    <p>sign exponent mantissa</p>
    <p>Not addable</p>
  </div>
  <div class="page">
    <p>Quantization for Batching</p>
    <p>Floating gradient values cannot be batched -&gt; quantization</p>
    <p>+</p>
    <p>=</p>
    <p>126</p>
    <p>Batching with generic quantization</p>
    <p>-1</p>
    <p>original value</p>
    <p>quantized value</p>
    <p>Limitations  Restrictive: client # is required  Overflow easily: all positive integers  No differentiation between positive and negative</p>
    <p>overflows</p>
  </div>
  <div class="page">
    <p>Our Quantization &amp; Batching Solution</p>
    <p>Desired quantization for aggregation  Flexible  Aggregation results are unbatchable only with</p>
    <p>ciphertexts alone  Overflow-aware  If overflow happens, we can tell the sign</p>
  </div>
  <div class="page">
    <p>Our Quantization &amp; Batching Solution</p>
    <p>-1</p>
    <p>-126</p>
    <p>+1</p>
    <p>-7</p>
    <p>BatchCrypt</p>
    <p>-1</p>
    <p>z bit padding r bit value</p>
    <p>original value</p>
    <p>quantized value sign bit</p>
    <p>Customized quantization for aggregation  Distinguish overflow</p>
    <p>Signed integer  Positive and negative cancel out each other</p>
    <p>Symmetric range  Uniform quantization</p>
    <p>[-1, 1] is mapped to [-127, 127]</p>
    <p>+</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Our Quantization &amp; Batching Solution</p>
    <p>-1</p>
    <p>-126</p>
    <p>+1</p>
    <p>-7</p>
    <p>BatchCrypt</p>
    <p>-1</p>
    <p>z bit padding r bit value</p>
    <p>original value</p>
    <p>quantized value sign bit</p>
    <p>Customized quantization for aggregation  Signed integer  Symmetric range  Uniform quantization</p>
    <p>Challenges: 1. Differentiate overflows:</p>
    <p>two sign bits</p>
    <p>+</p>
    <p>=</p>
  </div>
  <div class="page">
    <p>Gradient Clipping</p>
    <p>Gradients are unbounded Quantization range is bounded Clipping is required Tradeoff:</p>
    <p>Smaller  Higher resolution within ||</p>
    <p>More diminished range information</p>
  </div>
  <div class="page">
    <p>Gradient Clipping</p>
    <p>Gradients are unbounded quantization range is bounded Clipping is required q Profiling quantization loss with a sample dataset [10]</p>
    <p>FL has non-iid data  Gradients range diminishes during training: optimal shifts</p>
    <p>q Analytical clipping with an online model  Model the noises with distribution fitting  Flexible &amp; adaptable</p>
    <p>[10] http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf</p>
  </div>
  <div class="page">
    <p>dACIQ: Analytical Gradient Clipping</p>
    <p>Gradients distribu^on is bell-shaped: Gaussian like  Conven^onal gaussian fibng: MLE, BI</p>
    <p>Requires a lot of informaVon  ComputaVonally intensive</p>
    <p>dACIQ proposes a Gaussian Fibng method for distributed dataset o Only requires max, min, and size o ComputaVonally efficient: online o Stochas5c Rounding [11] o Layer-wise quanVzaVon</p>
    <p>[11] Banner, Ron, Yury Nahshan, and Daniel Soudry. &quot;Post training 4-bit quantization of convolutional networks for rapiddeployment.&quot; Advances in Neural Information Processing Systems. 2019.</p>
  </div>
  <div class="page">
    <p>Introducing BatchCrypt</p>
    <p>Built atop FATE v1.1  Support TensorFlow, MXNet, and extendable to</p>
    <p>other frameworks  Implemented in Python  Utilize Joblib, Numba for maximum parallelism</p>
    <p>Client Worker</p>
    <p>ML backend TensorFlow</p>
    <p>FATE HE Mgr. Comm. Mgr.</p>
    <p>BatchCrypt dACIQ Quantizer</p>
    <p>Dist. Fitting</p>
    <p>Initializer Encrypt</p>
    <p>Remote GetMXNet</p>
    <p>Advance Scaler Quantize / Dequantize</p>
    <p>Encode / Decode Numba Parallel</p>
    <p>Batch / Unbatch Joblib Parallel</p>
    <p>Clipping</p>
    <p>BatchCrypt</p>
  </div>
  <div class="page">
    <p>Evaluations Setup</p>
    <p>Model Type Network Weights</p>
    <p>FMNIST Image Classification 3-layer-FC 101.77K</p>
    <p>CIFAR Image Classification AlexNet 1.25M</p>
    <p>LSTM-ptb Text Generation LSTM 4.02M</p>
    <p>Test Models</p>
    <p>Test Bed</p>
    <p>o AWS o Cluster of 10, spanning 5 locations o C5.4xlarge instances (16 vCPUs, 32 GB memory)</p>
    <p>Region US W. Tokyo US E. London HK</p>
    <p>Up (Mbps) 9841 116 165 97 81</p>
    <p>Down (Mbps) 9842 122 151 84 84</p>
    <p>Bandwidth from clients to aggregator</p>
  </div>
  <div class="page">
    <p>BatchCrypts Quantization Quality</p>
    <p>FMNIST test accuracy</p>
    <p>- Negligible loss</p>
    <p>- Quantization sometimes outperforms plain: randomness adds regularization</p>
    <p>CIFAR test accuracy</p>
    <p>LSTM loss</p>
  </div>
  <div class="page">
    <p>BatchCrypts Effectiveness: Computation</p>
    <p>client</p>
    <p>Iteration time breakdown of LSTM</p>
    <p>aggregator</p>
    <p>- Compared with stock FATE - Batch size set to 100 - 16 bit quantization</p>
    <p>- 23.3X for FMNIST - 70.8X for CIFAR - 92.8X for LSTM</p>
    <p>Larger the model, beier the results</p>
  </div>
  <div class="page">
    <p>BatchCrypts Effectiveness: Communication</p>
    <p>time</p>
    <p>Network traffic consumed by communication per iteration</p>
    <p>traffic</p>
    <p>- Compared with stock FATE - Batch size set to 100 - 16 bit quantization</p>
    <p>- 66X for FMNIST - 71X for CIFAR - 101X for LSTM</p>
  </div>
  <div class="page">
    <p>BatchCrypts Overhead</p>
    <p>time</p>
    <p>Time and traffic per iteration</p>
    <p>traffic</p>
    <p>- Compared with plain distributed training without encryption</p>
    <p>- Batch size set to 100 - 16 bit quantization</p>
    <p>- Overhead significantly reduced</p>
    <p>- Practical to deploy</p>
    <p>Feasible to train large models now</p>
  </div>
  <div class="page">
    <p>BatchCrypts Effectiveness: Convergence</p>
    <p>Total Vme and communicaVon unVl convergence</p>
    <p>Model Mode Epochs Acc. /Loss Time (h) Traffic (GB)</p>
    <p>FMNIST stock 40 88.62% 122.5 2228.3 batch 68 88.37% 8.9 58.7 plain 40 88.62% 3.2 11.17</p>
    <p>CIFAR stock 285 73.79% 9495.6 16422.0 batch 279 74.04% 131.3 227.8 plain 285 73.79% 34.2 11.39</p>
    <p>LSTM stock 20 0.0357 8484.4 15347.3 batch 23 0.0335 105.2 175.9 plain 20 0.0357 12.3 10.4</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Characterized HE enabled cross-silo FL  Designed an efficient HE batching scheme BatchCrypt o Codesigning quantization, coding, &amp; batching o Online analytical clipping dACIQ</p>
    <p>Implemented, and evaluated it on AWS o Up to 99% cost reduction</p>
  </div>
  <div class="page">
    <p>Thank you for coming!</p>
    <p>BatchCrypt is open sourced at https://github.com/marcoszh/BatchCrypt</p>
    <p>Find me</p>
    <p>hVps://marcoszh.github.io/ GraduaWng soon &amp; seeking opportuniWes</p>
  </div>
</Presentation>
