<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>The Structure of the Worst Noise in Gaussian Vector Broadcast Channels</p>
    <p>Wei Yu</p>
    <p>University of Toronto</p>
    <p>March 19, 2003</p>
    <p>DIMACS Workshop on Network Information Theory</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Sum capacity of Gaussian vector broadcast channels.</p>
    <p>Complete characterization of the worst-noise.</p>
    <p>Efficient numerical solution for the dual channel.</p>
    <p>Does duality extend beyond the power constrained channels?</p>
    <p>DIMACS Workshop on Network Information Theory 1</p>
  </div>
  <div class="page">
    <p>Gaussian Vector Broadcast Channel</p>
    <p>Non-degraded broadcast channel:</p>
    <p>Zn</p>
    <p>Xn Y n1</p>
    <p>Y nK</p>
    <p>H</p>
    <p>W1  2nR1</p>
    <p>WK  2nRK</p>
    <p>W1(Y n1 )</p>
    <p>WK(Y nK)</p>
    <p>Capacity region is still unknown.</p>
    <p>Sum capacity C = max{R1 +    + RK} is recently solved.</p>
    <p>DIMACS Workshop on Network Information Theory 2</p>
  </div>
  <div class="page">
    <p>Martons Achievability Region</p>
    <p>For a broadcast channel p(y1, y2|x):</p>
    <p>R1  I(U1; Y1) R2  I(U2; Y2)</p>
    <p>R1 + R2  I(U1; Y1) + I(U2; Y2)  I(U1; U2)</p>
    <p>for some auxiliary random variables p(u1, u2)p(x|u1, u2).</p>
    <p>For the Gaussian broadcast channel: I(U2; Y2)  I(U1; U2) is achieved with precoding.</p>
    <p>DIMACS Workshop on Network Information Theory 3</p>
  </div>
  <div class="page">
    <p>Writing on Dirty Paper</p>
    <p>Gaussian Channel ... with Transmitter Side Information</p>
    <p>Z  N (0, Szz) Z  N (0, Szz)S  N (0, Sss)</p>
    <p>XX YY</p>
    <p>C = 1 2</p>
    <p>log |Sxx + Szz|</p>
    <p>|Szz| C =</p>
    <p>log |Sxx + Szz|</p>
    <p>|Szz|</p>
    <p>Capacities are the same if S is known non-causally at the transmitter.</p>
    <p>C = max p(u,x|s)</p>
    <p>I(U ; Y )  I(U ; S) = max p(x)</p>
    <p>I(X; Y |S)</p>
    <p>DIMACS Workshop on Network Information Theory 4</p>
  </div>
  <div class="page">
    <p>Precoding for the Broadcast Channel</p>
    <p>W1  2nR1</p>
    <p>W2  2nR2</p>
    <p>H1</p>
    <p>H2</p>
    <p>Xn1 (W1, X n 2 )</p>
    <p>Xn2 (W2)</p>
    <p>Xn</p>
    <p>Zn1</p>
    <p>Zn2</p>
    <p>Y n1</p>
    <p>Y n2</p>
    <p>W1(Y n 1 )</p>
    <p>W2(Y n 2 )</p>
    <p>R1 = I(X1; Y1|X2) = 1 2</p>
    <p>log |H1S1HT1 + Sz1z1|</p>
    <p>|Sz1z1|</p>
    <p>R2 = I(X2; Y2) = 1 2</p>
    <p>log |H2S2HT2 + H2S1HT2 + Sz2z2|</p>
    <p>|H2S1HT2 + Sz2z2|</p>
    <p>DIMACS Workshop on Network Information Theory 5</p>
  </div>
  <div class="page">
    <p>Converse: Satos Outer Bound</p>
    <p>Broadcast capacity does not depend on noise correlation: Sato (78).</p>
    <p>x1x1x1</p>
    <p>x2x2x2</p>
    <p>y1 y1y1</p>
    <p>y2 y2y2</p>
    <p>z1</p>
    <p>z2</p>
    <p>z1z  1</p>
    <p>z2 z  2=</p>
    <p>if</p>
    <p>{ p(z1) = p(z1) p(z2) = p(z2)</p>
    <p>, not necessarily p(z1, z2) = p(z1, z  2).</p>
    <p>So, sum capacity C  min Szz</p>
    <p>max Sxx</p>
    <p>I(X; Y).</p>
    <p>DIMACS Workshop on Network Information Theory 6</p>
  </div>
  <div class="page">
    <p>Three Proofs of the Sum Capacity Result</p>
    <p>DIMACS Workshop on Network Information Theory 7</p>
  </div>
  <div class="page">
    <p>DFE Approach</p>
    <p>x</p>
    <p>z</p>
    <p>H HT</p>
    <p>feedforward filter</p>
    <p>1GT Decision</p>
    <p>I  G</p>
    <p>Decision-feedback at the receiver is equivalent to transmitter precoding.</p>
    <p>(Non-Singular) Worst Noise  Diagonal feedforward filter</p>
    <p>Fix Sxx, min Szz</p>
    <p>I(X; Y) is achievable.</p>
    <p>DIMACS Workshop on Network Information Theory 8</p>
  </div>
  <div class="page">
    <p>Uplink-Downlink Duality Approach</p>
    <p>X1 X2Y1 Y2</p>
    <p>Z1  N (0, Q) Z2  N (0, I)</p>
    <p>E[XT1 X1]  P E[X T 2 QX2]  P</p>
    <p>H HT</p>
    <p>Uplink and downlink channels are duals.  The noise covariance and input constraint are duals.  Worst-noise gives an input constraint that decouples the inputs.</p>
    <p>C = max Sxx</p>
    <p>min Szz</p>
    <p>I(X; Y)</p>
    <p>DIMACS Workshop on Network Information Theory 9</p>
  </div>
  <div class="page">
    <p>Convex Duality Approach</p>
    <p>X</p>
    <p>X1</p>
    <p>X2</p>
    <p>Y1</p>
    <p>Y2</p>
    <p>Y</p>
    <p>Z1</p>
    <p>Z2</p>
    <p>Z</p>
    <p>P P</p>
    <p>H1</p>
    <p>H2</p>
    <p>HT1</p>
    <p>HT2</p>
    <p>Satos bound: C  min Szz</p>
    <p>max Sxx</p>
    <p>I(X; Y).</p>
    <p>Broadcast/Multiple-Access duality: C  max Sxx</p>
    <p>I(X; Y).</p>
    <p>Convex duality: max Sxx</p>
    <p>min Szz</p>
    <p>I(X; Y) = max Sxx</p>
    <p>I(X; Y).</p>
    <p>DIMACS Workshop on Network Information Theory 10</p>
  </div>
  <div class="page">
    <p>Objective</p>
    <p>Completely characterize the worst-noise.</p>
    <p>Duality through minimax.  Worst-noise through duality.</p>
    <p>Efficient numerical solution for the dual channel.</p>
    <p>Does duality extend beyond the power constrained channel?</p>
    <p>DIMACS Workshop on Network Information Theory 11</p>
  </div>
  <div class="page">
    <p>Minimax Capacity</p>
    <p>Gaussian vector broadcast channel sum capacity is the solution of</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| subject to tr(Sxx)  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>]</p>
    <p>Sxx, Szz  0</p>
    <p>The minimax problem is convex in Szz, concave in Sxx.</p>
    <p>How to solve this minimax problem?</p>
    <p>DIMACS Workshop on Network Information Theory 12</p>
  </div>
  <div class="page">
    <p>Duality through Minimax</p>
    <p>Two KKT conditions must be satisfied simultaneously:</p>
    <p>HT (HSxxHT + Szz)1H = I</p>
    <p>S1zz  (HSxxHT + Szz)1 = [</p>
    <p>1 0 0 2</p>
    <p>]</p>
    <p>For the moment, assume that H is invertible.</p>
    <p>HT S1zz H  I = H T H</p>
    <p>H(HT H + I)1HT = Szz</p>
    <p>This is a water-filling condition for the dual channel.</p>
    <p>DIMACS Workshop on Network Information Theory 13</p>
  </div>
  <div class="page">
    <p>Power Constraint in the Dual Channel</p>
    <p>Interpretation of dual variable:  = C</p>
    <p>P , i =</p>
    <p>C</p>
    <p>Szizi .</p>
    <p>Thus, capacity is preserved if P =</p>
    <p>(</p>
    <p>i</p>
    <p>i</p>
    <p>) Szizi</p>
    <p>Capacity C = min max 1 2</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| .</p>
    <p>Thus, capacity is preserved if P P</p>
    <p>= Szizi</p>
    <p>Therefore,</p>
    <p>i i</p>
    <p>= P .</p>
    <p>DIMACS Workshop on Network Information Theory 14</p>
  </div>
  <div class="page">
    <p>Construct the Dual Channel</p>
    <p>KKT condition: H(HT DH + I)1HT = 1  Szz</p>
    <p>where D = / is diagonal, trace(D) =</p>
    <p>i i/ = P .</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>] . Thus, constraint on D: trace(D1) + trace(D2)  P .</p>
    <p>X1</p>
    <p>X2</p>
    <p>Y</p>
    <p>Z E[X1X</p>
    <p>T 1 ] = D1</p>
    <p>E[X2X T 2 ] = D2</p>
    <p>trace(D1) + trace(D2)  P</p>
    <p>HT1</p>
    <p>HT2</p>
    <p>DIMACS Workshop on Network Information Theory 15</p>
  </div>
  <div class="page">
    <p>Yet Another Derivation for Duality</p>
    <p>The duality between broadcast channel and multiple-access channel:</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| max</p>
    <p>D</p>
    <p>log |HT DH + I|</p>
    <p>|I| s.t. tr(Sxx)  P s.t. tr(D)  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>] D is diagonal</p>
    <p>Sxx, Szz  0 D  0</p>
    <p>KKT conditions for minimax = KKT condition for max.</p>
    <p>DIMACS Workshop on Network Information Theory 16</p>
  </div>
  <div class="page">
    <p>Worst-Noise Through Minimax</p>
    <p>Solve the dual multiple access channel problem with power constraint P . Obtain (, ). Then:</p>
    <p>Szz = H(H T H + I)1HT</p>
    <p>Sxx = (I) 1  (HT H + I)1</p>
    <p>What if H is not invertible, or Szz is singular?</p>
    <p>DIMACS Workshop on Network Information Theory 17</p>
  </div>
  <div class="page">
    <p>Decision-Feedback Equalization with Singular Noise</p>
    <p>With non-singular noise: S1zz  (HSxxH T + Szz)</p>
    <p>1 = [</p>
    <p>1 0 0 2</p>
    <p>] .</p>
    <p>If H is low-rank, Szz can be singular.</p>
    <p>X H</p>
    <p>Z</p>
    <p>m-dimensional m  n n &gt; m</p>
    <p>Linear Estimation/DFE</p>
    <p>is not unique if |Sz| = 0.</p>
    <p>DIMACS Workshop on Network Information Theory 18</p>
  </div>
  <div class="page">
    <p>Necessary and Sufficient Condition for Diagonalization</p>
    <p>Suppose that the worst-noise |Szz| = 0, let</p>
    <p>Szz = U SzzU T ,</p>
    <p>where Szz is n  n, Szz is m  m, m &lt; n.</p>
    <p>It is always possible to write H = U H.</p>
    <p>There exists a DFE with diagonal feedforward filter if and only if</p>
    <p>S1zz  (HSxxH T + Szz)</p>
    <p>1 = U T [</p>
    <p>1 0 0 2</p>
    <p>] U</p>
    <p>DIMACS Workshop on Network Information Theory 19</p>
  </div>
  <div class="page">
    <p>Singular Worst-Noise</p>
    <p>It can be verified that the diagonalization condition is satisfied by:</p>
    <p>S(0)zz = H(H T H + I)1HT</p>
    <p>Sxx = (I) 1  (HT H + I)1</p>
    <p>However: S(0)zz does not necessarily have 1s on the diagonal.</p>
    <p>S(0)zz =</p>
    <p>I ? ? ? I ? ? ? ?</p>
    <p>.</p>
    <p>DIMACS Workshop on Network Information Theory 20</p>
  </div>
  <div class="page">
    <p>Characterization of the Worst-Noise</p>
    <p>Theorem 1. The following steps solve the worst noise in y = Hx + z:</p>
    <p>Sxx = (I)1  (HT H + I)1.</p>
    <p>I ? ? ? I ? ? ? ?</p>
    <p>+</p>
    <p>=</p>
    <p>I ? ? ? I ? ? ? I</p>
    <p>.</p>
    <p>DIMACS Workshop on Network Information Theory 21</p>
  </div>
  <div class="page">
    <p>Worst-Noise is Not Unique</p>
    <p>The same Sxx water-fills the entire class of S (0) zz + Szz.</p>
    <p>S(0)zz + [</p>
    <p>] = [U|U ]</p>
    <p>([ Szz 0 0 0</p>
    <p>] + [</p>
    <p>S11 S  12</p>
    <p>S21 S  22</p>
    <p>]) [U|U ]T ,</p>
    <p>where S11  S  12S</p>
    <p>1 22 S</p>
    <p>21 = 0.</p>
    <p>The entire class of worst-noise is related by linear estimation:</p>
    <p>E[z + z1|z  2] = z.</p>
    <p>The class of (Sxx, Szz) that satisfies the KKT condition is precisely:</p>
    <p>(Sxx, S (0) zz + S</p>
    <p>zz)</p>
    <p>DIMACS Workshop on Network Information Theory 22</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Complete characterization of the worst-noise.</p>
    <p>Duality through minimax.  Worst-noise through duality.</p>
    <p>Efficient numerical solution for the dual channel.</p>
    <p>Does duality extend beyond the power constrained channel?</p>
    <p>DIMACS Workshop on Network Information Theory 23</p>
  </div>
  <div class="page">
    <p>Sum Power Gaussian Vector Multiple Access Channel</p>
    <p>X1</p>
    <p>X2</p>
    <p>H1</p>
    <p>H2</p>
    <p>Z</p>
    <p>Y</p>
    <p>P</p>
    <p>max Sxx</p>
    <p>log |HT SxxH + I|</p>
    <p>s.t. tr(Sxx)  P Sxx is diagonal</p>
    <p>Sxx  0</p>
    <p>An efficient way to find the worst-noise is to solve the dual problem.</p>
    <p>Previous numerical solution: Jindal, Jafar, Vishwanath, Goldsmith.</p>
    <p>DIMACS Workshop on Network Information Theory 24</p>
  </div>
  <div class="page">
    <p>Iterative Water-filling</p>
    <p>Iterative water-filling: Optimize each of Si while fixing all others.</p>
    <p>max Si</p>
    <p>log</p>
    <p>i</p>
    <p>HiSiH T i + I</p>
    <p>maxSi 1 2</p>
    <p>log</p>
    <p>i</p>
    <p>HiSiH T i + I</p>
    <p>s.t. tr(Si)  Pi s.t.</p>
    <p>i</p>
    <p>tr(Si)  P</p>
    <p>Si  0 Si  0</p>
    <p>Individual Constraints Coupled Constraint</p>
    <p>Iterative water-filling only works with the individual power constraints.</p>
    <p>DIMACS Workshop on Network Information Theory 25</p>
  </div>
  <div class="page">
    <p>Dual Decomposition for the Sum-Power Problem</p>
    <p>Take Lagrangian dual with respect to the coupled constraint only:</p>
    <p>max 1 2</p>
    <p>log</p>
    <p>i</p>
    <p>HiSiH T i + I</p>
    <p>g() = max 1 2</p>
    <p>log</p>
    <p>i</p>
    <p>HiSiH T i + I</p>
    <p>s.t.</p>
    <p>i</p>
    <p>Pi  P   (</p>
    <p>i</p>
    <p>Pi  P )</p>
    <p>tr(Si)  Pi s.t. tr(Si)  Pi Si  0 Si  0</p>
    <p>Sum Power Capacity = min &gt;0</p>
    <p>g()</p>
    <p>DIMACS Workshop on Network Information Theory 26</p>
  </div>
  <div class="page">
    <p>Iterative Water-filling for the Dual Problem</p>
    <p>By introducing a Lagrange multiplier , constraints are decoupled:</p>
    <p>g() = max Si</p>
    <p>log</p>
    <p>i</p>
    <p>HiSiH T i + I</p>
    <p>(</p>
    <p>i</p>
    <p>Pi  P</p>
    <p>)</p>
    <p>s.t. tr(Si)  Pi Si  0</p>
    <p>To solve g(): Iteratively optimize each of (Si, Pi).  To find min g() over  &gt; 0:</p>
    <p>Decrease  if</p>
    <p>i Pi &lt; P . Increase  if</p>
    <p>i Pi &gt; P .</p>
    <p>DIMACS Workshop on Network Information Theory 27</p>
  </div>
  <div class="page">
    <p>Convergence of the Dual Decomposition Algorithm</p>
    <p>3 transmit antennas</p>
    <p>50 receivers each with a single antenna</p>
    <p>typically 3-6 active</p>
    <p>i.i.d. Gaussian channel</p>
    <p>Bisection on . 0 20 40 60 80 100 120</p>
    <p>iterations</p>
    <p>su m</p>
    <p>c a</p>
    <p>p a</p>
    <p>ci ty</p>
    <p>( b</p>
    <p>its /t</p>
    <p>ra n</p>
    <p>sm is</p>
    <p>si o</p>
    <p>n )</p>
    <p>DIMACS Workshop on Network Information Theory 28</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Complete characterization of the worst-noise.</p>
    <p>Duality through minimax.  Worst-noise through duality.</p>
    <p>Efficient numerical solution for the dual channel.</p>
    <p>Does duality extend beyond the power constrained channel?</p>
    <p>DIMACS Workshop on Network Information Theory 29</p>
  </div>
  <div class="page">
    <p>Broadcast Channel under Linear Covariance Constraint</p>
    <p>The DFE achievability result works with any fixed Sxx.</p>
    <p>The capacity of the broadcast channel under covariance constraint:</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| subject to tr(QSxx)  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>]</p>
    <p>Sxx, Szz  0</p>
    <p>What is the duality result in this case?</p>
    <p>DIMACS Workshop on Network Information Theory 30</p>
  </div>
  <div class="page">
    <p>KKT Condition for Minimax</p>
    <p>Two KKT conditions must be satisfied simultaneously:</p>
    <p>HT (HSxxHT + Szz)1H = Q</p>
    <p>S1zz  (HSxxHT + Szz)1 = [</p>
    <p>1 0 0 2</p>
    <p>]</p>
    <p>For simplicity, assume invertible H.</p>
    <p>H(HT H + Q)1HT = Szz</p>
    <p>with</p>
    <p>i tr(i)</p>
    <p>= P</p>
    <p>DIMACS Workshop on Network Information Theory 31</p>
  </div>
  <div class="page">
    <p>Duality under Linear Covariance Constraint</p>
    <p>The duality between broadcast channel and multiple-access channel:</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| max</p>
    <p>D</p>
    <p>log |HT DH + Q|</p>
    <p>|Q| s.t. tr(QSxx)  P s.t. tr(D)  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>] D is diagonal</p>
    <p>Sxx, Szz  0 D  0</p>
    <p>The above two problems have the same KKT conditions.</p>
    <p>DIMACS Workshop on Network Information Theory 32</p>
  </div>
  <div class="page">
    <p>Generalized Duality</p>
    <p>X</p>
    <p>X1</p>
    <p>X2</p>
    <p>Y1</p>
    <p>Y2</p>
    <p>Y</p>
    <p>Z1</p>
    <p>Z2</p>
    <p>Z</p>
    <p>tr(SxxQ1)  P tr(SxxQ2)  PSzz  N (0, Q2) Szz  N (0, Q1)</p>
    <p>H1</p>
    <p>H2</p>
    <p>HT1</p>
    <p>HT2</p>
    <p>Q1: Input constraint in BC and Noise covariance in MAC. Q2: Worst noise covariance in BC and Input constraint in MAC.</p>
    <p>DIMACS Workshop on Network Information Theory 33</p>
  </div>
  <div class="page">
    <p>Broadcast Channel under Convex Covariance Constraint</p>
    <p>Under arbitrary convex constraint, DFE still works.</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| subject to f (Sxx)  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>]</p>
    <p>Sxx, Szz  0</p>
    <p>Does duality exist in this case?</p>
    <p>DIMACS Workshop on Network Information Theory 34</p>
  </div>
  <div class="page">
    <p>Duality under Convex Covariance Constraint</p>
    <p>Duality still exists, but the values of the dual variables are not known:</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| max</p>
    <p>D</p>
    <p>log |HT H + Q|</p>
    <p>|Q| s.t. f (Sxx)  P s.t. tr()  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>] D is diagonal</p>
    <p>Sxx, Szz  0 D  0</p>
    <p>Q = f (). But if f () is non-linear, tr() 6= P .</p>
    <p>DIMACS Workshop on Network Information Theory 35</p>
  </div>
  <div class="page">
    <p>Peak Power Constrained Broadcast Channel</p>
    <p>Duality exists, but not computationally useful. Need to solve minimax.</p>
    <p>max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz| max</p>
    <p>D</p>
    <p>log |HT H + Q|</p>
    <p>|Q| s.t. Sxx(i, i)  Pi s.t. tr()  P</p>
    <p>Szz = [</p>
    <p>I ? ? I</p>
    <p>] D is diagonal</p>
    <p>Sxx, Szz  0 D  0</p>
    <p>Here, Q =</p>
    <p>1 0 . . .</p>
    <p>. But, i, P  are not known.</p>
    <p>DIMACS Workshop on Network Information Theory 36</p>
  </div>
  <div class="page">
    <p>Concluding Remarks</p>
    <p>Sum capacity of a Gaussian vector broadcast channel is:</p>
    <p>C = max Sxx</p>
    <p>min Szz</p>
    <p>log |HSxxHT + Szz|</p>
    <p>|Szz|</p>
    <p>If the input constraint is a linear covariance constraint:</p>
    <p>C = max D</p>
    <p>log |HT DH + Q|</p>
    <p>|Q|</p>
    <p>Minimax is a more fundamental expression than duality.</p>
    <p>Duality, when exists, has computational advantage.</p>
    <p>DIMACS Workshop on Network Information Theory 37</p>
  </div>
</Presentation>
