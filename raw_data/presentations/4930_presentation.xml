<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Good Word Attacks on Statistical Spam Filters</p>
    <p>Daniel Lowd University of Washington</p>
    <p>(Joint work with Christopher Meek,</p>
    <p>Microsoft Research)</p>
  </div>
  <div class="page">
    <p>Content-based Spam Filtering</p>
    <p>cheap = 1.0 mortgage = 1.5</p>
    <p>Total score = 2.5</p>
    <p>From: spammer@example.com Cheap mortgage now!!!</p>
    <p>Feature Weights</p>
    <p>&gt; 1.0 (threshold)</p>
    <p>Spam</p>
  </div>
  <div class="page">
    <p>Good Word Attacks</p>
    <p>cheap = 1.0 mortgage = 1.5 Stanford = -1.0</p>
    <p>CEAS = -1.0 Total score = 0.5</p>
    <p>From: spammer@example.com Cheap mortgage now!!! Stanford CEAS</p>
    <p>Feature Weights</p>
    <p>&lt; 1.0 (threshold)</p>
    <p>OK</p>
  </div>
  <div class="page">
    <p>Can we efficiently find a list of good words?  Types of attacks</p>
    <p>Passive attacks -- no filter access  Active attacks -- test emails allowed</p>
    <p>Metrics  Expected number of words required to get median</p>
    <p>(blocked) spam past the filter  Number of query messages sent</p>
    <p>Playing the Adversary</p>
  </div>
  <div class="page">
    <p>Filter Configuration</p>
    <p>Models used  Nave Bayes: generative  Maximum Entropy (Maxent): discriminative</p>
    <p>Training  500,000 messages from Hotmail feedback loop  276,000 features  Maxent let 30% less spam through</p>
  </div>
  <div class="page">
    <p>Comparison of Filter Weights</p>
    <p>spammygood</p>
  </div>
  <div class="page">
    <p>Passive Attacks</p>
    <p>Heuristics  Select random dictionary words (Dictionary)  Select most frequent English words (Freq. Word)  Select highest ratio: English freq./spam freq. (Freq. Ratio)</p>
    <p>Spam corpus: spamarchive.org  English corpora:</p>
    <p>Reuters news articles  Written English  Spoken English  1992 USENET</p>
  </div>
  <div class="page">
    <p>Passive Attack Results</p>
  </div>
  <div class="page">
    <p>Active Attacks</p>
    <p>Learn which words are best by sending test messages (queries) through the filter</p>
    <p>First-N: Find n good words using as few queries as possible</p>
    <p>Best-N: Find the best n words</p>
  </div>
  <div class="page">
    <p>First-N Attack Step 1: Find a Barely spam message</p>
    <p>Threshold</p>
    <p>Legitimate Spam</p>
    <p>Barely spam</p>
    <p>Hi, mom! Cheap mortgage now!!!</p>
    <p>Barely legit.</p>
    <p>mortgage now!!!</p>
    <p>now!!!</p>
    <p>Original spam</p>
    <p>Original legit.</p>
  </div>
  <div class="page">
    <p>First-N Attack Step 2: Test each word</p>
    <p>Threshold</p>
    <p>Legitimate Spam</p>
    <p>Good words Barely spam message</p>
    <p>Less good words</p>
  </div>
  <div class="page">
    <p>Best-N Attack</p>
    <p>Key idea: use spammy words to sort the good words.</p>
    <p>Threshold</p>
    <p>Legitimate Spam Better</p>
    <p>Worse</p>
  </div>
  <div class="page">
    <p>Active Attack Results (n = 100)</p>
    <p>Best-N twice as effective as First-N  Maxent more vulnerable to active attacks  Active attacks much more effective than</p>
    <p>passive attacks</p>
  </div>
  <div class="page">
    <p>Defenses</p>
    <p>Add noise or vary threshold  Intentionally reduces accuracy  Easily defeated by sampling techniques</p>
    <p>Language model  Easily defeated by selecting passages  Easily defeated by similar language models</p>
    <p>Frequent retraining with case amplification  Completely negates attack effectiveness  No accuracy loss on original spam  See paper for more details</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Effective attacks do not require filter access.  Given filter access, even more effective</p>
    <p>attacks are possible.  Frequent retraining is a promising defense.</p>
    <p>See also: Lowd &amp; Meek, Adversarial Learning, KDD 2005</p>
  </div>
</Presentation>
