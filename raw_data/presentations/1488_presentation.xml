<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Parity Logging with Reserved Space: Towards Efficient Updates and Recovery in Erasure-coded</p>
    <p>Clustered Storage</p>
    <p>Jeremy C. W. Chan*, Qian Ding*, Patrick P. C. Lee, Helen H. W. Chan</p>
    <p>The Chinese University of Hong Kong</p>
    <p>FAST14</p>
    <p>The first two authors contributed equally to this work.</p>
  </div>
  <div class="page">
    <p>Clustered storage systems provide scalable storage by striping data across multiple nodes</p>
    <p>e.g., GFS, HDFS, Azure, Ceph, Panasas, Lustre, etc.</p>
    <p>Maintain data availability with redundancy</p>
    <p>Replication</p>
    <p>Erasure coding</p>
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>With explosive data growth, enterprises move to erasure-coded storage to save footprints and cost</p>
    <p>e.g., 3-way replication has 200% overhead; erasure coding can reduce overhead to 33% [Huang, ATC12]</p>
    <p>Erasure coding recap:</p>
    <p>Encodes data chunks to create parity chunks</p>
    <p>Any subset of data/parity chunks can recover original data chunks</p>
    <p>Erasure coding introduces two challenges: (1) updates and (2) recovery/degraded reads</p>
    <p>Motivation</p>
  </div>
  <div class="page">
    <p>When a data chunk is updated, its encoded parity chunks need to be updated</p>
    <p>Two update approaches:</p>
    <p>In-place updates: overwrites existing chunks</p>
    <p>Log-based updates: appends changes</p>
    <p>Challenges</p>
  </div>
  <div class="page">
    <p>Failures are common</p>
    <p>Data may be permanently loss due to crashes</p>
    <p>90% of failures are transient (e.g., reboots, power loss, network connectivity loss, stragglers) [Ford, OSDI10]</p>
    <p>Recovery/degraded read approach:</p>
    <p>Reads enough data and parity chunks</p>
    <p>Reconstructs lost/unavailable chunks</p>
    <p>Challenges</p>
  </div>
  <div class="page">
    <p>How to achieve both efficient updates and fast recovery in clustered storage systems?</p>
    <p>Target scenario:</p>
    <p>Server workloads with frequent updates</p>
    <p>Commodity configurations with frequent failures</p>
    <p>Disk-based storage</p>
    <p>Potential bottlenecks in clustered storage systems</p>
    <p>Network I/O</p>
    <p>Disk I/O</p>
    <p>Challenges</p>
  </div>
  <div class="page">
    <p>Propose parity-logging with reserved space</p>
    <p>Uses hybrid in-place and log-based updates</p>
    <p>Puts deltas in a reserved space next to parity chunks to mitigate disk seeks</p>
    <p>Predicts and reclaims reserved space in workload-aware manner</p>
    <p>Achieves both efficient updates and fast recovery</p>
    <p>Build a clustered storage system prototype CodFS</p>
    <p>Incorporates different erasure coding and update schemes</p>
    <p>Released as open-source software</p>
    <p>Conduct extensive trace-driven testbed experiments</p>
    <p>Our Contributions</p>
  </div>
  <div class="page">
    <p>MSR Cambridge traces</p>
    <p>Block-level I/O traces captured by Microsoft Research Cambridge in 2007</p>
    <p>36 volumes (179 disks) on 13 servers</p>
    <p>Workloads including home directories and project directories</p>
    <p>Harvard NFS traces (DEAS03)</p>
    <p>NFS requests/responses of a NetApp file server in 2003</p>
    <p>Mixed workloads including email, research and development</p>
    <p>Background: Trace Analysis</p>
  </div>
  <div class="page">
    <p>MSR Trace Analysis</p>
    <p>Distribution of update size in 10 volumes of MSR Cambridge traces</p>
    <p>Updates are small</p>
    <p>All updates are smaller than 512KB</p>
    <p>8 in 10 volumes show more than 60% of tiny updates (&lt;4KB)</p>
  </div>
  <div class="page">
    <p>Updates are intensive</p>
    <p>9 in 10 volumes show more than 90% update writes over all writes</p>
    <p>Update coverage varies</p>
    <p>Measured by the fraction of WSS that is updated at least once throughout the trace period</p>
    <p>Large variation among different workloads  need a dynamic algorithm for handling updates</p>
    <p>Similar observations for Harvard traces</p>
    <p>MSR Trace Analysis</p>
  </div>
  <div class="page">
    <p>Objective #1: Efficient handling of small, intensive updates in an erasure-coded clustered storage</p>
  </div>
  <div class="page">
    <p>Make use of linearity of erasure coding</p>
    <p>CodFS reduces network traffic by only sending parity delta</p>
    <p>Question: How to save data update (A) and parity delta (A) on disk?</p>
    <p>Saving Network Traffic in Parity Updates</p>
    <p>A B C P</p>
    <p>Linear combination with some encoding</p>
    <p>coefficient</p>
    <p>A</p>
    <p>B C P</p>
    <p>Update A</p>
    <p>P is equivalent to</p>
    <p>applying the same encoding</p>
    <p>coefficient</p>
    <p>parity delta (A)</p>
    <p>A P</p>
  </div>
  <div class="page">
    <p>Used in host-based file systems (e.g., NTFS and ext4)</p>
    <p>Also used for parity updates in RAID systems</p>
    <p>Update Approach #1: in-place updates (overwrite)</p>
    <p>A B C Disk B C Disk</p>
    <p>Update A</p>
    <p>Problem: significant I/O to read and update parities</p>
  </div>
  <div class="page">
    <p>Used by most clustered storage systems (e.g. GFS, Azure)</p>
    <p>Original concept from log-structured file system (LFS)</p>
    <p>Convert random writes to sequential writes</p>
    <p>Update Approach #2: log-based updates (logging)</p>
    <p>A B C Disk B C Disk</p>
    <p>Update A</p>
    <p>A</p>
    <p>Problem: fragmentation to chunk A</p>
  </div>
  <div class="page">
    <p>Objective #2: Preserves sequentiality in large read (e.g. recovery) for both data and parity chunks</p>
  </div>
  <div class="page">
    <p>Data Update Parity Delta</p>
    <p>Full-overwrite (FO) O O</p>
    <p>Full-logging (FL) L L</p>
    <p>Parity-logging (PL) O L</p>
    <p>Parity-logging with reserved space (PLR)</p>
    <p>O L</p>
    <p>Parity Update Schemes</p>
    <p>O: Overwrite L: Logging</p>
    <p>Our Proposal</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a b</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a b a</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>a b c d a</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>b b</p>
    <p>b</p>
    <p>b</p>
    <p>a b c d a b</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>b c c</p>
    <p>c</p>
    <p>c</p>
    <p>a b c d a b c</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b a b</p>
    <p>b</p>
    <p>b</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
  </div>
  <div class="page">
    <p>Parity Update Schemes</p>
    <p>Storage Node 1 Storage Node 2 Storage Node 3</p>
    <p>FO</p>
    <p>FL</p>
    <p>PL</p>
    <p>PLR</p>
    <p>Data stream</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a</p>
    <p>a b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>b</p>
    <p>a</p>
    <p>c d a</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>d</p>
    <p>b</p>
    <p>b</p>
    <p>c</p>
    <p>c</p>
    <p>FL: disk seek for chunk b</p>
    <p>FL&amp;PL: disk seek for parity chunk b</p>
    <p>PLR: No seeks for both data</p>
    <p>and parity</p>
    <p>FO: extra read for merging</p>
    <p>parity</p>
    <p>a</p>
    <p>a</p>
    <p>c</p>
    <p>c</p>
    <p>c</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b</p>
    <p>a+b a b</p>
    <p>b</p>
    <p>b</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
    <p>c+d</p>
  </div>
  <div class="page">
    <p>Implementation - CodFS</p>
    <p>CodFS Architecture  Exploits parallelization</p>
    <p>across nodes and within each node</p>
    <p>Provides a file system interface based on FUSE</p>
    <p>OSD: Modular Design</p>
  </div>
  <div class="page">
    <p>Testbed: 22 nodes with commodity hardware</p>
    <p>12-node storage cluster</p>
    <p>10 client nodes sending</p>
    <p>Connected via a Gigabit switch</p>
    <p>Experiments</p>
    <p>Baseline tests</p>
    <p>Show CodFS can achieve theoretical throughput</p>
    <p>Synthetic workload evaluation</p>
    <p>Real-world workload evaluation</p>
    <p>Experiments</p>
    <p>Focus of this talk</p>
  </div>
  <div class="page">
    <p>Synthetic Workload Evaluation</p>
    <p>Random Write Logging parity (FL, PL, PLR) helps random writes by saving disk seeks and parity read overhead FO has 20% less IOPS than others</p>
    <p>IOZone record length: 128KB RDP coding (6,4)</p>
  </div>
  <div class="page">
    <p>Synthetic Workload Evaluation</p>
    <p>Sequential Read Recovery</p>
    <p>No seeks in recovery for FO</p>
    <p>and PLR</p>
    <p>Only FL needs disk seeks in reading</p>
    <p>data chunk</p>
    <p>merge overhead</p>
  </div>
  <div class="page">
    <p>Fixing Storage Overhead</p>
    <p>PLR (6,4)</p>
    <p>FO/FL/PL (8,6)</p>
    <p>FO/FL/PL (8,4)</p>
    <p>Data Chunk</p>
    <p>Parity Chunk</p>
    <p>Reserved Space</p>
    <p>FO (8,6) is still 20% slower than PLR (6,4) in random writes</p>
    <p>PLR and FO are still much faster than FL and PL</p>
    <p>Random Write Recovery</p>
  </div>
  <div class="page">
    <p>Remaining problem</p>
    <p>What is the appropriate reserved space size?  Too small  frequent merges</p>
    <p>Too large  waste of space</p>
    <p>Can we shrink the reserved space if it is not used?</p>
    <p>Baseline approach</p>
    <p>Fixed reserved space size</p>
    <p>Workload-aware management approach</p>
    <p>Predict: exponential moving average to guess reserved space size</p>
    <p>Shrink: release unused space back to system</p>
    <p>Merge: merge all parity deltas back to parity chunk</p>
    <p>Dynamic Resizing of Reserved Space</p>
  </div>
  <div class="page">
    <p>Dynamic Resizing of Reserved Space</p>
    <p>Step 1: Compute utility using past workload pattern</p>
    <p>Step 2: Compute utility using past workload pattern</p>
    <p>smoothing factor</p>
    <p>current usage</p>
    <p>previous usage</p>
    <p>no. of chunk to shrink</p>
    <p>Step 3: Perform shrink</p>
    <p>disk</p>
    <p>disk</p>
    <p>shrink</p>
    <p>disk</p>
    <p>write new data chunks</p>
    <p>shrinking reserved space as a multiple of chunk size avoids</p>
    <p>creating unusable holes</p>
  </div>
  <div class="page">
    <p>Dynamic Resizing of Reserved Space</p>
    <p>Shrink + merge performs a merge after the daily shrinking</p>
    <p>Shrink only performs shrinking at 00:00 and 12:00 each day</p>
    <p>*(10,8) Cauchy RS Coding with 16MB segments</p>
    <p>Reserved space overhead under different shrink strategies in Harvard trace</p>
  </div>
  <div class="page">
    <p>Penalty of Over-shrinking</p>
    <p>Less than 1% of writes are stalled by a merge operation</p>
    <p>Penalty of inaccurate prediction</p>
    <p>Average number of merges per 1000 writes under different shrink strategies in the Harvard trace</p>
    <p>*(10,8) Cauchy RS Coding with 16MB segments</p>
  </div>
  <div class="page">
    <p>Latency analysis</p>
    <p>Metadata management</p>
    <p>Consistency / locking</p>
    <p>Applicability to different workloads</p>
    <p>Open Issues</p>
  </div>
  <div class="page">
    <p>Key idea: Parity logging with reserved space</p>
    <p>Keep parity updates next to parity chunks to reduce disk seeks</p>
    <p>Workload aware scheme to predict and adjust the reserved space size</p>
    <p>Build CodFS prototype that achieves efficient updates and fast recovery</p>
    <p>Source code: http://ansrlab.cse.cuhk.edu.hk/software/codfs</p>
    <p>Conclusions</p>
  </div>
  <div class="page">
    <p>Backup</p>
  </div>
  <div class="page">
    <p>MSR Cambridge Traces Replay</p>
    <p>Update Throughput</p>
    <p>PLR ~ PL ~ FL &gt;&gt; FO</p>
  </div>
  <div class="page">
    <p>MSR Cambridge Traces Replay</p>
    <p>Recovery Throughput</p>
    <p>PLR ~ FO &gt;&gt; FL ~ PL</p>
  </div>
</Presentation>
