<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Falcon: Scaling IO Performance in Multi-SSD Volumes</p>
    <p>Pradeep Kumar H Howie Huang</p>
    <p>The George Washington University</p>
  </div>
  <div class="page">
    <p>SSDs in Big Data Applications  Recent trends advocate using many SSDs for higher throughput in</p>
    <p>Graph Analytics</p>
    <p>Machine Learning</p>
    <p>Key-Value stores, etc.</p>
    <p>New techniques are taking advantage of high random IOPS of SSDs  Fine grained IOs in graph processing [FAST17]</p>
    <p>Doing random IOs in graph processing [ATC16]</p>
    <p>Range scan in WiscKey is many parallel random IOs [FAST16]</p>
    <p>Increasing use of batched IO interfaces such as libaio in Linux</p>
  </div>
  <div class="page">
    <p>Existing IO Model</p>
    <p>IO Performance High Low</p>
    <p>P ro</p>
    <p>g ra</p>
    <p>m m</p>
    <p>in g</p>
    <p>C o</p>
    <p>m p</p>
    <p>le xi</p>
    <p>ty</p>
    <p>Low</p>
    <p>High</p>
    <p>Kernel-managed IO (1 application IO thread)</p>
    <p>Application-managed IO (1 application IO thread per-SSD)</p>
    <p>Falcon (1 application IO thread)</p>
    <p>Kernel-managed IO (many application IO threads)</p>
    <p>(a) Application-managed</p>
    <p>SSD</p>
    <p>IO Stack</p>
    <p>SSD</p>
    <p>User space Kernel Space</p>
    <p>SSD SSD</p>
    <p>IO Stack</p>
    <p>(b) Kernel-managed</p>
    <p>Volume</p>
    <p>Application IO Threads</p>
    <p>Computing Threads</p>
    <p>Userspace IO Buffer</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Linux: IO Flow and IO States</p>
    <p>Process Next Request</p>
    <p>enqueue to software-queue</p>
    <p>Unplug Phase</p>
    <p>sort</p>
    <p>no</p>
    <p>enqueue to plug-list</p>
    <p>n o</p>
    <p>yes</p>
    <p>yes</p>
    <p>yes</p>
    <p>no</p>
    <p>dispatch IO to driver</p>
    <p>Plug Phase</p>
    <p>Dispatch Phase</p>
    <p>tag available?</p>
    <p>merge ?</p>
    <p>unplug?</p>
    <p>no</p>
    <p>IRQ event completion</p>
    <p>Complete IOFree tag</p>
    <p>bio</p>
    <p>Completion Phase</p>
    <p>classify</p>
    <p>complete8</p>
    <p>merge3</p>
    <p>ready4</p>
    <p>wait5</p>
    <p>insert6</p>
    <p>dispatch7</p>
    <p>SCSI Layer and Drivers</p>
    <p>SSD1 SSDm</p>
    <p>Block Layer Instance (SSD1)</p>
    <p>Block Layer Instance (SSDm)</p>
    <p>Applications</p>
    <p>bio1 biom</p>
    <p>Volume Manager Instance</p>
    <p>bio</p>
    <p>VFSDirect IO Page Cache</p>
    <p>biostart1</p>
    <p>split2</p>
    <p>start1</p>
    <p>split2</p>
    <p>IO Phases: Plug, Unplug, Dispatch, Completion</p>
    <p>IO States: start, split, merge, wait, ready, insert, dispatch, complete</p>
  </div>
  <div class="page">
    <p>Linux: Mixes IO Batching and IO Serving Tasks</p>
    <p>Examples:  Mixing batching with merge and tag allocation in</p>
    <p>plug phase</p>
    <p>Mixing classify with sort in unplug phase</p>
    <p>Root cause:  Many tasks are tied to plug-list</p>
    <p>Not designed for multi-SSD volume</p>
    <p>Creates many Insufficiencies  Lack of parallelism in IO processing</p>
    <p>Inefficient Merge and Sort</p>
    <p>Unpredictable blocking</p>
    <p>Unplug Phase (sort, classify)</p>
    <p>bio1 bio2 biom</p>
    <p>Plug Phase (batch, merge, tag allocation)</p>
    <p>Dispatch Phase (dispatch)</p>
    <p>To SCSI Layer and Drivers</p>
    <p>Software queues</p>
    <p>Software queues</p>
    <p>Linux block layer control flow</p>
    <p>Thread-specific plug-list</p>
  </div>
  <div class="page">
    <p>Insufficiency #1: Lack of Parallelism</p>
    <p>Increasing stack latency of member SSDs  E.g., Stack Latency of sda is less than sdb</p>
    <p>Effect of sequential IO serving and round-robin dispatch  IOs of last drive will acquire insert after IOs of every other drive gets</p>
    <p>dispatched</p>
    <p>sda sdb sdc sdd sde sdf sdg sdh 8-SSD (avg)</p>
    <p>S ta</p>
    <p>ck L</p>
    <p>a te</p>
    <p>n cy</p>
    <p>(i</p>
    <p>n u</p>
    <p>se c)</p>
    <p>|________8-SSD Volume Member Drives ___________|</p>
  </div>
  <div class="page">
    <p>Insufficiency #2: Inefficient Merge and Sort</p>
    <p>Stack Latency in 8-SSD volume is greater than 1-SSD system  Stack latency is greater than device latency in 8-SSD volume</p>
    <p>Plug-list intermixes IOs destined to all member drives  Search for a merge candidate even in unrelated IOs  Larger sorting workload across SSDs</p>
    <p>La te</p>
    <p>n cy</p>
    <p>( u</p>
    <p>se c)</p>
    <p>Stack Latency Device Latency</p>
    <p>(a) Absolute latency (b) Percentage</p>
  </div>
  <div class="page">
    <p>Insufficiency #3: Unpredictable Blocking</p>
    <p>If tag allocation fails, the IO thread blocks waiting for a free tag</p>
    <p>Uncertainty about active IO count in the pipeline  Storage controller dependent</p>
    <p>Compromises the IO scalability in SATA controller connected SSD volume</p>
    <p>LSI 9300-8i HBA Intel C602 SATA</p>
    <p>N o</p>
    <p>rm a</p>
    <p>li ze</p>
    <p>d</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t</p>
    <p>Q u</p>
    <p>e u</p>
    <p>e D</p>
    <p>e p</p>
    <p>th</p>
    <p>Time (Sec)</p>
    <p>sda sdb sda+sdb Available tag</p>
    <p>(a) IO performance Scaling (b) Tag usage in 2-SSD SATA volume</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Falcon: Separates IO Batching from IO Serving Tasks</p>
    <p>SCSI Layer and Drivers</p>
    <p>SSD1 SSDm</p>
    <p>FBL Instance (SSD1)</p>
    <p>FBL Instance (SSDm)</p>
    <p>bio1 biom</p>
    <p>Volume Manager Instance</p>
    <p>Falcon IO Management Layer (FML)</p>
    <p>Applications</p>
    <p>VFSDirect IO Page Cache</p>
    <p>bio bio</p>
    <p>Falcon Threads</p>
    <p>start1 start1</p>
    <p>split2 split2</p>
    <p>Falcon IO Management Layer (FML) Performs IO batching tasks Only</p>
    <p>Falcon Block Layer (FBL) performs</p>
    <p>IO Serving tasks only</p>
    <p>Classification Phase (classify)</p>
    <p>bio1 bio2 biom</p>
    <p>Batching Phase (batch)</p>
    <p>Thread-specific plug-list</p>
    <p>FML</p>
    <p>Process Phase merge, tag</p>
    <p>allocation, dispatch</p>
    <p>Sort Phase (sort)</p>
    <p>FBL</p>
    <p>software queues</p>
    <p>Process Phase merge, tag</p>
    <p>allocation, dispatch</p>
    <p>Sort Phase (sort)</p>
    <p>FBL</p>
    <p>Software queues</p>
    <p>To SCSI Layer and Drivers</p>
  </div>
  <div class="page">
    <p>Falcon: Feature Comparison with Linux</p>
    <p>Block Layer Features</p>
    <p>Linux 1-SSD</p>
    <p>Linux Multi-SSD</p>
    <p>Falcon Volume</p>
    <p>Parallel Processing NA</p>
    <p>Per-Drive Sort</p>
    <p>Neighbor Merge</p>
    <p>Dynamic Tag Management</p>
    <p>Well-suited for multi-SSD volume</p>
    <p>Improvements are applicable to 1-SSD system also</p>
  </div>
  <div class="page">
    <p>Falcon IO Management Layer</p>
    <p>IO batching in plug-list  No processing, just batching</p>
    <p>Classification  Single pass operation  No sorting</p>
    <p>Enabling parallel processing  Creates Falcon threads per FBL</p>
    <p>Uniform unplug criteria  32 per-SSD, 256 for 8-SSD volume  Lower criteria for low IO demand,</p>
    <p>and latency sensitive applications  See the paper for more details</p>
    <p>Move per-drive queues to software queues</p>
    <p>insert</p>
    <p>Classification Phase</p>
    <p>Classify plug-list to temporary per-drive queues</p>
    <p>no</p>
    <p>enqueue to plug-list</p>
    <p>Process Next IO Request</p>
    <p>yes</p>
    <p>Batching Phase</p>
    <p>unplug?</p>
    <p>bio (from Volume manager Layer)</p>
    <p>Spawn Falcon threads, if needed</p>
  </div>
  <div class="page">
    <p>Falcon Block Layer</p>
    <p>Sort Phase  Per-Drive Sort</p>
    <p>Process Phase  Neighbor Merge  Dynamic tag allocation  Dispatch</p>
    <p>Completion Phase</p>
    <p>Able to saturate a Samsung 950 Pro 512GB NVMe SSD  1375 MB/s (13% better than</p>
    <p>Linux)</p>
    <p>dispatch</p>
    <p>ready</p>
    <p>Process Phase</p>
    <p>Sort software-queue</p>
    <p>Allocate tag</p>
    <p>Neighbor merge</p>
    <p>Sort Phase</p>
    <p>merge</p>
    <p>IRQ event completion</p>
    <p>Complete IO</p>
    <p>complete</p>
    <p>bio (from FML layer)</p>
    <p>To SCSI layer and Drivers</p>
    <p>Completion Phase</p>
  </div>
  <div class="page">
    <p>Dynamic Tag Management</p>
    <p>Allocate a tag only if a dispatch is required</p>
    <p>Bio-queue keeps bio objects yet to be dispatched</p>
    <p>Pressure point controls the active IO count in the pipeline</p>
    <p>(a) IO Throughput Scaling for SATA controller</p>
    <p>u e</p>
    <p>u e</p>
    <p>D e</p>
    <p>p th</p>
    <p>Time (sec)</p>
    <p>sda-Linux sdb-Linux sda-Falcon sdb-Falcon</p>
    <p>(b) Tag usage in 2-SSD SATA volume</p>
    <p>Linux Falcon</p>
    <p>N o</p>
    <p>rm a</p>
    <p>li ze</p>
    <p>d</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Evaluation Setup</p>
    <p>Falcon: 600 lines of C kernel code add</p>
    <p>Ubuntu 16.04 version with Kernel version 4.4.0, Blk-mq block layer</p>
    <p>2 Intel Xeon CPU E5-2620 2GHz with 6 cores each</p>
    <p>32GB DRAM</p>
    <p>8 Samsung EVO 850 500 GB SSDs, connected using LSI SAS9300-8i HBA</p>
    <p>4KB Stripe size is used by default</p>
    <p>Raw volume, Ext4 and XFS file systems are evaluated</p>
    <p>Revised FIO is used as micro-benchmark</p>
  </div>
  <div class="page">
    <p>Ext4 File Random IO</p>
    <p>Ext4 has file inode lock issue</p>
    <p>1.77x speedup for random read</p>
    <p>1.59x speedup for random write</p>
    <p>(a) Single file read throughput (b) Single file write throughput</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>e c)</p>
    <p>IO thread count</p>
    <p>Linux Falcon</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>e c)</p>
    <p>IO thread count</p>
    <p>Linux Falcon</p>
  </div>
  <div class="page">
    <p>Buffered Random Write</p>
    <p>Buffer cache has just 1 thread per volume for flushing dirty pages</p>
    <p>1.39x speedup for 4-SSD volume</p>
    <p>1.59x speedup for 8-SSD volume</p>
    <p>h ro</p>
    <p>u g</p>
    <p>h p</p>
    <p>u t</p>
    <p>(M B</p>
    <p>/s e</p>
    <p>c) SSD count</p>
    <p>Linux Falcon</p>
  </div>
  <div class="page">
    <p>Graph Processing  G-Store[SC16] is used</p>
    <p>Semi-external graph analytics engine  Configurable number of IO threads  Linux setup, 1 and 8 IO threads  Falcon : 1 IO thread</p>
    <p>8-SSD volume, XFS filesystem</p>
    <p>Kronecker graph scale 28, edge factor 16 is used</p>
    <p>BFS, kCore: High random IO</p>
    <p>Connected component (CC), PageRank (PR): Sequential IO</p>
    <p>BFS kCore CC PR</p>
    <p>S p</p>
    <p>e e</p>
    <p>d u</p>
    <p>p</p>
    <p>Linux 1 IO Thread Linux 8 IO Threads Falcon</p>
    <p>Graph Processing</p>
    <p>4.12x speedup over Linux 1 IO thread setup</p>
    <p>1.78x speedup over Linux 8 IO thread</p>
  </div>
  <div class="page">
    <p>IO Trace Replay</p>
    <p>Trace Name Read (%) IO size range Size (GB) Type</p>
    <p>UM-Financial1 23.16 512B - 16715KB 17.22 Online transaction processing</p>
    <p>UM-Financial2 82.34 512B - 256.5KB 8.44 Online transaction processing</p>
    <p>UM-Websearch1 99.98 512B - 1111KB 15.24 Web Search</p>
    <p>UM-Websearch2 99.98 8KB - 32KB 65.82 Web Search</p>
    <p>FIU-Home 1 512B - 512KB 34.58 Research group activities</p>
    <p>FIU-Mail 8.58 4KB - 4KB 86.64 Mail Server</p>
    <p>FIU-Webuser 10.33 4KB - 128KB 30.94 Web User</p>
    <p>FIU-Web-vm 21.8 4KB - 4KB 54.52 Webmail proxy/online course management</p>
    <p>UM-Financial1 UM-Financial2 UM-Websearch1 UM-Webserach2 FIU-Home FIU-Mail FIU-Webuser FIU-Web-vm</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (M</p>
    <p>B /s</p>
    <p>e c)</p>
    <p>Linux Falcon</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>Separating batching from IO serving tasks is the key for IO scalability in multi-SSD volume</p>
    <p>Falcon enforces per-drive processing  Improves the IO stack performance</p>
    <p>Parallelizes the IO serving tasks across member SSDs</p>
    <p>Falcon improves the performance by 1.69x for various applications on 8-SSD volume</p>
    <p>Falcon achieves 1.13x throughput for an NVMe SSD</p>
  </div>
  <div class="page">
    <p>Thank You</p>
    <p>Falcon is open source now  https://github.com/iHeartGraph/falcon</p>
  </div>
</Presentation>
