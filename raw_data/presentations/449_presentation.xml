<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Just-In-Time Analy1cs on Large File Systems</p>
    <p>H. Howie Huang, Nan Zhang, Wei Wang George Washington University</p>
    <p>Gautam Das University of Texas at Arlington</p>
    <p>Alexander S. Szalay Johns Hopkins University</p>
    <p>FAST 2011</p>
    <p>February 17, 2011</p>
  </div>
  <div class="page">
    <p>Outline  Introduction  Aggregate Query Processing  Evaluation  Related Work  Conclusion</p>
  </div>
  <div class="page">
    <p>Motivations  Large file systems are common</p>
    <p>Users are interested in performing Just-In-Time analytics  Must be completed within a short amount of time  Has no prior knowledge of file system being analyzed</p>
    <p>Border patrol  E.g., check a travelers laptop for pirated movies and software ISO</p>
  </div>
  <div class="page">
    <p>Data Analytics  Aggregate Query  E.g., What is the total size of various types of documents?  SELECT SUM(file.size) FROM filesystem WHERE file.extension IN { txt, doc}; // AVG and COUNT are also in this category</p>
    <p>Top-k Query  E.g., Which are the 100 largest files that belong to John?  SELECT TOP 100 files FROM filesystem WHERE file.owner = John ORDER BY file.size DESC;</p>
  </div>
  <div class="page">
    <p>Current Approaches  Scan file system for each query  E.g., find command in Linux  Inefficient  Growing gap between storage performance and capacity</p>
    <p>Utilize pre-built indexes that are regularly updated  E.g., Google Desktop and Beagle  Undesirable when the metadata indexes are not available  The queries are scarcely needed</p>
  </div>
  <div class="page">
    <p>At a Glance  Tradeoff between query accuracy and cost</p>
    <p>Provide approximate (i.e., statistically accurate) answers that reside close from the precise answer</p>
    <p>Glance, a just-in-time query processing system  Produce answers based on a small number of samples (files or folders)</p>
    <p>File system agnostic  Works seamlessly with the tree structure of the system  Can be applied instantly over any new file system</p>
    <p>Remove the need of disk crawling and index building  Without a priori knowledge or pre-processing of file system</p>
  </div>
  <div class="page">
    <p>Glance Architecture  Consists of two algorithms</p>
    <p>FS_Agg for approximate processing of aggregate queries  FS_Agg_Basic: a random descent technique for unbiased</p>
    <p>aggregate estimations</p>
    <p>Two enhancements to reduce the error and performance overhead</p>
    <p>FS_TopK for approximate processing of top-k queries  A pruning-based technique</p>
  </div>
  <div class="page">
    <p>FS_Agg_Basic - Random Descent</p>
    <p>Solid f0, f1, f2= 2,2,2 and s0,s1,s2 = 4,1,0 Estimation of 2 + 8 + 8 = 18</p>
    <p>Dotted f0,f1,f2 = 2,0,1 and s0,s1,s2 = 4,2,0 Estimation of 2 + 0 + 8 = 10</p>
    <p>In this section, we develop FS Agg, our algorithm for processing aggregate queries. We first describe</p>
    <p>FS Agg Basic, a vanilla algorithm which illustrates our main idea of aggregate estimation without bias through a</p>
    <p>random descent process within a file system. Then, we</p>
    <p>describe two ideas to make the vanilla algorithm practical</p>
    <p>over very large file systems: high-level crawling leverages the special properties of a file system to reduce the</p>
    <p>standard error of estimation, and breadth-first implementation improves both accuracy and efficiency of query processing. Finally, we combine all three techniques to</p>
    <p>produce FS Agg.</p>
    <p>directed acyclic graph (DAG), depending on whether the</p>
    <p>file system allows hard links to the same file. The random</p>
    <p>descent process we are about to discuss can be applied to</p>
    <p>both cases with little change. For the ease of understand</p>
    <p>ing, we first focus on the case of tree-like folder structure,</p>
    <p>and then discuss a simple extension to DAG at the end of</p>
    <p>this subsection.</p>
    <p>Figure 1: Random descents on a tree-like structure</p>
    <p>Figure 1 depicts a tree structure with root correspond</p>
    <p>ing to the root directory of a file system, which we shall</p>
    <p>use as a running example throughout the paper. One can</p>
    <p>see from the figure that there are two types of nodes in</p>
    <p>the tree: folders (directories) and files. A file is always</p>
    <p>a leaf node. The children of a folder consist of all sub</p>
    <p>folders and files in the folder. We refer to the branches</p>
    <p>coming out of a folder node as subfolder-branches and</p>
    <p>file-branches, respectively, according to their destination</p>
    <p>type. We refer to a folder with no subfolder-branches</p>
    <p>as a leaf-folder. Note that this differs from a leaf in the tree, which can be either a file or a folder containing nei</p>
    <p>ther subfolder nor file. The random descent process starts</p>
    <p>from the root and ends at a leaf-folder. At each node,</p>
    <p>we choose a subfolder branch of the node uniformly at</p>
    <p>random for further exploration. During the descent pro</p>
    <p>cess, we evaluate all file branches encountered at each</p>
    <p>node along the path, and generate an aggregate estima</p>
    <p>tion based on these file branches.</p>
    <p>To make the idea more concrete, consider an exam</p>
    <p>ple of estimating the COUNT of all files in the system.</p>
    <p>At the beginning of random descent, we access the root</p>
    <p>to obtain the number of its file- and subfolder-branches</p>
    <p>f0 and s0, respectively, and record them as our evaluation for the root. Then, we randomly choose a subfolder</p>
    <p>branch for further descent, and repeat this process until</p>
    <p>we arrive at a folder with no subfolder. Suppose that the</p>
    <p>numbers we recorded during such a descent process are</p>
    <p>f0, s0, f1, s1, . . . , fh, sh, where sh = 0 because each descent ends at a leaf-folder. We estimate the COUNT of</p>
    <p>all files as</p>
    <p>n = h</p>
    <p>i=0</p>
    <p>fi  i1</p>
    <p>j=0</p>
    <p>sj</p>
    <p>, (1)</p>
    <p>where i1</p>
    <p>j=0 sj is assumed to be 1 when i = 0. Two examples of such a random descent process are marked in</p>
    <p>Figure 1 as red solid and blue dotted lines, respectively.</p>
    <p>The solid descent produces f0, f1, f2 = 2, 2, 2 and s0, s1, s2 = 4, 1, 0, leading to an estimation of 2 + 8 + 8 = 18. The dotted one produces f0, f1, f2 = 2, 0, 1 and s0, s1, s2 = 4, 2, 0, leading to an estimation of 2 + 0 + 8 = 10. The random descent process</p>
    <p>can be repeated multiple times (by restarting from the</p>
    <p>root) to produce a more accurate result (by taking the av</p>
    <p>erage of estimations generated by all descents).</p>
    <p>Unbiasedness: Somewhat surprisingly, the estimation produced by each random descent process is completely</p>
    <p>unbiased - i.e., the expected value of the estimation is exactly equal to the total number of files in the system.</p>
    <p>To understand why, consider the total number of files at</p>
    <p>the i-th level (with root being Level 0) of the tree (e.g., Files 1 and 2 in Figure 1 are at Level 3), denoted by Fi. According to the definition of a tree, each i-level file belongs to one and only one folder at Level i  1. For each (i  1)-level folder vi1, let |vi1| and p(vi1) be the number of (i-level) files in vi1 and the probability for vi1 to be reached in the random descent process, respectively. One can see that |vi1|/p(vi1) is an unbiased estimation for F(i) because</p>
    <p>E</p>
    <p>|vi1| p(vi1)</p>
    <p>=</p>
    <p>vi1</p>
    <p>p(vi1)</p>
    <p>|vi1| p(vi1)</p>
    <p>= Fi. (2)</p>
    <p>With our design of the random descent process, the prob</p>
    <p>ability p(vi1) is</p>
    <p>p(vi1) = i2</p>
    <p>j=0</p>
    <p>sj(vi1) , (3)</p>
    <p>Estimate the COUNT of all files in the system</p>
    <p>fi: number of files sj: number of subfolders</p>
  </div>
  <div class="page">
    <p>Unbiased Estimation  The estimation produced by each random</p>
    <p>descent process is completely unbiased</p>
    <p>The expected value of the estimation is exactly equal to the total number of files in the system</p>
    <p>|vi-1|: the number of (i-level) files in the folder vi-1  p(v-1): the probability for vi-1 to be reached in the</p>
    <p>random descent</p>
    <p>In this section, we develop FS Agg, our algorithm for processing aggregate queries. We first describe</p>
    <p>FS Agg Basic, a vanilla algorithm which illustrates our main idea of aggregate estimation without bias through a</p>
    <p>random descent process within a file system. Then, we</p>
    <p>describe two ideas to make the vanilla algorithm practical</p>
    <p>over very large file systems: high-level crawling leverages the special properties of a file system to reduce the</p>
    <p>standard error of estimation, and breadth-first implementation improves both accuracy and efficiency of query processing. Finally, we combine all three techniques to</p>
    <p>produce FS Agg.</p>
    <p>directed acyclic graph (DAG), depending on whether the</p>
    <p>file system allows hard links to the same file. The random</p>
    <p>descent process we are about to discuss can be applied to</p>
    <p>both cases with little change. For the ease of understand</p>
    <p>ing, we first focus on the case of tree-like folder structure,</p>
    <p>and then discuss a simple extension to DAG at the end of</p>
    <p>this subsection.</p>
    <p>Figure 1: Random descents on a tree-like structure</p>
    <p>Figure 1 depicts a tree structure with root correspond</p>
    <p>ing to the root directory of a file system, which we shall</p>
    <p>use as a running example throughout the paper. One can</p>
    <p>see from the figure that there are two types of nodes in</p>
    <p>the tree: folders (directories) and files. A file is always</p>
    <p>a leaf node. The children of a folder consist of all sub</p>
    <p>folders and files in the folder. We refer to the branches</p>
    <p>coming out of a folder node as subfolder-branches and</p>
    <p>file-branches, respectively, according to their destination</p>
    <p>type. We refer to a folder with no subfolder-branches</p>
    <p>as a leaf-folder. Note that this differs from a leaf in the tree, which can be either a file or a folder containing nei</p>
    <p>ther subfolder nor file. The random descent process starts</p>
    <p>from the root and ends at a leaf-folder. At each node,</p>
    <p>we choose a subfolder branch of the node uniformly at</p>
    <p>random for further exploration. During the descent pro</p>
    <p>cess, we evaluate all file branches encountered at each</p>
    <p>node along the path, and generate an aggregate estima</p>
    <p>tion based on these file branches.</p>
    <p>To make the idea more concrete, consider an exam</p>
    <p>ple of estimating the COUNT of all files in the system.</p>
    <p>At the beginning of random descent, we access the root</p>
    <p>to obtain the number of its file- and subfolder-branches</p>
    <p>f0 and s0, respectively, and record them as our evaluation for the root. Then, we randomly choose a subfolder</p>
    <p>branch for further descent, and repeat this process until</p>
    <p>we arrive at a folder with no subfolder. Suppose that the</p>
    <p>numbers we recorded during such a descent process are</p>
    <p>f0, s0, f1, s1, . . . , fh, sh, where sh = 0 because each descent ends at a leaf-folder. We estimate the COUNT of</p>
    <p>all files as</p>
    <p>n = h</p>
    <p>i=0</p>
    <p>fi  i1</p>
    <p>j=0</p>
    <p>sj</p>
    <p>, (1)</p>
    <p>where i1</p>
    <p>j=0 sj is assumed to be 1 when i = 0. Two examples of such a random descent process are marked in</p>
    <p>Figure 1 as red solid and blue dotted lines, respectively.</p>
    <p>The solid descent produces f0, f1, f2 = 2, 2, 2 and s0, s1, s2 = 4, 1, 0, leading to an estimation of 2 + 8 + 8 = 18. The dotted one produces f0, f1, f2 = 2, 0, 1 and s0, s1, s2 = 4, 2, 0, leading to an estimation of 2 + 0 + 8 = 10. The random descent process</p>
    <p>can be repeated multiple times (by restarting from the</p>
    <p>root) to produce a more accurate result (by taking the av</p>
    <p>erage of estimations generated by all descents).</p>
    <p>Unbiasedness: Somewhat surprisingly, the estimation produced by each random descent process is completely</p>
    <p>unbiased - i.e., the expected value of the estimation is exactly equal to the total number of files in the system.</p>
    <p>To understand why, consider the total number of files at</p>
    <p>the i-th level (with root being Level 0) of the tree (e.g., Files 1 and 2 in Figure 1 are at Level 3), denoted by Fi. According to the definition of a tree, each i-level file belongs to one and only one folder at Level i  1. For each (i  1)-level folder vi1, let |vi1| and p(vi1) be the number of (i-level) files in vi1 and the probability for vi1 to be reached in the random descent process, respectively. One can see that |vi1|/p(vi1) is an unbiased estimation for F(i) because</p>
    <p>E</p>
    <p>|vi1| p(vi1)</p>
    <p>=</p>
    <p>vi1</p>
    <p>p(vi1)</p>
    <p>|vi1| p(vi1)</p>
    <p>= Fi. (2)</p>
    <p>With our design of the random descent process, the prob</p>
    <p>ability p(vi1) is</p>
    <p>p(vi1) = i2</p>
    <p>j=0</p>
    <p>sj(vi1) , (3)</p>
  </div>
  <div class="page">
    <p>Processing of Aggregate Queries</p>
    <p>SUM: similar to COUNT, but set fi as the SUM of a meta attribute over all files</p>
    <p>AVG: compute as SUM/COUNT  Such an estimation is no longer unbiased</p>
    <p>Selection conditions: only evaluate fi over the files that satisfy the conditions</p>
  </div>
  <div class="page">
    <p>Disadvantages of Basic Algorithm</p>
    <p>Two types of folders may lead to extremely high estimation variance  High-level leaf-folders, i.e., shallow folders with no subfolders  Deep-level folders which reside at much lower levels</p>
  </div>
  <div class="page">
    <p>FS_Agg Improvements  High-level crawling for level i and above  Eliminate the negative impact of high-level leaf</p>
    <p>folders on estimation variance</p>
    <p>Breath-first descent instead of depth-first  At any level of the tree, randomly selects a set of</p>
    <p>folders to access at the next level</p>
    <p>Significantly increase the selection probability for a deep folder</p>
  </div>
  <div class="page">
    <p>Evaluation Setup  A prototype in C code for Linux/ext3  FS_Agg has three parameters</p>
    <p>h - the number of (highest) levels for crawling  Psel - the selection probability  Smin- the minimum number of selections  psel and smin determine how many subfolders to be selected</p>
    <p>FS_TopK has a parameter   - the (estimation) enlargement ratio</p>
    <p>Hardware  Intel Core 2 Duo processor, 4GB RAM, and 1TB Samsung</p>
    <p>Report the average of five runs</p>
  </div>
  <div class="page">
    <p>Test File Systems  Windows file systems from Microsoft trace  m100K (largest with &lt;100K), m1M, m10M (largest in the</p>
    <p>trace)</p>
    <p>m100M (largest 33 systems), and m1B  Plan 9 (Unix-like) systems from Bell Lab (~2M files)  NFS from Harvard trace (2.3M files)  Synthetic file systems generated by Impressions  E.g., i10K, i100K, i1M</p>
    <p>Welcome large real-world file systems</p>
  </div>
  <div class="page">
    <p>Metrics  Query accuracy  For aggregate queries, the relative error of the</p>
    <p>approximate answer apx compared with the precise one ans - i.e., |apx  ans| / |ans|</p>
    <p>For top-k queries, the percentage of items that are common in the approximate and precise top-k lists</p>
    <p>Query efficiency  Query time, i.e., the runtime of query processing  Query cost, i.e., the ratio of the number of directories</p>
    <p>visited by Glance to that of crawling the file system</p>
  </div>
  <div class="page">
    <p>Aggregate Queries</p>
    <p>Glance consistently generates accurate query answers  E.g., for m10M, sampling 30% of directories produces an</p>
    <p>answer with 2% average error</p>
  </div>
  <div class="page">
    <p>Aggregate Queries  Accuracy and Costs</p>
    <p>For all file systems, Glance produces the answers with &lt;10% relative error  The performance of Glance is independent of the type of the file system</p>
    <p>Achieves over 90% accuracy for NFS, Plan 9, and NTFS (m10M to m1B)  The cost ranges from less than 12% of crawling for large systems with 1B files and 80% for the small</p>
    <p>The algorithm scales well to large file systems</p>
  </div>
  <div class="page">
    <p>Aggregate Query Runtimes</p>
    <p>For different values of h from 3 to 5, query runs slightly longer but the accuracy improves</p>
    <p>The absolute runtime depends heavily on the size of the file system  A few seconds for m100K, several minutes for nfs (2.3M files), and 1.2 hours</p>
    <p>for m100M (not shown in the figure)</p>
  </div>
  <div class="page">
    <p>Top-k Queries on File Size</p>
    <p>For all but one case (m1M), Glance is capable of locating at least 50% of all top-k files (for pb, more than 95% are located)</p>
    <p>The cost is as little as 4% of crawling (for m10M)</p>
  </div>
  <div class="page">
    <p>Top-k Query Runtime</p>
    <p>The runtime is correlated to the size of the file system  The first point of each line stands for top-50 and the second for</p>
    <p>top-100  The queries take only a few seconds for small file systems, and</p>
    <p>up to ten minutes for large systems (e.g., m10M)</p>
  </div>
  <div class="page">
    <p>Related Work  Metadata query on file systems  Spyglass [Leung et al 2009]  SmartStore [Hua et al 2009]  Utilize multi-dimensional structures (e.g., K-D</p>
    <p>trees and R-trees) to build indexes upon subtree partitions or semantic groups</p>
    <p>Database sampling and query processing  Random sampling [Cochran 1977]  Sampling of hidden web databases [Dasgupta et al</p>
  </div>
  <div class="page">
    <p>Future Directions  Glance is not yet an any-time algorithm and</p>
    <p>cannot be stopped in the middle of the execution  Be predictive about the run-time and self-adjust the</p>
    <p>work flow based on the real-time requirements</p>
    <p>Currently employs a static strategy over file systems and queries  Leverage the results from the previous queries to</p>
    <p>significantly expedite the future ones  Utilize the semantic knowledge of a file system</p>
  </div>
  <div class="page">
    <p>Summary  Just-in-time analytics over a large-scale file system through</p>
    <p>its tree- or DAG-like structure</p>
    <p>A random descent technique to produce unbiased estimations for SUM and COUNT queries and accurate estimations for other aggregate queries</p>
    <p>A pruning-based technique for the approximate processing of top-k queries</p>
    <p>A comprehensive set of experiments that demonstrate the effectiveness of our approach over real-world file systems</p>
    <p>OCI, CISE</p>
  </div>
</Presentation>
