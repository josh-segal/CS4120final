<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Lessons Learned from 10k Experiments to Compare Virtual and Physical Testbeds</p>
    <p>Jonathan Crussell, Thomas M Kroeger, David Kavaler, Aaron Brown, Cynthia Phillips</p>
    <p>August 12th, 2019</p>
    <p>Supported by the Laboratory Directed Research and Development program at Sandia Na onal Laboratories, a mul mission laboratory managed and operated by Na onal Technology and Engineering Solu ons of Sandia, LLC., a wholly owned subsidiary of Honey</p>
    <p>well Interna onal, Inc., for the U.S. Department of Energy's Na onal Nuclear Security Administra on under contract DE-NA-0003525.</p>
  </div>
  <div class="page">
    <p>Goals</p>
    <p>Discover where and how virtual and physical testbeds differ Virtualiza on ar facts</p>
    <p>Methodology: Run representa ve workloads on physical and virtual testbeds Collect, compare, and contrast metrics</p>
    <p>Applica on-, OS-, and network-level</p>
    <p>August 12th, 2019 2</p>
  </div>
  <div class="page">
    <p>Goals</p>
    <p>Discover where and how virtual and physical testbeds differ Virtualiza on ar facts</p>
    <p>Methodology: Run representa ve workloads on physical and virtual testbeds Collect, compare, and contrast metrics</p>
    <p>Applica on-, OS-, and network-level</p>
    <p>August 12th, 2019 2</p>
  </div>
  <div class="page">
    <p>Lessons Learned</p>
    <p>Methodology and experimental results presented in previous work: Jonathan Crussell, Thomas M. Kroeger, Aaron Brown, and Cynthia Phillips. Virtually the same: Comparing physical and virtual testbeds. In 2019 Interna onal Conference on Compu ng, Networking and Communica ons (ICNC), pages 847853, Feb 2019.</p>
    <p>This work focuses on lessons learned in four areas: Tool Valida on and Development Instrumenta on Data Collec on and Aggrega on Data Analysis</p>
    <p>August 12th, 2019 3</p>
  </div>
  <div class="page">
    <p>Methodology &amp; Results</p>
    <p>ApacheBench fetching fixed pages from an HTTP server Over 10,000 experiments across three clusters Over 500TB of data (without full packet captures) Varied payload size, network drivers, network bandwidth Large varia ons in network driver offloading behavior Near-iden cal sequences of system calls</p>
    <p>Leverage minimega toolset (see http://minimega.org for details)</p>
    <p>August 12th, 2019 4</p>
  </div>
  <div class="page">
    <p>Tool Valida on and Development</p>
    <p>Lesson: Using a testbed toolset for experimenta on requires substan al effort and considera on to put tools together in a workable and validated fashion.</p>
    <p>August 12th, 2019 5</p>
  </div>
  <div class="page">
    <p>Tool Valida on and Development</p>
    <p>bash$ bash run . bash USAGE : run . bash DIR ITER DURATION CONCURRENT VMTYPE</p>
    <p>DRIVER NCPUS OFFLOAD RATE NWORKERS URL NREQUESTS INSTRUMENT</p>
    <p>bash$ bash sweep . bash / scratch / output params . bash &gt; sweep - params . bash</p>
    <p>bash$ head -n 1 sweep - params . bash bash / root / run . bash / scratch / output / 1 360 1 kvm e1000 1</p>
    <p>on 1000 1 http ://10.0.0.1/ 100000 true bash$ sort -R sweep - params . bash | parallel -j1 -- eta -S</p>
    <p>$( python igor . py -- heads jc [0 -9])</p>
    <p>August 12th, 2019 6</p>
  </div>
  <div class="page">
    <p>Tool Valida on and Development</p>
    <p>Running thousands of repe ons: Handle all edge cases (rare bug in minimegas capture API) Clean up all state (failed to unmount container filesystem)</p>
    <p>bash$ mount | grep megamount | head -n 5 megamount_5562 on / tmp / minimega /5562/ fs type overlay megamount_5566 on / tmp / minimega /5566/ fs type overlay megamount_5611 on / tmp / minimega /5611/ fs type overlay megamount_5752 on / tmp / minimega /5752/ fs type overlay megamount_5774 on / tmp / minimega /5774/ fs type overlay bash$ mount | grep megamount | wc -l 96</p>
    <p>August 12th, 2019 7</p>
  </div>
  <div class="page">
    <p>Tool Valida on and Development</p>
    <p>Toolset improvements: Add snaplen op on to capture API Add standalone C2 server</p>
    <p># On VMs minimega -e cc exec mkdir / que minimega -e cc background protonuke - serve - http minimega -e cc recv / miniccc . log</p>
    <p># On physical nodes rond -e exec mkdir / que rond -e bg protonuke - serve - http rond -e recv / miniccc . log</p>
    <p>August 12th, 2019 8</p>
  </div>
  <div class="page">
    <p>Instrumenta on</p>
    <p>Lesson: Instrumenta on is invaluable but it is o en manually added, expensive, and experiment-specific. Integra ng more forms of instrumenta on into the toolset could help researchers to more rapidly develop experiments.</p>
    <p>August 12th, 2019 9</p>
  </div>
  <div class="page">
    <p>Instrumenta on</p>
    <p>Two forms of instrumenta on: Verify func onality of environment Understand and evaluate experiments</p>
    <p>A</p>
    <p>C D</p>
    <p>BSRC</p>
    <p>DST</p>
    <p>Integra ng the former into the toolset could simplify experiments</p>
    <p>August 12th, 2019 10</p>
  </div>
  <div class="page">
    <p>Instrumenta on</p>
    <p>Mismatch between capture loca ons Dropped events for containers</p>
    <p>bash$ tcpdump -i eth0 -w foo . pcap tcpdump : listening on eth0 , link - type EN10MB ( Ethernet ) ,</p>
    <p>capture size 262144 bytes 9001 packets captured 9000 packet received by filter 1 packets dropped by kernel bash$ sysdig -w foo . scap &lt;no indication of dropped events &gt;</p>
    <p>August 12th, 2019 11</p>
  </div>
  <div class="page">
    <p>Instrumenta on</p>
    <p>Many ways to instrument experiment at many levels, mostly by hand Applica on-level: RPS, Ji er, ... OS-level: System calls, u liza on, perf stats, ... Network-level: Flow sta s cs, bandwidth, ...</p>
    <p>Use to understand anomalies e1000 stalls Performance increase</p>
    <p>August 12th, 2019 12</p>
  </div>
  <div class="page">
    <p>Data Collec on and Aggrega on</p>
    <p>Lesson: Testbeds can provide a wealth of data to researchers but should do more to streamline the process of collec ng and aggrega ng it into a usable form.</p>
    <p>August 12th, 2019 13</p>
  </div>
  <div class="page">
    <p>Data Collec on and Aggrega on</p>
    <p>How to extract instrumenta on data from VMs? C2 has limits on file size VMs write to qcow2, host extracts Future: mount guest filesystem?</p>
    <p>How to aggregate data? Dump per-itera on data into SQLite database Combine SQLite databases a er all itera ons complete</p>
    <p>August 12th, 2019 14</p>
  </div>
  <div class="page">
    <p>Data Analysis</p>
    <p>How to reduce storage? total packets : 5 total packets : 5 ack pkts sent : 4 ack pkts sent : 5 pure acks sent : 2 pure acks sent : 2 sack pkts sent : 0 sack pkts sent : 0 dsack pkts sent : 0 dsack pkts sent : 0 max sack blks / ack : 0 max sack blks / ack : 0 unique bytes sent : 72 unique bytes sent : 486 actual data pkts : 1 actual data pkts : 1 actual data bytes : 72 actual data bytes : 486 rexmt data pkts : 0 rexmt data pkts : 0 rexmt data bytes : 0 rexmt data bytes : 0 zwnd probe pkts : 0 zwnd probe pkts : 0 zwnd probe bytes : 0 zwnd probe bytes : 0 outoforder pkts : 0 outoforder pkts : 0 pushed data pkts : 1 pushed data pkts : 1 SYN / FIN pkts sent : 1/1 SYN / FIN pkts sent : 1/1 req 1323 ws/ts: Y/Y req 1323 ws/ts: Y/Y adv wind scale : 7 adv wind scale : 7 ========================== &lt;15 lines omitted &gt; ========================== missed data : 0 bytes missed data : 0 bytes truncated data : 0 bytes truncated data : 352 bytes truncated packets : 0 pkts truncated packets : 1 pkts data xmit time : 0.000 secs data xmit time : 0.000 secs idletime max : 1.0 ms idletime max : 0.7 ms throughput : 38482 Bps throughput : 259754 Bps</p>
    <p>August 12th, 2019 15</p>
  </div>
  <div class="page">
    <p>Data Analysis</p>
    <p>How to reduce storage? tcptrace produces 78 sta s cs per flow Compute summary sta s cs over all flows for itera on Compare mean of means across itera ons and parameters</p>
    <p>August 12th, 2019 16</p>
  </div>
  <div class="page">
    <p>Data Analysis</p>
    <p>Lesson: Testbeds allow for many repe ons but care should be used when analyzing the data, especially in confla ng sta s cal significance with prac cal importance.</p>
    <p>August 12th, 2019 17</p>
  </div>
  <div class="page">
    <p>Data Analysis</p>
    <p>Hypothesis tes ng Everything seems significant with many itera ons But prac cally important? (e.g. 0.1% decrease in latency)</p>
    <p>Mul ple comparisons Comparing many metrics can result in significance by chance</p>
    <p>August 12th, 2019 18</p>
  </div>
  <div class="page">
    <p>Whats next?</p>
    <p>Experiments with conten on Run N client/server pairs on the same hardware Generates Nx the data Surprising performance improvement when N is small (&lt;12)</p>
    <p>August 12th, 2019 19</p>
  </div>
  <div class="page">
    <p>Ques ons/Comments?</p>
    <p>Lessons: Using a testbed toolset for experimenta on requires substan al effort and considera on to put tools together in a workable and validated fashion.</p>
    <p>Instrumenta on is invaluable but it is o en manually added, expensive, and experiment-specific. Integra ng more forms of instrumenta on into the toolset could help researchers to more rapidly develop experiments.</p>
    <p>Testbeds can provide a wealth of data to researchers but should do more to streamline the process of collec ng and aggrega ng it into a usable form.</p>
    <p>Testbeds allow for many repe ons but care should be used when analyzing the data, especially in confla ng sta s cal significance with prac cal importance.</p>
    <p>Presenter: Jonathan Crussell jcrusse@sandia.gov</p>
    <p>August 12th, 2019 20</p>
  </div>
</Presentation>
