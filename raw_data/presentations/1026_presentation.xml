<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Repair Pipelining for Erasure-Coded Storage</p>
    <p>Runhui Li, Xiaolu Li, Patrick P. C. Lee, Qun Huang</p>
    <p>The Chinese University of Hong Kong</p>
    <p>USENIX ATC 2017</p>
  </div>
  <div class="page">
    <p>Introduction</p>
    <p>Fault tolerance for distributed storage is critical</p>
    <p>Availability: data remains accessible under failures</p>
    <p>Durability: no data loss even under failures</p>
    <p>Erasure coding is a promising redundancy technique</p>
    <p>Minimum data redundancy via data encoding</p>
    <p>Higher reliability with same storage redundancy than replication</p>
    <p>Reportedly deployed in Google, Azure, Facebook</p>
    <p>e.g., Azure reduces redundancy from 3x (replication) to 1.33x (erasure coding)</p>
    <p>PBs saving</p>
  </div>
  <div class="page">
    <p>Erasure Coding</p>
    <p>Divide file data to k blocks</p>
    <p>Encode k (uncoded) blocks to n coded blocks</p>
    <p>Distribute the set of n coded blocks (stripe) to n nodes</p>
    <p>Fault-tolerance: any k out of n blocks can recover file data</p>
    <p>Nodes</p>
    <p>(n, k) = (4, 2)</p>
    <p>File encode divide</p>
    <p>A B</p>
    <p>C D</p>
    <p>A+C B+D</p>
    <p>A+D B+C+D</p>
    <p>A B</p>
    <p>C D</p>
    <p>A+C B+D</p>
    <p>A+D B+C+D</p>
    <p>A B</p>
    <p>C D</p>
    <p>Remark: for systematic codes, k of n coded blocks are the original k uncoded blocks</p>
  </div>
  <div class="page">
    <p>Erasure Coding</p>
    <p>Practical erasure codes satisfy linearity and addition associativity</p>
    <p>Each block can be expressed as a linear combination of any k blocks in the</p>
    <p>same stripe, based on Galois Field arithmetic</p>
    <p>e.g., block B = a1B1 + a2B2 + a3B3 + a4B4</p>
    <p>for k = 4, coefficients ais, and blocks Bis</p>
    <p>Also applicable to XOR-based erasure codes</p>
    <p>Examples: Reed-Solomon codes, regenerating codes, LRC</p>
  </div>
  <div class="page">
    <p>Erasure Coding</p>
    <p>Good: Low redundancy with high fault tolerance</p>
    <p>Bad: High repair penalty</p>
    <p>In general, k blocks retrieved to repair a failed block</p>
    <p>Mitigating repair penalty of erasure coding is a hot topic</p>
    <p>New erasure codes to reduce repair bandwidth or I/O</p>
    <p>e.g., Regenerating codes, LRC, Hitchhiker</p>
    <p>Efficient repair approaches for general erasure codes</p>
    <p>e.g., lazy repair, PPR</p>
  </div>
  <div class="page">
    <p>Conventional Repair</p>
    <p>Single-block repair:</p>
    <p>Retrieve k blocks from k working nodes (helpers)</p>
    <p>Store the repaired block at requestor</p>
    <p>Repair time = k timeslots</p>
    <p>Bottlenecked by requestors downlink</p>
    <p>Uneven bandwidth usage (e.g., links among helpers are idle)</p>
    <p>k = 4 helpers requestor</p>
    <p>Network</p>
    <p>R N1 N2 N3 N4</p>
    <p>Bottleneck</p>
  </div>
  <div class="page">
    <p>Partial-Parallel-Repair (PPR)</p>
    <p>Exploit linearity and addition associativity to perform repair in a</p>
    <p>divide-and-conquer manner</p>
    <p>[Mitra, EuroSys16]</p>
    <p>k = 4 helpers requestor</p>
    <p>Network</p>
    <p>R N1 N2 N3 N4</p>
    <p>Repair time = ceil(log2(k+1)) timeslots</p>
    <p>Timeslot 1:</p>
    <p>N1 sends a1B1 to N2  a1B1+a2B2 N3 sends a3B3 to N4  a3B3+a4B4</p>
    <p>Timeslot 2:</p>
    <p>N2 sends a1B1+a2B2 to N4</p>
    <p>a1B1+a2B2+a3B3+a4B4</p>
    <p>Timeslot 3:</p>
    <p>N4  R  repaired block</p>
  </div>
  <div class="page">
    <p>Open Question</p>
    <p>Repair time of erasure coding remains larger than normal read time</p>
    <p>Repair-optimal erasure codes still read more data than amount of failed data</p>
    <p>Erasure coding is mainly for warm/cold data</p>
    <p>Repair penalty only applies to less frequently accessed data</p>
    <p>Hot data remains replicated</p>
    <p>Can we reduce repair time of erasure coding to almost the</p>
    <p>same as the normal read time?</p>
    <p>Create opportunity for storing hot data with erasure coding</p>
  </div>
  <div class="page">
    <p>Our Contributions</p>
    <p>Repair pipelining, a technique to speed up repair for general</p>
    <p>erasure coding</p>
    <p>Applicable for degraded reads and full-node recovery</p>
    <p>O(1) repair time in homogeneous settings</p>
    <p>Extensions to heterogeneous settings</p>
    <p>A prototype ECPipe integrated with HDFS and QFS</p>
    <p>Experiments on local cluster and Amazon EC2</p>
    <p>Reduction of repair time by 90% and 80% over conventional repair and</p>
    <p>partial-parallel-repair (PPR), respectively</p>
  </div>
  <div class="page">
    <p>Repair Pipelining</p>
    <p>Goals:</p>
    <p>Eliminate bottlenecked links</p>
    <p>Effectively utilize available bandwidth resources in repair</p>
    <p>Key observation: coding unit (word) is much smaller than</p>
    <p>read/write unit (block)</p>
    <p>e.g., word size ~ 1 byte; block size ~ 64 MiB</p>
    <p>Words at the same offset are encoded together in erasure coding</p>
    <p>n blocks of a stripe</p>
    <p>words at the same offset</p>
    <p>are encoded together</p>
    <p>word</p>
  </div>
  <div class="page">
    <p>Repair Pipelining</p>
    <p>Idea: slicing a block</p>
    <p>Each slice comprises multiple words (e.g., slice size ~ 32 KiB)</p>
    <p>Pipeline the repair of each slice through a linear path</p>
    <p>Single-block repair:</p>
    <p>time</p>
    <p>k = 4</p>
    <p>s = 6 slices N1 N2 N3 N4 R</p>
    <p>N1 N2 N3 N4 R</p>
    <p>N1 N2 N3 N4 R</p>
    <p>N1 N2 N3 N4 R</p>
    <p>N1 N2 N3 N4 R</p>
    <p>N1 N2 N3 N4 R</p>
    <p>a1b1 a1b1+a2b2</p>
    <p>a1b1+a2b2 +a3b3</p>
    <p>a1b1+a2b2 +a3b3+a4b4</p>
  </div>
  <div class="page">
    <p>Repair Pipelining</p>
    <p>Two types of single-failure repair (most common case):</p>
    <p>Degraded read</p>
    <p>Repairing an unavailable block at a client</p>
    <p>Full-node recovery</p>
    <p>Repairing all lost blocks of a failed node at one or multiple nodes</p>
    <p>Greedy scheduling of multiple stripes across helpers</p>
    <p>Challenge: repair degraded by stragglers</p>
    <p>Any repair of erasure coding faces similar problems due to data retrievals from</p>
    <p>multiple helpers</p>
    <p>Our approach: address heterogeneity and bypass stragglers</p>
  </div>
  <div class="page">
    <p>Extension to Heterogeneity</p>
    <p>Heterogeneity: link bandwidths are different</p>
    <p>Case 1: limited bandwidth when a client issues reads to a remote</p>
    <p>storage system</p>
    <p>Cyclic version of repair pipelining: allow a client to issue parallel reads</p>
    <p>from multiple helpers</p>
    <p>Case 2: arbitrary link bandwidths</p>
    <p>Weighted path selection: select the best path of helpers for repair</p>
  </div>
  <div class="page">
    <p>Repair Pipelining (Cyclic Version)</p>
    <p>Requestor receives repaired data from k-1 helpers</p>
    <p>Repair time in homogeneous environments  1 timeslot for large s</p>
    <p>N1 N2 N3 N4</p>
    <p>N2 N3 N4 N1</p>
    <p>N3 N4 N1 N2</p>
    <p>R</p>
    <p>N4 N1 N2</p>
    <p>N1 N2 N3 N4</p>
    <p>N2 N3 N4 N1</p>
    <p>N3 N4 N1 N2</p>
    <p>Send to</p>
    <p>requestor</p>
    <p>R</p>
    <p>N4 N1 N2</p>
    <p>Send to</p>
    <p>requestor</p>
    <p>Group 1</p>
    <p>Group 2</p>
  </div>
  <div class="page">
    <p>Weighted Path Selection</p>
    <p>Goal: Find a path of k + 1 nodes (i.e., k helpers and requestor)</p>
    <p>that minimizes the maximum link weight</p>
    <p>e.g., set link weight as inverse of link bandwidth</p>
    <p>Any straggler is associated with large weight</p>
    <p>Brute-force search is expensive</p>
    <p>(n-1)!/(n-1-k)! permutations</p>
    <p>Our algorithm:</p>
    <p>Apply brute-force search, but avoid search of non-optimal paths</p>
    <p>If link L has weight larger than the max weight of the current optimal path, any path</p>
    <p>containing L must be non-optimal</p>
    <p>Remain optimal, with much less search time</p>
  </div>
  <div class="page">
    <p>Implementation</p>
    <p>Requestor implemented as a C++/Java class</p>
    <p>Each helper daemon directly reads local blocks via native FS</p>
    <p>Coordinator access block locations and block-to-stripe mappings</p>
    <p>ECPipe is integrated with HDFS and QFS, with around 110 and</p>
    <p>Helper</p>
    <p>Node</p>
    <p>Helper</p>
    <p>Node</p>
    <p>Helper</p>
    <p>Node</p>
    <p>Coordinator Requestor</p>
    <p>control flow</p>
    <p>datal flow</p>
    <p>ECPipe: a middleware atop distributed storage system</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ECPipe performance on a 1Gb/s local cluster</p>
    <p>Single-block repair time vs. slice size</p>
    <p>for (n,k) = (14,10)</p>
    <p>Trade-off of slice size:</p>
    <p>Too small: transmission overhead is</p>
    <p>significant</p>
    <p>Too large: less parallelization</p>
    <p>Best slice size = 32 KiB</p>
    <p>Repair pipelining (basic and cyclic)</p>
    <p>outperforms conventional and PPR</p>
    <p>by 90.9% and 80.4%, resp.</p>
    <p>Only 7% more than direct send time</p>
    <p>over a 1Gb/s link  O(1) repair time</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ECPipe performance on a 1Gb/s local cluster</p>
    <p>Full-node recovery rate vs. number of requestors</p>
    <p>for (n,k) = (14,10)</p>
    <p>Recovery rate increases with</p>
    <p>number of requestors</p>
    <p>Repair pipelining (RP and</p>
    <p>RP+scheduling) achieves high</p>
    <p>recovery rate</p>
    <p>Greedy scheduling balances repair</p>
    <p>load across helpers when there</p>
    <p>are more requestors (i.e., more</p>
    <p>resource contention)</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>ECPipe performance on Amazon EC2</p>
    <p>Weighted path selection reduces single-block repair time of basic</p>
    <p>repair pipelining by up to 45%</p>
  </div>
  <div class="page">
    <p>Evaluation</p>
    <p>Single-block repair performance on HDFS and QFS</p>
    <p>ECPipe significantly improves repair performance</p>
    <p>Conventional repair under ECPipe outperforms original conventional repair inside</p>
    <p>distributed file systems (by ~20%)</p>
    <p>Avoid fetching blocks via distributed storage system routine</p>
    <p>Performance gain is mainly due to repair pipelining (by ~90%)</p>
    <p>QFS: slice size QFS: block size HDFS: (n,k)</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Repair pipelining, a general technique that enables very fast</p>
    <p>repair for erasure-coded storage</p>
    <p>Contributions:</p>
    <p>Designs for both degraded reads and full-node recovery</p>
    <p>Extensions to heterogeneity</p>
    <p>Prototype implementation ECPipe</p>
    <p>Extensive experiments on local cluster and Amazon EC2</p>
    <p>Source code:</p>
    <p>http://adslab.cse.cuhk.edu.hk/software/ecpipe</p>
  </div>
</Presentation>
