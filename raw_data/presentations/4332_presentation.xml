<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>July 17th 2018</p>
    <p>Yang Xu, Jiawei Liu, Wei Yang, and Liusheng Huang</p>
    <p>School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230027, China</p>
    <p>Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings</p>
  </div>
  <div class="page">
    <p>OUTLINE</p>
    <p>Introduction01 Latent Meaning Models02</p>
    <p>Experimental Setup03 Experimental Results04</p>
    <p>Conclusion05 !2</p>
  </div>
  <div class="page"/>
  <div class="page">
    <p>Word-level Word Embedding</p>
    <p>word-word co-occurrence matrix</p>
    <p>e.g., GloVe (Pennington et al.)</p>
    <p>w(t-2)</p>
    <p>w(t-1)</p>
    <p>w(t+1)</p>
    <p>w(t+2)</p>
    <p>w(t)</p>
    <p>SUM</p>
    <p>INPUT PROJECTION OUTPUT</p>
    <p>CBOW</p>
    <p>w(t-2)</p>
    <p>w(t-1)</p>
    <p>w(t+1)</p>
    <p>w(t+2)</p>
    <p>w(t)</p>
    <p>SUM</p>
    <p>INPUT PROJECTION OUTPUT</p>
    <p>Skip-gram</p>
    <p>e.g., CBOW, Skip-gram (Mikolov et al.)</p>
  </div>
  <div class="page">
    <p>Morphology-based Word Embedding</p>
    <p>Training Model</p>
    <p>Morpheme Embeddings</p>
    <p>Word Embeddings</p>
    <p>Prefix Root Suffix</p>
    <p>Word</p>
    <p>incredible</p>
    <p>Generated Word Vectors</p>
    <p>Morpheme Embeddings Prefix Root Suffix</p>
    <p>Generated Word</p>
    <p>Generative Model</p>
    <p>!5</p>
  </div>
  <div class="page">
    <p>Our Original Intention</p>
    <p>!6</p>
    <p>Word-level models: InputWords; Output Word Embeddings</p>
    <p>Morphology-based models: Input Words + Morphemes Output Word Embeddings + Morpheme Embeddings</p>
    <p>Our Latent Meaning Models: InputWords + Latent Meanings of Morphemes Output Word Embeddings ( no by-product, e.g., morpheme embedding)</p>
    <p>PURPOSE: to not only encode morphological properties into words, but also enhance the semantic similarities among word embeddings</p>
  </div>
  <div class="page">
    <p>Explicit Models &amp; Our Models</p>
    <p>it is an incredible</p>
    <p>unbelievable</p>
    <p>thing</p>
    <p>it is that</p>
    <p>in cred ible</p>
    <p>un believ able</p>
    <p>not believe able capable</p>
    <p>not believe able capable</p>
    <p>Prefix Latent Meaning</p>
    <p>in un</p>
    <p>in, not not</p>
    <p>Root Latent Meaning</p>
    <p>believ cred</p>
    <p>believe believe</p>
    <p>Suffix Latent Meaning</p>
    <p>able ible</p>
    <p>able, capale able, capale</p>
    <p>sentence i :</p>
    <p>sentence j :</p>
    <p>Explicit models directly use morphemes</p>
    <p>Our models employ the latent meanings</p>
    <p>of morphemes</p>
    <p>Corpus</p>
    <p>Lookup table</p>
    <p>in</p>
    <p>*Note: The lookup table can be derived from morphological lexicons.</p>
    <p>!7</p>
  </div>
  <div class="page">
    <p>!8</p>
  </div>
  <div class="page">
    <p>CBOW with Negative Sampling</p>
    <p>!9</p>
    <p>ti-2</p>
    <p>ti-1</p>
    <p>ti+1</p>
    <p>ti+2</p>
    <p>ti</p>
    <p>SUM</p>
    <p>INPUT PROJECTION OUTPUT</p>
    <p>(Context Words)</p>
    <p>(Target Word)</p>
    <p>Sequence of tokens</p>
    <p>= 1</p>
    <p>=1</p>
    <p>log( | ())</p>
    <p>Objective Function:</p>
    <p>Negative Sampling:</p>
  </div>
  <div class="page">
    <p>Three Specific Models</p>
    <p>!10</p>
    <p>LMM-A (Latent Meaning Model-Average)</p>
    <p>LMM-S (Latent Meaning Model-Similarity)</p>
    <p>LMM-M (Latent Meaning Model-Max)</p>
  </div>
  <div class="page">
    <p>Word Map</p>
    <p>Prefix Latent Meaning</p>
    <p>in un</p>
    <p>in, not not</p>
    <p>Root Latent Meaning</p>
    <p>believ cred</p>
    <p>believe believe</p>
    <p>Suffix Latent Meaning</p>
    <p>able ible</p>
    <p>able, capale able, capale</p>
    <p>Lookup tableincredible</p>
    <p>in cred ible</p>
    <p>unbelievable</p>
    <p>un believ able</p>
    <p>Word Prefix Root Suffix</p>
    <p>incredible in not believe able capable</p>
    <p>unbelievable not believe able capable</p>
    <p>Word Map</p>
    <p>*Note: The derivational morphemes, not the inflectional morphemes, are mainly concerned</p>
    <p>!11</p>
    <p>#rows = |vocabulary|</p>
  </div>
  <div class="page">
    <p>Latent Meaning Model-Average (LMM-A)</p>
    <p>A paradigm of LMM-A</p>
    <p>not</p>
    <p>Latent Meaning</p>
    <p>Prefix</p>
    <p>Root</p>
    <p>Suffix</p>
    <p>it</p>
    <p>is</p>
    <p>incredible</p>
    <p>thing</p>
    <p>an</p>
    <p>SUM</p>
    <p>in</p>
    <p>capable</p>
    <p>believe</p>
    <p>able</p>
    <p>An item of the Word Map</p>
    <p>incredible notin believe able capable Word Prefix Root Suffix</p>
    <p>!12</p>
    <p>Sequence of tokens</p>
    <p>The latent meanings of s morphemes have equal contributions to</p>
    <p>The modified embedding of :</p>
    <p>is utilized for training</p>
    <p>: a set of latent meanings of s morphemes : the length of</p>
    <p>()</p>
  </div>
  <div class="page">
    <p>Latent Meaning Model-Similarity (LMM-S)</p>
    <p>not</p>
    <p>Latent Meaning</p>
    <p>Prefix</p>
    <p>Root</p>
    <p>Suffix</p>
    <p>it</p>
    <p>is</p>
    <p>incredible</p>
    <p>thing</p>
    <p>an</p>
    <p>SUM</p>
    <p>in</p>
    <p>capable</p>
    <p>believe</p>
    <p>able</p>
    <p>An item of the Word Map</p>
    <p>incredible notin believe able capable Word Prefix Root Suffix</p>
    <p>? in</p>
    <p>?not</p>
    <p>?believe</p>
    <p>?capable</p>
    <p>?able</p>
    <p>A paradigm of LMM-S</p>
    <p>!13</p>
    <p>The latent meanings of s morphemes are assigned with different weights:</p>
    <p>The modified embedding of :</p>
    <p>: a set of latent meanings of s morphemes</p>
    <p>&lt;, &gt; = cos(, )</p>
    <p>cos(, ) ,</p>
    <p>Sequence of tokens</p>
    <p>()</p>
  </div>
  <div class="page">
    <p>Latent Meaning Model-Max (LMM-M)</p>
    <p>not</p>
    <p>Latent Meaning</p>
    <p>Prefix</p>
    <p>Root</p>
    <p>Suffix</p>
    <p>it</p>
    <p>is</p>
    <p>incredible</p>
    <p>thing</p>
    <p>an</p>
    <p>SUM</p>
    <p>in</p>
    <p>capable</p>
    <p>believe</p>
    <p>able</p>
    <p>An item of the Word Map</p>
    <p>incredible notin believe able capable Word Prefix Root Suffix</p>
    <p>?not</p>
    <p>?believe</p>
    <p>?able</p>
    <p>A paradigm of LMM-M</p>
    <p>!14</p>
    <p>Keep the latent meanings that have maximum similarities to :</p>
    <p>The modified embedding of :</p>
    <p>= { ,  , }</p>
    <p>= max</p>
    <p>(, ),      = max</p>
    <p>(, ),</p>
    <p>= max</p>
    <p>(, ),</p>
    <p>Sequence of tokens</p>
    <p>()</p>
  </div>
  <div class="page">
    <p>Update Rules for LMMs</p>
    <p>!15</p>
    <p>New Objective Function (After modifying the input layer of CBOW):</p>
    <p>= 1</p>
    <p>=1</p>
    <p>log( |  ()</p>
    <p>)</p>
    <p>All parameters introduced by our models can be directly derived using the word map and word embeddings</p>
    <p>Update not just but the embeddings of the latent meanings with the same weights as they are assigned in the forward propagation period</p>
  </div>
  <div class="page">
    <p>!16</p>
  </div>
  <div class="page">
    <p>Corpus &amp; Word Map</p>
    <p>!17</p>
    <p>Corpus Word Map</p>
    <p>News corpus of 2009 (2013 ACL Eighth Workshop)</p>
    <p>Size: 1.7GB</p>
    <p>~500 million tokens</p>
    <p>~600,000 words</p>
    <p>Digits &amp; punctuation marks are filtered</p>
    <p>Morpheme segmentation using Morefessor (Creutz &amp; Lagus, 2007)</p>
    <p>Assign latent meanings</p>
    <p>Lookup table  derived from the resources provided</p>
    <p>by Michigan State University*  90 prefixes, 382 roots, 67 suffixes</p>
    <p>*Resources web link: https://msu.edu/~defores1/gre/roots/gre_rts_afx1.htm</p>
  </div>
  <div class="page">
    <p>Baselines &amp; Parameter Settings</p>
    <p>!18</p>
    <p>Baselines: ! Word-level models: CBOW, Skip-gram, GloVe</p>
    <p>! Explicitly Morpheme-related Model (EMM)</p>
    <p>Morphemes</p>
    <p>Prefix</p>
    <p>Root</p>
    <p>Suffix</p>
    <p>it</p>
    <p>is</p>
    <p>incredible</p>
    <p>thing</p>
    <p>an</p>
    <p>SUM in</p>
    <p>ible</p>
    <p>cred</p>
    <p>A paradigm of EMM</p>
    <p>Super-parameter Settings: ! Equal settings to all models</p>
    <p>! Vector Dimension: 200</p>
    <p>! Context window size: 5</p>
    <p>! #Negative_Samples: 20</p>
  </div>
  <div class="page">
    <p>Evaluation Benchmarks (1/2)</p>
    <p>!19</p>
    <p>Word Similarity:</p>
    <p>Syntactic Analogy: ! a b as c ? (d)  e.g., Queen King as Woman (Man)</p>
    <p>! Microsoft Research Syntactic Analogies dataset (8000 items)</p>
    <p>Name #Pairs Name #Pairs Name #Pairs RG-65 65 Rare-Word 2034 Men-3k 3000</p>
    <p>Wordsim-353 353 SCWS 2003 WS-353-Related 252</p>
    <p>Dataset</p>
    <p>Gold Standard Datasets Widely-used Datasets</p>
  </div>
  <div class="page">
    <p>!20</p>
    <p>Evaluation Benchmarks (2/2)</p>
    <p>Text Classification:</p>
    <p>! 20 Newsgroups dataset (19000 documents of 20 different topics) ! 4 text classification tasks, each involves 10 topics ! Training/Validation/Test subsets (6:2:2) ! Feature vector: average word embedding of words in each document ! L2-regularized logistic regression classifier</p>
  </div>
  <div class="page">
    <p>!21</p>
  </div>
  <div class="page">
    <p>The Results on Word Similarity</p>
    <p>!22</p>
    <p>(Given different models) Spearmans rank correlation (%) on different datasets</p>
    <p>CBOW Skip-gram GloVe EMM LMM-A LMM-S LMM-M</p>
    <p>Wordsim-353 58.77 61.94 49.40 60.01 62.05 63.13 61.54</p>
    <p>Rare-Word 40.58 36.42 33.40 40.83 43.12 42.14 40.51</p>
    <p>RG-65 56.50 62.81 59.92 60.85 62.51 62.49 63.07</p>
    <p>SCWS 63.13 60.20 47.98 60.28 61.86 61.71 63.02</p>
    <p>Men-3k 68.07 66.30 60.56 66.76 66.26 68.36 64.65</p>
    <p>WS-353-Related 49.72 57.05 47.46 54.48 56.14 58.47 55.19</p>
  </div>
  <div class="page">
    <p>The Results on Syntactic Analogy</p>
    <p>!23</p>
    <p>Syntactic analogy performance (%)</p>
    <p>CBOW Skip-gram GloVe EMM LMM-A LMM-S LMM-M</p>
    <p>Syntactic Analogy</p>
    <p>Question: a b as c (d)  Answer:</p>
  </div>
  <div class="page">
    <p>The Results on Text Classification</p>
    <p>!24</p>
    <p>Average text classification accuracy across the 4 tasks (%)</p>
    <p>CBOW Skip-gram GloVe EMM LMM-A LMM-S LMM-M</p>
    <p>Text Classification</p>
  </div>
  <div class="page">
    <p>The Impact of Corpus Size</p>
    <p>!25</p>
    <p>Results on Wordsim-353 task with different corpus size</p>
  </div>
  <div class="page">
    <p>The Impact of Context Window Size</p>
    <p>!26</p>
    <p>Results on Wordsim-353 task with different context window size</p>
  </div>
  <div class="page">
    <p>Word Embedding Visualization</p>
    <p>!27</p>
    <p>Visualization of word embeddings based on PCA</p>
    <p>latent meanings of morphemes</p>
  </div>
  <div class="page">
    <p>!28</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>Employ latent meanings of morphemes rather than the internal compositions themselves to train word embeddings</p>
    <p>By modifying the input layer and update rules of CBOW, we proposed three latent meaning models (LMM-A, LMM-S, LMM-M)</p>
    <p>The comprehensive quality of word embedings are enhanced by incorporating latent meanings of morphemes</p>
    <p>In the future, we intend to evaluate our models for some morpheme-rich languages like Russian, German, etc.</p>
    <p>!29</p>
  </div>
  <div class="page">
    <p>Questions? Thank you!</p>
  </div>
</Presentation>
