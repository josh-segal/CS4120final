<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>GridGraph: Large-Scale Graph Processing on a Single Machine</p>
    <p>Using 2-Level Hierarchical Par::oning</p>
    <p>Xiaowei ZHU, Wentao HAN, Wenguang CHEN Tsinghua University</p>
  </div>
  <div class="page">
    <p>Widely-Used Graph Processing</p>
  </div>
  <div class="page">
    <p>Exis:ng Solu:ons  Shared memory</p>
    <p>Single-node &amp; in-memory  Ligra, Galois, Polymer</p>
    <p>Distributed  Mul:-node &amp; in-memory  GraphLab, GraphX, PowerLyra</p>
    <p>Out-of-core  Single-node &amp; disk-based  GraphChi, X-Stream, TurboGraph</p>
  </div>
  <div class="page">
    <p>Exis:ng Solu:ons  Shared memory</p>
    <p>Single-node &amp; in-memory  Ligra, Galois, Polymer</p>
    <p>Distributed  Mul:-node &amp; in-memory  GraphLab, GraphX, PowerLyra</p>
    <p>Out-of-core  Single-node &amp; disk-based  GraphChi, X-Stream, TurboGraph</p>
    <p>Large-scale Limited capability to big graphs</p>
    <p>Irregular structure Imbalance of computa:on and communica:on</p>
    <p>Inevitable random access Expensive disk random access</p>
  </div>
  <div class="page">
    <p>Exis:ng Solu:ons  Shared memory</p>
    <p>Single-node &amp; in-memory  Ligra, Galois, Polymer</p>
    <p>Distributed  Mul:-node &amp; in-memory  GraphLab, GraphX, PowerLyra</p>
    <p>Out-of-core  Single-node &amp; disk-based  GraphChi, X-Stream, TurboGraph Most cost effec0ve!</p>
    <p>Large-scale Limited capability to big graphs</p>
    <p>Irregular structure Balance of computa:on and communica:on</p>
    <p>Inevitable random access Expensive disk random access</p>
  </div>
  <div class="page">
    <p>State-of-the-Art Methodology  X-Stream  Access edges sequen0ally from disks  Access ver:ces randomly inside memory</p>
    <p>Guarantee locality of vertex accesses by par::oning</p>
  </div>
  <div class="page">
    <p>State-of-the-Art Methodology  X-Stream  Access edges sequen0ally from slow memory  Access ver:ces randomly inside fast memory</p>
    <p>Guarantee locality of vertex accesses by par::oning</p>
    <p>Disk</p>
    <p>Memory</p>
    <p>Cache In-memory</p>
    <p>Out-of-core 7</p>
  </div>
  <div class="page">
    <p>Edge-Centric ScaQer-Gather set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p>
    <p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p>
    <p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Streams edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel [40].</p>
    <p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p>
    <p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p>
    <p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p>
    <p>Figure 3 shows how memory is accessed in the edge streaming model. The appeal of the streaming approach</p>
    <p>Edges (sequential read)</p>
    <p>Updates (sequential write)</p>
    <p>Vertices (random read/write)</p>
    <p>Updates (sequential read)</p>
    <p>Vertices (random read/write)</p>
    <p>Figure 3: Streaming Memory Access</p>
    <p>to graph processing stems from the fact that it allows sequential (and therefore much faster) access to Slow Storage for the (usually large) edge and update streams. The problem is that it requires random access to the vertices, which for large graphs may not fit in Fast Storage. To solve this problem, we introduce the notion of streaming partitions, described next.</p>
    <p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partitions vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partitions vertex set.</p>
    <p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p>
    <p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all</p>
    <p>set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p>
    <p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p>
    <p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Streams edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel [40].</p>
    <p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p>
    <p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p>
    <p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p>
    <p>Figure 3 shows how memory is accessed in the edge streaming model. The appeal of the streaming approach</p>
    <p>Edges (sequential read)</p>
    <p>Updates (sequential write)</p>
    <p>Vertices (random read/write)</p>
    <p>Updates (sequential read)</p>
    <p>Vertices (random read/write)</p>
    <p>Figure 3: Streaming Memory Access</p>
    <p>to graph processing stems from the fact that it allows sequential (and therefore much faster) access to Slow Storage for the (usually large) edge and update streams. The problem is that it requires random access to the vertices, which for large graphs may not fit in Fast Storage. To solve this problem, we introduce the notion of streaming partitions, described next.</p>
    <p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partitions vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partitions vertex set.</p>
    <p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p>
    <p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all</p>
    <p>X-Stream: Edge-centric Graph Processing using Streaming Par::ons, A. Roy et al., SOSP 2013</p>
    <p>ScaQer: for each streaming par::on</p>
    <p>load source vertex chunk of edges into fast memory stream edges append to several updates</p>
    <p>Gather: for each streaming par::on</p>
    <p>load des0na0on vertex chunk of updates into fast memory stream updates apply to ver:ces</p>
    <p>source</p>
    <p>des0na0on</p>
  </div>
  <div class="page">
    <p>Mo:va:on</p>
    <p>Ques:on: Is it possible to apply on-the-fly updates? (Thus bypass the writes and reads of updates.)</p>
    <p>set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p>
    <p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p>
    <p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Streams edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel [40].</p>
    <p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p>
    <p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p>
    <p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p>
    <p>Figure 3 shows how memory is accessed in the edge streaming model. The appeal of the streaming approach</p>
    <p>Edges (sequential read)</p>
    <p>Updates (sequential write)</p>
    <p>Vertices (random read/write)</p>
    <p>Updates (sequential read)</p>
    <p>Vertices (random read/write)</p>
    <p>Figure 3: Streaming Memory Access</p>
    <p>to graph processing stems from the fact that it allows sequential (and therefore much faster) access to Slow Storage for the (usually large) edge and update streams. The problem is that it requires random access to the vertices, which for large graphs may not fit in Fast Storage. To solve this problem, we introduce the notion of streaming partitions, described next.</p>
    <p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partitions vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partitions vertex set.</p>
    <p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p>
    <p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all</p>
    <p>set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p>
    <p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p>
    <p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Streams edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel [40].</p>
    <p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p>
    <p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p>
    <p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p>
    <p>Figure 3 shows how memory is accessed in the edge streaming model. The appeal of the streaming approach</p>
    <p>Edges (sequential read)</p>
    <p>Updates (sequential write)</p>
    <p>Vertices (random read/write)</p>
    <p>Updates (sequential read)</p>
    <p>Vertices (random read/write)</p>
    <p>Figure 3: Streaming Memory Access</p>
    <p>to graph processing stems from the fact that it allows sequential (and therefore much faster) access to Slow Storage for the (usually large) edge and update streams. The problem is that it requires random access to the vertices, which for large graphs may not fit in Fast Storage. To solve this problem, we introduce the notion of streaming partitions, described next.</p>
    <p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partitions vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partitions vertex set.</p>
    <p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p>
    <p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all</p>
    <p>source</p>
    <p>des0na0on</p>
    <p>Can be as large as O(E)!</p>
  </div>
  <div class="page">
    <p>Basic Idea</p>
    <p>Answer: Guarantee the locality of both source and des0na0on ver:ces when streaming edges!</p>
    <p>set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p>
    <p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p>
    <p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Streams edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel [40].</p>
    <p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p>
    <p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p>
    <p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p>
    <p>Figure 3 shows how memory is accessed in the edge streaming model. The appeal of the streaming approach</p>
    <p>Edges (sequential read)</p>
    <p>Updates (sequential write)</p>
    <p>Vertices (random read/write)</p>
    <p>Updates (sequential read)</p>
    <p>Vertices (random read/write)</p>
    <p>Figure 3: Streaming Memory Access</p>
    <p>to graph processing stems from the fact that it allows sequential (and therefore much faster) access to Slow Storage for the (usually large) edge and update streams. The problem is that it requires random access to the vertices, which for large graphs may not fit in Fast Storage. To solve this problem, we introduce the notion of streaming partitions, described next.</p>
    <p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partitions vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partitions vertex set.</p>
    <p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p>
    <p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all</p>
    <p>Streaming-Apply: for each streaming edge block</p>
    <p>load source and des0na0on vertex chunk of edges into memory stream edges read from source ver:ces write to des0na0on ver:ces</p>
    <p>source &amp; des0na0on</p>
  </div>
  <div class="page">
    <p>Solu:on  Grid representa:on  Dual sliding windows  Selec:ve scheduling</p>
    <p>2-level hierarchical par::oning</p>
  </div>
  <div class="page">
    <p>Edge Block(2, 1)</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3) Source Chunk 2</p>
    <p>Des:na:on Chunk 1</p>
    <p>Grid Representa:on</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Ver:ces par::oned into P equalized chunks  Edges par::oned into P  P blocks</p>
    <p>Row  source  Column  des:na:on</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Streaming-Apply Processing Model  Stream edges block by block</p>
    <p>Each block corresponding to two vertex chunks  Source chunk + des:na:on chunk  Fit into memory</p>
    <p>Difference with scaQer-gather  2 phases  1 phase  Updates are applied on-the-fly</p>
    <p>P=4 dest. chunk 2</p>
    <p>src. chunk 3</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows  Access edge blocks in column-oriented order  From lef to right</p>
    <p>Des:na:on window slides as column moves  From top to boQom</p>
    <p>Source window slides as row moves</p>
    <p>Op:mize write amount  1 pass over the des:na:on ver:ces</p>
    <p>P=4</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>!&quot;#$%&quot;&amp;'! = 1  ! + !  !&quot;#$%&quot;&amp;'! !&quot;#$%&amp;'%%!!!!&quot;(!)</p>
    <p>!</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Ini:alize</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Object I/O Amt.</p>
    <p>Edges 0</p>
    <p>Src. vertex 0</p>
    <p>Dest. vertex 0</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Stream Block (1, 1)</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Cache source vertex chunk 1 in memory (miss); Cache des:na:on vertex chunk 1 in memory (miss); Read edges (1, 2), (2, 1) from disk;</p>
    <p>Object I/O Amt.</p>
    <p>Edges 0  2</p>
    <p>Src. vertex 0  2</p>
    <p>Dest. vertex 0  2</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Stream Block (2, 1)</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Object I/O Amt.</p>
    <p>Edges 2  4</p>
    <p>Src. vertex 2  4</p>
    <p>Dest. vertex 2</p>
    <p>Cache source vertex chunk 2 in memory (miss); Cache des:na:on vertex chunk 1 in memory (hit); Read edges (3, 2), (4, 2) from disk;</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Stream Block (1, 2)</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Object I/O Amt.</p>
    <p>Edges 4  6</p>
    <p>Src. vertex 4  6</p>
    <p>Dest. vertex 2  6</p>
    <p>Cache source vertex chunk 1 in memory (miss); Cache des:na:on vertex chunk 2 in memory (miss);</p>
    <p>Write back des:na:on vertex chunk 1 to disk; Read edges (1, 3), (2, 4) from disk;</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Stream Block (2, 2)</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Object I/O Amt.</p>
    <p>Edges 6  7</p>
    <p>Src. vertex 6  8</p>
    <p>Dest. vertex 6</p>
    <p>Cache source vertex chunk 2 in memory (miss); Cache des:na:on vertex chunk 2 in memory (hit); Read edges (4, 3) from disk;</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>Dual Sliding Windows</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>NewPR</p>
    <p>Deg</p>
    <p>Itera:on 1 finishes</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Object I/O Amt.</p>
    <p>Edges 7</p>
    <p>Src. vertex 8</p>
    <p>Dest. vertex 6  8</p>
    <p>Write back des:na:on vertex chunk 2 to disk;</p>
    <p>P=2</p>
  </div>
  <div class="page">
    <p>I/O Access Amount  For 1 itera:on</p>
    <p>P pass over the source ver:ces (read)</p>
    <p>E + (2 + P)  V</p>
    <p>Implica:on: P should be the minimum value that enables needed vertex data to be fit into memory.</p>
  </div>
  <div class="page">
    <p>I/O Access Amount  For 1 itera:on</p>
    <p>P pass over the source ver:ces (read)</p>
    <p>E + (2 + P)  V</p>
    <p>Implica:on: P should be the minimum value that enables needed vertex data to be fit into memory.</p>
    <p>Disk</p>
    <p>Memory</p>
    <p>Cache</p>
  </div>
  <div class="page">
    <p>Memory Access Amount  For 1 itera:on</p>
    <p>P pass over the source ver:ces (read)</p>
    <p>E + (2 + P)  V</p>
    <p>Disk</p>
    <p>Memory</p>
    <p>Cache</p>
    <p>Implica:on: P should be the minimum value that enables needed vertex data to be fit into memory.</p>
  </div>
  <div class="page">
    <p>Selec:ve Scheduling  Skip blocks with no ac:ve edges  Very simple but important op:miza:on  Effec:ve for lots of algorithms</p>
    <p>BFS, WCC,</p>
  </div>
  <div class="page">
    <p>Selec:ve Scheduling</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>BFS from 1 with P = 2</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3) Itera:on 1</p>
    <p>Ac0ve True False False False</p>
    <p>Parent 1 -1 -1 -1</p>
    <p>Ac0ve False True True False</p>
    <p>Parent 1 1 1 -1</p>
    <p>Before</p>
    <p>Afer</p>
    <p>Access 4 edges</p>
  </div>
  <div class="page">
    <p>Selec:ve Scheduling</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>BFS from 1 with P = 2</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>Ac0ve False True True False</p>
    <p>Parent 1 1 1 -1</p>
    <p>Ac0ve False False False True</p>
    <p>Parent 1 1 1 2</p>
    <p>Before</p>
    <p>Afer</p>
    <p>Access 7 edges Itera:on 2</p>
  </div>
  <div class="page">
    <p>Selec:ve Scheduling</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>BFS from 1 with P = 2</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>Ac0ve False False False True</p>
    <p>Parent 1 1 1 2</p>
    <p>Ac0ve False False False False</p>
    <p>Parent 1 1 1 2</p>
    <p>Before</p>
    <p>Afer</p>
    <p>Access 3 edges Itera:on 3</p>
  </div>
  <div class="page">
    <p>Selec:ve Scheduling</p>
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>BFS from 1 with P = 2</p>
    <p>(1, 2) (2, 1)</p>
    <p>(1, 3) (2, 4)</p>
    <p>(3, 2) (4, 2)</p>
    <p>(4, 3)</p>
    <p>Parent 1 1 1 2</p>
    <p>in all BFS finishes</p>
  </div>
  <div class="page">
    <p>(a) An example graph</p>
    <p>! Block(2,)1)</p>
    <p>(1,)2)) (2,)1)</p>
    <p>(1,)3)) (2,)4)</p>
    <p>(3,)2)) (4,)2)</p>
    <p>(4,)3)Chunk)2) #</p>
    <p>Chunk)1) $</p>
    <p>(b) Grid representation</p>
    <p>Figure 1: Organization of the edge blocks</p>
    <p>Figure 2: Edge block size distribution of Twitter graph using a 32  32 partitioning.</p>
    <p>queue (to achieve substantial sequential disk bandwidth, we choose 24MB to be the size of each edge batch).</p>
    <p>After the partitioning process, GridGraph is ready to do computation. However, due to the irregular structure of real world graphs, some edge blocks might be too small to achieve substantial sequential bandwidth on HDDs. Figure 2 shows the distribution of edge block sizes in Twitter [12] graph using a 32  32 partitioning, which conforms to the power-law [7], with a large number of small files and a few big ones. Thus full sequential bandwidth can not be achieved sometimes due to potentially frequent disk seeks. To avoid such performance loss, an extra merge phase is required for GridGraph to perform better on HDD based systems, in which the edge block files are appended into a large file one by one and the start offset of each block is recorded in metadata. The time taken by each phase is shown in Section 4.</p>
    <p>X-Stream, on the other hand, does not require explicit preprocessing. Edges are shuffled to several files according to the streaming partition. No sorting is required and the number of partitions is quite small. For many graphs that all the vertex data can be fit into memory, only one streaming partitions is needed. However, this partitioning strategy makes it inefficient for selective scheduling, which can largely affect its performance on many iterative algorithms that only a portion of the vertices are used in some iterations.</p>
    <p>It takes very short time for GridGraph to complete the preprocessing. Moreover, the generated grid format can be utilized in all algorithms running on the same graph. By partitioning, GridGraph is able to conduct selective scheduling and reduce uncessary accesses to edge blocks without active edges1. We can see that this contributes a lot in many iterative algorithms like BFS and WCC (see Section 4), which a large portion of vertices are inactive in many iterations.</p>
    <p>The selection of the number of partitions P is very important. With a more fine-grained partitioning (which means a larger value of P), while the preprocessing time becomes longer, better access locality of vertex data and more potential in selective scheduling can be achieved. Thus a larger P is preferred in partitioning. Currently, we choose P in such a way that the vertex data can be fit into last level cache. We choose P to be the minimum integer such that</p>
    <p>V P U  C,</p>
    <p>where C is the size of last level cache and U is the data size of each vertex. This partitioning shows not only good performance (especially for in-memory situations) but also reasonable preprocessing cost. In Section 4, we evaluate the impact of P and discuss the trade-offs inside.</p>
    <p>GridGraph uses a streaming-apply processing model in which only one (read-only) pass over the edges is required and the write I/O amount is optimized to one pass over the vertices.</p>
    <p>Impact of P on Selec:ve Scheduling BFS from 1</p>
    <p>Effect becomes beQer with more fine-grained par::oning.</p>
    <p>P 1 2 4</p>
    <p>Edge accesses 21(=7+7+7) 14=(4+7+3) 7=(2+3+2)</p>
    <p>Implica:on: A larger value of P is preferred.</p>
  </div>
  <div class="page">
    <p>Dilemma on Selec:on of P</p>
    <p>P small large</p>
    <p>Coarse-grained Fewer accesses on ver:ces Poorer locality Less selec:ve scheduling</p>
    <p>Fine-grained BeQer locality More selec:ve scheduling More accesses on ver:ces</p>
  </div>
  <div class="page">
    <p>Dilemma from Memory Hierarchy  Different selec:ons of P  Disk  Memory hierarchy</p>
    <p>Fit hot vertex data into memory  Memory  Cache hierarchy</p>
    <p>Fit hot vertex data into cache  Disk  Memory  Cache</p>
    <p>? Disk</p>
    <p>Memory</p>
    <p>Cache</p>
  </div>
  <div class="page">
    <p>Q  V / M  P  V / C  C &lt;&lt; M  P &gt;&gt; Q</p>
    <p>Group the small blocks into larger ones</p>
    <p>P = number of par::ons, M = memory capacity, C = LLC capacity, V = size of ver:ces</p>
    <p>P=4 Q=2</p>
  </div>
  <div class="page">
    <p>Programming Interface  StreamVer:ces(Fv, F)</p>
    <p>StreamEdges(Fe, F)</p>
    <p>Algorithm 1 Vertex Streaming Interface function STREAMVERTICES(Fv,F )</p>
    <p>Sum = 0 for each vertex do</p>
    <p>if F(vertex) then Sum += Fv(edge)</p>
    <p>end if end for return Sum</p>
    <p>end function</p>
    <p>Algorithm 2 Edge Streaming Interface function STREAMEDGES(Fe,F )</p>
    <p>Sum = 0 for each active block do . block with active edges</p>
    <p>for each edge 2 block do if F(edge.source) then</p>
    <p>Sum += Fe(edge) end if</p>
    <p>end for end for return Sum</p>
    <p>end function</p>
    <p>F is an optional user defined function which accepts a vertex as input and should returns a boolean value to indicate whether the vertex is needed in streaming. It is used when the algorithm needs selective scheduling to skip some useless streaming and is often used together with a bitmap, which can express the active vertex set compactly. Fe and Fv are user defined functions which describe the behavior of streaming. They accept an edge (for Fe), or a vertex (for Fv) as input, and should return a value of type R. The return values are accumulated and as the final reduced result to user. This value is often used to get the number of activated vertices, but is not restricted to this usage, e.g. users can use this function to get the sum of differences between iterations in PageRank to decide whether to stop computation.</p>
    <p>GridGraph stores vertex data on disks. Each vertex data file corresponds to a vertex data vector. We use the memory mapping mechanism to reference vertex data backed in files. It provides convenient and transparent access to vectors, and simplifies the programming model: developers can treat it as normal arrays just as if they are in memory.</p>
    <p>We use PageRank [19] as an example to show how to implement algorithms using GridGraph (shown in Algorithm 32). PageRank is a link analysis algorithm that</p>
    <p>calculates a numerical weighting to each vertex in the graph to measure its relative importance among the vertices. The PR value of each vertex is initialized to 1. In each iteration, each vertex sends out their contributions to neighbors, which is the current PR value divided by its out degree. Each vertex sums up the contributions collected from neighbors and sets it as the new PR value. It converges when the mean difference reaches some threshold3.</p>
    <p>Algorithm 3 PageRank function CONTRIBUTE(e)</p>
    <p>Accum(&amp;NewPR[e.dest], PR[e.source]Deg[e.source] ) end function function COMPUTE(v)</p>
    <p>NewPR[v] = 1  d + d  NewPR[v] return |NewPR[v] PR[v]|</p>
    <p>end function d = 0.85 PR = {1,...,1} Converged = 0 while Converged do</p>
    <p>NewPR = {0,...,0} StreamEdges(Contribute) Diff = StreamVertices(Compute) Swap(PR, NewPR) Converged = DiffV  Threshold</p>
    <p>end while</p>
    <p>The access sequence of blocks can be row-oriented or column-oriented, based on the update pattern. Assume that a vertex state is propagated from the source vertex to the destination vertex (which is the typical pattern in a lot of applications), i.e. source vertex data is read and destination vertex data is written. Since the column of each edge block corresponds to the destination vertex chunk, column oriented access order is preferred in this case. The destination vertex chunk is cached in memory when GridGraph streams blocks in the same column from top to bottom, so that expensive disk write operations are aggregated and minimized. This property is very impor</p>
  </div>
  <div class="page">
    <p>Applica:ons  BFS, WCC, SpMV, PageRank</p>
    <p>Evalua:on  Test environment  AWS EC2 i2.xlarge</p>
    <p>4 hyperthread cores  30.5GB memory  1  800GB SSD</p>
    <p>AWS EC2 d2.xlarge  4 hyperthread cores  30.5GB memory  3  2TB HDD</p>
    <p>(a) 4x4 grid</p>
    <p>(b) 2x2 virtual grid</p>
    <p>Figure 4: A 2-Level Hierarchical Partitioning Example. The number inside each block indicates the access sequence.</p>
    <p>This 2-level hierarchical partitioning provides not only flexibility but also efficiency since the higher level partitioning is virtual and GridGraph is able to utilize the outcome of lower level partitioning thus no more actual overhead is added. At the same time, good properties of the original fine-grained edge grid such as more selective scheduling chances can still be leveraged.</p>
    <p>GridGraph streams each block sequentially. Before streaming, GridGraph first checks the activeness of each vertex chunk. Edge blocks are streamed one by one in the sequence that dual sliding windows needs, and if the corresponding source vertex chunk of the block is active, it is added to the task list.</p>
    <p>GridGraph does computation as follows:</p>
    <p>Each edge is first checked by a user defined filter function F , and if the source vertex is active, Fe is called on this edge to apply updates onto the source or destination vertex (note that we do not encourage users to apply updates onto both the source and destination vertex, which might make the memory mapped vector suffer from unexpected write backs onto the slow level storage).</p>
    <p>We evaluate GridGraph on several real world social graphs and web graphs, and shows significant performance improvement compared with current out-of-core</p>
    <p>Dataset V E Data size P</p>
    <p>LiveJournal 4.85M 69.0M 527 MB 4 Twitter 61.6M 1.47B 11 GB 32</p>
    <p>UK 106M 3.74B 28 GB 64 Yahoo 1.41B 6.64B 50 GB 512</p>
    <p>Table 2: Graph datasets used in evaluation.</p>
    <p>graph engines. GridGraph is even competitive with distributed systems when more powerful hardware can be utilized.</p>
    <p>For the I/O scalability evaluation, we also use more powerful i2.2xlarge, i2.4xlarge, and i2.8xlarge instances, which contain multiple (2, 4, 8) 800GB SSDs, as well as more (8, 16, 32) cores and (61GB, 122GB, 244GB) memory.</p>
    <p>For each system, we run BFS, WCC, SpMV and Pagerank on 4 datasets: LiveJournal [2], Twitter [12], UK [4] and Yahoo [29]. All the graphs are real-world graphs with power-law degree distributions. LiveJournal and Twitter are social graphs, showing the following relationship between users within each online social network. UK and Yahoo are web graphs that consist of hyperlink relationships between web pages, with larger diameters than social graphs. Table 2 shows the magnitude, as well as our selection of P for each graph. For BFS and WCC, we run them until convergence, i.e. no more vertices can be found or updated; for SpMV, we run one iteration to calculate the multiplication result; and for PageRank, we run 20 iterations on each graph.</p>
  </div>
  <div class="page">
    <p>BFS WCC SpMV PageRank</p>
    <p>GraphChi</p>
    <p>X-Stream</p>
    <p>GridGraph</p>
    <p>LiveJournal</p>
    <p>BFS WCC SpMV PageRank</p>
    <p>GraphChi</p>
    <p>X-Stream</p>
    <p>GridGraph</p>
    <p>TwiQer</p>
    <p>BFS WCC SpMV PageRank</p>
    <p>GraphChi</p>
    <p>X-Stream</p>
    <p>GridGraph</p>
    <p>UK Yahoo Run0me(S) BFS WCC SpMV PageRank</p>
    <p>GraphChi - 114162 2676 13076</p>
    <p>X-Stream - - 1076 9957</p>
    <p>GridGraph 16815 3602 263.1 4719</p>
    <p>- indicates failing to finish in 48 hours i2.xlarge, memory limited to 8GB 35</p>
  </div>
  <div class="page">
    <p>Disk Bandwidth Usage</p>
    <p>i2.xlarge (SSD) d2.xlarge (HDD) BFS WCC SpMV PageR. BFS WCC SpMV PageR.</p>
    <p>LiveJournal GraphChi 22.81 17.60 10.12 53.97 21.22 14.93 10.69 45.97 X-Stream 6.54 14.65 6.63 18.22 6.29 13.47 6.10 18.45 GridGraph 2.97 4.39 2.21 12.86 3.36 4.67 2.30 14.21</p>
    <p>Twitter GraphChi 437.7 469.8 273.1 1263 443.3 406.1 220.7 1064 X-Stream 435.9 1199 143.9 1779 408.8 1089 128.3 1634 GridGraph 204.8 286.5 50.13 538.1 196.3 276.3 42.33 482.1</p>
    <p>UK GraphChi 2768 1779 412.3 2083 3203 1709 401.2 2191 X-Stream 8081 12057 383.7 4374 7301 11066 319.4 4015 GridGraph 1843 1709 116.8 1347 1730 1609 97.38 1359</p>
    <p>Yahoo GraphChi - 114162 2676 13076 - 106735 3110 18361 X-Stream - - 1076 9957 - - 1007 10575 GridGraph 16815 3602 263.1 4719 30178 4077 277.6 5118</p>
    <p>Table 1: Execution time (in seconds) with 8 GB memory. - indicates that the corresponding system failed to finish execution in 48 hours.</p>
    <p>GraphChi runs all algorithms in asynchronous mode, and an in-memory optimization is used when the number of vertices are small enough, so that vertex data can be allocated and hold in memory and thus edges are not modified during computation, which contributes a lot to performance on Twitter and UK graph.</p>
    <p>Table 1 presents the performance of chosen algorithms on different graphs and systems with memory limited to 8GB to illustrate the applicability. Under this configuration, only the LiveJournal graph can be fit into memory, while other graphs require access to disks. We can see that GridGraph outperforms GraphChi and X-Stream on both HDD based d2.xlarge and SSD based i2.xlarge instances, and the performance does not vary much except for BFS on Yahoo, which lots of seek is experienced during the computation, thus making SSDs more advantageous than HDDs. In fact, sometimes better results can be achieved on d2.xlarge instance due to the fact that the peak sequential bandwidth of 3 HDDs on d2.xlarge is slightly greater than that of 1 SSD on i2.xlarge.</p>
    <p>Figure 5 shows the disk bandwidth usage of 3 systems, which records the I/O throughput of a 10-minute interval running PageRank on Yahoo graph, using a d2.xlarge instance. X-Stream and GridGraph are available to exploit high sequential disk bandwidth while GraphChi is not so ideal due to more fragmented reads and writes across many shards. GridGraph try to minimize write amount thus more I/O is spent on read while X-Stream has to write a lot more data.</p>
    <p>Figure 5: Disk bandwidth usage chart of a 10-minute interval on GraphChi, X-Stream and GridGraph. R = average read bandwidth; W = average write bandwidth.</p>
    <p>For algorithms that all the vertices are active in computation, like SpMV and PageRank (thus every edge is streamed), GridGraph has significant reduction in I/O amount that is needed to complete computation. GraphChi needs to read from and write to edges to propagate vertex states, thus 2  E I/O amount to edges are required. X-Stream needs to read edges and generate updates in scatter phase, and read updates in gather phase, thus a total I/O amount of 3  E is required (note that the size of updates is in the same magnitude as edges). On the other hand, GridGraph only requires one read pass over the edges and several passes over the vertices. The write amount is also optimized in GridGraph, which</p>
    <p>I/O throughput of a 10-minute interval running PageRank on Yahoo graph 36</p>
  </div>
  <div class="page">
    <p>Effect of Dual Sliding Windows</p>
    <p>PageRank on Yahoo</p>
    <p>Reads Writes</p>
    <p>I/ O A m ou</p>
    <p>nt</p>
    <p>GraphChi</p>
    <p>X-Stream</p>
    <p>GridGraph</p>
  </div>
  <div class="page">
    <p>Effect of Selec:ve Scheduling</p>
    <p>WCC on TwiQer</p>
    <p>I/ O A m ou</p>
    <p>nt</p>
    <p>Itera0on</p>
    <p>X-Stream</p>
    <p>GridGraph</p>
  </div>
  <div class="page">
    <p>Impact of P on In-Memory Performance</p>
    <p>Ru n0</p>
    <p>m e( s)</p>
    <p>P</p>
    <p>PageRank on TwiQer Edges cached in 30.5GB memory</p>
    <p>Op:mal around P=32 where needed vertex data can be fit into L3 cache.</p>
  </div>
  <div class="page">
    <p>Impact of Q on Out-of-Core Performance</p>
    <p>SpMV on Yahoo Memory limited to 8GB</p>
    <p>Op:mal at Q=2 where needed vertex data can be fit into memory.</p>
    <p>Ru n0</p>
    <p>m e( s)</p>
  </div>
  <div class="page">
    <p>Comparison with Distributed Systems</p>
    <p>PowerGraph, GraphX *  16  m2.4xlarge, $15.98/h</p>
    <p>GridGraph  i2.4xlarge, $3.41/h</p>
    <p>4 SSDs  1.8GB/s disk bandwidth</p>
    <p>* GraphX: Graph Processing in a Distributed Dataflow Framework, JE Gonzalez et al., OSDI 2014 41</p>
    <p>TwiQer WCC</p>
    <p>TwiQer PageRank</p>
    <p>UK WCC</p>
    <p>UK PageRank</p>
    <p>PowerGraph</p>
    <p>GraphX</p>
    <p>GridGraph</p>
  </div>
  <div class="page">
    <p>Conclusion  GridGraph  Dual sliding windows</p>
    <p>Reduce I/O amount, especially writes  Selec:ve scheduling</p>
    <p>Reduce unnecessary I/O  2-level hierarchical grid par::oning</p>
    <p>Applicable to 3-level (cache-memory-disk) hierarchy</p>
  </div>
  <div class="page">
    <p>Thanks!</p>
  </div>
</Presentation>
