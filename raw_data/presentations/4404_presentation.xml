<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Learning Matching Models with Weak Supervision for Response Selection</p>
    <p>in Retrieval-based Chatbots</p>
    <p>Yu Wu SKLSDE, Beihang University</p>
    <p>wuyu@buaa.edu.cn</p>
    <p>Wei Wu Microsoft Corporation wuwei@microsoft.com</p>
    <p>Zhoujun Li SKLSDE, Beihang University</p>
    <p>lizj@buaa.edu.cn</p>
    <p>Ming Zhou Microsoft Research</p>
    <p>mingzhou@microsoft.com</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Task, challenges, and ideas</p>
    <p>Our approach  A new learning method for matching models.</p>
    <p>Experiment  Datasets</p>
    <p>Evaluation and analysis</p>
  </div>
  <div class="page">
    <p>Task: retrieval-based chatbots</p>
    <p>Given a message, find most suitable responses</p>
    <p>Large repository of message-response pairs</p>
    <p>Take it as a search problem</p>
    <p>Retrieval Feature</p>
    <p>generation Ranking</p>
    <p>Context-response matching</p>
    <p>Learning to rankIndex</p>
    <p>Context Responses</p>
  </div>
  <div class="page">
    <p>Related Work</p>
    <p>Previous works focus on network architectures.  Single Turn</p>
    <p>CNN, RNN, syntactic based neural networks .</p>
    <p>Multiple Turn  CNN, RNN, attention mechanism</p>
    <p>These models are data hungry, so they are trained on large scale negative sampled dataset.</p>
    <p>State-of-the-art multi-turn architecture (Wu et al. ACL 2017)</p>
  </div>
  <div class="page">
    <p>Background-----Loss Function</p>
    <p>Cross Entropy Loss (Pointwise loss) Hinge Loss (Pairwise loss)</p>
    <p>=   plog( )   +    &gt;</p>
    <p>= max(0,    + + )</p>
  </div>
  <div class="page">
    <p>Background: traditional training method</p>
    <p>Given a (Q,R) pair, we first randomly</p>
    <p>sampled N instances ,</p>
    <p>.</p>
    <p>Update the designed model with the use of point-wise cross</p>
    <p>entropy loss.</p>
    <p>Test model on human annotation</p>
    <p>data.</p>
    <p>Two problem: 1. Most of the randomly sampled responses are far from the semantics</p>
    <p>of the messages or the contexts. 2. Some of randomly sampled responses are false negatives which</p>
    <p>pollute the training data as noise.</p>
  </div>
  <div class="page">
    <p>Challenges of Response Selection in Chatbots</p>
    <p>Negative sampling oversimplifies response selection task in the training phrase.  Train: Given a utterance, positive responses are collected from human</p>
    <p>conversations, but negative ones are negative sampled.</p>
    <p>Test: Given a utterance, a bunch of responses are returned by a search engine. Human annotators are asked to label these responses.</p>
    <p>Human labeling is expensive and exhausting, one cannot have large scale labeled data for model training.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Task, challenges, and ideas</p>
    <p>Our approach  A new learning method for matching models.</p>
    <p>Experiment  Datasets</p>
    <p>Evaluation and analysis</p>
  </div>
  <div class="page">
    <p>Our Idea</p>
    <p>Index</p>
    <p>Out training process</p>
    <p>Query</p>
    <p>R R_1 R_2 R_3</p>
    <p>R_N</p>
    <p>R is the ground-truth response, and R_i is a retrieved instance.</p>
    <p>Hinge loss (, )  (, 1 ) + 1 (, )  (,2) + 2 (, )  (,3 ) + 3</p>
    <p>(, )  (,_ ) + _</p>
    <p>Optimization</p>
    <p>_ is a confidence score for each instance. Our method encourages the model to be more confident to classify a response with a high  as a negative one.</p>
    <p>The margin in our loss is dynamic.</p>
  </div>
  <div class="page">
    <p>How to calculate the dynamic margin?</p>
    <p>We employ a Seq2Seq model to compute .  Seq2Seq model is a unsupervised</p>
    <p>model.</p>
    <p>It is able to compute a conditional probability likelihood    without human annotation.</p>
    <p>= max(0, 2 ,</p>
    <p>2 ,  1)</p>
  </div>
  <div class="page">
    <p>A new training method</p>
    <p>Pre-train the matching model</p>
    <p>with negative sampling and cross</p>
    <p>entropy loss.</p>
    <p>Given a (Q,R) pair, retrieve N instances</p>
    <p>,</p>
    <p>from a pre-defined index.</p>
    <p>Update the designed model</p>
    <p>with the dynamic hinge loss.</p>
    <p>Test model on human annotation</p>
    <p>da</p>
    <p>The pre-training process enables the matching model to distinguish semantically far away responses.</p>
  </div>
  <div class="page">
    <p>Outline</p>
    <p>Task, challenges, and ideas</p>
    <p>Our approach  A new learning method for matching models.</p>
    <p>Experiment  Datasets</p>
    <p>Evaluation and analysis</p>
  </div>
  <div class="page">
    <p>Dataset</p>
    <p>STC data set (Wang et al., 2013)  Single-turn response selection  Over 4 million post-response pairs (true response) in Weibo for training.  The test set consists of 422 posts with each one associated with around 30</p>
    <p>responses labeled by human annotators in good and bad.</p>
    <p>Douban Conversation Corpus (Wu et al., 2017)  Multi-turn response selection  0.5 million context-response (true response) pairs for training  In the test set, every context has 10 response candidates, and each of the</p>
    <p>response has a label good or bad judged by human annotators.</p>
  </div>
  <div class="page">
    <p>Evaluation Results</p>
  </div>
  <div class="page">
    <p>Ablation Test</p>
    <p>+WSrand: negative samples are randomly generated.</p>
    <p>+const: the marginal in the loss function is a static number.</p>
    <p>+WS: Our full model</p>
  </div>
  <div class="page">
    <p>More Findings</p>
    <p>Updating the Seq2Seq model is not beneficial to the discriminator.</p>
    <p>The number of negative instances is an important hyperparameter for our model.</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>We study a less explored problem in retrieval-based chatbots.</p>
    <p>We propose of a new method that can leverage unlabeled data to learn matching models for retrieval-based chatbots.</p>
    <p>We empirically verify the effectiveness of the method on public data sets.</p>
  </div>
</Presentation>
