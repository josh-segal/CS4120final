<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Distributing Deep Neural Networks with Containerized Partitions at the Edge</p>
    <p>Li Zhou1, Hao Wen2, Radu Teodorescu1, David H.C. Du2</p>
    <p>July 9, 2019</p>
  </div>
  <div class="page">
    <p>Todays Problem: Move ML to the Edge</p>
    <p>ML: DNNs, CNNs  Tasks: computer vision (image</p>
    <p>recognition, object detection), natural language processing, etc.</p>
    <p>Run ML on IoT devices</p>
    <p>IoT: edge devices, 5G  Applications: smart homes, cities,</p>
    <p>autonomous cars, etc.</p>
    <p>Figures are from internet sources</p>
  </div>
  <div class="page">
    <p>DNNs at the Edge</p>
    <p>Privacy concerns (GDPR)  Cloud availability, data transfer latency</p>
    <p>Cloud computing (model is executed by cloud-only, or edge-cloud collaboration)</p>
    <p>Figure is from internet sources</p>
  </div>
  <div class="page">
    <p>DNNs at the Edge</p>
    <p>Model compression: smaller model  Customized accelerator: faster device  Pipeline: throughput improved  Parallelization: model 1) partition and 2)</p>
    <p>parallelization</p>
    <p>Goal: Efficient deployment of DNNs inference at the Edge with IoT devices</p>
    <p>Edge computing (model is stored and executed in edge devices)</p>
  </div>
  <div class="page">
    <p>Model Partition Problem and Contribution</p>
    <p>How to partition the workload efficiently across devices?  How to account for the higher communication latency in wirelessly connected devices?  How to optimize execution across heterogeneous devices possessing different compute</p>
    <p>capabilities?</p>
  </div>
  <div class="page">
    <p>Design Overview</p>
    <p>Deployment</p>
    <p>Distribute and execute containerized partitions across edge devices through k8s</p>
    <p>Hardware Profiling</p>
    <p>Linear regression models for  DNN layers  Network</p>
    <p>Model Partition and Parallelization</p>
    <p>Find out the optimal partition points for a given model based on current computational resources and network</p>
  </div>
  <div class="page">
    <p>Layer-wise Parallelization</p>
    <p>Each layer is parallelized independently  Each device compute a subset of output  Partial outputs are aggregated, partitioned</p>
    <p>before the execution of the next layer</p>
    <p>Substantial comm overhead  Benefits may be defeated</p>
  </div>
  <div class="page">
    <p>Fused-layer Parallelization</p>
    <p>Consecutive conv layers are first fused as a block  Partitioning is performed layer-by-layer starting from</p>
    <p>the last layer in the fused block.  Input elements are recursively calculated based on</p>
    <p>output  Extend conv layer by fi/2 on height and width</p>
    <p>Less comm, more comp</p>
  </div>
  <div class="page">
    <p>Dynamic Programming based Search</p>
    <p>Run a model G on a list of devices D with parallelization S  G = Glw  Gfl, S = Slw  Sfl</p>
    <p>Minimize: T(G, D, S) = li  G lw tl(i) + l(i, j)  G</p>
    <p>fl tl(i, j)  We solve the problem with memorized DP, search all the possible combinations, to</p>
    <p>minimize T</p>
    <p>Layer: l(i), t l (i) Fused-layer block:</p>
    <p>l(i,j), t f (i,j)</p>
  </div>
  <div class="page">
    <p>Deployment</p>
    <p>Docker container + Kubernetes  Small scheduling unit (flexibility): each partition</p>
    <p>runs in a pod  Reduce config complexity: all pods are running</p>
    <p>based on the same Docker image</p>
    <p>Pipeline: partitions belonging to different blocks form a pipeline (one device for each stage in evaluation).</p>
    <p>Parallel: partitions belonging to the same block run in parallel</p>
  </div>
  <div class="page">
    <p>Latency Improvement</p>
    <p>Limited by network  Speedup: VGG-16: 1.6x ~ 3.7x, YOLOv2: 1.3x ~ 1.9x  Implication: more improvements are observed under faster network, and less compute</p>
    <p>capability</p>
    <p>Speedup is relative to 1 device at 1GHz/750MHz/500MHz in 100Mbps WIFI</p>
    <p>VGG-16 VGG-16 YOLOv2</p>
  </div>
  <div class="page">
    <p>Heterogeneity</p>
    <p>The input portion sizes are adjusted to the devices computing capability, e.g., one device runs at 1/2 compute capability of other devices</p>
    <p>Slow devices may be dropped  We achieve a reduction of 51% and 39%</p>
    <p>in execution time respectively in medium and fast network.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>This work proposes a collaborative convolutional neural network (CNN) acceleration framework for Internet-of-things (IoT)</p>
    <p>The framework leverages spatial partitioning through fusion of the convolution layers</p>
    <p>A dynamic programming-based search algorithm has been proposed to decide the optimal partition and parallelization for a DNN model</p>
    <p>Achieves 1.3x ~ 3.7x speedup with 8 edge devices for two popular CNN models</p>
  </div>
  <div class="page">
    <p>Discussion</p>
    <p>Feedback  Granularity for DNN containerizations: model vs partition  Impact of 5G for ML at the edge</p>
    <p>Controversial points  Efforts on parallelizing model vs customized accelerator or model compression</p>
    <p>Open issues  Efficient resource management and scheduling  QoS (Quality of service), Fault-tolerance</p>
    <p>Fall apart  Optimal solution falls into using 1 device under faster devices, slower network  Complex model like DenseNet</p>
  </div>
  <div class="page">
    <p>Thank you! zholi@cse.ohio-state.edu</p>
  </div>
</Presentation>
