<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Dynamic Cache Partitioning for Simultaneous Multithreading</p>
    <p>Systems G. Edward Suh Larry Rudolph Srinivas Devadas</p>
    <p>LCS, MIT</p>
  </div>
  <div class="page">
    <p>Simultaneous Multithreading (SMT) Systems</p>
    <p>Combines superscalar architecture with multithreaded architectures Low IPC comes from two sources</p>
    <p>Data dependencies Data delay (memory bottleneck)</p>
    <p>SMT relieves the dependency problem Helps to hide the memory latency</p>
    <p>SMT increases the total footprint Puts more pressure on the memory system</p>
  </div>
  <div class="page">
    <p>Strategy  Partition the Cache</p>
    <p>Control the amount of data for each thread minimizes the number of misses On-line monitoring of thread characteristics</p>
    <p>Marginal gain; gi(x): Additional hits by increasing the cache space from x blocks to x+1 blocks</p>
    <p>Deciding cache allocation to each thread Based on the marginal gain of each thread</p>
    <p>Partitioning mechanism Augmented LRU replacement policy</p>
  </div>
  <div class="page">
    <p>Example: Marginal Gains</p>
    <p>Cache: 4-way associative, 8192 sets 2 simultaneous threads Add 4 counters for each thread</p>
    <p>Counters for Thread 1</p>
    <p>Counters for Thread 2</p>
  </div>
  <div class="page">
    <p>Example: Marginal Gains</p>
    <p>Cache: 4-way associative, 8192 sets 2 simultaneous threads Add 4 counters for each thread</p>
    <p>Counters for Thread 1</p>
    <p>Counters for Thread 2</p>
    <p>Thread 1 Hit</p>
    <p>on the MRU Block</p>
  </div>
  <div class="page">
    <p>Example: Marginal Gains</p>
    <p>Cache: 4-way associative, 8192 sets 2 simultaneous threads Add 4 counters for each thread</p>
    <p>Counters for Thread 1</p>
    <p>Counters for Thread 2</p>
    <p>Thread 1 Hit</p>
    <p>on the 3rd MRU Block</p>
  </div>
  <div class="page">
    <p>Example: Marginal Gains</p>
    <p>Cache: 4-way associative, 8192 sets 2 simultaneous threads Add 4 counters for each thread</p>
    <p>Counters for Thread 1</p>
    <p>Counters for Thread 2</p>
    <p>Thread 2 Hit</p>
    <p>on the 2nd MRU Block</p>
  </div>
  <div class="page">
    <p>Example: Marginal Gains</p>
    <p>Cache: 4-way associative, 8192 sets 2 simultaneous threads Add 4 counters for each thread</p>
    <p>Counters for Thread 1</p>
    <p>Counters for Thread 2</p>
    <p>Marginal Gain of the First 8192 BlocksMarginal Gain of the Second 8192 Blocks</p>
  </div>
  <div class="page">
    <p>Example: Partitioning Decision</p>
    <p>Counters for Thread 1 250282409987</p>
    <p>Counters for Thread 2 24374615682111</p>
    <p>: 0Allocation to Thread 2</p>
    <p>: 0Allocation to Thread 1</p>
    <p>: 8192*4Unassigned Blocks</p>
  </div>
  <div class="page">
    <p>Example: Partitioning Decision</p>
    <p>Counters for Thread 1 250282409987</p>
    <p>Counters for Thread 2 24374615682111</p>
    <p>: 8192Allocation to Thread 2</p>
    <p>: 0Allocation to Thread 1</p>
    <p>: 8192*3Unassigned Blocks</p>
  </div>
  <div class="page">
    <p>Example: Partitioning Decision</p>
    <p>Counters for Thread 1 250282409987</p>
    <p>Counters for Thread 2 24374615682111</p>
    <p>: 16384Allocation to Thread 2</p>
    <p>: 0Allocation to Thread 1</p>
    <p>: 8192*2Unassigned Blocks</p>
  </div>
  <div class="page">
    <p>Example: Partitioning Decision</p>
    <p>Counters for Thread 1 250282409987</p>
    <p>Counters for Thread 2 24374615682111</p>
    <p>: 16384Allocation to Thread 2</p>
    <p>: 8192Allocation to Thread 1</p>
    <p>: 8192Unassigned Blocks</p>
  </div>
  <div class="page">
    <p>Example: Partitioning Decision</p>
    <p>Counters for Thread 1 250282409987</p>
    <p>Counters for Thread 2 24374615682111</p>
    <p>: 24576Allocation to Thread 2</p>
    <p>: 8192Allocation to Thread 1</p>
    <p>: 0Unassigned Blocks</p>
  </div>
  <div class="page">
    <p>AF DC</p>
    <p>AE DC</p>
    <p>A B DC</p>
    <p>AF DC</p>
    <p>AE DC</p>
    <p>A B DC</p>
    <p>Example: Augmented LRU</p>
    <p>Cache Set MRU LRU</p>
    <p>FG CA</p>
    <p>E</p>
    <p>F</p>
    <p>G</p>
  </div>
  <div class="page">
    <p>Experimental Setup</p>
    <p>On-line L2 cache partitioning Combine SimpleScalar with a cache simulator System configuration</p>
    <p>Executes up to 4 threads simultaneously 4 ALUs and 1 Multiplier 32-KB 8-way L1 caches (latency 1 cycle) Various size 8-way L2 caches (latency 10 cycles)</p>
    <p>Benchmarks SPEC CPU2000; art and mcf</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>Im pr</p>
    <p>ov em</p>
    <p>en t</p>
    <p>Memory Latency</p>
    <p>L2 S</p>
    <p>iz e</p>
    <p>IPC Improvement (Partitioned IPC/LRU IPC)</p>
  </div>
  <div class="page">
    <p>Discussion of Results</p>
    <p>Small caches Nothing helps: should change the workload</p>
    <p>Medium caches Partitioning helps Improvement related to latency (more than linear)</p>
    <p>Large caches Partitioning does not help: All workloads fit into the cache</p>
  </div>
  <div class="page">
    <p>Relevant Cache Sizes Partitioning helps for medium size caches Relevant cache sizes depend on the characteristics of threads and the number of active threads</p>
    <p>K B</p>
    <p>K B</p>
    <p>B</p>
    <p>B</p>
    <p>B</p>
    <p>Im pr</p>
    <p>ov em</p>
    <p>en t</p>
    <p>Memory Latency</p>
    <p>L2 Size</p>
    <p>IPC Improv e me nt: 2 thre ads</p>
    <p>K B</p>
    <p>K B</p>
    <p>B</p>
    <p>B</p>
    <p>B</p>
    <p>Im pr</p>
    <p>ov em</p>
    <p>en t</p>
    <p>Memory Latency</p>
    <p>L2 Size</p>
    <p>IPC Improv e me nt: 4 thre ads</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Simultaneous Multithreading may significantly degrade the cache performance Smart partitioning can relieve the problem for medium size caches</p>
    <p>The relevant size varies depending on the characteristics and the number of threads</p>
    <p>Cache-Aware thread scheduling is needed for small caches</p>
  </div>
</Presentation>
