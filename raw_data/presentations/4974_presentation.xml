<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Evaluation of ConnectX Virtual Protocol Interconnect for Data Centers</p>
    <p>Ryan E. Grant Ahmad Afsahi Pavan Balaji</p>
    <p>Department of Electrical and Computer Engineering, Queens University Mathematics and Computer Science, Argonne National Laboratory</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Data Centers: Towards a unified network stack  High End Computing (HEC) systems proliferating into all</p>
    <p>domains  Scientific Computing has been the traditional big customer  Enterprise Computing (large data centers) is increasingly becoming a</p>
    <p>competitor as well  Googles data centers  Oracles investment in high speed networking stacks (mainly through DAPL</p>
    <p>and SDP)  Investment from financial institutes such as Credit Suisse in low-latency</p>
    <p>networks such as InfiniBand</p>
    <p>A change of domain always brings new requirements with it  A single unified network stack is the holy grail!  Maintaining density and power, while achieving high performance</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>InfiniBand and Ethernet in Data Centers</p>
    <p>Ethernet has been the network of choice for data centers  Ubiquitous connectivity to all external clients due to backward</p>
    <p>compatibility  Internal communication, external communication and management are all</p>
    <p>unified on to a single network  There has also been a push for power to be distributed on the same</p>
    <p>channel as well (using Power over Ethernet), but thats still not a reality</p>
    <p>InfiniBand (IB) in data centers  Ethernet is (arguably) lagging behind with respect to some of the</p>
    <p>features provided by other high-speed networks such as IB  Bandwidth (32 Gbps vs. 10 Gbps today), features (scalability features such</p>
    <p>as shared queues while using zero-copy communication and RDMA)  The point of this paper is not about which is better, but to deal with the</p>
    <p>fact that data centers are looking for ways to converge both technologies</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Convergence of InfiniBand and Ethernet</p>
    <p>Researchers have been looking at different ways for a converged InfiniBand/Ethernet fabric  Virtual Protocol Interconnect (VPI)  InfiniBand over Ethernet (or RDMA over Ethernet)  InfiniBand over Converged Enhanced Ethernet (or RDMA over CEE)</p>
    <p>VPI is the first convergence model introduced by Mellanox Technologies, and will be the focus of study in this paper</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Single network firmware to support both IB and Ethernet</p>
    <p>Autosensing of layer-2 protocol  Can be configured to automatically</p>
    <p>work with either IB or Ethernet networks</p>
    <p>Multi-port adapters can use one port on IB and another on Ethernet</p>
    <p>Multiple use modes:  Data centers with IB inside the cluster</p>
    <p>and Ethernet outside  Clusters with IB network and Ethernet</p>
    <p>management</p>
    <p>Virtual Protocol Interconnect (VPI)</p>
    <p>IB Link LayerIB Link Layer</p>
    <p>IB Port Ethernet Port</p>
    <p>Hardware</p>
    <p>TCP/IP support TCP/IP</p>
    <p>support</p>
    <p>Ethernet Link Layer Ethernet Link Layer</p>
    <p>IB Network Layer</p>
    <p>IB Network Layer</p>
    <p>IPIP</p>
    <p>IB Transport Layer</p>
    <p>IB Transport Layer TCP</p>
    <p>TCP</p>
    <p>IB VerbsIB Verbs SocketsSockets</p>
    <p>ApplicationsApplications</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Goals of this paper</p>
    <p>To understand the performance and capabilities of VPI  Comparison of VPI-IB with VPI-Ethernet with different</p>
    <p>software stacks  Openfabrics Verbs  TCP/IP sockets (both traditional and through the Sockets Direct</p>
    <p>Protocol)  Detailed studies with micro-benchmarks and a Enterprise</p>
    <p>Data center setup</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Presentation Roadmap</p>
    <p>Introduction</p>
    <p>Micro-benchmark based Performance Evaluation</p>
    <p>Performance Analysis of Enterprise Data Centers</p>
    <p>Concluding Remarks and Future Work</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Software Stack Layout</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>Sockets ApplicationSockets Application</p>
    <p>Sockets APISockets API</p>
    <p>KernelKernel TCP/IP Sockets</p>
    <p>Provider</p>
    <p>TCP/IP Transport Driver</p>
    <p>Driver</p>
    <p>User</p>
    <p>VPI capable Network AdapterVPI capable Network Adapter</p>
    <p>Sockets Direct Protocol</p>
    <p>Sockets Direct Protocol</p>
    <p>(Possible) Kernel Bypass</p>
    <p>RDMA Semantics</p>
    <p>Verbs ApplicationVerbs Application</p>
    <p>Verbs APIVerbs API</p>
    <p>EthernetEthernet InfiniBandInfiniBand</p>
    <p>Zero-copy Communication</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Software Stack Layout (details)</p>
    <p>Three software stacks: TCP/IP, SDP and native verbs  VPI-Ethernet can only use TCP/IP  VPI-IB can use any one of the three</p>
    <p>TCP/IP and SDP provide transparent portability for existing data center applications over IB  TCP/IP is more mature (preferable for conservative data centers)  SDP can (potentially) provide better performance:</p>
    <p>Can internally use more of IB features than TCP/IP, since it natively utilizes IBs hardware implemented protocol (network and transport)</p>
    <p>But is not as mature: parts of the stack not as optimized as TCP/IP</p>
    <p>Native verbs is also a possibility, but requires modifications to existing data center applications (studies by Pandas group)</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Experimental Setup</p>
    <p>Four Dell PowerEdge R805 SMP servers  Each server has two quad-core 2.0 GHz AMD Opteron</p>
    <p>processors  12 KB instruction cache and 16 KB L1 data cache on each core  512 KB L2 cache for each core  2MB L3 cache on chip</p>
    <p>8 GB DDR2 SDRAM on an 1800 MHz memory controller  Each node has one ConnectX VPI capable adapter (4X DDR IB</p>
    <p>and 10Gbps Ethernet) on a PCIe x8 bus  Fedora Core 5 (linux kernel 2.6.20) was used with OFED 1.4  Compiler: gcc-4.1.1</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>One-way Latency and Bandwidth</p>
    <p>Message Size (bytes)</p>
    <p>La te</p>
    <p>nc y</p>
    <p>(u s)</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>IPoIB SDP 10GE Native Verbs</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>dw id</p>
    <p>th (M</p>
    <p>bp s)</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Multi-stream Bandwidth</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b ps</p>
    <p>)</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b ps</p>
    <p>)</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b ps</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Simultaneous IB/10GE Communication</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b ps</p>
    <p>)</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>Message Size (bytes)</p>
    <p>B an</p>
    <p>d w</p>
    <p>id th</p>
    <p>( M</p>
    <p>b ps</p>
    <p>)</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Presentation Roadmap</p>
    <p>Introduction</p>
    <p>Micro-benchmark based Performance Evaluation</p>
    <p>Performance Analysis of Enterprise Data Centers</p>
    <p>Concluding Remarks and Future Work</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Data Center Setup</p>
    <p>Three-tier data center  Apache 2 web server for static</p>
    <p>content  JBoss 5 application server for server</p>
    <p>side java processing  MySQL database system</p>
    <p>Trace workload: TPC-W benchmark representing a real web-based bookstore</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>ClientClient</p>
    <p>Web Server (Apache)</p>
    <p>Web Server (Apache)</p>
    <p>Application Server (JBoss)</p>
    <p>Application Server (JBoss)</p>
    <p>Database Server (MySQL)</p>
    <p>Database Server (MySQL)</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Data Center Throughput</p>
    <p>Time (seconds)</p>
    <p>W eb</p>
    <p>In st</p>
    <p>ru cti</p>
    <p>on s</p>
    <p>pe r</p>
    <p>Se co</p>
    <p>nd</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>Average 82.23</p>
    <p>Average 87.15</p>
    <p>Average 85.08</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Data Center Response Time (Itemized)</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Presentation Roadmap</p>
    <p>Introduction</p>
    <p>Micro-benchmark based Performance Evaluation</p>
    <p>Performance Analysis of Enterprise Data Centers</p>
    <p>Concluding Remarks and Future Work</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Concluding Remarks</p>
    <p>Increasing push for a converged network fabric  Enterprise data centers in HEC: power, density and performance</p>
    <p>Different convergence technologies upcoming: VPI was one of the first such technology introduced by Mellanox</p>
    <p>We studied the performance and capabilities of VPI with micro-benchmarks and an enterprise data center setup  Performance numbers indicate that VPI can give a reasonable</p>
    <p>performance boost to data centers without overly complicating the network infrastructure</p>
    <p>Whats still needed? Self-adapting switches  Current switches either do IB or 10GE, not both  On the roadmap for several switch vendors</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Future Work</p>
    <p>Improvements to SDP (of course)  We need to look at other convergence technologies as well</p>
    <p>RDMA over Ethernet (or CEE) is upcoming  Already accepted into the Open Fabrics Verbs  True convergence with respect to verbs</p>
    <p>InfiniBand features such as RDMA will automatically migrate to 10GE  All the SDP benefits will translate to 10GE as well</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Funding Acknowledgments</p>
    <p>Natural Sciences and Engineering Research Council of Canada  Canada Foundation of Innovation and Ontario Innovation</p>
    <p>Trust  US Office of Advanced Scientific Computing Research (DOE</p>
    <p>ASCR)  US National Science Foundation (NSF)  Mellanox Technologies</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
  </div>
  <div class="page">
    <p>Thank you!</p>
    <p>Contacts: Ryan Grant: ryan.grant@queensu.ca Ahmad Afsahi: ahmad.afsahi@queensu.ca Pavan Balaji: balaji@mcs.anl.gov</p>
  </div>
  <div class="page">
    <p>Backup Slides</p>
  </div>
  <div class="page">
    <p>Pavan Balaji, Argonne National Laboratory</p>
    <p>Data Center Response Time (itemized)</p>
    <p>Home Prod. Detail Search Request Shopping Cart Buy Request Order Inquiry Admin Request</p>
    <p>Time (seconds)</p>
    <p>% In</p>
    <p>te ra</p>
    <p>cti o</p>
    <p>n s</p>
    <p>ICPADS (12/09/2009), Shenzhen, China</p>
    <p>Home Prod. Detail Search Request Shopping Cart Buy Request Order Inquiry Admin Request</p>
    <p>Time (seconds)</p>
    <p>% In</p>
    <p>te ra</p>
    <p>cti o</p>
    <p>n s</p>
    <p>Home Prod. Detail Search Request Shopping Cart Buy Request Order Inquiry Admin Request</p>
    <p>Time (seconds)</p>
    <p>% In</p>
    <p>te ra</p>
    <p>cti o</p>
    <p>n s</p>
  </div>
</Presentation>
