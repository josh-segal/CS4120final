<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>USENIX ATC20 2020 USENIX Annual Technical Conference JULY 1517, 2020</p>
    <p>FineStream: Fine-Grained Window-Based Stream Processing on CPU-GPU Integrated Architectures</p>
    <p>Feng Zhang, Lin Yang, Shuhao Zhang, Bingsheng He, Wei Lu, Xiaoyong Du</p>
    <p>Renmin University of China</p>
    <p>Technische Universitat Berlin</p>
    <p>National University of Singapore</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Bulk-synchronous parallel model  query granularity</p>
    <p>Continuous operator model  operator granularity</p>
    <p>operator 1</p>
    <p>operator 2</p>
    <p>operator n</p>
    <p>query CPU</p>
    <p>GPU</p>
    <p>operator 1</p>
    <p>operator 2</p>
    <p>operator n</p>
    <p>query CPU</p>
    <p>GPU</p>
    <p>CPU and GPU can concurrently execute in both cases  only the granularity is different.</p>
    <p>This paper[SIGMOD16] Saber: Window-based hybrid stream</p>
    <p>processing for heterogeneous architectures</p>
  </div>
  <div class="page">
    <p>2011, Jan</p>
    <p>AMD APU  2014, Apr</p>
    <p>Nvidia Tegra</p>
    <p>Benefits  No PCI-e transfer overhead</p>
    <p>Shared global memory</p>
    <p>High energy efficiency</p>
    <p>2012, Jan</p>
    <p>Intel Ivy Bridge</p>
  </div>
  <div class="page">
    <p>Integrated architectures vs. discrete architectures</p>
    <p>Integrated architectures Discrete architectures</p>
    <p>Architecture A10-7850K Ryzen5 2400G GTX 1080Ti V100</p>
    <p>#cores 512+4 704+4 3584 5120</p>
    <p>TFLOPS 0.9 1.7 11.3 14.1</p>
    <p>bandwidth (GB/s) 25.6 38.4 484.4 900</p>
    <p>price ($) 209 169 1100 8999</p>
    <p>TDP (W) 95 65 250 300</p>
  </div>
  <div class="page">
    <p>Data stream</p>
    <p>Window</p>
    <p>Operator</p>
    <p>Query</p>
    <p>* Batch</p>
    <p>tuple</p>
    <p>window w1</p>
    <p>window w2</p>
    <p>window size</p>
    <p>window slide</p>
    <p>data stream</p>
    <p>operator 1</p>
    <p>operator 2</p>
    <p>operator n</p>
    <p>query</p>
    <p>results</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Varying Operator-Device Preference</p>
    <p>CPU queue:</p>
    <p>GPU queue:</p>
    <p>time</p>
    <p>operator 2 aggregation</p>
    <p>operator 1 group-by</p>
    <p>operator 1 group-by</p>
    <p>operator 2 aggregation</p>
    <p>query on CPU query on GPU</p>
  </div>
  <div class="page">
    <p>Performance (tuples/s) of operators on the CPU and the GPU of the integrated architecture.</p>
    <p>Operator CPU only GPU only Device choice</p>
    <p>Projection 14.2 14.3 GPU</p>
    <p>Selection 13.1 14.1 GPU</p>
    <p>Aggregation 14.7 13.5 CPU</p>
    <p>Group-by 8.1 12.4 GPU</p>
    <p>Join 0.7 0.1 CPU</p>
  </div>
  <div class="page">
    <p>Fine-Grained Stream Processing  A fine-grained stream processing method that can consider both integrated</p>
    <p>architecture characteristics and operator features shall have better performance.  memory bandwidth limit</p>
    <p>operators - preferred devices</p>
    <p>CPU and GPU have good performance</p>
    <p>consider the interplay of operator features and architecture difference.</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Challenge 1: Application topology combined with architectural characteristics</p>
    <p>System DRAM</p>
    <p>CPU core</p>
    <p>CPU</p>
    <p>CPU core</p>
    <p>CPU core</p>
    <p>GPU core</p>
    <p>GPU</p>
    <p>GPU core</p>
    <p>GPU core</p>
    <p>Shared Memory Management Unit</p>
    <p>CPU Cache GPU Cache</p>
    <p>OP2 OP3</p>
    <p>OP5 OP6</p>
    <p>OP9 OP10</p>
    <p>OP7</p>
    <p>OP11</p>
  </div>
  <div class="page">
    <p>Challenge 2: SQL query plan optimization with shared main memory</p>
    <p>query on CPU</p>
    <p>query on GPU</p>
    <p>query on CPU</p>
    <p>query on GPU</p>
    <p>CPU queue:</p>
    <p>GPU queue:</p>
    <p>time</p>
    <p>CPU only 18.2 ms</p>
    <p>GPU only 6.7 ms</p>
    <p>CPU-GPU co-run 22.4 ms</p>
  </div>
  <div class="page">
    <p>Challenge 3: Adjustment for dynamic workload</p>
    <p>OP3</p>
    <p>OP1</p>
    <p>OP290%</p>
    <p>OP1</p>
    <p>OP210%</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Overview</p>
    <p>batch</p>
    <p>online profiling</p>
    <p>performance model</p>
    <p>op1 op2 op1</p>
    <p>operators</p>
    <p>dev dev dev</p>
    <p>device mapping</p>
    <p>dispatcher</p>
    <p>results</p>
    <p>streambatch</p>
    <p>SQL</p>
    <p>FineStream</p>
  </div>
  <div class="page">
    <p>Topology</p>
    <p>OP1 OP2 OP3</p>
    <p>OP4 OP5 OP6</p>
    <p>OP8 OP9 OP10</p>
    <p>OP7</p>
    <p>OP11</p>
    <p>branch1</p>
    <p>branch2</p>
    <p>branch3</p>
    <p>pathcritical</p>
  </div>
  <div class="page">
    <p>Optimization 1: Branch Co-Running</p>
    <p>tstage3</p>
    <p>timebranch 3</p>
    <p>branch 2</p>
    <p>branch 1</p>
    <p>tstage 1</p>
    <p>tstage2 tstage3</p>
    <p>timebranch 3</p>
    <p>branch 2</p>
    <p>branch 1</p>
    <p>tstage1 tstage2</p>
    <p>branch 3</p>
    <p>(a) Branch parallelism. (b) Branch scheduling optimization.</p>
  </div>
  <div class="page">
    <p>Optimization 2: Batch Pipeline</p>
    <p>OP3</p>
    <p>OP6</p>
    <p>OP10</p>
    <p>OP7</p>
    <p>OP11</p>
    <p>phase 1 phase 2 PH i: phase i B i: batch i</p>
    <p>OP1 OP2</p>
    <p>OP4 OP5</p>
    <p>OP8 OP9 PH1 B1 PH1 B2</p>
    <p>PH2 B1 PH2 B2</p>
    <p>time</p>
    <p>(a) Phase partitioning. (b) Batch pipeline.</p>
  </div>
  <div class="page">
    <p>Optimization 3: Handling Dynamic Workload  Light-Weight Resource Reallocation</p>
    <p>Query Plan Adjustment</p>
    <p>OP3</p>
    <p>OP1</p>
    <p>OP290%</p>
    <p>(a) 90% workload goes to OP2. (b) 90% workload goes to OP3.</p>
    <p>Shared memory</p>
    <p>GPU CUs</p>
    <p>CPU CUs</p>
    <p>Integrated architectures</p>
    <p>OP3</p>
    <p>OP1</p>
    <p>OP210%</p>
    <p>Shared memory</p>
    <p>CPU CUs</p>
    <p>GPU CUs</p>
    <p>Integrated architectures</p>
  </div>
  <div class="page">
    <p>thread 1</p>
    <p>stream 1</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>thread 2 operators</p>
    <p>OPi</p>
    <p>CPU</p>
    <p>GPU</p>
    <p>bandwidth utilization</p>
    <p>performance</p>
    <p>OP1</p>
    <p>parallelism utilization Branch</p>
    <p>Co-Running</p>
    <p>Batch Pipeline</p>
    <p>DAG 1 OP CPU% GPU%</p>
    <p>OP1</p>
    <p>OPi</p>
    <p>batch3</p>
    <p>batch4 dynamicworkload detection</p>
    <p>operator dataflow</p>
    <p>monitoring</p>
    <p>still low performance?</p>
    <p>time</p>
    <p>yes resource reallocation</p>
    <p>migration detected</p>
    <p>DAG i</p>
    <p>default</p>
    <p>batch1</p>
    <p>batch2</p>
    <p>query plan adjustment</p>
    <p>Execution flow</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>Platforms  AMD A10- 7850K</p>
    <p>Ryzen 5 2400G</p>
    <p>Datasets  Google compute cluster monitoring</p>
    <p>Anomaly detection in smart grids</p>
    <p>Linear road benchmark</p>
    <p>Synthetically generated dataset</p>
    <p>Benchmarks  Nine queries</p>
    <p>Example - Q1</p>
    <p>(Google compute cluster monitoring)</p>
    <p>select timestamp, category, sum(cpu)</p>
    <p>as totalCPU</p>
    <p>from TaskEvents [range 256 slide 1]</p>
    <p>group by category</p>
  </div>
  <div class="page">
    <p>Throughput: FineStream achieves the best performance in most cases.</p>
    <p>Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9</p>
    <p>A10-7850K Ryzen5 2400G</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (1</p>
    <p>E 5</p>
    <p>tu p</p>
    <p>le s/</p>
    <p>s)</p>
    <p>Single Saber FineStream</p>
  </div>
  <div class="page">
    <p>Latency: Low latency in most cases.</p>
    <p>Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9</p>
    <p>A10-7850K Ryzen5 2400G</p>
    <p>La te</p>
    <p>n cy</p>
    <p>( s)</p>
    <p>Single Saber FineStream</p>
  </div>
  <div class="page">
    <p>Throughput vs. latency  Queries with high throughput usually have low latency, and vice versa.</p>
    <p>T h</p>
    <p>ro u</p>
    <p>g h</p>
    <p>p u</p>
    <p>t (1</p>
    <p>E 5</p>
    <p>t u</p>
    <p>p le</p>
    <p>s/ s)</p>
    <p>Latency (s)</p>
    <p>FineStream(A10-7850K)</p>
    <p>Saber(A10-7850K)</p>
    <p>FineStream(Ryzen5)</p>
    <p>Saber(Ryzen)</p>
  </div>
  <div class="page">
    <p>Utilization  FineStream utilizes the GPU device better on the integrated architecture.</p>
    <p>Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9</p>
    <p>CPU GPU</p>
    <p>u ti</p>
    <p>li za</p>
    <p>ti o</p>
    <p>n (</p>
    <p>% )</p>
    <p>Saber FineStream</p>
  </div>
  <div class="page">
    <p>Comparison with Discrete Architectures  Throughput: The discrete GPUs exhibit 1.8x to 5.7x higher throughput than</p>
    <p>the integrated architectures, due to the more computational power of discrete GPUs.</p>
    <p>Latency:</p>
    <p>Discrete GPUs: Ttotal = TPCIe_transmit + Tcompute</p>
    <p>Integrated GPUs: Ttotal = Tcompute</p>
  </div>
  <div class="page">
    <p>Comparison with Discrete Architectures  High Price-Throughput Ratio</p>
    <p>Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9</p>
    <p>P ri</p>
    <p>ce -p</p>
    <p>e rf</p>
    <p>o rm</p>
    <p>a n</p>
    <p>ce r</p>
    <p>a ti</p>
    <p>o</p>
    <p>(p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n ce</p>
    <p>/U S</p>
    <p>D )</p>
  </div>
  <div class="page">
    <p>Comparison with Discrete Architectures  High Energy Efficiency</p>
    <p>Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9</p>
    <p>E n</p>
    <p>e rg</p>
    <p>y e</p>
    <p>ff ic</p>
    <p>ie n</p>
    <p>cy</p>
    <p>(p e</p>
    <p>rf o</p>
    <p>rm a</p>
    <p>n ce</p>
    <p>/W a</p>
    <p>tt )</p>
  </div>
  <div class="page">
    <p>Outline</p>
  </div>
  <div class="page">
    <p>The first fine-grained window-based relational stream processing.</p>
    <p>Lightweight query plan adaptations handling dynamic workloads.</p>
    <p>FineStream evaluation on a set of stream queries.</p>
    <p>fengzhang@ruc.edu.cn, yanglin2330@ruc.edu.cn, shuhao.zhang@tu-berlin.de, hebs@comp.nus.edu.sg, lu-wei@ruc.edu.cn, duyong@ruc.edu.cn</p>
    <p>Feng Zhang, Lin Yang, Shuhao Zhang, Bingsheng He, Wei Lu, Xiaoyong Du Renmin University of China, Technische Universitat Berlin, National University of Singapore</p>
    <p>USENIX ATC20 2020 USENIX Annual Technical Conference JULY 1517, 2020</p>
  </div>
</Presentation>
