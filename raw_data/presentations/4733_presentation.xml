<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Least Squares  Policy Iteration Bias-Variance Trade-o in Control Problems</p>
    <p>Christophe Thiry and Bruno Scherrer</p>
    <p>INRIA - LORIA - Maia Team</p>
    <p>June 2010</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 1/27</p>
  </div>
  <div class="page">
    <p>Markov Decision Processes (Puterman, 1994; Bertsekas &amp; Tsitsiklis, 1996; Sutton &amp; Barto, 1998)</p>
    <p>reward r action a  A</p>
    <p>Agent Policy  : X  A</p>
    <p>Reward function r(i, a)</p>
    <p>Environment Transition probabilities pij(a)</p>
    <p>state i  X</p>
    <p>Goal : Find a policy  : X  A that maximizes for all i</p>
    <p>v(i) = E</p>
    <p>[  k=0</p>
    <p>kr(ik,(ik ))</p>
    <p>i0 = i ]</p>
    <p>(0 &lt;  &lt; 1)</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 2/27</p>
  </div>
  <div class="page">
    <p>For any , the value v of a policy  satises</p>
    <p>v(i) = r(i,(i)) +   j</p>
    <p>pij ((i))v (j)  v = T v</p>
    <p>The optimal value v satises</p>
    <p>v(i) = max a</p>
    <p>r(i,a) +   j</p>
    <p>pij (a)v(j)  v = T v</p>
    <p>A policy  is greedy(v) if T v = T v</p>
    <p>Any policy  that is greedy with respect to the optimal value v is optimal (and v = v</p>
    <p>)</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 3/27</p>
  </div>
  <div class="page">
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 4/27</p>
  </div>
  <div class="page">
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 5/27</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Value Iteration</p>
    <p>vk+1 T vk</p>
    <p>Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  v</p>
    <p>k+1</p>
    <p>Modied Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  (T</p>
    <p>k+1)mvk m  N</p>
    <p>Policy Iteration (Bertsekas &amp; Ioe, 1996)</p>
    <p>k+1  greedy(vk) vk+1  (1)</p>
    <p>i=0</p>
    <p>i (T k+1)i+1vk   [0,1]</p>
    <p>Optimistic Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1</p>
    <p>i=0 i (T</p>
    <p>k+1)i+1vk i  0,</p>
    <p>i=0 i = 1</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 6/27</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Value Iteration</p>
    <p>k+1  greedy(vk) vk+1 T vk = T</p>
    <p>k+1vk</p>
    <p>Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  v</p>
    <p>k+1 = (T k+1)vk</p>
    <p>Modied Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  (T</p>
    <p>k+1)mvk m  N</p>
    <p>Policy Iteration (Bertsekas &amp; Ioe, 1996)</p>
    <p>k+1  greedy(vk) vk+1  (1)</p>
    <p>i=0</p>
    <p>i (T k+1)i+1vk   [0,1]</p>
    <p>Optimistic Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1</p>
    <p>i=0 i (T</p>
    <p>k+1)i+1vk i  0,</p>
    <p>i=0 i = 1</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 6/27</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Value Iteration</p>
    <p>k+1  greedy(vk) vk+1 T vk = T</p>
    <p>k+1vk</p>
    <p>Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  v</p>
    <p>k+1 = (T k+1)vk</p>
    <p>Modied Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  (T</p>
    <p>k+1)mvk m  N</p>
    <p>Policy Iteration (Bertsekas &amp; Ioe, 1996)</p>
    <p>k+1  greedy(vk) vk+1  (1)</p>
    <p>i=0</p>
    <p>i (T k+1)i+1vk   [0,1]</p>
    <p>Optimistic Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1</p>
    <p>i=0 i (T</p>
    <p>k+1)i+1vk i  0,</p>
    <p>i=0 i = 1</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 6/27</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Value Iteration</p>
    <p>k+1  greedy(vk) vk+1 T vk = T</p>
    <p>k+1vk</p>
    <p>Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  v</p>
    <p>k+1 = (T k+1)vk</p>
    <p>Modied Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  (T</p>
    <p>k+1)mvk m  N</p>
    <p>Policy Iteration (Bertsekas &amp; Ioe, 1996)</p>
    <p>k+1  greedy(vk) vk+1  (1)</p>
    <p>i=0</p>
    <p>i (T k+1)i+1vk   [0,1]</p>
    <p>Optimistic Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1</p>
    <p>i=0 i (T</p>
    <p>k+1)i+1vk i  0,</p>
    <p>i=0 i = 1</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 6/27</p>
  </div>
  <div class="page">
    <p>Algorithms</p>
    <p>Value Iteration</p>
    <p>k+1  greedy(vk) vk+1 T vk = T</p>
    <p>k+1vk</p>
    <p>Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  v</p>
    <p>k+1 = (T k+1)vk</p>
    <p>Modied Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1  (T</p>
    <p>k+1)mvk m  N</p>
    <p>Policy Iteration (Bertsekas &amp; Ioe, 1996)</p>
    <p>k+1  greedy(vk) vk+1  (1)</p>
    <p>i=0</p>
    <p>i (T k+1)i+1vk   [0,1]</p>
    <p>Optimistic Policy Iteration</p>
    <p>k+1  greedy(vk) vk+1</p>
    <p>i=0 i (T</p>
    <p>k+1)i+1vk i  0,</p>
    <p>i=0 i = 1</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 6/27</p>
  </div>
  <div class="page">
    <p>Optimism in the greedy partition</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 7/27</p>
  </div>
  <div class="page">
    <p>Optimism in the greedy partition</p>
    <p>Policy Iteration</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 7/27</p>
  </div>
  <div class="page">
    <p>Optimism in the greedy partition</p>
    <p>Policy Iteration</p>
    <p>Value Iteration</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 7/27</p>
  </div>
  <div class="page">
    <p>Optimism in the greedy partition</p>
    <p>Policy Iteration</p>
    <p>Value Iteration</p>
    <p>Opt. PI</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 7/27</p>
  </div>
  <div class="page">
    <p>Relation with Temporal Dierences</p>
    <p>The general backup has the following incremental form</p>
    <p>vk+1   i=0</p>
    <p>i (T k+1 )i+1vk = vk + k+1</p>
    <p>where i, k+1(i) := Ek+1</p>
    <p>j=0</p>
    <p>j jk (ij, ij+1)</p>
    <p>i0 = i</p>
    <p>k (i, j) := r(i) + vk (j) vk (i) is the temporal dierence associated to transition i  j. j :=</p>
    <p>lj l is a non-increasing weight sequence (0 = 1) ;</p>
    <p>the faster it decreases (the more optimistic), the smaller the variance of k+1 (optimism  bias-variance trade-o)</p>
    <p>Ex : In  Policy Iteration, i = (1)i, thus j = j (TD()) Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 8/27</p>
  </div>
  <div class="page">
    <p>A picture of OPI</p>
    <p>shallow backups</p>
    <p>deep backups</p>
    <p>backups</p>
    <p>backups full</p>
    <p>sample Monte Carlo Temporal Difference Learning</p>
    <p>Iteration Iteration PolicyValue</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 9/27</p>
  </div>
  <div class="page">
    <p>A picture of OPI</p>
    <p>shallow backups</p>
    <p>deep backups</p>
    <p>backups</p>
    <p>backups full</p>
    <p>sample Monte Carlo Temporal Difference Learning</p>
    <p>Iteration Iteration PolicyValue</p>
    <p>(Sutton &amp; Barto, 1998), chap. 10</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 9/27</p>
  </div>
  <div class="page">
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 10/27</p>
  </div>
  <div class="page">
    <p>Least-Squares Policy Iteration (Lagoudakis &amp; Parr, 2003; Munos, 2003)</p>
    <p>Consider the state-action value :</p>
    <p>q(i,a) = E</p>
    <p>[  k=0</p>
    <p>kr(ik,(ik ))</p>
    <p>i0 = i,a0 = a ]</p>
    <p>Linear approximation : q = w</p>
    <p>q(i,a) =</p>
    <p>p k=1</p>
    <p>wk (i,a) where p |X|</p>
    <p>Value/weight update with LSTD(0) / Residual minimization</p>
    <p>wk+1 = T k+1 wk+1 / wk+1 = arg min w wT k+1 w</p>
    <p>Model-free, sample-ecient, o-policy, O(p3) per iteration. Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 11/27</p>
  </div>
  <div class="page">
    <p>From LSPI to LSPI</p>
    <p>The  Policy Iteration update</p>
    <p>qk+1  (1)  i=0</p>
    <p>i (T k+1 )i+1qk</p>
    <p>can be cast as the computation of the xed point of :</p>
    <p>qk+1 = (1)T k+1qk + T k+1qk+1  qk+1 = Mk+1qk+1</p>
    <p>When  = 1, this is Policy Iteration. When  &lt; 1, Mk+1 is a damped version of T k+1.</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 12/27</p>
  </div>
  <div class="page">
    <p>Least-Squares  Policy Iteration</p>
    <p>Consider the state-action value :</p>
    <p>q(i,a) = E</p>
    <p>[  k=0</p>
    <p>kr(ik,(ik ))</p>
    <p>i0 = i,a0 = a ]</p>
    <p>Linear approximation : q = w</p>
    <p>q(i,a) =</p>
    <p>p k=1</p>
    <p>wk (i,a) where p |X|</p>
    <p>Value/weight update with LSTD(0) / Residual minimization</p>
    <p>wk+1 = Mk+1wk+1 / wk+1 = arg min w wMk+1w</p>
    <p>Model-free, sample-ecient, o-policy, O(p3) per iteration, Optimistic / bias-variance tradeo</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 13/27</p>
  </div>
  <div class="page">
    <p>Related works</p>
    <p>Algorithm bi a sv a ri a n c e</p>
    <p>e  c ie n t</p>
    <p>o  -p o li c y</p>
    <p>PI with TD() (Sutton &amp; Barto, 1998) X PI with LSTD(0) (Bradtke &amp; Barto, 1996) X X PI with LSTD() (Boyan, 2002) X X PI with LSPE() (Yu &amp; Bertsekas, 2009) X X PI (Bertsekas &amp; Ioe, 1996) X X LSPI (Lagoudakis &amp; Parr, 2003) X X LSPI X X X</p>
    <p>Introduce bias-variance tradeo through optimism (instead of depth of backups), which allows to keep o-policy</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 14/27</p>
  </div>
  <div class="page">
    <p>Performance bounds</p>
    <p>Consider Approximate Optimistic PI :</p>
    <p>k+1  qk qk+1</p>
    <p>i0</p>
    <p>i (T k+1 )i+1qk + k+1.</p>
    <p>Proposition</p>
    <p>If  is a uniform upper bound of the error (k, k  .) then the loss of using k instead of  satises :</p>
    <p>lim sup k</p>
    <p>q qk  2</p>
    <p>(1)2 .</p>
    <p>Known for VI and PI (Bertsekas &amp; Tsitsiklis, 1996). New for Modied Policy Iteration !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 15/27</p>
  </div>
  <div class="page">
    <p>Proof idea</p>
    <p>Analysis of</p>
    <p>Value Iteration : based on contraction</p>
    <p>Policy Iteration : based on monotonicity</p>
    <p>Optimistic Policy Iteration :</p>
    <p>+ (qk+1  k+1) qk+1   sk+1</p>
    <p>max dk+1   max dk +  + A max bk max sk+1  B max bk max bk   max bk1 + (1 + )</p>
    <p>where bk = qk T qk, 0   &lt; 1, A &gt; 0, B &gt; 0. Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 16/27</p>
  </div>
  <div class="page">
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 17/27</p>
  </div>
  <div class="page">
    <p>Chain problem (Lagoudakis &amp; Parr, 2003)</p>
    <p>Chain with n states and 2 actions : left or right</p>
    <p>Actions succeeds with prob. 0,9 and is reversed otherwise.</p>
    <p>Rewards at states 1 and n</p>
    <p>Feature functions : polynoms and RBFs</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 18/27</p>
  </div>
  <div class="page">
    <p>Inuence of</p>
    <p>Fixed point</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 19/27</p>
  </div>
  <div class="page">
    <p>Inuence of</p>
    <p>Bellman residual</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 20/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Policy oscillations and</p>
    <p>V a le u r d e la</p>
    <p>p o li ti q u e</p>
    <p>If convergence, the performance bound can be improved to 2 1  !</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 21/27</p>
  </div>
  <div class="page">
    <p>Tetris (Lagoudakis &amp; Parr, 2003)</p>
    <p>LSPI with 1 000 samples</p>
    <p>Fixed point</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 22/27</p>
  </div>
  <div class="page">
    <p>Tetris (Lagoudakis &amp; Parr, 2003)</p>
    <p>LSPI with 1 000 samples</p>
    <p>Bellman residual</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 23/27</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>Optimistic Policy Iteration as a unied view of VI, PI, MPI, PI</p>
    <p>Least-Squares Implementation of PI (based on LSTD(0) or residual minimization) : model-free, sample-ecient, o-policy and bias-variance tradeo (through optimism)</p>
    <p>Performance bound similar to those of VI/PI</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 24/27</p>
  </div>
  <div class="page">
    <p>Current and future work</p>
    <p>Adapting  automatically</p>
    <p>LS /Modied PI based on LSTD() / LSTD(m) (on-policy)</p>
    <p>Better understanding of the stochastic approximation of Optimistic PI (Tsitsiklis, 2002)</p>
    <p>shallow backups</p>
    <p>deep backups</p>
    <p>backups</p>
    <p>backups full</p>
    <p>sample Monte Carlo Temporal Difference Learning</p>
    <p>Iteration Iteration PolicyValue</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 25/27</p>
  </div>
  <div class="page">
    <p>References I</p>
    <p>Bertsekas, D.P., &amp; Ioe, S. 1996. Temporal dierences-based policy iteration and applications in neuro-dynamic programming. Tech. rept. LIDS-P-2349. MIT.</p>
    <p>Bertsekas, D.P., &amp; Tsitsiklis, J.N. 1996. Neurodynamic Programming. Athena Scientic.</p>
    <p>Boyan, J. A. 2002. Technical Update : Least-Squares Temporal Dierence Learning. Machine Learning, 49, 233246.</p>
    <p>Bradtke, S. J., &amp; Barto, A.G. 1996. Linear Least-Squares Algorithms for Temporal Dierence Learning. Machine Learning, 22, 3357.</p>
    <p>Lagoudakis, M. G., &amp; Parr, R. 2003. Least-Squares Policy Iteration. Journal of Machine Learning Research, 4, 11071149.</p>
    <p>Munos, R. 2003. Error Bounds for Approximate Policy Iteration. Pages 560567 of : ICML.</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 26/27</p>
  </div>
  <div class="page">
    <p>References II</p>
    <p>Puterman, M. 1994. Markov Decision Processes. Wiley, New York.</p>
    <p>Sutton, R.S., &amp; Barto, A.G. 1998. Reinforcement Learning, An introduction. BradFord Book. The MIT Press.</p>
    <p>Tsitsiklis, John N. 2002. On the Convergence of Optimistic Policy Iteration. Journal of Machine Learning Research, 3, 2002.</p>
    <p>Yu, H., &amp; Bertsekas, D. P. 2009. Convergence Results for Some Temporal Dierence Methods Based on Least Squares. IEEE Trans. Automatic Control, 54, 15151531.</p>
    <p>Christophe Thiry and Bruno Scherrer INRIA - LORIA - Maia Team 27/27</p>
  </div>
</Presentation>
