<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>SiLo: A Similarity-Locality based NearExact Deduplication Scheme with</p>
    <p>Low RAM Overhead and High Throughput</p>
    <p>Wen Xia Hong Jiang Dan Feng Yu Hua,</p>
    <p>Huazhong University of Science and Technology  University of Nebraska-Lincoln</p>
  </div>
  <div class="page">
    <p>Data Deduplication</p>
    <p>Why deduplication ?</p>
    <p>Reduces the storage space overheads.</p>
    <p>Minimizes the network transmission of redundant data.</p>
    <p>Deduplication Technique.</p>
    <p>Data fingerprints: MD5, SHA-1, SHA-256.</p>
    <p>Remove duplicate data by checking its fingerprints.</p>
    <p>Deduplication granularity.</p>
    <p>File-level.</p>
    <p>Chunk-level.</p>
    <p>Fixed-length Chunking; Content Defined Chunking.</p>
  </div>
  <div class="page">
    <p>Deduplication Challenges</p>
    <p>Files Chunks Fingerprints Store Chunking Hashing Indexing</p>
    <p>The Scalability of</p>
    <p>Deduplication Indexing</p>
    <p>Deduplicate 800</p>
    <p>TB unique data.</p>
    <p>SHA-1 signature.</p>
    <p>Avg. 8KB Chunk.</p>
    <p>are generated .</p>
    <p>Global indexing.</p>
    <p>Disk bottleneck.</p>
  </div>
  <div class="page">
    <p>Locality-based Approaches (1)</p>
    <p>RAM</p>
    <p>DISK</p>
    <p>A B C A A B C</p>
    <p>F G H</p>
    <p>Input data stream</p>
    <p>Locality Cache</p>
    <p>L M N</p>
    <p>Hit</p>
    <p>Global index on the disk</p>
    <p>F G</p>
    <p>Hit</p>
    <p>A B C</p>
    <p>Miss Hit</p>
    <p>A G N Q</p>
    <p>Input data stream</p>
  </div>
  <div class="page">
    <p>Locality based approaches (2)</p>
    <p>DDFS, Sparse Indexing, ChunkStash.</p>
    <p>Exploit locality of backup streams.</p>
    <p>It maximizes the RAM utilization and reduces</p>
    <p>frequent accesses to on-disk index by retaining</p>
    <p>access locality in the locality cache.</p>
    <p>Limitations.</p>
    <p>Work poorly when backup stream lacks locality.</p>
    <p>High RAM consumed.</p>
  </div>
  <div class="page">
    <p>Similarity-based Approaches (1)</p>
    <p>A B C D</p>
    <p>E F G H</p>
    <p>File 1</p>
    <p>File 2</p>
    <p>DISK</p>
    <p>A B C J</p>
    <p>Backup File 3</p>
    <p>RAM</p>
    <p>Similarity Index</p>
    <p>F C</p>
    <p>extract the similarity</p>
    <p>characteristics of File 3</p>
    <p>Lookup C in the</p>
    <p>Similarity Index</p>
    <p>Deduplicate File 3 with</p>
    <p>File 1 Achieve a single on-disk index access</p>
    <p>for chunk lookup per file thus avoid</p>
    <p>global index on the disk.</p>
  </div>
  <div class="page">
    <p>Limitation of These Approaches (2)</p>
    <p>Exploit similarity of backup streams.</p>
    <p>Avoid global indexing and achieve a single disk read.</p>
    <p>Minimize the RAM overhead for indexing fingerprints.</p>
    <p>Limitation.</p>
    <p>Degradation of Deduplication efficiency.</p>
    <p>| | Pr[min( ( )) min( ( ))]</p>
    <p>| |</p>
    <p>S S H S H S</p>
    <p>S S</p>
    <p>Theorem 1: Consider two files S1 and S2, Let min(H(S)) denote the similarity characteristic of file S. Then similarity degree between the two files is quantified by the probability that min(H(S1))= min(H(S2)), which is</p>
    <p>dependent on the percentage of data common to both files:</p>
  </div>
  <div class="page">
    <p>Evaluation of Similarity Approach</p>
    <p>Similarity based Deduplication efficiency is dependent on the similarity degree of data stream</p>
  </div>
  <div class="page">
    <p>Observation</p>
    <p>The deduplication of small files and large files.</p>
    <p>Small files</p>
    <p>( 64KB)</p>
    <p>Large files</p>
    <p>( 2 MB)</p>
    <p>Percentage of total file number</p>
    <p>80%  20%</p>
    <p>Percentage of total space</p>
    <p>20%  80%</p>
    <p>Grouping many highly</p>
    <p>correlated small files</p>
    <p>into a segment to</p>
    <p>minimize dedupe</p>
    <p>overheads</p>
    <p>Dividing the large</p>
    <p>files into many small</p>
    <p>segments to expose</p>
    <p>more similarity</p>
    <p>characteristics</p>
  </div>
  <div class="page">
    <p>Intuition</p>
    <p>The combination of similarity and locality.</p>
    <p>(a) Similarity approach</p>
    <p>Similar Similar</p>
    <p>Existing data stream</p>
    <p>Input data stream</p>
    <p>Locality Enhancement</p>
    <p>Potential duplicate</p>
  </div>
  <div class="page">
    <p>System Architecture Overview</p>
    <p>A disk-inline backup storage system.</p>
    <p>Chunking</p>
    <p>User Interface</p>
    <p>File Agent Job Agent Deduplication Metadata Agent</p>
    <p>Storage Agent</p>
    <p>Contain Store</p>
    <p>Job MetaData Cache HashTable Block Store</p>
    <p>File Deamon</p>
    <p>Storage Server</p>
    <p>MDS</p>
    <p>Storage Agent</p>
    <p>Contain Store</p>
    <p>Storage Agent</p>
    <p>Contain Store</p>
    <p>Backup Server Deduplication Server</p>
    <p>Disk Disk Disk</p>
    <p>Network</p>
    <p>Deduplication Server</p>
    <p>is to store and look up</p>
    <p>all fingerprints of files</p>
    <p>and chunks.</p>
    <p>Backup Server is the</p>
    <p>manager of the backup</p>
    <p>system that directs all File</p>
    <p>Agents and Storage Servers.</p>
    <p>Storage Server is the</p>
    <p>repository for backed</p>
    <p>up data.</p>
    <p>File Deamon is a deamon</p>
    <p>program providing a</p>
    <p>functional interface in</p>
    <p>users computers.</p>
  </div>
  <div class="page">
    <p>Deduplication Server</p>
    <p>Deduplication Server.</p>
    <p>It is most likely the performance bottleneck.</p>
    <p>Block Block</p>
    <p>Block Block</p>
    <p>Block Block</p>
    <p>Seg Key</p>
    <p>DISK</p>
    <p>RAM</p>
    <p>SHTable Read Cache</p>
    <p>Write Buffer</p>
    <p>Block</p>
    <p>Block</p>
    <p>RepChunk ID</p>
    <p>Block ID</p>
    <p>Chunk ID</p>
    <p>LHTable</p>
    <p>Segment</p>
    <p>Similarity</p>
    <p>Hash Table</p>
    <p>Locality</p>
    <p>Cache</p>
    <p>The similarity</p>
    <p>unit, (sequence</p>
    <p>of chunks)</p>
    <p>The locality unit,</p>
    <p>(sequence of</p>
    <p>segments)</p>
  </div>
  <div class="page">
    <p>The Similarity Algorithm</p>
    <p>Structuring data from backup streams into segments according to the following three principles.</p>
    <p>P1. Correlated small files in a backup stream are to be grouped into a segment.</p>
    <p>P2. A large file in a backup stream is divided into several independent segments.</p>
    <p>P3. All segments are of approximately the same size (e.g., 2MB).</p>
    <p>Small File1</p>
    <p>Segment1 Metadata Block</p>
    <p>Metadata</p>
    <p>Segment3</p>
    <p>Segment2</p>
    <p>Segment1</p>
    <p>Block</p>
    <p>Metadata</p>
    <p>Segment3</p>
    <p>Segment2</p>
    <p>Segment1</p>
    <p>Small File2</p>
    <p>Small File3</p>
    <p>Segment2 Metadata</p>
    <p>Part 1 of large file L1</p>
    <p>Segment3 Metadata</p>
    <p>Part 2 of large file L1</p>
    <p>Reduce</p>
    <p>RAM usage</p>
    <p>Eliminate</p>
    <p>more data</p>
    <p>Block Block</p>
  </div>
  <div class="page">
    <p>The Locality Algorithm</p>
    <p>The locality algorithm groups several contiguous segments into a block and preserves their locality-layout on the disk.</p>
    <p>It maximizes the RAM utilization and reduces frequent</p>
    <p>accesses to on-disk index by retaining access locality</p>
    <p>in the locality cache.</p>
    <p>By exploiting the inherent locality in backup streams,</p>
    <p>the block-based SiLo locality algorithm can eliminate</p>
    <p>more duplicate data.</p>
    <p>S11 S12 S13 S14 .. S1k</p>
    <p>S21 S22 S23 S24 .. S2k</p>
    <p>Segment Similar Block</p>
    <p>Similar</p>
    <p>B1</p>
    <p>B2</p>
    <p>S14</p>
    <p>S24</p>
    <p>Duplicate</p>
    <p>more data</p>
  </div>
  <div class="page">
    <p>SiLo Workflow</p>
    <p>The locality algorithm helps detect more potentially duplicate chunks that are missed by the similarity algorithm.</p>
    <p>(a) Input backup stream</p>
    <p>(big file) (Small files)</p>
    <p>(big file)</p>
    <p>(b) Segmenting large files and grouping small files</p>
    <p>(c) Similarity detection</p>
    <p>(block)</p>
    <p>(segment)</p>
    <p>(d) Locality-enhanced similartity detection by chunks filtering</p>
    <p>N N NNN</p>
    <p>(potentially duplicate) (potentially duplicate)</p>
    <p>(segment)</p>
    <p>(dup chunks) (new chunks) (dup chunks)</p>
    <p>similar similar</p>
    <p>Input segments</p>
    <p>Segments in cache or on disk</p>
  </div>
  <div class="page">
    <p>RAM Consideration</p>
    <p>RAM usage of SiLo:</p>
    <p>The locality cache? A small portion.</p>
    <p>The similarity hash table? The main portion.</p>
    <p>RAM usage analysis:</p>
    <p>SiLo requires only 30 MB for deduplicating 1TB unique data.</p>
    <p>Extreme Binning requires 300 MB for deduplicating 1TB unique</p>
    <p>data. (Avg. file size of 200KB).</p>
    <p>Sparse Indexing uses 170 MB of RAM space for a TB-scale</p>
    <p>deduplication system, whereas the Sparse Indexing paper</p>
    <p>estimates that DDFS would require 360 MB RAM to maintain a</p>
    <p>partial index depending on locality in backup streams.</p>
  </div>
  <div class="page">
    <p>Performance Evaluation</p>
    <p>Interplay of similarity and locality algorithms.</p>
    <p>Quantitative analysis of our similarity and locality</p>
    <p>algorithms.</p>
    <p>Comparison of state-of-the-art work.</p>
    <p>Locality approach: ChunkStash-HDD.</p>
    <p>Similarity approach: Extreme Binning.</p>
    <p>Four datasets.</p>
    <p>Feature One-set Inc-set Linux-set Full-set</p>
    <p>Locality Weak Weak Strong Strong</p>
    <p>Similarity Weak Strong Strong Strong</p>
    <p>Small files</p>
  </div>
  <div class="page">
    <p>Small File1</p>
    <p>Segment1 Metadata Block</p>
    <p>Metadata</p>
    <p>Segment3</p>
    <p>Segment2</p>
    <p>Segment1</p>
    <p>Block</p>
    <p>Metadata</p>
    <p>Segment3</p>
    <p>Segment2</p>
    <p>Segment1</p>
    <p>Small File2</p>
    <p>Small File3</p>
    <p>Segment2 Metadata</p>
    <p>Part 1 of large file L1</p>
    <p>Segment3 Metadata</p>
    <p>Part 2 of large file L1</p>
    <p>Interplay of Similarity and Locality</p>
    <p>Segment size</p>
    <p>Segment size</p>
    <p>Locality</p>
    <p>unit</p>
    <p>Similarity</p>
    <p>unit</p>
    <p>Percentage of duplicate data eliminated and Time overhead of SiLo deduplication as a function of block size and segment size.</p>
    <p>The larger the block size is, the</p>
    <p>more locality can be retained.</p>
    <p>The smaller the segment size is,</p>
    <p>the more similarity can be</p>
    <p>exposed.</p>
  </div>
  <div class="page">
    <p>Locality Enhancement Evaluation</p>
    <p>The full Exploitation of locality jointly with similarity can remove almost all of the redundant data missed by the similarity detection.</p>
    <p>Duplicate data removed by</p>
    <p>similarity-only approach.</p>
    <p>Duplicate data removed by</p>
    <p>Locality-enhanced approach.</p>
    <p>Duplicate data missed by our</p>
    <p>SiLo approach.</p>
  </div>
  <div class="page">
    <p>Duplicate Elimination</p>
    <p>SiLo achieves near-exact duplicate elimination under all workloads.</p>
    <p>SiLo with segment size of 4MB</p>
    <p>The similarity</p>
    <p>approach:</p>
    <p>Extreme</p>
    <p>Binning</p>
    <p>Locality approach: ChunkStash-HDD</p>
  </div>
  <div class="page">
    <p>RAM Usage for Indexing</p>
    <p>SiLo consumes a RAM capacity that is only 1/411/60 and 1/31/90 respectively of that consumed by ChunkStash and Extreme Binning.</p>
    <p>Extreme Binning</p>
    <p>performs poorly</p>
    <p>on the Linux-set.</p>
  </div>
  <div class="page">
    <p>Deduplication Throughput</p>
    <p>Our evaluations on deduplication throughput suggest that SiLo outperforms ChunkStash by a factor of about 3 and Extreme Binning by a factor of about 1.5.</p>
  </div>
  <div class="page">
    <p>Summary</p>
    <p>SiLo, a near-exact deduplication system.</p>
    <p>effectively and complementarily exploits similarity and locality</p>
    <p>achieve high duplicate elimination and throughput at extremely low RAM</p>
    <p>overheads.</p>
    <p>Combination of similarity and locality.</p>
    <p>SiLo proposes a new similarity algorithm that groups many small</p>
    <p>strongly correlated files into a segment and segments a large file to</p>
    <p>better expose and exploit their similarity characteristics.</p>
    <p>SiLo proposes an effective locality approach that captures more similar</p>
    <p>and duplicate data missed by the probabilistic similarity detection and</p>
    <p>also improve the deduplication throughput.</p>
    <p>Our experimental evaluation of SiLo.</p>
    <p>Quantitative analysis and demonstration of our similarity and locality</p>
    <p>algorithms.</p>
    <p>SiLo system consistently and significantly outperforms two existing</p>
    <p>state-of-the-art systems.</p>
  </div>
  <div class="page">
    <p>www.themegallery.com</p>
    <p>Questions?</p>
    <p>Acknowledgements</p>
    <p>(863 Program) of China under Grant No.2009AA01A401 and 2009AA01A402,</p>
    <p>NSFC No.61025008,60933002,60873028,60703046,</p>
    <p>Changjiang innovative group of Education of China No. IRT0725,</p>
    <p>Fundamental Research Funds for the central universities, HUST, under grant 2010MS043,</p>
    <p>The US NSF under Grants NSF-IIS-0916859, NSF-CCF-0937993 and NSF-CNS-1016609.</p>
  </div>
</Presentation>
