<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Virtualized Infrastructures, Systems, &amp; Applications</p>
    <p>KnowledgeNet: Disaggregated and Distributed Training and Serving of Deep</p>
    <p>Neural Networks</p>
    <p>Saman Biookaghazadeh, Yitao Chen, Kaiqi Zhao, Ming Zhao</p>
    <p>Arizona State University</p>
    <p>http://visa.lab.asu.edu</p>
  </div>
  <div class="page">
    <p>Background</p>
    <p>Deep neural networks (DNNs) haven shown great potential o Solving many challenging problems o Relying on large systems to learn on big data</p>
    <p>Limitations of centralized learning o Cannot scale to handle future demands</p>
    <p>20 billion connected devices by 2020  Real-time learning/decision making</p>
    <p>o Cannot exploit the capabilities of the increasingly powerful edge  Multiprocessing, accelerators</p>
  </div>
  <div class="page">
    <p>Objectives</p>
    <p>Distributed learning across heterogeneous resources o Including diverse and potentially weak resources o Across edge devices and cloud resources</p>
    <p>Limitations of existing solutions o Data parallelism does not work</p>
    <p>for heterogeneous resources o Conventional backpropagation</p>
    <p>based training makes model parallelism difficult</p>
    <p>o Edge devices cannot train large models</p>
    <p>o Model compression works only for inference, not training</p>
  </div>
  <div class="page">
    <p>Proposed approach</p>
    <p>Using synthetic gradients to achieve model parallelism o Split a large DNN model into many small models o Deploy small models on distributed and potentially weak resources o Use synthetic gradients to decouple the training of the small</p>
    <p>models</p>
  </div>
  <div class="page">
    <p>Forward path</p>
    <p>Backward path</p>
    <p>To training a neural network model  forward path: calculate loss of the class prediction  backward path: calculate the gradients of loss w.r.t the parameters</p>
    <p>Need tens of thousands iterations to achieve a high accuracy</p>
    <p>Layers are coupled and training are sequential</p>
    <p>Limited scalability in training</p>
    <p>C on</p>
    <p>v 1 _ 1</p>
    <p>C on</p>
    <p>v 1 _ 2</p>
    <p>P oo</p>
    <p>l 1</p>
    <p>C on</p>
    <p>v 2 _ 1</p>
    <p>C on</p>
    <p>v 2 _ 2</p>
    <p>P oo</p>
    <p>l 2</p>
    <p>C on</p>
    <p>v 3 _ 1</p>
    <p>C on</p>
    <p>v 3 _ 2</p>
    <p>C on</p>
    <p>v 3 _ 3</p>
    <p>P oo</p>
    <p>l 3</p>
    <p>F C</p>
    <p>F C</p>
    <p>F C</p>
    <p>Conventional training</p>
    <p>Tightly coupled</p>
    <p>F C</p>
    <p>F C</p>
    <p>F C</p>
    <p>h(i): layer output of the ith layer (i): error of the ith layer</p>
    <p>(4_3)(4_2)</p>
    <p>Po ol</p>
    <p>(4_1)</p>
    <p>h(3)</p>
  </div>
  <div class="page">
    <p>Training with synthetic gradients</p>
    <p>M3</p>
    <p>h(3_2) h(3_3)</p>
    <p>h(i): layer output of the ith layer (i): error of the ith layer</p>
    <p>(3_4)(3_3)C o n v</p>
    <p>Po ol</p>
    <p>C o n v</p>
    <p>C o n v</p>
    <p>Use another small neural network to predict the error synthetic gradients</p>
    <p>Use synthetic gradients to train the small model instead of waiting for the actual gradients from backpropagation</p>
    <p>h(3_1)</p>
    <p>(3_2)</p>
    <p>(4_1) (synthetic error)</p>
    <p>M4</p>
    <p>F C</p>
    <p>(4_1) - (4_1)Minimize</p>
    <p>Decouple the layers</p>
  </div>
  <div class="page">
    <p>Proposed approach</p>
    <p>Using knowledge transfer to improve training on weak resources o Train a large, teacher model in</p>
    <p>the cloud o Train many small, student</p>
    <p>models on the devices o Exploit the knowledge of the</p>
    <p>teacher to train the students o Improve the accuracy and</p>
    <p>convergence speed of the students</p>
    <p>Teacher Student</p>
  </div>
  <div class="page">
    <p>Knowledge transfer</p>
    <p>conv1</p>
    <p>conv2</p>
    <p>conv3</p>
    <p>conv4</p>
    <p>conv5</p>
    <p>conv6</p>
    <p>fc1</p>
    <p>softmax</p>
    <p>conv1</p>
    <p>conv2</p>
    <p>conv3</p>
    <p>fc1</p>
    <p>softmax</p>
    <p>RMSE</p>
    <p>RMSE</p>
    <p>RMSE</p>
    <p>RMSE</p>
    <p>cross entrop</p>
    <p>correct labels</p>
    <p>Input data</p>
    <p>Teacher network</p>
    <p>Student network</p>
    <p>Minimize squared difference between layer outputs (RMSE) to allow student to learning from teacher representations</p>
  </div>
  <div class="page">
    <p>Preliminary results</p>
    <p>Using synthetic gradients to achieve model parallelism o Model: a simple 4-Layer</p>
    <p>One convolution layer and three fully-connected</p>
    <p>o MNIST Dataset o 500K Iteration of training</p>
    <p>Back propagation</p>
    <p>Synthetic gradients</p>
    <p>VGG16 0.994 0.845</p>
    <p>Training accuracy</p>
    <p>A c c u</p>
    <p>ra c y</p>
    <p>Iterations</p>
    <p>sg validation bp validation</p>
  </div>
  <div class="page">
    <p>Preliminary results</p>
    <p>Using knowledge transfer to improve training on weak resources o Teacher Model: VGG-16 (8.5M parameters) o Student Model: miniaturized VGG-16 (3.2M parameters) o Training student for 100 Epochs</p>
    <p>Accuracy</p>
    <p>Teacher 76.88%</p>
    <p>Student (dependent) 74.12%</p>
    <p>Student (independent) 61.24%</p>
    <p>Are Existing Knowledge Transfer Techniques Effective For Deep Learning on Edge Devices? R. Sharma, S. Biookaghazadeh, and M. Zhao, EDGE, 2018</p>
  </div>
  <div class="page">
    <p>Conclusions and future work</p>
    <p>Rethink DNN platforms to handle future learning needs o Utilize heterogeneous resources to train DNNs o Distribute learning across edge and cloud</p>
    <p>Achieve efficient model parallelism o Use synthetic gradients to decouple the training of distributed</p>
    <p>models o Need to further improve its accuracy for complex models</p>
    <p>Overcome the resource constraints of edge devices o Use knowledge transfer to help train on-device models o Need to further study its effectiveness under scenarios with</p>
    <p>limited data and limited supervision</p>
  </div>
  <div class="page">
    <p>Acknowledgement</p>
    <p>National Science Foundation o GEARS project: CNS-1629888 o CNS-1619653, CNS-1562837, CNS</p>
    <p>VISA Lab @ ASU o Saman, Yitao, Kaiqi, and others</p>
    <p>Thank you! o Questions and suggestions? o Come to our poster at Happy</p>
    <p>Hour!</p>
  </div>
</Presentation>
