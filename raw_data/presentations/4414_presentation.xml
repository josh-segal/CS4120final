<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Simplified Abugidas</p>
    <p>Chenchen Ding, Masao Utiyama, and Eiichiro Sumita</p>
    <p>ASTREC, NICT, Japan</p>
  </div>
  <div class="page">
    <p>Writing Systems : Hierarchy</p>
    <p>can</p>
    <p>phonogram</p>
    <p>logogram</p>
    <p>segmental</p>
    <p>syllabic abugida</p>
    <p>alphabet</p>
    <p>abjad</p>
  </div>
  <div class="page">
    <p>Writing Systems : Population</p>
    <p>phonogram</p>
    <p>logogram</p>
    <p>segmental</p>
    <p>syllabic abugida</p>
    <p>alphabet</p>
    <p>abjad</p>
  </div>
  <div class="page">
    <p>Writing Systems : How to Input</p>
    <p>Keyboard</p>
    <p>Input method</p>
    <p>Keyboard</p>
    <p>Logogram Syllabic Abugida Alphabet &amp; Abjad&gt; &gt; &gt;</p>
    <p>Keyboard  ?</p>
    <p>typically 50  70</p>
  </div>
  <div class="page">
    <p>Writing Systems : How to Input</p>
    <p>Keyboard</p>
    <p>Input method</p>
    <p>Keyboard</p>
    <p>Logogram Syllabic Abugida Alphabet &amp; Abjad&gt; &gt; &gt;</p>
  </div>
  <div class="page">
    <p>Motivation of This Study</p>
    <p>Can abugidas be inputted more efficiently?  To insert a light layer of input method</p>
    <p>To type less and to recover automatically</p>
    <p>Related work  Various approaches for Chinese and Japanese</p>
    <p>To take advantage of redundancy in a writing system</p>
  </div>
  <div class="page">
    <p>Outline of This Study</p>
    <p>Khmer script as an example</p>
    <p>/noon/</p>
    <p>/naen/</p>
    <p>/nun/</p>
    <p>/nein/</p>
    <p>vowel diacritic omission</p>
    <p>consonant character merging</p>
    <p>N N</p>
    <p>(a) ABUGIDA SIMPLIFICATION</p>
    <p>J T N N</p>
    <p>(b) RECOVERY</p>
    <p>machine learning methods</p>
  </div>
  <div class="page">
    <p>Abugida Simplification</p>
    <p>Four Brahmic abugidas  Thai, Burmese (Myanmar), Khmer (Cambodian), and Lao</p>
    <p>Based on phonetics / conventional usages  reduced to 21 symbols</p>
    <p>TH</p>
    <p>MY          KM</p>
    <p>LO</p>
    <p>OMITTED</p>
    <p>I II I II I II I II</p>
    <p>MN K G U C J I Y T D N L P B M W R S H Q A E</p>
    <p>TH</p>
    <p>MY</p>
    <p>KM</p>
    <p>LO</p>
    <p>A P</p>
    <p>P .</p>
    <p>DENTALPALATE PRE-V.</p>
    <p>DE-V. PLOSIVE</p>
    <p>N A</p>
    <p>S .</p>
    <p>M E R G E D</p>
    <p>R -L</p>
    <p>IK E</p>
    <p>S -L</p>
    <p>IK E</p>
    <p>H -L</p>
    <p>IK E</p>
    <p>LO N</p>
    <p>G -A</p>
    <p>Z E</p>
    <p>R O</p>
    <p>-C .</p>
    <p>LABIAL</p>
    <p>PLOSIVE</p>
    <p>N A</p>
    <p>S .</p>
    <p>A P</p>
    <p>P .PLOSIVE</p>
    <p>N A</p>
    <p>S .</p>
    <p>GUTTURAL</p>
    <p>PLOSIVE N</p>
    <p>A S</p>
    <p>.</p>
    <p>A P</p>
    <p>P .</p>
  </div>
  <div class="page">
    <p>Abugida Simplification</p>
    <p>Khmer script as an example</p>
    <p>Around one quarter characters ( ) saved</p>
    <p>J T N N +  +  +  +  + =</p>
    <p>Leng.= len (&quot;J T N N&quot;)</p>
    <p>len (&quot;     &quot;) = 4</p>
    <p>Thai Burmese Khmer Lao</p>
    <p>Leng. 76.0% 74.0% 77.6% 72.8%</p>
  </div>
  <div class="page">
    <p>Recovery Methods</p>
    <p>To formulate as a sequential labeling task  However, list-wise search as in conditional random fields is costing</p>
    <p>To solve by point-wise classification  Support vector machine (SVM) as a baseline</p>
    <p>Recurrent neural network (RNN) as a state-of-the-art method</p>
    <p>Setting for the SVM baseline  Linear kernel with N-gram features</p>
    <p>Using LIBLINEAR library</p>
    <p>Wrapped by the KyTea toolkit</p>
  </div>
  <div class="page">
    <p>RNN Structure and Settings</p>
    <p>Bi-gram of graphemes as input</p>
    <p>Embedding  Bi-directional LSTM  Linear transform  Softmax</p>
    <p>Original writing units as output</p>
    <p>Implemented by DyNet</p>
    <p>Trained by Adam  Initial learning rate 0.001</p>
    <p>Controlled by a validation set</p>
    <p>Multi-model ensemble</p>
    <p>J T N N</p>
    <p>LSTM-RNN</p>
    <p>input</p>
    <p>output</p>
    <p>linear</p>
    <p>softmax</p>
    <p>(2562)</p>
  </div>
  <div class="page">
    <p>Experimental Results</p>
    <p>Asian Lang. Treebank data  20,000 sent., newswire</p>
    <p>SVM  Up to 5-gram for TH, KM, LO</p>
    <p>7-gram for Burmese</p>
    <p>RNN  M : M-ensemble</p>
    <p>16-ensemble is adequate</p>
    <p>@N : Top-N results</p>
    <p>Top-4 is satisfactory</p>
    <p>: p &lt; 0.01,  : p &lt; 0.001</p>
    <p>Embedding + bi-LSTM &gt; N-gram features</p>
  </div>
  <div class="page">
    <p>Experimental Results : Training Data Size</p>
    <p>RNN outperforms SVM, regardless of the training data size</p>
    <p>To p</p>
    <p>-1 a</p>
    <p>cc u</p>
    <p>ra cy</p>
    <p>Number of graphemes after simplification</p>
    <p>TH-SVM</p>
    <p>MY-SVM</p>
    <p>KH-SVM</p>
    <p>LO-SVM</p>
    <p>TH-RNN</p>
    <p>MY-RNN</p>
    <p>KH-RNN</p>
    <p>LO-RNN200K 2M</p>
  </div>
  <div class="page">
    <p>Manual Evaluation</p>
    <p>On Burmese and Khmer best results by RNN</p>
    <p>Conducted by native-speakers</p>
    <p>To classify errors into four-level  0. acceptable, i.e., alternative spelling</p>
    <p>1. clear and easy to identify the correct result</p>
    <p>2. confusing but possible to identify the correct result</p>
    <p>3. incomprehensible</p>
  </div>
  <div class="page">
    <p>Conclusion and Future Work</p>
    <p>Abugidas can be simplified largely and recovered with high accuracy  Four Brahmic abugidas are investigated</p>
    <p>Simplified into a compact symbol set (around 20 graphemes)</p>
    <p>Recovered satisfactorily by standard machine learning method</p>
    <p>Experimentally show the feasibility to encode abugidas in a lossy manner</p>
    <p>Future work  Language specific investigation</p>
    <p>To integrate dictionary</p>
    <p>To develop practical input method for abugidas</p>
  </div>
  <div class="page">
    <p>Thanks for your kind attention</p>
  </div>
</Presentation>
