<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Fault-Tolerant Communication Runtime Support for Data-Centric</p>
    <p>Programming Models</p>
    <p>Abhinav Vishnu1, Huub Van Dam1, Bert De Jong1,</p>
    <p>Pavan Balaji2, Shuaiwen Song3</p>
  </div>
  <div class="page">
    <p>Faults Are Becoming Increasingly Prevalent</p>
    <p>Many scientific domains have insatiable computational requirements</p>
    <p>Chemistry, Astrophysics etc.</p>
    <p>Many hundreds of thousands of processing elements are being combined</p>
    <p>Hardware faults are becoming increasingly commonplace</p>
    <p>Designing fault resilient systems and system stack is imperative</p>
    <p>Large body of work with Checkpoint/Restart</p>
    <p>Application driven and transparent (BLCR)</p>
    <p>Message Logging</p>
    <p>Matches very well with MPI semantics and fixed process model</p>
    <p>System #Cores MTBF</p>
    <p>ASCI/Q 8192 6.5 hrs</p>
    <p>ASCI White</p>
    <p>Jaguar 4 Core</p>
    <p>Source: Mueller et al., ICPADS 2010</p>
  </div>
  <div class="page">
    <p>Quick Overview of Data Centric / Partitioned Global Address Space (PGAS) Models</p>
    <p>Abstractions for arbitrary distributed data structures</p>
    <p>Arrays, Trees etc</p>
    <p>Fit for irregular global address space accesses</p>
    <p>Natural mechanism for load balancing</p>
    <p>Decouple of data and computation We focus on Global Arrays</p>
    <p>Fundamental building block for other structures</p>
    <p>Uses one-sided communication runtime system for data transfer (ARMCI)</p>
    <p>What are the requirements for fault tolerance from the runtime?</p>
    <p>Global Address</p>
    <p>Space view</p>
    <p>Physically distributed</p>
    <p>data</p>
    <p>Global Arrays Layer</p>
    <p>Global Arrays Layer</p>
    <p>ApplicationsApplications</p>
    <p>Fault Tolerant Runtime</p>
    <p>Fault Tolerant Runtime</p>
  </div>
  <div class="page">
    <p>Task-Based Execution using PGAS: Characteristic of Various Applications</p>
    <p>Each task</p>
    <p>executed by any process</p>
    <p>Reads/updates arbitrary global address space</p>
    <p>Requirements for fault tolerance</p>
    <p>Continue with available nodes</p>
    <p>Recover data on fault</p>
    <p>Recovery proportional to degree of failure</p>
    <p>Global Address Space (RO)</p>
    <p>Global Address Space (RW)</p>
    <p>Task Collection</p>
    <p>P0</p>
    <p>PN-1</p>
    <p>Compute</p>
    <p>Get</p>
    <p>Put/Accumulate</p>
  </div>
  <div class="page">
    <p>Other Questions for Fault Tolerance</p>
    <p>Interconnection NetworkInterconnection Network</p>
    <p>Power supplyPower supply</p>
    <p>Is Node 2 Dead?</p>
    <p>What should the process manager do? What about the collective operations</p>
    <p>and their semantics?</p>
    <p>What about the one-sided operations to the failed node?</p>
    <p>What about the lost data and computation?</p>
    <p>Node 1 Node 2 Node 3 Node 4</p>
    <p>We answer these questions in this work</p>
  </div>
  <div class="page">
    <p>Provided Solutions</p>
    <p>Problem Solution</p>
  </div>
  <div class="page">
    <p>Design of Fault Tolerant Communication Runtime System</p>
    <p>ApplicationApplication</p>
    <p>Data Redundancy/Fault Recovery LayerData Redundancy/Fault Recovery Layer</p>
    <p>Global Arrays LayerGlobal Arrays Layer</p>
    <p>Fault Resilient ARMCI</p>
    <p>Fault Resilient ARMCI</p>
    <p>Fault Resilient Process Manager</p>
    <p>Fault Resilient Process Manager</p>
    <p>Fault Tolerance Management Infrastructure</p>
    <p>Fault Tolerance Management Infrastructure</p>
    <p>Fault Tolerant Barrier</p>
    <p>Fault Tolerant Barrier</p>
    <p>Handling Data</p>
    <p>Redundancy</p>
    <p>Handling Data</p>
    <p>Redundancy</p>
    <p>NetworkNetwork</p>
  </div>
  <div class="page">
    <p>ARMCI: underlying communication runtime for Global Arrays</p>
    <p>Aggregate Remote Memory Copy Interface (ARMCI) Provides one-sided communication primitives</p>
    <p>Put, Get, Accumulate, Atomic Memory Operations</p>
    <p>Abstract network interfaces</p>
    <p>Established &amp; available on all leading platforms Cray XTs, XE</p>
    <p>IBM Blue Gene L | P</p>
    <p>Commodity interconnects (InfiniBand, Ethernet etc)</p>
    <p>Further upcoming platforms IBM Blue Waters, Blue GeneQ</p>
    <p>Cray Cascade</p>
  </div>
  <div class="page">
    <p>FTMI Protocols (Example with InfiniBand)</p>
    <p>No Response</p>
    <p>No Response</p>
    <p>Node 2 is dead</p>
    <p>Network Adapter</p>
    <p>Node</p>
    <p>Requires response</p>
    <p>from remote</p>
    <p>process, Less</p>
    <p>Reliable</p>
    <p>Ping Message</p>
    <p>RDMA Read</p>
    <p>Reliable Notification,</p>
    <p>Most Reliable</p>
  </div>
  <div class="page">
    <p>Fault Resilient Process Manager</p>
    <p>Adaptation from OSU-MVAICH Process Manager</p>
    <p>Provides MPI style (not fault tolerant) collectives Based on TCP/IP for bootstrapping</p>
    <p>Generic enough for any machine which has at least Ethernet control network</p>
    <p>Ignores any TCP/IP errors</p>
    <p>Layers rely on FTMI for higher accuracy fault information</p>
    <p>Interconnection NetworkInterconnection Network</p>
  </div>
  <div class="page">
    <p>Expected Data Redundancy Model: Impact on ARMCI</p>
    <p>Expected Data Redundancy Model</p>
    <p>Staggered data model Simultaneous updates may result in both copies in an inconsistent state Each copy should be updated one by one Every Write based Primitive (Put/Acc) should be Fenced</p>
    <p>WaitProc  Wait for all nonblocking operations to complete</p>
    <p>Fence  Ensure all writes to a process have finished</p>
    <p>N1N1 N2N2 N3N3 N4N4</p>
    <p>Primary Copy</p>
    <p>Shadow Copy</p>
    <p>N4N4N1N1 N2N2 N3N3</p>
    <p>Data</p>
    <p>Data</p>
  </div>
  <div class="page">
    <p>Fault Resilient ARMCI  Communication Protocols</p>
    <p>Multiple phases of communication in ARMCI Put/Get/Acc are implemented as a combination of these phases On Failures</p>
    <p>Either process/thread may be waiting for data, while other process is dead</p>
    <p>Use Timeout based FTMI to detect failures</p>
    <p>If FTMI detects failure, return error, if necessary</p>
    <p>Asynchronous Agent</p>
    <p>Process</p>
  </div>
  <div class="page">
    <p>Fault Resilient Collective Communication Primitives</p>
    <p>Barrier is fundamental non-data moving collective communication operation</p>
    <p>Ga_sync = Fence to all processes + Barrier</p>
    <p>Used at various execution points in ARMCI</p>
    <p>We have implemented multiple fault tolerant barrier algorithms</p>
    <p>Fault tolerant version of based on high concurrency all-to-all personalized exchange</p>
    <p>Fault tolerant version of hypercube based implementation</p>
  </div>
  <div class="page">
    <p>Usage of FTMI</p>
    <p>General functionality for fault detection Potential use as a component for fault tolerance backplane</p>
    <p>ARMCI Layer Different phases of one-sided communication protocols</p>
    <p>Put, Get, Accumulate</p>
    <p>Fault Resilient Barrier Different steps of the fault tolerant algorithm</p>
    <p>Potential use at application layer for designing recovery algorithm</p>
  </div>
  <div class="page">
    <p>Performance Evaluation Methodology</p>
    <p>Comparisons of following approaches: FT-ARMCI, No Fault</p>
    <p>FT-ARMCI, One Node Fault (Actual)</p>
    <p>Original</p>
    <p>Testbed: InfiniBand DDR with AMD Barcelona CPUs</p>
    <p>Used 128 Nodes for performance evaluation</p>
  </div>
  <div class="page">
    <p>Overhead of FT-ARMCI (No Faults)</p>
    <p>Latency comparisons for Original and FT-ARMCI implementation Objective is to understand the overhead of pure communication benchmarks</p>
    <p>Put and Accumulate primitives</p>
    <p>Overhead is observed due to synchronous writes</p>
    <p>Possible performance optimizations for future Piggyback acknowledgement with data</p>
  </div>
  <div class="page">
    <p>Performance Evaluation with Faults</p>
    <p>Pure communication Benchmark Real scientific application would have a combination of computation and communication</p>
    <p>Objective is to understand the overhead when a fault occurs</p>
    <p>The primary overhead is due to timeout by the hardware for fault detection Test continues to execute, when actual faults occur</p>
  </div>
  <div class="page">
    <p>Conclusions</p>
    <p>We presented the execution models of data centric programming models for fault tolerance We presented the design for fault tolerant communication runtime system</p>
    <p>Fault tolerance management infrastructure</p>
    <p>Process manager and barrier collective operation</p>
    <p>Fault tolerant communication protocols</p>
    <p>Our evaluation shows We can perform continued execution in presence of node faults</p>
    <p>Improvable overhead is observed in absence of faults</p>
    <p>Acceptable degradation in presence of node faults</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>This is a part of active R&amp;D: Leverage infrastructure for Global Arrays</p>
    <p>Significant application impact</p>
    <p>Handling simultaneous and consecutive failures</p>
    <p>Actively pursuing designs for upcoming high end systems Blue Waters, Gemini etc</p>
    <p>Our thanks to: eXtreme Scale Computing Initiative (XSCI) @ PNNL</p>
    <p>US Department of Energy</p>
    <p>Molecular Science Computing Facility @ PNNL</p>
  </div>
</Presentation>
