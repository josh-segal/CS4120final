<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>End-to-end I/O Monitoring on a Leading Supercomputer</p>
    <p>Bin Yang, Xu Ji, Xiaosong Ma, Xiyang Wang, Tianyu Zhang, Xiupeng Zhu, Nosayba El-Sayed,</p>
    <p>Yibo Yang, Haidong Lan, Jidong Zhai, Weiguo Liu, Wei Xue</p>
  </div>
  <div class="page">
    <p>Motivation: Large Supercomputers I/O Problems</p>
    <p>Application</p>
    <p>High-level I/O library MPI-IO implementation</p>
    <p>Parallel file system Storage hardware</p>
    <p>B yt</p>
    <p>e/ s</p>
    <p>/ F LO</p>
    <p>P S</p>
    <p>I/O vs. computation bandwidth of top supercomputers in past decade</p>
    <p>Supercomputers useful through data  I/O lags behind computation</p>
    <p>Secondary storage slower than processors/memory</p>
    <p>Long I/O path Less scalable or consistent, shared resources among many applications</p>
    <p>Applications tightly control time spent on I/O</p>
  </div>
  <div class="page">
    <p>Motivation: I/O System Not Well Utilized</p>
    <p>Performance bottleneck, contention point, AND system under-utilization!</p>
  </div>
  <div class="page">
    <p>Beacon: End-to-end Supercomputer I/O Monitoring</p>
    <p>Understand HPC I/O for designing future systems/applications  Lightweight end-to-end I/O resource monitoring</p>
    <p>Collects per-application-level traces and statistics  Correlates monitoring data from multiple system levels  Performance diagnosis, application I/O profiling, anomaly detection</p>
    <p>Deployed at TaihuLight, worlds No.3 supercomputer  Production use since April 2017  No user effort required  Identified design/configuration/policy problems  Code and monitoring data released</p>
  </div>
  <div class="page">
    <p>Monitoring and management network (Gigabit Ethernet)</p>
    <p>Architecture Overview of TaihuLight Online1 (default) Online2 (reserved)</p>
    <p>I/O forwarding nodes (288)</p>
    <p>Storage nodes (2*144)</p>
    <p>Metadata nodes (2)</p>
    <p>Login nodes</p>
    <p>IB FDR</p>
    <p>Forwarding network</p>
    <p>TaihuLight compute nodes (40960)</p>
    <p>Supernode (256 nodes)</p>
    <p>IB FDR</p>
    <p>Storage network</p>
    <p>Sugon 60disk arrays</p>
    <p>(72)</p>
  </div>
  <div class="page">
    <p>Overview of Beacon LWFS client</p>
    <p>Lustre client</p>
    <p>LWFS server Lustre server MDS</p>
    <p>I/O data collector (Logstash)</p>
    <p>In-memory cache (Redis) Distributed I/O record database (Elasticsearch)</p>
    <p>Web interface for query/visualization</p>
    <p>In-memory cache for recent Jobs (Redis)</p>
    <p>Per-job I/O information summarization (MySQL) Log processing&amp;inter-node compression (Elasticsearch)</p>
    <p>Dedicated Beacon server</p>
    <p>App users</p>
    <p>Sys admin</p>
    <p>Job database (MySQL)</p>
    <p>N85</p>
    <p>Tracing</p>
    <p>Profiling</p>
    <p>Sampled monitoring</p>
    <p>N1 N80 N81 N82 N83 N84</p>
    <p>Periodic update</p>
    <p>Compute nodes Forwarding nodes Storage nodes Metadata servers</p>
    <p>Client-side compression</p>
  </div>
  <div class="page">
    <p>Multi-level I/O Monitoring</p>
    <p>Node IP Timestamp Operation File descriptor Offset Size Count</p>
    <p>LWFS client trace entry</p>
    <p>LWFS server log entry</p>
    <p>Fwd IP Timestamp Operation Latency Execution time Count Queue length</p>
    <p>Fwd IP OST ID Read RPC Count Write RPC Count Pages per RPC Snapshot Time</p>
    <p>Lustre server log entry</p>
    <p>OST ID Read RPC Count Write RPC Count Pages per RPC Snapshot Time</p>
    <p>Job ID Nodelist Job Name Start time End Time</p>
    <p>MDS log entry Fwd IP Operation1 Count Opearation2 Count  OperationN Count Snapshot Time</p>
    <p>Lustre client log entry</p>
    <p>Job scheduler log entry</p>
  </div>
  <div class="page">
    <p>Cross-layer I/O Profiling Data Analysis</p>
    <p>Per-job I/O performance analysis  Job to compute nodes: batch scheduler history lookup  Compute to forwarding nodes: per-job mapping info lookup  Forwarding to storage nodes: file system lookup commands</p>
    <p>I/O subsystem monitoring for administrators  Visualization and query services  Any node, any time</p>
    <p>Automatic anomaly detection</p>
  </div>
  <div class="page">
    <p>Beacon Demo</p>
  </div>
  <div class="page">
    <p>Case Study 1: Application I/O Behavior Analysis</p>
    <p>Read</p>
    <p>N-N N-M N-1 1-1</p>
    <p>Write</p>
    <p>N-N N-M N-1 1-1 Distribution of file access modes, in access volume</p>
    <p>N-1 10%</p>
    <p>N-M 1%</p>
    <p>N-N 63%</p>
    <p>N-1 2%</p>
    <p>N-M 2%</p>
    <p>N-N 49%</p>
    <p>File</p>
    <p>N-N I/O mode</p>
    <p>P1</p>
    <p>File1</p>
    <p>P2</p>
    <p>File2</p>
    <p>PN</p>
    <p>FileN</p>
    <p>N-1 I/O mode</p>
    <p>P1</p>
    <p>File</p>
    <p>P2 PN N-M I/O mode</p>
    <p>P1</p>
    <p>File1</p>
    <p>P2</p>
    <p>File2</p>
    <p>PM</p>
    <p>FileM</p>
    <p>PN</p>
    <p>Based on 18-month data collection on TaihuLight  116,765 jobs using at least 32 compute nodes</p>
  </div>
  <div class="page">
    <p>Case Study 1: Application I/O Behavior Analysis</p>
    <p>Read</p>
    <p>N-N N-M N-1 1-1</p>
    <p>Write</p>
    <p>N-N N-M N-1 1-1 Distribution of file access modes, in access volume</p>
    <p>N-1 10%</p>
    <p>N-M 1%</p>
    <p>N-N 63%</p>
    <p>N-1 2%</p>
    <p>N-M 2%</p>
    <p>N-N 49%</p>
    <p>File</p>
    <p>N-N I/O mode</p>
    <p>P1</p>
    <p>File1</p>
    <p>P2</p>
    <p>File2</p>
    <p>PN</p>
    <p>FileN</p>
    <p>N-1 I/O mode</p>
    <p>P1</p>
    <p>File</p>
    <p>P2 PN N-M I/O mode</p>
    <p>P1</p>
    <p>File1</p>
    <p>P2</p>
    <p>File2</p>
    <p>PM</p>
    <p>FileM</p>
    <p>PN Problem: inefficient I/O modes  Large portion of I/O using sequential (1-1) mode  N-1 still used  N-M (parallel I/O w. aggregation) rarely adopted</p>
    <p>Based on 18-month data collection on TaihuLight  116,765 jobs using at least 32 compute nodes</p>
    <p>Solution: application/user behavior correction  Targeted suggestions to large-scale, I/O-intensive applications</p>
  </div>
  <div class="page">
    <p>I/O v</p>
    <p>ol um</p>
    <p>e (T</p>
    <p>B )</p>
    <p>Time (hour)</p>
    <p>Sample read volume history (thrashing)</p>
    <p>Comp_Read Fwd_Read</p>
    <p>Case Study 2: Cross-layer I/O Volume Comparison</p>
    <p>I/O v</p>
    <p>ol um</p>
    <p>e (T</p>
    <p>B )</p>
    <p>Time (hour)</p>
    <p>Sample read volume history (normal)</p>
    <p>Comp_Read Fwd_Read</p>
    <p>Problem: cache thrashing at forwarding nodes  Lustre aggressive prefetching  N-N I/O mode</p>
    <p>Solution: reducing per-thread prefetching within same application  Applying per-file prefetching limit  Switching from N-N to N-M mode</p>
  </div>
  <div class="page">
    <p>Case Study 3: Unbalanced Forwarding Node Utilization</p>
    <p>Sample TaihuLight one-day load summary, showing peak load level by hour</p>
    <p>Problem: forwarding resource over-provisioning and load imbalance  Mostly under-utilized  Overwhelmed under intense I/O bursts  Application-oblivious forwarding node allocation</p>
    <p>(a) Regular load (07/01/2017) (b) Heavy load (07/28/2017)</p>
  </div>
  <div class="page">
    <p>Case Study 3: Unbalanced Forwarding Node Utilization</p>
    <p>Sample TaihuLight one-day load summary, showing peak load level by hour</p>
    <p>Solution: DFRA (Dynamic Forwarding Resource Allocation)  Automatic, application-aware allocation and isolation  FAST 2019, presentation in an hour</p>
    <p>(a) Regular load (07/01/2017) (b) Heavy load (07/28/2017)</p>
  </div>
  <div class="page">
    <p>Case Study 4: MDS Request Priority Setting</p>
    <p>Problem: metadata requests given priority over file I/O, based on  Single-MDS design  Need for interactive user experience</p>
    <p>M D</p>
    <p>O P</p>
    <p>S</p>
    <p>Time (s)</p>
    <p>DNDC-solo</p>
    <p>DNDC-corun (before)</p>
    <p>DNDC-corun (after)</p>
    <p>I/O th</p>
    <p>ro ug</p>
    <p>hp ut</p>
    <p>(G B</p>
    <p>/s )</p>
    <p>Time (s)</p>
    <p>LAMMPS-solo</p>
    <p>LAMMPS-corun (before)</p>
    <p>LAMMPS-corun (after)</p>
    <p>Solution: probabilistic processing between two queues  Metadata requests stop receiving full priority</p>
  </div>
  <div class="page">
    <p>Case Study 5: Anomaly Detection Example</p>
    <p>I/O b</p>
    <p>an dw</p>
    <p>id th</p>
    <p>(M</p>
    <p>B /s</p>
    <p>)</p>
    <p>OST ID</p>
    <p>Sample OST performance during Shentu run</p>
    <p>Parallel graph engine Shentu  160,000 processes  400 Lustre OSTs</p>
    <p>Solution: automatic screening of OSTs for performance anomaly  Redirect from slow ones temporarily</p>
  </div>
  <div class="page">
    <p>Beacon Evaluation: Overhead on Applications Application #Process Tw/o (s) Tw (s) % Slowdown</p>
    <p>MPI-ION 64 26.6 26.8 0.79%</p>
    <p>MPI-ION 128 31.5 31.6 0.25%</p>
    <p>MPI-ION 256 41.6 41.9 0.72%</p>
    <p>MPI-ION 512 57.9 58.4 0.86%</p>
    <p>MPI-ION 1024 123.1 123.5 0.36%</p>
    <p>WRF1 1024 2813.3 2819.1 0.21%</p>
    <p>DNDC 2048 1041.2 1045.5 0.53%</p>
    <p>XCFD 4000 2642.1 2644.6 0.09%</p>
    <p>GKUA 16384 297.5 299.9 0.82%</p>
    <p>GKUA 32768 182.8 184.1 0.66%</p>
    <p>AWP 130000 3233.5 3241.5 0.25%</p>
    <p>Shentu 160000 5468.2 5476.3 0.15%</p>
    <p>Average application execution time: before and after Beacon turned on 1717</p>
  </div>
  <div class="page">
    <p>Beacon Evaluation: Resource Consumption Node type CPU Usage Memory Usage (MB) Compute node 0.0% 10</p>
    <p>Forwarding node 0.1% 6</p>
    <p>Storage node 0.1% 5</p>
    <p>Average resource consumption on monitoring nodes</p>
    <p>Web interface for query/visualization</p>
    <p>In-memory cache for recent Jobs (Redis)</p>
    <p>Per-job I/O information summarization (MySQL) Log processing &amp; inter-node compression (Elasticsearch)</p>
    <p>Dedicated Beacon server</p>
    <p>App users</p>
    <p>Sys admin</p>
    <p>Space consumption on dedicated Beacon server: ~10TB monitoring data (after 2 rounds of compression) in 18 months</p>
  </div>
  <div class="page">
    <p>Generality</p>
    <p>Beacon applies to other supercomputers  General building blocks</p>
    <p>Operation log collection and compression  Scheduler-assisted, per-application data correlation and analysis  Online anomaly identification</p>
    <p>Implemented using popular, open-source software  Logstash, Redis, ElasticSearch, MySQL</p>
  </div>
  <div class="page">
    <p>Conclusion</p>
    <p>https://github.com/Beaconsys/Beacon</p>
    <p>Detailed, end-to-end I/O monitoring affordable and effective  Helps understand sources of inefficiency  Exposes hidden design or configuration problems  Facilitates better application I/O practice</p>
  </div>
</Presentation>
