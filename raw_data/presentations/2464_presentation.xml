<?xml version="1.0" ?>
<Presentation>
  <div class="page">
    <p>Using Provenance to Extract Semantic File Attributes</p>
    <p>Daniel Margo and Robin Smogor Harvard University</p>
  </div>
  <div class="page">
    <p>Semantic Attributes</p>
    <p>Human-meaningful data adjectives.</p>
    <p>Applications:  Search (Google Desktop, Windows Live)</p>
    <p>Namespaces (iTunes, Perspective [Salmon, FAST'09])</p>
    <p>Preference Solicitation (Pandora)</p>
    <p>And more...</p>
    <p>Make data more valuable (like provenance!) - Only...</p>
  </div>
  <div class="page">
    <p>Where do Attributes Come From?</p>
    <p>Manual labeling - intractable.</p>
    <p>Automated content extraction:</p>
    <p>Arguably, Google.</p>
    <p>Visual extraction (La Cascia et al., '98)</p>
    <p>Acoustic extraction (QueST, MULTIMEDIA'07)</p>
    <p>Problems:</p>
    <p>Need extractors for each content type.</p>
    <p>Ignorant of inter-data relationships: dependency, history, usage, provenance, context.</p>
  </div>
  <div class="page">
    <p>How Might Context Predict Attributes? Examples:</p>
    <p>If an application always reads a file in its directory, that file is probably a component.</p>
    <p>If an application occasionally writes a file outside its directory, that's probably content.</p>
    <p>Etc...</p>
    <p>Prior work:</p>
    <p>Context search [Gyllstrom IUI'08, Shah USENIX'07]</p>
    <p>Attribute propagation via context [Soules '04]</p>
  </div>
  <div class="page">
    <p>The Goal</p>
    <p>File relationships  attribute predictions.</p>
    <p>Begin with a provenance-aware system (PASS)</p>
    <p>Run some file-oriented workflow(s).</p>
    <p>Output per-file data into a machine learner.</p>
    <p>Train learner to predict semantic attributes.  Simple! Only...</p>
  </div>
  <div class="page">
    <p>The Challenge</p>
    <p>...like fitting a square peg into a round hole!</p>
    <p>Provenance  graphs  quadratic scale.</p>
    <p>Typical learner handles ~hundreds of features.</p>
    <p>Needs relevant feature extraction.</p>
    <p>Going to throw out a lot of data.</p>
  </div>
  <div class="page">
    <p>about:PASS</p>
    <p>Linux research kernel.</p>
    <p>Collects provenance at system call interface.</p>
    <p>Logs file and process provenance as a DAG.</p>
    <p>Nodes are versions of files and processes.</p>
    <p>Must resolve many-to-one node to file mapping.</p>
  </div>
  <div class="page">
    <p>Resolving Nodes to Files</p>
    <p>Simple solution: discard version data.</p>
    <p>Introduces cycles (false dependencies).</p>
    <p>Increases graph density.</p>
    <p>Alternatively: merge nodes by file name.</p>
    <p>Similar to above; introduces more falsity.</p>
    <p>But guarantees direct mapping.</p>
    <p>More complicated post-processing?</p>
    <p>Future work.</p>
  </div>
  <div class="page">
    <p>Graph Transformations</p>
    <p>File graph: reduce graph to just files.</p>
    <p>Emphasizes data dependency, e.g. libraries.</p>
    <p>Process graph: reduce graph to just processes.</p>
    <p>Emphasizes workflow, omits specific inputs.</p>
    <p>Ancestor and descendant subgraphs.</p>
    <p>Defined as transitive closure.</p>
    <p>On a per-file basis.</p>
  </div>
  <div class="page">
    <p>Statistics</p>
    <p>How to convert per-file subgraphs to statistics?</p>
    <p>Experiments with partitioning, clustering:</p>
    <p>Graclus (partitioner), GraphClust.</p>
    <p>Failure: graph sparsity, different structural assumptions produce poor results.</p>
    <p>Success with dumb statistics:</p>
    <p>Node and edge counts, path depths, neighbors.</p>
    <p>For both ancestor and descendant graphs.</p>
    <p>Still a work in progress.</p>
  </div>
  <div class="page">
    <p>Feature Extraction: Summary</p>
    <p>2 ways to merge (by versions or path names).</p>
    <p>3 graph representations (full, process, file).</p>
    <p>4 statistics for both ancestors and descendants.</p>
    <p>Totals 48 possible features-per-file...</p>
    <p>...plus 11 features from stat syscall.</p>
    <p>Content-free metadata.</p>
  </div>
  <div class="page">
    <p>Classification</p>
    <p>Classification via decision trees.</p>
    <p>Transparent logic: can evaluate, conclude, improve.</p>
    <p>Standard decision tree techniques:</p>
    <p>Prune splits via lower bound on information gain.</p>
    <p>Train on 90% of data set, validate on 10%.</p>
    <p>k-means to collapse real-valued feature spaces.</p>
    <p>Requires labeled training data...</p>
  </div>
  <div class="page">
    <p>Labeling Problem</p>
    <p>First challenge: how to label training data?</p>
    <p>Semantic attributes are subjective.</p>
    <p>No reason provenance should predict any random attribute; must be well-chosen.</p>
  </div>
  <div class="page">
    <p>Labeling Solution</p>
    <p>Initial evaluation using file extensions as label.</p>
    <p>Semantically meaningful, but not subjective.  Pre-labeled.  Intuitively, usage predicts file type.  Reverse has been shown: extension predicts usage</p>
    <p>[Mesnier ICAC'04].</p>
  </div>
  <div class="page">
    <p>Whats the Data Set?</p>
    <p>Second challenge: finding a data set.</p>
    <p>Needs a large heterogeneous file workflow.</p>
    <p>Still a work in progress.</p>
    <p>In interim, Linux kernel compile.</p>
    <p>138,243 nodes, 1,338,134 edges, 68,312 deversioned nodes, 34,347 unique path names, and 21,650 files-on-disk (manifest files).</p>
    <p>Long brute-force analysis; used 23 features.</p>
  </div>
  <div class="page">
    <p>Precision, Recall, and Accuracy</p>
    <p>Standard metrics in machine learning:</p>
    <p>Precision: for a given extension prediction, how many predictions were correct?</p>
    <p>Recall: for a given extension, how many files with that extension received the correct prediction?</p>
    <p>Accuracy: how many of all the files received the correct prediction?</p>
  </div>
  <div class="page">
    <p>Results</p>
    <p>85.68% extension prediction accuracy.</p>
    <p>79.79% on manifest files (present on disk).</p>
    <p>Table at left.  Confuses source files.  If fixed, 94.08%.</p>
    <p>93.76% on nonmanifest objects.</p>
  </div>
  <div class="page">
    <p>Number of Records Needed</p>
  </div>
  <div class="page">
    <p>Talking Points</p>
    <p>Is source file confusion wrong?</p>
    <p>.c/.h/.S have similar usage from PASS perspective.</p>
    <p>source file may be right semantic level.</p>
    <p>Can fix using 2nd-degree neighbors (object files).</p>
    <p>Other than this, high accuracy.</p>
    <p>Especially on non-manifest objects  content-free.</p>
    <p>Noteworthy features  ancestral file count, edge count, max path depth; descendant edge count</p>
  </div>
  <div class="page">
    <p>Future Work</p>
    <p>More feature extraction.</p>
    <p>Evaluate more attributes...</p>
    <p>...on more data sets.</p>
    <p>More sophisticated classifiers (neural nets).</p>
    <p>Better understanding!</p>
  </div>
</Presentation>
